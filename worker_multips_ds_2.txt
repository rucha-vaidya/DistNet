Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Received variable values from ps
Connected to both PSs
2017-05-09 20:59:51.996147: step 0, loss = 4.68 (72.3 examples/sec; 1.769 sec/batch)
2017-05-09 20:59:53.042485: step 10, loss = 4.41 (1223.4 examples/sec; 0.105 sec/batch)
2017-05-09 20:59:54.306129: step 20, loss = 4.13 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-09 20:59:55.578043: step 30, loss = 4.31 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-09 20:59:56.859971: step 40, loss = 4.18 (998.5 examples/sec; 0.128 sec/batch)
2017-05-09 20:59:58.129732: step 50, loss = 4.12 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-09 20:59:59.432054: step 60, loss = 3.94 (982.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:00:00.695422: step 70, loss = 4.00 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-09 21:00:01.987629: step 80, loss = 3.88 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:00:03.273090: step 90, loss = 3.89 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:00:04.650797: step 100, loss = 3.95 (929.1 examples/sec; 0.138 sec/batch)
2017-05-09 21:00:05.822036: step 110, loss = 3.60 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-09 21:00:07.087530: step 120, loss = 3.59 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-09 21:00:08.356303: step 130, loss = 3.69 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-09 21:00:09.665431: step 140, loss = 3.63 (977.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:00:10.918790: step 150, loss = 3.69 (1021.3 examples/sec; 0.125 sec/batch)
2017-05-09 21:00:12.175758: step 160, loss = 3.85 (1018.3 examples/sec; 0.126 sec/batch)
2017-05-09 21:00:13.466477: step 170, loss = 3.36 (991.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:00:14.763081: step 180, loss = 3.82 (987.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:00:16.020158: step 190, loss = 3.38 (1018.2 examples/sec; 0.126 sec/batch)
2017-05-09 21:00:17.408831: step 200, loss = 3.40 (921.7 examples/sec; 0.139 sec/batch)
2017-05-09 21:00:18.582853: step 210, loss = 3.42 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-09 21:00:19.893889: step 220, loss = 3.41 (976.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:00:21.199584: step 230, loss = 3.33 (980.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:00:22.502635: step 240, loss = 3.38 (982.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:00:23.795937: step 250, loss = 3.34 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:00:25.069233: step 260, loss = 3.31 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:00:26.332909: step 270, loss = 3.19 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-09 21:00:27.594742: step 280, loss = 3.07 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-09 21:00:28.878606: step 290, loss = 3.00 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:00:30.247613: step 300, loss = 3.14 (935.0 examples/sec; 0.137 sec/batch)
2017-05-09 21:00:31.413050: step 310, loss = 3.16 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-09 21:00:32.678112: step 320, loss = 2.92 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-09 21:00:33.967258: step 330, loss = 3.19 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:00:35.236726: step 340, loss = 2.90 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:00:36.533934: step 350, loss = 3.01 (986.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:00:37.806399: step 360, loss = 2.77 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:00:39.111367: step 370, loss = 3.09 (980.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:00:40.409083: step 380, loss = 2.88 (986.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:00:41.724839: step 390, loss = 2.90 (972.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:00:43.145668: step 400, loss = 2.86 (900.9 examples/sec; 0.142 sec/batch)
2017-05-09 21:00:44.331811: step 410, loss = 2.84 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-09 21:00:45.599852: step 420, loss = 2.78 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:00:46.896573: step 430, loss = 2.70 (987.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:00:48.181750: step 440, loss = 2.78 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:00:49.460678: step 450, loss = 2.83 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:00:50.750040: step 460, loss = 2.75 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:00:52.036888: step 470, loss = 2.58 (994.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:00:53.333233: step 480, loss = 2.60 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:00:54.627741: step 490, loss = 2.63 (988.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:00:56.042160: step 500, loss = 2.79 (905.0 examples/sec; 0.141 sec/batch)
2017-05-09 21:00:57.217202: step 510, loss = 2.47 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-09 21:00:58.506931: step 520, loss = 2.49 (992.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:00:59.813656: step 530, loss = 2.72 (979.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:01:01.143018: step 540, loss = 2.60 (962.9 examples/sec; 0.133 sec/batch)
2017-05-09 21:01:02.430108: step 550, loss = 2.48 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:01:03.703811: step 560, loss = 2.45 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:01:04.985712: step 570, loss = 2.41 (998.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:01:06.275592: step 580, loss = 2.13 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:01:07.560502: step 590, loss = 2.70 (996.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:01:08.947301: step 600, loss = 2.23 (923.0 examples/sec; 0.139 sec/batch)
2017-05-09 21:01:10.119788: step 610, loss = 2.57 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-09 21:01:11.395249: step 620, loss = 2.37 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:01:12.682667: step 630, loss = 2.37 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:01:13.985892: step 640, loss = 2.65 (982.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:01:15.276357: step 650, loss = 2.17 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:01:16.559323: step 660, loss = 2.60 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:01:17.857703: step 670, loss = 2.22 (985.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:01:19.170988: step 680, loss = 2.51 (974.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:01:20.454996: step 690, loss = 2.24 (996.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:01:21.846597: step 700, loss = 2.22 (919.8 examples/sec; 0.139 sec/batch)
2017-05-09 21:01:23.012544: step 710, loss = 2.25 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-09 21:01:24.291486: step 720, loss = 2.16 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:01:25.566856: step 730, loss = 2.23 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:01:26.838537: step 740, loss = 2.16 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-09 21:01:28.127512: step 750, loss = 2.19 (993.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:01:29.392007: step 760, loss = 2.25 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-09 21:01:30.674137: step 770, loss = 2.20 (998.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:01:31.979474: step 780, loss = 2.05 (980.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:01:33.296544: step 790, loss = 2.05 (971.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:01:34.701823: step 800, loss = 1.99 (910.9 examples/sec; 0.141 sec/batch)
2017-05-09 21:01:35.896811: step 810, loss = 1.97 (1071.1 examples/sec; 0.119 sec/batch)
2017-05-09 21:01:37.180637: step 820, loss = 1.98 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:01:38.462019: step 830, loss = 2.18 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:01:39.761889: step 840, loss = 1.98 (984.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:01:41.041223: step 850, loss = 2.13 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:01:42.338923: step 860, loss = 2.02 (986.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:01:43.643801: step 870, loss = 2.13 (980.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:01:44.895805: step 880, loss = 2.06 (1022.4 examples/sec; 0.125 sec/batch)
2017-05-09 21:01:46.166644: step 890, loss = 2.03 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:01:47.544637: step 900, loss = 1.91 (928.9 examples/sec; 0.138 sec/batch)
2017-05-09 21:01:48.718998: step 910, loss = 1.86 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-09 21:01:50.036071: step 920, loss = 2.03 (971.9 examples/sec; 0.132 sec/batch)
2017-05-09 21:01:51.312987: step 930, loss = 1.97 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:01:52.613615: step 940, loss = 2.10 (984.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:01:53.922362: step 950, loss = 2.02 (978.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:01:55.207308: step 960, loss = 1.95 (996.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:01:56.475319: step 970, loss = 1.69 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-09 21:01:57.761145: step 980, loss = 2.01 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:01:59.049657: step 990, loss = 1.83 (993.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:02:00.439097: step 1000, loss = 1.88 (921.2 examples/sec; 0.139 sec/batch)
2017-05-09 21:02:01.614536: step 1010, loss = 2.01 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-09 21:02:02.882949: step 1020, loss = 1.75 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:02:04.133598: step 1030, loss = 1.83 (1023.5 examples/sec; 0.125 sec/batch)
2017-05-09 21:02:05.414480: step 1040, loss = 1.86 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:02:06.729079: step 1050, loss = 1.98 (973.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:02:08.002805: step 1060, loss = 1.84 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:02:09.282320: step 1070, loss = 2.11 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:02:10.617918: step 1080, loss = 2.06 (958.4 examples/sec; 0.134 sec/batch)
2017-05-09 21:02:11.927951: step 1090, loss = 1.99 (977.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:02:13.348834: step 1100, loss = 1.70 (900.8 examples/sec; 0.142 sec/batch)
2017-05-09 21:02:14.546372: step 1110, loss = 1.89 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-09 21:02:15.836893: step 1120, loss = 2.00 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:02:17.165660: step 1130, loss = 1.69 (963.3 examples/sec; 0.133 sec/batch)
2017-05-09 21:02:18.462869: step 1140, loss = 1.75 (986.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:02:19.767940: step 1150, loss = 1.64 (980.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:02:21.034376: step 1160, loss = 1.62 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:02:22.315830: step 1170, loss = 1.84 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:02:23.604488: step 1180, loss = 1.60 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:02:24.902913: step 1190, loss = 1.63 (985.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:02:26.282135: step 1200, loss = 1.45 (928.1 examples/sec; 0.138 sec/batch)
2017-05-09 21:02:27.441285: step 1210, loss = 1.64 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-09 21:02:28.710058: step 1220, loss = 1.59 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:02:29.990435: step 1230, loss = 1.69 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:02:31.260700: step 1240, loss = 1.67 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:02:32.529176: step 1250, loss = 1.76 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:02:33.808552: step 1260, loss = 1.61 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:02:35.093384: step 1270, loss = 1.66 (996.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:02:36.386653: step 1280, loss = 1.55 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:02:37.664861: step 1290, loss = 1.91 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:02:39.046091: step 1300, loss = 1.50 (926.7 examples/sec; 0.138 sec/batch)
2017-05-09 21:02:40.221703: step 1310, loss = 1.41 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-09 21:02:41.535985: step 1320, loss = 1.57 (973.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:02:42.822949: step 1330, loss = 1.83 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:02:44.117639: step 1340, loss = 2.06 (988.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:02:45.413964: step 1350, loss = 1.43 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:02:46.698747: step 1360, loss = 1.54 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:02:47.964627: step 1370, loss = 1.63 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:02:49.235526: step 1380, loss = 1.59 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:02:50.507984: step 1390, loss = 1.55 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:02:51.958101: step 1400, loss = 1.51 (882.7 examples/sec; 0.145 sec/batch)
2017-05-09 21:02:53.120339: step 1410, loss = 1.78 (1101.3 examples/sec; 0.116 sec/batch)
2017-05-09 21:02:54.404366: step 1420, loss = 1.78 (996.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:02:55.683858: step 1430, loss = 1.46 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:02:56.962351: step 1440, loss = 1.43 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:02:58.242644: step 1450, loss = 1.39 (999.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:02:59.527251: step 1460, loss = 1.47 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:03:00.790077: step 1470, loss = 1.58 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-09 21:03:02.065580: step 1480, loss = 1.65 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:03:03.353365: step 1490, loss = 1.82 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:03:04.743252: step 1500, loss = 1.68 (920.9 examples/sec; 0.139 sec/batch)
2017-05-09 21:03:05.938498: step 1510, loss = 1.46 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-09 21:03:07.206255: step 1520, loss = 1.49 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:03:08.483246: step 1530, loss = 1.54 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:03:09.756047: step 1540, loss = 1.36 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:03:11.014997: step 1550, loss = 1.43 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-09 21:03:12.296586: step 1560, loss = 1.55 (998.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:03:13.591567: step 1570, loss = 1.51 (988.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:03:14.869552: step 1580, loss = 1.38 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:03:16.154896: step 1590, loss = 1.33 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:03:17.531743: step 1600, loss = 1.45 (929.7 examples/sec; 0.138 sec/batch)
2017-05-09 21:03:18.728530: step 1610, loss = 1.33 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-09 21:03:19.998239: step 1620, loss = 1.50 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:03:21.282753: step 1630, loss = 1.49 (996.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:03:22.611700: step 1640, loss = 1.48 (963.2 examples/sec; 0.133 sec/batch)
2017-05-09 21:03:23.910246: step 1650, loss = 1.48 (985.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:03:25.191339: step 1660, loss = 1.45 (999.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:03:26.486053: step 1670, loss = 1.54 (988.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:03:27.785639: step 1680, loss = 1.21 (984.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:03:29.080123: step 1690, loss = 1.67 (988.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:03:30.462770: step 1700, loss = 1.45 (925.8 examples/sec; 0.138 sec/batch)
2017-05-09 21:03:31.656733: step 1710, loss = 1.28 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-09 21:03:32.952896: step 1720, loss = 1.37 (987.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:03:34.240701: step 1730, loss = 1.51 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:03:35.544025: step 1740, loss = 1.16 (982.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:03:36.827965: step 1750, loss = 1.36 (996.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:03:38.122836: step 1760, loss = 1.41 (988.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:03:39.400837: step 1770, loss = 1.23 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:03:40.702698: step 1780, loss = 1.20 (983.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:03:42.018865: step 1790, loss = 1.68 (972.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:03:43.403163: step 1800, loss = 1.24 (924.7 examples/sec; 0.138 sec/batch)
2017-05-09 21:03:44.574651: step 1810, loss = 1.45 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-09 21:03:45.862522: step 1820, loss = 1.40 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:03:47.155999: step 1830, loss = 1.29 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:03:48.451789: step 1840, loss = 1.27 (987.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:03:49.744130: step 1850, loss = 1.63 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:03:51.017022: step 1860, loss = 1.13 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:03:52.292128: step 1870, loss = 1.03 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:03:53.612326: step 1880, loss = 1.53 (969.6 examples/sec; 0.132 sec/batch)
2017-05-09 21:03:54.888408: step 1890, loss = 1.36 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:03:56.281947: step 1900, loss = 1.31 (918.5 examples/sec; 0.139 sec/batch)
2017-05-09 21:03:57.471818: step 1910, loss = 1.32 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-09 21:03:58.753951: step 1920, loss = 1.29 (998.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:04:00.032149: step 1930, loss = 1.41 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:04:01.353740: step 1940, loss = 1.70 (968.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:04:02.647301: step 1950, loss = 1.44 (989.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:04:03.942841: step 1960, loss = 1.28 (988.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:04:05.211198: step 1970, loss = 1.46 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:04:06.523934: step 1980, loss = 1.30 (975.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:04:07.808868: step 1990, loss = 1.31 (996.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:04:09.177593: step 2000, loss = 1.29 (935.2 examples/sec; 0.137 sec/batch)
2017-05-09 21:04:10.379089: step 2010, loss = 1.29 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-09 21:04:11.665237: step 2020, loss = 1.12 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:04:12.959792: step 2030, loss = 1.13 (988.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:04:14.246029: step 2040, loss = 1.18 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:04:15.529697: step 2050, loss = 1.53 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:04:16.816661: step 2060, loss = 1.35 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:04:18.088558: step 2070, loss = 1.49 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:04:19.372677: step 2080, loss = 1.09 (996.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:04:20.649689: step 2090, loss = 1.37 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:04:22.034969: step 2100, loss = 1.24 (924.0 examples/sec; 0.139 sec/batch)
2017-05-09 21:04:23.217201: step 2110, loss = 1.25 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-09 21:04:24.484768: step 2120, loss = 0.99 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-09 21:04:25.768729: step 2130, loss = 1.31 (996.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:04:27.065076: step 2140, loss = 1.08 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:04:28.345097: step 2150, loss = 1.38 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:04:29.621899: step 2160, loss = 1.18 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:04:30.914161: step 2170, loss = 1.27 (990.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:04:32.187438: step 2180, loss = 1.51 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:04:33.481104: step 2190, loss = 1.58 (989.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:04:34.843750: step 2200, loss = 1.26 (939.3 examples/sec; 0.136 sec/batch)
2017-05-09 21:04:36.030747: step 2210, loss = 1.20 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-09 21:04:37.322838: step 2220, loss = 1.23 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:04:38.610265: step 2230, loss = 1.16 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:04:39.888665: step 2240, loss = 1.16 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:04:41.194055: step 2250, loss = 1.32 (980.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:04:42.442113: step 2260, loss = 1.18 (1025.6 examples/sec; 0.125 sec/batch)
2017-05-09 21:04:43.747273: step 2270, loss = 1.48 (980.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:04:45.052636: step 2280, loss = 1.02 (980.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:04:46.360426: step 2290, loss = 1.22 (978.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:04:47.743826: step 2300, loss = 1.33 (925.3 examples/sec; 0.138 sec/batch)
2017-05-09 21:04:48.907716: step 2310, loss = 1.12 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-09 21:04:50.202169: step 2320, loss = 1.31 (988.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:04:51.472045: step 2330, loss = 1.26 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:04:52.775632: step 2340, loss = 1.19 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:04:54.050970: step 2350, loss = 1.20 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:04:55.358270: step 2360, loss = 1.18 (979.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:04:56.628552: step 2370, loss = 1.24 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:04:57.920722: step 2380, loss = 1.49 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:04:59.203689: step 2390, loss = 1.14 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:05:00.580905: step 2400, loss = 1.15 (929.4 examples/sec; 0.138 sec/batch)
2017-05-09 21:05:01.791630: step 2410, loss = 1.14 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-09 21:05:03.066439: step 2420, loss = 1.19 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:05:04.355813: step 2430, loss = 1.32 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:05:05.625147: step 2440, loss = 1.19 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:05:06.926341: step 2450, loss = 1.28 (983.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:05:08.189574: step 2460, loss = 1.15 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-09 21:05:09.471665: step 2470, loss = 1.28 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:05:10.757785: step 2480, loss = 1.13 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:05:12.025034: step 2490, loss = 1.24 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:05:13.434716: step 2500, loss = 1.07 (908.0 examples/sec; 0.141 sec/batch)
2017-05-09 21:05:14.634418: step 2510, loss = 1.28 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-09 21:05:15.914925: step 2520, loss = 1.14 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:05:17.188866: step 2530, loss = 1.12 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-09 21:05:18.454287: step 2540, loss = 1.38 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-09 21:05:19.733857: step 2550, loss = 1.26 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:05:21.021952: step 2560, loss = 1.26 (993.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:05:22.308533: step 2570, loss = 1.09 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:05:23.622684: step 2580, loss = 1.39 (974.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:05:24.931778: step 2590, loss = 1.37 (977.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:05:26.307824: step 2600, loss = 1.19 (930.2 examples/sec; 0.138 sec/batch)
2017-05-09 21:05:27.513929: step 2610, loss = 1.45 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-09 21:05:28.808278: step 2620, loss = 1.10 (988.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:05:30.123760: step 2630, loss = 1.09 (973.0 examples/sec; 0.132 sec/batch)
2017-05-09 21:05:31.410091: step 2640, loss = 1.12 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:05:32.728394: step 2650, loss = 1.13 (970.9 examples/sec; 0.132 sec/batch)
2017-05-09 21:05:34.062860: step 2660, loss = 1.19 (959.2 examples/sec; 0.133 sec/batch)
2017-05-09 21:05:35.350776: step 2670, loss = 1.30 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:05:36.635128: step 2680, loss = 1.00 (996.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:05:37.911433: step 2690, loss = 1.22 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:05:39.301274: step 2700, loss = 1.15 (921.0 examples/sec; 0.139 sec/batch)
2017-05-09 21:05:40.504560: step 2710, loss = 1.17 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-09 21:05:41.787319: step 2720, loss = 1.19 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:05:43.099321: step 2730, loss = 1.19 (975.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:05:44.412681: step 2740, loss = 1.80 (974.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:05:45.717651: step 2750, loss = 1.23 (980.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:05:47.021081: step 2760, loss = 1.15 (982.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:05:48.322692: step 2770, loss = 1.24 (983.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:05:49.612518: step 2780, loss = 1.10 (992.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:05:50.903193: step 2790, loss = 1.09 (991.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:05:52.296217: step 2800, loss = 1.41 (918.9 examples/sec; 0.139 sec/batch)
2017-05-09 21:05:53.503792: step 2810, loss = 1.27 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-09 21:05:54.812658: step 2820, loss = 1.29 (977.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:05:56.118802: step 2830, loss = 1.15 (980.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:05:57.439680: step 2840, loss = 1.06 (969.1 examples/sec; 0.132 sec/batch)
2017-05-09 21:05:58.748672: step 2850, loss = 1.29 (977.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:06:00.023866: step 2860, loss = 0.92 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:06:01.304292: step 2870, loss = 1.08 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:06:02.595718: step 2880, loss = 0.88 (991.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:06:03.889095: step 2890, loss = 1.30 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:06:05.276762: step 2900, loss = 1.22 (922.4 examples/sec; 0.139 sec/batch)
2017-05-09 21:06:06.498582: step 2910, loss = 1.31 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-09 21:06:07.773383: step 2920, loss = 1.16 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:06:09.061283: step 2930, loss = 1.07 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:06:10.369896: step 2940, loss = 0.94 (978.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:06:11.634621: step 2950, loss = 0.91 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-09 21:06:12.911498: step 2960, loss = 1.08 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:06:14.200838: step 2970, loss = 1.03 (992.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:06:15.496089: step 2980, loss = 1.25 (988.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:06:16.758283: step 2990, loss = 1.13 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-09 21:06:18.144138: step 3000, loss = 1.18 (923.6 examples/sec; 0.139 sec/batch)
2017-05-09 21:06:19.349866: step 3010, loss = 0.99 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-09 21:06:20.662722: step 3020, loss = 1.08 (975.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:06:21.952341: step 3030, loss = 1.14 (992.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:06:23.222098: step 3040, loss = 1.23 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:06:24.512034: step 3050, loss = 1.00 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:06:25.814518: step 3060, loss = 1.15 (982.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:06:27.129424: step 3070, loss = 0.93 (973.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:06:28.423391: step 3080, loss = 0.92 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:06:29.694623: step 3090, loss = 1.18 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:06:31.070875: step 3100, loss = 1.20 (930.1 examples/sec; 0.138 sec/batch)
2017-05-09 21:06:32.250888: step 3110, loss = 1.18 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-09 21:06:33.532989: step 3120, loss = 0.99 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:06:34.823896: step 3130, loss = 1.04 (991.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:06:36.123562: step 3140, loss = 1.10 (984.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:06:37.391200: step 3150, loss = 0.85 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-09 21:06:38.662162: step 3160, loss = 0.98 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:06:39.943096: step 3170, loss = 1.08 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:06:41.225751: step 3180, loss = 1.34 (997.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:06:42.502693: step 3190, loss = 1.13 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:06:43.909759: step 3200, loss = 1.01 (909.7 examples/sec; 0.141 sec/batch)
2017-05-09 21:06:45.107666: step 3210, loss = 1.16 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-09 21:06:46.411218: step 3220, loss = 1.09 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:06:47.695544: step 3230, loss = 1.00 (996.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:06:48.979934: step 3240, loss = 0.97 (996.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:06:50.263257: step 3250, loss = 0.94 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:06:51.549935: step 3260, loss = 1.04 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:06:52.829844: step 3270, loss = 1.08 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:06:54.102011: step 3280, loss = 1.25 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:06:55.381937: step 3290, loss = 1.14 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:06:56.764073: step 3300, loss = 0.97 (926.1 examples/sec; 0.138 sec/batch)
2017-05-09 21:06:57.954686: step 3310, loss = 1.05 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-09 21:06:59.257833: step 3320, loss = 1.16 (982.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:07:00.549031: step 3330, loss = 1.09 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:07:01.866898: step 3340, loss = 1.27 (971.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:07:03.200637: step 3350, loss = 1.43 (959.7 examples/sec; 0.133 sec/batch)
2017-05-09 21:07:04.475838: step 3360, loss = 1.15 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:07:05.762094: step 3370, loss = 1.00 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:07:07.028690: step 3380, loss = 1.08 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:07:08.409949: step 3390, loss = 1.13 (926.7 examples/sec; 0.138 sec/batch)
2017-05-09 21:07:09.799838: step 3400, loss = 1.13 (920.9 examples/sec; 0.139 sec/batch)
2017-05-09 21:07:11.003850: step 3410, loss = 1.01 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-09 21:07:12.408423: step 3420, loss = 1.20 (911.3 examples/sec; 0.140 sec/batch)
2017-05-09 21:07:13.680631: step 3430, loss = 1.05 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:07:14.956666: step 3440, loss = 1.13 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:07:16.242121: step 3450, loss = 1.04 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:07:17.513867: step 3460, loss = 1.13 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-09 21:07:18.781239: step 3470, loss = 0.97 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:07:20.079866: step 3480, loss = 1.35 (985.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:07:21.345783: step 3490, loss = 1.19 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:07:22.726787: step 3500, loss = 1.08 (926.9 examples/sec; 0.138 sec/batch)
2017-05-09 21:07:23.892545: step 3510, loss = 1.09 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-09 21:07:25.175566: step 3520, loss = 1.05 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:07:26.476939: step 3530, loss = 1.07 (983.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:07:27.745612: step 3540, loss = 1.19 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:07:29.034717: step 3550, loss = 1.45 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:07:30.307880: step 3560, loss = 1.05 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:07:31.596624: step 3570, loss = 0.95 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:07:32.886216: step 3580, loss = 1.07 (992.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:07:34.161988: step 3590, loss = 1.01 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:07:35.529158: step 3600, loss = 1.00 (936.2 examples/sec; 0.137 sec/batch)
2017-05-09 21:07:36.714537: step 3610, loss = 1.21 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-09 21:07:38.000945: step 3620, loss = 1.01 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:07:39.284357: step 3630, loss = 1.27 (997.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:07:40.591143: step 3640, loss = 1.33 (979.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:07:41.902196: step 3650, loss = 1.08 (976.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:07:43.202279: step 3660, loss = 1.12 (984.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:07:44.482741: step 3670, loss = 1.16 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:07:45.897725: step 3680, loss = 1.05 (904.6 examples/sec; 0.141 sec/batch)
2017-05-09 21:07:47.196092: step 3690, loss = 1.23 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:07:48.594328: step 3700, loss = 1.14 (915.4 examples/sec; 0.140 sec/batch)
2017-05-09 21:07:49.806101: step 3710, loss = 0.88 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-09 21:07:51.092186: step 3720, loss = 1.15 (995.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:07:52.377298: step 3730, loss = 1.09 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:07:53.662648: step 3740, loss = 1.14 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:07:54.955714: step 3750, loss = 1.17 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:07:56.248301: step 3760, loss = 1.22 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:07:57.548809: step 3770, loss = 1.05 (984.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:07:58.816531: step 3780, loss = 1.17 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:08:00.084482: step 3790, loss = 1.07 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-09 21:08:01.470159: step 3800, loss = 1.13 (923.7 examples/sec; 0.139 sec/batch)
2017-05-09 21:08:02.632065: step 3810, loss = 1.12 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-09 21:08:03.922655: step 3820, loss = 0.96 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:08:05.199994: step 3830, loss = 1.12 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:08:06.493053: step 3840, loss = 1.09 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:08:07.768776: step 3850, loss = 0.85 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:08:09.079052: step 3860, loss = 1.00 (976.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:08:10.344853: step 3870, loss = 1.19 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:08:11.675378: step 3880, loss = 1.08 (962.0 examples/sec; 0.133 sec/batch)
2017-05-09 21:08:12.986358: step 3890, loss = 1.13 (976.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:08:14.412869: step 3900, loss = 1.10 (897.3 examples/sec; 0.143 sec/batch)
2017-05-09 21:08:15.653524: step 3910, loss = 0.96 (1031.7 examples/sec; 0.124 sec/batch)
2017-05-09 21:08:16.917672: step 3920, loss = 1.77 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-09 21:08:18.207559: step 3930, loss = 1.20 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:08:19.712847: step 3940, loss = 1.00 (850.3 examples/sec; 0.151 sec/batch)
2017-05-09 21:08:20.985824: step 3950, loss = 1.00 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-09 21:08:22.260179: step 3960, loss = 1.24 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:08:23.553194: step 3970, loss = 1.18 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:08:24.872730: step 3980, loss = 0.90 (970.0 examples/sec; 0.132 sec/batch)
2017-05-09 21:08:26.286022: step 3990, loss = 1.03 (905.7 examples/sec; 0.141 sec/batch)
2017-05-09 21:08:27.779235: step 4000, loss = 0.99 (857.2 examples/sec; 0.149 sec/batch)
2017-05-09 21:08:28.981148: step 4010, loss = 1.01 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-09 21:08:30.272890: step 4020, loss = 1.26 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:08:31.544589: step 4030, loss = 1.08 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-09 21:08:32.835434: step 4040, loss = 1.05 (991.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:08:34.119829: step 4050, loss = 1.03 (996.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:08:35.423797: step 4060, loss = 1.15 (981.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:08:36.811749: step 4070, loss = 1.13 (922.2 examples/sec; 0.139 sec/batch)
2017-05-09 21:08:38.105844: step 4080, loss = 1.10 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:08:39.388257: step 4090, loss = 1.05 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:08:40.755909: step 4100, loss = 1.26 (935.9 examples/sec; 0.137 sec/batch)
2017-05-09 21:08:41.967553: step 4110, loss = 1.35 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-09 21:08:43.240677: step 4120, loss = 0.85 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:08:44.524630: step 4130, loss = 1.46 (996.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:08:45.812950: step 4140, loss = 1.17 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:08:47.098086: step 4150, loss = 1.00 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:08:48.380162: step 4160, loss = 1.04 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:08:49.665187: step 4170, loss = 1.52 (996.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:08:50.972943: step 4180, loss = 1.48 (978.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:08:52.280871: step 4190, loss = 1.05 (978.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:08:53.703587: step 4200, loss = 1.17 (899.7 examples/sec; 0.142 sec/batch)
2017-05-09 21:08:54.868635: step 4210, loss = 1.04 (1098.7 examples/sec; 0.117 sec/batch)
2017-05-09 21:08:56.148692: step 4220, loss = 0.93 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:08:57.431898: step 4230, loss = 1.00 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:08:58.727254: step 4240, loss = 0.83 (988.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:09:00.038437: step 4250, loss = 0.93 (976.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:09:01.333703: step 4260, loss = 1.22 (988.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:09:02.615828: step 4270, loss = 1.16 (998.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:09:03.909246: step 4280, loss = 1.14 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:09:05.195626: step 4290, loss = 0.97 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:09:06.579207: step 4300, loss = 0.99 (925.1 examples/sec; 0.138 sec/batch)
2017-05-09 21:09:07.768830: step 4310, loss = 1.15 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-09 21:09:09.050713: step 4320, loss = 0.95 (998.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:09:10.393993: step 4330, loss = 1.09 (952.9 examples/sec; 0.134 sec/batch)
2017-05-09 21:09:11.666499: step 4340, loss = 0.95 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:09:12.982076: step 4350, loss = 1.13 (972.9 examples/sec; 0.132 sec/batch)
2017-05-09 21:09:14.266288: step 4360, loss = 0.99 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:09:15.546484: step 4370, loss = 1.05 (999.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:09:16.838044: step 4380, loss = 0.92 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:09:18.132008: step 4390, loss = 1.05 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:09:19.530643: step 4400, loss = 1.03 (915.2 examples/sec; 0.140 sec/batch)
2017-05-09 21:09:20.747736: step 4410, loss = 1.32 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-09 21:09:22.246263: step 4420, loss = 0.99 (854.2 examples/sec; 0.150 sec/batch)
2017-05-09 21:09:23.517098: step 4430, loss = 1.06 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:09:24.806571: step 4440, loss = 1.18 (992.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:09:26.091338: step 4450, loss = 0.85 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:09:27.412070: step 4460, loss = 0.91 (969.2 examples/sec; 0.132 sec/batch)
2017-05-09 21:09:28.697676: step 4470, loss = 0.87 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:09:29.968046: step 4480, loss = 1.13 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:09:31.261210: step 4490, loss = 1.06 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:09:32.706015: step 4500, loss = 1.06 (885.9 examples/sec; 0.144 sec/batch)
2017-05-09 21:09:33.858716: step 4510, loss = 1.14 (1110.4 examples/sec; 0.115 sec/batch)
2017-05-09 21:09:35.147842: step 4520, loss = 1.19 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:09:36.469903: step 4530, loss = 1.08 (968.2 examples/sec; 0.132 sec/batch)
2017-05-09 21:09:37.758432: step 4540, loss = 1.04 (993.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:09:39.058819: step 4550, loss = 1.45 (984.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:09:40.345027: step 4560, loss = 1.16 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:09:41.665579: step 4570, loss = 1.19 (969.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:09:42.977038: step 4580, loss = 1.22 (976.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:09:44.270656: step 4590, loss = 1.10 (989.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:09:45.691099: step 4600, loss = 1.08 (901.1 examples/sec; 0.142 sec/batch)
2017-05-09 21:09:46.908680: step 4610, loss = 1.00 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-09 21:09:48.218588: step 4620, loss = 1.01 (977.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:09:49.524184: step 4630, loss = 1.12 (980.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:09:50.838406: step 4640, loss = 1.20 (974.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:09:52.158432: step 4650, loss = 0.96 (969.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:09:53.447650: step 4660, loss = 1.17 (992.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:09:54.724695: step 4670, loss = 1.00 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:09:56.018646: step 4680, loss = 1.08 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:09:57.313390: step 4690, loss = 1.09 (988.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:09:58.686909: step 4700, loss = 0.97 (931.9 examples/sec; 0.137 sec/batch)
2017-05-09 21:09:59.869959: step 4710, loss = 0.94 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-09 21:10:01.145560: step 4720, loss = 1.06 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:10:02.456186: step 4730, loss = 1.05 (976.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:10:03.739900: step 4740, loss = 1.35 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:10:05.000472: step 4750, loss = 1.00 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-09 21:10:06.288337: step 4760, loss = 1.08 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:10:07.558312: step 4770, loss = 1.23 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:10:08.833117: step 4780, loss = 1.02 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:10:10.129478: step 4790, loss = 1.19 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:10:11.501010: step 4800, loss = 0.90 (933.3 examples/sec; 0.137 sec/batch)
2017-05-09 21:10:12.676964: step 4810, loss = 0.97 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-09 21:10:13.971699: step 4820, loss = 1.30 (988.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:10:15.271311: step 4830, loss = 1.14 (984.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:10:16.538172: step 4840, loss = 1.03 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:10:17.810878: step 4850, loss = 0.82 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:10:19.120806: step 4860, loss = 0.93 (977.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:10:20.383412: step 4870, loss = 1.00 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-09 21:10:21.665422: step 4880, loss = 0.96 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:10:22.925200: step 4890, loss = 0.88 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-09 21:10:24.318431: step 4900, loss = 1.19 (918.7 examples/sec; 0.139 sec/batch)
2017-05-09 21:10:25.534581: step 4910, loss = 1.03 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-09 21:10:26.810990: step 4920, loss = 1.25 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:10:28.063967: step 4930, loss = 1.06 (1021.6 examples/sec; 0.125 sec/batch)
2017-05-09 21:10:29.363165: step 4940, loss = 1.05 (985.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:10:30.635067: step 4950, loss = 0.94 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:10:31.925043: step 4960, loss = 1.04 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:10:33.212720: step 4970, loss = 1.08 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:10:34.489253: step 4980, loss = 0.79 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:10:35.781788: step 4990, loss = 1.09 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:10:37.144604: step 5000, loss = 1.11 (939.2 examples/sec; 0.136 sec/batch)
2017-05-09 21:10:38.333736: step 5010, loss = 1.10 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-09 21:10:39.598935: step 5020, loss = 1.06 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:10:40.893490: step 5030, loss = 1.12 (988.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:10:42.196949: step 5040, loss = 1.15 (982.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:10:43.473019: step 5050, loss = 1.03 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:10:44.756198: step 5060, loss = 0.98 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:10:46.053392: step 5070, loss = 0.89 (986.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:10:47.337558: step 5080, loss = 1.08 (996.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:10:48.607629: step 5090, loss = 1.10 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-09 21:10:49.995751: step 5100, loss = 1.10 (922.1 examples/sec; 0.139 sec/batch)
2017-05-09 21:10:51.188640: step 5110, loss = 0.83 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-09 21:10:52.476983: step 5120, loss = 0.88 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:10:53.762427: step 5130, loss = 1.11 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:10:55.044489: step 5140, loss = 0.88 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:10:56.305657: step 5150, loss = 1.07 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-09 21:10:57.609936: step 5160, loss = 1.02 (981.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:10:58.936115: step 5170, loss = 1.41 (965.2 examples/sec; 0.133 sec/batch)
2017-05-09 21:11:00.212024: step 5180, loss = 1.01 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:11:01.522418: step 5190, loss = 1.22 (976.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:11:02.895385: step 5200, loss = 0.91 (932.3 examples/sec; 0.137 sec/batch)
2017-05-09 21:11:04.083061: step 5210, loss = 0.88 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-09 21:11:05.409011: step 5220, loss = 1.05 (965.3 examples/sec; 0.133 sec/batch)
2017-05-09 21:11:06.717715: step 5230, loss = 1.16 (978.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:11:07.997961: step 5240, loss = 0.99 (999.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:11:09.281920: step 5250, loss = 1.02 (996.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:11:10.569077: step 5260, loss = 0.85 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:11:11.866626: step 5270, loss = 0.98 (986.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:11:13.149125: step 5280, loss = 1.18 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:11:14.437282: step 5290, loss = 0.95 (993.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:11:15.816895: step 5300, loss = 0.94 (927.8 examples/sec; 0.138 sec/batch)
2017-05-09 21:11:17.007351: step 5310, loss = 0.96 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-09 21:11:18.307144: step 5320, loss = 0.84 (984.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:11:19.602607: step 5330, loss = 0.84 (988.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:11:20.891080: step 5340, loss = 1.04 (993.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:11:22.179810: step 5350, loss = 0.88 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:11:23.477926: step 5360, loss = 1.08 (986.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:11:24.768295: step 5370, loss = 0.77 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:11:26.061165: step 5380, loss = 1.03 (990.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:11:27.340243: step 5390, loss = 1.10 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:11:28.728314: step 5400, loss = 1.05 (922.1 examples/sec; 0.139 sec/batch)
2017-05-09 21:11:29.949048: step 5410, loss = 1.12 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-09 21:11:31.253882: step 5420, loss = 0.87 (981.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:11:32.531386: step 5430, loss = 0.97 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:11:33.815020: step 5440, loss = 1.00 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:11:35.091271: step 5450, loss = 0.89 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:11:36.376057: step 5460, loss = 1.27 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:11:37.718929: step 5470, loss = 1.03 (953.2 examples/sec; 0.134 sec/batch)
2017-05-09 21:11:39.029845: step 5480, loss = 1.10 (976.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:11:40.318941: step 5490, loss = 0.89 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:11:41.724174: step 5500, loss = 0.90 (910.9 examples/sec; 0.141 sec/batch)
2017-05-09 21:11:42.909483: step 5510, loss = 1.14 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-09 21:11:44.187127: step 5520, loss = 1.06 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:11:45.509009: step 5530, loss = 0.87 (968.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:11:46.821937: step 5540, loss = 0.98 (974.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:11:48.097506: step 5550, loss = 0.96 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:11:49.413343: step 5560, loss = 0.96 (972.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:11:50.712030: step 5570, loss = 0.89 (985.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:11:51.983802: step 5580, loss = 0.85 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-09 21:11:53.266488: step 5590, loss = 1.01 (997.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:11:54.657630: step 5600, loss = 0.99 (920.1 examples/sec; 0.139 sec/batch)
2017-05-09 21:11:55.844075: step 5610, loss = 0.86 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-09 21:11:57.135368: step 5620, loss = 0.98 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:11:58.419509: step 5630, loss = 0.84 (996.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:11:59.681274: step 5640, loss = 1.21 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-09 21:12:00.981915: step 5650, loss = 1.16 (984.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:12:02.283754: step 5660, loss = 0.97 (983.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:12:03.561125: step 5670, loss = 0.96 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:12:04.862234: step 5680, loss = 1.07 (983.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:12:06.207480: step 5690, loss = 1.06 (951.5 examples/sec; 0.135 sec/batch)
2017-05-09 21:12:07.608853: step 5700, loss = 1.16 (913.4 examples/sec; 0.140 sec/batch)
2017-05-09 21:12:08.799894: step 5710, loss = 0.95 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-09 21:12:10.082307: step 5720, loss = 1.09 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:12:11.369119: step 5730, loss = 0.98 (994.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:12:12.642241: step 5740, loss = 1.14 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:12:13.954523: step 5750, loss = 0.92 (975.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:12:15.255188: step 5760, loss = 0.98 (984.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:12:16.527131: step 5770, loss = 0.92 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:12:17.828504: step 5780, loss = 0.99 (983.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:12:19.133365: step 5790, loss = 1.01 (980.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:12:20.501158: step 5800, loss = 1.08 (935.8 examples/sec; 0.137 sec/batch)
2017-05-09 21:12:21.675690: step 5810, loss = 1.03 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-09 21:12:22.962008: step 5820, loss = 0.90 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:12:24.249580: step 5830, loss = 1.26 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:12:25.527804: step 5840, loss = 0.81 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:12:26.831248: step 5850, loss = 1.06 (982.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:12:28.091271: step 5860, loss = 0.92 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-09 21:12:29.378595: step 5870, loss = 0.90 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:12:30.663131: step 5880, loss = 1.16 (996.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:12:31.954953: step 5890, loss = 1.09 (990.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:12:33.364695: step 5900, loss = 0.87 (908.0 examples/sec; 0.141 sec/batch)
2017-05-09 21:12:34.552915: step 5910, loss = 0.87 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-09 21:12:35.869382: step 5920, loss = 0.99 (972.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:12:37.155934: step 5930, loss = 1.10 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:12:38.435358: step 5940, loss = 1.05 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:12:39.704411: step 5950, loss = 0.94 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:12:41.009343: step 5960, loss = 1.08 (980.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:12:42.300123: step 5970, loss = 0.97 (991.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:12:43.591146: step 5980, loss = 1.00 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:12:44.859877: step 5990, loss = 0.91 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:12:46.239306: step 6000, loss = 0.97 (927.9 examples/sec; 0.138 sec/batch)
2017-05-09 21:12:47.437716: step 6010, loss = 1.07 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-09 21:12:48.721230: step 6020, loss = 1.04 (997.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:12:50.033287: step 6030, loss = 1.11 (975.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:12:51.338006: step 6040, loss = 0.95 (981.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:12:52.643107: step 6050, loss = 0.84 (980.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:12:53.968584: step 6060, loss = 1.06 (965.7 examples/sec; 0.133 sec/batch)
2017-05-09 21:12:55.282187: step 6070, loss = 0.99 (974.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:12:56.557087: step 6080, loss = 0.90 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:12:57.840729: step 6090, loss = 1.06 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:12:59.227178: step 6100, loss = 1.10 (923.2 examples/sec; 0.139 sec/batch)
2017-05-09 21:13:00.433110: step 6110, loss = 0.99 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-09 21:13:01.740191: step 6120, loss = 1.06 (979.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:13:03.037679: step 6130, loss = 0.91 (986.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:13:04.301621: step 6140, loss = 0.96 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-09 21:13:05.581543: step 6150, loss = 1.21 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:13:06.878381: step 6160, loss = 0.85 (987.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:13:08.159915: step 6170, loss = 0.89 (998.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:13:09.477097: step 6180, loss = 0.82 (971.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:13:10.746491: step 6190, loss = 1.02 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:13:12.137518: step 6200, loss = 0.86 (920.2 examples/sec; 0.139 sec/batch)
2017-05-09 21:13:13.298070: step 6210, loss = 0.97 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-09 21:13:14.584213: step 6220, loss = 0.97 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:13:15.871341: step 6230, loss = 1.18 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:13:17.162922: step 6240, loss = 0.95 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:13:18.448711: step 6250, loss = 0.85 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:13:19.733783: step 6260, loss = 0.98 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:13:21.060726: step 6270, loss = 1.05 (964.6 examples/sec; 0.133 sec/batch)
2017-05-09 21:13:22.351404: step 6280, loss = 1.07 (991.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:13:23.635081: step 6290, loss = 0.90 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:13:25.062435: step 6300, loss = 0.75 (896.8 examples/sec; 0.143 sec/batch)
2017-05-09 21:13:26.282538: step 6310, loss = 1.12 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-09 21:13:27.593095: step 6320, loss = 1.41 (976.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:13:28.890928: step 6330, loss = 1.15 (986.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:13:30.183356: step 6340, loss = 0.92 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:13:31.451874: step 6350, loss = 1.10 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:13:32.728396: step 6360, loss = 1.02 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:13:34.002181: step 6370, loss = 0.98 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:13:35.275637: step 6380, loss = 1.01 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:13:36.581487: step 6390, loss = 1.09 (980.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:13:37.986187: step 6400, loss = 1.09 (911.2 examples/sec; 0.140 sec/batch)
2017-05-09 21:13:39.195402: step 6410, loss = 1.06 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-09 21:13:40.520148: step 6420, loss = 0.89 (966.2 examples/sec; 0.132 sec/batch)
2017-05-09 21:13:41.791541: step 6430, loss = 1.12 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-09 21:13:43.060918: step 6440, loss = 1.00 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:13:44.344803: step 6450, loss = 0.85 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:13:45.656336: step 6460, loss = 0.99 (976.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:13:46.926568: step 6470, loss = 1.04 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:13:48.216013: step 6480, loss = 0.86 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:13:49.513530: step 6490, loss = 0.91 (986.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:13:50.890546: step 6500, loss = 1.14 (929.5 examples/sec; 0.138 sec/batch)
2017-05-09 21:13:52.086598: step 6510, loss = 0.95 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-09 21:13:53.368556: step 6520, loss = 1.12 (998.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:13:54.638498: step 6530, loss = 0.87 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:13:55.926353: step 6540, loss = 1.11 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:13:57.222439: step 6550, loss = 1.00 (987.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:13:58.497632: step 6560, loss = 0.83 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:13:59.760750: step 6570, loss = 0.89 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-09 21:14:01.047963: step 6580, loss = 0.95 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:14:02.340390: step 6590, loss = 0.96 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:14:03.725046: step 6600, loss = 1.04 (924.4 examples/sec; 0.138 sec/batch)
2017-05-09 21:14:04.930026: step 6610, loss = 1.14 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-09 21:14:06.219827: step 6620, loss = 0.97 (992.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:14:07.513345: step 6630, loss = 1.06 (989.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:14:08.828470: step 6640, loss = 1.33 (973.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:14:10.133869: step 6650, loss = 1.11 (980.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:14:11.428821: step 6660, loss = 1.00 (988.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:14:12.732393: step 6670, loss = 0.85 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:14:13.999753: step 6680, loss = 0.96 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:14:15.277138: step 6690, loss = 1.10 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:14:16.693905: step 6700, loss = 0.91 (903.5 examples/sec; 0.142 sec/batch)
2017-05-09 21:14:17.890196: step 6710, loss = 1.03 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-09 21:14:19.179944: step 6720, loss = 0.93 (992.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:14:20.486114: step 6730, loss = 1.08 (980.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:14:21.786642: step 6740, loss = 1.07 (984.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:14:23.078915: step 6750, loss = 1.07 (990.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:14:24.381525: step 6760, loss = 1.09 (982.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:14:25.653204: step 6770, loss = 1.07 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-09 21:14:26.932330: step 6780, loss = 0.94 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:14:28.208199: step 6790, loss = 1.04 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:14:29.594843: step 6800, loss = 0.95 (923.1 examples/sec; 0.139 sec/batch)
2017-05-09 21:14:30.786533: step 6810, loss = 1.04 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-09 21:14:32.073269: step 6820, loss = 1.10 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:14:33.340561: step 6830, loss = 0.91 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:14:34.619145: step 6840, loss = 0.88 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:14:35.904329: step 6850, loss = 0.95 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:14:37.186606: step 6860, loss = 0.90 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:14:38.472534: step 6870, loss = 0.99 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:14:39.754377: step 6880, loss = 1.24 (998.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:14:41.051107: step 6890, loss = 1.08 (987.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:14:42.432682: step 6900, loss = 1.13 (926.5 examples/sec; 0.138 sec/batch)
2017-05-09 21:14:43.657783: step 6910, loss = 1.09 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-09 21:14:44.966084: step 6920, loss = 0.91 (978.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:14:46.251361: step 6930, loss = 0.84 (995.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:14:47.524582: step 6940, loss = 1.02 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:14:48.843837: step 6950, loss = 0.98 (970.2 examples/sec; 0.132 sec/batch)
2017-05-09 21:14:50.140125: step 6960, loss = 1.04 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:14:51.438885: step 6970, loss = 0.93 (985.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:14:52.707209: step 6980, loss = 0.98 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:14:54.006553: step 6990, loss = 1.02 (985.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:14:55.425386: step 7000, loss = 1.12 (902.2 examples/sec; 0.142 sec/batch)
2017-05-09 21:14:56.603126: step 7010, loss = 1.14 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-09 21:14:57.900219: step 7020, loss = 1.00 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:14:59.168547: step 7030, loss = 0.92 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:15:00.449778: step 7040, loss = 1.11 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:15:01.726807: step 7050, loss = 1.05 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:15:03.039566: step 7060, loss = 0.89 (975.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:15:04.321977: step 7070, loss = 0.99 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:15:05.611283: step 7080, loss = 0.94 (992.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:15:06.909381: step 7090, loss = 1.08 (986.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:15:08.263791: step 7100, loss = 0.89 (945.1 examples/sec; 0.135 sec/batch)
2017-05-09 21:15:09.453073: step 7110, loss = 1.18 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-09 21:15:10.726348: step 7120, loss = 0.88 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:15:12.008961: step 7130, loss = 0.91 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:15:13.325018: step 7140, loss = 1.01 (972.6 examples/sec; 0.132 sec/batch)
2017-05-09 21:15:14.626093: step 7150, loss = 0.93 (983.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:15:15.943886: step 7160, loss = 1.22 (971.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:15:17.236808: step 7170, loss = 0.91 (990.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:15:18.549553: step 7180, loss = 1.03 (975.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:15:19.844986: step 7190, loss = 0.87 (988.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:15:21.276706: step 7200, loss = 0.87 (894.0 examples/sec; 0.143 sec/batch)
2017-05-09 21:15:22.707202: step 7210, loss = 1.12 (894.8 examples/sec; 0.143 sec/batch)
2017-05-09 21:15:24.006294: step 7220, loss = 1.07 (985.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:15:25.318404: step 7230, loss = 0.93 (975.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:15:26.599940: step 7240, loss = 0.96 (998.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:15:27.870105: step 7250, loss = 1.23 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:15:29.158944: step 7260, loss = 1.12 (993.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:15:30.471740: step 7270, loss = 1.11 (975.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:15:31.757766: step 7280, loss = 1.04 (995.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:15:33.040371: step 7290, loss = 0.98 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:15:34.412508: step 7300, loss = 0.89 (932.9 examples/sec; 0.137 sec/batch)
2017-05-09 21:15:35.634545: step 7310, loss = 1.24 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-09 21:15:36.920255: step 7320, loss = 0.95 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:15:38.212172: step 7330, loss = 1.00 (990.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:15:39.501620: step 7340, loss = 1.16 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:15:40.780630: step 7350, loss = 0.92 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:15:42.080156: step 7360, loss = 0.85 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:15:43.361124: step 7370, loss = 0.97 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:15:44.641175: step 7380, loss = 0.89 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:15:45.936496: step 7390, loss = 0.97 (988.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:15:47.345913: step 7400, loss = 1.15 (908.2 examples/sec; 0.141 sec/batch)
2017-05-09 21:15:48.565650: step 7410, loss = 1.11 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-09 21:15:49.879850: step 7420, loss = 1.15 (974.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:15:51.174702: step 7430, loss = 0.82 (988.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:15:52.445148: step 7440, loss = 1.09 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-09 21:15:53.756766: step 7450, loss = 0.98 (975.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:15:55.033219: step 7460, loss = 0.98 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:15:56.321095: step 7470, loss = 0.95 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:15:57.639480: step 7480, loss = 1.03 (970.9 examples/sec; 0.132 sec/batch)
2017-05-09 21:15:58.963738: step 7490, loss = 0.99 (966.6 examples/sec; 0.132 sec/batch)
2017-05-09 21:16:00.365513: step 7500, loss = 1.01 (913.1 examples/sec; 0.140 sec/batch)
2017-05-09 21:16:01.573162: step 7510, loss = 1.18 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-09 21:16:02.864748: step 7520, loss = 0.93 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:16:04.142381: step 7530, loss = 0.91 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:16:05.447329: step 7540, loss = 0.85 (980.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:16:06.754625: step 7550, loss = 0.89 (979.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:16:08.065355: step 7560, loss = 0.90 (976.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:16:09.361957: step 7570, loss = 1.02 (987.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:16:10.659734: step 7580, loss = 0.99 (986.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:16:11.977219: step 7590, loss = 0.82 (971.6 examples/sec; 0.132 sec/batch)
2017-05-09 21:16:13.370477: step 7600, loss = 0.93 (918.7 examples/sec; 0.139 sec/batch)
2017-05-09 21:16:14.564287: step 7610, loss = 0.75 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-09 21:16:15.837359: step 7620, loss = 1.06 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:16:17.128778: step 7630, loss = 1.10 (991.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:16:18.407649: step 7640, loss = 0.99 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:16:19.707658: step 7650, loss = 1.02 (984.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:16:20.994788: step 7660, loss = 0.87 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:16:22.298931: step 7670, loss = 1.05 (981.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:16:23.597823: step 7680, loss = 0.98 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:16:24.903274: step 7690, loss = 1.07 (980.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:16:26.303989: step 7700, loss = 0.93 (913.8 examples/sec; 0.140 sec/batch)
2017-05-09 21:16:27.479474: step 7710, loss = 1.02 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-09 21:16:28.799359: step 7720, loss = 0.90 (969.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:16:30.091926: step 7730, loss = 0.92 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:16:31.360762: step 7740, loss = 0.84 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-09 21:16:32.642064: step 7750, loss = 0.89 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:16:33.940167: step 7760, loss = 1.12 (986.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:16:35.223374: step 7770, loss = 1.03 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:16:36.502015: step 7780, loss = 1.10 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:16:37.765663: step 7790, loss = 0.95 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-09 21:16:39.169538: step 7800, loss = 0.94 (911.8 examples/sec; 0.140 sec/batch)
2017-05-09 21:16:40.338823: step 7810, loss = 0.91 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-09 21:16:41.613788: step 7820, loss = 1.07 (1003.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:16:42.902781: step 7830, loss = 0.73 (993.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:16:44.189141: step 7840, loss = 1.07 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:16:45.494375: step 7850, loss = 1.02 (980.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:16:46.805778: step 7860, loss = 0.96 (976.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:16:48.087088: step 7870, loss = 1.02 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:16:49.379679: step 7880, loss = 0.82 (990.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:16:50.662959: step 7890, loss = 1.03 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:16:52.063862: step 7900, loss = 0.86 (913.7 examples/sec; 0.140 sec/batch)
2017-05-09 21:16:53.262402: step 7910, loss = 1.09 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-09 21:16:54.553360: step 7920, loss = 0.83 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:16:55.832220: step 7930, loss = 0.84 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:16:57.107305: step 7940, loss = 0.82 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:16:58.411276: step 7950, loss = 0.97 (981.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:16:59.733728: step 7960, loss = 1.03 (967.9 examples/sec; 0.132 sec/batch)
2017-05-09 21:17:01.036132: step 7970, loss = 0.94 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:17:02.299342: step 7980, loss = 0.94 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-09 21:17:03.589832: step 7990, loss = 0.93 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:17:04.953633: step 8000, loss = 1.05 (938.6 examples/sec; 0.136 sec/batch)
2017-05-09 21:17:06.158978: step 8010, loss = 0.94 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-09 21:17:07.434718: step 8020, loss = 0.98 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:17:08.720603: step 8030, loss = 1.09 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:17:10.022667: step 8040, loss = 1.23 (983.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:17:11.325511: step 8050, loss = 1.13 (982.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:17:12.609288: step 8060, loss = 0.84 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:17:13.919604: step 8070, loss = 1.15 (976.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:17:15.203752: step 8080, loss = 0.86 (996.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:17:16.477560: step 8090, loss = 0.88 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:17:17.865974: step 8100, loss = 1.07 (921.9 examples/sec; 0.139 sec/batch)
2017-05-09 21:17:19.045820: step 8110, loss = 0.91 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-09 21:17:20.337926: step 8120, loss = 1.01 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:17:21.645358: step 8130, loss = 0.91 (979.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:17:22.934919: step 8140, loss = 1.06 (992.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:17:24.230908: step 8150, loss = 1.05 (987.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:17:25.514954: step 8160, loss = 0.89 (996.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:17:26.801375: step 8170, loss = 0.97 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:17:28.070586: step 8180, loss = 0.99 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-09 21:17:29.380118: step 8190, loss = 0.83 (977.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:17:30.762881: step 8200, loss = 0.88 (925.7 examples/sec; 0.138 sec/batch)
2017-05-09 21:17:31.939133: step 8210, loss = 0.96 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-09 21:17:33.239747: step 8220, loss = 1.27 (984.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:17:34.526239: step 8230, loss = 0.99 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:17:35.804274: step 8240, loss = 0.86 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:17:37.127010: step 8250, loss = 1.16 (967.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:17:38.429823: step 8260, loss = 0.91 (982.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:17:39.711485: step 8270, loss = 0.98 (998.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:17:40.992820: step 8280, loss = 0.83 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:17:42.286946: step 8290, loss = 1.00 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:17:43.680824: step 8300, loss = 1.01 (918.3 examples/sec; 0.139 sec/batch)
2017-05-09 21:17:44.874265: step 8310, loss = 0.99 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-09 21:17:46.165286: step 8320, loss = 0.97 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:17:47.456427: step 8330, loss = 0.99 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:17:48.745812: step 8340, loss = 0.82 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:17:50.071832: step 8350, loss = 1.08 (965.3 examples/sec; 0.133 sec/batch)
2017-05-09 21:17:51.393617: step 8360, loss = 0.99 (968.4 examples/sec; 0.132 sec/batch)
2017-05-09 21:17:52.688798: step 8370, loss = 0.91 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:17:54.017383: step 8380, loss = 0.84 (963.4 examples/sec; 0.133 sec/batch)
2017-05-09 21:17:55.292644: step 8390, loss = 1.14 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:17:56.659346: step 8400, loss = 0.81 (936.6 examples/sec; 0.137 sec/batch)
2017-05-09 21:17:57.871171: step 8410, loss = 0.90 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-09 21:17:59.193016: step 8420, loss = 1.25 (968.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:18:00.499145: step 8430, loss = 1.10 (980.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:18:01.798098: step 8440, loss = 1.03 (985.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:18:03.071664: step 8450, loss = 0.94 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:18:04.387970: step 8460, loss = 1.04 (972.4 examples/sec; 0.132 sec/batch)
2017-05-09 21:18:05.670824: step 8470, loss = 0.92 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:18:06.954278: step 8480, loss = 1.10 (997.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:18:08.235714: step 8490, loss = 0.97 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:18:09.596238: step 8500, loss = 1.07 (940.8 examples/sec; 0.136 sec/batch)
2017-05-09 21:18:10.805097: step 8510, loss = 0.85 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-09 21:18:12.095557: step 8520, loss = 0.90 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:18:13.420224: step 8530, loss = 0.82 (966.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:18:14.694801: step 8540, loss = 1.05 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:18:15.984627: step 8550, loss = 1.00 (992.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:18:17.284736: step 8560, loss = 0.74 (984.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:18:18.566969: step 8570, loss = 0.86 (998.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:18:19.816547: step 8580, loss = 0.83 (1024.4 examples/sec; 0.125 sec/batch)
2017-05-09 21:18:21.151257: step 8590, loss = 0.89 (959.0 examples/sec; 0.133 sec/batch)
2017-05-09 21:18:22.569310: step 8600, loss = 0.99 (902.7 examples/sec; 0.142 sec/batch)
2017-05-09 21:18:23.744653: step 8610, loss = 1.05 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-09 21:18:25.042428: step 8620, loss = 0.98 (986.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:18:26.328084: step 8630, loss = 0.87 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:18:27.615413: step 8640, loss = 0.94 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:18:28.952625: step 8650, loss = 0.99 (957.2 examples/sec; 0.134 sec/batch)
2017-05-09 21:18:30.370585: step 8660, loss = 0.87 (902.7 examples/sec; 0.142 sec/batch)
2017-05-09 21:18:31.971294: step 8670, loss = 1.03 (799.6 examples/sec; 0.160 sec/batch)
2017-05-09 21:18:33.534878: step 8680, loss = 0.87 (818.6 examples/sec; 0.156 sec/batch)
2017-05-09 21:18:34.814741: step 8690, loss = 1.01 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:18:36.211692: step 8700, loss = 0.87 (916.3 examples/sec; 0.140 sec/batch)
2017-05-09 21:18:37.401810: step 8710, loss = 0.90 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-09 21:18:38.669064: step 8720, loss = 0.92 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:18:39.940619: step 8730, loss = 1.19 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:18:41.260090: step 8740, loss = 0.85 (970.1 examples/sec; 0.132 sec/batch)
2017-05-09 21:18:42.548531: step 8750, loss = 0.81 (993.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:18:43.854895: step 8760, loss = 1.05 (979.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:18:45.136885: step 8770, loss = 0.95 (998.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:18:46.422093: step 8780, loss = 0.88 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:18:47.705615: step 8790, loss = 0.95 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:18:49.130367: step 8800, loss = 0.84 (898.4 examples/sec; 0.142 sec/batch)
2017-05-09 21:18:50.337569: step 8810, loss = 1.06 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-09 21:18:51.628616: step 8820, loss = 0.73 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:18:52.965436: step 8830, loss = 1.20 (957.5 examples/sec; 0.134 sec/batch)
2017-05-09 21:18:54.248872: step 8840, loss = 0.80 (997.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:18:55.547832: step 8850, loss = 0.99 (985.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:18:56.867571: step 8860, loss = 0.95 (969.9 examples/sec; 0.132 sec/batch)
2017-05-09 21:18:58.181104: step 8870, loss = 1.08 (974.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:18:59.499899: step 8880, loss = 1.06 (970.6 examples/sec; 0.132 sec/batch)
2017-05-09 21:19:00.811497: step 8890, loss = 1.18 (975.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:19:02.183829: step 8900, loss = 1.02 (932.7 examples/sec; 0.137 sec/batch)
2017-05-09 21:19:03.415813: step 8910, loss = 1.07 (1039.0 examples/sec; 0.123 sec/batch)
2017-05-09 21:19:04.725757: step 8920, loss = 0.86 (977.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:19:06.018358: step 8930, loss = 0.87 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:19:07.311860: step 8940, loss = 1.06 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:19:08.614624: step 8950, loss = 1.04 (982.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:19:09.926546: step 8960, loss = 0.88 (975.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:19:11.236433: step 8970, loss = 0.84 (977.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:19:12.565092: step 8980, loss = 1.17 (963.4 examples/sec; 0.133 sec/batch)
2017-05-09 21:19:13.843694: step 8990, loss = 0.92 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:19:15.247322: step 9000, loss = 0.72 (911.9 examples/sec; 0.140 sec/batch)
2017-05-09 21:19:16.454937: step 9010, loss = 1.02 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-09 21:19:17.742414: step 9020, loss = 1.14 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:19:19.053833: step 9030, loss = 0.83 (976.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:19:20.346782: step 9040, loss = 0.99 (990.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:19:21.663675: step 9050, loss = 1.03 (972.0 examples/sec; 0.132 sec/batch)
2017-05-09 21:19:22.962748: step 9060, loss = 0.96 (985.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:19:24.247832: step 9070, loss = 0.75 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:19:25.540658: step 9080, loss = 1.01 (990.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:19:26.831115: step 9090, loss = 0.90 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:19:28.224435: step 9100, loss = 0.90 (918.7 examples/sec; 0.139 sec/batch)
2017-05-09 21:19:29.418877: step 9110, loss = 0.97 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-09 21:19:30.704237: step 9120, loss = 0.97 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:19:31.984586: step 9130, loss = 1.10 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:19:33.284017: step 9140, loss = 0.98 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:19:34.583811: step 9150, loss = 0.98 (984.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:19:35.878554: step 9160, loss = 0.93 (988.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:19:37.161832: step 9170, loss = 0.98 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:19:38.487166: step 9180, loss = 0.97 (965.8 examples/sec; 0.133 sec/batch)
2017-05-09 21:19:39.792576: step 9190, loss = 1.01 (980.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:19:41.182095: step 9200, loss = 1.03 (921.2 examples/sec; 0.139 sec/batch)
2017-05-09 21:19:42.378048: step 9210, loss = 1.07 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-09 21:19:43.653669: step 9220, loss = 1.06 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:19:44.926993: step 9230, loss = 0.92 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:19:46.209746: step 9240, loss = 0.89 (997.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:19:47.490814: step 9250, loss = 0.93 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:19:48.779295: step 9260, loss = 1.02 (993.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:19:50.056979: step 9270, loss = 0.99 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:19:51.338757: step 9280, loss = 1.00 (998.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:19:52.677214: step 9290, loss = 0.83 (956.3 examples/sec; 0.134 sec/batch)
2017-05-09 21:19:54.091982: step 9300, loss = 0.84 (904.7 examples/sec; 0.141 sec/batch)
2017-05-09 21:19:55.334867: step 9310, loss = 0.97 (1029.9 examples/sec; 0.124 sec/batch)
2017-05-09 21:19:56.638992: step 9320, loss = 0.88 (981.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:19:57.915243: step 9330, loss = 0.88 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:19:59.205148: step 9340, loss = 0.95 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:20:00.515150: step 9350, loss = 1.08 (977.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:20:01.796525: step 9360, loss = 0.88 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:20:03.077571: step 9370, loss = 0.94 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:20:04.356798: step 9380, loss = 0.94 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:20:05.651992: step 9390, loss = 0.76 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:20:07.039602: step 9400, loss = 0.82 (922.4 examples/sec; 0.139 sec/batch)
2017-05-09 21:20:08.252175: step 9410, loss = 1.05 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-09 21:20:09.542003: step 9420, loss = 0.94 (992.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:20:10.815057: step 9430, loss = 1.06 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-09 21:20:12.114702: step 9440, loss = 1.05 (984.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:20:13.391578: step 9450, loss = 0.82 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:20:14.685201: step 9460, loss = 0.91 (989.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:20:15.952770: step 9470, loss = 1.18 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-09 21:20:17.256644: step 9480, loss = 0.92 (981.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:20:18.554010: step 9490, loss = 1.09 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:20:19.936510: step 9500, loss = 0.99 (925.9 examples/sec; 0.138 sec/batch)
2017-05-09 21:20:21.123579: step 9510, loss = 1.06 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-09 21:20:22.410037: step 9520, loss = 0.82 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:20:23.682071: step 9530, loss = 0.93 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:20:24.955718: step 9540, loss = 0.77 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:20:26.233998: step 9550, loss = 0.85 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:20:27.514339: step 9560, loss = 0.82 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:20:28.797489: step 9570, loss = 0.96 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:20:30.112987: step 9580, loss = 0.93 (973.0 examples/sec; 0.132 sec/batch)
2017-05-09 21:20:31.389681: step 9590, loss = 0.92 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:20:32.769264: step 9600, loss = 0.98 (927.8 examples/sec; 0.138 sec/batch)
2017-05-09 21:20:33.965573: step 9610, loss = 0.99 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-09 21:20:35.271398: step 9620, loss = 1.08 (980.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:20:36.582755: step 9630, loss = 0.83 (976.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:20:37.913681: step 9640, loss = 1.17 (961.7 examples/sec; 0.133 sec/batch)
2017-05-09 21:20:39.209992: step 9650, loss = 1.02 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:20:40.506395: step 9660, loss = 1.01 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:20:41.815651: step 9670, loss = 0.88 (977.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:20:43.081845: step 9680, loss = 0.95 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:20:44.354177: step 9690, loss = 0.92 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:20:45.760745: step 9700, loss = 1.07 (910.0 examples/sec; 0.141 sec/batch)
2017-05-09 21:20:46.941314: step 9710, loss = 0.83 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-09 21:20:48.229242: step 9720, loss = 0.88 (993.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:20:49.527403: step 9730, loss = 0.98 (986.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:20:50.805829: step 9740, loss = 1.03 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:20:52.078672: step 9750, loss = 1.21 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:20:53.365174: step 9760, loss = 0.96 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:20:54.646136: step 9770, loss = 1.00 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:20:55.909110: step 9780, loss = 1.03 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-09 21:20:57.196120: step 9790, loss = 0.96 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:20:58.590931: step 9800, loss = 0.99 (917.7 examples/sec; 0.139 sec/batch)
2017-05-09 21:20:59.764920: step 9810, loss = 0.98 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-09 21:21:01.027683: step 9820, loss = 0.99 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-09 21:21:02.340417: step 9830, loss = 0.88 (975.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:21:03.649888: step 9840, loss = 0.75 (977.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:21:04.935131: step 9850, loss = 1.07 (995.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:21:06.232067: step 9860, loss = 1.09 (986.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:21:07.525777: step 9870, loss = 0.80 (989.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:21:08.814137: step 9880, loss = 1.00 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:21:10.093137: step 9890, loss = 0.87 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:21:11.453125: step 9900, loss = 0.93 (941.2 examples/sec; 0.136 sec/batch)
2017-05-09 21:21:12.630289: step 9910, loss = 1.20 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-09 21:21:13.902925: step 9920, loss = 0.92 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-09 21:21:15.179965: step 9930, loss = 0.89 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:21:16.450196: step 9940, loss = 1.10 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:21:17.754068: step 9950, loss = 1.02 (981.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:21:19.039269: step 9960, loss = 0.78 (995.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:21:20.311716: step 9970, loss = 0.87 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:21:21.598125: step 9980, loss = 0.82 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:21:22.891337: step 9990, loss = 0.93 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:21:24.289278: step 10000, loss = 0.90 (915.6 examples/sec; 0.140 sec/batch)
2017-05-09 21:21:25.519963: step 10010, loss = 1.01 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-09 21:21:26.829882: step 10020, loss = 0.82 (977.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:21:28.123822: step 10030, loss = 0.90 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:21:29.414343: step 10040, loss = 0.87 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:21:30.708350: step 10050, loss = 0.75 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:21:31.991386: step 10060, loss = 0.90 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:21:33.283317: step 10070, loss = 0.83 (990.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:21:34.583477: step 10080, loss = 0.89 (984.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:21:35.856201: step 10090, loss = 0.97 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:21:37.270995: step 10100, loss = 0.89 (904.7 examples/sec; 0.141 sec/batch)
2017-05-09 21:21:38.393969: step 10110, loss = 0.94 (1139.8 examples/sec; 0.112 sec/batch)
2017-05-09 21:21:39.661830: step 10120, loss = 1.10 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:21:40.978437: step 10130, loss = 0.95 (972.2 examples/sec; 0.132 sec/batch)
2017-05-09 21:21:42.251606: step 10140, loss = 0.89 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:21:43.531000: step 10150, loss = 0.95 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:21:44.836324: step 10160, loss = 0.99 (980.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:21:46.115803: step 10170, loss = 0.89 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:21:47.399307: step 10180, loss = 0.86 (997.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:21:48.675195: step 10190, loss = 1.04 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:21:50.093326: step 10200, loss = 0.92 (902.6 examples/sec; 0.142 sec/batch)
2017-05-09 21:21:51.272620: step 10210, loss = 0.92 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-09 21:21:52.553983: step 10220, loss = 0.89 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:21:53.886224: step 10230, loss = 1.13 (960.8 examples/sec; 0.133 sec/batch)
2017-05-09 21:21:55.158363: step 10240, loss = 1.06 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:21:56.431189: step 10250, loss = 0.98 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:21:57.730595: step 10260, loss = 0.84 (985.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:21:59.018770: step 10270, loss = 0.82 (993.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:22:00.319731: step 10280, loss = 1.01 (983.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:22:01.580160: step 10290, loss = 0.90 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-09 21:22:02.951238: step 10300, loss = 0.86 (933.6 examples/sec; 0.137 sec/batch)
2017-05-09 21:22:04.181078: step 10310, loss = 1.01 (1040.8 examples/sec; 0.123 sec/batch)
2017-05-09 21:22:05.465885: step 10320, loss = 0.94 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:22:06.751149: step 10330, loss = 0.83 (995.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:22:08.046108: step 10340, loss = 1.05 (988.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:22:09.348311: step 10350, loss = 0.92 (982.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:22:10.642889: step 10360, loss = 0.73 (988.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:22:11.937974: step 10370, loss = 0.97 (988.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:22:13.228698: step 10380, loss = 0.92 (991.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:22:14.517091: step 10390, loss = 0.81 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:22:15.889801: step 10400, loss = 1.39 (932.5 examples/sec; 0.137 sec/batch)
2017-05-09 21:22:17.069952: step 10410, loss = 0.81 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-09 21:22:18.356096: step 10420, loss = 0.87 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:22:19.635140: step 10430, loss = 0.95 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:22:20.909951: step 10440, loss = 0.87 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:22:22.176984: step 10450, loss = 0.91 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:22:23.467853: step 10460, loss = 0.83 (991.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:22:24.764132: step 10470, loss = 0.88 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:22:26.047726: step 10480, loss = 0.97 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:22:27.333626: step 10490, loss = 0.94 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:22:28.725859: step 10500, loss = 0.95 (919.4 examples/sec; 0.139 sec/batch)
2017-05-09 21:22:29.910503: step 10510, loss = 0.97 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-09 21:22:31.160248: step 10520, loss = 0.96 (1024.2 examples/sec; 0.125 sec/batch)
2017-05-09 21:22:32.438663: step 10530, loss = 0.80 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:22:33.726186: step 10540, loss = 1.07 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:22:35.016864: step 10550, loss = 0.82 (991.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:22:36.296516: step 10560, loss = 0.91 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:22:37.589985: step 10570, loss = 1.08 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:22:38.856572: step 10580, loss = 0.84 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:22:40.145011: step 10590, loss = 0.79 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:22:41.534641: step 10600, loss = 0.92 (921.1 examples/sec; 0.139 sec/batch)
2017-05-09 21:22:42.738928: step 10610, loss = 0.89 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-09 21:22:44.061030: step 10620, loss = 1.01 (968.2 examples/sec; 0.132 sec/batch)
2017-05-09 21:22:45.378681: step 10630, loss = 1.11 (971.4 examples/sec; 0.132 sec/batch)
2017-05-09 21:22:46.707027: step 10640, loss = 0.96 (963.6 examples/sec; 0.133 sec/batch)
2017-05-09 21:22:47.982064: step 10650, loss = 0.83 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:22:49.278260: step 10660, loss = 0.97 (987.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:22:50.586038: step 10670, loss = 0.92 (978.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:22:51.853746: step 10680, loss = 1.06 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:22:53.152064: step 10690, loss = 0.81 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:22:54.533161: step 10700, loss = 0.89 (926.8 examples/sec; 0.138 sec/batch)
2017-05-09 21:22:55.731060: step 10710, loss = 1.05 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-09 21:22:57.049350: step 10720, loss = 0.99 (971.0 examples/sec; 0.132 sec/batch)
2017-05-09 21:22:58.351350: step 10730, loss = 0.79 (983.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:22:59.666260: step 10740, loss = 1.08 (973.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:23:00.983047: step 10750, loss = 0.95 (972.1 examples/sec; 0.132 sec/batch)
2017-05-09 21:23:02.262918: step 10760, loss = 0.87 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:23:03.551748: step 10770, loss = 1.03 (993.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:23:04.836764: step 10780, loss = 0.84 (996.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:23:06.144528: step 10790, loss = 0.95 (978.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:23:07.517517: step 10800, loss = 0.77 (932.3 examples/sec; 0.137 sec/batch)
2017-05-09 21:23:08.688718: step 10810, loss = 0.99 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-09 21:23:09.973895: step 10820, loss = 0.99 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:23:11.259754: step 10830, loss = 0.91 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:23:12.558094: step 10840, loss = 1.31 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:23:13.852901: step 10850, loss = 1.00 (988.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:23:15.154117: step 10860, loss = 0.88 (983.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:23:16.447619: step 10870, loss = 0.93 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:23:17.734671: step 10880, loss = 0.94 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:23:19.035626: step 10890, loss = 0.99 (983.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:23:20.407897: step 10900, loss = 0.95 (932.8 examples/sec; 0.137 sec/batch)
2017-05-09 21:23:21.591786: step 10910, loss = 0.83 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-09 21:23:22.864944: step 10920, loss = 0.94 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:23:24.132544: step 10930, loss = 0.80 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:23:25.432649: step 10940, loss = 0.82 (984.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:23:26.704682: step 10950, loss = 0.91 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:23:28.007699: step 10960, loss = 0.88 (982.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:23:29.286067: step 10970, loss = 0.82 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:23:30.584368: step 10980, loss = 1.09 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:23:31.880249: step 10990, loss = 0.98 (987.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:23:33.255167: step 11000, loss = 0.80 (931.0 examples/sec; 0.137 sec/batch)
2017-05-09 21:23:34.441869: step 11010, loss = 0.88 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-09 21:23:35.707855: step 11020, loss = 1.05 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:23:37.026233: step 11030, loss = 0.96 (970.9 examples/sec; 0.132 sec/batch)
2017-05-09 21:23:38.326379: step 11040, loss = 1.20 (984.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:23:39.643497: step 11050, loss = 0.79 (971.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:23:40.954815: step 11060, loss = 0.92 (976.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:23:42.234317: step 11070, loss = 1.02 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:23:43.537893: step 11080, loss = 0.89 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:23:44.833816: step 11090, loss = 0.98 (987.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:23:46.204871: step 11100, loss = 0.94 (933.6 examples/sec; 0.137 sec/batch)
2017-05-09 21:23:47.424998: step 11110, loss = 0.96 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-09 21:23:48.723033: step 11120, loss = 0.92 (986.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:23:50.057516: step 11130, loss = 1.26 (959.2 examples/sec; 0.133 sec/batch)
2017-05-09 21:23:51.340251: step 11140, loss = 0.86 (997.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:23:52.627316: step 11150, loss = 0.83 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:23:53.918848: step 11160, loss = 0.97 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:23:55.210227: step 11170, loss = 1.00 (991.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:23:56.496360: step 11180, loss = 1.05 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:23:57.804057: step 11190, loss = 0.78 (978.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:23:59.207107: step 11200, loss = 0.84 (912.3 examples/sec; 0.140 sec/batch)
2017-05-09 21:24:00.403145: step 11210, loss = 0.87 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-09 21:24:01.706806: step 11220, loss = 1.01 (981.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:24:02.993322: step 11230, loss = 1.00 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:24:04.279567: step 11240, loss = 0.91 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:24:05.557554: step 11250, loss = 0.85 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:24:06.843398: step 11260, loss = 0.97 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:24:08.114386: step 11270, loss = 1.03 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:24:09.401524: step 11280, loss = 0.95 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:24:10.707979: step 11290, loss = 0.87 (979.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:24:12.070776: step 11300, loss = 0.81 (939.2 examples/sec; 0.136 sec/batch)
2017-05-09 21:24:13.263892: step 11310, loss = 1.08 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-09 21:24:14.552568: step 11320, loss = 0.83 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:24:15.833308: step 11330, loss = 1.09 (999.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:24:17.130072: step 11340, loss = 0.85 (987.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:24:18.439862: step 11350, loss = 0.80 (977.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:24:19.706686: step 11360, loss = 0.98 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:24:20.993248: step 11370, loss = 0.93 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:24:22.283310: step 11380, loss = 0.76 (992.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:24:23.560356: step 11390, loss = 0.88 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:24:24.947435: step 11400, loss = 0.94 (922.8 examples/sec; 0.139 sec/batch)
2017-05-09 21:24:26.141557: step 11410, loss = 1.02 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-09 21:24:27.419359: step 11420, loss = 0.83 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:24:28.714692: step 11430, loss = 0.83 (988.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:24:30.013888: step 11440, loss = 0.70 (985.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:24:31.297564: step 11450, loss = 0.80 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:24:32.552224: step 11460, loss = 1.21 (1020.2 examples/sec; 0.125 sec/batch)
2017-05-09 21:24:33.832331: step 11470, loss = 1.01 (999.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:24:35.122869: step 11480, loss = 1.10 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:24:36.398218: step 11490, loss = 0.82 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:24:37.787825: step 11500, loss = 1.07 (921.1 examples/sec; 0.139 sec/batch)
2017-05-09 21:24:38.976381: step 11510, loss = 1.08 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-09 21:24:40.294278: step 11520, loss = 0.82 (971.2 examples/sec; 0.132 sec/batch)
2017-05-09 21:24:41.584671: step 11530, loss = 0.82 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:24:42.887286: step 11540, loss = 0.79 (982.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:24:44.207478: step 11550, loss = 0.91 (969.6 examples/sec; 0.132 sec/batch)
2017-05-09 21:24:45.524065: step 11560, loss = 1.23 (972.2 examples/sec; 0.132 sec/batch)
2017-05-09 21:24:46.819085: step 11570, loss = 1.27 (988.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:24:48.095528: step 11580, loss = 0.94 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:24:49.375122: step 11590, loss = 1.03 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:24:50.762406: step 11600, loss = 0.84 (922.7 examples/sec; 0.139 sec/batch)
2017-05-09 21:24:51.948572: step 11610, loss = 0.99 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-09 21:24:53.231904: step 11620, loss = 1.01 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:24:54.517104: step 11630, loss = 0.91 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:24:55.795795: step 11640, loss = 0.76 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:24:57.088055: step 11650, loss = 0.97 (990.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:24:58.381657: step 11660, loss = 0.97 (989.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:24:59.668869: step 11670, loss = 0.92 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:25:00.945021: step 11680, loss = 0.80 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:25:02.251528: step 11690, loss = 1.09 (979.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:25:03.636117: step 11700, loss = 0.92 (924.5 examples/sec; 0.138 sec/batch)
2017-05-09 21:25:04.836940: step 11710, loss = 0.97 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-09 21:25:06.125982: step 11720, loss = 1.14 (993.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:25:07.392226: step 11730, loss = 0.83 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:25:08.695833: step 11740, loss = 0.92 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:25:09.988814: step 11750, loss = 1.08 (990.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:25:11.332160: step 11760, loss = 0.96 (952.8 examples/sec; 0.134 sec/batch)
2017-05-09 21:25:12.634673: step 11770, loss = 0.84 (982.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:25:13.916046: step 11780, loss = 0.86 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:25:15.218951: step 11790, loss = 0.83 (982.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:25:16.654094: step 11800, loss = 1.09 (891.9 examples/sec; 0.144 sec/batch)
2017-05-09 21:25:17.800210: step 11810, loss = 0.89 (1116.8 examples/sec; 0.115 sec/batch)
2017-05-09 21:25:19.080922: step 11820, loss = 0.83 (999.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:25:20.382781: step 11830, loss = 0.93 (983.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:25:21.705634: step 11840, loss = 0.89 (967.6 examples/sec; 0.132 sec/batch)
2017-05-09 21:25:22.977497: step 11850, loss = 0.84 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:25:24.259939: step 11860, loss = 0.98 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:25:25.568198: step 11870, loss = 0.78 (978.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:25:26.850076: step 11880, loss = 1.00 (998.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:25:28.148094: step 11890, loss = 0.90 (986.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:25:29.546575: step 11900, loss = 0.80 (915.3 examples/sec; 0.140 sec/batch)
2017-05-09 21:25:30.740785: step 11910, loss = 0.87 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-09 21:25:32.030524: step 11920, loss = 0.67 (992.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:25:33.315159: step 11930, loss = 1.23 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:25:34.612591: step 11940, loss = 0.81 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:25:35.913189: step 11950, loss = 1.03 (984.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:25:37.198025: step 11960, loss = 0.76 (996.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:25:38.487789: step 11970, loss = 1.03 (992.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:25:39.796753: step 11980, loss = 0.86 (977.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:25:41.081630: step 11990, loss = 0.95 (996.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:25:42.476172: step 12000, loss = 0.79 (917.9 examples/sec; 0.139 sec/batch)
2017-05-09 21:25:43.655343: step 12010, loss = 0.82 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-09 21:25:44.987001: step 12020, loss = 1.16 (961.2 examples/sec; 0.133 sec/batch)
2017-05-09 21:25:46.306974: step 12030, loss = 1.13 (969.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:25:47.572966: step 12040, loss = 0.92 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:25:48.888876: step 12050, loss = 0.90 (972.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:25:50.171934: step 12060, loss = 0.78 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:25:51.446344: step 12070, loss = 0.88 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:25:52.724427: step 12080, loss = 0.91 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:25:54.051973: step 12090, loss = 1.14 (964.2 examples/sec; 0.133 sec/batch)
2017-05-09 21:25:55.480897: step 12100, loss = 0.98 (895.8 examples/sec; 0.143 sec/batch)
2017-05-09 21:25:56.669979: step 12110, loss = 0.86 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-09 21:25:57.979637: step 12120, loss = 1.00 (977.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:25:59.295491: step 12130, loss = 0.95 (972.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:26:00.584993: step 12140, loss = 0.86 (992.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:26:01.908772: step 12150, loss = 0.91 (966.9 examples/sec; 0.132 sec/batch)
2017-05-09 21:26:03.176249: step 12160, loss = 0.89 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:26:04.462315: step 12170, loss = 0.89 (995.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:26:05.765245: step 12180, loss = 0.80 (982.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:26:07.042909: step 12190, loss = 0.88 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:26:08.413654: step 12200, loss = 0.89 (933.8 examples/sec; 0.137 sec/batch)
2017-05-09 21:26:09.616346: step 12210, loss = 1.01 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-09 21:26:10.900497: step 12220, loss = 0.95 (996.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:26:12.196156: step 12230, loss = 0.94 (987.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:26:13.479066: step 12240, loss = 0.78 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:26:14.764734: step 12250, loss = 0.83 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:26:16.063877: step 12260, loss = 0.96 (985.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:26:17.365116: step 12270, loss = 0.96 (983.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:26:18.626672: step 12280, loss = 0.93 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-09 21:26:19.926388: step 12290, loss = 1.01 (984.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:26:21.307975: step 12300, loss = 1.07 (926.5 examples/sec; 0.138 sec/batch)
2017-05-09 21:26:22.493795: step 12310, loss = 0.93 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-09 21:26:23.779535: step 12320, loss = 0.86 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:26:25.055622: step 12330, loss = 0.81 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:26:26.372742: step 12340, loss = 1.10 (971.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:26:27.660327: step 12350, loss = 0.85 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:26:28.943122: step 12360, loss = 1.02 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:26:30.243866: step 12370, loss = 0.91 (984.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:26:31.534524: step 12380, loss = 0.93 (991.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:26:32.824080: step 12390, loss = 0.85 (992.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:26:34.206939: step 12400, loss = 0.92 (925.6 examples/sec; 0.138 sec/batch)
2017-05-09 21:26:35.390557: step 12410, loss = 0.78 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-09 21:26:36.669640: step 12420, loss = 0.90 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:26:37.957019: step 12430, loss = 0.92 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:26:39.244846: step 12440, loss = 0.76 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:26:40.537646: step 12450, loss = 0.94 (990.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:26:41.834137: step 12460, loss = 0.91 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:26:43.114709: step 12470, loss = 0.95 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:26:44.398585: step 12480, loss = 0.83 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:26:45.679765: step 12490, loss = 0.92 (999.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:26:47.081927: step 12500, loss = 1.01 (912.9 examples/sec; 0.140 sec/batch)
2017-05-09 21:26:48.289558: step 12510, loss = 0.85 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-09 21:26:49.597969: step 12520, loss = 0.91 (978.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:26:50.873196: step 12530, loss = 0.89 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:26:52.170295: step 12540, loss = 0.89 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:26:53.460526: step 12550, loss = 0.87 (992.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:26:54.762020: step 12560, loss = 0.96 (983.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:26:56.027840: step 12570, loss = 0.79 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:26:57.328684: step 12580, loss = 1.28 (984.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:26:58.627783: step 12590, loss = 0.79 (985.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:27:00.041842: step 12600, loss = 1.04 (905.2 examples/sec; 0.141 sec/batch)
2017-05-09 21:27:01.218809: step 12610, loss = 1.03 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-09 21:27:02.532171: step 12620, loss = 0.81 (974.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:27:03.794254: step 12630, loss = 0.86 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-09 21:27:05.080168: step 12640, loss = 0.92 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:27:06.376299: step 12650, loss = 0.76 (987.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:27:07.649600: step 12660, loss = 0.78 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:27:08.930005: step 12670, loss = 0.67 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:27:10.226784: step 12680, loss = 0.97 (987.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:27:11.507606: step 12690, loss = 1.03 (999.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:27:12.867989: step 12700, loss = 0.88 (940.9 examples/sec; 0.136 sec/batch)
2017-05-09 21:27:14.074235: step 12710, loss = 0.83 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-09 21:27:15.354001: step 12720, loss = 0.94 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:27:16.637552: step 12730, loss = 0.76 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:27:17.907344: step 12740, loss = 0.89 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:27:19.248163: step 12750, loss = 0.83 (954.6 examples/sec; 0.134 sec/batch)
2017-05-09 21:27:20.524981: step 12760, loss = 1.04 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:27:21.837833: step 12770, loss = 1.08 (975.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:27:23.137297: step 12780, loss = 0.95 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:27:24.461501: step 12790, loss = 1.34 (966.6 examples/sec; 0.132 sec/batch)
2017-05-09 21:27:25.877417: step 12800, loss = 0.87 (904.0 examples/sec; 0.142 sec/batch)
2017-05-09 21:27:27.135816: step 12810, loss = 1.03 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-09 21:27:28.415359: step 12820, loss = 0.96 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:27:29.709857: step 12830, loss = 1.07 (988.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:27:31.003052: step 12840, loss = 0.99 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:27:32.298068: step 12850, loss = 0.87 (988.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:27:33.577608: step 12860, loss = 0.91 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:27:34.877583: step 12870, loss = 0.88 (984.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:27:36.165192: step 12880, loss = 0.85 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:27:37.469885: step 12890, loss = 1.07 (981.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:27:38.852711: step 12900, loss = 0.92 (925.6 examples/sec; 0.138 sec/batch)
2017-05-09 21:27:40.039230: step 12910, loss = 0.84 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-09 21:27:41.315122: step 12920, loss = 0.87 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:27:42.600083: step 12930, loss = 0.83 (996.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:27:43.883768: step 12940, loss = 0.93 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:27:45.182603: step 12950, loss = 0.96 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:27:46.446209: step 12960, loss = 0.89 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-09 21:27:47.750094: step 12970, loss = 0.76 (981.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:27:49.064895: step 12980, loss = 0.91 (973.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:27:50.399691: step 12990, loss = 0.65 (958.9 examples/sec; 0.133 sec/batch)
2017-05-09 21:27:51.805557: step 13000, loss = 0.88 (910.5 examples/sec; 0.141 sec/batch)
2017-05-09 21:27:53.017400: step 13010, loss = 0.84 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-09 21:27:54.309265: step 13020, loss = 0.87 (990.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:27:55.621088: step 13030, loss = 1.00 (975.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:27:56.911792: step 13040, loss = 0.87 (991.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:27:58.208010: step 13050, loss = 0.89 (987.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:27:59.515015: step 13060, loss = 1.05 (979.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:28:00.784004: step 13070, loss = 0.97 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:28:02.101955: step 13080, loss = 0.77 (971.2 examples/sec; 0.132 sec/batch)
2017-05-09 21:28:03.373672: step 13090, loss = 0.81 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-09 21:28:04.766400: step 13100, loss = 0.78 (919.1 examples/sec; 0.139 sec/batch)
2017-05-09 21:28:05.966772: step 13110, loss = 0.83 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-09 21:28:07.257691: step 13120, loss = 0.82 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:28:08.529673: step 13130, loss = 0.83 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:28:09.830507: step 13140, loss = 0.86 (984.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:28:11.135656: step 13150, loss = 1.09 (980.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:28:12.434801: step 13160, loss = 0.83 (985.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:28:13.762636: step 13170, loss = 0.90 (964.0 examples/sec; 0.133 sec/batch)
2017-05-09 21:28:15.080547: step 13180, loss = 1.47 (971.2 examples/sec; 0.132 sec/batch)
2017-05-09 21:28:16.401963: step 13190, loss = 1.01 (968.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:28:17.779556: step 13200, loss = 1.05 (929.2 examples/sec; 0.138 sec/batch)
2017-05-09 21:28:18.962093: step 13210, loss = 0.71 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-09 21:28:20.256240: step 13220, loss = 0.82 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:28:21.536627: step 13230, loss = 0.89 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:28:22.835377: step 13240, loss = 0.84 (985.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:28:24.112989: step 13250, loss = 0.81 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:28:25.393258: step 13260, loss = 1.00 (999.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:28:26.696800: step 13270, loss = 0.84 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:28:27.997097: step 13280, loss = 0.90 (984.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:28:29.299819: step 13290, loss = 0.79 (982.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:28:30.685635: step 13300, loss = 0.88 (923.7 examples/sec; 0.139 sec/batch)
2017-05-09 21:28:31.914870: step 13310, loss = 1.10 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-09 21:28:33.210917: step 13320, loss = 0.89 (987.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:28:34.496944: step 13330, loss = 0.97 (995.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:28:35.787083: step 13340, loss = 0.86 (992.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:28:37.065874: step 13350, loss = 0.94 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:28:38.351986: step 13360, loss = 0.99 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:28:39.652337: step 13370, loss = 0.97 (984.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:28:40.942955: step 13380, loss = 0.82 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:28:42.234097: step 13390, loss = 0.99 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:28:43.596338: step 13400, loss = 0.93 (939.6 examples/sec; 0.136 sec/batch)
2017-05-09 21:28:44.807804: step 13410, loss = 0.87 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-09 21:28:46.084881: step 13420, loss = 0.79 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:28:47.389226: step 13430, loss = 0.78 (981.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:28:48.655844: step 13440, loss = 0.90 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:28:49.980573: step 13450, loss = 0.80 (966.2 examples/sec; 0.132 sec/batch)
2017-05-09 21:28:51.274094: step 13460, loss = 0.90 (989.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:28:52.597163: step 13470, loss = 0.93 (967.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:28:53.884730: step 13480, loss = 0.94 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:28:55.145282: step 13490, loss = 0.89 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-09 21:28:56.543358: step 13500, loss = 0.99 (915.5 examples/sec; 0.140 sec/batch)
2017-05-09 21:28:57.726542: step 13510, loss = 0.93 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-09 21:28:59.023225: step 13520, loss = 0.83 (987.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:29:00.323975: step 13530, loss = 1.09 (984.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:29:01.620738: step 13540, loss = 0.86 (987.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:29:02.903408: step 13550, loss = 0.99 (997.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:29:04.185478: step 13560, loss = 0.81 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:29:05.474586: step 13570, loss = 0.90 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:29:06.763280: step 13580, loss = 0.93 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:29:08.038589: step 13590, loss = 0.89 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:29:09.425070: step 13600, loss = 0.78 (923.2 examples/sec; 0.139 sec/batch)
2017-05-09 21:29:10.611821: step 13610, loss = 0.80 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-09 21:29:11.917016: step 13620, loss = 0.78 (980.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:29:13.206902: step 13630, loss = 0.77 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:29:14.504770: step 13640, loss = 0.99 (986.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:29:15.782804: step 13650, loss = 0.94 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:29:17.059596: step 13660, loss = 0.86 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:29:18.343311: step 13670, loss = 0.95 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:29:19.641439: step 13680, loss = 0.95 (986.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:29:20.932031: step 13690, loss = 0.79 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:29:22.302088: step 13700, loss = 1.00 (934.3 examples/sec; 0.137 sec/batch)
2017-05-09 21:29:23.490434: step 13710, loss = 0.94 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-09 21:29:24.807228: step 13720, loss = 0.92 (972.1 examples/sec; 0.132 sec/batch)
2017-05-09 21:29:26.091966: step 13730, loss = 0.91 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:29:27.371636: step 13740, loss = 0.87 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:29:28.658482: step 13750, loss = 0.90 (994.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:29:29.949585: step 13760, loss = 1.02 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:29:31.233317: step 13770, loss = 1.20 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:29:32.547487: step 13780, loss = 0.82 (974.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:29:33.868110: step 13790, loss = 0.77 (969.2 examples/sec; 0.132 sec/batch)
2017-05-09 21:29:35.228301: step 13800, loss = 0.80 (941.0 examples/sec; 0.136 sec/batch)
2017-05-09 21:29:36.431800: step 13810, loss = 0.83 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-09 21:29:37.712295: step 13820, loss = 0.89 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:29:39.000665: step 13830, loss = 0.92 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:29:40.288154: step 13840, loss = 0.90 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:29:41.583347: step 13850, loss = 0.79 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:29:42.869650: step 13860, loss = 0.90 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:29:44.153902: step 13870, loss = 0.96 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:29:45.460498: step 13880, loss = 0.90 (979.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:29:46.744165: step 13890, loss = 1.00 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:29:48.116299: step 13900, loss = 0.95 (932.9 examples/sec; 0.137 sec/batch)
2017-05-09 21:29:49.284733: step 13910, loss = 1.44 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-09 21:29:50.567103: step 13920, loss = 0.89 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:29:51.861298: step 13930, loss = 0.87 (989.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:29:53.169018: step 13940, loss = 0.97 (978.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:29:54.458243: step 13950, loss = 0.91 (992.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:29:55.731805: step 13960, loss = 1.01 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:29:57.023749: step 13970, loss = 1.08 (990.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:29:58.316250: step 13980, loss = 0.91 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:29:59.579147: step 13990, loss = 0.93 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-09 21:30:00.961520: step 14000, loss = 1.17 (925.9 examples/sec; 0.138 sec/batch)
2017-05-09 21:30:02.170309: step 14010, loss = 0.77 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-09 21:30:03.477537: step 14020, loss = 0.84 (979.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:30:04.772703: step 14030, loss = 0.93 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:30:06.041757: step 14040, loss = 0.92 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:30:07.331190: step 14050, loss = 1.02 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:30:08.608707: step 14060, loss = 1.16 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:30:09.908642: step 14070, loss = 0.94 (984.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:30:11.196943: step 14080, loss = 1.01 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:30:12.468823: step 14090, loss = 0.91 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:30:13.834809: step 14100, loss = 0.94 (937.1 examples/sec; 0.137 sec/batch)
2017-05-09 21:30:15.035423: step 14110, loss = 0.96 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-09 21:30:16.345123: step 14120, loss = 0.93 (977.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:30:17.632849: step 14130, loss = 0.94 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:30:18.915281: step 14140, loss = 1.15 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:30:20.225317: step 14150, loss = 0.83 (977.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:30:21.529301: step 14160, loss = 0.81 (981.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:30:22.834103: step 14170, loss = 0.88 (981.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:30:24.131785: step 14180, loss = 0.78 (986.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:30:25.408398: step 14190, loss = 0.83 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:30:26.801410: step 14200, loss = 0.96 (918.9 examples/sec; 0.139 sec/batch)
2017-05-09 21:30:27.997291: step 14210, loss = 0.83 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-09 21:30:29.299489: step 14220, loss = 1.06 (983.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:30:30.580787: step 14230, loss = 0.96 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:30:31.883070: step 14240, loss = 1.25 (982.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:30:33.206743: step 14250, loss = 0.98 (967.0 examples/sec; 0.132 sec/batch)
2017-05-09 21:30:34.539793: step 14260, loss = 1.00 (960.2 examples/sec; 0.133 sec/batch)
2017-05-09 21:30:35.831241: step 14270, loss = 0.98 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:30:37.114075: step 14280, loss = 0.78 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:30:38.390018: step 14290, loss = 0.93 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:30:39.780268: step 14300, loss = 0.85 (920.7 examples/sec; 0.139 sec/batch)
2017-05-09 21:30:40.983888: step 14310, loss = 0.79 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-09 21:30:42.278583: step 14320, loss = 0.93 (988.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:30:43.599598: step 14330, loss = 0.86 (969.0 examples/sec; 0.132 sec/batch)
2017-05-09 21:30:44.882023: step 14340, loss = 0.81 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:30:46.214405: step 14350, loss = 0.89 (960.7 examples/sec; 0.133 sec/batch)
2017-05-09 21:30:47.505997: step 14360, loss = 1.12 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:30:48.779093: step 14370, loss = 0.86 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:30:50.084688: step 14380, loss = 0.84 (980.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:30:51.376964: step 14390, loss = 0.94 (990.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:30:52.792973: step 14400, loss = 0.94 (904.0 examples/sec; 0.142 sec/batch)
2017-05-09 21:30:54.017394: step 14410, loss = 0.76 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-09 21:30:55.343612: step 14420, loss = 1.17 (965.2 examples/sec; 0.133 sec/batch)
2017-05-09 21:30:56.632167: step 14430, loss = 0.75 (993.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:30:57.926544: step 14440, loss = 0.83 (988.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:30:59.199984: step 14450, loss = 0.83 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:31:00.508532: step 14460, loss = 0.86 (978.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:31:01.803482: step 14470, loss = 0.92 (988.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:31:03.127804: step 14480, loss = 0.86 (966.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:31:04.411716: step 14490, loss = 0.88 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:31:05.810325: step 14500, loss = 0.97 (915.2 examples/sec; 0.140 sec/batch)
2017-05-09 21:31:07.007961: step 14510, loss = 0.93 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-09 21:31:08.294710: step 14520, loss = 0.88 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:31:09.579332: step 14530, loss = 0.65 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:31:10.845305: step 14540, loss = 0.86 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:31:12.132873: step 14550, loss = 0.85 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:31:13.421604: step 14560, loss = 1.01 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:31:14.723315: step 14570, loss = 0.98 (983.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:31:16.023165: step 14580, loss = 0.91 (984.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:31:17.314209: step 14590, loss = 0.97 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:31:18.715356: step 14600, loss = 0.91 (913.5 examples/sec; 0.140 sec/batch)
2017-05-09 21:31:19.898649: step 14610, loss = 0.80 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-09 21:31:21.185774: step 14620, loss = 0.63 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:31:22.484869: step 14630, loss = 0.81 (985.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:31:23.782813: step 14640, loss = 0.84 (986.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:31:25.092365: step 14650, loss = 0.82 (977.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:31:26.376850: step 14660, loss = 0.98 (996.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:31:27.657761: step 14670, loss = 0.95 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:31:28.958197: step 14680, loss = 0.95 (984.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:31:30.245337: step 14690, loss = 0.86 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:31:31.644953: step 14700, loss = 0.90 (914.5 examples/sec; 0.140 sec/batch)
2017-05-09 21:31:32.859804: step 14710, loss = 0.79 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-09 21:31:34.196193: step 14720, loss = 1.03 (957.8 examples/sec; 0.134 sec/batch)
2017-05-09 21:31:35.516368: step 14730, loss = 1.15 (969.6 examples/sec; 0.132 sec/batch)
2017-05-09 21:31:36.839148: step 14740, loss = 1.11 (967.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:31:38.141663: step 14750, loss = 0.88 (982.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:31:39.441482: step 14760, loss = 0.75 (984.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:31:40.744060: step 14770, loss = 0.87 (982.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:31:42.032374: step 14780, loss = 0.80 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:31:43.323324: step 14790, loss = 1.01 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:31:44.688672: step 14800, loss = 0.83 (937.5 examples/sec; 0.137 sec/batch)
2017-05-09 21:31:45.887381: step 14810, loss = 0.88 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-09 21:31:47.198549: step 14820, loss = 0.88 (976.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:31:48.498436: step 14830, loss = 0.93 (984.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:31:49.805964: step 14840, loss = 0.81 (979.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:31:51.103347: step 14850, loss = 1.06 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:31:52.418034: step 14860, loss = 0.83 (973.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:31:53.718791: step 14870, loss = 0.95 (984.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:31:55.016016: step 14880, loss = 0.76 (986.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:31:56.308384: step 14890, loss = 0.68 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:31:57.686982: step 14900, loss = 0.80 (928.5 examples/sec; 0.138 sec/batch)
2017-05-09 21:31:58.882076: step 14910, loss = 0.79 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-09 21:32:00.182285: step 14920, loss = 0.83 (984.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:32:01.472785: step 14930, loss = 0.90 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:32:02.752413: step 14940, loss = 0.93 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:32:04.011382: step 14950, loss = 0.76 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-09 21:32:05.305093: step 14960, loss = 1.10 (989.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:32:06.614100: step 14970, loss = 0.92 (977.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:32:07.901551: step 14980, loss = 0.88 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:32:09.235319: step 14990, loss = 0.85 (959.7 examples/sec; 0.133 sec/batch)
2017-05-09 21:32:10.668270: step 15000, loss = 0.94 (893.3 examples/sec; 0.143 sec/batch)
2017-05-09 21:32:11.858580: step 15010, loss = 0.82 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-09 21:32:13.147269: step 15020, loss = 0.84 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:32:14.478364: step 15030, loss = 0.99 (961.6 examples/sec; 0.133 sec/batch)
2017-05-09 21:32:15.757782: step 15040, loss = 0.92 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:32:17.043379: step 15050, loss = 0.78 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:32:18.375500: step 15060, loss = 0.97 (960.9 examples/sec; 0.133 sec/batch)
2017-05-09 21:32:19.677323: step 15070, loss = 1.15 (983.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:32:20.992324: step 15080, loss = 0.95 (973.4 examples/sec; 0.132 sec/batch)
2017-05-09 21:32:22.294373: step 15090, loss = 0.99 (983.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:32:23.677747: step 15100, loss = 0.77 (925.3 examples/sec; 0.138 sec/batch)
2017-05-09 21:32:24.879638: step 15110, loss = 0.99 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-09 21:32:26.226345: step 15120, loss = 1.08 (950.5 examples/sec; 0.135 sec/batch)
2017-05-09 21:32:27.510339: step 15130, loss = 0.88 (996.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:32:28.811008: step 15140, loss = 0.73 (984.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:32:30.105845: step 15150, loss = 0.80 (988.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:32:31.448794: step 15160, loss = 0.89 (953.1 examples/sec; 0.134 sec/batch)
2017-05-09 21:32:32.789374: step 15170, loss = 1.06 (954.8 examples/sec; 0.134 sec/batch)
2017-05-09 21:32:34.103386: step 15180, loss = 0.92 (974.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:32:35.395036: step 15190, loss = 0.90 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:32:36.802842: step 15200, loss = 0.77 (909.2 examples/sec; 0.141 sec/batch)
2017-05-09 21:32:38.007844: step 15210, loss = 0.90 (1062.2 examples/sec; 0.120 sec/batch)
2017-05-09 21:32:39.296315: step 15220, loss = 0.84 (993.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:32:40.627619: step 15230, loss = 0.99 (961.5 examples/sec; 0.133 sec/batch)
2017-05-09 21:32:41.937179: step 15240, loss = 0.80 (977.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:32:43.239917: step 15250, loss = 0.86 (982.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:32:44.522364: step 15260, loss = 0.92 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:32:45.810086: step 15270, loss = 0.94 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:32:47.066966: step 15280, loss = 0.97 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-09 21:32:48.399584: step 15290, loss = 0.83 (960.5 examples/sec; 0.133 sec/batch)
2017-05-09 21:32:49.803722: step 15300, loss = 0.95 (911.6 examples/sec; 0.140 sec/batch)
2017-05-09 21:32:51.034295: step 15310, loss = 0.76 (1040.2 examples/sec; 0.123 sec/batch)
2017-05-09 21:32:52.320639: step 15320, loss = 0.97 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:32:53.623941: step 15330, loss = 0.90 (982.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:32:54.908696: step 15340, loss = 0.67 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:32:56.189405: step 15350, loss = 0.83 (999.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:32:57.471480: step 15360, loss = 0.84 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:32:58.746865: step 15370, loss = 0.80 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:33:00.028163: step 15380, loss = 0.93 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:33:01.330961: step 15390, loss = 0.82 (982.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:33:02.737604: step 15400, loss = 1.03 (910.0 examples/sec; 0.141 sec/batch)
2017-05-09 21:33:03.920778: step 15410, loss = 0.92 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-09 21:33:05.203235: step 15420, loss = 0.85 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:33:06.485765: step 15430, loss = 0.74 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:33:07.768421: step 15440, loss = 0.89 (997.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:33:09.073247: step 15450, loss = 1.01 (981.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:33:10.373312: step 15460, loss = 0.93 (984.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:33:11.656998: step 15470, loss = 1.06 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:33:12.953302: step 15480, loss = 1.00 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:33:14.249183: step 15490, loss = 1.13 (987.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:33:15.635199: step 15500, loss = 0.80 (923.5 examples/sec; 0.139 sec/batch)
2017-05-09 21:33:16.826875: step 15510, loss = 0.92 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-09 21:33:18.104347: step 15520, loss = 0.93 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:33:19.390431: step 15530, loss = 0.74 (995.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:33:20.666389: step 15540, loss = 0.91 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:33:21.953079: step 15550, loss = 0.86 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:33:23.245682: step 15560, loss = 0.78 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:33:24.515327: step 15570, loss = 0.67 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:33:25.848243: step 15580, loss = 0.85 (960.3 examples/sec; 0.133 sec/batch)
2017-05-09 21:33:27.162263: step 15590, loss = 1.15 (974.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:33:28.544058: step 15600, loss = 0.90 (926.3 examples/sec; 0.138 sec/batch)
2017-05-09 21:33:29.764045: step 15610, loss = 0.96 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-09 21:33:31.093194: step 15620, loss = 0.79 (963.0 examples/sec; 0.133 sec/batch)
2017-05-09 21:33:32.368286: step 15630, loss = 1.01 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:33:33.695438: step 15640, loss = 1.04 (964.5 examples/sec; 0.133 sec/batch)
2017-05-09 21:33:34.999797: step 15650, loss = 0.90 (981.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:33:36.324480: step 15660, loss = 0.95 (966.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:33:37.595870: step 15670, loss = 0.78 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:33:38.881126: step 15680, loss = 0.89 (995.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:33:40.175986: step 15690, loss = 0.81 (988.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:33:41.566882: step 15700, loss = 0.91 (920.3 examples/sec; 0.139 sec/batch)
2017-05-09 21:33:42.776216: step 15710, loss = 0.98 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-09 21:33:44.065771: step 15720, loss = 0.84 (992.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:33:45.365720: step 15730, loss = 0.74 (984.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:33:46.686843: step 15740, loss = 0.96 (968.9 examples/sec; 0.132 sec/batch)
2017-05-09 21:33:47.991907: step 15750, loss = 0.99 (980.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:33:49.282903: step 15760, loss = 1.29 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:33:50.597559: step 15770, loss = 1.01 (973.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:33:51.910181: step 15780, loss = 0.93 (975.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:33:53.212128: step 15790, loss = 0.92 (983.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:33:54.594227: step 15800, loss = 0.84 (926.1 examples/sec; 0.138 sec/batch)
2017-05-09 21:33:55.816381: step 15810, loss = 0.85 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-09 21:33:57.098960: step 15820, loss = 0.85 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:33:58.406817: step 15830, loss = 0.71 (978.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:33:59.679120: step 15840, loss = 1.17 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:34:00.958327: step 15850, loss = 0.79 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:34:02.238130: step 15860, loss = 0.93 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:34:03.554236: step 15870, loss = 0.86 (972.6 examples/sec; 0.132 sec/batch)
2017-05-09 21:34:04.850817: step 15880, loss = 0.93 (987.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:34:06.148987: step 15890, loss = 1.06 (986.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:34:07.549750: step 15900, loss = 0.93 (913.8 examples/sec; 0.140 sec/batch)
2017-05-09 21:34:08.729768: step 15910, loss = 0.77 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-09 21:34:10.025904: step 15920, loss = 0.99 (987.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:34:11.372102: step 15930, loss = 0.81 (950.8 examples/sec; 0.135 sec/batch)
2017-05-09 21:34:12.685856: step 15940, loss = 0.84 (974.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:34:13.983929: step 15950, loss = 0.73 (986.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:34:15.277277: step 15960, loss = 1.02 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:34:16.570731: step 15970, loss = 1.12 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:34:17.862539: step 15980, loss = 0.90 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:34:19.142867: step 15990, loss = 0.81 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:34:20.527302: step 16000, loss = 0.79 (924.6 examples/sec; 0.138 sec/batch)
2017-05-09 21:34:21.722589: step 16010, loss = 0.89 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-09 21:34:23.009273: step 16020, loss = 0.97 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:34:24.297796: step 16030, loss = 0.75 (993.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:34:25.572204: step 16040, loss = 0.76 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:34:26.842805: step 16050, loss = 0.88 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:34:28.114313: step 16060, loss = 0.84 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:34:29.415112: step 16070, loss = 0.82 (984.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:34:30.745092: step 16080, loss = 0.90 (962.4 examples/sec; 0.133 sec/batch)
2017-05-09 21:34:32.027145: step 16090, loss = 0.83 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:34:33.426480: step 16100, loss = 0.95 (914.9 examples/sec; 0.140 sec/batch)
2017-05-09 21:34:34.602111: step 16110, loss = 1.11 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-09 21:34:35.890799: step 16120, loss = 0.90 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:34:37.186133: step 16130, loss = 0.81 (988.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:34:38.491803: step 16140, loss = 0.87 (980.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:34:39.768812: step 16150, loss = 0.92 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:34:41.056595: step 16160, loss = 1.06 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:34:42.374222: step 16170, loss = 0.97 (971.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:34:43.639525: step 16180, loss = 1.03 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:34:44.921299: step 16190, loss = 0.92 (998.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:34:46.309762: step 16200, loss = 0.95 (921.9 examples/sec; 0.139 sec/batch)
2017-05-09 21:34:47.494817: step 16210, loss = 0.77 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-09 21:34:48.798609: step 16220, loss = 0.86 (981.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:34:50.140096: step 16230, loss = 0.71 (954.2 examples/sec; 0.134 sec/batch)
2017-05-09 21:34:51.469318: step 16240, loss = 0.94 (963.0 examples/sec; 0.133 sec/batch)
2017-05-09 21:34:52.746729: step 16250, loss = 0.86 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:34:54.048257: step 16260, loss = 0.87 (983.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:34:55.340626: step 16270, loss = 0.92 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:34:56.642060: step 16280, loss = 0.91 (983.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:34:57.914081: step 16290, loss = 0.85 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:34:59.320317: step 16300, loss = 0.83 (910.2 examples/sec; 0.141 sec/batch)
2017-05-09 21:35:00.495749: step 16310, loss = 0.82 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-09 21:35:01.810797: step 16320, loss = 1.16 (973.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:35:03.122843: step 16330, loss = 0.72 (975.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:35:04.418492: step 16340, loss = 0.70 (988.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:35:05.729715: step 16350, loss = 0.89 (976.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:35:07.019409: step 16360, loss = 0.80 (992.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:35:08.340989: step 16370, loss = 0.98 (968.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:35:09.643368: step 16380, loss = 0.82 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:35:10.940112: step 16390, loss = 0.88 (987.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:35:12.317869: step 16400, loss = 0.72 (929.0 examples/sec; 0.138 sec/batch)
2017-05-09 21:35:13.501364: step 16410, loss = 0.82 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-09 21:35:14.789610: step 16420, loss = 0.93 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:35:16.068916: step 16430, loss = 0.84 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:35:17.359314: step 16440, loss = 0.90 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:35:18.660834: step 16450, loss = 0.83 (983.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:35:19.927674: step 16460, loss = 0.91 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:35:21.251351: step 16470, loss = 0.71 (967.0 examples/sec; 0.132 sec/batch)
2017-05-09 21:35:22.563205: step 16480, loss = 0.91 (975.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:35:23.862196: step 16490, loss = 1.05 (985.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:35:25.269696: step 16500, loss = 0.90 (909.4 examples/sec; 0.141 sec/batch)
2017-05-09 21:35:26.465396: step 16510, loss = 0.82 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-09 21:35:27.749823: step 16520, loss = 0.81 (996.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:35:29.047286: step 16530, loss = 1.11 (986.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:35:30.344091: step 16540, loss = 0.87 (987.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:35:31.614818: step 16550, loss = 0.88 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:35:32.922545: step 16560, loss = 1.05 (978.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:35:34.217227: step 16570, loss = 0.86 (988.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:35:35.539768: step 16580, loss = 0.77 (967.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:35:36.871138: step 16590, loss = 0.80 (961.4 examples/sec; 0.133 sec/batch)
2017-05-09 21:35:38.259560: step 16600, loss = 0.94 (921.9 examples/sec; 0.139 sec/batch)
2017-05-09 21:35:39.443671: step 16610, loss = 0.91 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-09 21:35:40.737191: step 16620, loss = 0.95 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:35:42.040504: step 16630, loss = 0.87 (982.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:35:43.339977: step 16640, loss = 0.72 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:35:44.654500: step 16650, loss = 0.78 (973.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:35:45.941192: step 16660, loss = 1.10 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:35:47.205372: step 16670, loss = 1.04 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-09 21:35:48.491085: step 16680, loss = 1.12 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:35:49.787107: step 16690, loss = 0.87 (987.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:35:51.145497: step 16700, loss = 0.87 (942.3 examples/sec; 0.136 sec/batch)
2017-05-09 21:35:52.364736: step 16710, loss = 0.85 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-09 21:35:53.671408: step 16720, loss = 0.81 (979.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:35:54.960642: step 16730, loss = 0.84 (992.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:35:56.253589: step 16740, loss = 0.84 (990.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:35:57.538523: step 16750, loss = 0.96 (996.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:35:58.832924: step 16760, loss = 0.78 (988.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:36:00.128317: step 16770, loss = 1.01 (988.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:36:01.432867: step 16780, loss = 0.95 (981.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:36:02.761403: step 16790, loss = 1.05 (963.5 examples/sec; 0.133 sec/batch)
2017-05-09 21:36:04.153301: step 16800, loss = 0.90 (919.6 examples/sec; 0.139 sec/batch)
2017-05-09 21:36:05.356614: step 16810, loss = 0.78 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-09 21:36:06.652093: step 16820, loss = 1.04 (988.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:36:07.952627: step 16830, loss = 0.93 (984.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:36:09.252719: step 16840, loss = 0.74 (984.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:36:10.514307: step 16850, loss = 0.90 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-09 21:36:11.804944: step 16860, loss = 0.93 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:36:13.138260: step 16870, loss = 0.77 (960.0 examples/sec; 0.133 sec/batch)
2017-05-09 21:36:14.448303: step 16880, loss = 0.79 (977.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:36:15.758690: step 16890, loss = 0.75 (976.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:36:17.139070: step 16900, loss = 0.91 (927.3 examples/sec; 0.138 sec/batch)
2017-05-09 21:36:18.335574: step 16910, loss = 0.94 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-09 21:36:19.613137: step 16920, loss = 0.91 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:36:20.941078: step 16930, loss = 0.94 (963.9 examples/sec; 0.133 sec/batch)
2017-05-09 21:36:22.227593: step 16940, loss = 0.92 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:36:23.540076: step 16950, loss = 0.91 (975.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:36:24.841659: step 16960, loss = 1.15 (983.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:36:26.155959: step 16970, loss = 0.72 (973.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:36:27.460475: step 16980, loss = 0.95 (981.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:36:28.761020: step 16990, loss = 0.83 (984.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:36:30.148294: step 17000, loss = 0.82 (922.7 examples/sec; 0.139 sec/batch)
2017-05-09 21:36:31.358692: step 17010, loss = 1.00 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-09 21:36:32.699334: step 17020, loss = 1.11 (954.8 examples/sec; 0.134 sec/batch)
2017-05-09 21:36:33.987924: step 17030, loss = 0.86 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:36:35.296524: step 17040, loss = 0.70 (978.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:36:36.595102: step 17050, loss = 1.02 (985.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:36:37.905861: step 17060, loss = 0.72 (976.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:36:39.202827: step 17070, loss = 0.80 (986.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:36:40.506683: step 17080, loss = 0.82 (981.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:36:41.789884: step 17090, loss = 0.97 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:36:43.195590: step 17100, loss = 0.92 (910.6 examples/sec; 0.141 sec/batch)
2017-05-09 21:36:44.401737: step 17110, loss = 0.88 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-09 21:36:45.708365: step 17120, loss = 0.98 (979.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:36:47.009628: step 17130, loss = 1.00 (983.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:36:48.290092: step 17140, loss = 0.79 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:36:49.580715: step 17150, loss = 0.87 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:36:50.866992: step 17160, loss = 0.83 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:36:52.150345: step 17170, loss = 0.94 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:36:53.466758: step 17180, loss = 0.77 (972.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:36:54.749329: step 17190, loss = 0.90 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:36:56.145537: step 17200, loss = 0.79 (916.8 examples/sec; 0.140 sec/batch)
2017-05-09 21:36:57.341156: step 17210, loss = 0.84 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-09 21:36:58.617595: step 17220, loss = 0.96 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:36:59.920294: step 17230, loss = 0.91 (982.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:37:01.217735: step 17240, loss = 0.81 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:37:02.487860: step 17250, loss = 1.02 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-09 21:37:03.780353: step 17260, loss = 0.77 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:37:05.075849: step 17270, loss = 0.83 (988.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:37:06.359890: step 17280, loss = 0.81 (996.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:37:07.624358: step 17290, loss = 0.84 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-09 21:37:09.031522: step 17300, loss = 0.81 (909.6 examples/sec; 0.141 sec/batch)
2017-05-09 21:37:10.205857: step 17310, loss = 0.92 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-09 21:37:11.507019: step 17320, loss = 0.92 (983.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:37:12.792902: step 17330, loss = 0.85 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:37:14.086819: step 17340, loss = 0.83 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:37:15.386992: step 17350, loss = 0.86 (984.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:37:16.677045: step 17360, loss = 0.79 (992.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:37:17.974824: step 17370, loss = 0.91 (986.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:37:19.255590: step 17380, loss = 0.87 (999.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:37:20.547326: step 17390, loss = 0.95 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:37:21.936689: step 17400, loss = 0.83 (921.3 examples/sec; 0.139 sec/batch)
2017-05-09 21:37:23.111519: step 17410, loss = 0.83 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-09 21:37:24.386150: step 17420, loss = 0.88 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:37:25.724089: step 17430, loss = 0.87 (956.7 examples/sec; 0.134 sec/batch)
2017-05-09 21:37:27.007721: step 17440, loss = 0.98 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:37:28.304361: step 17450, loss = 1.30 (987.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:37:29.591945: step 17460, loss = 0.90 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:37:30.909070: step 17470, loss = 1.33 (971.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:37:32.190183: step 17480, loss = 0.89 (999.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:37:33.502438: step 17490, loss = 0.93 (975.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:37:34.893243: step 17500, loss = 1.06 (920.3 examples/sec; 0.139 sec/batch)
2017-05-09 21:37:36.087231: step 17510, loss = 0.75 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-09 21:37:37.367582: step 17520, loss = 0.88 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:37:38.663721: step 17530, loss = 0.81 (987.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:37:39.960732: step 17540, loss = 0.91 (986.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:37:41.289036: step 17550, loss = 1.00 (963.6 examples/sec; 0.133 sec/batch)
2017-05-09 21:37:42.596248: step 17560, loss = 0.75 (979.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:37:43.882921: step 17570, loss = 0.67 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:37:45.207081: step 17580, loss = 0.87 (966.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:37:46.520129: step 17590, loss = 0.80 (974.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:37:47.956983: step 17600, loss = 1.05 (890.8 examples/sec; 0.144 sec/batch)
2017-05-09 21:37:49.116986: step 17610, loss = 0.85 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-09 21:37:50.413294: step 17620, loss = 0.85 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:37:51.703971: step 17630, loss = 0.89 (991.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:37:53.011025: step 17640, loss = 0.97 (979.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:37:54.292638: step 17650, loss = 0.74 (998.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:37:55.616978: step 17660, loss = 1.06 (966.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:37:56.917505: step 17670, loss = 1.05 (984.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:37:58.226008: step 17680, loss = 0.82 (978.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:37:59.554686: step 17690, loss = 0.86 (963.4 examples/sec; 0.133 sec/batch)
2017-05-09 21:38:00.945784: step 17700, loss = 0.82 (920.1 examples/sec; 0.139 sec/batch)
2017-05-09 21:38:02.148328: step 17710, loss = 0.97 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-09 21:38:03.433906: step 17720, loss = 0.74 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:38:04.748699: step 17730, loss = 1.00 (973.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:38:06.064608: step 17740, loss = 0.98 (972.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:38:07.364631: step 17750, loss = 0.76 (984.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:38:08.644900: step 17760, loss = 0.92 (999.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:38:09.923935: step 17770, loss = 0.73 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:38:11.206067: step 17780, loss = 0.89 (998.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:38:12.501171: step 17790, loss = 0.88 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:38:13.878095: step 17800, loss = 0.93 (929.6 examples/sec; 0.138 sec/batch)
2017-05-09 21:38:15.058092: step 17810, loss = 0.85 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-09 21:38:16.358017: step 17820, loss = 1.05 (984.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:38:17.666604: step 17830, loss = 1.33 (978.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:38:18.971028: step 17840, loss = 0.99 (981.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:38:20.294034: step 17850, loss = 1.01 (967.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:38:21.607174: step 17860, loss = 0.80 (974.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:38:22.946698: step 17870, loss = 0.84 (955.6 examples/sec; 0.134 sec/batch)
2017-05-09 21:38:24.235424: step 17880, loss = 0.77 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:38:25.539288: step 17890, loss = 0.88 (981.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:38:26.919662: step 17900, loss = 0.80 (927.3 examples/sec; 0.138 sec/batch)
2017-05-09 21:38:28.110716: step 17910, loss = 0.84 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-09 21:38:29.411162: step 17920, loss = 0.93 (984.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:38:30.695530: step 17930, loss = 0.76 (996.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:38:32.017755: step 17940, loss = 0.83 (968.1 examples/sec; 0.132 sec/batch)
2017-05-09 21:38:33.322363: step 17950, loss = 0.86 (981.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:38:34.605181: step 17960, loss = 0.81 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:38:35.903329: step 17970, loss = 0.93 (986.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:38:37.235510: step 17980, loss = 0.91 (960.8 examples/sec; 0.133 sec/batch)
2017-05-09 21:38:38.511019: step 17990, loss = 0.93 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:38:39.878762: step 18000, loss = 0.85 (935.8 examples/sec; 0.137 sec/batch)
2017-05-09 21:38:41.073061: step 18010, loss = 0.75 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-09 21:38:42.358866: step 18020, loss = 0.64 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:38:43.653175: step 18030, loss = 0.84 (988.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:38:44.959050: step 18040, loss = 0.76 (980.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:38:46.239058: step 18050, loss = 0.90 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:38:47.546542: step 18060, loss = 0.77 (979.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:38:48.877525: step 18070, loss = 0.81 (961.7 examples/sec; 0.133 sec/batch)
2017-05-09 21:38:50.185783: step 18080, loss = 0.88 (978.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:38:51.491311: step 18090, loss = 1.01 (980.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:38:52.862125: step 18100, loss = 0.80 (933.7 examples/sec; 0.137 sec/batch)
2017-05-09 21:38:54.093800: step 18110, loss = 0.92 (1039.2 examples/sec; 0.123 sec/batch)
2017-05-09 21:38:55.399217: step 18120, loss = 0.76 (980.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:38:56.728385: step 18130, loss = 0.83 (963.0 examples/sec; 0.133 sec/batch)
2017-05-09 21:38:58.035834: step 18140, loss = 0.74 (979.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:38:59.330289: step 18150, loss = 0.81 (988.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:39:00.607644: step 18160, loss = 0.79 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:39:01.909489: step 18170, loss = 0.91 (983.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:39:03.196797: step 18180, loss = 0.90 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:39:04.492031: step 18190, loss = 0.75 (988.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:39:05.897104: step 18200, loss = 0.89 (911.0 examples/sec; 0.141 sec/batch)
2017-05-09 21:39:07.084753: step 18210, loss = 0.79 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-09 21:39:08.360001: step 18220, loss = 0.80 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:39:09.663698: step 18230, loss = 0.89 (981.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:39:10.956214: step 18240, loss = 0.95 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:39:12.242898: step 18250, loss = 0.91 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:39:13.535660: step 18260, loss = 0.91 (990.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:39:14.839918: step 18270, loss = 1.04 (981.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:39:16.140919: step 18280, loss = 0.82 (983.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:39:17.438410: step 18290, loss = 0.71 (986.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:39:18.821885: step 18300, loss = 0.62 (925.2 examples/sec; 0.138 sec/batch)
2017-05-09 21:39:20.058268: step 18310, loss = 0.76 (1035.3 examples/sec; 0.124 sec/batch)
2017-05-09 21:39:21.342658: step 18320, loss = 0.85 (996.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:39:22.627601: step 18330, loss = 0.82 (996.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:39:23.902140: step 18340, loss = 0.87 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:39:25.199331: step 18350, loss = 0.82 (986.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:39:26.508899: step 18360, loss = 0.90 (977.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:39:27.807735: step 18370, loss = 0.88 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:39:29.095040: step 18380, loss = 0.90 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:39:30.379327: step 18390, loss = 0.86 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:39:31.751865: step 18400, loss = 0.74 (932.6 examples/sec; 0.137 sec/batch)
2017-05-09 21:39:32.950212: step 18410, loss = 0.86 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-09 21:39:34.265939: step 18420, loss = 0.86 (972.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:39:35.565487: step 18430, loss = 0.79 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:39:36.884654: step 18440, loss = 0.93 (970.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:39:38.165556: step 18450, loss = 1.19 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:39:39.455524: step 18460, loss = 0.83 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:39:40.742931: step 18470, loss = 0.95 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:39:42.035077: step 18480, loss = 0.93 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:39:43.319694: step 18490, loss = 0.89 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:39:44.688270: step 18500, loss = 1.06 (935.3 examples/sec; 0.137 sec/batch)
2017-05-09 21:39:45.874235: step 18510, loss = 0.71 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-09 21:39:47.164733: step 18520, loss = 1.01 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:39:48.471015: step 18530, loss = 0.69 (979.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:39:49.770856: step 18540, loss = 0.73 (984.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:39:51.075041: step 18550, loss = 0.79 (981.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:39:52.456898: step 18560, loss = 0.93 (926.3 examples/sec; 0.138 sec/batch)
2017-05-09 21:39:53.666417: step 18570, loss = 0.89 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-09 21:39:54.957048: step 18580, loss = 0.77 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:39:56.210651: step 18590, loss = 0.89 (1021.1 examples/sec; 0.125 sec/batch)
2017-05-09 21:39:57.593902: step 18600, loss = 0.98 (925.4 examples/sec; 0.138 sec/batch)
2017-05-09 21:39:58.801557: step 18610, loss = 0.93 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-09 21:40:00.095424: step 18620, loss = 0.81 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:40:01.398328: step 18630, loss = 0.79 (982.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:40:02.712846: step 18640, loss = 0.83 (973.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:40:03.999552: step 18650, loss = 1.03 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:40:05.307604: step 18660, loss = 0.87 (978.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:40:06.594710: step 18670, loss = 0.80 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:40:07.891476: step 18680, loss = 0.74 (987.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:40:09.191797: step 18690, loss = 0.94 (984.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:40:10.582134: step 18700, loss = 0.86 (920.6 examples/sec; 0.139 sec/batch)
2017-05-09 21:40:11.775816: step 18710, loss = 0.85 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-09 21:40:13.062733: step 18720, loss = 0.79 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:40:14.348662: step 18730, loss = 0.72 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:40:15.638066: step 18740, loss = 0.83 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:40:16.926851: step 18750, loss = 0.94 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:40:18.212564: step 18760, loss = 0.98 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:40:19.479760: step 18770, loss = 0.75 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:40:20.752973: step 18780, loss = 0.89 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:40:22.048505: step 18790, loss = 0.90 (988.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:40:23.449081: step 18800, loss = 0.83 (913.9 examples/sec; 0.140 sec/batch)
2017-05-09 21:40:24.645815: step 18810, loss = 0.89 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-09 21:40:25.933768: step 18820, loss = 1.16 (993.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:40:27.225067: step 18830, loss = 0.88 (991.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:40:28.526937: step 18840, loss = 0.90 (983.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:40:29.819624: step 18850, loss = 0.85 (990.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:40:31.109239: step 18860, loss = 0.91 (992.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:40:32.407411: step 18870, loss = 0.73 (986.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:40:33.693728: step 18880, loss = 0.92 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:40:34.958960: step 18890, loss = 0.81 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:40:36.329379: step 18900, loss = 0.73 (934.0 examples/sec; 0.137 sec/batch)
2017-05-09 21:40:37.517024: step 18910, loss = 0.87 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-09 21:40:38.784285: step 18920, loss = 0.94 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:40:40.083961: step 18930, loss = 0.88 (984.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:40:41.365236: step 18940, loss = 1.05 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:40:42.641228: step 18950, loss = 1.01 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:40:43.921892: step 18960, loss = 0.82 (999.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:40:45.227283: step 18970, loss = 0.80 (980.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:40:46.507332: step 18980, loss = 1.10 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:40:47.775591: step 18990, loss = 0.79 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:40:49.151769: step 19000, loss = 0.74 (930.1 examples/sec; 0.138 sec/batch)
2017-05-09 21:40:50.344932: step 19010, loss = 0.80 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-09 21:40:51.637182: step 19020, loss = 0.85 (990.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:40:52.915940: step 19030, loss = 0.66 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:40:54.197481: step 19040, loss = 0.89 (998.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:40:55.504497: step 19050, loss = 0.77 (979.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:40:56.774297: step 19060, loss = 0.92 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:40:58.062194: step 19070, loss = 0.77 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:40:59.413982: step 19080, loss = 0.75 (946.9 examples/sec; 0.135 sec/batch)
2017-05-09 21:41:00.694347: step 19090, loss = 0.83 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:41:02.224607: step 19100, loss = 0.88 (836.5 examples/sec; 0.153 sec/batch)
2017-05-09 21:41:03.449067: step 19110, loss = 0.81 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-09 21:41:04.698560: step 19120, loss = 0.88 (1024.4 examples/sec; 0.125 sec/batch)
2017-05-09 21:41:05.983255: step 19130, loss = 0.92 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:41:07.287791: step 19140, loss = 0.84 (981.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:41:08.607853: step 19150, loss = 0.83 (969.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:41:09.880538: step 19160, loss = 0.85 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:41:11.211671: step 19170, loss = 0.81 (961.6 examples/sec; 0.133 sec/batch)
2017-05-09 21:41:12.506944: step 19180, loss = 0.91 (988.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:41:13.817715: step 19190, loss = 1.03 (976.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:41:15.202370: step 19200, loss = 0.90 (924.4 examples/sec; 0.138 sec/batch)
2017-05-09 21:41:16.406824: step 19210, loss = 0.88 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-09 21:41:17.732948: step 19220, loss = 0.88 (965.2 examples/sec; 0.133 sec/batch)
2017-05-09 21:41:19.028234: step 19230, loss = 0.70 (988.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:41:20.351216: step 19240, loss = 0.93 (967.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:41:21.627164: step 19250, loss = 0.99 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:41:22.927676: step 19260, loss = 0.70 (984.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:41:24.215888: step 19270, loss = 0.81 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:41:25.501544: step 19280, loss = 1.06 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:41:26.804402: step 19290, loss = 0.90 (982.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:41:28.191296: step 19300, loss = 0.77 (922.9 examples/sec; 0.139 sec/batch)
2017-05-09 21:41:29.403981: step 19310, loss = 0.72 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-09 21:41:30.702492: step 19320, loss = 0.92 (985.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:41:32.001718: step 19330, loss = 0.89 (985.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:41:33.337256: step 19340, loss = 0.99 (958.4 examples/sec; 0.134 sec/batch)
2017-05-09 21:41:34.625979: step 19350, loss = 0.81 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:41:35.946982: step 19360, loss = 0.84 (969.0 examples/sec; 0.132 sec/batch)
2017-05-09 21:41:37.220526: step 19370, loss = 0.85 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:41:38.525219: step 19380, loss = 0.75 (981.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:41:39.810628: step 19390, loss = 0.89 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:41:41.220628: step 19400, loss = 0.94 (907.8 examples/sec; 0.141 sec/batch)
2017-05-09 21:41:42.429643: step 19410, loss = 0.93 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-09 21:41:43.722120: step 19420, loss = 0.84 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:41:45.011136: step 19430, loss = 0.76 (993.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:41:46.271792: step 19440, loss = 0.78 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-09 21:41:47.568059: step 19450, loss = 0.92 (987.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:41:48.842619: step 19460, loss = 0.88 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:41:50.146650: step 19470, loss = 0.90 (981.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:41:51.444913: step 19480, loss = 0.88 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:41:52.726404: step 19490, loss = 0.78 (998.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:41:54.095272: step 19500, loss = 0.87 (935.1 examples/sec; 0.137 sec/batch)
2017-05-09 21:41:55.305128: step 19510, loss = 0.97 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-09 21:41:56.604908: step 19520, loss = 0.74 (984.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:41:57.916019: step 19530, loss = 0.81 (976.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:41:59.212544: step 19540, loss = 1.27 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:42:00.517627: step 19550, loss = 0.89 (980.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:42:01.808038: step 19560, loss = 0.80 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:42:03.103688: step 19570, loss = 0.77 (987.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:42:04.377923: step 19580, loss = 0.89 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-09 21:42:05.700401: step 19590, loss = 0.86 (967.9 examples/sec; 0.132 sec/batch)
2017-05-09 21:42:07.085240: step 19600, loss = 0.78 (924.3 examples/sec; 0.138 sec/batch)
2017-05-09 21:42:08.295365: step 19610, loss = 0.91 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-09 21:42:09.613002: step 19620, loss = 0.71 (971.4 examples/sec; 0.132 sec/batch)
2017-05-09 21:42:10.907237: step 19630, loss = 0.71 (989.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:42:12.186849: step 19640, loss = 0.92 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:42:13.474655: step 19650, loss = 0.80 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:42:14.774568: step 19660, loss = 0.91 (984.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:42:16.060395: step 19670, loss = 0.77 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:42:17.347907: step 19680, loss = 0.89 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:42:18.653339: step 19690, loss = 0.81 (980.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:42:20.026050: step 19700, loss = 0.85 (932.5 examples/sec; 0.137 sec/batch)
2017-05-09 21:42:21.212543: step 19710, loss = 0.92 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-09 21:42:22.497652: step 19720, loss = 0.77 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:42:23.783926: step 19730, loss = 0.74 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:42:25.078512: step 19740, loss = 0.82 (988.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:42:26.372958: step 19750, loss = 0.84 (988.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:42:27.667901: step 19760, loss = 0.86 (988.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:42:28.936621: step 19770, loss = 1.11 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:42:30.225846: step 19780, loss = 0.79 (992.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:42:31.517278: step 19790, loss = 0.99 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:42:32.899810: step 19800, loss = 0.83 (925.8 examples/sec; 0.138 sec/batch)
2017-05-09 21:42:34.105823: step 19810, loss = 1.01 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-09 21:42:35.382180: step 19820, loss = 0.95 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:42:36.661207: step 19830, loss = 0.92 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:42:37.937099: step 19840, loss = 0.85 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:42:39.207997: step 19850, loss = 0.74 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:42:40.508174: step 19860, loss = 1.00 (984.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:42:41.793739: step 19870, loss = 1.17 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:42:43.106590: step 19880, loss = 1.00 (975.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:42:44.386083: step 19890, loss = 0.93 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:42:45.778881: step 19900, loss = 0.78 (919.0 examples/sec; 0.139 sec/batch)
2017-05-09 21:42:46.977564: step 19910, loss = 0.93 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-09 21:42:48.270128: step 19920, loss = 0.97 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:42:49.617050: step 19930, loss = 0.83 (950.3 examples/sec; 0.135 sec/batch)
2017-05-09 21:42:50.891987: step 19940, loss = 0.92 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:42:52.189657: step 19950, loss = 0.84 (986.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:42:53.504741: step 19960, loss = 1.04 (973.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:42:54.799675: step 19970, loss = 1.07 (988.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:42:56.085982: step 19980, loss = 0.82 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:42:57.374115: step 19990, loss = 0.85 (993.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:42:58.745432: step 20000, loss = 0.67 (933.4 examples/sec; 0.137 sec/batch)
2017-05-09 21:42:59.939007: step 20010, loss = 0.85 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-09 21:43:01.228859: step 20020, loss = 0.81 (992.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:43:02.532092: step 20030, loss = 0.91 (982.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:43:03.808601: step 20040, loss = 0.85 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:43:05.103598: step 20050, loss = 0.86 (988.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:43:06.403545: step 20060, loss = 0.79 (984.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:43:07.684773: step 20070, loss = 0.79 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:43:08.959251: step 20080, loss = 0.75 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:43:10.256687: step 20090, loss = 0.91 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:43:11.656140: step 20100, loss = 0.85 (914.6 examples/sec; 0.140 sec/batch)
2017-05-09 21:43:12.861500: step 20110, loss = 0.94 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-09 21:43:14.136652: step 20120, loss = 0.84 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:43:15.431932: step 20130, loss = 0.91 (988.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:43:16.712604: step 20140, loss = 0.82 (999.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:43:18.021561: step 20150, loss = 0.81 (977.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:43:19.304608: step 20160, loss = 0.94 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:43:20.596191: step 20170, loss = 0.84 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:43:21.889863: step 20180, loss = 0.76 (989.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:43:23.187386: step 20190, loss = 0.90 (986.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:43:24.562869: step 20200, loss = 0.86 (930.6 examples/sec; 0.138 sec/batch)
2017-05-09 21:43:25.767226: step 20210, loss = 0.84 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-09 21:43:27.098425: step 20220, loss = 0.84 (961.5 examples/sec; 0.133 sec/batch)
2017-05-09 21:43:28.394085: step 20230, loss = 0.82 (987.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:43:29.696952: step 20240, loss = 0.86 (982.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:43:31.006752: step 20250, loss = 1.24 (977.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:43:32.287875: step 20260, loss = 0.74 (999.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:43:33.570180: step 20270, loss = 0.88 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:43:34.879288: step 20280, loss = 0.82 (977.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:43:36.156256: step 20290, loss = 0.84 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:43:37.556130: step 20300, loss = 0.92 (914.4 examples/sec; 0.140 sec/batch)
2017-05-09 21:43:38.737505: step 20310, loss = 0.97 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-09 21:43:40.030637: step 20320, loss = 0.99 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:43:41.333060: step 20330, loss = 0.89 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:43:42.620290: step 20340, loss = 0.88 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:43:43.940857: step 20350, loss = 0.84 (969.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:43:45.261690: step 20360, loss = 0.88 (969.1 examples/sec; 0.132 sec/batch)
2017-05-09 21:43:46.597487: step 20370, loss = 0.81 (958.3 examples/sec; 0.134 sec/batch)
2017-05-09 21:43:47.875693: step 20380, loss = 0.89 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:43:49.155312: step 20390, loss = 0.87 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:43:50.541152: step 20400, loss = 0.74 (923.6 examples/sec; 0.139 sec/batch)
2017-05-09 21:43:51.740122: step 20410, loss = 0.81 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-09 21:43:53.032743: step 20420, loss = 0.76 (990.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:43:54.319564: step 20430, loss = 0.82 (994.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:43:55.605956: step 20440, loss = 0.76 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:43:56.895153: step 20450, loss = 0.90 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:43:58.192325: step 20460, loss = 0.85 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:43:59.492206: step 20470, loss = 0.78 (984.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:44:00.765780: step 20480, loss = 0.66 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:44:02.064030: step 20490, loss = 0.89 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:44:03.449344: step 20500, loss = 0.79 (924.0 examples/sec; 0.139 sec/batch)
2017-05-09 21:44:04.668894: step 20510, loss = 0.91 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-09 21:44:05.953897: step 20520, loss = 1.05 (996.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:44:07.238606: step 20530, loss = 0.91 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:44:08.517818: step 20540, loss = 0.98 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:44:09.806807: step 20550, loss = 0.88 (993.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:44:11.102955: step 20560, loss = 0.95 (987.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:44:12.361041: step 20570, loss = 1.04 (1017.4 examples/sec; 0.126 sec/batch)
2017-05-09 21:44:13.633891: step 20580, loss = 0.96 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:44:14.937519: step 20590, loss = 0.95 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:44:16.333165: step 20600, loss = 0.93 (917.1 examples/sec; 0.140 sec/batch)
2017-05-09 21:44:17.525507: step 20610, loss = 0.72 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-09 21:44:18.814651: step 20620, loss = 0.77 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:44:20.093293: step 20630, loss = 1.17 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:44:21.357762: step 20640, loss = 0.72 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-09 21:44:22.644623: step 20650, loss = 0.89 (994.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:44:23.937335: step 20660, loss = 0.81 (990.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:44:25.240565: step 20670, loss = 0.82 (982.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:44:26.556880: step 20680, loss = 0.84 (972.4 examples/sec; 0.132 sec/batch)
2017-05-09 21:44:27.857147: step 20690, loss = 0.67 (984.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:44:29.276045: step 20700, loss = 0.86 (902.1 examples/sec; 0.142 sec/batch)
2017-05-09 21:44:30.488038: step 20710, loss = 0.72 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-09 21:44:31.805961: step 20720, loss = 1.06 (971.2 examples/sec; 0.132 sec/batch)
2017-05-09 21:44:33.128973: step 20730, loss = 0.84 (967.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:44:34.423239: step 20740, loss = 0.78 (989.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:44:35.733083: step 20750, loss = 0.90 (977.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:44:37.008343: step 20760, loss = 0.85 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:44:38.305758: step 20770, loss = 1.04 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:44:39.593384: step 20780, loss = 0.94 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:44:40.862778: step 20790, loss = 0.88 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:44:42.238138: step 20800, loss = 0.79 (930.7 examples/sec; 0.138 sec/batch)
2017-05-09 21:44:43.444925: step 20810, loss = 0.92 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-09 21:44:44.721218: step 20820, loss = 1.39 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:44:46.046488: step 20830, loss = 0.76 (965.9 examples/sec; 0.133 sec/batch)
2017-05-09 21:44:47.356244: step 20840, loss = 0.90 (977.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:44:48.662856: step 20850, loss = 0.73 (979.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:44:49.964243: step 20860, loss = 0.85 (983.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:44:51.260096: step 20870, loss = 0.96 (987.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:44:52.532897: step 20880, loss = 0.77 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:44:53.844569: step 20890, loss = 0.77 (975.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:44:55.230691: step 20900, loss = 0.87 (923.4 examples/sec; 0.139 sec/batch)
2017-05-09 21:44:56.413842: step 20910, loss = 0.94 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-09 21:44:57.707044: step 20920, loss = 0.87 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:44:58.994858: step 20930, loss = 0.89 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:45:00.264134: step 20940, loss = 0.86 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:45:01.522582: step 20950, loss = 0.97 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-09 21:45:02.793145: step 20960, loss = 0.82 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:45:04.065540: step 20970, loss = 0.98 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:45:05.358405: step 20980, loss = 1.01 (990.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:45:06.673043: step 20990, loss = 0.94 (973.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:45:08.061610: step 21000, loss = 0.89 (921.8 examples/sec; 0.139 sec/batch)
2017-05-09 21:45:09.263560: step 21010, loss = 1.16 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-09 21:45:10.549336: step 21020, loss = 0.87 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:45:11.861480: step 21030, loss = 1.17 (975.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:45:13.184246: step 21040, loss = 0.92 (967.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:45:14.479015: step 21050, loss = 0.68 (988.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:45:15.777561: step 21060, loss = 0.94 (985.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:45:17.091917: step 21070, loss = 1.01 (973.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:45:18.385541: step 21080, loss = 0.99 (989.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:45:19.682790: step 21090, loss = 0.94 (986.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:45:21.058725: step 21100, loss = 0.90 (930.3 examples/sec; 0.138 sec/batch)
2017-05-09 21:45:22.227938: step 21110, loss = 0.86 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-09 21:45:23.498988: step 21120, loss = 0.82 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:45:24.784766: step 21130, loss = 0.91 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:45:26.089023: step 21140, loss = 1.07 (981.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:45:27.395049: step 21150, loss = 0.86 (980.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:45:28.685179: step 21160, loss = 0.90 (992.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:45:29.966488: step 21170, loss = 0.81 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:45:31.252953: step 21180, loss = 0.87 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:45:32.523481: step 21190, loss = 0.83 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-09 21:45:33.896301: step 21200, loss = 0.76 (932.4 examples/sec; 0.137 sec/batch)
2017-05-09 21:45:35.074220: step 21210, loss = 0.82 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-09 21:45:36.340363: step 21220, loss = 0.85 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:45:37.606165: step 21230, loss = 0.79 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:45:38.868249: step 21240, loss = 0.97 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-09 21:45:40.154365: step 21250, loss = 1.06 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:45:41.458043: step 21260, loss = 0.75 (981.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:45:42.737942: step 21270, loss = 0.63 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:45:44.019377: step 21280, loss = 1.09 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:45:45.290967: step 21290, loss = 0.93 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:45:46.662918: step 21300, loss = 0.91 (933.0 examples/sec; 0.137 sec/batch)
2017-05-09 21:45:47.857110: step 21310, loss = 0.85 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-09 21:45:49.179360: step 21320, loss = 0.74 (968.0 examples/sec; 0.132 sec/batch)
2017-05-09 21:45:50.469312: step 21330, loss = 0.89 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:45:51.770063: step 21340, loss = 0.99 (984.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:45:53.090390: step 21350, loss = 0.79 (969.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:45:54.377947: step 21360, loss = 0.83 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:45:55.653798: step 21370, loss = 1.01 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:45:56.948801: step 21380, loss = 0.91 (988.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:45:58.234512: step 21390, loss = 0.80 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:45:59.621867: step 21400, loss = 0.80 (922.6 examples/sec; 0.139 sec/batch)
2017-05-09 21:46:00.796982: step 21410, loss = 0.83 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-09 21:46:02.087239: step 21420, loss = 0.88 (992.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:46:03.366813: step 21430, loss = 0.84 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:46:04.655787: step 21440, loss = 0.80 (993.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:46:05.925638: step 21450, loss = 0.92 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:46:07.223714: step 21460, loss = 0.95 (986.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:46:08.525961: step 21470, loss = 0.97 (982.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:46:09.822291: step 21480, loss = 0.91 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:46:11.091580: step 21490, loss = 0.82 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:46:12.479975: step 21500, loss = 1.07 (921.9 examples/sec; 0.139 sec/batch)
2017-05-09 21:46:13.676859: step 21510, loss = 0.87 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-09 21:46:14.974127: step 21520, loss = 0.77 (986.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:46:16.241881: step 21530, loss = 0.86 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:46:17.524224: step 21540, loss = 0.59 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:46:18.814019: step 21550, loss = 0.70 (992.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:46:20.088604: step 21560, loss = 0.79 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:46:21.382718: step 21570, loss = 0.84 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:46:22.681588: step 21580, loss = 0.79 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:46:23.960752: step 21590, loss = 0.79 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:46:25.340982: step 21600, loss = 0.89 (927.4 examples/sec; 0.138 sec/batch)
2017-05-09 21:46:26.525002: step 21610, loss = 0.89 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-09 21:46:27.793066: step 21620, loss = 1.28 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:46:29.097854: step 21630, loss = 0.81 (981.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:46:30.377216: step 21640, loss = 0.83 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:46:31.664900: step 21650, loss = 0.82 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:46:32.949021: step 21660, loss = 0.78 (996.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:46:34.239230: step 21670, loss = 0.85 (992.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:46:35.551645: step 21680, loss = 0.89 (975.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:46:36.839355: step 21690, loss = 0.84 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:46:38.257334: step 21700, loss = 1.07 (902.7 examples/sec; 0.142 sec/batch)
2017-05-09 21:46:39.467421: step 21710, loss = 0.96 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-09 21:46:40.728705: step 21720, loss = 0.77 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-09 21:46:42.021232: step 21730, loss = 0.89 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:46:43.288648: step 21740, loss = 0.86 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:46:44.568404: step 21750, loss = 0.69 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:46:45.832083: step 21760, loss = 0.83 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-09 21:46:47.103968: step 21770, loss = 0.82 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:46:48.367738: step 21780, loss = 0.83 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-09 21:46:49.654582: step 21790, loss = 0.77 (994.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:46:51.023516: step 21800, loss = 0.88 (935.0 examples/sec; 0.137 sec/batch)
2017-05-09 21:46:52.221058: step 21810, loss = 0.80 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-09 21:46:53.514516: step 21820, loss = 0.79 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:46:54.802477: step 21830, loss = 0.92 (993.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:46:56.103156: step 21840, loss = 0.99 (984.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:46:57.384669: step 21850, loss = 0.78 (998.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:46:58.682959: step 21860, loss = 0.93 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:46:59.996668: step 21870, loss = 0.86 (974.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:47:01.302359: step 21880, loss = 0.75 (980.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:47:02.597813: step 21890, loss = 0.82 (988.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:47:03.989914: step 21900, loss = 0.97 (919.5 examples/sec; 0.139 sec/batch)
2017-05-09 21:47:05.186220: step 21910, loss = 0.88 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-09 21:47:06.465126: step 21920, loss = 0.95 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:47:07.743536: step 21930, loss = 0.87 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:47:09.024006: step 21940, loss = 0.81 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:47:10.307144: step 21950, loss = 1.02 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:47:11.578648: step 21960, loss = 0.82 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:47:12.881002: step 21970, loss = 0.99 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:47:14.185655: step 21980, loss = 0.76 (981.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:47:15.479159: step 21990, loss = 0.86 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:47:16.869168: step 22000, loss = 0.85 (920.9 examples/sec; 0.139 sec/batch)
2017-05-09 21:47:18.036558: step 22010, loss = 0.83 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-09 21:47:19.327719: step 22020, loss = 0.94 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:47:20.640427: step 22030, loss = 0.85 (975.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:47:21.950869: step 22040, loss = 0.87 (976.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:47:23.258870: step 22050, loss = 0.75 (978.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:47:24.560514: step 22060, loss = 0.81 (983.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:47:25.853539: step 22070, loss = 1.04 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:47:27.132705: step 22080, loss = 0.69 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:47:28.422790: step 22090, loss = 0.68 (992.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:47:29.809800: step 22100, loss = 0.85 (922.8 examples/sec; 0.139 sec/batch)
2017-05-09 21:47:31.024726: step 22110, loss = 0.90 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-09 21:47:32.329384: step 22120, loss = 0.74 (981.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:47:33.624325: step 22130, loss = 0.91 (988.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:47:34.911408: step 22140, loss = 0.79 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:47:36.205106: step 22150, loss = 0.78 (989.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:47:37.476724: step 22160, loss = 0.94 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:47:38.773628: step 22170, loss = 0.81 (987.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:47:40.052523: step 22180, loss = 0.87 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:47:41.334876: step 22190, loss = 0.68 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:47:42.719213: step 22200, loss = 0.87 (924.6 examples/sec; 0.138 sec/batch)
2017-05-09 21:47:43.891621: step 22210, loss = 0.99 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-09 21:47:45.193571: step 22220, loss = 0.85 (983.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:47:46.495250: step 22230, loss = 0.88 (983.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:47:47.784498: step 22240, loss = 0.79 (992.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:47:49.067125: step 22250, loss = 0.87 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:47:50.343327: step 22260, loss = 0.77 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:47:51.682770: step 22270, loss = 1.06 (955.6 examples/sec; 0.134 sec/batch)
2017-05-09 21:47:52.962108: step 22280, loss = 0.72 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:47:54.260517: step 22290, loss = 0.86 (985.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:47:55.668932: step 22300, loss = 1.02 (908.8 examples/sec; 0.141 sec/batch)
2017-05-09 21:47:56.849016: step 22310, loss = 0.80 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-09 21:47:58.140780: step 22320, loss = 0.78 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:47:59.442741: step 22330, loss = 0.90 (983.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:48:00.733069: step 22340, loss = 0.83 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:48:02.023997: step 22350, loss = 0.83 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:48:03.327305: step 22360, loss = 0.76 (982.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:48:04.626131: step 22370, loss = 0.84 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:48:05.911691: step 22380, loss = 0.88 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:48:07.188676: step 22390, loss = 0.95 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:48:08.586526: step 22400, loss = 0.84 (915.7 examples/sec; 0.140 sec/batch)
2017-05-09 21:48:09.774856: step 22410, loss = 0.80 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-09 21:48:11.045713: step 22420, loss = 0.81 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:48:12.313684: step 22430, loss = 0.82 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-09 21:48:13.583769: step 22440, loss = 0.98 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-09 21:48:14.897283: step 22450, loss = 0.76 (974.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:48:16.180938: step 22460, loss = 0.62 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:48:17.469912: step 22470, loss = 0.68 (993.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:48:18.758868: step 22480, loss = 0.83 (993.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:48:20.066787: step 22490, loss = 0.74 (978.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:48:21.446926: step 22500, loss = 0.81 (927.4 examples/sec; 0.138 sec/batch)
2017-05-09 21:48:22.619579: step 22510, loss = 0.72 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-09 21:48:23.899492: step 22520, loss = 0.98 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:48:25.178008: step 22530, loss = 0.72 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:48:26.480817: step 22540, loss = 1.12 (982.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:48:27.798314: step 22550, loss = 0.96 (971.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:48:29.082951: step 22560, loss = 0.78 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:48:30.375408: step 22570, loss = 0.87 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:48:31.693125: step 22580, loss = 0.83 (971.4 examples/sec; 0.132 sec/batch)
2017-05-09 21:48:32.961621: step 22590, loss = 0.92 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:48:34.370322: step 22600, loss = 0.81 (908.6 examples/sec; 0.141 sec/batch)
2017-05-09 21:48:35.593500: step 22610, loss = 0.73 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-09 21:48:36.879978: step 22620, loss = 0.84 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:48:38.204090: step 22630, loss = 1.13 (966.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:48:39.493779: step 22640, loss = 0.87 (992.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:48:40.766391: step 22650, loss = 0.87 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-09 21:48:42.070647: step 22660, loss = 0.85 (981.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:48:43.333835: step 22670, loss = 0.83 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-09 21:48:44.656946: step 22680, loss = 1.06 (967.4 examples/sec; 0.132 sec/batch)
2017-05-09 21:48:45.969207: step 22690, loss = 0.81 (975.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:48:47.396510: step 22700, loss = 1.01 (896.8 examples/sec; 0.143 sec/batch)
2017-05-09 21:48:48.578572: step 22710, loss = 0.71 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-09 21:48:49.887110: step 22720, loss = 0.76 (978.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:48:51.194213: step 22730, loss = 0.83 (979.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:48:52.482476: step 22740, loss = 0.97 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:48:53.768887: step 22750, loss = 0.95 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:48:55.046726: step 22760, loss = 0.77 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:48:56.324401: step 22770, loss = 0.76 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:48:57.612904: step 22780, loss = 0.80 (993.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:48:58.895072: step 22790, loss = 0.79 (998.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:49:00.298287: step 22800, loss = 0.85 (912.2 examples/sec; 0.140 sec/batch)
2017-05-09 21:49:01.498184: step 22810, loss = 0.68 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-09 21:49:02.806704: step 22820, loss = 0.99 (978.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:49:04.119258: step 22830, loss = 0.96 (975.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:49:05.385670: step 22840, loss = 1.04 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:49:06.670871: step 22850, loss = 0.95 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:49:07.963330: step 22860, loss = 0.83 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:49:09.244400: step 22870, loss = 0.70 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:49:10.521706: step 22880, loss = 0.72 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:49:11.818332: step 22890, loss = 0.99 (987.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:49:13.212979: step 22900, loss = 1.06 (917.8 examples/sec; 0.139 sec/batch)
2017-05-09 21:49:14.447767: step 22910, loss = 1.13 (1036.6 examples/sec; 0.123 sec/batch)
2017-05-09 21:49:15.746857: step 22920, loss = 0.72 (985.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:49:17.050293: step 22930, loss = 0.90 (982.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:49:18.346818: step 22940, loss = 0.83 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:49:19.645128: step 22950, loss = 0.86 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:49:20.961860: step 22960, loss = 0.81 (972.1 examples/sec; 0.132 sec/batch)
2017-05-09 21:49:22.273567: step 22970, loss = 0.95 (975.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:49:23.580537: step 22980, loss = 0.99 (979.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:49:24.866243: step 22990, loss = 0.86 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:49:26.288223: step 23000, loss = 0.73 (900.2 examples/sec; 0.142 sec/batch)
2017-05-09 21:49:27.489125: step 23010, loss = 0.84 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-09 21:49:28.776353: step 23020, loss = 0.88 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:49:30.076964: step 23030, loss = 1.00 (984.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:49:31.382500: step 23040, loss = 0.84 (980.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:49:32.656778: step 23050, loss = 0.89 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-09 21:49:33.947727: step 23060, loss = 0.87 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:49:35.252304: step 23070, loss = 0.88 (981.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:49:36.525536: step 23080, loss = 0.98 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:49:37.810603: step 23090, loss = 0.83 (996.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:49:39.190821: step 23100, loss = 0.88 (927.4 examples/sec; 0.138 sec/batch)
2017-05-09 21:49:40.378303: step 23110, loss = 0.82 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-09 21:49:41.636377: step 23120, loss = 0.84 (1017.4 examples/sec; 0.126 sec/batch)
2017-05-09 21:49:42.944004: step 23130, loss = 0.71 (978.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:49:44.232475: step 23140, loss = 0.75 (993.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:49:45.503276: step 23150, loss = 0.88 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:49:46.785387: step 23160, loss = 0.77 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:49:48.066300: step 23170, loss = 1.01 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:49:49.370139: step 23180, loss = 0.90 (981.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:49:50.652194: step 23190, loss = 1.05 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:49:52.015761: step 23200, loss = 0.92 (938.7 examples/sec; 0.136 sec/batch)
2017-05-09 21:49:53.217913: step 23210, loss = 1.00 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-09 21:49:54.520022: step 23220, loss = 1.04 (983.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:49:55.799069: step 23230, loss = 0.78 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:49:57.130527: step 23240, loss = 0.81 (961.4 examples/sec; 0.133 sec/batch)
2017-05-09 21:49:58.435078: step 23250, loss = 0.82 (981.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:49:59.756040: step 23260, loss = 0.81 (969.0 examples/sec; 0.132 sec/batch)
2017-05-09 21:50:01.053417: step 23270, loss = 0.99 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:50:02.389201: step 23280, loss = 0.86 (958.2 examples/sec; 0.134 sec/batch)
2017-05-09 21:50:03.671263: step 23290, loss = 0.79 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:50:05.045699: step 23300, loss = 0.75 (931.3 examples/sec; 0.137 sec/batch)
2017-05-09 21:50:06.259299: step 23310, loss = 0.93 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-09 21:50:07.533538: step 23320, loss = 0.76 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-09 21:50:08.822239: step 23330, loss = 0.69 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:50:10.112386: step 23340, loss = 0.93 (992.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:50:11.387390: step 23350, loss = 0.79 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:50:12.672288: step 23360, loss = 0.90 (996.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:50:13.974197: step 23370, loss = 0.97 (983.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:50:15.275963: step 23380, loss = 0.93 (983.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:50:16.563327: step 23390, loss = 0.87 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:50:17.920494: step 23400, loss = 1.06 (943.2 examples/sec; 0.136 sec/batch)
2017-05-09 21:50:19.097668: step 23410, loss = 1.01 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-09 21:50:20.410526: step 23420, loss = 1.16 (975.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:50:21.709138: step 23430, loss = 0.99 (985.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:50:22.981530: step 23440, loss = 0.82 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:50:24.263438: step 23450, loss = 0.79 (998.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:50:25.540105: step 23460, loss = 0.79 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:50:26.827735: step 23470, loss = 0.83 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:50:28.101204: step 23480, loss = 0.92 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:50:29.386631: step 23490, loss = 0.90 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:50:30.789189: step 23500, loss = 0.98 (912.6 examples/sec; 0.140 sec/batch)
2017-05-09 21:50:31.931076: step 23510, loss = 0.85 (1120.9 examples/sec; 0.114 sec/batch)
2017-05-09 21:50:33.228672: step 23520, loss = 0.73 (986.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:50:34.499506: step 23530, loss = 1.00 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:50:35.791976: step 23540, loss = 0.81 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:50:37.064870: step 23550, loss = 0.76 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:50:38.361369: step 23560, loss = 0.69 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:50:39.651081: step 23570, loss = 0.85 (992.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:50:40.939550: step 23580, loss = 0.93 (993.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:50:42.228207: step 23590, loss = 0.89 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:50:43.619026: step 23600, loss = 0.76 (920.3 examples/sec; 0.139 sec/batch)
2017-05-09 21:50:44.809346: step 23610, loss = 0.85 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-09 21:50:46.107389: step 23620, loss = 0.88 (986.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:50:47.390406: step 23630, loss = 0.78 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:50:48.698475: step 23640, loss = 0.82 (978.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:50:50.001639: step 23650, loss = 0.79 (982.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:50:51.297549: step 23660, loss = 0.79 (987.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:50:52.612310: step 23670, loss = 0.92 (973.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:50:53.908527: step 23680, loss = 0.90 (987.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:50:55.189669: step 23690, loss = 0.88 (999.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:50:56.578999: step 23700, loss = 0.67 (921.3 examples/sec; 0.139 sec/batch)
2017-05-09 21:50:57.808719: step 23710, loss = 0.80 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-09 21:50:59.091050: step 23720, loss = 0.87 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:51:00.380276: step 23730, loss = 0.79 (992.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:51:01.659973: step 23740, loss = 0.65 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:51:02.943507: step 23750, loss = 0.83 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:51:04.235682: step 23760, loss = 0.80 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:51:05.535272: step 23770, loss = 0.81 (984.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:51:06.821418: step 23780, loss = 0.70 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:51:08.105351: step 23790, loss = 0.85 (996.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:51:09.495400: step 23800, loss = 0.73 (920.8 examples/sec; 0.139 sec/batch)
2017-05-09 21:51:10.710514: step 23810, loss = 1.09 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-09 21:51:11.986265: step 23820, loss = 0.90 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:51:13.267187: step 23830, loss = 0.88 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:51:14.564269: step 23840, loss = 0.96 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:51:15.950515: step 23850, loss = 0.72 (923.4 examples/sec; 0.139 sec/batch)
2017-05-09 21:51:17.228345: step 23860, loss = 0.81 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:51:18.538158: step 23870, loss = 0.91 (977.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:51:19.788377: step 23880, loss = 0.85 (1023.8 examples/sec; 0.125 sec/batch)
2017-05-09 21:51:21.075922: step 23890, loss = 0.91 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:51:22.459961: step 23900, loss = 0.85 (924.8 examples/sec; 0.138 sec/batch)
2017-05-09 21:51:23.639958: step 23910, loss = 1.10 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-09 21:51:24.931044: step 23920, loss = 0.68 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:51:26.275399: step 23930, loss = 0.63 (952.1 examples/sec; 0.134 sec/batch)
2017-05-09 21:51:27.577277: step 23940, loss = 0.84 (983.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:51:28.864408: step 23950, loss = 0.86 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:51:30.168972: step 23960, loss = 0.65 (981.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:51:31.439299: step 23970, loss = 0.94 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:51:32.725051: step 23980, loss = 0.94 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:51:34.008858: step 23990, loss = 0.69 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:51:35.438227: step 24000, loss = 0.90 (895.5 examples/sec; 0.143 sec/batch)
2017-05-09 21:51:36.601204: step 24010, loss = 0.88 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-09 21:51:37.892874: step 24020, loss = 0.89 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:51:39.163803: step 24030, loss = 0.85 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:51:40.456305: step 24040, loss = 1.04 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:51:41.751768: step 24050, loss = 0.81 (988.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:51:43.023007: step 24060, loss = 0.83 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:51:44.286931: step 24070, loss = 0.78 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-09 21:51:45.555785: step 24080, loss = 0.76 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-09 21:51:46.806873: step 24090, loss = 0.87 (1023.1 examples/sec; 0.125 sec/batch)
2017-05-09 21:51:48.211485: step 24100, loss = 1.07 (911.3 examples/sec; 0.140 sec/batch)
2017-05-09 21:51:49.385515: step 24110, loss = 0.79 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-09 21:51:50.671922: step 24120, loss = 0.77 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:51:51.955228: step 24130, loss = 0.77 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:51:53.252005: step 24140, loss = 0.80 (987.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:51:54.541118: step 24150, loss = 0.85 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:51:55.832367: step 24160, loss = 0.72 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:51:57.097218: step 24170, loss = 0.80 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-09 21:51:58.386696: step 24180, loss = 0.75 (992.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:51:59.652970: step 24190, loss = 0.99 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-09 21:52:01.012750: step 24200, loss = 0.75 (941.3 examples/sec; 0.136 sec/batch)
2017-05-09 21:52:02.208964: step 24210, loss = 0.90 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-09 21:52:03.543970: step 24220, loss = 0.85 (958.8 examples/sec; 0.134 sec/batch)
2017-05-09 21:52:04.838054: step 24230, loss = 0.90 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:52:06.131236: step 24240, loss = 0.69 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:52:07.397956: step 24250, loss = 0.72 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-09 21:52:08.680668: step 24260, loss = 0.87 (997.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:52:09.988208: step 24270, loss = 0.85 (978.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:52:11.271308: step 24280, loss = 0.77 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:52:12.516890: step 24290, loss = 1.28 (1027.6 examples/sec; 0.125 sec/batch)
2017-05-09 21:52:13.883578: step 24300, loss = 1.01 (936.6 examples/sec; 0.137 sec/batch)
2017-05-09 21:52:15.082751: step 24310, loss = 0.75 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-09 21:52:16.356213: step 24320, loss = 0.82 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:52:17.654526: step 24330, loss = 1.07 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:52:18.924889: step 24340, loss = 0.76 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:52:20.326370: step 24350, loss = 0.83 (913.3 examples/sec; 0.140 sec/batch)
2017-05-09 21:52:21.623302: step 24360, loss = 0.83 (987.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:52:22.884994: step 24370, loss = 0.91 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-09 21:52:24.174042: step 24380, loss = 0.73 (993.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:52:25.448642: step 24390, loss = 0.71 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:52:26.844293: step 24400, loss = 0.92 (917.1 examples/sec; 0.140 sec/batch)
2017-05-09 21:52:28.019134: step 24410, loss = 0.83 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-09 21:52:29.302869: step 24420, loss = 1.01 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:52:30.589250: step 24430, loss = 0.87 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:52:31.867348: step 24440, loss = 0.84 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:52:33.141003: step 24450, loss = 0.74 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:52:34.441347: step 24460, loss = 0.74 (984.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:52:35.725699: step 24470, loss = 0.79 (996.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:52:36.995746: step 24480, loss = 0.80 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-09 21:52:38.283491: step 24490, loss = 0.79 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:52:39.677541: step 24500, loss = 0.82 (918.2 examples/sec; 0.139 sec/batch)
2017-05-09 21:52:40.867955: step 24510, loss = 0.92 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-09 21:52:42.156092: step 24520, loss = 1.00 (993.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:52:43.492354: step 24530, loss = 0.83 (957.9 examples/sec; 0.134 sec/batch)
2017-05-09 21:52:44.783604: step 24540, loss = 0.84 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:52:46.051050: step 24550, loss = 0.77 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:52:47.319838: step 24560, loss = 0.77 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-09 21:52:48.611946: step 24570, loss = 0.89 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:52:49.886573: step 24580, loss = 0.78 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:52:51.159303: step 24590, loss = 0.76 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:52:52.552801: step 24600, loss = 1.00 (918.6 examples/sec; 0.139 sec/batch)
2017-05-09 21:52:53.738668: step 24610, loss = 0.86 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-09 21:52:55.048939: step 24620, loss = 0.84 (976.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:52:56.353833: step 24630, loss = 0.89 (980.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:52:57.651167: step 24640, loss = 0.91 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:52:58.940538: step 24650, loss = 0.85 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:53:00.216990: step 24660, loss = 0.99 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:53:01.510641: step 24670, loss = 1.01 (989.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:53:02.816722: step 24680, loss = 0.68 (980.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:53:04.134962: step 24690, loss = 0.91 (971.0 examples/sec; 0.132 sec/batch)
2017-05-09 21:53:05.559595: step 24700, loss = 0.80 (898.5 examples/sec; 0.142 sec/batch)
2017-05-09 21:53:06.756305: step 24710, loss = 0.74 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-09 21:53:08.044712: step 24720, loss = 0.96 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:53:09.336381: step 24730, loss = 0.90 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:53:10.620484: step 24740, loss = 0.96 (996.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:53:11.903571: step 24750, loss = 0.83 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:53:13.187885: step 24760, loss = 0.83 (996.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:53:14.462524: step 24770, loss = 0.78 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:53:15.735938: step 24780, loss = 0.81 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:53:17.004751: step 24790, loss = 0.87 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-09 21:53:18.412211: step 24800, loss = 0.90 (909.4 examples/sec; 0.141 sec/batch)
2017-05-09 21:53:19.625221: step 24810, loss = 0.80 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-09 21:53:20.928019: step 24820, loss = 0.96 (982.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:53:22.200067: step 24830, loss = 0.90 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:53:23.492178: step 24840, loss = 0.76 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:53:24.784828: step 24850, loss = 0.83 (990.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:53:26.073514: step 24860, loss = 0.82 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:53:27.389795: step 24870, loss = 1.08 (972.4 examples/sec; 0.132 sec/batch)
2017-05-09 21:53:28.726703: step 24880, loss = 0.85 (957.4 examples/sec; 0.134 sec/batch)
2017-05-09 21:53:30.040124: step 24890, loss = 0.74 (974.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:53:31.454861: step 24900, loss = 0.84 (904.8 examples/sec; 0.141 sec/batch)
2017-05-09 21:53:32.613039: step 24910, loss = 0.84 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-09 21:53:33.898223: step 24920, loss = 0.75 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:53:35.185886: step 24930, loss = 0.83 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:53:36.440261: step 24940, loss = 1.01 (1020.4 examples/sec; 0.125 sec/batch)
2017-05-09 21:53:37.717800: step 24950, loss = 0.79 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:53:39.008143: step 24960, loss = 0.84 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:53:40.299580: step 24970, loss = 0.71 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:53:41.572030: step 24980, loss = 0.87 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:53:42.855553: step 24990, loss = 1.04 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:53:44.229960: step 25000, loss = 0.82 (931.3 examples/sec; 0.137 sec/batch)
2017-05-09 21:53:45.433510: step 25010, loss = 0.81 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-09 21:53:46.744348: step 25020, loss = 0.84 (976.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:53:48.019812: step 25030, loss = 0.77 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:53:49.334344: step 25040, loss = 0.73 (973.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:53:50.649748: step 25050, loss = 0.82 (973.1 examples/sec; 0.132 sec/batch)
2017-05-09 21:53:51.988925: step 25060, loss = 0.88 (955.8 examples/sec; 0.134 sec/batch)
2017-05-09 21:53:53.407690: step 25070, loss = 0.88 (902.2 examples/sec; 0.142 sec/batch)
2017-05-09 21:53:54.673674: step 25080, loss = 0.76 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:53:55.965175: step 25090, loss = 0.98 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:53:57.344391: step 25100, loss = 0.88 (928.1 examples/sec; 0.138 sec/batch)
2017-05-09 21:53:58.524724: step 25110, loss = 0.84 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-09 21:53:59.791129: step 25120, loss = 0.75 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:54:01.075316: step 25130, loss = 0.73 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:54:02.354129: step 25140, loss = 0.94 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:54:03.643952: step 25150, loss = 1.03 (992.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:54:04.908857: step 25160, loss = 0.89 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-09 21:54:06.231971: step 25170, loss = 1.00 (967.4 examples/sec; 0.132 sec/batch)
2017-05-09 21:54:07.521321: step 25180, loss = 0.92 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:54:08.799335: step 25190, loss = 0.92 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:54:10.167608: step 25200, loss = 0.90 (935.5 examples/sec; 0.137 sec/batch)
2017-05-09 21:54:11.349713: step 25210, loss = 0.68 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-09 21:54:12.630290: step 25220, loss = 0.97 (999.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:54:13.911882: step 25230, loss = 0.91 (998.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:54:15.181989: step 25240, loss = 0.79 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-09 21:54:16.455556: step 25250, loss = 0.68 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:54:17.756131: step 25260, loss = 0.79 (984.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:54:19.048431: step 25270, loss = 0.79 (990.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:54:20.318743: step 25280, loss = 0.78 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:54:21.595146: step 25290, loss = 0.71 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:54:23.002883: step 25300, loss = 1.01 (909.3 examples/sec; 0.141 sec/batch)
2017-05-09 21:54:24.203578: step 25310, loss = 1.10 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-09 21:54:25.491507: step 25320, loss = 0.94 (993.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:54:26.763898: step 25330, loss = 0.95 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:54:28.055128: step 25340, loss = 0.86 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:54:29.323325: step 25350, loss = 0.97 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:54:30.615164: step 25360, loss = 0.75 (990.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:54:31.894940: step 25370, loss = 0.76 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:54:33.182145: step 25380, loss = 0.73 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:54:34.482332: step 25390, loss = 0.97 (984.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:54:35.854429: step 25400, loss = 0.81 (932.9 examples/sec; 0.137 sec/batch)
2017-05-09 21:54:37.036686: step 25410, loss = 1.01 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-09 21:54:38.311772: step 25420, loss = 0.89 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:54:39.587272: step 25430, loss = 0.65 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:54:40.864909: step 25440, loss = 0.89 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:54:42.150063: step 25450, loss = 0.73 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:54:43.433690: step 25460, loss = 1.03 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:54:44.689705: step 25470, loss = 0.91 (1019.1 examples/sec; 0.126 sec/batch)
2017-05-09 21:54:45.962634: step 25480, loss = 0.78 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:54:47.251767: step 25490, loss = 0.93 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:54:48.628565: step 25500, loss = 0.98 (929.7 examples/sec; 0.138 sec/batch)
2017-05-09 21:54:49.835714: step 25510, loss = 0.67 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-09 21:54:51.108532: step 25520, loss = 0.85 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:54:52.380795: step 25530, loss = 0.83 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:54:53.672749: step 25540, loss = 0.84 (990.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:54:54.976223: step 25550, loss = 0.73 (982.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:54:56.268517: step 25560, loss = 0.80 (990.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:54:57.544678: step 25570, loss = 0.84 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:54:58.823612: step 25580, loss = 0.70 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:55:00.095564: step 25590, loss = 0.84 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:55:01.467064: step 25600, loss = 1.08 (933.3 examples/sec; 0.137 sec/batch)
2017-05-09 21:55:02.687800: step 25610, loss = 0.74 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-09 21:55:03.986624: step 25620, loss = 0.94 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:55:05.287698: step 25630, loss = 0.86 (983.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:55:06.547386: step 25640, loss = 0.84 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-09 21:55:07.821535: step 25650, loss = 0.76 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:55:09.083225: step 25660, loss = 0.91 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-09 21:55:10.356988: step 25670, loss = 1.04 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:55:11.642179: step 25680, loss = 0.87 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:55:12.912514: step 25690, loss = 0.92 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:55:14.283739: step 25700, loss = 0.89 (933.5 examples/sec; 0.137 sec/batch)
2017-05-09 21:55:15.464869: step 25710, loss = 0.92 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-09 21:55:16.729061: step 25720, loss = 0.89 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-09 21:55:18.008781: step 25730, loss = 0.70 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:55:19.302740: step 25740, loss = 0.87 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:55:20.593137: step 25750, loss = 0.92 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:55:21.902909: step 25760, loss = 0.79 (977.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:55:23.174304: step 25770, loss = 0.89 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-09 21:55:24.450519: step 25780, loss = 0.76 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:55:25.711312: step 25790, loss = 0.97 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-09 21:55:27.101804: step 25800, loss = 0.79 (920.5 examples/sec; 0.139 sec/batch)
2017-05-09 21:55:28.288629: step 25810, loss = 0.80 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-09 21:55:29.628729: step 25820, loss = 0.89 (955.2 examples/sec; 0.134 sec/batch)
2017-05-09 21:55:30.918269: step 25830, loss = 0.90 (992.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:55:32.230025: step 25840, loss = 0.84 (975.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:55:33.516374: step 25850, loss = 0.93 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:55:34.833750: step 25860, loss = 0.91 (971.6 examples/sec; 0.132 sec/batch)
2017-05-09 21:55:36.139223: step 25870, loss = 0.94 (980.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:55:37.402693: step 25880, loss = 0.89 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-09 21:55:38.685939: step 25890, loss = 0.73 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:55:40.051843: step 25900, loss = 0.77 (937.1 examples/sec; 0.137 sec/batch)
2017-05-09 21:55:41.242762: step 25910, loss = 0.86 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-09 21:55:42.529018: step 25920, loss = 0.75 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:55:43.811897: step 25930, loss = 0.94 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:55:45.077544: step 25940, loss = 0.68 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:55:46.373090: step 25950, loss = 0.94 (988.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:55:47.654121: step 25960, loss = 0.74 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:55:48.934425: step 25970, loss = 0.81 (999.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:55:50.213953: step 25980, loss = 0.79 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:55:51.488127: step 25990, loss = 0.80 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:55:52.850261: step 26000, loss = 0.94 (939.7 examples/sec; 0.136 sec/batch)
2017-05-09 21:55:54.037182: step 26010, loss = 0.76 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-09 21:55:55.300912: step 26020, loss = 1.09 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-09 21:55:56.581792: step 26030, loss = 0.78 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:55:57.879721: step 26040, loss = 0.87 (986.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:55:59.141681: step 26050, loss = 1.02 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-09 21:56:00.441654: step 26060, loss = 0.90 (984.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:56:01.710369: step 26070, loss = 0.84 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:56:02.975970: step 26080, loss = 1.02 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:56:04.244950: step 26090, loss = 0.62 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:56:05.614161: step 26100, loss = 0.98 (934.8 examples/sec; 0.137 sec/batch)
2017-05-09 21:56:06.809902: step 26110, loss = 0.88 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-09 21:56:08.089734: step 26120, loss = 0.84 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:56:09.376429: step 26130, loss = 0.99 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:56:10.648653: step 26140, loss = 0.70 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:56:11.908789: step 26150, loss = 0.76 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-09 21:56:13.206041: step 26160, loss = 0.98 (986.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:56:14.494405: step 26170, loss = 0.88 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:56:15.754030: step 26180, loss = 1.04 (1016.2 examples/sec; 0.126 sec/batch)
2017-05-09 21:56:17.044555: step 26190, loss = 1.05 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:56:18.424581: step 26200, loss = 0.86 (927.5 examples/sec; 0.138 sec/batch)
2017-05-09 21:56:19.605729: step 26210, loss = 0.83 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-09 21:56:20.885229: step 26220, loss = 0.84 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:56:22.171047: step 26230, loss = 0.66 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:56:23.457322: step 26240, loss = 0.92 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:56:24.744582: step 26250, loss = 0.86 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:56:26.038366: step 26260, loss = 1.00 (989.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:56:27.312172: step 26270, loss = 1.17 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:56:28.596763: step 26280, loss = 0.93 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:56:29.852883: step 26290, loss = 0.80 (1019.0 examples/sec; 0.126 sec/batch)
2017-05-09 21:56:31.238993: step 26300, loss = 1.07 (923.4 examples/sec; 0.139 sec/batch)
2017-05-09 21:56:32.411965: step 26310, loss = 0.90 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-09 21:56:33.678036: step 26320, loss = 0.79 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:56:34.994468: step 26330, loss = 0.81 (972.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:56:36.267428: step 26340, loss = 0.86 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-09 21:56:37.574046: step 26350, loss = 0.81 (979.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:56:38.835862: step 26360, loss = 0.82 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-09 21:56:40.108015: step 26370, loss = 1.00 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:56:41.403147: step 26380, loss = 1.24 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:56:42.720149: step 26390, loss = 0.87 (971.9 examples/sec; 0.132 sec/batch)
2017-05-09 21:56:44.081871: step 26400, loss = 0.78 (940.0 examples/sec; 0.136 sec/batch)
2017-05-09 21:56:45.250596: step 26410, loss = 0.94 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-09 21:56:46.519545: step 26420, loss = 0.75 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:56:47.792851: step 26430, loss = 0.83 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:56:49.084536: step 26440, loss = 0.63 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:56:50.367322: step 26450, loss = 0.72 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:56:51.658936: step 26460, loss = 0.75 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:56:52.938251: step 26470, loss = 0.82 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:56:54.201158: step 26480, loss = 0.86 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-09 21:56:55.478104: step 26490, loss = 0.87 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:56:56.813293: step 26500, loss = 0.84 (958.7 examples/sec; 0.134 sec/batch)
2017-05-09 21:56:58.011810: step 26510, loss = 0.97 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-09 21:56:59.274486: step 26520, loss = 0.78 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-09 21:57:00.547350: step 26530, loss = 0.84 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:57:01.831011: step 26540, loss = 0.91 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:57:03.113813: step 26550, loss = 0.74 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:57:04.401056: step 26560, loss = 0.73 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:57:05.701436: step 26570, loss = 0.92 (984.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:57:06.969828: step 26580, loss = 0.77 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:57:08.255266: step 26590, loss = 0.69 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:57:09.632195: step 26600, loss = 0.72 (929.6 examples/sec; 0.138 sec/batch)
2017-05-09 21:57:10.803606: step 26610, loss = 0.71 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-09 21:57:12.082996: step 26620, loss = 0.80 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:57:13.358527: step 26630, loss = 0.78 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:57:14.630913: step 26640, loss = 0.79 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:57:15.909780: step 26650, loss = 0.74 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:57:17.185049: step 26660, loss = 1.02 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:57:18.465109: step 26670, loss = 0.95 (999.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:57:19.748305: step 26680, loss = 0.90 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:57:21.067041: step 26690, loss = 0.79 (970.6 examples/sec; 0.132 sec/batch)
2017-05-09 21:57:22.454551: step 26700, loss = 0.82 (922.5 examples/sec; 0.139 sec/batch)
2017-05-09 21:57:23.711638: step 26710, loss = 0.76 (1018.2 examples/sec; 0.126 sec/batch)
2017-05-09 21:57:24.995507: step 26720, loss = 0.74 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:57:26.304201: step 26730, loss = 0.70 (978.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:57:27.599406: step 26740, loss = 0.92 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:57:28.865989: step 26750, loss = 0.81 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:57:30.141671: step 26760, loss = 0.77 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:57:31.428762: step 26770, loss = 0.76 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:57:32.846961: step 26780, loss = 0.86 (902.6 examples/sec; 0.142 sec/batch)
2017-05-09 21:57:34.480989: step 26790, loss = 1.04 (783.4 examples/sec; 0.163 sec/batch)
2017-05-09 21:57:36.202375: step 26800, loss = 0.93 (743.6 examples/sec; 0.172 sec/batch)
2017-05-09 21:57:37.598729: step 26810, loss = 0.77 (916.7 examples/sec; 0.140 sec/batch)
2017-05-09 21:57:39.179007: step 26820, loss = 0.79 (810.0 examples/sec; 0.158 sec/batch)
2017-05-09 21:57:40.724488: step 26830, loss = 0.88 (828.2 examples/sec; 0.155 sec/batch)
2017-05-09 21:57:42.340800: step 26840, loss = 0.86 (791.9 examples/sec; 0.162 sec/batch)
2017-05-09 21:57:43.888256: step 26850, loss = 0.84 (827.2 examples/sec; 0.155 sec/batch)
2017-05-09 21:57:45.454326: step 26860, loss = 0.79 (817.3 examples/sec; 0.157 sec/batch)
2017-05-09 21:57:47.004234: step 26870, loss = 0.72 (825.9 examples/sec; 0.155 sec/batch)
2017-05-09 21:57:48.532393: step 26880, loss = 0.63 (837.6 examples/sec; 0.153 sec/batch)
2017-05-09 21:57:50.013313: step 26890, loss = 0.98 (864.3 examples/sec; 0.148 sec/batch)
2017-05-09 21:57:51.717240: step 26900, loss = 0.91 (751.2 examples/sec; 0.170 sec/batch)
2017-05-09 21:57:53.126823: step 26910, loss = 0.87 (908.1 examples/sec; 0.141 sec/batch)
2017-05-09 21:57:54.801758: step 26920, loss = 0.79 (764.2 examples/sec; 0.167 sec/batch)
2017-05-09 21:57:56.502202: step 26930, loss = 0.90 (752.7 examples/sec; 0.170 sec/batch)
2017-05-09 21:57:58.145638: step 26940, loss = 0.99 (778.9 examples/sec; 0.164 sec/batch)
2017-05-09 21:57:59.770669: step 26950, loss = 0.82 (787.7 examples/sec; 0.163 sec/batch)
2017-05-09 21:58:01.395399: step 26960, loss = 0.65 (787.8 examples/sec; 0.162 sec/batch)
2017-05-09 21:58:03.002939: step 26970, loss = 0.69 (796.2 examples/sec; 0.161 sec/batch)
2017-05-09 21:58:04.529585: step 26980, loss = 0.96 (838.4 examples/sec; 0.153 sec/batch)
2017-05-09 21:58:06.174785: step 26990, loss = 0.88 (778.0 examples/sec; 0.165 sec/batch)
2017-05-09 21:58:07.868578: step 27000, loss = 0.78 (755.7 examples/sec; 0.169 sec/batch)
2017-05-09 21:58:09.241137: step 27010, loss = 0.75 (932.6 examples/sec; 0.137 sec/batch)
2017-05-09 21:58:10.758429: step 27020, loss = 0.92 (843.6 examples/sec; 0.152 sec/batch)
2017-05-09 21:58:12.341042: step 27030, loss = 0.85 (808.8 examples/sec; 0.158 sec/batch)
2017-05-09 21:58:13.955759: step 27040, loss = 0.90 (792.7 examples/sec; 0.161 sec/batch)
2017-05-09 21:58:15.670948: step 27050, loss = 1.07 (746.3 examples/sec; 0.172 sec/batch)
2017-05-09 21:58:17.289618: step 27060, loss = 0.79 (790.8 examples/sec; 0.162 sec/batch)
2017-05-09 21:58:18.809572: step 27070, loss = 0.83 (842.1 examples/sec; 0.152 sec/batch)
2017-05-09 21:58:20.365030: step 27080, loss = 0.78 (822.9 examples/sec; 0.156 sec/batch)
2017-05-09 21:58:21.935814: step 27090, loss = 0.74 (814.9 examples/sec; 0.157 sec/batch)
2017-05-09 21:58:23.841077: step 27100, loss = 0.79 (671.8 examples/sec; 0.191 sec/batch)
2017-05-09 21:58:25.245982: step 27110, loss = 0.80 (911.1 examples/sec; 0.140 sec/batch)
2017-05-09 21:58:26.824253: step 27120, loss = 0.94 (811.0 examples/sec; 0.158 sec/batch)
2017-05-09 21:58:28.430985: step 27130, loss = 0.84 (796.6 examples/sec; 0.161 sec/batch)
2017-05-09 21:58:30.011681: step 27140, loss = 0.72 (809.8 examples/sec; 0.158 sec/batch)
2017-05-09 21:58:31.498674: step 27150, loss = 0.81 (860.8 examples/sec; 0.149 sec/batch)
2017-05-09 21:58:33.100919: step 27160, loss = 0.89 (798.9 examples/sec; 0.160 sec/batch)
2017-05-09 21:58:34.609307: step 27170, loss = 0.72 (848.6 examples/sec; 0.151 sec/batch)
2017-05-09 21:58:36.146072: step 27180, loss = 0.76 (832.9 examples/sec; 0.154 sec/batch)
2017-05-09 21:58:37.608406: step 27190, loss = 0.84 (875.3 examples/sec; 0.146 sec/batch)
2017-05-09 21:58:39.009029: step 27200, loss = 0.88 (913.9 examples/sec; 0.140 sec/batch)
2017-05-09 21:58:40.182811: step 27210, loss = 0.86 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-09 21:58:41.752161: step 27220, loss = 0.71 (815.6 examples/sec; 0.157 sec/batch)
2017-05-09 21:58:43.483805: step 27230, loss = 0.65 (739.2 examples/sec; 0.173 sec/batch)
2017-05-09 21:58:45.155708: step 27240, loss = 0.89 (765.6 examples/sec; 0.167 sec/batch)
2017-05-09 21:58:46.753688: step 27250, loss = 0.82 (801.0 examples/sec; 0.160 sec/batch)
2017-05-09 21:58:48.358958: step 27260, loss = 0.87 (797.4 examples/sec; 0.161 sec/batch)
2017-05-09 21:58:49.946374: step 27270, loss = 0.97 (806.3 examples/sec; 0.159 sec/batch)
2017-05-09 21:58:51.574941: step 27280, loss = 0.80 (786.0 examples/sec; 0.163 sec/batch)
2017-05-09 21:58:53.142122: step 27290, loss = 0.73 (816.8 examples/sec; 0.157 sec/batch)
2017-05-09 21:58:54.619662: step 27300, loss = 1.04 (866.3 examples/sec; 0.148 sec/batch)
2017-05-09 21:58:55.950265: step 27310, loss = 0.91 (962.0 examples/sec; 0.133 sec/batch)
2017-05-09 21:58:57.304268: step 27320, loss = 1.05 (945.3 examples/sec; 0.135 sec/batch)
2017-05-09 21:58:58.583177: step 27330, loss = 0.85 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:58:59.897478: step 27340, loss = 0.83 (973.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:59:01.205371: step 27350, loss = 0.85 (978.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:59:02.480976: step 27360, loss = 0.71 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:59:03.831234: step 27370, loss = 0.75 (948.0 examples/sec; 0.135 sec/batch)
2017-05-09 21:59:05.118520: step 27380, loss = 0.81 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:59:06.415393: step 27390, loss = 0.89 (987.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:59:07.809641: step 27400, loss = 0.79 (918.1 examples/sec; 0.139 sec/batch)
2017-05-09 21:59:09.046260: step 27410, loss = 0.71 (1035.1 examples/sec; 0.124 sec/batch)
2017-05-09 21:59:10.331611: step 27420, loss = 0.74 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:59:11.955438: step 27430, loss = 0.82 (788.3 examples/sec; 0.162 sec/batch)
2017-05-09 21:59:13.460862: step 27440, loss = 1.02 (850.3 examples/sec; 0.151 sec/batch)
2017-05-09 21:59:14.893959: step 27450, loss = 0.79 (893.2 examples/sec; 0.143 sec/batch)
2017-05-09 21:59:16.393436: step 27460, loss = 0.79 (853.6 examples/sec; 0.150 sec/batch)
2017-05-09 21:59:17.970409: step 27470, loss = 0.82 (811.7 examples/sec; 0.158 sec/batch)
2017-05-09 21:59:19.442720: step 27480, loss = 0.90 (869.4 examples/sec; 0.147 sec/batch)
2017-05-09 21:59:21.001245: step 27490, loss = 0.79 (821.3 examples/sec; 0.156 sec/batch)
2017-05-09 21:59:22.413196: step 27500, loss = 0.67 (906.5 examples/sec; 0.141 sec/batch)
2017-05-09 21:59:23.599543: step 27510, loss = 0.81 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-09 21:59:24.889822: step 27520, loss = 0.82 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:59:26.184363: step 27530, loss = 0.94 (988.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:59:27.503845: step 27540, loss = 1.00 (970.1 examples/sec; 0.132 sec/batch)
2017-05-09 21:59:28.765202: step 27550, loss = 0.83 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-09 21:59:30.053485: step 27560, loss = 0.79 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:59:31.350641: step 27570, loss = 0.70 (986.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:59:32.626597: step 27580, loss = 1.03 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:59:33.879940: step 27590, loss = 0.83 (1021.3 examples/sec; 0.125 sec/batch)
2017-05-09 21:59:35.265814: step 27600, loss = 0.72 (923.6 examples/sec; 0.139 sec/batch)
2017-05-09 21:59:36.471394: step 27610, loss = 0.98 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-09 21:59:37.739142: step 27620, loss = 0.94 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:59:39.026381: step 27630, loss = 0.73 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:59:40.297098: step 27640, loss = 0.84 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:59:41.590581: step 27650, loss = 0.92 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:59:42.867310: step 27660, loss = 1.03 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:59:44.136873: step 27670, loss = 0.89 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:59:45.420639: step 27680, loss = 0.78 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:59:46.704158: step 27690, loss = 0.81 (997.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:59:48.074984: step 27700, loss = 0.95 (933.7 examples/sec; 0.137 sec/batch)
2017-05-09 21:59:49.270362: step 27710, loss = 0.66 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-09 21:59:50.550235: step 27720, loss = 0.86 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:59:51.832381: step 27730, loss = 0.82 (998.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:59:53.151026: step 27740, loss = 0.99 (970.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:59:54.461012: step 27750, loss = 0.96 (977.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:59:55.748722: step 27760, loss = 0.94 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:59:57.047091: step 27770, loss = 0.70 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:59:58.334232: step 27780, loss = 0.69 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:59:59.609833: step 27790, loss = 0.70 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:00:00.985392: step 27800, loss = 0.81 (930.5 examples/sec; 0.138 sec/batch)
2017-05-09 22:00:02.192505: step 27810, loss = 0.89 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-09 22:00:03.467364: step 27820, loss = 1.02 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:00:04.794506: step 27830, loss = 0.79 (964.5 examples/sec; 0.133 sec/batch)
2017-05-09 22:00:06.093492: step 27840, loss = 0.81 (985.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:00:07.424739: step 27850, loss = 0.70 (961.5 examples/sec; 0.133 sec/batch)
2017-05-09 22:00:08.708762: step 27860, loss = 0.82 (996.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:00:09.992942: step 27870, loss = 0.81 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:00:11.258995: step 27880, loss = 0.74 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:00:12.538929: step 27890, loss = 0.84 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:00:13.929536: step 27900, loss = 0.86 (920.5 examples/sec; 0.139 sec/batch)
2017-05-09 22:00:15.135631: step 27910, loss = 0.70 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-09 22:00:16.428673: step 27920, loss = 0.88 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:00:17.734092: step 27930, loss = 0.77 (980.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:00:19.028588: step 27940, loss = 0.82 (988.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:00:20.298547: step 27950, loss = 0.91 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:00:21.539282: step 27960, loss = 0.87 (1031.7 examples/sec; 0.124 sec/batch)
2017-05-09 22:00:22.826098: step 27970, loss = 0.82 (994.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:00:24.113566: step 27980, loss = 0.84 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:00:25.397831: step 27990, loss = 0.73 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:00:26.798953: step 28000, loss = 0.81 (913.5 examples/sec; 0.140 sec/batch)
2017-05-09 22:00:27.974927: step 28010, loss = 0.68 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-09 22:00:29.254613: step 28020, loss = 1.12 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:00:30.536079: step 28030, loss = 0.71 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:00:31.799123: step 28040, loss = 0.84 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-09 22:00:33.104935: step 28050, loss = 0.70 (980.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:00:34.382492: step 28060, loss = 0.80 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:00:35.665043: step 28070, loss = 0.82 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:00:36.976749: step 28080, loss = 0.85 (975.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:00:38.270607: step 28090, loss = 0.73 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:00:39.637677: step 28100, loss = 0.88 (936.3 examples/sec; 0.137 sec/batch)
2017-05-09 22:00:40.830176: step 28110, loss = 0.97 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-09 22:00:42.119563: step 28120, loss = 0.82 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:00:43.391138: step 28130, loss = 0.67 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:00:44.670881: step 28140, loss = 0.79 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:00:45.988796: step 28150, loss = 0.80 (971.2 examples/sec; 0.132 sec/batch)
2017-05-09 22:00:47.274269: step 28160, loss = 0.84 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:00:48.547351: step 28170, loss = 0.80 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:00:49.816273: step 28180, loss = 0.81 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:00:51.113716: step 28190, loss = 0.91 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:00:52.469065: step 28200, loss = 0.84 (944.4 examples/sec; 0.136 sec/batch)
2017-05-09 22:00:53.620313: step 28210, loss = 0.87 (1111.8 examples/sec; 0.115 sec/batch)
2017-05-09 22:00:54.879514: step 28220, loss = 0.74 (1016.5 examples/sec; 0.126 sec/batch)
2017-05-09 22:00:56.177820: step 28230, loss = 0.78 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:00:57.566523: step 28240, loss = 0.90 (921.7 examples/sec; 0.139 sec/batch)
2017-05-09 22:00:58.861035: step 28250, loss = 0.84 (988.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:01:00.133600: step 28260, loss = 0.76 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:01:01.424758: step 28270, loss = 1.02 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:01:02.714527: step 28280, loss = 0.70 (992.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:01:04.000463: step 28290, loss = 0.65 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:01:05.405974: step 28300, loss = 1.02 (910.7 examples/sec; 0.141 sec/batch)
2017-05-09 22:01:06.588758: step 28310, loss = 0.90 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-09 22:01:07.866986: step 28320, loss = 0.72 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:01:09.152655: step 28330, loss = 1.04 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:01:10.414872: step 28340, loss = 0.90 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-09 22:01:11.705902: step 28350, loss = 0.87 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:01:12.983102: step 28360, loss = 0.80 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:01:14.356287: step 28370, loss = 0.77 (932.1 examples/sec; 0.137 sec/batch)
2017-05-09 22:01:15.633822: step 28380, loss = 0.99 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:01:16.897941: step 28390, loss = 0.92 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-09 22:01:18.284657: step 28400, loss = 0.92 (923.0 examples/sec; 0.139 sec/batch)
2017-05-09 22:01:19.468622: step 28410, loss = 0.74 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-09 22:01:20.738320: step 28420, loss = 0.91 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:01:22.017366: step 28430, loss = 0.89 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:01:23.298850: step 28440, loss = 0.74 (998.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:01:24.608475: step 28450, loss = 0.72 (977.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:01:25.888214: step 28460, loss = 0.73 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:01:27.160326: step 28470, loss = 0.78 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:01:28.451931: step 28480, loss = 0.96 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:01:29.744402: step 28490, loss = 1.05 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:01:31.162177: step 28500, loss = 0.88 (902.8 examples/sec; 0.142 sec/batch)
2017-05-09 22:01:32.342198: step 28510, loss = 0.77 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-09 22:01:33.646740: step 28520, loss = 1.07 (981.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:01:34.929058: step 28530, loss = 0.73 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:01:36.204406: step 28540, loss = 0.81 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:01:37.524800: step 28550, loss = 0.89 (969.4 examples/sec; 0.132 sec/batch)
2017-05-09 22:01:38.813891: step 28560, loss = 0.80 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:01:40.149673: step 28570, loss = 0.89 (958.2 examples/sec; 0.134 sec/batch)
2017-05-09 22:01:41.461435: step 28580, loss = 0.81 (975.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:01:42.760854: step 28590, loss = 0.84 (985.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:01:44.137671: step 28600, loss = 0.75 (929.7 examples/sec; 0.138 sec/batch)
2017-05-09 22:01:45.429876: step 28610, loss = 0.82 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:01:46.895068: step 28620, loss = 0.78 (873.6 examples/sec; 0.147 sec/batch)
2017-05-09 22:01:48.310257: step 28630, loss = 0.73 (904.5 examples/sec; 0.142 sec/batch)
2017-05-09 22:01:49.729023: step 28640, loss = 0.89 (902.2 examples/sec; 0.142 sec/batch)
2017-05-09 22:01:51.025382: step 28650, loss = 0.73 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:01:52.303655: step 28660, loss = 0.95 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:01:53.579755: step 28670, loss = 0.71 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:01:54.848256: step 28680, loss = 0.79 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:01:56.116519: step 28690, loss = 0.78 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:01:57.513338: step 28700, loss = 0.81 (916.4 examples/sec; 0.140 sec/batch)
2017-05-09 22:01:58.696485: step 28710, loss = 0.76 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-09 22:01:59.969120: step 28720, loss = 0.85 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:02:01.270695: step 28730, loss = 0.80 (983.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:02:02.553649: step 28740, loss = 0.89 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:02:03.843282: step 28750, loss = 0.68 (992.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:02:05.152137: step 28760, loss = 0.72 (978.0 examples/sec; 0.131 sec/batch)
2017-05-09 22:02:06.444565: step 28770, loss = 0.81 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:02:07.736076: step 28780, loss = 0.69 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:02:09.032890: step 28790, loss = 0.98 (987.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:02:10.414354: step 28800, loss = 0.90 (926.6 examples/sec; 0.138 sec/batch)
2017-05-09 22:02:11.596397: step 28810, loss = 0.82 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-09 22:02:12.879764: step 28820, loss = 0.84 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:02:14.178800: step 28830, loss = 0.70 (985.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:02:15.481273: step 28840, loss = 0.82 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:02:16.758938: step 28850, loss = 0.80 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:02:18.063907: step 28860, loss = 0.74 (980.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:02:19.338770: step 28870, loss = 0.88 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:02:20.676693: step 28880, loss = 0.75 (956.7 examples/sec; 0.134 sec/batch)
2017-05-09 22:02:21.972665: step 28890, loss = 0.75 (987.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:02:23.365674: step 28900, loss = 0.82 (918.9 examples/sec; 0.139 sec/batch)
2017-05-09 22:02:24.569831: step 28910, loss = 0.72 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-09 22:02:25.875080: step 28920, loss = 0.87 (980.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:02:27.168275: step 28930, loss = 0.86 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:02:28.439396: step 28940, loss = 0.79 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:02:29.735710: step 28950, loss = 0.96 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:02:31.002672: step 28960, loss = 0.83 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:02:32.296479: step 28970, loss = 0.69 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:02:33.609921: step 28980, loss = 0.91 (974.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:02:34.905021: step 28990, loss = 0.80 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:02:36.299394: step 29000, loss = 0.79 (918.0 examples/sec; 0.139 sec/batch)
2017-05-09 22:02:37.478618: step 29010, loss = 0.99 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-09 22:02:38.755083: step 29020, loss = 0.76 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:02:40.038674: step 29030, loss = 0.98 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:02:41.324120: step 29040, loss = 0.82 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:02:42.613195: step 29050, loss = 0.74 (993.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:02:43.893370: step 29060, loss = 0.83 (999.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:02:45.162565: step 29070, loss = 0.76 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:02:46.455849: step 29080, loss = 1.00 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:02:47.789861: step 29090, loss = 0.85 (959.5 examples/sec; 0.133 sec/batch)
2017-05-09 22:02:49.155375: step 29100, loss = 0.85 (937.4 examples/sec; 0.137 sec/batch)
2017-05-09 22:02:50.346968: step 29110, loss = 0.82 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-09 22:02:51.642201: step 29120, loss = 0.81 (988.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:02:52.934135: step 29130, loss = 0.76 (990.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:02:54.219801: step 29140, loss = 0.94 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:02:55.553634: step 29150, loss = 0.82 (959.6 examples/sec; 0.133 sec/batch)
2017-05-09 22:02:56.838564: step 29160, loss = 0.82 (996.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:02:58.104880: step 29170, loss = 0.81 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:02:59.372480: step 29180, loss = 0.70 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:03:00.628275: step 29190, loss = 1.05 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-09 22:03:02.038872: step 29200, loss = 0.75 (907.4 examples/sec; 0.141 sec/batch)
2017-05-09 22:03:03.354095: step 29210, loss = 0.77 (973.2 examples/sec; 0.132 sec/batch)
2017-05-09 22:03:04.687179: step 29220, loss = 0.78 (960.2 examples/sec; 0.133 sec/batch)
2017-05-09 22:03:05.956539: step 29230, loss = 0.88 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:03:07.255580: step 29240, loss = 0.78 (985.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:03:08.558528: step 29250, loss = 0.78 (982.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:03:09.847359: step 29260, loss = 0.75 (993.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:03:11.141942: step 29270, loss = 0.76 (988.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:03:12.430356: step 29280, loss = 0.85 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:03:13.721914: step 29290, loss = 0.70 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:03:15.104975: step 29300, loss = 0.77 (925.5 examples/sec; 0.138 sec/batch)
2017-05-09 22:03:16.324107: step 29310, loss = 0.60 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-09 22:03:17.630813: step 29320, loss = 0.80 (979.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:03:18.956153: step 29330, loss = 0.72 (965.8 examples/sec; 0.133 sec/batch)
2017-05-09 22:03:20.244481: step 29340, loss = 0.94 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:03:21.521413: step 29350, loss = 0.85 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:03:22.808309: step 29360, loss = 0.92 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:03:24.095768: step 29370, loss = 0.80 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:03:25.429947: step 29380, loss = 0.87 (959.4 examples/sec; 0.133 sec/batch)
2017-05-09 22:03:26.746688: step 29390, loss = 0.79 (972.1 examples/sec; 0.132 sec/batch)
2017-05-09 22:03:28.128970: step 29400, loss = 0.84 (926.0 examples/sec; 0.138 sec/batch)
2017-05-09 22:03:29.324785: step 29410, loss = 0.88 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-09 22:03:30.590109: step 29420, loss = 0.81 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:03:31.856366: step 29430, loss = 0.75 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:03:33.162414: step 29440, loss = 0.77 (980.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:03:34.455599: step 29450, loss = 0.84 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:03:35.784017: step 29460, loss = 1.07 (963.6 examples/sec; 0.133 sec/batch)
2017-05-09 22:03:37.030815: step 29470, loss = 0.73 (1026.6 examples/sec; 0.125 sec/batch)
2017-05-09 22:03:38.326320: step 29480, loss = 0.79 (988.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:03:39.599550: step 29490, loss = 0.87 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:03:40.959790: step 29500, loss = 0.72 (941.0 examples/sec; 0.136 sec/batch)
2017-05-09 22:03:42.131573: step 29510, loss = 0.78 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-09 22:03:43.415966: step 29520, loss = 1.05 (996.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:03:44.722051: step 29530, loss = 0.82 (980.0 examples/sec; 0.131 sec/batch)
2017-05-09 22:03:46.052817: step 29540, loss = 0.88 (961.9 examples/sec; 0.133 sec/batch)
2017-05-09 22:03:47.313973: step 29550, loss = 0.82 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-09 22:03:48.580756: step 29560, loss = 0.92 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:03:49.860576: step 29570, loss = 0.87 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:03:51.141852: step 29580, loss = 0.79 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:03:52.410657: step 29590, loss = 0.91 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:03:53.800383: step 29600, loss = 0.88 (921.0 examples/sec; 0.139 sec/batch)
2017-05-09 22:03:54.970177: step 29610, loss = 0.80 (1094.2 examples/sec; 0.117 sec/batch)
2017-05-09 22:03:56.290787: step 29620, loss = 0.83 (969.2 examples/sec; 0.132 sec/batch)
2017-05-09 22:03:57.563247: step 29630, loss = 0.85 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:03:58.844241: step 29640, loss = 0.84 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:04:00.119081: step 29650, loss = 0.70 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:04:01.416861: step 29660, loss = 0.77 (986.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:04:02.683109: step 29670, loss = 0.80 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:04:03.969127: step 29680, loss = 0.87 (995.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:04:05.266164: step 29690, loss = 0.87 (986.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:04:06.647629: step 29700, loss = 0.91 (926.6 examples/sec; 0.138 sec/batch)
2017-05-09 22:04:07.820952: step 29710, loss = 0.75 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-09 22:04:09.081942: step 29720, loss = 0.73 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-09 22:04:10.383222: step 29730, loss = 0.80 (983.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:04:11.665239: step 29740, loss = 0.84 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:04:12.923285: step 29750, loss = 0.78 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-09 22:04:14.181210: step 29760, loss = 0.79 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-09 22:04:15.485188: step 29770, loss = 0.90 (981.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:04:16.764174: step 29780, loss = 0.80 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:04:18.053278: step 29790, loss = 0.86 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:04:19.434617: step 29800, loss = 0.85 (926.6 examples/sec; 0.138 sec/batch)
2017-05-09 22:04:20.635189: step 29810, loss = 0.88 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-09 22:04:21.905838: step 29820, loss = 0.96 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:04:23.192668: step 29830, loss = 0.69 (994.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:04:24.468856: step 29840, loss = 1.26 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:04:25.738788: step 29850, loss = 0.86 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:04:27.031311: step 29860, loss = 0.59 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:04:28.298720: step 29870, loss = 0.95 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:04:29.571290: step 29880, loss = 0.76 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:04:30.854234: step 29890, loss = 0.79 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:04:32.243601: step 29900, loss = 0.74 (921.3 examples/sec; 0.139 sec/batch)
2017-05-09 22:04:33.441564: step 29910, loss = 0.74 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-09 22:04:34.730231: step 29920, loss = 0.77 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:04:36.009931: step 29930, loss = 0.74 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:04:37.315811: step 29940, loss = 0.74 (980.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:04:38.591502: step 29950, loss = 0.79 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:04:39.894628: step 29960, loss = 0.97 (982.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:04:41.193322: step 29970, loss = 0.84 (985.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:04:42.489863: step 29980, loss = 0.87 (987.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:04:43.744749: step 29990, loss = 0.72 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-09 22:04:45.124251: step 30000, loss = 0.85 (927.9 examples/sec; 0.138 sec/batch)
2017-05-09 22:04:46.335742: step 30010, loss = 0.81 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-09 22:04:47.646294: step 30020, loss = 0.79 (976.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:04:48.909818: step 30030, loss = 0.85 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-09 22:04:50.202490: step 30040, loss = 0.87 (990.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:04:51.470716: step 30050, loss = 0.76 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:04:52.756388: step 30060, loss = 0.87 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:04:54.051155: step 30070, loss = 0.78 (988.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:04:55.352393: step 30080, loss = 1.02 (983.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:04:56.623135: step 30090, loss = 0.86 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:04:57.991934: step 30100, loss = 1.02 (935.1 examples/sec; 0.137 sec/batch)
2017-05-09 22:04:59.177002: step 30110, loss = 0.83 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-09 22:05:00.439709: step 30120, loss = 0.76 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-09 22:05:01.736286: step 30130, loss = 0.81 (987.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:05:03.009140: step 30140, loss = 0.86 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:05:04.276946: step 30150, loss = 0.64 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:05:05.584492: step 30160, loss = 0.92 (979.0 examples/sec; 0.131 sec/batch)
2017-05-09 22:05:06.875671: step 30170, loss = 0.93 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:05:08.159976: step 30180, loss = 0.97 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:05:09.424907: step 30190, loss = 0.64 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-09 22:05:10.795766: step 30200, loss = 0.86 (933.7 examples/sec; 0.137 sec/batch)
2017-05-09 22:05:11.970787: step 30210, loss = 0.78 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-09 22:05:13.250781: step 30220, loss = 0.75 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:05:14.550798: step 30230, loss = 0.77 (984.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:05:15.801492: step 30240, loss = 0.80 (1023.4 examples/sec; 0.125 sec/batch)
2017-05-09 22:05:17.137359: step 30250, loss = 0.74 (958.2 examples/sec; 0.134 sec/batch)
2017-05-09 22:05:18.465240: step 30260, loss = 0.84 (963.9 examples/sec; 0.133 sec/batch)
2017-05-09 22:05:19.757190: step 30270, loss = 1.07 (990.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:05:21.052392: step 30280, loss = 0.77 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:05:22.322330: step 30290, loss = 0.83 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:05:23.705722: step 30300, loss = 0.85 (925.3 examples/sec; 0.138 sec/batch)
2017-05-09 22:05:24.907632: step 30310, loss = 0.81 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-09 22:05:26.202697: step 30320, loss = 0.84 (988.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:05:27.479194: step 30330, loss = 1.02 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:05:28.765594: step 30340, loss = 0.93 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:05:30.050013: step 30350, loss = 0.99 (996.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:05:31.343096: step 30360, loss = 0.89 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:05:32.601728: step 30370, loss = 0.94 (1017.0 examples/sec; 0.126 sec/batch)
2017-05-09 22:05:33.864784: step 30380, loss = 0.97 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-09 22:05:35.140773: step 30390, loss = 0.91 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:05:36.497646: step 30400, loss = 0.87 (943.3 examples/sec; 0.136 sec/batch)
2017-05-09 22:05:37.686286: step 30410, loss = 0.70 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-09 22:05:38.969800: step 30420, loss = 0.87 (997.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:05:40.239074: step 30430, loss = 0.79 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:05:41.530850: step 30440, loss = 0.84 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:05:42.813202: step 30450, loss = 0.84 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:05:44.072298: step 30460, loss = 0.73 (1016.6 examples/sec; 0.126 sec/batch)
2017-05-09 22:05:45.387825: step 30470, loss = 1.00 (973.0 examples/sec; 0.132 sec/batch)
2017-05-09 22:05:46.670079: step 30480, loss = 0.61 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:05:47.965347: step 30490, loss = 0.76 (988.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:05:49.337767: step 30500, loss = 0.65 (932.7 examples/sec; 0.137 sec/batch)
2017-05-09 22:05:50.521889: step 30510, loss = 0.84 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-09 22:05:51.809845: step 30520, loss = 0.90 (993.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:05:53.086892: step 30530, loss = 0.80 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:05:54.374584: step 30540, loss = 0.82 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:05:55.656372: step 30550, loss = 0.76 (998.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:05:56.924375: step 30560, loss = 0.81 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:05:58.192590: step 30570, loss = 0.73 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:05:59.457054: step 30580, loss = 0.90 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-09 22:06:00.733054: step 30590, loss = 0.76 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:06:02.104168: step 30600, loss = 0.87 (933.6 examples/sec; 0.137 sec/batch)
2017-05-09 22:06:03.286932: step 30610, loss = 0.75 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-09 22:06:04.579906: step 30620, loss = 0.91 (990.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:06:05.860357: step 30630, loss = 0.96 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:06:07.124374: step 30640, loss = 1.09 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-09 22:06:08.401135: step 30650, loss = 0.74 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:06:09.664499: step 30660, loss = 0.81 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-09 22:06:10.958776: step 30670, loss = 1.07 (989.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:06:12.241095: step 30680, loss = 0.92 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:06:13.517600: step 30690, loss = 0.82 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:06:14.887262: step 30700, loss = 0.83 (934.5 examples/sec; 0.137 sec/batch)
2017-05-09 22:06:16.069137: step 30710, loss = 0.70 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-09 22:06:17.366217: step 30720, loss = 0.73 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:06:18.639961: step 30730, loss = 0.73 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:06:19.910588: step 30740, loss = 0.81 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:06:21.193704: step 30750, loss = 0.93 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:06:22.506352: step 30760, loss = 0.72 (975.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:06:23.778976: step 30770, loss = 0.88 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:06:25.052823: step 30780, loss = 0.79 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:06:26.330353: step 30790, loss = 0.84 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:06:27.693912: step 30800, loss = 0.66 (938.7 examples/sec; 0.136 sec/batch)
2017-05-09 22:06:28.884247: step 30810, loss = 0.79 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-09 22:06:30.161029: step 30820, loss = 0.94 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:06:31.418928: step 30830, loss = 0.68 (1017.6 examples/sec; 0.126 sec/batch)
2017-05-09 22:06:32.678607: step 30840, loss = 0.82 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-09 22:06:33.970438: step 30850, loss = 0.68 (990.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:06:35.239712: step 30860, loss = 0.81 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:06:36.534493: step 30870, loss = 0.86 (988.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:06:37.796885: step 30880, loss = 0.78 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-09 22:06:39.080605: step 30890, loss = 0.87 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:06:40.426111: step 30900, loss = 0.91 (951.3 examples/sec; 0.135 sec/batch)
2017-05-09 22:06:41.633021: step 30910, loss = 0.82 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-09 22:06:42.933003: step 30920, loss = 0.76 (984.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:06:44.221806: step 30930, loss = 0.89 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:06:45.497436: step 30940, loss = 0.82 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:06:46.781466: step 30950, loss = 1.09 (996.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:06:48.063855: step 30960, loss = 0.78 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:06:49.329147: step 30970, loss = 0.75 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:06:50.626088: step 30980, loss = 0.66 (986.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:06:51.904083: step 30990, loss = 0.73 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:06:53.278669: step 31000, loss = 0.78 (931.2 examples/sec; 0.137 sec/batch)
2017-05-09 22:06:54.481474: step 31010, loss = 0.86 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-09 22:06:55.768863: step 31020, loss = 0.81 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:06:57.056681: step 31030, loss = 0.93 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:06:58.339630: step 31040, loss = 0.66 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:06:59.639893: step 31050, loss = 0.96 (984.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:07:00.918112: step 31060, loss = 0.97 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:07:02.191332: step 31070, loss = 0.91 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:07:03.489601: step 31080, loss = 0.95 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:07:04.784327: step 31090, loss = 0.74 (988.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:07:06.153553: step 31100, loss = 0.86 (934.8 examples/sec; 0.137 sec/batch)
2017-05-09 22:07:07.348086: step 31110, loss = 0.70 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-09 22:07:08.611591: step 31120, loss = 0.89 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-09 22:07:09.883011: step 31130, loss = 0.99 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:07:11.162925: step 31140, loss = 0.78 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:07:12.417593: step 31150, loss = 0.87 (1020.2 examples/sec; 0.125 sec/batch)
2017-05-09 22:07:13.706297: step 31160, loss = 0.69 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:07:14.998493: step 31170, loss = 0.63 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:07:16.278165: step 31180, loss = 0.98 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:07:17.581774: step 31190, loss = 0.74 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:07:18.958243: step 31200, loss = 0.85 (929.9 examples/sec; 0.138 sec/batch)
2017-05-09 22:07:20.148364: step 31210, loss = 0.87 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-09 22:07:21.440143: step 31220, loss = 1.01 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:07:22.759920: step 31230, loss = 0.87 (969.9 examples/sec; 0.132 sec/batch)
2017-05-09 22:07:24.058947: step 31240, loss = 0.93 (985.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:07:25.360665: step 31250, loss = 0.78 (983.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:07:26.681251: step 31260, loss = 0.77 (969.3 examples/sec; 0.132 sec/batch)
2017-05-09 22:07:27.948500: step 31270, loss = 0.74 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:07:29.237641: step 31280, loss = 0.75 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:07:30.548976: step 31290, loss = 0.73 (976.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:07:31.912644: step 31300, loss = 0.92 (938.6 examples/sec; 0.136 sec/batch)
2017-05-09 22:07:33.114962: step 31310, loss = 0.86 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-09 22:07:34.422097: step 31320, loss = 0.85 (979.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:07:35.726853: step 31330, loss = 0.77 (981.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:07:37.033794: step 31340, loss = 0.93 (979.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:07:38.317310: step 31350, loss = 0.82 (997.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:07:39.593203: step 31360, loss = 0.75 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:07:40.887795: step 31370, loss = 0.75 (988.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:07:42.181481: step 31380, loss = 0.94 (989.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:07:43.478494: step 31390, loss = 0.92 (986.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:07:44.862193: step 31400, loss = 0.77 (925.1 examples/sec; 0.138 sec/batch)
2017-05-09 22:07:46.053782: step 31410, loss = 0.76 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-09 22:07:47.341107: step 31420, loss = 0.86 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:07:48.607314: step 31430, loss = 0.82 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:07:49.904765: step 31440, loss = 0.90 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:07:51.186210: step 31450, loss = 0.78 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:07:52.472024: step 31460, loss = 0.90 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:07:53.770813: step 31470, loss = 0.85 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:07:55.071696: step 31480, loss = 0.77 (983.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:07:56.359497: step 31490, loss = 0.90 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:07:57.738306: step 31500, loss = 0.99 (928.3 examples/sec; 0.138 sec/batch)
2017-05-09 22:07:58.934204: step 31510, loss = 0.83 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-09 22:08:00.202903: step 31520, loss = 0.85 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:08:01.481525: step 31530, loss = 0.87 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:08:02.754533: step 31540, loss = 0.85 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:08:04.015355: step 31550, loss = 0.65 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-09 22:08:05.297956: step 31560, loss = 0.84 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:08:06.589371: step 31570, loss = 0.74 (991.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:08:07.901351: step 31580, loss = 0.89 (975.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:08:09.204969: step 31590, loss = 0.91 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:08:10.598549: step 31600, loss = 0.82 (918.5 examples/sec; 0.139 sec/batch)
2017-05-09 22:08:11.815590: step 31610, loss = 0.90 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-09 22:08:13.072772: step 31620, loss = 0.72 (1018.2 examples/sec; 0.126 sec/batch)
2017-05-09 22:08:14.375141: step 31630, loss = 0.71 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:08:15.665844: step 31640, loss = 0.87 (991.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:08:16.955207: step 31650, loss = 0.79 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:08:18.234175: step 31660, loss = 0.86 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:08:19.520381: step 31670, loss = 0.79 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:08:20.802735: step 31680, loss = 0.95 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:08:22.093217: step 31690, loss = 0.93 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:08:23.483979: step 31700, loss = 0.83 (920.4 examples/sec; 0.139 sec/batch)
2017-05-09 22:08:24.658040: step 31710, loss = 0.81 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-09 22:08:25.954104: step 31720, loss = 0.73 (987.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:08:27.251522: step 31730, loss = 1.04 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:08:28.515393: step 31740, loss = 0.74 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-09 22:08:29.782529: step 31750, loss = 0.82 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:08:31.071773: step 31760, loss = 0.97 (992.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:08:32.360375: step 31770, loss = 0.78 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:08:33.651876: step 31780, loss = 0.91 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:08:34.940230: step 31790, loss = 0.91 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:08:36.299940: step 31800, loss = 0.79 (941.4 examples/sec; 0.136 sec/batch)
2017-05-09 22:08:37.478598: step 31810, loss = 0.93 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-09 22:08:38.771982: step 31820, loss = 0.79 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:08:40.071376: step 31830, loss = 0.95 (985.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:08:41.365425: step 31840, loss = 0.79 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:08:42.677024: step 31850, loss = 0.80 (975.9 examples/sec; 0.131 sec/batch)
2017-05-09 22:08:43.974687: step 31860, loss = 0.79 (986.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:08:45.288208: step 31870, loss = 0.92 (974.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:08:46.582113: step 31880, loss = 0.82 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:08:47.891861: step 31890, loss = 0.84 (977.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:08:49.254108: step 31900, loss = 0.80 (939.6 examples/sec; 0.136 sec/batch)
2017-05-09 22:08:50.456599: step 31910, loss = 0.79 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-09 22:08:51.763388: step 31920, loss = 0.90 (979.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:08:53.050803: step 31930, loss = 0.91 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:08:54.314578: step 31940, loss = 0.95 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-09 22:08:55.583289: step 31950, loss = 0.80 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:08:56.845107: step 31960, loss = 0.73 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-09 22:08:58.118621: step 31970, loss = 0.93 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:08:59.442300: step 31980, loss = 0.84 (967.0 examples/sec; 0.132 sec/batch)
2017-05-09 22:09:00.695253: step 31990, loss = 0.79 (1021.6 examples/sec; 0.125 sec/batch)
2017-05-09 22:09:02.089945: step 32000, loss = 0.73 (917.8 examples/sec; 0.139 sec/batch)
2017-05-09 22:09:03.262594: step 32010, loss = 0.91 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-09 22:09:04.560966: step 32020, loss = 0.69 (985.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:09:05.829746: step 32030, loss = 0.88 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:09:07.124146: step 32040, loss = 0.88 (988.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:09:08.404671: step 32050, loss = 0.92 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:09:09.701334: step 32060, loss = 0.76 (987.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:09:10.988074: step 32070, loss = 0.77 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:09:12.260068: step 32080, loss = 0.94 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:09:13.534189: step 32090, loss = 0.81 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:09:14.930123: step 32100, loss = 0.91 (916.9 examples/sec; 0.140 sec/batch)
2017-05-09 22:09:16.122702: step 32110, loss = 0.93 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-09 22:09:17.416939: step 32120, loss = 1.01 (989.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:09:18.722829: step 32130, loss = 0.75 (980.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:09:20.035968: step 32140, loss = 1.06 (974.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:09:21.286089: step 32150, loss = 0.93 (1023.9 examples/sec; 0.125 sec/batch)
2017-05-09 22:09:22.568131: step 32160, loss = 0.65 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:09:23.833719: step 32170, loss = 0.83 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:09:25.127058: step 32180, loss = 0.76 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:09:26.408228: step 32190, loss = 0.75 (999.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:09:27.786840: step 32200, loss = 0.80 (928.5 examples/sec; 0.138 sec/batch)
2017-05-09 22:09:28.956204: step 32210, loss = 0.83 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-09 22:09:30.241869: step 32220, loss = 0.65 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:09:31.507119: step 32230, loss = 0.77 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:09:32.802832: step 32240, loss = 0.74 (987.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:09:34.101543: step 32250, loss = 1.01 (985.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:09:35.391109: step 32260, loss = 0.85 (992.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:09:36.660355: step 32270, loss = 0.78 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:09:37.948549: step 32280, loss = 0.92 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:09:39.233485: step 32290, loss = 0.80 (996.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:09:40.577468: step 32300, loss = 0.83 (952.4 examples/sec; 0.134 sec/batch)
2017-05-09 22:09:41.766315: step 32310, loss = 0.75 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-09 22:09:43.062936: step 32320, loss = 0.75 (987.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:09:44.367369: step 32330, loss = 0.76 (981.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:09:45.649851: step 32340, loss = 0.80 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:09:46.956765: step 32350, loss = 0.75 (979.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:09:48.264721: step 32360, loss = 1.00 (978.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:09:49.538006: step 32370, loss = 0.84 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:09:50.818788: step 32380, loss = 0.86 (999.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:09:52.101962: step 32390, loss = 0.88 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:09:53.546204: step 32400, loss = 0.67 (886.3 examples/sec; 0.144 sec/batch)
2017-05-09 22:09:54.686943: step 32410, loss = 0.91 (1122.1 examples/sec; 0.114 sec/batch)
2017-05-09 22:09:55.978000: step 32420, loss = 0.80 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:09:57.260846: step 32430, loss = 0.78 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:09:58.528045: step 32440, loss = 0.83 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:09:59.810718: step 32450, loss = 0.76 (997.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:10:01.097256: step 32460, loss = 0.80 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:10:02.429807: step 32470, loss = 0.80 (960.6 examples/sec; 0.133 sec/batch)
2017-05-09 22:10:03.695281: step 32480, loss = 0.96 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:10:04.970219: step 32490, loss = 0.82 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:10:06.363434: step 32500, loss = 0.77 (918.7 examples/sec; 0.139 sec/batch)
2017-05-09 22:10:07.544391: step 32510, loss = 0.79 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-09 22:10:08.820397: step 32520, loss = 0.97 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:10:10.089624: step 32530, loss = 0.82 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:10:11.355292: step 32540, loss = 0.78 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:10:12.752406: step 32550, loss = 0.75 (916.2 examples/sec; 0.140 sec/batch)
2017-05-09 22:10:14.163932: step 32560, loss = 0.88 (906.8 examples/sec; 0.141 sec/batch)
2017-05-09 22:10:15.433207: step 32570, loss = 0.76 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:10:16.703977: step 32580, loss = 0.98 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:10:17.986144: step 32590, loss = 0.88 (998.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:10:19.362895: step 32600, loss = 0.72 (929.7 examples/sec; 0.138 sec/batch)
2017-05-09 22:10:20.555626: step 32610, loss = 0.83 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-09 22:10:21.834838: step 32620, loss = 0.85 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:10:23.115342: step 32630, loss = 0.78 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:10:24.399898: step 32640, loss = 0.84 (996.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:10:25.677331: step 32650, loss = 0.78 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:10:26.942882: step 32660, loss = 0.82 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:10:28.248037: step 32670, loss = 0.86 (980.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:10:29.548894: step 32680, loss = 0.76 (984.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:10:30.807656: step 32690, loss = 0.77 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-09 22:10:32.189783: step 32700, loss = 0.90 (926.1 examples/sec; 0.138 sec/batch)
2017-05-09 22:10:33.385121: step 32710, loss = 0.81 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-09 22:10:34.654599: step 32720, loss = 0.74 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:10:35.943910: step 32730, loss = 0.77 (992.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:10:37.230727: step 32740, loss = 0.73 (994.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:10:38.517939: step 32750, loss = 0.71 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:10:39.778994: step 32760, loss = 0.77 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-09 22:10:41.075559: step 32770, loss = 0.84 (987.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:10:42.362861: step 32780, loss = 0.72 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:10:43.658058: step 32790, loss = 0.73 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:10:45.054603: step 32800, loss = 0.86 (916.5 examples/sec; 0.140 sec/batch)
2017-05-09 22:10:46.232792: step 32810, loss = 0.80 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-09 22:10:47.526324: step 32820, loss = 0.78 (989.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:10:48.788931: step 32830, loss = 0.69 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-09 22:10:50.071758: step 32840, loss = 0.93 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:10:51.342859: step 32850, loss = 0.83 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:10:52.626858: step 32860, loss = 0.78 (996.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:10:53.923041: step 32870, loss = 0.75 (987.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:10:55.190633: step 32880, loss = 0.85 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:10:56.461357: step 32890, loss = 0.77 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:10:57.841907: step 32900, loss = 0.67 (927.2 examples/sec; 0.138 sec/batch)
2017-05-09 22:10:59.012638: step 32910, loss = 0.68 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-09 22:11:00.282391: step 32920, loss = 0.73 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:11:01.564520: step 32930, loss = 1.08 (998.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:11:02.847090: step 32940, loss = 0.73 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:11:04.135290: step 32950, loss = 0.65 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:11:05.431642: step 32960, loss = 0.92 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:11:06.745823: step 32970, loss = 1.06 (974.0 examples/sec; 0.131 sec/batch)
2017-05-09 22:11:08.039548: step 32980, loss = 0.69 (989.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:11:09.331972: step 32990, loss = 0.88 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:11:10.704691: step 33000, loss = 0.78 (932.5 examples/sec; 0.137 sec/batch)
2017-05-09 22:11:11.899761: step 33010, loss = 0.90 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-09 22:11:13.197385: step 33020, loss = 0.87 (986.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:11:14.477283: step 33030, loss = 0.71 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:11:15.789037: step 33040, loss = 0.80 (975.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:11:17.059712: step 33050, loss = 0.97 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:11:18.340318: step 33060, loss = 0.94 (999.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:11:19.617939: step 33070, loss = 0.68 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:11:20.902947: step 33080, loss = 0.69 (996.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:11:22.177338: step 33090, loss = 0.77 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:11:23.558602: step 33100, loss = 0.76 (926.7 examples/sec; 0.138 sec/batch)
2017-05-09 22:11:24.759861: step 33110, loss = 0.83 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-09 22:11:26.041674: step 33120, loss = 0.77 (998.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:11:27.321297: step 33130, loss = 0.74 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:11:28.597090: step 33140, loss = 0.73 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:11:29.866340: step 33150, loss = 0.86 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:11:31.161140: step 33160, loss = 0.75 (988.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:11:32.439324: step 33170, loss = 0.75 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:11:33.730123: step 33180, loss = 1.17 (991.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:11:35.028307: step 33190, loss = 1.55 (986.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:11:36.412120: step 33200, loss = 0.89 (925.0 examples/sec; 0.138 sec/batch)
2017-05-09 22:11:37.603678: step 33210, loss = 0.94 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-09 22:11:38.943100: step 33220, loss = 0.79 (955.6 examples/sec; 0.134 sec/batch)
2017-05-09 22:11:40.245825: step 33230, loss = 0.73 (982.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:11:41.552668: step 33240, loss = 0.66 (979.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:11:42.842337: step 33250, loss = 0.65 (992.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:11:44.122878: step 33260, loss = 0.76 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:11:45.400752: step 33270, loss = 0.76 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:11:46.678002: step 33280, loss = 0.85 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:11:47.968910: step 33290, loss = 0.97 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:11:49.340296: step 33300, loss = 0.74 (933.4 examples/sec; 0.137 sec/batch)
2017-05-09 22:11:50.509168: step 33310, loss = 0.69 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-09 22:11:51.819627: step 33320, loss = 0.69 (976.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:11:53.108200: step 33330, loss = 0.83 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:11:54.407175: step 33340, loss = 0.75 (985.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:11:55.708018: step 33350, loss = 0.84 (984.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:11:57.038357: step 33360, loss = 0.76 (962.2 examples/sec; 0.133 sec/batch)
2017-05-09 22:11:58.346597: step 33370, loss = 0.90 (978.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:11:59.626576: step 33380, loss = 0.90 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:12:00.907157: step 33390, loss = 0.94 (999.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:12:02.292090: step 33400, loss = 0.75 (924.2 examples/sec; 0.138 sec/batch)
2017-05-09 22:12:03.459299: step 33410, loss = 0.87 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-09 22:12:04.736568: step 33420, loss = 0.94 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:12:06.050157: step 33430, loss = 0.77 (974.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:12:07.328073: step 33440, loss = 0.88 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:12:08.599191: step 33450, loss = 0.85 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:12:09.884221: step 33460, loss = 0.99 (996.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:12:11.177284: step 33470, loss = 0.77 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:12:12.472938: step 33480, loss = 0.88 (987.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:12:13.757741: step 33490, loss = 0.70 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:12:15.128407: step 33500, loss = 0.83 (933.9 examples/sec; 0.137 sec/batch)
2017-05-09 22:12:16.311482: step 33510, loss = 0.66 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-09 22:12:17.611487: step 33520, loss = 0.93 (984.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:12:18.902687: step 33530, loss = 0.81 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:12:20.177585: step 33540, loss = 0.80 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:12:21.459253: step 33550, loss = 0.88 (998.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:12:22.744053: step 33560, loss = 0.82 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:12:24.031044: step 33570, loss = 0.76 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:12:25.299246: step 33580, loss = 0.78 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:12:26.589298: step 33590, loss = 0.82 (992.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:12:27.951141: step 33600, loss = 0.89 (939.9 examples/sec; 0.136 sec/batch)
2017-05-09 22:12:29.140278: step 33610, loss = 0.65 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-09 22:12:30.409928: step 33620, loss = 0.79 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:12:31.678854: step 33630, loss = 0.93 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:12:32.985756: step 33640, loss = 0.82 (979.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:12:34.260347: step 33650, loss = 0.82 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:12:35.565120: step 33660, loss = 0.88 (981.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:12:36.828220: step 33670, loss = 0.83 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-09 22:12:38.111986: step 33680, loss = 0.77 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:12:39.395567: step 33690, loss = 0.71 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:12:40.762790: step 33700, loss = 0.77 (936.2 examples/sec; 0.137 sec/batch)
2017-05-09 22:12:41.992192: step 33710, loss = 0.88 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-09 22:12:43.253041: step 33720, loss = 0.91 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-09 22:12:44.549292: step 33730, loss = 0.61 (987.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:12:45.836971: step 33740, loss = 0.74 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:12:47.133983: step 33750, loss = 0.97 (986.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:12:48.412066: step 33760, loss = 0.69 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:12:49.681181: step 33770, loss = 0.72 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:12:50.970581: step 33780, loss = 0.90 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:12:52.288772: step 33790, loss = 0.91 (971.0 examples/sec; 0.132 sec/batch)
2017-05-09 22:12:53.658002: step 33800, loss = 0.83 (934.8 examples/sec; 0.137 sec/batch)
2017-05-09 22:12:54.869225: step 33810, loss = 0.91 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-09 22:12:56.145129: step 33820, loss = 0.70 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:12:57.405158: step 33830, loss = 0.81 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-09 22:12:58.682212: step 33840, loss = 0.61 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:12:59.951667: step 33850, loss = 0.77 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:13:01.247077: step 33860, loss = 0.77 (988.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:13:02.511647: step 33870, loss = 0.65 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-09 22:13:03.790989: step 33880, loss = 0.79 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:13:05.070695: step 33890, loss = 0.68 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:13:06.433135: step 33900, loss = 0.61 (939.5 examples/sec; 0.136 sec/batch)
2017-05-09 22:13:07.635775: step 33910, loss = 0.61 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-09 22:13:08.901341: step 33920, loss = 0.99 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:13:10.201107: step 33930, loss = 0.83 (984.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:13:11.485514: step 33940, loss = 0.81 (996.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:13:12.800491: step 33950, loss = 0.75 (973.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:13:14.101973: step 33960, loss = 0.84 (983.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:13:15.404513: step 33970, loss = 0.97 (982.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:13:16.707940: step 33980, loss = 0.72 (982.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:13:18.014897: step 33990, loss = 0.80 (979.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:13:19.420793: step 34000, loss = 0.85 (910.4 examples/sec; 0.141 sec/batch)
2017-05-09 22:13:20.595791: step 34010, loss = 0.81 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-09 22:13:21.883548: step 34020, loss = 0.72 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:13:23.197173: step 34030, loss = 0.84 (974.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:13:24.453997: step 34040, loss = 0.98 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-09 22:13:25.730951: step 34050, loss = 0.85 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:13:27.002817: step 34060, loss = 0.76 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:13:28.273769: step 34070, loss = 0.89 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:13:29.550823: step 34080, loss = 0.79 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:13:30.827585: step 34090, loss = 0.85 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:13:32.217359: step 34100, loss = 0.65 (921.0 examples/sec; 0.139 sec/batch)
2017-05-09 22:13:33.408282: step 34110, loss = 0.83 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-09 22:13:34.708391: step 34120, loss = 0.87 (984.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:13:35.989090: step 34130, loss = 0.92 (999.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:13:37.283106: step 34140, loss = 0.73 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:13:38.563507: step 34150, loss = 0.67 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:13:39.822305: step 34160, loss = 0.90 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-09 22:13:41.125870: step 34170, loss = 0.86 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:13:42.390561: step 34180, loss = 0.86 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-09 22:13:43.671037: step 34190, loss = 0.67 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:13:45.064499: step 34200, loss = 0.80 (918.6 examples/sec; 0.139 sec/batch)
2017-05-09 22:13:46.267057: step 34210, loss = 0.81 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-09 22:13:47.540939: step 34220, loss = 0.84 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:13:48.825875: step 34230, loss = 1.04 (996.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:13:50.100845: step 34240, loss = 0.81 (1003.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:13:51.375976: step 34250, loss = 0.80 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:13:52.661280: step 34260, loss = 0.74 (995.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:13:53.947170: step 34270, loss = 0.80 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:13:55.233833: step 34280, loss = 1.07 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:13:56.546652: step 34290, loss = 0.81 (975.0 examples/sec; 0.131 sec/batch)
2017-05-09 22:13:57.910242: step 34300, loss = 0.90 (938.7 examples/sec; 0.136 sec/batch)
2017-05-09 22:13:59.105514: step 34310, loss = 0.96 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-09 22:14:00.388053: step 34320, loss = 0.76 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:14:01.660602: step 34330, loss = 0.59 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:14:02.945796: step 34340, loss = 0.82 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:14:04.217322: step 34350, loss = 0.83 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:14:05.488336: step 34360, loss = 0.85 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:14:06.754374: step 34370, loss = 0.70 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:14:08.038221: step 34380, loss = 0.95 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:14:09.310729: step 34390, loss = 0.75 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:14:10.692850: step 34400, loss = 0.93 (926.1 examples/sec; 0.138 sec/batch)
2017-05-09 22:14:11.905072: step 34410, loss = 0.74 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-09 22:14:13.188115: step 34420, loss = 0.79 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:14:14.483880: step 34430, loss = 0.96 (987.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:14:15.768498: step 34440, loss = 0.69 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:14:17.069565: step 34450, loss = 0.81 (983.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:14:18.352999: step 34460, loss = 0.89 (997.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:14:19.614353: step 34470, loss = 0.76 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-09 22:14:20.883751: step 34480, loss = 0.76 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:14:22.153696: step 34490, loss = 0.94 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:14:23.499363: step 34500, loss = 0.92 (951.2 examples/sec; 0.135 sec/batch)
2017-05-09 22:14:24.701512: step 34510, loss = 0.92 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-09 22:14:26.025882: step 34520, loss = 0.80 (966.5 examples/sec; 0.132 sec/batch)
2017-05-09 22:14:27.310925: step 34530, loss = 0.78 (996.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:14:28.585020: step 34540, loss = 0.84 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:14:29.837961: step 34550, loss = 0.83 (1021.6 examples/sec; 0.125 sec/batch)
2017-05-09 22:14:31.109389: step 34560, loss = 0.71 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:14:32.372489: step 34570, loss = 0.78 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-09 22:14:33.668433: step 34580, loss = 0.85 (987.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:14:34.964890: step 34590, loss = 0.84 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:14:36.370149: step 34600, loss = 0.85 (910.9 examples/sec; 0.141 sec/batch)
2017-05-09 22:14:37.557924: step 34610, loss = 0.80 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-09 22:14:38.831805: step 34620, loss = 0.98 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:14:40.112651: step 34630, loss = 0.78 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:14:41.384074: step 34640, loss = 0.77 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:14:42.671432: step 34650, loss = 1.02 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:14:43.937072: step 34660, loss = 0.97 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:14:45.211095: step 34670, loss = 0.90 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:14:46.482362: step 34680, loss = 0.83 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:14:47.758434: step 34690, loss = 0.99 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:14:49.154343: step 34700, loss = 0.85 (917.0 examples/sec; 0.140 sec/batch)
2017-05-09 22:14:50.360329: step 34710, loss = 0.99 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-09 22:14:51.651325: step 34720, loss = 0.90 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:14:52.912482: step 34730, loss = 0.83 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-09 22:14:54.198355: step 34740, loss = 0.93 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:14:55.481138: step 34750, loss = 0.73 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:14:56.774231: step 34760, loss = 0.70 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:14:58.057870: step 34770, loss = 0.78 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:14:59.338677: step 34780, loss = 0.69 (999.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:15:00.604741: step 34790, loss = 1.05 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:15:02.018667: step 34800, loss = 0.94 (905.3 examples/sec; 0.141 sec/batch)
2017-05-09 22:15:03.218498: step 34810, loss = 0.93 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-09 22:15:04.504754: step 34820, loss = 0.66 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:15:05.783623: step 34830, loss = 0.85 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:15:07.044567: step 34840, loss = 0.79 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-09 22:15:08.341408: step 34850, loss = 0.76 (987.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:15:09.621236: step 34860, loss = 0.81 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:15:10.911612: step 34870, loss = 0.80 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:15:12.199535: step 34880, loss = 0.95 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:15:13.471118: step 34890, loss = 0.66 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:15:14.816829: step 34900, loss = 0.83 (951.2 examples/sec; 0.135 sec/batch)
2017-05-09 22:15:16.025270: step 34910, loss = 0.80 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-09 22:15:17.296552: step 34920, loss = 0.73 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:15:18.575634: step 34930, loss = 0.82 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:15:19.832886: step 34940, loss = 0.85 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-09 22:15:21.137317: step 34950, loss = 0.62 (981.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:15:22.413685: step 34960, loss = 0.83 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:15:23.695909: step 34970, loss = 0.77 (998.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:15:24.975346: step 34980, loss = 0.77 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:15:26.253022: step 34990, loss = 0.74 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:15:27.635534: step 35000, loss = 0.72 (925.9 examples/sec; 0.138 sec/batch)
2017-05-09 22:15:28.837922: step 35010, loss = 0.71 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-09 22:15:30.118615: step 35020, loss = 0.79 (999.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:15:31.400418: step 35030, loss = 0.76 (998.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:15:32.688141: step 35040, loss = 0.87 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:15:33.973654: step 35050, loss = 0.94 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:15:35.260757: step 35060, loss = 0.70 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:15:36.551876: step 35070, loss = 0.95 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:15:37.823251: step 35080, loss = 0.93 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:15:39.133913: step 35090, loss = 0.87 (976.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:15:40.540257: step 35100, loss = 0.70 (910.2 examples/sec; 0.141 sec/batch)
2017-05-09 22:15:41.741408: step 35110, loss = 0.78 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-09 22:15:43.043789: step 35120, loss = 0.83 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:15:44.313310: step 35130, loss = 0.79 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:15:45.577923: step 35140, loss = 0.69 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-09 22:15:46.839382: step 35150, loss = 0.69 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-09 22:15:48.104546: step 35160, loss = 0.85 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:15:49.366268: step 35170, loss = 0.72 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-09 22:15:50.682968: step 35180, loss = 0.68 (972.1 examples/sec; 0.132 sec/batch)
2017-05-09 22:15:51.974386: step 35190, loss = 0.86 (991.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:15:53.349071: step 35200, loss = 0.69 (931.1 examples/sec; 0.137 sec/batch)
2017-05-09 22:15:54.553829: step 35210, loss = 0.83 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-09 22:15:55.868857: step 35220, loss = 0.78 (973.4 examples/sec; 0.132 sec/batch)
2017-05-09 22:15:57.136664: step 35230, loss = 0.86 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:15:58.417010: step 35240, loss = 0.89 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:15:59.681364: step 35250, loss = 0.84 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-09 22:16:00.982315: step 35260, loss = 0.99 (983.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:16:02.266988: step 35270, loss = 0.86 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:16:03.540891: step 35280, loss = 0.79 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:16:04.815512: step 35290, loss = 0.78 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:16:06.203530: step 35300, loss = 0.78 (922.2 examples/sec; 0.139 sec/batch)
2017-05-09 22:16:07.410749: step 35310, loss = 0.83 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-09 22:16:08.669748: step 35320, loss = 0.83 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-09 22:16:09.962724: step 35330, loss = 0.76 (990.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:16:11.248171: step 35340, loss = 0.84 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:16:12.526650: step 35350, loss = 0.75 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:16:13.812867: step 35360, loss = 0.84 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:16:15.104120: step 35370, loss = 0.59 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:16:16.389321: step 35380, loss = 0.80 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:16:17.674071: step 35390, loss = 0.67 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:16:19.043214: step 35400, loss = 0.78 (934.9 examples/sec; 0.137 sec/batch)
2017-05-09 22:16:20.222231: step 35410, loss = 0.81 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-09 22:16:21.485339: step 35420, loss = 0.75 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-09 22:16:22.774356: step 35430, loss = 1.02 (993.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:16:24.059369: step 35440, loss = 0.77 (996.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:16:25.335266: step 35450, loss = 0.88 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:16:26.621058: step 35460, loss = 0.72 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:16:27.883840: step 35470, loss = 0.76 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-09 22:16:29.206383: step 35480, loss = 0.88 (967.8 examples/sec; 0.132 sec/batch)
2017-05-09 22:16:30.543244: step 35490, loss = 0.74 (957.5 examples/sec; 0.134 sec/batch)
2017-05-09 22:16:31.925076: step 35500, loss = 0.81 (926.3 examples/sec; 0.138 sec/batch)
2017-05-09 22:16:33.113488: step 35510, loss = 0.73 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-09 22:16:34.398745: step 35520, loss = 0.79 (995.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:16:35.668628: step 35530, loss = 0.78 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:16:36.933870: step 35540, loss = 0.94 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:16:38.231634: step 35550, loss = 0.76 (986.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:16:39.486175: step 35560, loss = 0.70 (1020.3 examples/sec; 0.125 sec/batch)
2017-05-09 22:16:40.780293: step 35570, loss = 0.85 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:16:42.097128: step 35580, loss = 0.89 (972.0 examples/sec; 0.132 sec/batch)
2017-05-09 22:16:43.394985: step 35590, loss = 0.76 (986.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:16:44.772834: step 35600, loss = 0.90 (929.0 examples/sec; 0.138 sec/batch)
2017-05-09 22:16:45.962090: step 35610, loss = 0.65 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-09 22:16:47.281765: step 35620, loss = 0.86 (969.9 examples/sec; 0.132 sec/batch)
2017-05-09 22:16:48.597099: step 35630, loss = 0.83 (973.1 examples/sec; 0.132 sec/batch)
2017-05-09 22:16:49.888611: step 35640, loss = 0.82 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:16:51.215663: step 35650, loss = 0.86 (964.5 examples/sec; 0.133 sec/batch)
2017-05-09 22:16:52.497679: step 35660, loss = 0.68 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:16:53.782915: step 35670, loss = 0.84 (995.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:16:55.062157: step 35680, loss = 0.70 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:16:56.338873: step 35690, loss = 0.72 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:16:57.721568: step 35700, loss = 0.76 (925.7 examples/sec; 0.138 sec/batch)
2017-05-09 22:16:58.897407: step 35710, loss = 0.82 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-09 22:17:00.179734: step 35720, loss = 0.92 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:17:01.464566: step 35730, loss = 0.78 (996.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:17:02.751016: step 35740, loss = 0.70 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:17:04.036479: step 35750, loss = 0.83 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:17:05.316826: step 35760, loss = 0.75 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:17:06.606807: step 35770, loss = 0.98 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:17:07.886104: step 35780, loss = 0.96 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:17:09.164434: step 35790, loss = 0.65 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:17:10.529517: step 35800, loss = 0.99 (937.7 examples/sec; 0.137 sec/batch)
2017-05-09 22:17:11.702438: step 35810, loss = 0.79 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-09 22:17:12.957103: step 35820, loss = 0.87 (1020.2 examples/sec; 0.125 sec/batch)
2017-05-09 22:17:14.260015: step 35830, loss = 0.81 (982.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:17:15.535781: step 35840, loss = 0.89 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:17:16.823658: step 35850, loss = 0.86 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:17:18.082617: step 35860, loss = 0.78 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-09 22:17:19.349224: step 35870, loss = 0.81 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:17:20.614715: step 35880, loss = 0.78 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:17:21.902896: step 35890, loss = 0.89 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:17:23.300214: step 35900, loss = 0.73 (916.0 examples/sec; 0.140 sec/batch)
2017-05-09 22:17:24.489160: step 35910, loss = 0.74 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-09 22:17:25.787908: step 35920, loss = 0.71 (985.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:17:27.092951: step 35930, loss = 0.87 (980.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:17:28.349551: step 35940, loss = 0.82 (1018.6 examples/sec; 0.126 sec/batch)
2017-05-09 22:17:29.623033: step 35950, loss = 0.91 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:17:30.908390: step 35960, loss = 0.69 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:17:32.177948: step 35970, loss = 0.92 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:17:33.469732: step 35980, loss = 0.85 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:17:34.738604: step 35990, loss = 0.92 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:17:36.131058: step 36000, loss = 0.88 (919.2 examples/sec; 0.139 sec/batch)
2017-05-09 22:17:37.314997: step 36010, loss = 0.73 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-09 22:17:38.595871: step 36020, loss = 0.84 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:17:39.889048: step 36030, loss = 0.86 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:17:41.178852: step 36040, loss = 0.79 (992.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:17:42.479894: step 36050, loss = 0.71 (983.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:17:43.758527: step 36060, loss = 0.78 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:17:45.063020: step 36070, loss = 0.81 (981.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:17:46.352867: step 36080, loss = 0.76 (992.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:17:47.624737: step 36090, loss = 0.89 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:17:49.041617: step 36100, loss = 0.66 (903.4 examples/sec; 0.142 sec/batch)
2017-05-09 22:17:50.244100: step 36110, loss = 0.76 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-09 22:17:51.516258: step 36120, loss = 1.00 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:17:52.801511: step 36130, loss = 0.71 (995.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:17:54.111965: step 36140, loss = 0.77 (976.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:17:55.388930: step 36150, loss = 0.76 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:17:56.681748: step 36160, loss = 0.76 (990.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:17:57.979495: step 36170, loss = 0.86 (986.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:17:59.277663: step 36180, loss = 1.01 (986.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:18:00.558181: step 36190, loss = 0.69 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:18:01.926996: step 36200, loss = 0.71 (935.1 examples/sec; 0.137 sec/batch)
2017-05-09 22:18:03.132131: step 36210, loss = 0.86 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-09 22:18:04.392659: step 36220, loss = 0.65 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-09 22:18:05.678786: step 36230, loss = 0.69 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:18:06.982614: step 36240, loss = 0.68 (981.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:18:08.272115: step 36250, loss = 0.93 (992.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:18:09.548552: step 36260, loss = 0.77 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:18:10.828328: step 36270, loss = 1.04 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:18:12.113642: step 36280, loss = 0.79 (995.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:18:13.414352: step 36290, loss = 0.67 (984.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:18:14.802515: step 36300, loss = 0.87 (922.1 examples/sec; 0.139 sec/batch)
2017-05-09 22:18:15.987695: step 36310, loss = 0.67 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-09 22:18:17.281535: step 36320, loss = 0.86 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:18:18.558027: step 36330, loss = 0.94 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:18:19.824140: step 36340, loss = 0.78 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:18:21.111685: step 36350, loss = 0.76 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:18:22.385799: step 36360, loss = 0.77 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:18:23.662630: step 36370, loss = 0.78 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:18:24.932308: step 36380, loss = 0.77 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:18:26.221091: step 36390, loss = 0.73 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:18:27.584156: step 36400, loss = 0.72 (939.1 examples/sec; 0.136 sec/batch)
2017-05-09 22:18:28.775782: step 36410, loss = 0.61 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-09 22:18:30.077788: step 36420, loss = 0.87 (983.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:18:31.335522: step 36430, loss = 0.88 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-09 22:18:32.614675: step 36440, loss = 0.62 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:18:33.908132: step 36450, loss = 0.66 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:18:35.161657: step 36460, loss = 0.69 (1021.1 examples/sec; 0.125 sec/batch)
2017-05-09 22:18:36.455612: step 36470, loss = 0.85 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:18:37.724797: step 36480, loss = 0.76 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:18:39.006741: step 36490, loss = 0.74 (998.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:18:40.363835: step 36500, loss = 0.91 (943.2 examples/sec; 0.136 sec/batch)
2017-05-09 22:18:41.564077: step 36510, loss = 0.90 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-09 22:18:42.885038: step 36520, loss = 0.84 (969.0 examples/sec; 0.132 sec/batch)
2017-05-09 22:18:44.170382: step 36530, loss = 0.94 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:18:45.479215: step 36540, loss = 0.69 (978.0 examples/sec; 0.131 sec/batch)
2017-05-09 22:18:46.758439: step 36550, loss = 0.69 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:18:48.031227: step 36560, loss = 0.72 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:18:49.323833: step 36570, loss = 1.05 (990.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:18:50.601273: step 36580, loss = 0.80 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:18:51.879676: step 36590, loss = 0.82 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:18:53.274360: step 36600, loss = 0.83 (917.8 examples/sec; 0.139 sec/batch)
2017-05-09 22:18:54.453303: step 36610, loss = 0.96 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-09 22:18:55.723490: step 36620, loss = 0.61 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:18:57.022515: step 36630, loss = 0.78 (985.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:18:58.296371: step 36640, loss = 0.74 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:18:59.582630: step 36650, loss = 0.64 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:19:00.856633: step 36660, loss = 1.21 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:19:02.122372: step 36670, loss = 0.75 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:19:03.390394: step 36680, loss = 0.78 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:19:04.650192: step 36690, loss = 0.66 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-09 22:19:06.024539: step 36700, loss = 0.74 (931.4 examples/sec; 0.137 sec/batch)
2017-05-09 22:19:07.208742: step 36710, loss = 0.77 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-09 22:19:08.486491: step 36720, loss = 0.81 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:19:09.798977: step 36730, loss = 0.94 (975.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:19:11.093716: step 36740, loss = 0.85 (988.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:19:12.373633: step 36750, loss = 0.96 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:19:13.648658: step 36760, loss = 0.82 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:19:14.930860: step 36770, loss = 0.63 (998.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:19:16.189357: step 36780, loss = 0.73 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-09 22:19:17.448840: step 36790, loss = 0.78 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-09 22:19:18.801387: step 36800, loss = 0.69 (946.4 examples/sec; 0.135 sec/batch)
2017-05-09 22:19:19.991255: step 36810, loss = 0.89 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-09 22:19:21.281133: step 36820, loss = 0.96 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:19:22.582797: step 36830, loss = 0.77 (983.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:19:23.884105: step 36840, loss = 0.84 (983.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:19:25.169035: step 36850, loss = 0.67 (996.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:19:26.459613: step 36860, loss = 0.84 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:19:27.725757: step 36870, loss = 0.95 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:19:28.998150: step 36880, loss = 0.99 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:19:30.283055: step 36890, loss = 0.82 (996.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:19:31.656189: step 36900, loss = 0.78 (932.2 examples/sec; 0.137 sec/batch)
2017-05-09 22:19:32.886935: step 36910, loss = 0.78 (1040.0 examples/sec; 0.123 sec/batch)
2017-05-09 22:19:34.148802: step 36920, loss = 0.78 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-09 22:19:35.420295: step 36930, loss = 0.80 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:19:36.707014: step 36940, loss = 0.92 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:19:37.987673: step 36950, loss = 0.70 (999.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:19:39.276047: step 36960, loss = 0.72 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:19:40.562164: step 36970, loss = 0.91 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:19:41.838807: step 36980, loss = 0.96 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:19:43.127183: step 36990, loss = 0.79 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:19:44.486504: step 37000, loss = 0.68 (941.6 examples/sec; 0.136 sec/batch)
2017-05-09 22:19:45.699292: step 37010, loss = 0.71 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-09 22:19:46.980380: step 37020, loss = 0.81 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:19:48.323438: step 37030, loss = 0.96 (953.0 examples/sec; 0.134 sec/batch)
2017-05-09 22:19:49.611620: step 37040, loss = 0.77 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:19:50.886381: step 37050, loss = 0.78 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:19:52.186421: step 37060, loss = 1.16 (984.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:19:53.493314: step 37070, loss = 0.70 (979.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:19:54.758634: step 37080, loss = 0.97 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:19:56.024609: step 37090, loss = 0.96 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:19:57.400934: step 37100, loss = 0.82 (930.0 examples/sec; 0.138 sec/batch)
2017-05-09 22:19:58.590243: step 37110, loss = 0.86 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-09 22:19:59.877402: step 37120, loss = 0.89 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:20:01.166753: step 37130, loss = 0.67 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:20:02.448059: step 37140, loss = 0.88 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:20:03.727643: step 37150, loss = 1.03 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:20:05.008680: step 37160, loss = 0.73 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:20:06.276202: step 37170, loss = 0.85 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:20:07.556417: step 37180, loss = 0.76 (999.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:20:08.827750: step 37190, loss = 0.81 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:20:10.239971: step 37200, loss = 0.76 (906.4 examples/sec; 0.141 sec/batch)
2017-05-09 22:20:11.419647: step 37210, loss = 0.84 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-09 22:20:12.691942: step 37220, loss = 0.80 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:20:13.980654: step 37230, loss = 0.77 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:20:15.264002: step 37240, loss = 0.80 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:20:16.532553: step 37250, loss = 0.68 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:20:17.808812: step 37260, loss = 0.83 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:20:19.091685: step 37270, loss = 0.86 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:20:20.362872: step 37280, loss = 0.79 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:20:21.640854: step 37290, loss = 0.84 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:20:22.998237: step 37300, loss = 0.93 (943.0 examples/sec; 0.136 sec/batch)
2017-05-09 22:20:24.192706: step 37310, loss = 0.94 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-09 22:20:25.456359: step 37320, loss = 0.81 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-09 22:20:26.730794: step 37330, loss = 0.72 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:20:28.031295: step 37340, loss = 0.89 (984.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:20:29.321582: step 37350, loss = 0.72 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:20:30.599128: step 37360, loss = 0.75 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:20:31.866334: step 37370, loss = 0.83 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:20:33.141972: step 37380, loss = 0.82 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:20:34.412992: step 37390, loss = 0.85 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:20:35.787212: step 37400, loss = 0.70 (931.4 examples/sec; 0.137 sec/batch)
2017-05-09 22:20:36.953707: step 37410, loss = 0.93 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-09 22:20:38.219293: step 37420, loss = 0.98 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:20:39.503745: step 37430, loss = 0.94 (996.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:20:40.755679: step 37440, loss = 0.74 (1022.4 examples/sec; 0.125 sec/batch)
2017-05-09 22:20:42.039129: step 37450, loss = 0.70 (997.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:20:43.308670: step 37460, loss = 0.77 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:20:44.606175: step 37470, loss = 0.87 (986.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:20:45.878386: step 37480, loss = 0.62 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:20:47.132487: step 37490, loss = 0.73 (1020.7 examples/sec; 0.125 sec/batch)
2017-05-09 22:20:48.490594: step 37500, loss = 0.90 (942.5 examples/sec; 0.136 sec/batch)
2017-05-09 22:20:49.701924: step 37510, loss = 0.73 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-09 22:20:50.991693: step 37520, loss = 0.80 (992.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:20:52.257293: step 37530, loss = 0.74 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:20:53.537040: step 37540, loss = 0.91 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:20:54.814421: step 37550, loss = 0.67 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:20:56.087089: step 37560, loss = 0.78 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:20:57.375629: step 37570, loss = 0.80 (993.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:20:58.666242: step 37580, loss = 0.77 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:20:59.930178: step 37590, loss = 0.67 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-09 22:21:01.293063: step 37600, loss = 0.90 (939.2 examples/sec; 0.136 sec/batch)
2017-05-09 22:21:02.507959: step 37610, loss = 0.90 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-09 22:21:03.808782: step 37620, loss = 0.65 (984.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:21:05.087892: step 37630, loss = 0.84 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:21:06.394311: step 37640, loss = 0.68 (979.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:21:07.669329: step 37650, loss = 0.87 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:21:08.930766: step 37660, loss = 0.77 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-09 22:21:10.234905: step 37670, loss = 0.75 (981.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:21:11.505580: step 37680, loss = 0.67 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:21:12.774228: step 37690, loss = 0.68 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:21:14.154321: step 37700, loss = 1.02 (927.5 examples/sec; 0.138 sec/batch)
2017-05-09 22:21:15.338628: step 37710, loss = 0.79 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-09 22:21:16.603601: step 37720, loss = 0.65 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-09 22:21:17.883110: step 37730, loss = 0.72 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:21:19.153193: step 37740, loss = 0.83 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:21:20.425893: step 37750, loss = 0.69 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:21:21.718049: step 37760, loss = 0.87 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:21:22.986751: step 37770, loss = 0.73 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:21:24.255309: step 37780, loss = 0.93 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:21:25.531689: step 37790, loss = 0.74 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:21:26.902521: step 37800, loss = 0.94 (933.7 examples/sec; 0.137 sec/batch)
2017-05-09 22:21:28.089516: step 37810, loss = 0.94 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-09 22:21:29.367400: step 37820, loss = 0.77 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:21:30.650649: step 37830, loss = 0.91 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:21:31.928537: step 37840, loss = 0.87 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:21:33.193668: step 37850, loss = 0.83 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:21:34.454028: step 37860, loss = 0.92 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-09 22:21:35.756768: step 37870, loss = 0.83 (982.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:21:37.053714: step 37880, loss = 1.20 (986.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:21:38.307369: step 37890, loss = 0.89 (1021.0 examples/sec; 0.125 sec/batch)
2017-05-09 22:21:39.656838: step 37900, loss = 0.84 (948.5 examples/sec; 0.135 sec/batch)
2017-05-09 22:21:40.868940: step 37910, loss = 0.90 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-09 22:21:42.166560: step 37920, loss = 0.79 (986.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:21:43.440097: step 37930, loss = 0.92 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:21:44.714587: step 37940, loss = 0.74 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:21:46.005779: step 37950, loss = 0.91 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:21:47.288576: step 37960, loss = 0.68 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:21:48.558170: step 37970, loss = 0.55 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:21:49.839932: step 37980, loss = 0.88 (998.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:21:51.115172: step 37990, loss = 0.72 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:21:52.488893: step 38000, loss = 0.69 (931.8 examples/sec; 0.137 sec/batch)
2017-05-09 22:21:53.667884: step 38010, loss = 0.70 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-09 22:21:54.978733: step 38020, loss = 0.83 (976.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:21:56.270185: step 38030, loss = 0.94 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:21:57.572028: step 38040, loss = 0.87 (983.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:21:58.872664: step 38050, loss = 0.75 (984.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:22:00.159012: step 38060, loss = 0.72 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:22:01.432610: step 38070, loss = 0.72 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:22:02.732542: step 38080, loss = 0.69 (984.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:22:03.999882: step 38090, loss = 0.68 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:22:05.381184: step 38100, loss = 0.77 (926.7 examples/sec; 0.138 sec/batch)
2017-05-09 22:22:06.585588: step 38110, loss = 0.75 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-09 22:22:07.866205: step 38120, loss = 0.81 (999.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:22:09.138343: step 38130, loss = 0.84 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:22:10.421567: step 38140, loss = 0.87 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:22:11.699075: step 38150, loss = 0.86 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:22:12.983604: step 38160, loss = 0.98 (996.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:22:14.286408: step 38170, loss = 0.70 (982.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:22:15.561464: step 38180, loss = 1.00 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:22:16.839174: step 38190, loss = 0.92 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:22:18.257968: step 38200, loss = 0.80 (902.2 examples/sec; 0.142 sec/batch)
2017-05-09 22:22:19.479175: step 38210, loss = 0.86 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-09 22:22:20.807962: step 38220, loss = 1.25 (963.3 examples/sec; 0.133 sec/batch)
2017-05-09 22:22:22.111149: step 38230, loss = 0.87 (982.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:22:23.379572: step 38240, loss = 0.74 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:22:24.663840: step 38250, loss = 0.76 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:22:25.965120: step 38260, loss = 0.78 (983.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:22:27.229764: step 38270, loss = 0.82 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-09 22:22:28.543517: step 38280, loss = 0.68 (974.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:22:29.826630: step 38290, loss = 0.74 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:22:31.198678: step 38300, loss = 0.72 (932.9 examples/sec; 0.137 sec/batch)
2017-05-09 22:22:32.383178: step 38310, loss = 0.85 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-09 22:22:33.645708: step 38320, loss = 0.87 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-09 22:22:34.945743: step 38330, loss = 0.77 (984.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:22:36.244916: step 38340, loss = 0.72 (985.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:22:37.540188: step 38350, loss = 0.76 (988.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:22:38.816564: step 38360, loss = 0.85 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:22:40.098164: step 38370, loss = 0.87 (998.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:22:41.381277: step 38380, loss = 0.69 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:22:42.641940: step 38390, loss = 0.71 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-09 22:22:44.024112: step 38400, loss = 0.72 (926.1 examples/sec; 0.138 sec/batch)
2017-05-09 22:22:45.236219: step 38410, loss = 0.91 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-09 22:22:46.544036: step 38420, loss = 0.78 (978.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:22:47.821807: step 38430, loss = 0.90 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:22:49.143657: step 38440, loss = 0.80 (968.3 examples/sec; 0.132 sec/batch)
2017-05-09 22:22:50.446428: step 38450, loss = 0.80 (982.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:22:51.734604: step 38460, loss = 0.75 (993.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:22:53.005417: step 38470, loss = 0.75 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:22:54.276601: step 38480, loss = 0.77 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:22:55.573690: step 38490, loss = 0.83 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:22:56.932818: step 38500, loss = 0.77 (941.8 examples/sec; 0.136 sec/batch)
2017-05-09 22:22:58.127194: step 38510, loss = 0.78 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-09 22:22:59.419014: step 38520, loss = 0.92 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:23:00.727432: step 38530, loss = 0.62 (978.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:23:02.038363: step 38540, loss = 0.77 (976.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:23:03.311771: step 38550, loss = 0.72 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:23:04.605754: step 38560, loss = 1.01 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:23:05.876943: step 38570, loss = 0.84 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:23:07.147032: step 38580, loss = 0.85 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:23:08.400109: step 38590, loss = 0.61 (1021.5 examples/sec; 0.125 sec/batch)
2017-05-09 22:23:09.777905: step 38600, loss = 0.86 (929.0 examples/sec; 0.138 sec/batch)
2017-05-09 22:23:11.010800: step 38610, loss = 0.92 (1038.2 examples/sec; 0.123 sec/batch)
2017-05-09 22:23:12.309040: step 38620, loss = 0.78 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:23:13.631109: step 38630, loss = 0.67 (968.2 examples/sec; 0.132 sec/batch)
2017-05-09 22:23:14.899859: step 38640, loss = 0.72 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:23:16.217622: step 38650, loss = 0.97 (971.3 examples/sec; 0.132 sec/batch)
2017-05-09 22:23:17.487207: step 38660, loss = 0.77 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:23:18.800295: step 38670, loss = 0.66 (974.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:23:20.131908: step 38680, loss = 1.12 (961.2 examples/sec; 0.133 sec/batch)
2017-05-09 22:23:21.433914: step 38690, loss = 0.76 (983.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:23:22.821949: step 38700, loss = 0.88 (922.2 examples/sec; 0.139 sec/batch)
2017-05-09 22:23:24.001241: step 38710, loss = 0.68 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-09 22:23:25.313601: step 38720, loss = 0.88 (975.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:23:26.591037: step 38730, loss = 0.66 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:23:27.883343: step 38740, loss = 0.72 (990.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:23:29.164867: step 38750, loss = 0.73 (998.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:23:30.436898: step 38760, loss = 0.82 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:23:31.727684: step 38770, loss = 0.72 (991.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:23:33.118951: step 38780, loss = 0.83 (920.0 examples/sec; 0.139 sec/batch)
2017-05-09 22:23:34.409907: step 38790, loss = 0.94 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:23:35.792500: step 38800, loss = 0.74 (925.8 examples/sec; 0.138 sec/batch)
2017-05-09 22:23:36.960936: step 38810, loss = 0.84 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-09 22:23:38.258270: step 38820, loss = 0.84 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:23:39.533211: step 38830, loss = 0.88 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:23:40.818801: step 38840, loss = 0.90 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:23:42.115787: step 38850, loss = 0.68 (986.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:23:43.387905: step 38860, loss = 0.77 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:23:44.675679: step 38870, loss = 0.74 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:23:45.954310: step 38880, loss = 0.68 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:23:47.239732: step 38890, loss = 0.65 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:23:48.624332: step 38900, loss = 0.65 (924.4 examples/sec; 0.138 sec/batch)
2017-05-09 22:23:49.840407: step 38910, loss = 1.21 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-09 22:23:51.122099: step 38920, loss = 0.82 (998.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:23:52.413586: step 38930, loss = 0.88 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:23:53.689811: step 38940, loss = 0.75 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:23:54.962678: step 38950, loss = 0.74 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:23:56.233322: step 38960, loss = 0.83 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:23:57.514337: step 38970, loss = 0.78 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:23:58.826258: step 38980, loss = 0.83 (975.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:24:00.128602: step 38990, loss = 0.73 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:24:01.484529: step 39000, loss = 0.87 (944.0 examples/sec; 0.136 sec/batch)
2017-05-09 22:24:02.694159: step 39010, loss = 0.92 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-09 22:24:03.968194: step 39020, loss = 0.71 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:24:05.249003: step 39030, loss = 0.69 (999.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:24:06.526271: step 39040, loss = 0.70 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:24:07.795257: step 39050, loss = 0.68 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:24:09.074397: step 39060, loss = 0.86 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:24:10.354742: step 39070, loss = 1.36 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:24:11.676118: step 39080, loss = 0.83 (968.7 examples/sec; 0.132 sec/batch)
2017-05-09 22:24:12.979835: step 39090, loss = 0.95 (981.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:24:14.363486: step 39100, loss = 0.84 (925.1 examples/sec; 0.138 sec/batch)
2017-05-09 22:24:15.546402: step 39110, loss = 0.69 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-09 22:24:16.837430: step 39120, loss = 0.73 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:24:18.101644: step 39130, loss = 0.83 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-09 22:24:19.395417: step 39140, loss = 0.65 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:24:20.665266: step 39150, loss = 0.76 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:24:21.946680: step 39160, loss = 0.93 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:24:23.233538: step 39170, loss = 0.92 (994.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:24:24.517715: step 39180, loss = 0.90 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:24:25.814736: step 39190, loss = 0.80 (986.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:24:27.204591: step 39200, loss = 0.73 (921.0 examples/sec; 0.139 sec/batch)
2017-05-09 22:24:28.400744: step 39210, loss = 0.62 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-09 22:24:29.710659: step 39220, loss = 0.74 (977.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:24:31.001588: step 39230, loss = 0.85 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:24:32.269983: step 39240, loss = 0.82 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:24:33.579745: step 39250, loss = 0.78 (977.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:24:34.864148: step 39260, loss = 0.67 (996.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:24:36.155605: step 39270, loss = 0.76 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:24:37.464076: step 39280, loss = 0.93 (978.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:24:38.782169: step 39290, loss = 0.79 (971.1 examples/sec; 0.132 sec/batch)
2017-05-09 22:24:40.149935: step 39300, loss = 0.86 (935.8 examples/sec; 0.137 sec/batch)
2017-05-09 22:24:41.339204: step 39310, loss = 0.72 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-09 22:24:42.593377: step 39320, loss = 0.80 (1020.6 examples/sec; 0.125 sec/batch)
2017-05-09 22:24:43.904196: step 39330, loss = 0.94 (976.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:24:45.196550: step 39340, loss = 0.80 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:24:46.498032: step 39350, loss = 0.79 (983.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:24:47.801788: step 39360, loss = 0.74 (981.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:24:49.097731: step 39370, loss = 0.73 (987.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:24:50.384248: step 39380, loss = 0.72 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:24:51.659191: step 39390, loss = 0.80 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:24:53.045972: step 39400, loss = 0.76 (923.0 examples/sec; 0.139 sec/batch)
2017-05-09 22:24:54.217425: step 39410, loss = 0.79 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-09 22:24:55.487192: step 39420, loss = 0.80 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:24:56.797990: step 39430, loss = 0.93 (976.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:24:58.094703: step 39440, loss = 0.88 (987.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:24:59.396596: step 39450, loss = 0.79 (983.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:25:00.664845: step 39460, loss = 0.81 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:25:01.944406: step 39470, loss = 0.83 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:25:03.270254: step 39480, loss = 0.84 (965.4 examples/sec; 0.133 sec/batch)
2017-05-09 22:25:04.550172: step 39490, loss = 0.87 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:25:05.946596: step 39500, loss = 0.87 (916.6 examples/sec; 0.140 sec/batch)
2017-05-09 22:25:07.143199: step 39510, loss = 0.79 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-09 22:25:08.417721: step 39520, loss = 0.74 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:25:09.696166: step 39530, loss = 0.84 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:25:10.970645: step 39540, loss = 0.82 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:25:12.283386: step 39550, loss = 0.76 (975.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:25:13.556966: step 39560, loss = 0.91 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:25:14.839927: step 39570, loss = 0.74 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:25:16.119164: step 39580, loss = 0.71 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:25:17.379850: step 39590, loss = 0.68 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-09 22:25:18.730320: step 39600, loss = 0.76 (947.8 examples/sec; 0.135 sec/batch)
2017-05-09 22:25:19.932295: step 39610, loss = 0.93 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-09 22:25:21.216510: step 39620, loss = 0.87 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:25:22.506944: step 39630, loss = 0.75 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:25:23.808917: step 39640, loss = 0.79 (983.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:25:25.092983: step 39650, loss = 0.77 (996.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:25:26.373964: step 39660, loss = 0.73 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:25:27.655430: step 39670, loss = 0.91 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:25:28.955163: step 39680, loss = 0.68 (984.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:25:30.223981: step 39690, loss = 0.61 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:25:31.620541: step 39700, loss = 0.76 (916.5 examples/sec; 0.140 sec/batch)
2017-05-09 22:25:32.826969: step 39710, loss = 0.71 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-09 22:25:34.113110: step 39720, loss = 0.86 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:25:35.420880: step 39730, loss = 0.77 (978.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:25:36.710544: step 39740, loss = 0.90 (992.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:25:37.994842: step 39750, loss = 0.84 (996.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:25:39.305144: step 39760, loss = 0.97 (976.9 examples/sec; 0.131 sec/batch)
2017-05-09 22:25:40.577424: step 39770, loss = 0.97 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:25:41.862518: step 39780, loss = 0.66 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:25:43.135177: step 39790, loss = 0.71 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:25:44.537233: step 39800, loss = 0.70 (912.9 examples/sec; 0.140 sec/batch)
2017-05-09 22:25:45.683953: step 39810, loss = 0.62 (1116.2 examples/sec; 0.115 sec/batch)
2017-05-09 22:25:46.990287: step 39820, loss = 0.82 (979.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:25:48.288942: step 39830, loss = 0.89 (985.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:25:49.574953: step 39840, loss = 0.84 (995.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:25:50.851715: step 39850, loss = 0.92 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:25:52.137876: step 39860, loss = 0.78 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:25:53.433847: step 39870, loss = 0.85 (987.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:25:54.714998: step 39880, loss = 0.75 (999.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:25:56.030240: step 39890, loss = 0.91 (973.2 examples/sec; 0.132 sec/batch)
2017-05-09 22:25:57.387176: step 39900, loss = 0.77 (943.3 examples/sec; 0.136 sec/batch)
2017-05-09 22:25:58.558790: step 39910, loss = 0.99 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-09 22:25:59.822113: step 39920, loss = 0.78 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-09 22:26:01.089113: step 39930, loss = 0.76 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:26:02.384482: step 39940, loss = 0.79 (988.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:26:03.657632: step 39950, loss = 0.89 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:26:04.946552: step 39960, loss = 0.87 (993.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:26:06.226946: step 39970, loss = 0.83 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:26:07.488105: step 39980, loss = 0.76 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-09 22:26:08.775481: step 39990, loss = 0.76 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:26:10.158297: step 40000, loss = 0.77 (925.7 examples/sec; 0.138 sec/batch)
2017-05-09 22:26:11.393004: step 40010, loss = 1.00 (1036.7 examples/sec; 0.123 sec/batch)
2017-05-09 22:26:12.701331: step 40020, loss = 0.91 (978.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:26:14.013806: step 40030, loss = 0.75 (975.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:26:15.329014: step 40040, loss = 0.73 (973.2 examples/sec; 0.132 sec/batch)
2017-05-09 22:26:16.596599: step 40050, loss = 0.69 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:26:17.873168: step 40060, loss = 0.73 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:26:19.156437: step 40070, loss = 0.74 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:26:20.432103: step 40080, loss = 0.77 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:26:21.729232: step 40090, loss = 0.78 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:26:23.095581: step 40100, loss = 0.76 (936.8 examples/sec; 0.137 sec/batch)
2017-05-09 22:26:24.310292: step 40110, loss = 0.71 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-09 22:26:25.605287: step 40120, loss = 0.80 (988.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:26:26.914506: step 40130, loss = 0.89 (977.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:26:28.195160: step 40140, loss = 1.11 (999.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:26:29.525096: step 40150, loss = 0.82 (962.5 examples/sec; 0.133 sec/batch)
2017-05-09 22:26:30.837711: step 40160, loss = 0.85 (975.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:26:32.137224: step 40170, loss = 0.82 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:26:33.428900: step 40180, loss = 0.81 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:26:34.716996: step 40190, loss = 0.67 (993.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:26:36.065245: step 40200, loss = 0.75 (949.4 examples/sec; 0.135 sec/batch)
2017-05-09 22:26:37.273475: step 40210, loss = 0.65 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-09 22:26:38.559588: step 40220, loss = 0.86 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:26:39.838921: step 40230, loss = 0.76 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:26:41.124078: step 40240, loss = 0.67 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:26:42.388587: step 40250, loss = 0.75 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-09 22:26:43.668013: step 40260, loss = 0.81 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:26:44.945036: step 40270, loss = 0.87 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:26:46.222105: step 40280, loss = 0.76 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:26:47.503279: step 40290, loss = 0.79 (999.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:26:48.866488: step 40300, loss = 0.78 (939.0 examples/sec; 0.136 sec/batch)
2017-05-09 22:26:50.053174: step 40310, loss = 0.91 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-09 22:26:51.329778: step 40320, loss = 0.96 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:26:52.625281: step 40330, loss = 0.76 (988.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:26:53.888621: step 40340, loss = 0.82 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-09 22:26:55.172640: step 40350, loss = 0.81 (996.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:26:56.461377: step 40360, loss = 1.09 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:26:57.758749: step 40370, loss = 0.77 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:26:59.085510: step 40380, loss = 0.68 (964.8 examples/sec; 0.133 sec/batch)
2017-05-09 22:27:00.364397: step 40390, loss = 0.70 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:27:01.743432: step 40400, loss = 0.86 (928.2 examples/sec; 0.138 sec/batch)
2017-05-09 22:27:02.925618: step 40410, loss = 0.71 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-09 22:27:04.227673: step 40420, loss = 0.79 (983.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:27:05.537400: step 40430, loss = 0.77 (977.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:27:06.795610: step 40440, loss = 1.19 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-09 22:27:08.083951: step 40450, loss = 0.90 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:27:09.355510: step 40460, loss = 0.85 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:27:10.646789: step 40470, loss = 0.72 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:27:11.912645: step 40480, loss = 0.65 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:27:13.216749: step 40490, loss = 0.66 (981.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:27:14.604814: step 40500, loss = 0.85 (922.1 examples/sec; 0.139 sec/batch)
2017-05-09 22:27:15.786879: step 40510, loss = 0.78 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-09 22:27:17.053542: step 40520, loss = 0.78 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:27:18.312189: step 40530, loss = 0.83 (1017.0 examples/sec; 0.126 sec/batch)
2017-05-09 22:27:19.583198: step 40540, loss = 0.87 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:27:20.864844: step 40550, loss = 0.84 (998.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:27:22.168317: step 40560, loss = 0.92 (982.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:27:23.452661: step 40570, loss = 0.85 (996.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:27:24.758887: step 40580, loss = 0.93 (979.9 examples/sec; 0.131 sec/batch)
2017-05-09 22:27:26.058108: step 40590, loss = 0.73 (985.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:27:27.406435: step 40600, loss = 0.84 (949.3 examples/sec; 0.135 sec/batch)
2017-05-09 22:27:28.586050: step 40610, loss = 1.10 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-09 22:27:29.862974: step 40620, loss = 0.90 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:27:31.132612: step 40630, loss = 0.74 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:27:32.400237: step 40640, loss = 0.82 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:27:33.711388: step 40650, loss = 0.83 (976.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:27:35.034644: step 40660, loss = 0.77 (967.3 examples/sec; 0.132 sec/batch)
2017-05-09 22:27:36.302506: step 40670, loss = 0.84 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:27:37.590893: step 40680, loss = 0.73 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:27:38.887033: step 40690, loss = 0.86 (987.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:27:40.272563: step 40700, loss = 0.85 (923.8 examples/sec; 0.139 sec/batch)
2017-05-09 22:27:41.465139: step 40710, loss = 0.82 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-09 22:27:42.748532: step 40720, loss = 0.88 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:27:44.022308: step 40730, loss = 0.82 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:27:45.298522: step 40740, loss = 0.93 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:27:46.573708: step 40750, loss = 0.77 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:27:47.880177: step 40760, loss = 0.76 (979.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:27:49.172258: step 40770, loss = 0.75 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:27:50.475807: step 40780, loss = 0.76 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:27:51.766578: step 40790, loss = 0.79 (991.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:27:53.129175: step 40800, loss = 0.85 (939.4 examples/sec; 0.136 sec/batch)
2017-05-09 22:27:54.321678: step 40810, loss = 0.88 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-09 22:27:55.638921: step 40820, loss = 0.69 (971.7 examples/sec; 0.132 sec/batch)
2017-05-09 22:27:56.941086: step 40830, loss = 0.81 (983.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:27:58.226738: step 40840, loss = 0.86 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:27:59.498429: step 40850, loss = 0.75 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:28:00.765606: step 40860, loss = 0.86 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:28:02.069811: step 40870, loss = 0.72 (981.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:28:03.342904: step 40880, loss = 0.75 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:28:04.634547: step 40890, loss = 0.72 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:28:06.041791: step 40900, loss = 0.72 (909.6 examples/sec; 0.141 sec/batch)
2017-05-09 22:28:07.238097: step 40910, loss = 0.80 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-09 22:28:08.511050: step 40920, loss = 0.73 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:28:09.807221: step 40930, loss = 0.87 (987.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:28:11.090106: step 40940, loss = 0.62 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:28:12.383745: step 40950, loss = 0.81 (989.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:28:13.653132: step 40960, loss = 0.97 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:28:14.933419: step 40970, loss = 0.69 (999.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:28:16.274864: step 40980, loss = 0.87 (954.2 examples/sec; 0.134 sec/batch)
2017-05-09 22:28:17.534045: step 40990, loss = 0.82 (1016.5 examples/sec; 0.126 sec/batch)
2017-05-09 22:28:18.912804: step 41000, loss = 0.87 (928.4 examples/sec; 0.138 sec/batch)
2017-05-09 22:28:20.174824: step 41010, loss = 1.08 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-09 22:28:21.469307: step 41020, loss = 0.74 (988.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:28:22.750677: step 41030, loss = 0.76 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:28:24.032861: step 41040, loss = 0.79 (998.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:28:25.300975: step 41050, loss = 0.75 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:28:26.597492: step 41060, loss = 0.89 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:28:27.866192: step 41070, loss = 0.69 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:28:29.147091: step 41080, loss = 0.76 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:28:30.413031: step 41090, loss = 0.77 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:28:31.791436: step 41100, loss = 0.74 (928.6 examples/sec; 0.138 sec/batch)
2017-05-09 22:28:32.972829: step 41110, loss = 0.83 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-09 22:28:34.264840: step 41120, loss = 0.97 (990.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:28:35.530087: step 41130, loss = 0.65 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:28:36.818868: step 41140, loss = 0.72 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:28:38.118051: step 41150, loss = 0.79 (985.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:28:39.408222: step 41160, loss = 0.66 (992.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:28:40.689659: step 41170, loss = 0.88 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:28:41.968886: step 41180, loss = 0.78 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:28:43.261230: step 41190, loss = 0.95 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:28:44.645730: step 41200, loss = 0.78 (924.5 examples/sec; 0.138 sec/batch)
2017-05-09 22:28:45.848186: step 41210, loss = 1.00 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-09 22:28:47.106659: step 41220, loss = 0.80 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-09 22:28:48.368574: step 41230, loss = 0.79 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-09 22:28:49.674716: step 41240, loss = 0.92 (980.0 examples/sec; 0.131 sec/batch)
2017-05-09 22:28:50.959111: step 41250, loss = 0.73 (996.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:28:52.226942: step 41260, loss = 0.94 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:28:53.515096: step 41270, loss = 0.79 (993.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:28:54.807649: step 41280, loss = 0.81 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:28:56.070178: step 41290, loss = 0.81 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-09 22:28:57.449548: step 41300, loss = 0.81 (928.0 examples/sec; 0.138 sec/batch)
2017-05-09 22:28:58.648976: step 41310, loss = 0.87 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-09 22:28:59.924665: step 41320, loss = 0.70 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:29:01.194578: step 41330, loss = 0.78 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:29:02.466195: step 41340, loss = 0.83 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:29:03.750708: step 41350, loss = 0.83 (996.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:29:05.078266: step 41360, loss = 0.70 (964.2 examples/sec; 0.133 sec/batch)
2017-05-09 22:29:06.359051: step 41370, loss = 0.74 (999.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:29:07.663090: step 41380, loss = 0.71 (981.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:29:08.964200: step 41390, loss = 0.79 (983.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:29:10.325875: step 41400, loss = 0.97 (940.0 examples/sec; 0.136 sec/batch)
2017-05-09 22:29:11.528075: step 41410, loss = 0.75 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-09 22:29:12.815372: step 41420, loss = 0.84 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:29:14.096814: step 41430, loss = 0.90 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:29:15.370516: step 41440, loss = 0.94 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:29:16.639329: step 41450, loss = 0.94 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:29:17.937188: step 41460, loss = 0.74 (986.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:29:19.218226: step 41470, loss = 0.91 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:29:20.488389: step 41480, loss = 0.87 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:29:21.782209: step 41490, loss = 0.77 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:29:23.177214: step 41500, loss = 0.77 (917.6 examples/sec; 0.140 sec/batch)
2017-05-09 22:29:24.355984: step 41510, loss = 0.83 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-09 22:29:25.632416: step 41520, loss = 0.82 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:29:26.946696: step 41530, loss = 1.12 (973.9 examples/sec; 0.131 sec/batch)
2017-05-09 22:29:28.244924: step 41540, loss = 0.78 (986.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:29:29.501017: step 41550, loss = 0.83 (1019.0 examples/sec; 0.126 sec/batch)
2017-05-09 22:29:30.773400: step 41560, loss = 0.70 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:29:32.047741: step 41570, loss = 0.70 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:29:33.330747: step 41580, loss = 0.78 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:29:34.623671: step 41590, loss = 0.79 (990.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:29:35.991312: step 41600, loss = 0.73 (935.9 examples/sec; 0.137 sec/batch)
2017-05-09 22:29:37.174675: step 41610, loss = 0.80 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-09 22:29:38.459430: step 41620, loss = 0.77 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:29:39.739038: step 41630, loss = 1.08 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:29:41.030988: step 41640, loss = 0.65 (990.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:29:42.308901: step 41650, loss = 0.76 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:29:43.600226: step 41660, loss = 0.62 (991.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:29:44.860713: step 41670, loss = 0.87 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-09 22:29:46.126164: step 41680, loss = 0.65 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:29:47.404717: step 41690, loss = 0.81 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:29:48.786182: step 41700, loss = 0.77 (926.5 examples/sec; 0.138 sec/batch)
2017-05-09 22:29:49.991255: step 41710, loss = 0.77 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-09 22:29:51.249763: step 41720, loss = 0.80 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-09 22:29:52.524930: step 41730, loss = 0.87 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:29:53.835715: step 41740, loss = 0.68 (976.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:29:55.128826: step 41750, loss = 0.85 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:29:56.416945: step 41760, loss = 1.13 (993.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:29:57.666849: step 41770, loss = 0.68 (1024.1 examples/sec; 0.125 sec/batch)
2017-05-09 22:29:58.935350: step 41780, loss = 0.58 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:30:00.208234: step 41790, loss = 0.84 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:30:01.563510: step 41800, loss = 0.72 (944.5 examples/sec; 0.136 sec/batch)
2017-05-09 22:30:02.784827: step 41810, loss = 0.74 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-09 22:30:04.096019: step 41820, loss = 0.71 (976.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:30:05.374320: step 41830, loss = 0.67 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:30:06.630201: step 41840, loss = 0.84 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-09 22:30:07.927505: step 41850, loss = 0.86 (986.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:30:09.221395: step 41860, loss = 0.88 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:30:10.521152: step 41870, loss = 0.90 (984.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:30:11.821221: step 41880, loss = 0.70 (984.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:30:13.094304: step 41890, loss = 0.66 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:30:14.486186: step 41900, loss = 0.55 (919.6 examples/sec; 0.139 sec/batch)
2017-05-09 22:30:15.678712: step 41910, loss = 0.74 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-09 22:30:16.945705: step 41920, loss = 0.72 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:30:18.236060: step 41930, loss = 0.72 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:30:19.513080: step 41940, loss = 0.84 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:30:20.813741: step 41950, loss = 0.69 (984.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:30:22.080855: step 41960, loss = 0.80 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:30:23.356361: step 41970, loss = 0.78 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:30:24.622958: step 41980, loss = 0.81 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:30:25.927582: step 41990, loss = 0.83 (981.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:30:27.301035: step 42000, loss = 0.85 (932.0 examples/sec; 0.137 sec/batch)
2017-05-09 22:30:28.483910: step 42010, loss = 0.64 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-09 22:30:29.756669: step 42020, loss = 0.76 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:30:31.027941: step 42030, loss = 0.80 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:30:32.298625: step 42040, loss = 0.72 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:30:33.622427: step 42050, loss = 1.26 (966.9 examples/sec; 0.132 sec/batch)
2017-05-09 22:30:34.887545: step 42060, loss = 0.80 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:30:36.180826: step 42070, loss = 0.60 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:30:37.464668: step 42080, loss = 0.73 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:30:38.747566: step 42090, loss = 0.94 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:30:40.144570: step 42100, loss = 0.95 (916.2 examples/sec; 0.140 sec/batch)
2017-05-09 22:30:41.367768: step 42110, loss = 0.77 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-09 22:30:42.678850: step 42120, loss = 1.06 (976.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:30:43.938012: step 42130, loss = 0.90 (1016.5 examples/sec; 0.126 sec/batch)
2017-05-09 22:30:45.218918: step 42140, loss = 0.80 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:30:46.503688: step 42150, loss = 0.81 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:30:47.779166: step 42160, loss = 0.78 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:30:49.062153: step 42170, loss = 0.74 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:30:50.330115: step 42180, loss = 1.03 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:30:51.641653: step 42190, loss = 0.85 (976.0 examples/sec; 0.131 sec/batch)
2017-05-09 22:30:53.014324: step 42200, loss = 0.87 (932.5 examples/sec; 0.137 sec/batch)
2017-05-09 22:30:54.202666: step 42210, loss = 0.84 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-09 22:30:55.516730: step 42220, loss = 0.67 (974.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:30:56.834372: step 42230, loss = 0.71 (971.4 examples/sec; 0.132 sec/batch)
2017-05-09 22:30:58.126377: step 42240, loss = 0.84 (990.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:30:59.393563: step 42250, loss = 0.71 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:31:00.666263: step 42260, loss = 0.72 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:31:01.956191: step 42270, loss = 0.66 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:31:03.243703: step 42280, loss = 0.77 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:31:04.526881: step 42290, loss = 0.82 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:31:05.932817: step 42300, loss = 0.95 (910.4 examples/sec; 0.141 sec/batch)
2017-05-09 22:31:07.112350: step 42310, loss = 0.69 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-09 22:31:08.389983: step 42320, loss = 0.74 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:31:09.648392: step 42330, loss = 0.88 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-09 22:31:10.931388: step 42340, loss = 0.58 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:31:12.184384: step 42350, loss = 0.62 (1021.6 examples/sec; 0.125 sec/batch)
2017-05-09 22:31:13.472419: step 42360, loss = 0.66 (993.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:31:14.750761: step 42370, loss = 0.75 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:31:16.036598: step 42380, loss = 0.80 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:31:17.325879: step 42390, loss = 0.77 (992.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:31:18.714939: step 42400, loss = 0.91 (921.5 examples/sec; 0.139 sec/batch)
2017-05-09 22:31:19.893558: step 42410, loss = 0.76 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-09 22:31:21.170329: step 42420, loss = 0.83 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:31:22.422043: step 42430, loss = 0.76 (1022.6 examples/sec; 0.125 sec/batch)
2017-05-09 22:31:23.706886: step 42440, loss = 0.94 (996.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:31:25.011507: step 42450, loss = 0.76 (981.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:31:26.283567: step 42460, loss = 0.74 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:31:27.581379: step 42470, loss = 0.81 (986.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:31:28.895510: step 42480, loss = 0.74 (974.0 examples/sec; 0.131 sec/batch)
2017-05-09 22:31:30.181223: step 42490, loss = 0.81 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:31:31.552112: step 42500, loss = 1.04 (933.7 examples/sec; 0.137 sec/batch)
2017-05-09 22:31:32.751848: step 42510, loss = 0.86 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-09 22:31:34.058694: step 42520, loss = 0.81 (979.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:31:35.355125: step 42530, loss = 0.82 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:31:36.630725: step 42540, loss = 0.79 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:31:37.893679: step 42550, loss = 0.61 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-09 22:31:39.164615: step 42560, loss = 0.97 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:31:40.419245: step 42570, loss = 0.64 (1020.2 examples/sec; 0.125 sec/batch)
2017-05-09 22:31:41.686707: step 42580, loss = 0.81 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:31:42.995809: step 42590, loss = 0.83 (977.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:31:44.389673: step 42600, loss = 0.73 (918.3 examples/sec; 0.139 sec/batch)
2017-05-09 22:31:45.567010: step 42610, loss = 0.65 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-09 22:31:46.844498: step 42620, loss = 0.66 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:31:48.126364: step 42630, loss = 0.82 (998.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:31:49.395576: step 42640, loss = 0.93 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:31:50.688372: step 42650, loss = 0.73 (990.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:31:51.980597: step 42660, loss = 1.04 (990.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:31:53.285146: step 42670, loss = 0.90 (981.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:31:54.570405: step 42680, loss = 0.87 (995.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:31:55.825133: step 42690, loss = 0.86 (1020.1 examples/sec; 0.125 sec/batch)
2017-05-09 22:31:57.191924: step 42700, loss = 0.63 (936.5 examples/sec; 0.137 sec/batch)
2017-05-09 22:31:58.370057: step 42710, loss = 0.85 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-09 22:31:59.641256: step 42720, loss = 0.79 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:32:00.934745: step 42730, loss = 0.70 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:32:02.225926: step 42740, loss = 0.91 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:32:03.497764: step 42750, loss = 0.83 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:32:04.778418: step 42760, loss = 0.82 (999.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:32:06.066231: step 42770, loss = 0.74 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:32:07.339161: step 42780, loss = 0.78 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:32:08.625877: step 42790, loss = 0.81 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:32:10.005755: step 42800, loss = 0.89 (927.6 examples/sec; 0.138 sec/batch)
2017-05-09 22:32:11.189493: step 42810, loss = 0.72 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-09 22:32:12.450002: step 42820, loss = 0.77 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-09 22:32:13.727799: step 42830, loss = 0.75 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:32:15.018327: step 42840, loss = 0.86 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:32:16.295413: step 42850, loss = 0.68 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:32:17.560586: step 42860, loss = 0.81 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:32:18.834086: step 42870, loss = 0.70 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:32:20.148037: step 42880, loss = 0.98 (974.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:32:21.437801: step 42890, loss = 0.63 (992.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:32:22.818062: step 42900, loss = 0.82 (927.4 examples/sec; 0.138 sec/batch)
2017-05-09 22:32:24.019751: step 42910, loss = 0.64 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-09 22:32:25.290419: step 42920, loss = 0.76 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:32:26.557504: step 42930, loss = 0.78 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:32:27.811425: step 42940, loss = 0.79 (1020.8 examples/sec; 0.125 sec/batch)
2017-05-09 22:32:29.089324: step 42950, loss = 0.84 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:32:30.347144: step 42960, loss = 0.79 (1017.6 examples/sec; 0.126 sec/batch)
2017-05-09 22:32:31.621870: step 42970, loss = 0.72 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:32:32.919796: step 42980, loss = 0.82 (986.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:32:34.235622: step 42990, loss = 0.80 (972.8 examples/sec; 0.132 sec/batch)
2017-05-09 22:32:35.624131: step 43000, loss = 0.89 (921.9 examples/sec; 0.139 sec/batch)
2017-05-09 22:32:36.838531: step 43010, loss = 0.82 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-09 22:32:38.157036: step 43020, loss = 0.87 (970.8 examples/sec; 0.132 sec/batch)
2017-05-09 22:32:39.453032: step 43030, loss = 0.69 (987.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:32:40.740641: step 43040, loss = 0.78 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:32:42.022105: step 43050, loss = 0.78 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:32:43.303579: step 43060, loss = 0.80 (998.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:32:44.579849: step 43070, loss = 0.73 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:32:45.848214: step 43080, loss = 0.80 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:32:47.140689: step 43090, loss = 0.77 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:32:48.524397: step 43100, loss = 0.73 (925.0 examples/sec; 0.138 sec/batch)
2017-05-09 22:32:49.708932: step 43110, loss = 0.72 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-09 22:32:50.987298: step 43120, loss = 0.76 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:32:52.262006: step 43130, loss = 0.68 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:32:53.552073: step 43140, loss = 0.85 (992.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:32:54.818268: step 43150, loss = 0.86 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:32:56.094707: step 43160, loss = 0.86 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:32:57.380011: step 43170, loss = 0.72 (995.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:32:58.684958: step 43180, loss = 0.77 (980.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:32:59.954085: step 43190, loss = 0.85 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:33:01.338370: step 43200, loss = 0.75 (924.7 examples/sec; 0.138 sec/batch)
2017-05-09 22:33:02.527202: step 43210, loss = 0.70 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-09 22:33:03.787582: step 43220, loss = 1.25 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-09 22:33:05.063292: step 43230, loss = 0.77 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:33:06.338372: step 43240, loss = 0.86 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:33:07.627046: step 43250, loss = 0.85 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:33:08.895250: step 43260, loss = 0.79 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:33:10.156581: step 43270, loss = 0.73 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-09 22:33:11.433878: step 43280, loss = 0.82 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:33:12.708204: step 43290, loss = 0.67 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:33:14.056365: step 43300, loss = 0.87 (949.4 examples/sec; 0.135 sec/batch)
2017-05-09 22:33:15.243086: step 43310, loss = 0.64 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-09 22:33:16.566275: step 43320, loss = 0.66 (967.4 examples/sec; 0.132 sec/batch)
2017-05-09 22:33:17.888609: step 43330, loss = 0.82 (968.0 examples/sec; 0.132 sec/batch)
2017-05-09 22:33:19.177912: step 43340, loss = 0.85 (992.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:33:20.423561: step 43350, loss = 0.64 (1027.6 examples/sec; 0.125 sec/batch)
2017-05-09 22:33:21.705339: step 43360, loss = 0.80 (998.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:33:22.976368: step 43370, loss = 0.77 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:33:24.259627: step 43380, loss = 0.71 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:33:25.550109: step 43390, loss = 0.69 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:33:26.938803: step 43400, loss = 0.75 (921.7 examples/sec; 0.139 sec/batch)
2017-05-09 22:33:28.122164: step 43410, loss = 0.75 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-09 22:33:29.386402: step 43420, loss = 0.72 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-09 22:33:30.686175: step 43430, loss = 0.92 (984.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:33:31.955679: step 43440, loss = 0.83 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:33:33.255255: step 43450, loss = 0.76 (984.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:33:34.570345: step 43460, loss = 0.68 (973.3 examples/sec; 0.132 sec/batch)
2017-05-09 22:33:35.843710: step 43470, loss = 1.02 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:33:37.144669: step 43480, loss = 0.77 (983.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:33:38.432434: step 43490, loss = 0.75 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:33:39.796382: step 43500, loss = 0.99 (938.5 examples/sec; 0.136 sec/batch)
2017-05-09 22:33:40.996348: step 43510, loss = 0.75 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-09 22:33:42.276630: step 43520, loss = 0.81 (999.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:33:43.557053: step 43530, loss = 0.78 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:33:44.835363: step 43540, loss = 0.74 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:33:46.135134: step 43550, loss = 0.86 (984.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:33:47.425861: step 43560, loss = 0.69 (991.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:33:48.726677: step 43570, loss = 0.59 (984.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:33:49.997797: step 43580, loss = 0.96 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:33:51.294739: step 43590, loss = 0.86 (986.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:33:52.666901: step 43600, loss = 0.81 (932.8 examples/sec; 0.137 sec/batch)
2017-05-09 22:33:53.863829: step 43610, loss = 0.73 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-09 22:33:55.141071: step 43620, loss = 0.92 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:33:56.402954: step 43630, loss = 0.73 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-09 22:33:57.672060: step 43640, loss = 0.83 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:33:58.963228: step 43650, loss = 0.76 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:34:00.245284: step 43660, loss = 0.68 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:34:01.551115: step 43670, loss = 0.74 (980.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:34:02.820055: step 43680, loss = 0.68 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:34:04.106224: step 43690, loss = 0.66 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:34:05.469281: step 43700, loss = 0.55 (939.1 examples/sec; 0.136 sec/batch)
2017-05-09 22:34:06.640784: step 43710, loss = 0.85 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-09 22:34:07.904043: step 43720, loss = 0.78 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-09 22:34:09.214072: step 43730, loss = 0.83 (977.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:34:10.506946: step 43740, loss = 0.87 (990.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:34:11.806861: step 43750, loss = 0.85 (984.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:34:13.058713: step 43760, loss = 0.78 (1022.5 examples/sec; 0.125 sec/batch)
2017-05-09 22:34:14.350672: step 43770, loss = 0.74 (990.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:34:15.618020: step 43780, loss = 0.76 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:34:16.881372: step 43790, loss = 0.74 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-09 22:34:18.246267: step 43800, loss = 0.85 (937.8 examples/sec; 0.136 sec/batch)
2017-05-09 22:34:19.438724: step 43810, loss = 0.85 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-09 22:34:20.726815: step 43820, loss = 0.84 (993.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:34:21.985601: step 43830, loss = 0.79 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-09 22:34:23.259870: step 43840, loss = 0.73 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:34:24.520309: step 43850, loss = 0.85 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-09 22:34:25.797563: step 43860, loss = 0.90 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:34:27.081859: step 43870, loss = 0.77 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:34:28.343402: step 43880, loss = 0.72 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-09 22:34:29.612357: step 43890, loss = 0.81 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:34:30.987233: step 43900, loss = 0.72 (931.0 examples/sec; 0.137 sec/batch)
2017-05-09 22:34:32.201952: step 43910, loss = 0.78 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-09 22:34:33.468485: step 43920, loss = 0.97 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:34:34.770373: step 43930, loss = 0.68 (983.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:34:36.080073: step 43940, loss = 0.81 (977.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:34:37.378281: step 43950, loss = 0.83 (986.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:34:38.673625: step 43960, loss = 0.82 (988.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:34:39.946326: step 43970, loss = 0.64 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:34:41.218764: step 43980, loss = 0.78 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:34:42.509376: step 43990, loss = 0.63 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:34:43.880108: step 44000, loss = 0.70 (933.8 examples/sec; 0.137 sec/batch)
2017-05-09 22:34:45.062708: step 44010, loss = 0.75 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-09 22:34:46.355737: step 44020, loss = 0.73 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:34:47.676526: step 44030, loss = 0.90 (969.1 examples/sec; 0.132 sec/batch)
2017-05-09 22:34:48.965945: step 44040, loss = 0.73 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:34:50.271988: step 44050, loss = 0.82 (980.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:34:51.562315: step 44060, loss = 0.88 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:34:52.847189: step 44070, loss = 0.82 (996.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:34:54.108427: step 44080, loss = 0.79 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-09 22:34:55.400989: step 44090, loss = 0.74 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:34:56.809355: step 44100, loss = 0.86 (908.9 examples/sec; 0.141 sec/batch)
2017-05-09 22:34:58.021430: step 44110, loss = 0.63 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-09 22:34:59.319951: step 44120, loss = 0.62 (985.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:35:00.595185: step 44130, loss = 0.89 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:35:01.913528: step 44140, loss = 0.90 (970.9 examples/sec; 0.132 sec/batch)
2017-05-09 22:35:03.190125: step 44150, loss = 0.75 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:35:04.455220: step 44160, loss = 0.76 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:35:05.783616: step 44170, loss = 0.82 (963.6 examples/sec; 0.133 sec/batch)
2017-05-09 22:35:07.070898: step 44180, loss = 0.80 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:35:08.360515: step 44190, loss = 0.85 (992.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:35:09.733669: step 44200, loss = 0.86 (932.2 examples/sec; 0.137 sec/batch)
2017-05-09 22:35:10.902562: step 44210, loss = 0.76 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-09 22:35:12.183883: step 44220, loss = 1.02 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:35:13.489197: step 44230, loss = 0.76 (980.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:35:14.749618: step 44240, loss = 0.74 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-09 22:35:16.052793: step 44250, loss = 0.71 (982.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:35:17.371151: step 44260, loss = 0.67 (970.9 examples/sec; 0.132 sec/batch)
2017-05-09 22:35:18.672129: step 44270, loss = 0.77 (983.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:35:19.957886: step 44280, loss = 0.86 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:35:21.234361: step 44290, loss = 0.88 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:35:22.682481: step 44300, loss = 0.94 (883.9 examples/sec; 0.145 sec/batch)
2017-05-09 22:35:23.816966: step 44310, loss = 0.77 (1128.2 examples/sec; 0.113 sec/batch)
2017-05-09 22:35:25.161679: step 44320, loss = 0.65 (951.9 examples/sec; 0.134 sec/batch)
2017-05-09 22:35:26.433859: step 44330, loss = 0.89 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:35:27.713331: step 44340, loss = 0.74 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:35:28.992471: step 44350, loss = 0.85 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:35:30.270086: step 44360, loss = 0.86 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:35:31.568344: step 44370, loss = 0.81 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:35:32.866799: step 44380, loss = 0.84 (985.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:35:34.135866: step 44390, loss = 0.95 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:35:35.499733: step 44400, loss = 0.93 (938.5 examples/sec; 0.136 sec/batch)
2017-05-09 22:35:36.682082: step 44410, loss = 0.76 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-09 22:35:37.946370: step 44420, loss = 0.73 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-09 22:35:39.219686: step 44430, loss = 0.71 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:35:40.507135: step 44440, loss = 0.79 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:35:41.794747: step 44450, loss = 0.84 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:35:43.076010: step 44460, loss = 0.71 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:35:44.345026: step 44470, loss = 0.81 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:35:45.656294: step 44480, loss = 0.88 (976.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:35:46.946884: step 44490, loss = 0.72 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:35:48.310578: step 44500, loss = 0.85 (938.6 examples/sec; 0.136 sec/batch)
2017-05-09 22:35:49.475879: step 44510, loss = 0.70 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-09 22:35:50.750003: step 44520, loss = 0.81 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:35:52.068532: step 44530, loss = 0.70 (970.8 examples/sec; 0.132 sec/batch)
2017-05-09 22:35:53.360529: step 44540, loss = 1.20 (990.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:35:54.677260: step 44550, loss = 0.59 (972.1 examples/sec; 0.132 sec/batch)
2017-05-09 22:35:55.962957: step 44560, loss = 0.78 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:35:57.237312: step 44570, loss = 0.77 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:35:58.507083: step 44580, loss = 0.67 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:35:59.790970: step 44590, loss = 0.76 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:36:01.165857: step 44600, loss = 0.70 (931.0 examples/sec; 0.137 sec/batch)
2017-05-09 22:36:02.380862: step 44610, loss = 0.68 (1053.5 examples/sec; 0.122 sec/batch)
2017-05-09 22:36:03.696094: step 44620, loss = 1.00 (973.2 examples/sec; 0.132 sec/batch)
2017-05-09 22:36:04.964990: step 44630, loss = 0.82 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:36:06.239184: step 44640, loss = 0.80 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:36:07.536521: step 44650, loss = 0.92 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:36:08.846745: step 44660, loss = 0.79 (976.9 examples/sec; 0.131 sec/batch)
2017-05-09 22:36:10.130261: step 44670, loss = 0.86 (997.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:36:11.427825: step 44680, loss = 0.92 (986.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:36:12.735728: step 44690, loss = 0.81 (978.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:36:14.123936: step 44700, loss = 0.68 (922.1 examples/sec; 0.139 sec/batch)
2017-05-09 22:36:15.315105: step 44710, loss = 0.73 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-09 22:36:16.576200: step 44720, loss = 0.83 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-09 22:36:17.862984: step 44730, loss = 1.02 (994.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:36:19.117541: step 44740, loss = 0.80 (1020.3 examples/sec; 0.125 sec/batch)
2017-05-09 22:36:20.406896: step 44750, loss = 0.75 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:36:21.689766: step 44760, loss = 0.76 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:36:22.961988: step 44770, loss = 0.63 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:36:24.241681: step 44780, loss = 0.77 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:36:25.548478: step 44790, loss = 0.80 (979.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:36:26.934287: step 44800, loss = 0.78 (923.6 examples/sec; 0.139 sec/batch)
2017-05-09 22:36:28.121114: step 44810, loss = 0.84 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-09 22:36:29.397299: step 44820, loss = 0.73 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:36:30.672971: step 44830, loss = 0.72 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:36:31.954737: step 44840, loss = 0.75 (998.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:36:33.253408: step 44850, loss = 0.72 (985.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:36:34.558072: step 44860, loss = 0.95 (981.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:36:35.835280: step 44870, loss = 0.79 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:36:37.106276: step 44880, loss = 0.70 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:36:38.403941: step 44890, loss = 0.87 (986.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:36:39.767717: step 44900, loss = 0.65 (938.6 examples/sec; 0.136 sec/batch)
2017-05-09 22:36:40.943376: step 44910, loss = 0.67 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-09 22:36:42.261254: step 44920, loss = 0.78 (971.3 examples/sec; 0.132 sec/batch)
2017-05-09 22:36:43.571167: step 44930, loss = 0.69 (977.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:36:44.878191: step 44940, loss = 0.84 (979.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:36:46.182957: step 44950, loss = 0.80 (981.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:36:47.477141: step 44960, loss = 0.90 (989.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:36:48.800112: step 44970, loss = 0.78 (967.5 examples/sec; 0.132 sec/batch)
2017-05-09 22:36:50.081562: step 44980, loss = 0.75 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:36:51.357056: step 44990, loss = 0.89 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:36:52.723964: step 45000, loss = 0.78 (936.4 examples/sec; 0.137 sec/batch)
2017-05-09 22:36:53.926100: step 45010, loss = 0.83 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-09 22:36:55.220162: step 45020, loss = 0.71 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:36:56.507959: step 45030, loss = 0.86 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:36:57.786197: step 45040, loss = 0.73 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:36:59.061392: step 45050, loss = 0.79 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:37:00.337198: step 45060, loss = 0.62 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:37:01.636817: step 45070, loss = 0.81 (984.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:37:02.914105: step 45080, loss = 0.71 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:37:04.208749: step 45090, loss = 0.93 (988.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:37:05.588551: step 45100, loss = 0.75 (927.7 examples/sec; 0.138 sec/batch)
2017-05-09 22:37:06.784744: step 45110, loss = 0.78 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-09 22:37:08.059475: step 45120, loss = 0.85 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:37:09.335469: step 45130, loss = 0.94 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:37:10.622437: step 45140, loss = 0.72 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:37:11.914271: step 45150, loss = 0.83 (990.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:37:13.197813: step 45160, loss = 0.76 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:37:14.495319: step 45170, loss = 0.78 (986.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:37:15.753767: step 45180, loss = 0.75 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-09 22:37:17.078101: step 45190, loss = 0.72 (966.5 examples/sec; 0.132 sec/batch)
2017-05-09 22:37:18.446957: step 45200, loss = 0.85 (935.1 examples/sec; 0.137 sec/batch)
2017-05-09 22:37:19.632649: step 45210, loss = 0.70 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-09 22:37:20.913311: step 45220, loss = 0.74 (999.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:37:22.176092: step 45230, loss = 0.79 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-09 22:37:23.453280: step 45240, loss = 0.67 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:37:24.739090: step 45250, loss = 0.69 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:37:26.003248: step 45260, loss = 0.73 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-09 22:37:27.276382: step 45270, loss = 0.64 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:37:28.560729: step 45280, loss = 0.73 (996.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:37:29.854097: step 45290, loss = 0.87 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:37:31.245566: step 45300, loss = 0.68 (919.9 examples/sec; 0.139 sec/batch)
2017-05-09 22:37:32.432224: step 45310, loss = 0.93 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-09 22:37:33.710634: step 45320, loss = 0.86 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:37:34.995929: step 45330, loss = 0.79 (995.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:37:36.287536: step 45340, loss = 0.68 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:37:37.605551: step 45350, loss = 1.16 (971.2 examples/sec; 0.132 sec/batch)
2017-05-09 22:37:38.920982: step 45360, loss = 0.76 (973.1 examples/sec; 0.132 sec/batch)
2017-05-09 22:37:40.220649: step 45370, loss = 0.76 (984.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:37:41.503658: step 45380, loss = 0.78 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:37:42.750011: step 45390, loss = 0.80 (1027.0 examples/sec; 0.125 sec/batch)
2017-05-09 22:37:44.129119: step 45400, loss = 0.90 (928.1 examples/sec; 0.138 sec/batch)
2017-05-09 22:37:45.353385: step 45410, loss = 0.81 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-09 22:37:46.611377: step 45420, loss = 0.64 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-09 22:37:47.886666: step 45430, loss = 0.84 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:37:49.177328: step 45440, loss = 0.66 (991.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:37:50.484600: step 45450, loss = 0.80 (979.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:37:51.784188: step 45460, loss = 0.81 (984.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:37:53.060671: step 45470, loss = 0.71 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:37:54.335255: step 45480, loss = 0.77 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:37:55.594504: step 45490, loss = 0.69 (1016.5 examples/sec; 0.126 sec/batch)
2017-05-09 22:37:56.981133: step 45500, loss = 0.71 (923.1 examples/sec; 0.139 sec/batch)
2017-05-09 22:37:58.160668: step 45510, loss = 0.72 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-09 22:37:59.439855: step 45520, loss = 0.76 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:38:00.736283: step 45530, loss = 0.77 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:38:02.005812: step 45540, loss = 0.76 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:38:03.281960: step 45550, loss = 0.90 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:38:04.535166: step 45560, loss = 0.85 (1021.4 examples/sec; 0.125 sec/batch)
2017-05-09 22:38:05.811359: step 45570, loss = 0.94 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:38:07.122750: step 45580, loss = 0.73 (976.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:38:08.400179: step 45590, loss = 0.98 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:38:09.754902: step 45600, loss = 0.81 (944.8 examples/sec; 0.135 sec/batch)
2017-05-09 22:38:10.948246: step 45610, loss = 0.74 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-09 22:38:12.220268: step 45620, loss = 0.64 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:38:13.477503: step 45630, loss = 0.90 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-09 22:38:14.738190: step 45640, loss = 0.81 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-09 22:38:16.038106: step 45650, loss = 0.85 (984.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:38:17.301471: step 45660, loss = 0.76 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-09 22:38:18.576427: step 45670, loss = 0.87 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:38:19.848682: step 45680, loss = 0.84 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:38:21.142491: step 45690, loss = 0.72 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:38:22.491937: step 45700, loss = 0.68 (948.5 examples/sec; 0.135 sec/batch)
2017-05-09 22:38:23.672721: step 45710, loss = 0.84 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-09 22:38:24.959973: step 45720, loss = 0.83 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:38:26.250323: step 45730, loss = 0.68 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:38:27.553899: step 45740, loss = 0.92 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:38:28.855158: step 45750, loss = 0.90 (983.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:38:30.146134: step 45760, loss = 0.87 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:38:31.434103: step 45770, loss = 0.75 (993.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:38:32.725830: step 45780, loss = 0.83 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:38:34.008211: step 45790, loss = 0.86 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:38:35.382537: step 45800, loss = 0.78 (931.4 examples/sec; 0.137 sec/batch)
2017-05-09 22:38:36.558553: step 45810, loss = 0.91 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-09 22:38:37.839408: step 45820, loss = 0.73 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:38:39.115323: step 45830, loss = 0.65 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:38:40.381944: step 45840, loss = 0.78 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:38:41.664336: step 45850, loss = 0.76 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:38:42.951374: step 45860, loss = 0.81 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:38:44.241303: step 45870, loss = 0.83 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:38:45.523695: step 45880, loss = 0.69 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:38:46.828506: step 45890, loss = 0.77 (981.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:38:48.260088: step 45900, loss = 0.88 (894.1 examples/sec; 0.143 sec/batch)
2017-05-09 22:38:49.422984: step 45910, loss = 0.80 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-09 22:38:50.716608: step 45920, loss = 0.79 (989.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:38:52.029740: step 45930, loss = 0.75 (974.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:38:53.348707: step 45940, loss = 0.80 (970.5 examples/sec; 0.132 sec/batch)
2017-05-09 22:38:54.616026: step 45950, loss = 0.62 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:38:55.900994: step 45960, loss = 0.96 (996.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:38:57.178266: step 45970, loss = 0.68 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:38:58.474865: step 45980, loss = 0.88 (987.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:38:59.743428: step 45990, loss = 0.78 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:39:01.118110: step 46000, loss = 0.90 (931.1 examples/sec; 0.137 sec/batch)
2017-05-09 22:39:02.302971: step 46010, loss = 0.59 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-09 22:39:03.584268: step 46020, loss = 0.63 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:39:04.875877: step 46030, loss = 0.81 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:39:06.173968: step 46040, loss = 0.77 (986.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:39:07.483226: step 46050, loss = 0.67 (977.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:39:08.740361: step 46060, loss = 0.91 (1018.2 examples/sec; 0.126 sec/batch)
2017-05-09 22:39:10.033784: step 46070, loss = 0.81 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:39:11.310724: step 46080, loss = 0.79 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:39:12.599736: step 46090, loss = 0.95 (993.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:39:14.017367: step 46100, loss = 0.80 (902.9 examples/sec; 0.142 sec/batch)
2017-05-09 22:39:15.262540: step 46110, loss = 0.98 (1028.0 examples/sec; 0.125 sec/batch)
2017-05-09 22:39:16.558133: step 46120, loss = 0.70 (988.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:39:17.852288: step 46130, loss = 0.82 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:39:19.177648: step 46140, loss = 0.86 (965.8 examples/sec; 0.133 sec/batch)
2017-05-09 22:39:20.489635: step 46150, loss = 0.86 (975.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:39:21.812100: step 46160, loss = 0.90 (967.9 examples/sec; 0.132 sec/batch)
2017-05-09 22:39:23.089508: step 46170, loss = 0.75 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:39:24.384277: step 46180, loss = 0.70 (988.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:39:25.652054: step 46190, loss = 0.64 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:39:27.021269: step 46200, loss = 0.71 (934.8 examples/sec; 0.137 sec/batch)
2017-05-09 22:39:28.209763: step 46210, loss = 0.60 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-09 22:39:29.510111: step 46220, loss = 0.62 (984.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:39:30.811623: step 46230, loss = 0.74 (983.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:39:32.103223: step 46240, loss = 0.81 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:39:33.437424: step 46250, loss = 0.98 (959.4 examples/sec; 0.133 sec/batch)
2017-05-09 22:39:34.734285: step 46260, loss = 0.82 (987.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:39:36.020925: step 46270, loss = 0.71 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:39:37.298772: step 46280, loss = 0.87 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:39:38.564152: step 46290, loss = 0.80 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:39:39.941484: step 46300, loss = 0.90 (929.4 examples/sec; 0.138 sec/batch)
2017-05-09 22:39:41.146411: step 46310, loss = 0.80 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-09 22:39:42.450333: step 46320, loss = 1.14 (981.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:39:43.730614: step 46330, loss = 0.89 (999.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:39:45.057128: step 46340, loss = 0.81 (964.9 examples/sec; 0.133 sec/batch)
2017-05-09 22:39:46.386686: step 46350, loss = 0.74 (962.7 examples/sec; 0.133 sec/batch)
2017-05-09 22:39:47.701338: step 46360, loss = 0.74 (973.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:39:49.003228: step 46370, loss = 0.86 (983.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:39:50.297919: step 46380, loss = 0.82 (988.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:39:51.561725: step 46390, loss = 0.69 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-09 22:39:53.044048: step 46400, loss = 0.94 (863.5 examples/sec; 0.148 sec/batch)
2017-05-09 22:39:54.136078: step 46410, loss = 0.92 (1172.1 examples/sec; 0.109 sec/batch)
2017-05-09 22:39:55.412775: step 46420, loss = 0.70 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:39:56.713911: step 46430, loss = 0.78 (983.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:39:57.975859: step 46440, loss = 0.85 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-09 22:39:59.248494: step 46450, loss = 0.75 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:40:00.520254: step 46460, loss = 0.96 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:40:01.806648: step 46470, loss = 1.02 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:40:03.093352: step 46480, loss = 0.55 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:40:04.387135: step 46490, loss = 0.72 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:40:05.763801: step 46500, loss = 0.65 (929.8 examples/sec; 0.138 sec/batch)
2017-05-09 22:40:06.945379: step 46510, loss = 0.67 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-09 22:40:08.224389: step 46520, loss = 0.88 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:40:09.516872: step 46530, loss = 0.66 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:40:10.795205: step 46540, loss = 0.76 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:40:12.068631: step 46550, loss = 0.69 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:40:13.348968: step 46560, loss = 0.73 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:40:14.636240: step 46570, loss = 0.73 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:40:15.896293: step 46580, loss = 0.70 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-09 22:40:17.209382: step 46590, loss = 0.79 (974.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:40:18.579808: step 46600, loss = 0.62 (934.0 examples/sec; 0.137 sec/batch)
2017-05-09 22:40:19.756258: step 46610, loss = 0.61 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-09 22:40:21.007828: step 46620, loss = 0.64 (1022.7 examples/sec; 0.125 sec/batch)
2017-05-09 22:40:22.279688: step 46630, loss = 0.95 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:40:23.586582: step 46640, loss = 0.80 (979.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:40:24.886255: step 46650, loss = 0.92 (984.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:40:26.193251: step 46660, loss = 0.67 (979.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:40:27.455967: step 46670, loss = 0.99 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-09 22:40:28.750843: step 46680, loss = 0.71 (988.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:40:30.025147: step 46690, loss = 0.68 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:40:31.382381: step 46700, loss = 0.72 (943.1 examples/sec; 0.136 sec/batch)
2017-05-09 22:40:32.584209: step 46710, loss = 0.87 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-09 22:40:33.882281: step 46720, loss = 0.74 (986.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:40:35.145069: step 46730, loss = 0.72 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-09 22:40:36.438465: step 46740, loss = 0.86 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:40:37.729617: step 46750, loss = 0.74 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:40:39.013409: step 46760, loss = 0.74 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:40:40.316017: step 46770, loss = 0.76 (982.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:40:41.575568: step 46780, loss = 0.83 (1016.2 examples/sec; 0.126 sec/batch)
2017-05-09 22:40:42.859498: step 46790, loss = 0.75 (996.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:40:44.255023: step 46800, loss = 0.87 (917.2 examples/sec; 0.140 sec/batch)
2017-05-09 22:40:45.445614: step 46810, loss = 0.88 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-09 22:40:46.717091: step 46820, loss = 0.90 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:40:47.998440: step 46830, loss = 0.67 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:40:49.267111: step 46840, loss = 0.86 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:40:50.540174: step 46850, loss = 0.82 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:40:51.815720: step 46860, loss = 0.68 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:40:53.101405: step 46870, loss = 0.74 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:40:54.369885: step 46880, loss = 0.91 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:40:55.669896: step 46890, loss = 0.65 (984.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:40:57.048893: step 46900, loss = 0.76 (928.2 examples/sec; 0.138 sec/batch)
2017-05-09 22:40:58.248335: step 46910, loss = 0.70 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-09 22:40:59.544308: step 46920, loss = 0.87 (987.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:41:00.826657: step 46930, loss = 0.73 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:41:02.109049: step 46940, loss = 0.77 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:41:03.374509: step 46950, loss = 0.77 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:41:04.645040: step 46960, loss = 0.70 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:41:05.934944: step 46970, loss = 0.85 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:41:07.215491: step 46980, loss = 0.82 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:41:08.528082: step 46990, loss = 0.77 (975.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:41:09.929016: step 47000, loss = 0.83 (913.7 examples/sec; 0.140 sec/batch)
2017-05-09 22:41:11.129084: step 47010, loss = 0.77 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-09 22:41:12.420765: step 47020, loss = 0.87 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:41:13.709398: step 47030, loss = 0.95 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:41:15.032722: step 47040, loss = 1.02 (967.2 examples/sec; 0.132 sec/batch)
2017-05-09 22:41:16.304999: step 47050, loss = 0.79 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:41:17.588224: step 47060, loss = 0.84 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:41:18.865204: step 47070, loss = 0.89 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:41:20.121115: step 47080, loss = 0.70 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-09 22:41:21.398818: step 47090, loss = 0.81 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:41:22.766353: step 47100, loss = 0.81 (936.0 examples/sec; 0.137 sec/batch)
2017-05-09 22:41:23.980694: step 47110, loss = 0.80 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-09 22:41:25.281023: step 47120, loss = 0.77 (984.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:41:26.549778: step 47130, loss = 0.77 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:41:27.825763: step 47140, loss = 0.79 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:41:29.112227: step 47150, loss = 0.70 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:41:30.390118: step 47160, loss = 0.85 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:41:31.656249: step 47170, loss = 0.73 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:41:32.938310: step 47180, loss = 0.75 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:41:34.221250: step 47190, loss = 0.69 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:41:35.555087: step 47200, loss = 0.79 (959.6 examples/sec; 0.133 sec/batch)
2017-05-09 22:41:36.733127: step 47210, loss = 0.65 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-09 22:41:38.001568: step 47220, loss = 0.72 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:41:39.289583: step 47230, loss = 0.77 (993.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:41:40.556975: step 47240, loss = 0.67 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:41:41.828482: step 47250, loss = 0.68 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:41:43.106956: step 47260, loss = 0.61 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:41:44.373236: step 47270, loss = 0.78 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:41:45.646050: step 47280, loss = 0.79 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:41:46.943496: step 47290, loss = 0.84 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:41:48.324340: step 47300, loss = 0.65 (927.0 examples/sec; 0.138 sec/batch)
2017-05-09 22:41:49.511521: step 47310, loss = 0.67 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-09 22:41:50.794307: step 47320, loss = 0.82 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:41:52.067354: step 47330, loss = 0.78 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:41:53.372348: step 47340, loss = 0.73 (980.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:41:54.644261: step 47350, loss = 0.73 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:41:55.913604: step 47360, loss = 0.68 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:41:57.205294: step 47370, loss = 0.77 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:41:58.493577: step 47380, loss = 0.67 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:41:59.749243: step 47390, loss = 0.85 (1019.4 examples/sec; 0.126 sec/batch)
2017-05-09 22:42:01.133259: step 47400, loss = 0.66 (924.8 examples/sec; 0.138 sec/batch)
2017-05-09 22:42:02.314168: step 47410, loss = 0.80 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-09 22:42:03.577876: step 47420, loss = 0.90 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-09 22:42:04.853185: step 47430, loss = 0.80 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:42:06.152223: step 47440, loss = 0.70 (985.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:42:07.433861: step 47450, loss = 0.93 (998.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:42:08.733305: step 47460, loss = 0.70 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:42:10.018142: step 47470, loss = 0.82 (996.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:42:11.297294: step 47480, loss = 0.69 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:42:12.582929: step 47490, loss = 0.82 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:42:13.948524: step 47500, loss = 0.82 (937.3 examples/sec; 0.137 sec/batch)
2017-05-09 22:42:15.138466: step 47510, loss = 0.79 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-09 22:42:16.404942: step 47520, loss = 0.79 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:42:17.677550: step 47530, loss = 0.78 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:42:18.945634: step 47540, loss = 0.61 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:42:20.248861: step 47550, loss = 0.87 (982.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:42:21.559335: step 47560, loss = 0.75 (976.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:42:22.852430: step 47570, loss = 0.83 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:42:24.143987: step 47580, loss = 0.83 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:42:25.432100: step 47590, loss = 0.69 (993.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:42:26.799366: step 47600, loss = 0.74 (936.2 examples/sec; 0.137 sec/batch)
2017-05-09 22:42:28.026072: step 47610, loss = 0.85 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-09 22:42:29.345795: step 47620, loss = 0.86 (969.9 examples/sec; 0.132 sec/batch)
2017-05-09 22:42:30.656555: step 47630, loss = 1.02 (976.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:42:31.931946: step 47640, loss = 0.87 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:42:33.223240: step 47650, loss = 0.78 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:42:34.493190: step 47660, loss = 0.80 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:42:35.784329: step 47670, loss = 1.04 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:42:37.042098: step 47680, loss = 0.76 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-09 22:42:38.325627: step 47690, loss = 0.70 (997.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:42:39.690921: step 47700, loss = 0.60 (937.5 examples/sec; 0.137 sec/batch)
2017-05-09 22:42:40.871890: step 47710, loss = 0.75 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-09 22:42:42.154493: step 47720, loss = 0.68 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:42:43.435682: step 47730, loss = 0.71 (999.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:42:44.714133: step 47740, loss = 0.86 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:42:46.005611: step 47750, loss = 0.71 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:42:47.293063: step 47760, loss = 0.92 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:42:48.593418: step 47770, loss = 0.75 (984.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:42:49.859234: step 47780, loss = 0.69 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:42:51.156469: step 47790, loss = 0.68 (986.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:42:52.538798: step 47800, loss = 0.78 (926.0 examples/sec; 0.138 sec/batch)
2017-05-09 22:42:53.739870: step 47810, loss = 0.87 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-09 22:42:55.024115: step 47820, loss = 0.73 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:42:56.286669: step 47830, loss = 0.68 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-09 22:42:57.578775: step 47840, loss = 0.97 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:42:58.854486: step 47850, loss = 0.73 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:43:00.108856: step 47860, loss = 0.71 (1020.4 examples/sec; 0.125 sec/batch)
2017-05-09 22:43:01.401817: step 47870, loss = 0.70 (990.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:43:02.657700: step 47880, loss = 0.81 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-09 22:43:03.936758: step 47890, loss = 0.85 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:43:05.299434: step 47900, loss = 0.65 (939.3 examples/sec; 0.136 sec/batch)
2017-05-09 22:43:06.491418: step 47910, loss = 0.70 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-09 22:43:07.789694: step 47920, loss = 1.02 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:43:09.063378: step 47930, loss = 0.88 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:43:10.321569: step 47940, loss = 0.61 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-09 22:43:11.596318: step 47950, loss = 0.84 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:43:12.851846: step 47960, loss = 0.80 (1019.5 examples/sec; 0.126 sec/batch)
2017-05-09 22:43:14.117827: step 47970, loss = 0.79 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:43:15.388428: step 47980, loss = 0.68 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:43:16.658978: step 47990, loss = 0.73 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:43:18.033044: step 48000, loss = 0.84 (931.5 examples/sec; 0.137 sec/batch)
2017-05-09 22:43:19.242165: step 48010, loss = 0.87 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-09 22:43:20.505389: step 48020, loss = 0.65 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-09 22:43:21.804279: step 48030, loss = 0.81 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:43:23.114564: step 48040, loss = 0.92 (976.9 examples/sec; 0.131 sec/batch)
2017-05-09 22:43:24.384393: step 48050, loss = 0.81 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:43:25.682675: step 48060, loss = 0.67 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:43:26.947592: step 48070, loss = 0.62 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-09 22:43:28.246037: step 48080, loss = 0.81 (985.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:43:29.532380: step 48090, loss = 0.65 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:43:30.902242: step 48100, loss = 0.81 (934.4 examples/sec; 0.137 sec/batch)
2017-05-09 22:43:32.078211: step 48110, loss = 0.84 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-09 22:43:33.350538: step 48120, loss = 0.89 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:43:34.643365: step 48130, loss = 0.76 (990.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:43:35.931904: step 48140, loss = 0.82 (993.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:43:37.213623: step 48150, loss = 0.70 (998.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:43:38.497586: step 48160, loss = 0.89 (996.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:43:39.764701: step 48170, loss = 0.85 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:43:41.006538: step 48180, loss = 0.81 (1030.7 examples/sec; 0.124 sec/batch)
2017-05-09 22:43:42.274048: step 48190, loss = 0.70 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:43:43.623778: step 48200, loss = 0.90 (948.3 examples/sec; 0.135 sec/batch)
2017-05-09 22:43:44.788186: step 48210, loss = 0.68 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-09 22:43:46.082009: step 48220, loss = 0.67 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:43:47.385921: step 48230, loss = 0.83 (981.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:43:48.687413: step 48240, loss = 0.80 (983.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:43:50.010944: step 48250, loss = 1.00 (967.1 examples/sec; 0.132 sec/batch)
2017-05-09 22:43:51.321364: step 48260, loss = 0.90 (976.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:43:52.631667: step 48270, loss = 0.74 (976.9 examples/sec; 0.131 sec/batch)
2017-05-09 22:43:53.904306: step 48280, loss = 0.68 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:43:55.185302: step 48290, loss = 0.70 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:43:56.533110: step 48300, loss = 0.64 (949.7 examples/sec; 0.135 sec/batch)
2017-05-09 22:43:57.736882: step 48310, loss = 0.77 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-09 22:43:59.050493: step 48320, loss = 0.89 (974.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:44:00.319131: step 48330, loss = 0.72 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:44:01.649357: step 48340, loss = 0.72 (962.2 examples/sec; 0.133 sec/batch)
2017-05-09 22:44:02.960528: step 48350, loss = 0.89 (976.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:44:04.251046: step 48360, loss = 0.72 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:44:05.526766: step 48370, loss = 0.70 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:44:06.794350: step 48380, loss = 0.54 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:44:08.086894: step 48390, loss = 0.65 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:44:09.470294: step 48400, loss = 0.87 (925.3 examples/sec; 0.138 sec/batch)
2017-05-09 22:44:10.664196: step 48410, loss = 0.83 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-09 22:44:11.938751: step 48420, loss = 0.64 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:44:13.221001: step 48430, loss = 0.74 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:44:14.489821: step 48440, loss = 0.80 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:44:15.783011: step 48450, loss = 0.68 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:44:17.070963: step 48460, loss = 0.67 (993.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:44:18.373483: step 48470, loss = 0.70 (982.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:44:19.666787: step 48480, loss = 0.63 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:44:20.942845: step 48490, loss = 0.54 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:44:22.330070: step 48500, loss = 0.87 (922.7 examples/sec; 0.139 sec/batch)
2017-05-09 22:44:23.511369: step 48510, loss = 0.74 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-09 22:44:24.802475: step 48520, loss = 0.75 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:44:26.086610: step 48530, loss = 0.76 (996.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:44:27.361250: step 48540, loss = 0.65 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:44:28.640172: step 48550, loss = 0.88 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:44:29.907858: step 48560, loss = 0.99 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:44:31.191152: step 48570, loss = 0.73 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:44:32.464068: step 48580, loss = 0.81 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:44:33.744157: step 48590, loss = 0.69 (999.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:44:35.127277: step 48600, loss = 0.79 (925.4 examples/sec; 0.138 sec/batch)
2017-05-09 22:44:36.337310: step 48610, loss = 0.81 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-09 22:44:37.630486: step 48620, loss = 0.91 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:44:38.933902: step 48630, loss = 0.80 (982.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:44:40.234634: step 48640, loss = 0.70 (984.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:44:41.512293: step 48650, loss = 0.63 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:44:42.799076: step 48660, loss = 0.82 (994.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:44:44.075918: step 48670, loss = 0.67 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:44:45.352293: step 48680, loss = 0.72 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:44:46.611323: step 48690, loss = 0.97 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-09 22:44:47.965336: step 48700, loss = 0.84 (945.3 examples/sec; 0.135 sec/batch)
2017-05-09 22:44:49.173813: step 48710, loss = 0.87 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-09 22:44:50.444096: step 48720, loss = 0.68 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:44:51.708053: step 48730, loss = 0.64 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-09 22:44:52.983261: step 48740, loss = 0.69 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:44:54.250326: step 48750, loss = 0.78 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:44:55.515128: step 48760, loss = 0.77 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-09 22:44:56.791635: step 48770, loss = 0.76 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:44:58.083357: step 48780, loss = 0.74 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:44:59.371704: step 48790, loss = 0.75 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:45:00.776404: step 48800, loss = 0.84 (911.2 examples/sec; 0.140 sec/batch)
2017-05-09 22:45:01.977981: step 48810, loss = 0.85 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-09 22:45:03.250301: step 48820, loss = 0.62 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:45:04.543350: step 48830, loss = 0.91 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:45:05.848766: step 48840, loss = 0.66 (980.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:45:07.171916: step 48850, loss = 0.75 (967.4 examples/sec; 0.132 sec/batch)
2017-05-09 22:45:08.443522: step 48860, loss = 0.63 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:45:09.759247: step 48870, loss = 0.66 (972.8 examples/sec; 0.132 sec/batch)
2017-05-09 22:45:11.054708: step 48880, loss = 0.72 (988.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:45:12.369558: step 48890, loss = 0.66 (973.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:45:13.761376: step 48900, loss = 0.66 (919.7 examples/sec; 0.139 sec/batch)
2017-05-09 22:45:14.936140: step 48910, loss = 0.83 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-09 22:45:16.235263: step 48920, loss = 0.71 (985.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:45:17.547543: step 48930, loss = 0.68 (975.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:45:18.843503: step 48940, loss = 0.84 (987.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:45:20.137884: step 48950, loss = 0.97 (988.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:45:21.440362: step 48960, loss = 0.84 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:45:22.723742: step 48970, loss = 0.73 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:45:23.999588: step 48980, loss = 0.70 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:45:25.292718: step 48990, loss = 0.78 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:45:26.670197: step 49000, loss = 0.79 (929.2 examples/sec; 0.138 sec/batch)
2017-05-09 22:45:27.853294: step 49010, loss = 0.75 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-09 22:45:29.129376: step 49020, loss = 0.66 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:45:30.416938: step 49030, loss = 0.74 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:45:31.674208: step 49040, loss = 0.80 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-09 22:45:32.963664: step 49050, loss = 0.83 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:45:34.277525: step 49060, loss = 1.08 (974.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:45:35.564200: step 49070, loss = 0.77 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:45:36.840793: step 49080, loss = 0.80 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:45:38.131920: step 49090, loss = 0.88 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:45:39.502378: step 49100, loss = 0.72 (934.0 examples/sec; 0.137 sec/batch)
2017-05-09 22:45:40.712575: step 49110, loss = 0.92 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-09 22:45:41.979004: step 49120, loss = 0.71 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:45:43.251900: step 49130, loss = 0.61 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:45:44.537648: step 49140, loss = 0.76 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:45:45.817502: step 49150, loss = 0.66 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:45:47.100261: step 49160, loss = 0.78 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:45:48.378125: step 49170, loss = 0.64 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:45:49.656522: step 49180, loss = 0.70 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:45:50.937937: step 49190, loss = 0.64 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:45:52.312673: step 49200, loss = 0.91 (931.1 examples/sec; 0.137 sec/batch)
2017-05-09 22:45:53.514105: step 49210, loss = 0.94 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-09 22:45:54.806063: step 49220, loss = 0.89 (990.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:45:56.099558: step 49230, loss = 0.77 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:45:57.376055: step 49240, loss = 0.82 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:45:58.656539: step 49250, loss = 0.78 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:45:59.928862: step 49260, loss = 0.97 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:46:01.205121: step 49270, loss = 0.70 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:46:02.465100: step 49280, loss = 0.79 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-09 22:46:03.741400: step 49290, loss = 0.85 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:46:05.151107: step 49300, loss = 1.07 (908.0 examples/sec; 0.141 sec/batch)
2017-05-09 22:46:06.360530: step 49310, loss = 0.67 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-09 22:46:07.623231: step 49320, loss = 0.80 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-09 22:46:08.911358: step 49330, loss = 0.85 (993.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:46:10.187122: step 49340, loss = 0.70 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:46:11.476729: step 49350, loss = 0.78 (992.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:46:12.757232: step 49360, loss = 0.86 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:46:14.030838: step 49370, loss = 0.67 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:46:15.302631: step 49380, loss = 0.89 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:46:16.578992: step 49390, loss = 0.71 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:46:17.967354: step 49400, loss = 0.76 (921.9 examples/sec; 0.139 sec/batch)
2017-05-09 22:46:19.149122: step 49410, loss = 0.60 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-09 22:46:20.441683: step 49420, loss = 0.77 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:46:21.756432: step 49430, loss = 0.77 (973.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:46:23.083349: step 49440, loss = 0.97 (964.6 examples/sec; 0.133 sec/batch)
2017-05-09 22:46:24.373811: step 49450, loss = 0.74 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:46:25.677953: step 49460, loss = 0.92 (981.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:46:26.964523: step 49470, loss = 0.76 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:46:28.243258: step 49480, loss = 0.95 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:46:29.518048: step 49490, loss = 0.69 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:46:30.917546: step 49500, loss = 0.86 (914.6 examples/sec; 0.140 sec/batch)
2017-05-09 22:46:32.135052: step 49510, loss = 0.67 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-09 22:46:33.408364: step 49520, loss = 0.67 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:46:34.690952: step 49530, loss = 0.72 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:46:35.972690: step 49540, loss = 0.66 (998.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:46:37.243439: step 49550, loss = 0.78 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:46:38.530876: step 49560, loss = 0.89 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:46:39.856145: step 49570, loss = 1.08 (965.8 examples/sec; 0.133 sec/batch)
2017-05-09 22:46:41.155655: step 49580, loss = 0.67 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:46:42.444127: step 49590, loss = 0.80 (993.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:46:43.830268: step 49600, loss = 0.72 (923.4 examples/sec; 0.139 sec/batch)
2017-05-09 22:46:45.018634: step 49610, loss = 0.92 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-09 22:46:46.295749: step 49620, loss = 0.87 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:46:47.561603: step 49630, loss = 0.64 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:46:48.837101: step 49640, loss = 0.65 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:46:50.127070: step 49650, loss = 0.81 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:46:51.403912: step 49660, loss = 0.73 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:46:52.663342: step 49670, loss = 0.76 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-09 22:46:53.929236: step 49680, loss = 0.80 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:46:55.199400: step 49690, loss = 0.64 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:46:56.581065: step 49700, loss = 0.82 (926.4 examples/sec; 0.138 sec/batch)
2017-05-09 22:46:57.811568: step 49710, loss = 0.71 (1040.2 examples/sec; 0.123 sec/batch)
2017-05-09 22:46:59.105385: step 49720, loss = 0.69 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:47:00.390832: step 49730, loss = 0.74 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:47:01.695842: step 49740, loss = 0.83 (980.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:47:02.978741: step 49750, loss = 0.74 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:47:04.283106: step 49760, loss = 0.75 (981.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:47:05.558131: step 49770, loss = 0.72 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:47:06.886694: step 49780, loss = 0.82 (963.4 examples/sec; 0.133 sec/batch)
2017-05-09 22:47:08.147230: step 49790, loss = 0.67 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-09 22:47:09.535243: step 49800, loss = 0.83 (922.2 examples/sec; 0.139 sec/batch)
2017-05-09 22:47:10.726293: step 49810, loss = 0.76 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-09 22:47:11.974164: step 49820, loss = 0.74 (1025.7 examples/sec; 0.125 sec/batch)
2017-05-09 22:47:13.277796: step 49830, loss = 0.98 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:47:14.541074: step 49840, loss = 0.79 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-09 22:47:15.802186: step 49850, loss = 0.80 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-09 22:47:17.077064: step 49860, loss = 0.66 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:47:18.384102: step 49870, loss = 0.89 (979.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:47:19.669557: step 49880, loss = 0.75 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:47:20.964036: step 49890, loss = 1.00 (988.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:47:22.348679: step 49900, loss = 0.80 (924.4 examples/sec; 0.138 sec/batch)
2017-05-09 22:47:23.526031: step 49910, loss = 0.67 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-09 22:47:24.816892: step 49920, loss = 0.76 (991.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:47:26.103873: step 49930, loss = 0.72 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:47:27.360289: step 49940, loss = 0.62 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-09 22:47:28.639162: step 49950, loss = 0.83 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:47:29.928318: step 49960, loss = 0.78 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:47:31.199746: step 49970, loss = 0.72 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:47:32.480903: step 49980, loss = 0.70 (999.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:47:33.756914: step 49990, loss = 0.82 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:47:35.132791: step 50000, loss = 0.73 (930.3 examples/sec; 0.138 sec/batch)
--- 6481.34218407 seconds ---
