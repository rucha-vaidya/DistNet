

<section class="main-content">
      <h2>
<a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a><b>Final Writeup</b></h2>

<h3>DistNet - Summary</h3>


<p>Distnet displays various techniques used in parallelizing high-accuracy 
  Convolutional Neural Network using a distributed training algorithm on a cluster of 
  nodes and a parameter-server architecture leveraging socket communication.
  The main goal is to find out which approach is beneficial in which scenario in 
  achieving similar accuracy as non-distributed training in lesser amount of time.</p>

<h3>Background</h3>
<p>Neural networks are a set of algorithms, modeled loosely after the human brain, that are designed to recognize patterns. They interpret sensory data through a kind of machine perception, labeling or clustering raw input. The patterns they recognize are numerical, contained in vectors, into which all real-world data, be it images, sound, text or time series, must be translated. Deep neural networks have more than a single hidden layer between the input and the output layer. Training the neural network is done through gradient descent and backpropagation. Deep neural networks with multiple convolution layers and followed by fully connected layers takes a long time to train due to the varying learning rates. To achieve high accuracy, the amount of data fed into these networks is extremely high, making the time for each epoch high. Training AlexNet on a single GPU(NVIDIA K20) takes about 100 epochs (6 days). Long training times for high-accuracy deep neural networks (DNNs) impede research into new DNN architectures and slow the development of high-accuracy DNNs. Hence, we would like to explore the use of a cluster of GPU machines to accelerate the learning. The speed and scalability of distributed algorithms is almost always limited by the overhead of communicating between servers; DNN training is not an exception to this rule. Therefore, the key consideration here is to reduce communication overhead wherever possible, while not degrading the accuracy of the DNN models that we train.
  </p>
  <h4>Parallelism Axis</h4>
    <p>Parallelism in the training deep neural networks can be explored along two axes. Depending on the architecture and communication
        model we choose, the parallelism axis will affect the speedup that can be achieved by learning on a cluster as opposed to a single node.
        The two kinds of parallelism are :
    </p>
     <ul> 
      <li>Model Parallelism: <br>
        In this type of Parallelism different parts of the model that is being learnt in the neural network are computed by different 
          machines in the distributed system. Model parallelism is efficient when the amount of computation per neuron 
          activity is high, because neuron activity is the unit being communicated.
         </li>
         <img src="assets/Model_parallelism.png" height="50%" width="50%"/>
      <li>Data Parallelism: <br>
         In this type of parallelism each machine in the distributed system has a copy of the entire model but the data 
         to be trained upon is distributed over the machines and the results are combined from each. Data parallelism is efficient when the
         amount of computation per weight is high, because the weight is the unit being communicated.</li>
          <img src="assets/data_parallelism.png" height="50%" width="50%"/>
    </ul>    
      

<h3>Approach</h3>
      
      <table class="pure-table pure-table-horizontal">
    <thead>
        <tr bgcolor="#f0f0f0">
            <th>Item</th>
            <th>Description</th>
           
        </tr>
    </thead>

    <tbody>
        <tr>
            
            <td>Dataset</td>
            <td>CIFAR-10. We chose this because it can be used for training in a suitable amount of time to get a high accuracy</td>
        </tr>

        <tr bgcolor="#f0f0f0">
            
            <td>Architecture</td>
            <td>AlexNet</td>
       </tr>

        <tr>
            <td>Language</td>
            <td>Python</td>
        </tr>
        
        <tr bgcolor="#f0f0f0">
         
            <td>Machine Intelligence Library </td>
            <td>TensorFlow</td>
        </tr>
        <tr>
         
            <td>Target Machines</td>
            <td>Nodes with atleast a single GPU</td>
        </tr>

        <tr bgcolor="#f0f0f0">
      
            <td>Message Passing</td>
            <td>Native Socket Communication</td> 
        </tr>
    </tbody>
</table>

 <h4>System Architecture Diagram</h4>  
      <div class="box" style="margin:4; padding:4">
<img src="assets/System_Arch.png"/>
            </div>
      <p>The image describes the architecture of the system which consists of multiple workers running TensorFlow instances calculating gradient values and sending them to the Parameter Server(s)</p>
      <!--<a id="designer-templates" class="anchor" href="#designer-templates" aria-hidden="true"><span class="octicon octicon-link"></span></a>Pre-process Depth</h3> -->

  <h4>Challenges</h4>
      <ol>
      <li>Deciding on a reliable and fast communication mechanism. Many of the wrappers around low-level sockets come with overheads and poor reliability(for eg. python bindings for MPI.
            Hence, we decided to use the low-level socket API directly, and write explicit functions to safely receive all the intended data</li>
       <li>Due to limitations of time and resources, we are implementing a distributed training algorithm for a relatively smaller data set - CIFAR-10. This allows us to spend sufficient time in
            developing the distributed algorithms rather than waiting for the training to complete. The results of these smaller data sets will reflect even more positively
            for larger data sets(more machines, more data in working set).</li>  
      <li>Our parameter server implementation uses multiple processes, shared memory and queues, as opposed to multi-threading, this is because
            a multi-threading pythonic server cannot utilize multiple cores because of the Global Interpreter Lock</li>
       <li>Splitting the tensorflow computation graph without tensorflow knowing about it is a difficult task. Multiple partial runs on the same graph are not fully supported, because of which we had to 
            ensure that we call compute and apply gradients on two different machines for all our implementations.</li>
      
      </ol>

 
 <h4>Baseline Implementation</h4>
      <p>We use the CIFAR-10 TensorFlow tutorial code that implements the AlexNet running on a single GPU as our baseline implementation</p>

  <h4>Axis of Parallelism Used</h4>
      <p>We have used the Data Parallelism approach as a major axis of parallelism. The CNN datasets generally have large sizes and thus dividing training data among the workers makes it fit in memory and also help to go over data faster. Using multiple workers helps us look at higher aggregate size batches of data at once,
         thereby reducing the loss faster, and subsequently reaching the said accuracy faster.</p>

   <h4>Exploring Design Space in Parallelization</h4>
      <div class="box" style="margin:4; padding:4">
<img src="assets/design_space.png"/>
            </div>
      <p>The project explores the various design decisions available in parallelizing the CNN training. Beginning with single Parameter Server, we tried the Synchronous and Asynchronous Stochastic Gradient Descent. In synchronous approach the workers go in sync with each other whereas in asynchronous the workers run iterations on their own speed without waiting for anyone else.
In the asynchronous case we tried the Locked version wherein every read and write of the parameter values at the Parameter Server is protected by a lock.  In the lock-free version similar to Project Adam we removed the locked access. 
In addition another Lazy Update approach was tested in which the parameter server handling the workers immediately sent the parameter values and a background process lazily applied the gradients received later. 

Continuing we also tried multiple parameter server models wherein the parameter values were sharded across two parameter servers. </p>
      
      
<h3>Results</h3>
<p>The results are measured using wall clock time. In all scenarios, the distributed trained model achieves the same accuracy as the baseline. Also, on reducing the baseline training time to the time
      taken by its best distributed counterparts, ends up reducing the accuracy as the baseline at any point has looked at much less data than the distributed version.
</p>
      <h4>Experiment Setup 1</h4>
      <p>To first get the correctness working we started out by running 2 workers on a single GHC machine. This was done because we couldnâ€™t get two GHC machines to communicate with each other due to some security issues. Because of this the GPU compute resources were shared among the 2 workers and the parameter server.</p>
      <div class="box" style="margin:4; padding:4">
<img src="assets/Single_GTX_1080.png"/>
            </div>
      <p>Salient Observations</p>
      <ol>
            <li>The maximum speedup observed is 1.24x. We do not get the expected 2x speedup because of the shared resources.</li>
            <li>There is not much difference in execution times of various parallelism approaches because there are only 2 workers in this experiment</li>
            <li>Also the Lazy Update approach did not affect the results much because the time taken to apply gradients was observed to be very small and not worth postponing.
</li>
            </ol>
      
      
      <h4>Experiment Setup 2</h4>
      <p>Three EC2 instances on AWS out of which two were workers and one was parameter server, each having NVIDIA GRID K520 GPU</p>
      <div class="box" style="margin:4; padding:4">
<img src="assets/AWS_cluster.png"/>
            </div>
      <p>Salient Observations</p>
      <ol>
            <li>The maximum speedup observed is 1.97x. This is as expected because now there are dedicated compute resources for each worker and parameter server. 
.</li>
            <li>The earlier observations about no prominent differences in execution times of various approaches hold true here as well because of number of workers just being 2.
            </li>
            </ol>
      
       <h4>Experiment Setup 3</h4>
      <p>Observing the effect on speedup by increasing the number of workers. Tests were conducted on AWS as before using 1, 2 and 4 workers and a single parameter server.</p>
      <div class="box" style="margin:4; padding:4">
<img src="assets/Scale_out_AWS.png"/>
            </div>
      <p>Salient Observations</p>
      <ol>
            <li>With 2 workers there is a speedup of 1.97 whereas it is 2.65 in case of 4 workers compared to 1 worker. This non-linear increase in speedup is due to communication overload and also because of bottleneck at the parameter server</li>
            
            </ol>
      
      
      <h4>Experiment Setup 4</h4>
      <p>To explore the effects of various parallelism approaches on scaling out we wanted to test with 4 workers on AWS. But due to shortage of credits ( Surprisingly we lost most AWS credits in network communication among our instances even though we ran for short periods of time) it was not possible there. Thus we moved to GHC and simulated 4 workers and a parameter server there on a single GPU based machine</p>
      <div class="box" style="margin:4; padding:4">
<img src="assets/Scale_out_AWS.png"/>
            </div>
      <p>Salient Observations</p>
      <ol>
            
            <li>Here the maximum speedup is 1.54x. This is because the compute resources are shared among so many processes.</li>
            <li>Here there is visible difference in the execution times of various parallelism approaches. The approaches perform better in the order of Synchronous, Asynchronous with locks and the best , Asynchronous without locks. The next analysis was done to confirm this.</li>
            </ol>
      
      
     <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/rucha-vaidya/parallel-final">DistNet</a> is maintained by <a href="https://github.com/NasrinJaleel93">NasrinJaleel93</a> and <a href="https://github.com/rucha-vaidya">rucha-vaidya</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    
