I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.7335
pciBusID 0000:04:00.0
Total memory: 7.92GiB
Free memory: 7.81GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x4c3ed20
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: 
name: Quadro K620
major: 5 minor: 0 memoryClockRate (GHz) 1.124
pciBusID 0000:03:00.0
Total memory: 1.95GiB
Free memory: 1.86GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:04:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:962] Ignoring gpu device (device: 1, name: Quadro K620, pci bus id: 0000:03:00.0) with Cuda multiprocessor count: 3. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Train Part 1 called
2017-05-02 14:53:29.605233: step 0, loss = 4.68 (101.2 examples/sec; 1.265 sec/batch)
2017-05-02 14:53:30.253434: step 10, loss = 4.50 (1974.7 examples/sec; 0.065 sec/batch)
2017-05-02 14:53:31.049250: step 20, loss = 4.96 (1608.4 examples/sec; 0.080 sec/batch)
2017-05-02 14:53:31.834931: step 30, loss = 4.34 (1629.2 examples/sec; 0.079 sec/batch)
2017-05-02 14:53:32.638544: step 40, loss = 4.34 (1592.8 examples/sec; 0.080 sec/batch)
2017-05-02 14:53:33.446490: step 50, loss = 4.41 (1584.3 examples/sec; 0.081 sec/batch)
2017-05-02 14:53:34.241270: step 60, loss = 4.22 (1610.5 examples/sec; 0.079 sec/batch)
2017-05-02 14:53:35.039624: step 70, loss = 4.13 (1603.3 examples/sec; 0.080 sec/batch)
2017-05-02 14:53:35.827937: step 80, loss = 4.50 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 14:53:36.635914: step 90, loss = 3.98 (1584.2 examples/sec; 0.081 sec/batch)
2017-05-02 14:53:37.575898: step 100, loss = 3.98 (1361.7 examples/sec; 0.094 sec/batch)
2017-05-02 14:53:38.244564: step 110, loss = 4.03 (1914.2 examples/sec; 0.067 sec/batch)
2017-05-02 14:53:39.045552: step 120, loss = 4.23 (1598.0 examples/sec; 0.080 sec/batch)
2017-05-02 14:53:39.841893: step 130, loss = 4.00 (1607.4 examples/sec; 0.080 sec/batch)
2017-05-02 14:53:40.650572: step 140, loss = 4.36 (1582.8 examples/sec; 0.081 sec/batch)
2017-05-02 14:53:41.447413: step 150, loss = 3.85 (1606.4 examples/sec; 0.080 sec/batch)
2017-05-02 14:53:42.245119: step 160, loss = 3.89 (1604.6 examples/sec; 0.080 sec/batch)
2017-05-02 14:53:43.049829: step 170, loss = 3.97 (1590.6 examples/sec; 0.080 sec/batch)
2017-05-02 14:53:43.842400: step 180, loss = 3.95 (1615.0 examples/sec; 0.079 sec/batch)
2017-05-02 14:53:44.634795: step 190, loss = 3.92 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 14:53:45.559215: step 200, loss = 3.83 (1384.7 examples/sec; 0.092 sec/batch)
2017-05-02 14:53:46.244591: step 210, loss = 3.73 (1867.6 examples/sec; 0.069 sec/batch)
2017-05-02 14:53:47.044138: step 220, loss = 3.81 (1600.9 examples/sec; 0.080 sec/batch)
2017-05-02 14:53:47.833186: step 230, loss = 3.67 (1622.2 examples/sec; 0.079 sec/batch)
2017-05-02 14:53:48.634544: step 240, loss = 3.54 (1597.3 examples/sec; 0.080 sec/batch)
2017-05-02 14:53:49.437973: step 250, loss = 3.56 (1593.2 examples/sec; 0.080 sec/batch)
2017-05-02 14:53:50.242908: step 260, loss = 3.82 (1590.2 examples/sec; 0.080 sec/batch)
2017-05-02 14:53:51.048104: step 270, loss = 3.58 (1589.7 examples/sec; 0.081 sec/batch)
2017-05-02 14:53:51.833479: step 280, loss = 3.57 (1629.8 examples/sec; 0.079 sec/batch)
2017-05-02 14:53:52.637596: step 290, loss = 3.63 (1591.8 examples/sec; 0.080 sec/batch)
2017-05-02 14:53:53.565709: step 300, loss = 3.68 (1379.1 examples/sec; 0.093 sec/batch)
2017-05-02 14:53:54.246227: step 310, loss = 3.46 (1880.9 examples/sec; 0.068 sec/batch)
2017-05-02 14:53:55.044843: step 320, loss = 3.39 (1602.8 examples/sec; 0.080 sec/batch)
2017-05-02 14:53:55.837273: step 330, loss = 3.49 (1615.3 examples/sec; 0.079 sec/batch)
2017-05-02 14:53:56.642172: step 340, loss = 3.38 (1590.3 examples/sec; 0.080 sec/batch)
2017-05-02 14:53:57.440321: step 350, loss = 3.37 (1603.7 examples/sec; 0.080 sec/batch)
2017-05-02 14:53:58.241152: step 360, loss = 3.38 (1598.3 examples/sec; 0.080 sec/batch)
2017-05-02 14:53:59.040215: step 370, loss = 3.28 (1601.9 examples/sec; 0.080 sec/batch)
2017-05-02 14:53:59.831049: step 380, loss = 3.49 (1618.5 examples/sec; 0.079 sec/batch)
2017-05-02 14:54:00.635819: step 390, loss = 3.34 (1590.5 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:01.551592: step 400, loss = 3.80 (1397.7 examples/sec; 0.092 sec/batch)
2017-05-02 14:54:02.239007: step 410, loss = 3.45 (1862.1 examples/sec; 0.069 sec/batch)
2017-05-02 14:54:03.033206: step 420, loss = 3.28 (1611.7 examples/sec; 0.079 sec/batch)
2017-05-02 14:54:03.829412: step 430, loss = 3.24 (1607.6 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:04.630659: step 440, loss = 3.40 (1597.5 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:05.439903: step 450, loss = 3.35 (1581.7 examples/sec; 0.081 sec/batch)
2017-05-02 14:54:06.244520: step 460, loss = 3.50 (1590.8 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:07.055429: step 470, loss = 3.28 (1578.5 examples/sec; 0.081 sec/batch)
2017-05-02 14:54:07.854431: step 480, loss = 3.34 (1602.0 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:08.651818: step 490, loss = 3.26 (1605.2 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:09.582861: step 500, loss = 3.12 (1374.8 examples/sec; 0.093 sec/batch)
2017-05-02 14:54:10.265331: step 510, loss = 3.13 (1875.5 examples/sec; 0.068 sec/batch)
2017-05-02 14:54:11.063165: step 520, loss = 3.12 (1604.3 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:11.854730: step 530, loss = 2.97 (1617.0 examples/sec; 0.079 sec/batch)
2017-05-02 14:54:12.659261: step 540, loss = 3.15 (1591.0 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:13.468708: step 550, loss = 3.05 (1581.3 examples/sec; 0.081 sec/batch)
2017-05-02 14:54:14.265954: step 560, loss = 3.12 (1605.5 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:15.067060: step 570, loss = 2.98 (1597.8 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:15.859140: step 580, loss = 3.05 (1616.0 examples/sec; 0.079 sec/batch)
2017-05-02 14:54:16.659482: step 590, loss = 2.87 (1599.3 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:17.598004: step 600, loss = 3.14 (1363.9 examples/sec; 0.094 sec/batch)
2017-05-02 14:54:18.260559: step 610, loss = 3.00 (1931.9 examples/sec; 0.066 sec/batch)
2017-05-02 14:54:19.057462: step 620, loss = 3.04 (1606.2 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:19.849643: step 630, loss = 3.14 (1615.8 examples/sec; 0.079 sec/batch)
2017-05-02 14:54:20.658216: step 640, loss = 2.83 (1583.0 examples/sec; 0.081 sec/batch)
2017-05-02 14:54:21.454307: step 650, loss = 2.95 (1607.9 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:22.264568: step 660, loss = 2.95 (1579.7 examples/sec; 0.081 sec/batch)
2017-05-02 14:54:23.061221: step 670, loss = 3.01 (1606.7 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:23.856922: step 680, loss = 2.90 (1608.7 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:24.654277: step 690, loss = 3.07 (1605.3 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:25.577499: step 700, loss = 2.85 (1386.4 examples/sec; 0.092 sec/batch)
2017-05-02 14:54:26.273079: step 710, loss = 3.02 (1840.2 examples/sec; 0.070 sec/batch)
2017-05-02 14:54:27.083072: step 720, loss = 2.90 (1580.3 examples/sec; 0.081 sec/batch)
2017-05-02 14:54:27.876649: step 730, loss = 2.89 (1613.0 examples/sec; 0.079 sec/batch)
2017-05-02 14:54:28.674074: step 740, loss = 2.85 (1605.1 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:29.472378: step 750, loss = 2.74 (1603.4 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:30.276644: step 760, loss = 2.77 (1591.5 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:31.070595: step 770, loss = 2.65 (1612.2 examples/sec; 0.079 sec/batch)
2017-05-02 14:54:31.860346: step 780, loss = 2.71 (1620.8 examples/sec; 0.079 sec/batch)
2017-05-02 14:54:32.654111: step 790, loss = 2.92 (1612.6 examples/sec; 0.079 sec/batch)
2017-05-02 14:54:33.576544: step 800, loss = 2.57 (1387.6 examples/sec; 0.092 sec/batch)
2017-05-02 14:54:34.268371: step 810, loss = 2.76 (1850.2 examples/sec; 0.069 sec/batch)
2017-05-02 14:54:35.065021: step 820, loss = 2.67 (1606.7 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:35.856369: step 830, loss = 2.94 (1617.5 examples/sec; 0.079 sec/batch)
2017-05-02 14:54:36.657361: step 840, loss = 2.61 (1598.0 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:37.454684: step 850, loss = 2.62 (1605.4 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:38.252185: step 860, loss = 2.54 (1605.0 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:39.060728: step 870, loss = 2.51 (1583.1 examples/sec; 0.081 sec/batch)
2017-05-02 14:54:39.852901: step 880, loss = 2.69 (1615.9 examples/sec; 0.079 sec/batch)
2017-05-02 14:54:40.658367: step 890, loss = 2.89 (1589.1 examples/sec; 0.081 sec/batch)
2017-05-02 14:54:41.575407: step 900, loss = 2.52 (1395.8 examples/sec; 0.092 sec/batch)
2017-05-02 14:54:42.269054: step 910, loss = 2.30 (1845.3 examples/sec; 0.069 sec/batch)
2017-05-02 14:54:43.068463: step 920, loss = 2.57 (1601.2 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:43.855446: step 930, loss = 2.40 (1626.5 examples/sec; 0.079 sec/batch)
2017-05-02 14:54:44.655580: step 940, loss = 2.63 (1599.7 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:45.458565: step 950, loss = 2.40 (1594.1 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:46.260931: step 960, loss = 2.49 (1595.3 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:47.062102: step 970, loss = 2.58 (1597.7 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:47.855028: step 980, loss = 2.51 (1614.3 examples/sec; 0.079 sec/batch)
2017-05-02 14:54:48.658049: step 990, loss = 2.55 (1594.0 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:49.564418: step 1000, loss = 2.51 (1412.2 examples/sec; 0.091 sec/batch)
2017-05-02 14:54:50.266502: step 1010, loss = 2.58 (1823.1 examples/sec; 0.070 sec/batch)
2017-05-02 14:54:51.067484: step 1020, loss = 2.25 (1598.0 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:51.854680: step 1030, loss = 2.64 (1626.0 examples/sec; 0.079 sec/batch)
2017-05-02 14:54:52.652543: step 1040, loss = 2.37 (1604.3 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:53.455623: step 1050, loss = 2.71 (1593.9 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:54.257555: step 1060, loss = 2.30 (1596.1 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:55.057617: step 1070, loss = 2.30 (1599.9 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:55.845932: step 1080, loss = 2.25 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 14:54:56.647682: step 1090, loss = 2.39 (1596.5 examples/sec; 0.080 sec/batch)
2017-05-02 14:54:57.569262: step 1100, loss = 2.24 (1388.9 examples/sec; 0.092 sec/batch)
2017-05-02 14:54:58.267075: step 1110, loss = 2.73 (1834.3 examples/sec; 0.070 sec/batch)
2017-05-02 14:54:59.076380: step 1120, loss = 2.26 (1581.6 examples/sec; 0.081 sec/batch)
2017-05-02 14:54:59.866788: step 1130, loss = 2.54 (1619.4 examples/sec; 0.079 sec/batch)
2017-05-02 14:55:00.668895: step 1140, loss = 2.13 (1595.8 examples/sec; 0.080 sec/batch)
2017-05-02 14:55:01.468442: step 1150, loss = 2.07 (1600.9 examples/sec; 0.080 sec/batch)
2017-05-02 14:55:02.286011: step 1160, loss = 2.24 (1565.6 examples/sec; 0.082 sec/batch)
2017-05-02 14:55:03.097266: step 1170, loss = 2.42 (1577.8 examples/sec; 0.081 sec/batch)
2017-05-02 14:55:03.888751: step 1180, loss = 2.24 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 14:55:04.687238: step 1190, loss = 2.22 (1603.0 examples/sec; 0.080 sec/batch)
2017-05-02 14:55:05.605080: step 1200, loss = 2.30 (1394.6 examples/sec; 0.092 sec/batch)
2017-05-02 14:55:06.292256: step 1210, loss = 2.19 (1862.7 examples/sec; 0.069 sec/batch)
2017-05-02 14:55:07.096397: step 1220, loss = 2.13 (1591.8 examples/sec; 0.080 sec/batch)
2017-05-02 14:55:07.888398: step 1230, loss = 2.22 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 14:55:08.697105: step 1240, loss = 2.24 (1582.8 examples/sec; 0.081 sec/batch)
2017-05-02 14:55:09.495227: step 1250, loss = 2.31 (1603.8 examples/sec; 0.080 sec/batch)
2017-05-02 14:55:10.289569: step 1260, loss = 2.28 (1611.4 examples/sec; 0.079 sec/batch)
2017-05-02 14:55:11.093407: step 1270, loss = 1.92 (1592.4 examples/sec; 0.080 sec/batch)
2017-05-02 14:55:11.883643: step 1280, loss = 2.32 (1619.8 examples/sec; 0.079 sec/batch)
2017-05-02 14:55:12.692365: step 1290, loss = 2.03 (1582.7 examples/sec; 0.081 sec/batch)
2017-05-02 14:55:13.609939: step 1300, loss = 2.26 (1395.0 examples/sec; 0.092 sec/batch)
2017-05-02 14:55:14.305274: step 1310, loss = 2.00 (1840.8 examples/sec; 0.070 sec/batch)
2017-05-02 14:55:15.105919: step 1320, loss = 2.24 (1598.7 examples/sec; 0.080 sec/batch)
2017-05-02 14:55:15.894784: step 1330, loss = 2.00 (1622.6 examples/sec; 0.079 sec/batch)
2017-05-02 14:55:16.700690: step 1340, loss = 2.07 (1588.3 examples/sec; 0.081 sec/batch)
2017-05-02 14:55:17.511808: step 1350, loss = 2.09 (1578.1 examples/sec; 0.081 sec/batch)
2017-05-02 14:55:18.310826: step 1360, loss = 1.92 (1602.0 examples/sec; 0.080 sec/batch)
2017-05-02 14:55:19.114238: step 1370, loss = 2.10 (1593.2 examples/sec; 0.080 sec/batch)
2017-05-02 14:55:19.903767: step 1380, loss = 2.15 (1621.2 examples/sec; 0.079 sec/batch)
2017-05-02 14:55:20.708594: step 1390, loss = 1.87 (1590.4 examples/sec; 0.080 sec/batch)
2017-05-02 14:55:21.617771: step 1400, loss = 2.28 (1407.9 examples/sec; 0.091 sec/batch)
2017-05-02 14:55:22.314176: step 1410, loss = 1.90 (1838.0 examples/sec; 0.070 sec/batch)
2017-05-02 14:55:23.122151: step 1420, loss = 2.08 (1584.2 examples/sec; 0.081 sec/batch)
2017-05-02 14:55:23.905677: step 1430, loss = 2.09 (1633.6 examples/sec; 0.078 sec/batch)
2017-05-02 14:55:24.705953: step 1440, loss = 1.89 (1599.5 examples/sec; 0.080 sec/batch)
2017-05-02 14:55:25.506515: step 1450, loss = 2.22 (1598.9 examples/sec; 0.080 sec/batch)
2017-05-02 14:55:26.309341: step 1460, loss = 2.22 (1594.4 examples/sec; 0.080 sec/batch)
2017-05-02 14:55:27.103100: step 1470, loss = 1.95 (1612.6 examples/sec; 0.079 sec/batch)
2017-05-02 14:55:27.895822: step 1480, loss = 1.89 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 14:55:28.699575: step 1490, loss = 2.18 (1592.5 examples/sec; 0.080 sec/batch)
2017-05-02 14:55:29.600757: step 1500, loss = 2.02 (1420.4 examples/sec; 0.090 sec/batch)
2017-05-02 14:55:30.310713: step 1510, loss = 2.11 (1802.9 examples/sec; 0.071 sec/batch)
2017-05-02 14:55:31.111372: step 1520, loss = 2.79 (1598.7 examples/sec; 0.080 sec/batch)
2017-05-02 14:55:31.899752: step 1530, loss = 1.93 (1623.6 examples/sec; 0.079 sec/batch)
2017-05-02 14:55:32.697714: step 1540, loss = 2.11 (1604.1 examples/sec; 0.080 sec/batch)
2017-05-02 14:55:33.506080: step 1550, loss = 2.07 (1583.4 examples/sec; 0.081 sec/batch)
2017-05-02 14:55:34.312788: step 1560, loss = 1.84 (1586.7 examples/sec; 0.081 sec/batch)
2017-05-02 14:55:35.117150: step 1570, loss = 1.83 (1591.3 examples/sec; 0.080 sec/batch)
2017-05-02 14:55:35.909675: step 1580, loss = 2.04 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 14:55:36.714235: step 1590, loss = 1.89 (1590.9 examples/sec; 0.080 sec/batch)
2017-05-02 14:55:37.625523: step 1600, loss = 2.03 (1404.6 examples/sec; 0.091 sec/batch)
2017-05-02 14:55:38.317513: step 1610, loss = 1.83 (1849.7 examples/sec; 0.069 sec/batch)
2017-05-02 14:55:39.118391: step 1620, loss = 1.68 (1598.2 examples/sec; 0.080 sec/batch)
2017-05-02 14:55:39.913557: step 1630, loss = 1.96 (1609.7 examples/sec; 0.080 sec/batch)
2017-05-02 14:55:40.711658: step 1640, loss = 1.73 (1603.8 examples/sec; 0.080 sec/batch)
2017-05-02 14:55:41.519427: step 1650, loss = 1.67 (1584.6 examples/sec; 0.081 sec/batch)
2017-05-02 14:55:42.321744: step 1660, loss = 1.85 (1595.4 examples/sec; 0.080 sec/batch)
2017-05-02 14:55:43.123453: step 1670, loss = 1.62 (1596.6 examples/sec; 0.080 sec/batch)
2017-05-02 14:55:43.912339: step 1680, loss = 1.76 (1622.6 examples/sec; 0.079 sec/batch)
2017-05-02 14:55:44.719547: step 1690, loss = 2.02 (1585.7 examples/sec; 0.081 sec/batch)
2017-05-02 14:55:45.618909: step 1700, loss = 1.68 (1423.3 examples/sec; 0.090 sec/batch)
2017-05-02 14:55:46.315428: step 1710, loss = 1.75 (1837.6 examples/sec; 0.070 sec/batch)
2017-05-02 14:55:47.118862: step 1720, loss = 1.86 (1593.2 examples/sec; 0.080 sec/batch)
2017-05-02 14:55:47.905495: step 1730, loss = 1.76 (1627.2 examples/sec; 0.079 sec/batch)
2017-05-02 14:55:48.705700: step 1740, loss = 1.81 (1599.6 examples/sec; 0.080 sec/batch)
2017-05-02 14:55:49.506544: step 1750, loss = 1.97 (1598.3 examples/sec; 0.080 sec/batch)
2017-05-02 14:55:50.305395: step 1760, loss = 1.78 (1602.3 examples/sec; 0.080 sec/batch)
2017-05-02 14:55:51.107373: step 1770, loss = 1.90 (1596.0 examples/sec; 0.080 sec/batch)
2017-05-02 14:55:51.903043: step 1780, loss = 1.74 (1608.7 examples/sec; 0.080 sec/batch)
2017-05-02 14:55:52.710729: step 1790, loss = 1.74 (1584.8 examples/sec; 0.081 sec/batch)
2017-05-02 14:55:53.619218: step 1800, loss = 1.70 (1408.9 examples/sec; 0.091 sec/batch)
2017-05-02 14:55:54.325151: step 1810, loss = 2.00 (1813.2 examples/sec; 0.071 sec/batch)
2017-05-02 14:55:55.115872: step 1820, loss = 1.92 (1618.8 examples/sec; 0.079 sec/batch)
2017-05-02 14:55:55.905213: step 1830, loss = 1.99 (1621.6 examples/sec; 0.079 sec/batch)
2017-05-02 14:55:56.704998: step 1840, loss = 1.80 (1600.4 examples/sec; 0.080 sec/batch)
2017-05-02 14:55:57.504993: step 1850, loss = 2.00 (1600.0 examples/sec; 0.080 sec/batch)
2017-05-02 14:55:58.306181: step 1860, loss = 1.91 (1597.6 examples/sec; 0.080 sec/batch)
2017-05-02 14:55:59.100304: step 1870, loss = 1.71 (1611.8 examples/sec; 0.079 sec/batch)
2017-05-02 14:55:59.894576: step 1880, loss = 1.82 (1611.5 examples/sec; 0.079 sec/batch)
2017-05-02 14:56:00.700702: step 1890, loss = 2.10 (1587.8 examples/sec; 0.081 sec/batch)
2017-05-02 14:56:01.603914: step 1900, loss = 1.77 (1417.2 examples/sec; 0.090 sec/batch)
2017-05-02 14:56:02.300954: step 1910, loss = 1.56 (1836.3 examples/sec; 0.070 sec/batch)
2017-05-02 14:56:03.102684: step 1920, loss = 1.64 (1596.5 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:03.891803: step 1930, loss = 1.74 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-02 14:56:04.698047: step 1940, loss = 1.66 (1587.6 examples/sec; 0.081 sec/batch)
2017-05-02 14:56:05.500215: step 1950, loss = 1.88 (1595.7 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:06.300531: step 1960, loss = 1.69 (1599.4 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:07.103381: step 1970, loss = 1.82 (1594.3 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:07.888500: step 1980, loss = 1.58 (1630.4 examples/sec; 0.079 sec/batch)
2017-05-02 14:56:08.686895: step 1990, loss = 1.67 (1603.2 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:09.590337: step 2000, loss = 1.72 (1416.8 examples/sec; 0.090 sec/batch)
2017-05-02 14:56:10.284153: step 2010, loss = 1.78 (1844.9 examples/sec; 0.069 sec/batch)
2017-05-02 14:56:11.087988: step 2020, loss = 1.63 (1592.4 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:11.887055: step 2030, loss = 1.49 (1601.9 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:12.683189: step 2040, loss = 1.74 (1607.8 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:13.488008: step 2050, loss = 1.53 (1590.4 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:14.289641: step 2060, loss = 1.78 (1596.7 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:15.087833: step 2070, loss = 1.37 (1603.6 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:15.884908: step 2080, loss = 1.64 (1605.9 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:16.678903: step 2090, loss = 1.60 (1612.2 examples/sec; 0.079 sec/batch)
2017-05-02 14:56:17.596450: step 2100, loss = 1.49 (1395.0 examples/sec; 0.092 sec/batch)
2017-05-02 14:56:18.288774: step 2110, loss = 1.48 (1848.8 examples/sec; 0.069 sec/batch)
2017-05-02 14:56:19.089917: step 2120, loss = 1.57 (1597.7 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:19.879986: step 2130, loss = 1.71 (1620.1 examples/sec; 0.079 sec/batch)
2017-05-02 14:56:20.680068: step 2140, loss = 1.53 (1599.8 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:21.484519: step 2150, loss = 1.68 (1591.1 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:22.284457: step 2160, loss = 1.52 (1600.1 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:23.084275: step 2170, loss = 1.40 (1600.4 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:23.870564: step 2180, loss = 1.44 (1627.9 examples/sec; 0.079 sec/batch)
2017-05-02 14:56:24.679182: step 2190, loss = 1.36 (1582.9 examples/sec; 0.081 sec/batch)
2017-05-02 14:56:25.592142: step 2200, loss = 1.59 (1402.0 examples/sec; 0.091 sec/batch)
2017-05-02 14:56:26.295492: step 2210, loss = 1.80 (1819.9 examples/sec; 0.070 sec/batch)
2017-05-02 14:56:27.096492: step 2220, loss = 1.51 (1598.0 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:27.887276: step 2230, loss = 1.39 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 14:56:28.697413: step 2240, loss = 1.54 (1580.0 examples/sec; 0.081 sec/batch)
2017-05-02 14:56:29.502299: step 2250, loss = 1.67 (1590.3 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:30.299265: step 2260, loss = 1.41 (1606.1 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:31.104459: step 2270, loss = 1.53 (1589.7 examples/sec; 0.081 sec/batch)
2017-05-02 14:56:31.891315: step 2280, loss = 1.76 (1626.7 examples/sec; 0.079 sec/batch)
2017-05-02 14:56:32.693245: step 2290, loss = 1.69 (1596.2 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:33.610098: step 2300, loss = 1.46 (1396.1 examples/sec; 0.092 sec/batch)
2017-05-02 14:56:34.303792: step 2310, loss = 1.68 (1845.2 examples/sec; 0.069 sec/batch)
2017-05-02 14:56:35.103646: step 2320, loss = 1.37 (1600.3 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:35.902356: step 2330, loss = 1.58 (1602.6 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:36.697613: step 2340, loss = 1.46 (1609.5 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:37.499283: step 2350, loss = 1.63 (1596.7 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:38.297589: step 2360, loss = 1.56 (1603.4 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:39.092933: step 2370, loss = 1.58 (1609.4 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:39.877490: step 2380, loss = 1.18 (1631.5 examples/sec; 0.078 sec/batch)
2017-05-02 14:56:40.688133: step 2390, loss = 1.43 (1579.0 examples/sec; 0.081 sec/batch)
2017-05-02 14:56:41.586921: step 2400, loss = 1.69 (1424.1 examples/sec; 0.090 sec/batch)
2017-05-02 14:56:42.279952: step 2410, loss = 1.38 (1847.0 examples/sec; 0.069 sec/batch)
2017-05-02 14:56:43.081445: step 2420, loss = 1.42 (1597.0 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:43.874926: step 2430, loss = 1.39 (1613.1 examples/sec; 0.079 sec/batch)
2017-05-02 14:56:44.676506: step 2440, loss = 1.52 (1596.9 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:45.476625: step 2450, loss = 1.34 (1599.8 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:46.280770: step 2460, loss = 1.47 (1591.8 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:47.079886: step 2470, loss = 1.52 (1601.8 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:47.875411: step 2480, loss = 1.47 (1609.0 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:48.671160: step 2490, loss = 1.49 (1608.5 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:49.579570: step 2500, loss = 1.52 (1409.1 examples/sec; 0.091 sec/batch)
2017-05-02 14:56:50.282333: step 2510, loss = 1.39 (1821.4 examples/sec; 0.070 sec/batch)
2017-05-02 14:56:51.080369: step 2520, loss = 1.56 (1603.9 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:51.863051: step 2530, loss = 1.44 (1635.4 examples/sec; 0.078 sec/batch)
2017-05-02 14:56:52.667675: step 2540, loss = 1.37 (1590.8 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:53.466874: step 2550, loss = 1.43 (1601.6 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:54.264103: step 2560, loss = 1.49 (1605.6 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:55.070080: step 2570, loss = 1.49 (1588.1 examples/sec; 0.081 sec/batch)
2017-05-02 14:56:55.861463: step 2580, loss = 1.39 (1617.4 examples/sec; 0.079 sec/batch)
2017-05-02 14:56:56.659457: step 2590, loss = 1.57 (1604.0 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:57.565510: step 2600, loss = 1.53 (1412.7 examples/sec; 0.091 sec/batch)
2017-05-02 14:56:58.269750: step 2610, loss = 1.29 (1817.6 examples/sec; 0.070 sec/batch)
2017-05-02 14:56:59.069203: step 2620, loss = 1.27 (1601.1 examples/sec; 0.080 sec/batch)
2017-05-02 14:56:59.874308: step 2630, loss = 1.37 (1589.8 examples/sec; 0.081 sec/batch)
2017-05-02 14:57:00.673211: step 2640, loss = 1.43 (1602.2 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:01.473550: step 2650, loss = 1.30 (1599.3 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:02.265938: step 2660, loss = 1.32 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 14:57:03.067979: step 2670, loss = 1.52 (1595.9 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:03.858599: step 2680, loss = 1.26 (1619.0 examples/sec; 0.079 sec/batch)
2017-05-02 14:57:04.655647: step 2690, loss = 1.29 (1605.9 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:05.568680: step 2700, loss = 1.32 (1401.9 examples/sec; 0.091 sec/batch)
2017-05-02 14:57:06.259014: step 2710, loss = 1.35 (1854.2 examples/sec; 0.069 sec/batch)
2017-05-02 14:57:07.057066: step 2720, loss = 1.31 (1603.9 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:07.850329: step 2730, loss = 1.46 (1613.6 examples/sec; 0.079 sec/batch)
2017-05-02 14:57:08.655370: step 2740, loss = 1.24 (1590.0 examples/sec; 0.081 sec/batch)
2017-05-02 14:57:09.457215: step 2750, loss = 1.20 (1596.3 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:10.261823: step 2760, loss = 1.28 (1590.8 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:11.072074: step 2770, loss = 1.33 (1579.7 examples/sec; 0.081 sec/batch)
2017-05-02 14:57:11.866083: step 2780, loss = 1.39 (1612.1 examples/sec; 0.079 sec/batch)
2017-05-02 14:57:12.673024: step 2790, loss = 1.17 (1586.2 examples/sec; 0.081 sec/batch)
2017-05-02 14:57:13.586630: step 2800, loss = 1.28 (1401.0 examples/sec; 0.091 sec/batch)
2017-05-02 14:57:14.277856: step 2810, loss = 1.36 (1851.8 examples/sec; 0.069 sec/batch)
2017-05-02 14:57:15.076234: step 2820, loss = 1.43 (1603.3 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:15.867757: step 2830, loss = 1.11 (1617.1 examples/sec; 0.079 sec/batch)
2017-05-02 14:57:16.668260: step 2840, loss = 1.33 (1599.0 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:17.473865: step 2850, loss = 1.26 (1588.9 examples/sec; 0.081 sec/batch)
2017-05-02 14:57:18.274737: step 2860, loss = 1.44 (1598.3 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:19.071636: step 2870, loss = 1.20 (1606.2 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:19.868113: step 2880, loss = 1.34 (1607.1 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:20.670670: step 2890, loss = 1.23 (1594.9 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:21.583669: step 2900, loss = 1.39 (1402.0 examples/sec; 0.091 sec/batch)
2017-05-02 14:57:22.284535: step 2910, loss = 1.21 (1826.3 examples/sec; 0.070 sec/batch)
2017-05-02 14:57:23.083001: step 2920, loss = 1.28 (1603.1 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:23.880445: step 2930, loss = 1.17 (1605.1 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:24.687088: step 2940, loss = 1.18 (1586.8 examples/sec; 0.081 sec/batch)
2017-05-02 14:57:25.489086: step 2950, loss = 1.50 (1596.0 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:26.295393: step 2960, loss = 1.20 (1587.5 examples/sec; 0.081 sec/batch)
2017-05-02 14:57:27.098919: step 2970, loss = 1.30 (1593.0 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:27.886872: step 2980, loss = 1.41 (1624.5 examples/sec; 0.079 sec/batch)
2017-05-02 14:57:28.688172: step 2990, loss = 1.36 (1597.4 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:29.590045: step 3000, loss = 1.33 (1419.3 examples/sec; 0.090 sec/batch)
2017-05-02 14:57:30.298147: step 3010, loss = 1.55 (1807.7 examples/sec; 0.071 sec/batch)
2017-05-02 14:57:31.100107: step 3020, loss = 1.31 (1596.1 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:31.894015: step 3030, loss = 1.22 (1612.3 examples/sec; 0.079 sec/batch)
2017-05-02 14:57:32.696427: step 3040, loss = 1.15 (1595.2 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:33.496758: step 3050, loss = 1.00 (1599.3 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:34.301219: step 3060, loss = 1.22 (1591.1 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:35.104196: step 3070, loss = 1.45 (1594.1 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:35.892854: step 3080, loss = 1.25 (1623.0 examples/sec; 0.079 sec/batch)
2017-05-02 14:57:36.694725: step 3090, loss = 1.65 (1596.3 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:37.596763: step 3100, loss = 1.26 (1419.0 examples/sec; 0.090 sec/batch)
2017-05-02 14:57:38.303379: step 3110, loss = 1.21 (1811.5 examples/sec; 0.071 sec/batch)
2017-05-02 14:57:39.099909: step 3120, loss = 1.25 (1607.0 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:39.893737: step 3130, loss = 1.33 (1612.4 examples/sec; 0.079 sec/batch)
2017-05-02 14:57:40.689690: step 3140, loss = 1.26 (1608.1 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:41.491874: step 3150, loss = 1.25 (1595.7 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:42.290367: step 3160, loss = 1.26 (1603.0 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:43.092190: step 3170, loss = 1.36 (1596.4 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:43.879266: step 3180, loss = 1.35 (1626.3 examples/sec; 0.079 sec/batch)
2017-05-02 14:57:44.682038: step 3190, loss = 1.25 (1594.5 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:45.605138: step 3200, loss = 1.25 (1386.6 examples/sec; 0.092 sec/batch)
2017-05-02 14:57:46.285573: step 3210, loss = 1.26 (1881.1 examples/sec; 0.068 sec/batch)
2017-05-02 14:57:47.084952: step 3220, loss = 1.14 (1601.2 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:47.874437: step 3230, loss = 1.28 (1621.3 examples/sec; 0.079 sec/batch)
2017-05-02 14:57:48.672318: step 3240, loss = 1.08 (1604.2 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:49.473593: step 3250, loss = 1.20 (1597.5 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:50.278001: step 3260, loss = 1.24 (1591.2 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:51.076987: step 3270, loss = 1.12 (1602.0 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:51.870806: step 3280, loss = 1.19 (1612.5 examples/sec; 0.079 sec/batch)
2017-05-02 14:57:52.673599: step 3290, loss = 1.30 (1594.4 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:53.581013: step 3300, loss = 1.34 (1410.6 examples/sec; 0.091 sec/batch)
2017-05-02 14:57:54.273641: step 3310, loss = 1.21 (1848.0 examples/sec; 0.069 sec/batch)
2017-05-02 14:57:55.077668: step 3320, loss = 1.33 (1592.0 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:55.873325: step 3330, loss = 1.13 (1608.7 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:56.680257: step 3340, loss = 1.10 (1586.3 examples/sec; 0.081 sec/batch)
2017-05-02 14:57:57.484570: step 3350, loss = 1.13 (1591.4 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:58.294386: step 3360, loss = 1.27 (1580.6 examples/sec; 0.081 sec/batch)
2017-05-02 14:57:59.096659: step 3370, loss = 1.20 (1595.5 examples/sec; 0.080 sec/batch)
2017-05-02 14:57:59.894599: step 3380, loss = 1.34 (1604.1 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:00.697404: step 3390, loss = 1.06 (1594.4 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:01.615686: step 3400, loss = 1.26 (1393.9 examples/sec; 0.092 sec/batch)
2017-05-02 14:58:02.328926: step 3410, loss = 1.39 (1794.6 examples/sec; 0.071 sec/batch)
2017-05-02 14:58:03.123948: step 3420, loss = 1.15 (1610.0 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:03.914657: step 3430, loss = 1.13 (1618.8 examples/sec; 0.079 sec/batch)
2017-05-02 14:58:04.719440: step 3440, loss = 1.07 (1590.5 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:05.517282: step 3450, loss = 1.22 (1604.3 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:06.318891: step 3460, loss = 1.15 (1596.8 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:07.122000: step 3470, loss = 1.02 (1593.8 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:07.916853: step 3480, loss = 1.13 (1610.4 examples/sec; 0.079 sec/batch)
2017-05-02 14:58:08.715656: step 3490, loss = 1.29 (1602.4 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:09.631337: step 3500, loss = 1.12 (1397.9 examples/sec; 0.092 sec/batch)
2017-05-02 14:58:10.326292: step 3510, loss = 1.16 (1841.9 examples/sec; 0.069 sec/batch)
2017-05-02 14:58:11.122033: step 3520, loss = 1.08 (1608.6 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:11.906872: step 3530, loss = 1.20 (1630.9 examples/sec; 0.078 sec/batch)
2017-05-02 14:58:12.708075: step 3540, loss = 1.14 (1597.6 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:13.511849: step 3550, loss = 1.36 (1592.5 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:14.314125: step 3560, loss = 1.36 (1595.5 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:15.112128: step 3570, loss = 1.02 (1604.0 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:15.904616: step 3580, loss = 1.11 (1615.2 examples/sec; 0.079 sec/batch)
2017-05-02 14:58:16.707012: step 3590, loss = 1.16 (1595.2 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:17.613244: step 3600, loss = 1.15 (1412.4 examples/sec; 0.091 sec/batch)
2017-05-02 14:58:18.313093: step 3610, loss = 1.12 (1829.0 examples/sec; 0.070 sec/batch)
2017-05-02 14:58:19.109872: step 3620, loss = 1.27 (1606.5 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:19.899764: step 3630, loss = 1.55 (1620.5 examples/sec; 0.079 sec/batch)
2017-05-02 14:58:20.703320: step 3640, loss = 1.06 (1592.9 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:21.504666: step 3650, loss = 1.12 (1597.3 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:22.305146: step 3660, loss = 1.25 (1599.0 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:23.105014: step 3670, loss = 1.17 (1600.3 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:23.885906: step 3680, loss = 1.12 (1639.2 examples/sec; 0.078 sec/batch)
2017-05-02 14:58:24.686371: step 3690, loss = 1.48 (1599.0 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:25.600481: step 3700, loss = 1.19 (1400.3 examples/sec; 0.091 sec/batch)
2017-05-02 14:58:26.304391: step 3710, loss = 1.28 (1818.4 examples/sec; 0.070 sec/batch)
2017-05-02 14:58:27.102687: step 3720, loss = 1.08 (1603.4 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:27.889767: step 3730, loss = 1.33 (1626.3 examples/sec; 0.079 sec/batch)
2017-05-02 14:58:28.690899: step 3740, loss = 1.09 (1597.8 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:29.501554: step 3750, loss = 1.19 (1578.9 examples/sec; 0.081 sec/batch)
2017-05-02 14:58:30.307157: step 3760, loss = 1.11 (1588.9 examples/sec; 0.081 sec/batch)
2017-05-02 14:58:31.103350: step 3770, loss = 1.12 (1607.6 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:31.896821: step 3780, loss = 1.14 (1613.2 examples/sec; 0.079 sec/batch)
2017-05-02 14:58:32.703005: step 3790, loss = 1.24 (1587.7 examples/sec; 0.081 sec/batch)
2017-05-02 14:58:33.618060: step 3800, loss = 1.12 (1398.8 examples/sec; 0.092 sec/batch)
2017-05-02 14:58:34.330851: step 3810, loss = 1.01 (1795.8 examples/sec; 0.071 sec/batch)
2017-05-02 14:58:35.132032: step 3820, loss = 1.17 (1597.7 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:35.917076: step 3830, loss = 1.17 (1630.5 examples/sec; 0.079 sec/batch)
2017-05-02 14:58:36.720077: step 3840, loss = 1.16 (1594.0 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:37.522953: step 3850, loss = 1.08 (1594.3 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:38.326632: step 3860, loss = 1.10 (1592.7 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:39.123315: step 3870, loss = 1.12 (1606.7 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:39.913561: step 3880, loss = 1.12 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 14:58:40.714507: step 3890, loss = 1.10 (1598.1 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:41.611128: step 3900, loss = 1.16 (1427.6 examples/sec; 0.090 sec/batch)
2017-05-02 14:58:42.324606: step 3910, loss = 1.24 (1794.0 examples/sec; 0.071 sec/batch)
2017-05-02 14:58:43.125287: step 3920, loss = 1.11 (1598.6 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:43.920353: step 3930, loss = 1.24 (1609.9 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:44.719535: step 3940, loss = 1.01 (1601.6 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:45.523430: step 3950, loss = 1.21 (1592.3 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:46.322973: step 3960, loss = 1.07 (1600.9 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:47.123470: step 3970, loss = 1.06 (1599.0 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:47.925180: step 3980, loss = 1.32 (1596.6 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:48.725510: step 3990, loss = 1.19 (1599.3 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:49.638507: step 4000, loss = 1.21 (1402.0 examples/sec; 0.091 sec/batch)
2017-05-02 14:58:50.333722: step 4010, loss = 1.18 (1841.2 examples/sec; 0.070 sec/batch)
2017-05-02 14:58:51.142534: step 4020, loss = 0.93 (1582.6 examples/sec; 0.081 sec/batch)
2017-05-02 14:58:51.934008: step 4030, loss = 1.32 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 14:58:52.739253: step 4040, loss = 1.14 (1589.6 examples/sec; 0.081 sec/batch)
2017-05-02 14:58:53.543505: step 4050, loss = 0.90 (1591.5 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:54.342710: step 4060, loss = 1.09 (1601.6 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:55.144387: step 4070, loss = 0.97 (1596.6 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:55.935501: step 4080, loss = 1.23 (1618.0 examples/sec; 0.079 sec/batch)
2017-05-02 14:58:56.732802: step 4090, loss = 1.04 (1605.4 examples/sec; 0.080 sec/batch)
2017-05-02 14:58:57.647062: step 4100, loss = 1.36 (1400.0 examples/sec; 0.091 sec/batch)
2017-05-02 14:58:58.326946: step 4110, loss = 0.92 (1882.7 examples/sec; 0.068 sec/batch)
2017-05-02 14:58:59.132663: step 4120, loss = 1.22 (1588.6 examples/sec; 0.081 sec/batch)
2017-05-02 14:58:59.919763: step 4130, loss = 0.96 (1626.2 examples/sec; 0.079 sec/batch)
2017-05-02 14:59:00.724714: step 4140, loss = 1.05 (1590.2 examples/sec; 0.080 sec/batch)
2017-05-02 14:59:01.528865: step 4150, loss = 1.09 (1591.7 examples/sec; 0.080 sec/batch)
2017-05-02 14:59:02.331963: step 4160, loss = 1.15 (1593.8 examples/sec; 0.080 sec/batch)
2017-05-02 14:59:03.137606: step 4170, loss = 1.25 (1588.8 examples/sec; 0.081 sec/batch)
2017-05-02 14:59:03.924925: step 4180, loss = 1.12 (1625.8 examples/sec; 0.079 sec/batch)
2017-05-02 14:59:04.732106: step 4190, loss = 1.05 (1585.8 examples/sec; 0.081 sec/batch)
2017-05-02 14:59:05.633170: step 4200, loss = 1.12 (1420.5 examples/sec; 0.090 sec/batch)
2017-05-02 14:59:06.338129: step 4210, loss = 0.90 (1815.7 examples/sec; 0.070 sec/batch)
2017-05-02 14:59:07.134458: step 4220, loss = 1.11 (1607.4 examples/sec; 0.080 sec/batch)
2017-05-02 14:59:07.928604: step 4230, loss = 0.98 (1611.8 examples/sec; 0.079 sec/batch)
2017-05-02 14:59:08.733287: step 4240, loss = 0.96 (1590.7 examples/sec; 0.080 sec/batch)
2017-05-02 14:59:09.537221: step 4250, loss = 1.21 (1592.2 examples/sec; 0.080 sec/batch)
2017-05-02 14:59:10.343403: step 4260, loss = 1.03 (1587.7 examples/sec; 0.081 sec/batch)
2017-05-02 14:59:11.150510: step 4270, loss = 0.96 (1585.9 examples/sec; 0.081 sec/batch)
2017-05-02 14:59:11.944938: step 4280, loss = 1.06 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-02 14:59:12.743252: step 4290, loss = 1.00 (1603.4 examples/sec; 0.080 sec/batch)
2017-05-02 14:59:13.653867: step 4300, loss = 1.30 (1405.6 examples/sec; 0.091 sec/batch)
2017-05-02 14:59:14.352241: step 4310, loss = 1.08 (1832.8 examples/sec; 0.070 sec/batch)
2017-05-02 14:59:15.148278: step 4320, loss = 1.03 (1608.0 examples/sec; 0.080 sec/batch)
2017-05-02 14:59:15.941463: step 4330, loss = 0.91 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 14:59:16.745604: step 4340, loss = 1.10 (1591.8 examples/sec; 0.080 sec/batch)
2017-05-02 14:59:17.548376: step 4350, loss = 1.10 (1594.5 examples/sec; 0.080 sec/batch)
2017-05-02 14:59:18.349131: step 4360, loss = 1.17 (1598.5 examples/sec; 0.080 sec/batch)
2017-05-02 14:59:19.143860: step 4370, loss = 1.02 (1610.6 examples/sec; 0.079 sec/batch)
2017-05-02 14:59:19.931282: step 4380, loss = 1.05 (1625.5 examples/sec; 0.079 sec/batch)
2017-05-02 14:59:20.733604: step 4390, loss = 1.08 (1595.4 examples/sec; 0.080 sec/batch)
2017-05-02 14:59:21.645261: step 4400, loss = 1.04 (1404.0 examples/sec; 0.091 sec/batch)
2017-05-02 14:59:22.347084: step 4410, loss = 1.06 (1823.8 examples/sec; 0.070 sec/batch)
2017-05-02 14:59:23.144609: step 4420, loss = 1.02 (1605.0 examples/sec; 0.080 sec/batch)
2017-05-02 14:59:23.934428: step 4430, loss = 0.96 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 14:59:24.731469: step 4440, loss = 0.99 (1605.9 examples/sec; 0.080 sec/batch)
2017-05-02 14:59:25.527040: step 4450, loss = 1.09 (1608.9 examples/sec; 0.080 sec/batch)
2017-05-02 14:59:26.320859: step 4460, loss = 1.00 (1612.5 examples/sec; 0.079 sec/batch)
2017-05-02 14:59:27.121753: step 4470, loss = 1.06 (1598.2 examples/sec; 0.080 sec/batch)
2017-05-02 14:59:27.908718: step 4480, loss = 0.96 (1626.5 examples/sec; 0.079 sec/batch)
2017-05-02 14:59:28.696872: step 4490, loss = 0.99 (1624.0 examples/sec; 0.079 sec/batch)
2017-05-02 14:59:29.622393: step 4500, loss = 1.01 (1383.0 examples/sec; 0.093 sec/batch)
2017-05-02 14:59:30.296431: step 4510, loss = 0.99 (1899.0 examples/sec; 0.067 sec/batch)
2017-05-02 14:59:31.098570: step 4520, loss = 1.19 (1595.7 examples/sec; 0.080 sec/batch)
2017-05-02 14:59:31.882329: step 4530, loss = 1.04 (1633.2 examples/sec; 0.078 sec/batch)
2017-05-02 14:59:32.671900: step 4540, loss = 1.17 (1621.2 examples/sec; 0.079 sec/batch)
2017-05-02 14:59:33.473673: step 4550, loss = 1.12 (1596.4 examples/sec; 0.080 sec/batch)
2017-05-02 14:59:34.268488: step 4560, loss = 1.15 (1610.4 examples/sec; 0.079 sec/batch)
2017-05-02 14:59:35.065193: step 4570, loss = 0.91 (1606.6 examples/sec; 0.080 sec/batch)
2017-05-02 14:59:35.858081: step 4580, loss = 1.08 (1614.3 examples/sec; 0.079 sec/batch)
2017-05-02 14:59:36.663935: step 4590, loss = 1.10 (1588.4 examples/sec; 0.081 sec/batch)
2017-05-02 14:59:37.557081: step 4600, loss = 0.91 (1433.1 examples/sec; 0.089 sec/batch)
2017-05-02 14:59:38.260308: step 4610, loss = 1.04 (1820.2 examples/sec; 0.070 sec/batch)
2017-05-02 14:59:39.052632: step 4620, loss = 0.99 (1615.5 examples/sec; 0.079 sec/batch)
2017-05-02 14:59:39.841582: step 4630, loss = 1.01 (1622.4 examples/sec; 0.079 sec/batch)
2017-05-02 14:59:40.634221: step 4640, loss = 1.16 (1614.9 examples/sec; 0.079 sec/batch)
2017-05-02 14:59:41.437460: step 4650, loss = 1.18 (1593.5 examples/sec; 0.080 sec/batch)
2017-05-02 14:59:42.230793: step 4660, loss = 1.19 (1613.5 examples/sec; 0.079 sec/batch)
2017-05-02 14:59:43.030825: step 4670, loss = 0.88 (1600.0 examples/sec; 0.080 sec/batch)
2017-05-02 14:59:43.817975: step 4680, loss = 1.04 (1626.1 examples/sec; 0.079 sec/batch)
2017-05-02 14:59:44.627942: step 4690, loss = 1.12 (1580.3 examples/sec; 0.081 sec/batch)
2017-05-02 14:59:45.524470: step 4700, loss = 1.11 (1427.8 examples/sec; 0.090 sec/batch)
2017-05-02 14:59:46.220805: step 4710, loss = 0.89 (1838.2 examples/sec; 0.070 sec/batch)
2017-05-02 14:59:47.018681: step 4720, loss = 1.12 (1604.3 examples/sec; 0.080 sec/batch)
2017-05-02 14:59:47.810905: step 4730, loss = 0.93 (1615.8 examples/sec; 0.079 sec/batch)
2017-05-02 14:59:48.607609: step 4740, loss = 1.12 (1606.6 examples/sec; 0.080 sec/batch)
2017-05-02 14:59:49.409184: step 4750, loss = 1.03 (1596.9 examples/sec; 0.080 sec/batch)
2017-05-02 14:59:50.201674: step 4760, loss = 0.91 (1615.2 examples/sec; 0.079 sec/batch)
2017-05-02 14:59:51.006254: step 4770, loss = 1.03 (1590.9 examples/sec; 0.080 sec/batch)
2017-05-02 14:59:51.784998: step 4780, loss = 0.87 (1643.7 examples/sec; 0.078 sec/batch)
2017-05-02 14:59:52.585185: step 4790, loss = 1.13 (1599.6 examples/sec; 0.080 sec/batch)
2017-05-02 14:59:53.483531: step 4800, loss = 1.05 (1424.8 examples/sec; 0.090 sec/batch)
2017-05-02 14:59:54.179184: step 4810, loss = 1.08 (1840.0 examples/sec; 0.070 sec/batch)
2017-05-02 14:59:54.977033: step 4820, loss = 1.13 (1604.3 examples/sec; 0.080 sec/batch)
2017-05-02 14:59:55.757097: step 4830, loss = 0.90 (1640.9 examples/sec; 0.078 sec/batch)
2017-05-02 14:59:56.559126: step 4840, loss = 1.04 (1595.9 examples/sec; 0.080 sec/batch)
2017-05-02 14:59:57.355469: step 4850, loss = 0.97 (1607.4 examples/sec; 0.080 sec/batch)
2017-05-02 14:59:58.153054: step 4860, loss = 1.06 (1604.8 examples/sec; 0.080 sec/batch)
2017-05-02 14:59:58.954174: step 4870, loss = 1.10 (1597.8 examples/sec; 0.080 sec/batch)
2017-05-02 14:59:59.742297: step 4880, loss = 1.07 (1624.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:00:00.552848: step 4890, loss = 1.05 (1579.2 examples/sec; 0.081 sec/batch)
2017-05-02 15:00:01.455250: step 4900, loss = 1.20 (1418.4 examples/sec; 0.090 sec/batch)
2017-05-02 15:00:02.151047: step 4910, loss = 1.13 (1839.6 examples/sec; 0.070 sec/batch)
2017-05-02 15:00:02.952503: step 4920, loss = 0.93 (1597.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:00:03.743763: step 4930, loss = 1.13 (1617.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:00:04.539145: step 4940, loss = 1.03 (1609.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:00:05.339692: step 4950, loss = 0.99 (1598.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:00:06.132563: step 4960, loss = 1.07 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:00:06.929989: step 4970, loss = 1.16 (1605.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:00:07.716137: step 4980, loss = 1.42 (1628.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:00:08.510185: step 4990, loss = 0.94 (1612.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:00:09.408675: step 5000, loss = 1.04 (1424.6 examples/sec; 0.090 sec/batch)
2017-05-02 15:00:10.102159: step 5010, loss = 0.84 (1845.8 examples/sec; 0.069 sec/batch)
2017-05-02 15:00:10.899644: step 5020, loss = 1.05 (1605.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:00:11.691116: step 5030, loss = 1.01 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:00:12.482958: step 5040, loss = 0.97 (1616.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:00:13.283162: step 5050, loss = 1.07 (1599.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:00:14.076381: step 5060, loss = 1.05 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:00:14.875417: step 5070, loss = 1.09 (1601.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:00:15.667367: step 5080, loss = 0.86 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:00:16.463513: step 5090, loss = 1.10 (1607.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:00:17.354173: step 5100, loss = 0.88 (1437.2 examples/sec; 0.089 sec/batch)
2017-05-02 15:00:18.056054: step 5110, loss = 0.87 (1823.7 examples/sec; 0.070 sec/batch)
2017-05-02 15:00:18.861515: step 5120, loss = 0.89 (1589.1 examples/sec; 0.081 sec/batch)
2017-05-02 15:00:19.647171: step 5130, loss = 1.03 (1629.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:00:20.442332: step 5140, loss = 1.05 (1609.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:00:21.236817: step 5150, loss = 1.01 (1611.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:00:22.035631: step 5160, loss = 1.06 (1602.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:00:22.830365: step 5170, loss = 1.04 (1610.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:00:23.614405: step 5180, loss = 0.93 (1632.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:00:24.414899: step 5190, loss = 0.87 (1599.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:00:25.311115: step 5200, loss = 1.16 (1428.2 examples/sec; 0.090 sec/batch)
2017-05-02 15:00:26.017816: step 5210, loss = 1.11 (1811.2 examples/sec; 0.071 sec/batch)
2017-05-02 15:00:26.815773: step 5220, loss = 1.08 (1604.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:00:27.602710: step 5230, loss = 1.00 (1626.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:00:28.396374: step 5240, loss = 0.83 (1612.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:00:29.195897: step 5250, loss = 1.09 (1601.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:00:29.991639: step 5260, loss = 0.87 (1608.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:00:30.786014: step 5270, loss = 1.09 (1611.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:00:31.574802: step 5280, loss = 1.24 (1622.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:00:32.369159: step 5290, loss = 1.09 (1611.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:00:33.274001: step 5300, loss = 1.00 (1414.6 examples/sec; 0.090 sec/batch)
2017-05-02 15:00:33.962201: step 5310, loss = 1.06 (1859.9 examples/sec; 0.069 sec/batch)
2017-05-02 15:00:34.759076: step 5320, loss = 0.90 (1606.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:00:35.550995: step 5330, loss = 1.15 (1616.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:00:36.342814: step 5340, loss = 1.06 (1616.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:00:37.139116: step 5350, loss = 0.96 (1607.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:00:37.932219: step 5360, loss = 1.00 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:00:38.734989: step 5370, loss = 0.86 (1594.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:00:39.522797: step 5380, loss = 0.95 (1624.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:00:40.317664: step 5390, loss = 0.90 (1610.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:00:41.215852: step 5400, loss = 1.04 (1425.1 examples/sec; 0.090 sec/batch)
2017-05-02 15:00:41.919980: step 5410, loss = 1.13 (1817.8 examples/sec; 0.070 sec/batch)
2017-05-02 15:00:42.731603: step 5420, loss = 0.90 (1577.1 examples/sec; 0.081 sec/batch)
2017-05-02 15:00:43.532332: step 5430, loss = 0.96 (1598.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:00:44.321926: step 5440, loss = 1.10 (1621.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:00:45.119534: step 5450, loss = 0.97 (1604.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:00:45.907173: step 5460, loss = 0.82 (1625.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:00:46.707645: step 5470, loss = 0.93 (1599.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:00:47.497157: step 5480, loss = 1.01 (1621.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:00:48.286982: step 5490, loss = 1.03 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:00:49.198444: step 5500, loss = 1.03 (1404.3 examples/sec; 0.091 sec/batch)
2017-05-02 15:00:49.876524: step 5510, loss = 1.08 (1887.7 examples/sec; 0.068 sec/batch)
2017-05-02 15:00:50.681314: step 5520, loss = 0.92 (1590.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:00:51.470941: step 5530, loss = 0.89 (1621.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:00:52.263595: step 5540, loss = 0.92 (1614.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:00:53.053629: step 5550, loss = 0.82 (1620.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:00:53.844830: step 5560, loss = 1.07 (1617.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:00:54.648099: step 5570, loss = 1.14 (1593.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:00:55.438520: step 5580, loss = 0.99 (1619.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:00:56.240331: step 5590, loss = 1.15 (1596.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:00:57.151859: step 5600, loss = 0.94 (1404.2 examples/sec; 0.091 sec/batch)
2017-05-02 15:00:57.842652: step 5610, loss = 0.92 (1852.9 examples/sec; 0.069 sec/batch)
2017-05-02 15:00:58.646083: step 5620, loss = 0.86 (1593.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:00:59.432788: step 5630, loss = 0.90 (1627.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:01:00.224783: step 5640, loss = 1.07 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:01:01.026841: step 5650, loss = 1.06 (1595.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:01:01.815119: step 5660, loss = 0.91 (1623.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:01:02.620007: step 5670, loss = 1.07 (1590.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:01:03.411577: step 5680, loss = 0.91 (1617.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:01:04.198184: step 5690, loss = 1.27 (1627.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:01:05.097324: step 5700, loss = 1.05 (1423.6 examples/sec; 0.090 sec/batch)
2017-05-02 15:01:05.786026: step 5710, loss = 1.11 (1858.7 examples/sec; 0.069 sec/batch)
2017-05-02 15:01:06.591966: step 5720, loss = 0.89 (1588.1 examples/sec; 0.081 sec/batch)
2017-05-02 15:01:07.391067: step 5730, loss = 0.88 (1601.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:01:08.184020: step 5740, loss = 1.06 (1614.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:01:08.972521: step 5750, loss = 0.99 (1623.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:01:09.769889: step 5760, loss = 0.88 (1605.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:01:10.561257: step 5770, loss = 0.93 (1617.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:01:11.355817: step 5780, loss = 1.14 (1611.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:01:12.146535: step 5790, loss = 1.05 (1618.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:01:13.046537: step 5800, loss = 1.10 (1422.2 examples/sec; 0.090 sec/batch)
2017-05-02 15:01:13.741703: step 5810, loss = 0.93 (1841.3 examples/sec; 0.070 sec/batch)
2017-05-02 15:01:14.535026: step 5820, loss = 1.11 (1613.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:01:15.330524: step 5830, loss = 0.79 (1609.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:01:16.120903: step 5840, loss = 1.01 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:01:16.925308: step 5850, loss = 0.85 (1591.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:01:17.718030: step 5860, loss = 0.99 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:01:18.522308: step 5870, loss = 0.94 (1591.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:01:19.316105: step 5880, loss = 1.01 (1612.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:01:20.107073: step 5890, loss = 0.89 (1618.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:01:21.019121: step 5900, loss = 1.00 (1403.4 examples/sec; 0.091 sec/batch)
2017-05-02 15:01:21.706487: step 5910, loss = 1.02 (1862.2 examples/sec; 0.069 sec/batch)
2017-05-02 15:01:22.503093: step 5920, loss = 1.04 (1606.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:01:23.299460: step 5930, loss = 1.03 (1607.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:01:24.086892: step 5940, loss = 0.88 (1625.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:01:24.887218: step 5950, loss = 1.05 (1599.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:01:25.693523: step 5960, loss = 1.03 (1587.5 examples/sec; 0.081 sec/batch)
2017-05-02 15:01:26.489808: step 5970, loss = 1.25 (1607.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:01:27.282342: step 5980, loss = 0.92 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:01:28.070217: step 5990, loss = 0.85 (1624.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:01:28.967330: step 6000, loss = 1.00 (1426.8 examples/sec; 0.090 sec/batch)
2017-05-02 15:01:29.672590: step 6010, loss = 0.93 (1814.9 examples/sec; 0.071 sec/batch)
2017-05-02 15:01:30.470605: step 6020, loss = 1.10 (1604.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:01:31.272849: step 6030, loss = 1.04 (1595.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:01:32.065161: step 6040, loss = 1.01 (1615.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:01:32.862462: step 6050, loss = 1.06 (1605.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:01:33.666340: step 6060, loss = 0.91 (1592.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:01:34.462317: step 6070, loss = 0.99 (1608.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:01:35.261909: step 6080, loss = 1.07 (1600.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:01:36.050514: step 6090, loss = 0.79 (1623.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:01:36.961629: step 6100, loss = 0.81 (1404.9 examples/sec; 0.091 sec/batch)
2017-05-02 15:01:37.641639: step 6110, loss = 1.03 (1882.3 examples/sec; 0.068 sec/batch)
2017-05-02 15:01:38.439340: step 6120, loss = 1.17 (1604.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:01:39.235824: step 6130, loss = 1.08 (1607.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:01:40.019769: step 6140, loss = 0.97 (1632.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:01:40.811954: step 6150, loss = 1.04 (1615.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:01:41.605403: step 6160, loss = 0.91 (1613.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:01:42.411099: step 6170, loss = 1.10 (1588.7 examples/sec; 0.081 sec/batch)
2017-05-02 15:01:43.199512: step 6180, loss = 1.00 (1623.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:01:43.988272: step 6190, loss = 1.03 (1622.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:01:44.897107: step 6200, loss = 1.01 (1408.4 examples/sec; 0.091 sec/batch)
2017-05-02 15:01:45.586932: step 6210, loss = 1.14 (1855.5 examples/sec; 0.069 sec/batch)
2017-05-02 15:01:46.388165: step 6220, loss = 0.95 (1597.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:01:47.176055: step 6230, loss = 0.93 (1624.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:01:47.961110: step 6240, loss = 1.01 (1630.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:01:48.756903: step 6250, loss = 1.04 (1608.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:01:49.553626: step 6260, loss = 0.88 (1606.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:01:50.362285: step 6270, loss = 0.86 (1582.9 examples/sec; 0.081 sec/batch)
2017-05-02 15:01:51.164462: step 6280, loss = 0.96 (1595.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:01:51.942575: step 6290, loss = 0.83 (1645.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:01:52.835936: step 6300, loss = 0.99 (1432.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:01:53.537577: step 6310, loss = 0.89 (1824.3 examples/sec; 0.070 sec/batch)
2017-05-02 15:01:54.336593: step 6320, loss = 1.01 (1602.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:01:55.130435: step 6330, loss = 0.84 (1612.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:01:55.915730: step 6340, loss = 1.05 (1630.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:01:56.718156: step 6350, loss = 0.90 (1595.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:01:57.514817: step 6360, loss = 0.94 (1606.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:01:58.307937: step 6370, loss = 0.81 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:01:59.106205: step 6380, loss = 1.00 (1603.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:01:59.892610: step 6390, loss = 1.06 (1627.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:00.794032: step 6400, loss = 0.92 (1420.0 examples/sec; 0.090 sec/batch)
2017-05-02 15:02:01.479771: step 6410, loss = 0.98 (1866.6 examples/sec; 0.069 sec/batch)
2017-05-02 15:02:02.271009: step 6420, loss = 0.89 (1617.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:03.063539: step 6430, loss = 0.89 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:03.851760: step 6440, loss = 0.90 (1623.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:04.645493: step 6450, loss = 1.04 (1612.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:05.447300: step 6460, loss = 0.98 (1596.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:02:06.246339: step 6470, loss = 0.99 (1601.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:02:07.038524: step 6480, loss = 0.94 (1615.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:07.826085: step 6490, loss = 0.85 (1625.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:08.727711: step 6500, loss = 0.86 (1419.7 examples/sec; 0.090 sec/batch)
2017-05-02 15:02:09.428112: step 6510, loss = 1.04 (1827.5 examples/sec; 0.070 sec/batch)
2017-05-02 15:02:10.234742: step 6520, loss = 1.03 (1586.8 examples/sec; 0.081 sec/batch)
2017-05-02 15:02:11.031235: step 6530, loss = 1.10 (1607.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:02:11.811564: step 6540, loss = 0.81 (1640.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:02:12.616317: step 6550, loss = 0.86 (1590.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:02:13.410852: step 6560, loss = 0.79 (1611.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:14.203349: step 6570, loss = 0.98 (1615.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:15.003071: step 6580, loss = 0.96 (1600.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:02:15.789503: step 6590, loss = 1.06 (1627.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:16.686479: step 6600, loss = 1.01 (1427.0 examples/sec; 0.090 sec/batch)
2017-05-02 15:02:17.381340: step 6610, loss = 0.91 (1842.1 examples/sec; 0.069 sec/batch)
2017-05-02 15:02:18.176421: step 6620, loss = 0.70 (1609.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:02:18.970475: step 6630, loss = 0.90 (1612.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:19.757906: step 6640, loss = 0.92 (1625.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:20.552383: step 6650, loss = 0.89 (1611.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:21.363747: step 6660, loss = 0.90 (1577.6 examples/sec; 0.081 sec/batch)
2017-05-02 15:02:22.152348: step 6670, loss = 0.91 (1623.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:22.945978: step 6680, loss = 1.02 (1612.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:23.732246: step 6690, loss = 1.01 (1628.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:24.634001: step 6700, loss = 0.85 (1419.4 examples/sec; 0.090 sec/batch)
2017-05-02 15:02:25.332657: step 6710, loss = 1.04 (1832.1 examples/sec; 0.070 sec/batch)
2017-05-02 15:02:26.127010: step 6720, loss = 0.90 (1611.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:26.932134: step 6730, loss = 0.78 (1589.8 examples/sec; 0.081 sec/batch)
2017-05-02 15:02:27.720448: step 6740, loss = 0.90 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:28.516492: step 6750, loss = 0.92 (1608.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:02:29.314619: step 6760, loss = 1.05 (1603.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:02:30.108794: step 6770, loss = 0.90 (1611.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:30.905117: step 6780, loss = 0.98 (1607.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:02:31.690833: step 6790, loss = 0.87 (1629.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:32.601743: step 6800, loss = 0.98 (1405.2 examples/sec; 0.091 sec/batch)
2017-05-02 15:02:33.284202: step 6810, loss = 1.09 (1875.6 examples/sec; 0.068 sec/batch)
2017-05-02 15:02:34.078977: step 6820, loss = 1.15 (1610.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:34.874577: step 6830, loss = 1.04 (1608.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:02:35.666219: step 6840, loss = 1.12 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:36.464794: step 6850, loss = 0.93 (1602.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:02:37.264053: step 6860, loss = 1.11 (1601.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:02:38.063013: step 6870, loss = 1.06 (1602.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:02:38.855975: step 6880, loss = 1.13 (1614.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:39.644483: step 6890, loss = 0.91 (1623.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:40.548716: step 6900, loss = 1.01 (1415.6 examples/sec; 0.090 sec/batch)
2017-05-02 15:02:41.240053: step 6910, loss = 0.77 (1851.5 examples/sec; 0.069 sec/batch)
2017-05-02 15:02:42.036112: step 6920, loss = 1.00 (1607.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:02:42.824975: step 6930, loss = 1.14 (1622.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:43.624676: step 6940, loss = 1.06 (1600.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:02:44.417508: step 6950, loss = 0.94 (1614.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:45.217633: step 6960, loss = 0.92 (1599.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:02:46.011988: step 6970, loss = 0.95 (1611.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:46.808107: step 6980, loss = 1.04 (1607.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:02:47.599115: step 6990, loss = 0.87 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:48.495718: step 7000, loss = 1.06 (1427.6 examples/sec; 0.090 sec/batch)
2017-05-02 15:02:49.192404: step 7010, loss = 0.78 (1837.3 examples/sec; 0.070 sec/batch)
2017-05-02 15:02:49.993762: step 7020, loss = 1.06 (1597.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:02:50.796925: step 7030, loss = 0.95 (1593.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:02:51.581860: step 7040, loss = 0.91 (1630.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:02:52.383370: step 7050, loss = 0.94 (1597.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:02:53.176390: step 7060, loss = 0.93 (1614.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:53.971267: step 7070, loss = 0.92 (1610.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:54.764139: step 7080, loss = 1.09 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:55.555058: step 7090, loss = 0.92 (1618.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:56.457784: step 7100, loss = 0.84 (1417.9 examples/sec; 0.090 sec/batch)
2017-05-02 15:02:57.156364: step 7110, loss = 0.93 (1832.3 examples/sec; 0.070 sec/batch)
2017-05-02 15:02:57.963408: step 7120, loss = 1.02 (1586.0 examples/sec; 0.081 sec/batch)
2017-05-02 15:02:58.757008: step 7130, loss = 0.81 (1612.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:02:59.547139: step 7140, loss = 1.07 (1620.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:03:00.347504: step 7150, loss = 0.83 (1599.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:03:01.140817: step 7160, loss = 0.79 (1613.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:03:01.937718: step 7170, loss = 1.19 (1606.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:03:02.736286: step 7180, loss = 0.82 (1602.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:03:03.517460: step 7190, loss = 1.03 (1638.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:03:04.409806: step 7200, loss = 0.95 (1434.4 examples/sec; 0.089 sec/batch)
2017-05-02 15:03:05.111759: step 7210, loss = 0.89 (1823.5 examples/sec; 0.070 sec/batch)
2017-05-02 15:03:05.904691: step 7220, loss = 0.81 (1614.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:03:06.691378: step 7230, loss = 0.99 (1627.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:03:07.486688: step 7240, loss = 0.92 (1609.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:03:08.281103: step 7250, loss = 1.01 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:03:09.067765: step 7260, loss = 0.92 (1627.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:03:09.867538: step 7270, loss = 0.91 (1600.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:03:10.664088: step 7280, loss = 0.81 (1606.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:03:11.454683: step 7290, loss = 1.01 (1619.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:03:12.356024: step 7300, loss = 1.03 (1420.1 examples/sec; 0.090 sec/batch)
2017-05-02 15:03:13.052496: step 7310, loss = 1.02 (1837.8 examples/sec; 0.070 sec/batch)
2017-05-02 15:03:13.844362: step 7320, loss = 0.90 (1616.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:03:14.647419: step 7330, loss = 0.82 (1593.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:03:15.433362: step 7340, loss = 0.80 (1628.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:03:16.225055: step 7350, loss = 0.75 (1616.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:03:17.022786: step 7360, loss = 0.88 (1604.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:03:17.821463: step 7370, loss = 0.88 (1602.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:03:18.617150: step 7380, loss = 0.78 (1608.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:03:19.406048: step 7390, loss = 0.86 (1622.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:03:20.298398: step 7400, loss = 0.85 (1434.4 examples/sec; 0.089 sec/batch)
2017-05-02 15:03:20.998416: step 7410, loss = 0.87 (1828.5 examples/sec; 0.070 sec/batch)
2017-05-02 15:03:21.797645: step 7420, loss = 0.82 (1601.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:03:22.593051: step 7430, loss = 0.99 (1609.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:03:23.385656: step 7440, loss = 0.90 (1614.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:03:24.176769: step 7450, loss = 0.88 (1618.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:03:24.970100: step 7460, loss = 0.91 (1613.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:03:25.768364: step 7470, loss = 0.97 (1603.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:03:26.557412: step 7480, loss = 0.92 (1622.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:03:27.341201: step 7490, loss = 1.04 (1633.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:03:28.250057: step 7500, loss = 1.04 (1408.4 examples/sec; 0.091 sec/batch)
2017-05-02 15:03:28.927129: step 7510, loss = 0.85 (1890.5 examples/sec; 0.068 sec/batch)
2017-05-02 15:03:29.751979: step 7520, loss = 1.10 (1551.8 examples/sec; 0.082 sec/batch)
2017-05-02 15:03:30.549935: step 7530, loss = 1.05 (1604.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:03:31.352123: step 7540, loss = 0.85 (1595.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:03:32.140218: step 7550, loss = 0.98 (1624.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:03:32.941719: step 7560, loss = 0.90 (1597.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:03:33.740587: step 7570, loss = 0.96 (1602.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:03:34.545015: step 7580, loss = 0.90 (1591.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:03:35.343860: step 7590, loss = 0.96 (1602.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:03:36.251477: step 7600, loss = 0.83 (1410.3 examples/sec; 0.091 sec/batch)
2017-05-02 15:03:36.932155: step 7610, loss = 0.85 (1880.5 examples/sec; 0.068 sec/batch)
2017-05-02 15:03:37.734996: step 7620, loss = 0.87 (1594.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:03:38.536524: step 7630, loss = 1.07 (1596.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:03:39.331736: step 7640, loss = 0.90 (1609.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:03:40.122075: step 7650, loss = 0.94 (1619.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:03:40.916550: step 7660, loss = 0.79 (1611.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:03:41.719410: step 7670, loss = 0.78 (1594.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:03:42.507661: step 7680, loss = 0.87 (1623.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:03:43.302404: step 7690, loss = 0.89 (1610.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:03:44.218553: step 7700, loss = 1.04 (1397.2 examples/sec; 0.092 sec/batch)
2017-05-02 15:03:44.892023: step 7710, loss = 1.00 (1900.6 examples/sec; 0.067 sec/batch)
2017-05-02 15:03:45.686790: step 7720, loss = 0.92 (1610.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:03:46.483978: step 7730, loss = 0.93 (1605.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:03:47.275038: step 7740, loss = 0.87 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:03:48.062260: step 7750, loss = 0.86 (1626.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:03:48.853353: step 7760, loss = 1.01 (1618.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:03:49.655474: step 7770, loss = 1.08 (1595.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:03:50.448603: step 7780, loss = 1.09 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:03:51.253991: step 7790, loss = 0.92 (1589.3 examples/sec; 0.081 sec/batch)
2017-05-02 15:03:52.152848: step 7800, loss = 0.89 (1424.0 examples/sec; 0.090 sec/batch)
2017-05-02 15:03:52.850628: step 7810, loss = 0.83 (1834.4 examples/sec; 0.070 sec/batch)
2017-05-02 15:03:53.649968: step 7820, loss = 1.03 (1601.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:03:54.440392: step 7830, loss = 1.00 (1619.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:03:55.238840: step 7840, loss = 1.05 (1603.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:03:56.028670: step 7850, loss = 1.02 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:03:56.828379: step 7860, loss = 0.99 (1600.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:03:57.629510: step 7870, loss = 1.10 (1597.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:03:58.426619: step 7880, loss = 0.96 (1605.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:03:59.223725: step 7890, loss = 1.27 (1605.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:00.116430: step 7900, loss = 0.92 (1433.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:04:00.808015: step 7910, loss = 0.83 (1850.9 examples/sec; 0.069 sec/batch)
2017-05-02 15:04:01.599360: step 7920, loss = 0.82 (1617.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:04:02.390607: step 7930, loss = 1.15 (1617.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:04:03.178389: step 7940, loss = 0.82 (1624.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:04:03.958305: step 7950, loss = 0.91 (1641.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:04:04.753573: step 7960, loss = 0.88 (1609.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:05.540524: step 7970, loss = 1.00 (1626.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:04:06.334861: step 7980, loss = 0.87 (1611.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:04:07.133427: step 7990, loss = 0.95 (1602.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:08.023602: step 8000, loss = 1.03 (1437.9 examples/sec; 0.089 sec/batch)
2017-05-02 15:04:08.709812: step 8010, loss = 0.86 (1865.3 examples/sec; 0.069 sec/batch)
2017-05-02 15:04:09.503301: step 8020, loss = 0.92 (1613.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:04:10.303309: step 8030, loss = 0.93 (1600.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:11.094771: step 8040, loss = 0.91 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:04:11.885437: step 8050, loss = 0.88 (1618.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:04:12.683165: step 8060, loss = 0.84 (1604.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:13.475132: step 8070, loss = 0.79 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:04:14.265149: step 8080, loss = 0.82 (1620.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:04:15.059394: step 8090, loss = 1.03 (1611.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:04:15.952019: step 8100, loss = 1.01 (1434.0 examples/sec; 0.089 sec/batch)
2017-05-02 15:04:16.653064: step 8110, loss = 0.81 (1825.8 examples/sec; 0.070 sec/batch)
2017-05-02 15:04:17.459486: step 8120, loss = 0.83 (1587.3 examples/sec; 0.081 sec/batch)
2017-05-02 15:04:18.259199: step 8130, loss = 1.02 (1600.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:19.059439: step 8140, loss = 0.92 (1599.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:19.846663: step 8150, loss = 0.95 (1626.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:04:20.641705: step 8160, loss = 0.98 (1610.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:21.437365: step 8170, loss = 0.84 (1608.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:22.230522: step 8180, loss = 1.06 (1613.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:04:23.026589: step 8190, loss = 1.05 (1607.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:23.941302: step 8200, loss = 0.97 (1399.3 examples/sec; 0.091 sec/batch)
2017-05-02 15:04:24.615945: step 8210, loss = 0.78 (1897.3 examples/sec; 0.067 sec/batch)
2017-05-02 15:04:25.411400: step 8220, loss = 0.88 (1609.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:26.207309: step 8230, loss = 0.91 (1608.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:27.006714: step 8240, loss = 1.01 (1601.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:27.794445: step 8250, loss = 0.89 (1624.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:04:28.594505: step 8260, loss = 0.97 (1599.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:29.390153: step 8270, loss = 0.79 (1608.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:30.186212: step 8280, loss = 1.26 (1607.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:30.990386: step 8290, loss = 1.00 (1591.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:31.885244: step 8300, loss = 1.02 (1430.4 examples/sec; 0.089 sec/batch)
2017-05-02 15:04:32.581036: step 8310, loss = 1.00 (1839.6 examples/sec; 0.070 sec/batch)
2017-05-02 15:04:33.380157: step 8320, loss = 1.00 (1601.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:34.180682: step 8330, loss = 0.88 (1599.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:34.978916: step 8340, loss = 0.86 (1603.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:35.767536: step 8350, loss = 0.74 (1623.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:04:36.565564: step 8360, loss = 1.05 (1603.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:37.362018: step 8370, loss = 0.87 (1607.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:38.153853: step 8380, loss = 1.06 (1616.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:04:38.954968: step 8390, loss = 0.95 (1598.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:39.839103: step 8400, loss = 1.03 (1446.9 examples/sec; 0.088 sec/batch)
2017-05-02 15:04:40.541074: step 8410, loss = 0.79 (1823.4 examples/sec; 0.070 sec/batch)
2017-05-02 15:04:41.335285: step 8420, loss = 0.79 (1611.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:04:42.128177: step 8430, loss = 1.00 (1614.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:04:42.932750: step 8440, loss = 0.79 (1590.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:43.715868: step 8450, loss = 0.89 (1634.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:04:44.514341: step 8460, loss = 1.14 (1603.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:45.315190: step 8470, loss = 1.06 (1598.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:46.111626: step 8480, loss = 0.93 (1607.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:46.907995: step 8490, loss = 0.99 (1607.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:47.798845: step 8500, loss = 0.90 (1436.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:04:48.497519: step 8510, loss = 0.85 (1832.0 examples/sec; 0.070 sec/batch)
2017-05-02 15:04:49.296030: step 8520, loss = 1.02 (1603.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:50.095341: step 8530, loss = 0.87 (1601.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:50.892503: step 8540, loss = 0.88 (1605.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:51.685973: step 8550, loss = 0.85 (1613.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:04:52.480282: step 8560, loss = 1.03 (1611.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:04:53.276510: step 8570, loss = 0.81 (1607.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:54.075267: step 8580, loss = 0.82 (1602.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:54.871081: step 8590, loss = 0.83 (1608.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:55.759479: step 8600, loss = 0.97 (1440.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:04:56.473720: step 8610, loss = 0.97 (1792.1 examples/sec; 0.071 sec/batch)
2017-05-02 15:04:57.273669: step 8620, loss = 0.95 (1600.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:58.074570: step 8630, loss = 0.82 (1598.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:04:58.869434: step 8640, loss = 0.74 (1610.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:04:59.657204: step 8650, loss = 0.87 (1624.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:00.457794: step 8660, loss = 0.95 (1598.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:05:01.258816: step 8670, loss = 1.03 (1598.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:05:02.049202: step 8680, loss = 0.94 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:02.847095: step 8690, loss = 0.82 (1604.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:05:03.734427: step 8700, loss = 0.89 (1442.5 examples/sec; 0.089 sec/batch)
2017-05-02 15:05:04.429435: step 8710, loss = 0.84 (1841.7 examples/sec; 0.070 sec/batch)
2017-05-02 15:05:05.219250: step 8720, loss = 0.97 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:06.015565: step 8730, loss = 0.86 (1607.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:05:06.810446: step 8740, loss = 1.09 (1610.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:07.595432: step 8750, loss = 1.16 (1630.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:05:08.390316: step 8760, loss = 0.87 (1610.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:09.183371: step 8770, loss = 0.92 (1614.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:09.981796: step 8780, loss = 0.94 (1603.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:05:10.779832: step 8790, loss = 0.83 (1603.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:05:11.665614: step 8800, loss = 0.85 (1445.0 examples/sec; 0.089 sec/batch)
2017-05-02 15:05:12.362770: step 8810, loss = 0.91 (1836.0 examples/sec; 0.070 sec/batch)
2017-05-02 15:05:13.148266: step 8820, loss = 0.79 (1629.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:13.939123: step 8830, loss = 0.96 (1618.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:14.732311: step 8840, loss = 0.78 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:15.517133: step 8850, loss = 1.02 (1631.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:05:16.307270: step 8860, loss = 0.76 (1620.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:17.100055: step 8870, loss = 0.91 (1614.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:17.894020: step 8880, loss = 0.86 (1612.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:18.687744: step 8890, loss = 1.05 (1612.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:19.595485: step 8900, loss = 0.92 (1410.1 examples/sec; 0.091 sec/batch)
2017-05-02 15:05:20.266630: step 8910, loss = 1.01 (1907.2 examples/sec; 0.067 sec/batch)
2017-05-02 15:05:21.059643: step 8920, loss = 0.92 (1614.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:21.852557: step 8930, loss = 1.00 (1614.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:22.655703: step 8940, loss = 0.90 (1593.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:05:23.443147: step 8950, loss = 0.97 (1625.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:24.235063: step 8960, loss = 0.79 (1616.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:25.041099: step 8970, loss = 0.97 (1588.0 examples/sec; 0.081 sec/batch)
2017-05-02 15:05:25.834435: step 8980, loss = 0.79 (1613.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:26.631058: step 8990, loss = 0.64 (1606.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:05:27.525931: step 9000, loss = 0.83 (1430.4 examples/sec; 0.089 sec/batch)
2017-05-02 15:05:28.211737: step 9010, loss = 0.77 (1866.4 examples/sec; 0.069 sec/batch)
2017-05-02 15:05:29.004718: step 9020, loss = 0.98 (1614.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:29.798211: step 9030, loss = 1.01 (1613.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:30.598471: step 9040, loss = 0.87 (1599.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:05:31.391147: step 9050, loss = 0.87 (1614.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:32.174158: step 9060, loss = 0.80 (1634.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:05:32.972106: step 9070, loss = 0.86 (1604.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:05:33.765454: step 9080, loss = 0.85 (1613.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:34.560095: step 9090, loss = 1.04 (1610.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:35.460350: step 9100, loss = 1.23 (1421.8 examples/sec; 0.090 sec/batch)
2017-05-02 15:05:36.138276: step 9110, loss = 0.87 (1888.1 examples/sec; 0.068 sec/batch)
2017-05-02 15:05:36.946649: step 9120, loss = 0.95 (1583.4 examples/sec; 0.081 sec/batch)
2017-05-02 15:05:37.747700: step 9130, loss = 0.70 (1597.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:05:38.542768: step 9140, loss = 0.93 (1609.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:05:39.335139: step 9150, loss = 1.02 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:40.121506: step 9160, loss = 0.92 (1627.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:40.911272: step 9170, loss = 0.79 (1620.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:41.708761: step 9180, loss = 0.92 (1605.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:05:42.504653: step 9190, loss = 1.04 (1608.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:05:43.395535: step 9200, loss = 0.76 (1436.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:05:44.098219: step 9210, loss = 0.86 (1821.6 examples/sec; 0.070 sec/batch)
2017-05-02 15:05:44.898343: step 9220, loss = 0.84 (1599.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:05:45.692410: step 9230, loss = 0.90 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:46.489615: step 9240, loss = 1.05 (1605.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:05:47.291407: step 9250, loss = 0.91 (1596.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:05:48.075394: step 9260, loss = 1.03 (1632.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:05:48.872171: step 9270, loss = 0.94 (1606.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:05:49.668824: step 9280, loss = 0.93 (1606.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:05:50.463401: step 9290, loss = 0.80 (1610.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:51.362952: step 9300, loss = 0.85 (1422.9 examples/sec; 0.090 sec/batch)
2017-05-02 15:05:52.056560: step 9310, loss = 0.83 (1845.4 examples/sec; 0.069 sec/batch)
2017-05-02 15:05:52.865343: step 9320, loss = 0.80 (1582.6 examples/sec; 0.081 sec/batch)
2017-05-02 15:05:53.662113: step 9330, loss = 0.85 (1606.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:05:54.456222: step 9340, loss = 0.89 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:55.250942: step 9350, loss = 0.93 (1610.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:56.036963: step 9360, loss = 0.93 (1628.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:56.828013: step 9370, loss = 0.93 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:57.622613: step 9380, loss = 0.91 (1610.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:58.414810: step 9390, loss = 0.84 (1615.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:05:59.314122: step 9400, loss = 0.96 (1423.3 examples/sec; 0.090 sec/batch)
2017-05-02 15:05:59.995053: step 9410, loss = 1.08 (1879.8 examples/sec; 0.068 sec/batch)
2017-05-02 15:06:00.787841: step 9420, loss = 0.77 (1614.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:01.579920: step 9430, loss = 0.93 (1616.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:02.370969: step 9440, loss = 0.91 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:03.166091: step 9450, loss = 0.82 (1609.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:06:03.955112: step 9460, loss = 0.88 (1622.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:04.744405: step 9470, loss = 0.89 (1621.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:05.530077: step 9480, loss = 0.87 (1629.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:06.328979: step 9490, loss = 0.90 (1602.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:06:07.217608: step 9500, loss = 1.08 (1440.4 examples/sec; 0.089 sec/batch)
2017-05-02 15:06:07.919266: step 9510, loss = 0.98 (1824.2 examples/sec; 0.070 sec/batch)
2017-05-02 15:06:08.709233: step 9520, loss = 0.79 (1620.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:09.499277: step 9530, loss = 0.84 (1620.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:10.298691: step 9540, loss = 0.87 (1601.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:06:11.099443: step 9550, loss = 0.92 (1598.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:06:11.883180: step 9560, loss = 0.88 (1633.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:06:12.685576: step 9570, loss = 0.73 (1595.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:06:13.481687: step 9580, loss = 0.93 (1607.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:06:14.275962: step 9590, loss = 1.09 (1611.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:15.180667: step 9600, loss = 0.89 (1414.8 examples/sec; 0.090 sec/batch)
2017-05-02 15:06:15.856005: step 9610, loss = 1.08 (1895.4 examples/sec; 0.068 sec/batch)
2017-05-02 15:06:16.654476: step 9620, loss = 0.98 (1603.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:06:17.447292: step 9630, loss = 0.96 (1614.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:18.246899: step 9640, loss = 0.87 (1600.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:06:19.039252: step 9650, loss = 1.04 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:19.831503: step 9660, loss = 0.68 (1615.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:20.625602: step 9670, loss = 0.72 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:21.414328: step 9680, loss = 1.04 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:22.209653: step 9690, loss = 1.04 (1609.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:06:23.109459: step 9700, loss = 0.89 (1422.5 examples/sec; 0.090 sec/batch)
2017-05-02 15:06:23.789930: step 9710, loss = 1.04 (1881.0 examples/sec; 0.068 sec/batch)
2017-05-02 15:06:24.588463: step 9720, loss = 0.99 (1602.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:06:25.383023: step 9730, loss = 0.95 (1611.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:26.179181: step 9740, loss = 1.04 (1607.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:06:26.972485: step 9750, loss = 1.06 (1613.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:27.758352: step 9760, loss = 0.78 (1628.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:28.548509: step 9770, loss = 0.98 (1619.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:29.345446: step 9780, loss = 1.11 (1606.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:06:30.137808: step 9790, loss = 0.87 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:31.030857: step 9800, loss = 0.93 (1433.3 examples/sec; 0.089 sec/batch)
2017-05-02 15:06:31.716166: step 9810, loss = 0.84 (1867.8 examples/sec; 0.069 sec/batch)
2017-05-02 15:06:32.503285: step 9820, loss = 0.87 (1626.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:33.301963: step 9830, loss = 0.96 (1602.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:06:34.090018: step 9840, loss = 1.06 (1624.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:34.879680: step 9850, loss = 0.94 (1620.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:35.665192: step 9860, loss = 0.89 (1629.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:36.449606: step 9870, loss = 0.74 (1631.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:06:37.241216: step 9880, loss = 0.98 (1617.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:38.028578: step 9890, loss = 1.02 (1625.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:38.928399: step 9900, loss = 0.95 (1422.5 examples/sec; 0.090 sec/batch)
2017-05-02 15:06:39.609495: step 9910, loss = 0.83 (1879.3 examples/sec; 0.068 sec/batch)
2017-05-02 15:06:40.400344: step 9920, loss = 0.94 (1618.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:41.187760: step 9930, loss = 0.93 (1625.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:41.981122: step 9940, loss = 0.97 (1613.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:42.777115: step 9950, loss = 0.84 (1608.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:06:43.561456: step 9960, loss = 0.90 (1631.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:06:44.355900: step 9970, loss = 0.79 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:45.146919: step 9980, loss = 0.87 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:45.944433: step 9990, loss = 0.88 (1605.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:06:46.831222: step 10000, loss = 0.91 (1443.4 examples/sec; 0.089 sec/batch)
2017-05-02 15:06:47.525413: step 10010, loss = 0.96 (1843.9 examples/sec; 0.069 sec/batch)
2017-05-02 15:06:48.321041: step 10020, loss = 0.89 (1608.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:06:49.111841: step 10030, loss = 0.94 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:49.898497: step 10040, loss = 0.86 (1627.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:50.694843: step 10050, loss = 1.15 (1607.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:06:51.487721: step 10060, loss = 0.76 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:52.272776: step 10070, loss = 0.91 (1630.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:53.063573: step 10080, loss = 0.87 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:53.855214: step 10090, loss = 0.89 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:54.766183: step 10100, loss = 1.08 (1405.1 examples/sec; 0.091 sec/batch)
2017-05-02 15:06:55.438926: step 10110, loss = 0.95 (1902.7 examples/sec; 0.067 sec/batch)
2017-05-02 15:06:56.219011: step 10120, loss = 0.70 (1640.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:06:57.014561: step 10130, loss = 0.90 (1609.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:06:57.808604: step 10140, loss = 0.91 (1612.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:58.602993: step 10150, loss = 0.98 (1611.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:06:59.394344: step 10160, loss = 0.96 (1617.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:00.184360: step 10170, loss = 0.71 (1620.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:00.968471: step 10180, loss = 0.85 (1632.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:07:01.768682: step 10190, loss = 1.03 (1599.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:07:02.656982: step 10200, loss = 0.95 (1441.0 examples/sec; 0.089 sec/batch)
2017-05-02 15:07:03.353172: step 10210, loss = 0.75 (1838.6 examples/sec; 0.070 sec/batch)
2017-05-02 15:07:04.141483: step 10220, loss = 0.86 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:04.933871: step 10230, loss = 0.95 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:05.731860: step 10240, loss = 1.02 (1604.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:07:06.526167: step 10250, loss = 0.78 (1611.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:07.317205: step 10260, loss = 1.00 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:08.110322: step 10270, loss = 0.64 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:08.910787: step 10280, loss = 1.09 (1599.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:07:09.713938: step 10290, loss = 0.80 (1593.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:07:10.609022: step 10300, loss = 0.94 (1430.0 examples/sec; 0.090 sec/batch)
2017-05-02 15:07:11.300566: step 10310, loss = 0.79 (1850.9 examples/sec; 0.069 sec/batch)
2017-05-02 15:07:12.088973: step 10320, loss = 0.88 (1623.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:12.883792: step 10330, loss = 0.86 (1610.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:13.672830: step 10340, loss = 1.13 (1622.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:14.463573: step 10350, loss = 0.79 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:15.253259: step 10360, loss = 1.00 (1620.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:16.041568: step 10370, loss = 0.88 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:16.832410: step 10380, loss = 0.84 (1618.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:17.621767: step 10390, loss = 0.86 (1621.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:18.513058: step 10400, loss = 0.86 (1436.1 examples/sec; 0.089 sec/batch)
2017-05-02 15:07:19.200042: step 10410, loss = 0.65 (1863.2 examples/sec; 0.069 sec/batch)
2017-05-02 15:07:19.986007: step 10420, loss = 1.03 (1628.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:20.783870: step 10430, loss = 0.76 (1604.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:07:21.571429: step 10440, loss = 0.81 (1625.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:22.364298: step 10450, loss = 0.99 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:23.155611: step 10460, loss = 0.79 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:23.934161: step 10470, loss = 0.86 (1644.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:07:24.727102: step 10480, loss = 1.12 (1614.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:25.528440: step 10490, loss = 0.87 (1597.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:07:26.425832: step 10500, loss = 0.90 (1426.4 examples/sec; 0.090 sec/batch)
2017-05-02 15:07:27.121200: step 10510, loss = 0.87 (1840.7 examples/sec; 0.070 sec/batch)
2017-05-02 15:07:27.906708: step 10520, loss = 0.91 (1629.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:28.698483: step 10530, loss = 0.83 (1616.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:29.491991: step 10540, loss = 0.88 (1613.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:30.289021: step 10550, loss = 0.80 (1606.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:07:31.091660: step 10560, loss = 0.78 (1594.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:07:31.864557: step 10570, loss = 0.95 (1656.1 examples/sec; 0.077 sec/batch)
2017-05-02 15:07:32.663311: step 10580, loss = 0.86 (1602.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:07:33.454856: step 10590, loss = 0.90 (1617.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:34.351420: step 10600, loss = 0.94 (1427.7 examples/sec; 0.090 sec/batch)
2017-05-02 15:07:35.039719: step 10610, loss = 0.98 (1859.7 examples/sec; 0.069 sec/batch)
2017-05-02 15:07:35.821947: step 10620, loss = 0.83 (1636.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:07:36.615200: step 10630, loss = 0.70 (1613.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:37.406389: step 10640, loss = 0.88 (1617.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:38.201752: step 10650, loss = 0.79 (1609.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:07:38.993260: step 10660, loss = 1.07 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:39.773783: step 10670, loss = 0.83 (1639.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:07:40.564285: step 10680, loss = 0.71 (1619.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:41.357146: step 10690, loss = 0.89 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:42.260872: step 10700, loss = 0.85 (1416.4 examples/sec; 0.090 sec/batch)
2017-05-02 15:07:42.941642: step 10710, loss = 0.79 (1880.2 examples/sec; 0.068 sec/batch)
2017-05-02 15:07:43.723707: step 10720, loss = 0.96 (1636.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:07:44.516675: step 10730, loss = 0.84 (1614.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:45.299211: step 10740, loss = 0.89 (1635.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:07:46.087475: step 10750, loss = 0.86 (1623.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:46.877461: step 10760, loss = 0.95 (1620.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:47.662965: step 10770, loss = 0.87 (1629.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:48.456354: step 10780, loss = 0.80 (1613.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:49.244501: step 10790, loss = 1.06 (1624.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:50.140598: step 10800, loss = 0.86 (1428.4 examples/sec; 0.090 sec/batch)
2017-05-02 15:07:50.834707: step 10810, loss = 1.04 (1844.1 examples/sec; 0.069 sec/batch)
2017-05-02 15:07:51.626291: step 10820, loss = 0.74 (1617.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:52.415857: step 10830, loss = 0.94 (1621.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:53.217973: step 10840, loss = 1.06 (1596.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:07:54.010787: step 10850, loss = 0.86 (1613.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:54.802914: step 10860, loss = 0.95 (1615.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:55.592798: step 10870, loss = 0.78 (1620.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:56.386463: step 10880, loss = 0.83 (1612.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:07:57.170173: step 10890, loss = 1.08 (1633.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:07:58.075032: step 10900, loss = 0.89 (1414.6 examples/sec; 0.090 sec/batch)
2017-05-02 15:07:58.760305: step 10910, loss = 0.80 (1867.9 examples/sec; 0.069 sec/batch)
2017-05-02 15:07:59.539433: step 10920, loss = 0.93 (1642.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:08:00.327472: step 10930, loss = 0.85 (1624.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:01.119985: step 10940, loss = 0.86 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:01.908864: step 10950, loss = 0.82 (1622.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:02.702272: step 10960, loss = 0.87 (1613.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:03.484030: step 10970, loss = 1.01 (1637.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:08:04.270400: step 10980, loss = 0.93 (1627.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:05.067750: step 10990, loss = 1.05 (1605.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:08:05.980376: step 11000, loss = 0.81 (1402.5 examples/sec; 0.091 sec/batch)
2017-05-02 15:08:06.660746: step 11010, loss = 0.81 (1881.3 examples/sec; 0.068 sec/batch)
2017-05-02 15:08:07.448409: step 11020, loss = 0.93 (1625.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:08.237512: step 11030, loss = 0.89 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:09.030361: step 11040, loss = 1.11 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:09.827749: step 11050, loss = 1.08 (1605.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:08:10.626694: step 11060, loss = 0.93 (1602.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:08:11.414459: step 11070, loss = 0.99 (1624.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:12.215578: step 11080, loss = 0.84 (1597.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:08:13.011608: step 11090, loss = 0.75 (1608.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:08:13.928727: step 11100, loss = 0.83 (1395.7 examples/sec; 0.092 sec/batch)
2017-05-02 15:08:14.600943: step 11110, loss = 0.92 (1904.2 examples/sec; 0.067 sec/batch)
2017-05-02 15:08:15.385389: step 11120, loss = 0.86 (1631.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:08:16.165487: step 11130, loss = 0.66 (1640.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:08:16.960609: step 11140, loss = 0.82 (1609.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:08:17.759224: step 11150, loss = 0.79 (1602.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:08:18.555849: step 11160, loss = 1.02 (1606.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:08:19.339956: step 11170, loss = 0.93 (1632.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:08:20.122042: step 11180, loss = 0.84 (1636.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:08:20.915055: step 11190, loss = 0.93 (1614.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:21.808375: step 11200, loss = 0.96 (1432.9 examples/sec; 0.089 sec/batch)
2017-05-02 15:08:22.502345: step 11210, loss = 0.84 (1844.5 examples/sec; 0.069 sec/batch)
2017-05-02 15:08:23.290908: step 11220, loss = 0.85 (1623.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:24.076028: step 11230, loss = 0.85 (1630.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:24.859457: step 11240, loss = 0.92 (1633.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:08:25.647579: step 11250, loss = 0.97 (1624.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:26.438405: step 11260, loss = 0.88 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:27.236516: step 11270, loss = 0.85 (1603.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:08:28.014347: step 11280, loss = 0.99 (1645.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:08:28.801444: step 11290, loss = 0.90 (1626.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:29.718749: step 11300, loss = 0.90 (1395.4 examples/sec; 0.092 sec/batch)
2017-05-02 15:08:30.380401: step 11310, loss = 0.82 (1934.5 examples/sec; 0.066 sec/batch)
2017-05-02 15:08:31.172279: step 11320, loss = 0.98 (1616.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:31.957017: step 11330, loss = 0.93 (1631.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:08:32.759702: step 11340, loss = 0.85 (1594.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:08:33.552186: step 11350, loss = 0.94 (1615.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:34.339314: step 11360, loss = 1.07 (1626.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:35.137100: step 11370, loss = 0.86 (1604.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:08:35.930356: step 11380, loss = 0.99 (1613.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:36.725534: step 11390, loss = 0.79 (1609.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:08:37.627685: step 11400, loss = 0.96 (1418.8 examples/sec; 0.090 sec/batch)
2017-05-02 15:08:38.319916: step 11410, loss = 0.89 (1849.1 examples/sec; 0.069 sec/batch)
2017-05-02 15:08:39.107823: step 11420, loss = 0.82 (1624.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:39.896486: step 11430, loss = 0.74 (1623.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:40.693216: step 11440, loss = 0.78 (1606.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:08:41.479300: step 11450, loss = 0.73 (1628.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:42.271612: step 11460, loss = 0.82 (1615.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:43.061486: step 11470, loss = 0.93 (1620.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:43.837870: step 11480, loss = 0.86 (1648.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:08:44.632966: step 11490, loss = 0.81 (1609.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:08:45.524416: step 11500, loss = 0.78 (1435.9 examples/sec; 0.089 sec/batch)
2017-05-02 15:08:46.218088: step 11510, loss = 0.81 (1845.3 examples/sec; 0.069 sec/batch)
2017-05-02 15:08:47.011938: step 11520, loss = 0.84 (1612.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:47.798032: step 11530, loss = 0.88 (1628.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:48.592174: step 11540, loss = 1.04 (1611.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:49.379426: step 11550, loss = 0.87 (1625.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:50.173294: step 11560, loss = 1.12 (1612.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:50.964280: step 11570, loss = 0.79 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:51.748562: step 11580, loss = 1.04 (1632.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:08:52.543873: step 11590, loss = 0.66 (1609.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:08:53.436276: step 11600, loss = 1.05 (1434.3 examples/sec; 0.089 sec/batch)
2017-05-02 15:08:54.133690: step 11610, loss = 0.87 (1835.3 examples/sec; 0.070 sec/batch)
2017-05-02 15:08:54.922734: step 11620, loss = 1.10 (1622.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:55.702111: step 11630, loss = 0.86 (1642.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:08:56.496391: step 11640, loss = 0.85 (1611.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:57.291797: step 11650, loss = 0.88 (1609.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:08:58.079252: step 11660, loss = 1.00 (1625.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:58.869143: step 11670, loss = 0.78 (1620.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:08:59.654342: step 11680, loss = 0.80 (1630.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:00.452089: step 11690, loss = 0.95 (1604.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:09:01.345475: step 11700, loss = 0.79 (1432.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:09:02.033485: step 11710, loss = 0.80 (1860.4 examples/sec; 0.069 sec/batch)
2017-05-02 15:09:02.827815: step 11720, loss = 0.81 (1611.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:03.605940: step 11730, loss = 0.78 (1645.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:09:04.396968: step 11740, loss = 0.82 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:05.185532: step 11750, loss = 0.87 (1623.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:05.982985: step 11760, loss = 0.94 (1605.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:09:06.774580: step 11770, loss = 0.79 (1617.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:07.563832: step 11780, loss = 0.80 (1621.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:08.353769: step 11790, loss = 0.88 (1620.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:09.254710: step 11800, loss = 0.93 (1420.7 examples/sec; 0.090 sec/batch)
2017-05-02 15:09:09.939957: step 11810, loss = 0.79 (1867.9 examples/sec; 0.069 sec/batch)
2017-05-02 15:09:10.730120: step 11820, loss = 0.89 (1619.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:11.519042: step 11830, loss = 0.66 (1622.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:12.295385: step 11840, loss = 0.92 (1648.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:09:13.088443: step 11850, loss = 1.00 (1614.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:13.881772: step 11860, loss = 0.84 (1613.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:14.669278: step 11870, loss = 1.01 (1625.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:15.457319: step 11880, loss = 0.96 (1624.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:16.244644: step 11890, loss = 0.92 (1625.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:17.142004: step 11900, loss = 0.97 (1426.4 examples/sec; 0.090 sec/batch)
2017-05-02 15:09:17.831245: step 11910, loss = 0.76 (1857.1 examples/sec; 0.069 sec/batch)
2017-05-02 15:09:18.623574: step 11920, loss = 1.11 (1615.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:19.426327: step 11930, loss = 0.77 (1594.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:09:20.219543: step 11940, loss = 0.83 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:21.007129: step 11950, loss = 0.89 (1625.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:21.805995: step 11960, loss = 0.86 (1602.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:09:22.600530: step 11970, loss = 0.82 (1611.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:23.395826: step 11980, loss = 0.85 (1609.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:09:24.176711: step 11990, loss = 0.91 (1639.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:09:25.097725: step 12000, loss = 1.02 (1389.8 examples/sec; 0.092 sec/batch)
2017-05-02 15:09:25.771802: step 12010, loss = 0.77 (1898.9 examples/sec; 0.067 sec/batch)
2017-05-02 15:09:26.566009: step 12020, loss = 0.89 (1611.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:27.351614: step 12030, loss = 0.94 (1629.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:28.137189: step 12040, loss = 0.84 (1629.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:28.933434: step 12050, loss = 0.98 (1607.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:09:29.727953: step 12060, loss = 0.79 (1611.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:30.521930: step 12070, loss = 0.93 (1612.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:31.307433: step 12080, loss = 0.75 (1629.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:32.086140: step 12090, loss = 0.77 (1643.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:09:32.975779: step 12100, loss = 0.82 (1438.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:09:33.669494: step 12110, loss = 0.85 (1845.1 examples/sec; 0.069 sec/batch)
2017-05-02 15:09:34.461436: step 12120, loss = 0.89 (1616.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:35.253169: step 12130, loss = 0.78 (1616.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:36.041193: step 12140, loss = 0.81 (1624.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:36.843355: step 12150, loss = 0.97 (1595.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:09:37.639964: step 12160, loss = 1.05 (1606.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:09:38.429981: step 12170, loss = 1.04 (1620.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:39.221966: step 12180, loss = 0.72 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:40.011937: step 12190, loss = 0.86 (1620.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:40.916743: step 12200, loss = 1.02 (1414.7 examples/sec; 0.090 sec/batch)
2017-05-02 15:09:41.591000: step 12210, loss = 0.94 (1898.4 examples/sec; 0.067 sec/batch)
2017-05-02 15:09:42.387793: step 12220, loss = 0.83 (1606.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:09:43.175817: step 12230, loss = 0.94 (1624.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:43.960267: step 12240, loss = 0.85 (1631.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:09:44.748720: step 12250, loss = 0.91 (1623.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:45.541836: step 12260, loss = 0.97 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:46.336220: step 12270, loss = 0.85 (1611.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:47.128204: step 12280, loss = 1.03 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:47.911651: step 12290, loss = 0.80 (1633.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:09:48.806619: step 12300, loss = 0.86 (1430.2 examples/sec; 0.089 sec/batch)
2017-05-02 15:09:49.505015: step 12310, loss = 0.94 (1832.8 examples/sec; 0.070 sec/batch)
2017-05-02 15:09:50.302211: step 12320, loss = 1.01 (1605.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:09:51.093166: step 12330, loss = 0.90 (1618.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:51.877760: step 12340, loss = 0.78 (1631.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:09:52.661440: step 12350, loss = 0.73 (1633.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:09:53.458412: step 12360, loss = 0.96 (1606.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:09:54.256910: step 12370, loss = 1.09 (1603.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:09:55.053870: step 12380, loss = 0.92 (1606.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:09:55.838160: step 12390, loss = 0.77 (1632.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:09:56.732590: step 12400, loss = 0.81 (1431.1 examples/sec; 0.089 sec/batch)
2017-05-02 15:09:57.432441: step 12410, loss = 1.21 (1829.0 examples/sec; 0.070 sec/batch)
2017-05-02 15:09:58.226309: step 12420, loss = 0.81 (1612.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:59.018772: step 12430, loss = 0.74 (1615.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:09:59.803909: step 12440, loss = 0.85 (1630.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:00.598637: step 12450, loss = 0.74 (1610.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:01.407494: step 12460, loss = 0.73 (1582.5 examples/sec; 0.081 sec/batch)
2017-05-02 15:10:02.208962: step 12470, loss = 0.70 (1597.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:10:03.003364: step 12480, loss = 0.90 (1611.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:03.784858: step 12490, loss = 0.87 (1637.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:10:04.670900: step 12500, loss = 0.84 (1444.7 examples/sec; 0.089 sec/batch)
2017-05-02 15:10:05.365855: step 12510, loss = 0.69 (1841.8 examples/sec; 0.069 sec/batch)
2017-05-02 15:10:06.160021: step 12520, loss = 0.80 (1611.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:06.954699: step 12530, loss = 0.89 (1610.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:07.734144: step 12540, loss = 0.85 (1642.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:10:08.528490: step 12550, loss = 0.58 (1611.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:09.321406: step 12560, loss = 0.89 (1614.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:10.126295: step 12570, loss = 0.83 (1590.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:10:10.921299: step 12580, loss = 0.87 (1610.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:10:11.707288: step 12590, loss = 0.78 (1628.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:12.600802: step 12600, loss = 0.76 (1432.6 examples/sec; 0.089 sec/batch)
2017-05-02 15:10:13.294144: step 12610, loss = 0.86 (1846.1 examples/sec; 0.069 sec/batch)
2017-05-02 15:10:14.083110: step 12620, loss = 0.97 (1622.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:14.874475: step 12630, loss = 0.89 (1617.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:15.657868: step 12640, loss = 0.73 (1633.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:10:16.452733: step 12650, loss = 0.74 (1610.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:17.247064: step 12660, loss = 0.98 (1611.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:18.037601: step 12670, loss = 1.04 (1619.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:18.829270: step 12680, loss = 0.78 (1616.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:19.616048: step 12690, loss = 0.86 (1626.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:20.513458: step 12700, loss = 0.89 (1426.3 examples/sec; 0.090 sec/batch)
2017-05-02 15:10:21.210376: step 12710, loss = 0.87 (1836.7 examples/sec; 0.070 sec/batch)
2017-05-02 15:10:22.006591: step 12720, loss = 0.84 (1607.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:10:22.796527: step 12730, loss = 0.94 (1620.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:23.578338: step 12740, loss = 0.97 (1637.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:10:24.365330: step 12750, loss = 0.77 (1626.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:25.155174: step 12760, loss = 0.75 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:25.951656: step 12770, loss = 1.05 (1607.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:10:26.752334: step 12780, loss = 0.84 (1598.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:10:27.545191: step 12790, loss = 0.83 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:28.447470: step 12800, loss = 0.89 (1418.6 examples/sec; 0.090 sec/batch)
2017-05-02 15:10:29.135528: step 12810, loss = 0.88 (1860.3 examples/sec; 0.069 sec/batch)
2017-05-02 15:10:29.924452: step 12820, loss = 0.90 (1622.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:30.720194: step 12830, loss = 0.78 (1608.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:10:31.512529: step 12840, loss = 0.93 (1615.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:32.293897: step 12850, loss = 0.87 (1638.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:10:33.083659: step 12860, loss = 0.84 (1620.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:33.874797: step 12870, loss = 0.91 (1617.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:34.670946: step 12880, loss = 0.84 (1607.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:10:35.460185: step 12890, loss = 0.81 (1621.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:36.352473: step 12900, loss = 0.86 (1434.5 examples/sec; 0.089 sec/batch)
2017-05-02 15:10:37.039595: step 12910, loss = 0.93 (1862.8 examples/sec; 0.069 sec/batch)
2017-05-02 15:10:37.832259: step 12920, loss = 0.77 (1614.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:38.625574: step 12930, loss = 0.96 (1613.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:39.416959: step 12940, loss = 0.75 (1617.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:40.209788: step 12950, loss = 0.78 (1614.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:41.016849: step 12960, loss = 0.83 (1586.0 examples/sec; 0.081 sec/batch)
2017-05-02 15:10:41.808282: step 12970, loss = 0.68 (1617.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:42.611592: step 12980, loss = 0.92 (1593.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:10:43.398793: step 12990, loss = 0.83 (1626.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:44.297901: step 13000, loss = 0.69 (1423.6 examples/sec; 0.090 sec/batch)
2017-05-02 15:10:44.989391: step 13010, loss = 0.84 (1851.1 examples/sec; 0.069 sec/batch)
2017-05-02 15:10:45.782346: step 13020, loss = 0.90 (1614.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:46.576154: step 13030, loss = 0.95 (1612.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:47.369905: step 13040, loss = 0.88 (1612.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:48.163150: step 13050, loss = 0.88 (1613.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:48.953018: step 13060, loss = 0.69 (1620.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:49.748055: step 13070, loss = 0.80 (1610.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:10:50.543492: step 13080, loss = 0.74 (1609.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:10:51.331820: step 13090, loss = 0.79 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:52.214227: step 13100, loss = 0.83 (1450.6 examples/sec; 0.088 sec/batch)
2017-05-02 15:10:52.911190: step 13110, loss = 0.85 (1836.5 examples/sec; 0.070 sec/batch)
2017-05-02 15:10:53.703934: step 13120, loss = 0.71 (1614.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:54.490468: step 13130, loss = 1.04 (1627.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:55.288819: step 13140, loss = 0.85 (1603.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:10:56.071677: step 13150, loss = 0.90 (1635.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:10:56.870358: step 13160, loss = 1.08 (1602.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:10:57.662285: step 13170, loss = 0.85 (1616.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:58.457251: step 13180, loss = 0.87 (1610.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:10:59.245994: step 13190, loss = 0.75 (1622.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:00.126233: step 13200, loss = 0.92 (1454.1 examples/sec; 0.088 sec/batch)
2017-05-02 15:11:00.826041: step 13210, loss = 0.86 (1829.1 examples/sec; 0.070 sec/batch)
2017-05-02 15:11:01.619323: step 13220, loss = 0.96 (1613.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:02.410227: step 13230, loss = 0.94 (1618.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:03.211525: step 13240, loss = 0.90 (1597.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:11:03.994664: step 13250, loss = 0.93 (1634.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:11:04.785704: step 13260, loss = 0.89 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:05.573850: step 13270, loss = 0.87 (1624.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:06.371683: step 13280, loss = 0.84 (1604.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:11:07.165094: step 13290, loss = 0.91 (1613.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:08.054365: step 13300, loss = 0.96 (1439.4 examples/sec; 0.089 sec/batch)
2017-05-02 15:11:08.734176: step 13310, loss = 0.95 (1882.9 examples/sec; 0.068 sec/batch)
2017-05-02 15:11:09.531214: step 13320, loss = 0.77 (1606.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:11:10.325138: step 13330, loss = 0.91 (1612.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:11.117871: step 13340, loss = 0.80 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:11.895569: step 13350, loss = 0.79 (1645.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:11:12.691153: step 13360, loss = 0.86 (1608.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:11:13.494606: step 13370, loss = 1.12 (1593.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:11:14.293308: step 13380, loss = 0.71 (1602.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:11:15.087426: step 13390, loss = 0.84 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:15.971455: step 13400, loss = 1.02 (1447.9 examples/sec; 0.088 sec/batch)
2017-05-02 15:11:16.665832: step 13410, loss = 0.82 (1843.4 examples/sec; 0.069 sec/batch)
2017-05-02 15:11:17.455639: step 13420, loss = 0.83 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:18.248740: step 13430, loss = 0.89 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:19.040574: step 13440, loss = 0.97 (1616.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:19.821189: step 13450, loss = 0.80 (1639.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:11:20.613195: step 13460, loss = 0.95 (1616.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:21.413043: step 13470, loss = 0.93 (1600.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:11:22.202409: step 13480, loss = 0.85 (1621.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:22.992756: step 13490, loss = 0.72 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:23.875720: step 13500, loss = 0.86 (1449.7 examples/sec; 0.088 sec/batch)
2017-05-02 15:11:24.571565: step 13510, loss = 0.89 (1839.5 examples/sec; 0.070 sec/batch)
2017-05-02 15:11:25.360166: step 13520, loss = 0.85 (1623.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:26.156211: step 13530, loss = 0.90 (1607.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:11:26.945587: step 13540, loss = 0.86 (1621.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:27.729588: step 13550, loss = 0.82 (1632.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:11:28.530732: step 13560, loss = 0.98 (1597.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:11:29.329915: step 13570, loss = 0.87 (1601.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:11:30.118598: step 13580, loss = 0.84 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:30.909930: step 13590, loss = 0.95 (1617.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:31.791864: step 13600, loss = 0.72 (1451.4 examples/sec; 0.088 sec/batch)
2017-05-02 15:11:32.486129: step 13610, loss = 0.95 (1843.7 examples/sec; 0.069 sec/batch)
2017-05-02 15:11:33.275717: step 13620, loss = 0.97 (1621.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:34.068206: step 13630, loss = 0.85 (1615.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:34.858472: step 13640, loss = 0.82 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:35.637749: step 13650, loss = 0.80 (1642.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:11:36.434865: step 13660, loss = 0.89 (1605.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:11:37.232221: step 13670, loss = 0.88 (1605.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:11:38.019531: step 13680, loss = 0.75 (1625.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:38.812648: step 13690, loss = 0.79 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:39.693476: step 13700, loss = 0.86 (1453.2 examples/sec; 0.088 sec/batch)
2017-05-02 15:11:40.386905: step 13710, loss = 0.84 (1845.9 examples/sec; 0.069 sec/batch)
2017-05-02 15:11:41.179694: step 13720, loss = 0.89 (1614.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:41.970986: step 13730, loss = 0.86 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:42.766025: step 13740, loss = 0.84 (1610.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:11:43.547009: step 13750, loss = 0.75 (1639.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:11:44.336850: step 13760, loss = 0.97 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:45.128451: step 13770, loss = 0.94 (1617.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:45.918914: step 13780, loss = 0.85 (1619.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:46.710492: step 13790, loss = 0.82 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:47.611647: step 13800, loss = 0.88 (1420.4 examples/sec; 0.090 sec/batch)
2017-05-02 15:11:48.285199: step 13810, loss = 0.87 (1900.4 examples/sec; 0.067 sec/batch)
2017-05-02 15:11:49.075345: step 13820, loss = 0.74 (1620.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:49.867785: step 13830, loss = 0.80 (1615.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:50.660497: step 13840, loss = 0.99 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:51.444748: step 13850, loss = 0.87 (1632.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:11:52.228167: step 13860, loss = 0.95 (1633.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:11:53.023663: step 13870, loss = 0.81 (1609.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:11:53.818512: step 13880, loss = 0.81 (1610.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:54.605556: step 13890, loss = 0.82 (1626.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:55.495854: step 13900, loss = 0.83 (1437.7 examples/sec; 0.089 sec/batch)
2017-05-02 15:11:56.186288: step 13910, loss = 0.88 (1853.9 examples/sec; 0.069 sec/batch)
2017-05-02 15:11:56.985741: step 13920, loss = 0.84 (1601.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:11:57.784907: step 13930, loss = 0.96 (1601.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:11:58.573765: step 13940, loss = 0.86 (1622.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:11:59.361090: step 13950, loss = 0.89 (1625.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:00.146153: step 13960, loss = 1.04 (1630.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:00.940529: step 13970, loss = 0.75 (1611.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:01.731092: step 13980, loss = 0.89 (1619.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:02.523356: step 13990, loss = 0.91 (1615.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:03.416402: step 14000, loss = 1.12 (1433.3 examples/sec; 0.089 sec/batch)
2017-05-02 15:12:04.102850: step 14010, loss = 0.89 (1864.7 examples/sec; 0.069 sec/batch)
2017-05-02 15:12:04.894203: step 14020, loss = 0.85 (1617.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:05.687662: step 14030, loss = 0.91 (1613.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:06.479991: step 14040, loss = 0.88 (1615.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:07.278771: step 14050, loss = 0.96 (1602.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:12:08.063718: step 14060, loss = 1.02 (1630.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:12:08.855639: step 14070, loss = 0.83 (1616.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:09.645722: step 14080, loss = 0.88 (1620.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:10.439789: step 14090, loss = 0.88 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:11.332529: step 14100, loss = 0.85 (1433.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:12:12.010577: step 14110, loss = 0.80 (1887.8 examples/sec; 0.068 sec/batch)
2017-05-02 15:12:12.801013: step 14120, loss = 0.66 (1619.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:13.596371: step 14130, loss = 0.99 (1609.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:12:14.378775: step 14140, loss = 0.82 (1636.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:12:15.171601: step 14150, loss = 0.92 (1614.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:15.946650: step 14160, loss = 0.79 (1651.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:12:16.740200: step 14170, loss = 0.97 (1613.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:17.530304: step 14180, loss = 0.82 (1620.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:18.327426: step 14190, loss = 0.86 (1605.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:12:19.217348: step 14200, loss = 0.84 (1438.3 examples/sec; 0.089 sec/batch)
2017-05-02 15:12:19.907758: step 14210, loss = 0.77 (1854.0 examples/sec; 0.069 sec/batch)
2017-05-02 15:12:20.703595: step 14220, loss = 0.82 (1608.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:12:21.496502: step 14230, loss = 0.80 (1614.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:22.289769: step 14240, loss = 0.75 (1613.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:23.080773: step 14250, loss = 1.00 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:23.862910: step 14260, loss = 0.95 (1636.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:12:24.658792: step 14270, loss = 0.90 (1608.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:12:25.445303: step 14280, loss = 0.92 (1627.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:26.244921: step 14290, loss = 0.96 (1600.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:12:27.143957: step 14300, loss = 0.87 (1423.8 examples/sec; 0.090 sec/batch)
2017-05-02 15:12:27.829784: step 14310, loss = 0.77 (1866.4 examples/sec; 0.069 sec/batch)
2017-05-02 15:12:28.618040: step 14320, loss = 0.88 (1623.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:29.410544: step 14330, loss = 0.94 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:30.202725: step 14340, loss = 0.83 (1615.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:30.989375: step 14350, loss = 0.68 (1627.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:31.768308: step 14360, loss = 0.86 (1643.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:12:32.561723: step 14370, loss = 0.95 (1613.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:33.352524: step 14380, loss = 0.90 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:34.145553: step 14390, loss = 0.91 (1614.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:35.061014: step 14400, loss = 0.91 (1398.2 examples/sec; 0.092 sec/batch)
2017-05-02 15:12:35.726650: step 14410, loss = 0.87 (1923.0 examples/sec; 0.067 sec/batch)
2017-05-02 15:12:36.531584: step 14420, loss = 0.89 (1590.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:12:37.323699: step 14430, loss = 0.71 (1615.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:38.119917: step 14440, loss = 0.92 (1607.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:12:38.911494: step 14450, loss = 0.91 (1617.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:39.695143: step 14460, loss = 0.94 (1633.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:12:40.493115: step 14470, loss = 0.90 (1604.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:12:41.288087: step 14480, loss = 0.79 (1610.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:42.073454: step 14490, loss = 0.85 (1629.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:42.984057: step 14500, loss = 0.85 (1405.7 examples/sec; 0.091 sec/batch)
2017-05-02 15:12:43.644599: step 14510, loss = 0.90 (1937.8 examples/sec; 0.066 sec/batch)
2017-05-02 15:12:44.435456: step 14520, loss = 0.70 (1618.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:45.222832: step 14530, loss = 0.87 (1625.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:46.013373: step 14540, loss = 0.84 (1619.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:46.808031: step 14550, loss = 1.02 (1610.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:47.600621: step 14560, loss = 0.81 (1615.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:48.393773: step 14570, loss = 0.86 (1613.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:49.186591: step 14580, loss = 0.92 (1614.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:49.975918: step 14590, loss = 0.77 (1621.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:50.874914: step 14600, loss = 0.79 (1423.9 examples/sec; 0.090 sec/batch)
2017-05-02 15:12:51.550602: step 14610, loss = 0.76 (1894.3 examples/sec; 0.068 sec/batch)
2017-05-02 15:12:52.341343: step 14620, loss = 0.86 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:53.137582: step 14630, loss = 0.80 (1607.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:12:53.932134: step 14640, loss = 0.95 (1611.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:54.726704: step 14650, loss = 0.99 (1610.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:55.515236: step 14660, loss = 0.75 (1623.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:56.307499: step 14670, loss = 1.00 (1615.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:57.099765: step 14680, loss = 0.98 (1615.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:57.894110: step 14690, loss = 0.78 (1611.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:12:58.782475: step 14700, loss = 0.80 (1440.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:12:59.478129: step 14710, loss = 0.80 (1840.0 examples/sec; 0.070 sec/batch)
2017-05-02 15:13:00.267510: step 14720, loss = 0.99 (1621.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:01.064279: step 14730, loss = 0.82 (1606.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:13:01.849484: step 14740, loss = 0.90 (1630.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:02.642071: step 14750, loss = 0.91 (1615.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:03.433117: step 14760, loss = 0.92 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:04.227648: step 14770, loss = 0.85 (1611.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:05.020630: step 14780, loss = 0.81 (1614.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:05.810740: step 14790, loss = 0.86 (1620.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:06.714178: step 14800, loss = 0.80 (1416.8 examples/sec; 0.090 sec/batch)
2017-05-02 15:13:07.397051: step 14810, loss = 0.96 (1874.4 examples/sec; 0.068 sec/batch)
2017-05-02 15:13:08.181287: step 14820, loss = 0.85 (1632.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:13:08.968003: step 14830, loss = 0.80 (1627.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:09.758993: step 14840, loss = 0.86 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:10.564658: step 14850, loss = 0.81 (1588.7 examples/sec; 0.081 sec/batch)
2017-05-02 15:13:11.357178: step 14860, loss = 0.70 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:12.153839: step 14870, loss = 0.82 (1606.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:13:12.946380: step 14880, loss = 0.85 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:13.742516: step 14890, loss = 0.80 (1607.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:13:14.658275: step 14900, loss = 0.84 (1397.8 examples/sec; 0.092 sec/batch)
2017-05-02 15:13:15.331004: step 14910, loss = 0.69 (1902.7 examples/sec; 0.067 sec/batch)
2017-05-02 15:13:16.120085: step 14920, loss = 0.74 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:16.913532: step 14930, loss = 0.88 (1613.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:17.710030: step 14940, loss = 0.65 (1607.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:13:18.503336: step 14950, loss = 0.77 (1613.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:19.295052: step 14960, loss = 0.90 (1616.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:20.082198: step 14970, loss = 0.97 (1626.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:20.872512: step 14980, loss = 0.83 (1619.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:21.667838: step 14990, loss = 0.92 (1609.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:13:22.563855: step 15000, loss = 0.85 (1428.6 examples/sec; 0.090 sec/batch)
2017-05-02 15:13:23.252956: step 15010, loss = 0.87 (1857.5 examples/sec; 0.069 sec/batch)
2017-05-02 15:13:24.024517: step 15020, loss = 0.77 (1659.0 examples/sec; 0.077 sec/batch)
2017-05-02 15:13:24.819571: step 15030, loss = 0.88 (1610.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:13:25.609825: step 15040, loss = 0.72 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:26.403788: step 15050, loss = 0.81 (1612.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:27.195907: step 15060, loss = 0.77 (1615.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:27.980848: step 15070, loss = 0.79 (1630.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:13:28.769572: step 15080, loss = 0.92 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:29.559341: step 15090, loss = 0.86 (1620.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:30.489814: step 15100, loss = 0.66 (1375.6 examples/sec; 0.093 sec/batch)
2017-05-02 15:13:31.181737: step 15110, loss = 0.82 (1849.9 examples/sec; 0.069 sec/batch)
2017-05-02 15:13:31.968848: step 15120, loss = 0.80 (1626.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:32.760538: step 15130, loss = 0.95 (1616.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:33.551365: step 15140, loss = 0.89 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:34.344451: step 15150, loss = 0.95 (1614.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:35.136214: step 15160, loss = 0.93 (1616.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:35.917269: step 15170, loss = 0.88 (1638.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:13:36.710840: step 15180, loss = 0.79 (1613.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:37.512054: step 15190, loss = 0.75 (1597.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:13:38.406028: step 15200, loss = 0.83 (1431.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:13:39.110228: step 15210, loss = 0.67 (1817.7 examples/sec; 0.070 sec/batch)
2017-05-02 15:13:39.886839: step 15220, loss = 0.85 (1648.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:13:40.679420: step 15230, loss = 0.82 (1615.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:41.476535: step 15240, loss = 0.86 (1605.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:13:42.264177: step 15250, loss = 0.71 (1625.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:43.059067: step 15260, loss = 0.73 (1610.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:43.833357: step 15270, loss = 0.91 (1653.1 examples/sec; 0.077 sec/batch)
2017-05-02 15:13:44.630531: step 15280, loss = 0.87 (1605.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:13:45.428604: step 15290, loss = 0.92 (1603.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:13:46.337752: step 15300, loss = 0.77 (1407.9 examples/sec; 0.091 sec/batch)
2017-05-02 15:13:47.024680: step 15310, loss = 0.85 (1863.4 examples/sec; 0.069 sec/batch)
2017-05-02 15:13:47.809132: step 15320, loss = 0.87 (1631.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:13:48.601014: step 15330, loss = 0.88 (1616.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:49.396383: step 15340, loss = 0.77 (1609.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:13:50.191786: step 15350, loss = 0.87 (1609.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:13:50.991985: step 15360, loss = 0.78 (1599.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:13:51.782338: step 15370, loss = 0.66 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:52.579484: step 15380, loss = 0.94 (1605.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:13:53.371292: step 15390, loss = 0.92 (1616.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:54.262327: step 15400, loss = 0.80 (1436.5 examples/sec; 0.089 sec/batch)
2017-05-02 15:13:54.960354: step 15410, loss = 0.68 (1833.8 examples/sec; 0.070 sec/batch)
2017-05-02 15:13:55.739619: step 15420, loss = 1.06 (1642.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:13:56.533099: step 15430, loss = 0.88 (1613.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:57.323223: step 15440, loss = 0.79 (1620.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:58.108692: step 15450, loss = 0.86 (1629.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:58.899459: step 15460, loss = 0.81 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:13:59.685682: step 15470, loss = 0.99 (1628.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:00.472903: step 15480, loss = 0.87 (1626.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:01.268783: step 15490, loss = 0.95 (1608.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:14:02.188899: step 15500, loss = 0.83 (1391.2 examples/sec; 0.092 sec/batch)
2017-05-02 15:14:02.859453: step 15510, loss = 0.96 (1908.8 examples/sec; 0.067 sec/batch)
2017-05-02 15:14:03.645658: step 15520, loss = 0.72 (1628.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:04.438577: step 15530, loss = 0.89 (1614.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:05.228393: step 15540, loss = 0.84 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:06.022817: step 15550, loss = 0.98 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:06.809938: step 15560, loss = 0.93 (1626.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:07.589665: step 15570, loss = 1.06 (1641.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:14:08.374390: step 15580, loss = 0.78 (1631.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:14:09.161938: step 15590, loss = 0.71 (1625.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:10.051035: step 15600, loss = 0.86 (1439.7 examples/sec; 0.089 sec/batch)
2017-05-02 15:14:10.754540: step 15610, loss = 0.88 (1819.5 examples/sec; 0.070 sec/batch)
2017-05-02 15:14:11.538018: step 15620, loss = 0.79 (1633.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:14:12.319384: step 15630, loss = 0.79 (1638.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:14:13.108099: step 15640, loss = 0.86 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:13.890436: step 15650, loss = 0.78 (1636.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:14:14.686053: step 15660, loss = 0.84 (1608.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:14:15.470961: step 15670, loss = 0.81 (1630.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:14:16.250625: step 15680, loss = 0.76 (1641.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:14:17.041860: step 15690, loss = 0.70 (1617.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:17.934479: step 15700, loss = 0.90 (1434.0 examples/sec; 0.089 sec/batch)
2017-05-02 15:14:18.626681: step 15710, loss = 0.82 (1849.2 examples/sec; 0.069 sec/batch)
2017-05-02 15:14:19.415799: step 15720, loss = 0.72 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:20.201858: step 15730, loss = 0.81 (1628.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:20.998067: step 15740, loss = 0.82 (1607.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:14:21.795683: step 15750, loss = 0.95 (1604.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:14:22.596775: step 15760, loss = 0.79 (1597.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:14:23.386124: step 15770, loss = 0.91 (1621.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:24.174154: step 15780, loss = 0.79 (1624.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:24.966215: step 15790, loss = 0.94 (1616.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:25.854023: step 15800, loss = 0.80 (1441.7 examples/sec; 0.089 sec/batch)
2017-05-02 15:14:26.546377: step 15810, loss = 0.67 (1848.8 examples/sec; 0.069 sec/batch)
2017-05-02 15:14:27.339342: step 15820, loss = 0.89 (1614.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:28.127473: step 15830, loss = 1.16 (1624.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:28.911582: step 15840, loss = 0.75 (1632.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:14:29.698840: step 15850, loss = 0.78 (1625.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:30.495762: step 15860, loss = 0.89 (1606.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:14:31.286111: step 15870, loss = 0.95 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:32.069074: step 15880, loss = 0.74 (1634.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:14:32.860499: step 15890, loss = 0.73 (1617.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:33.754839: step 15900, loss = 0.84 (1431.2 examples/sec; 0.089 sec/batch)
2017-05-02 15:14:34.442779: step 15910, loss = 0.84 (1860.6 examples/sec; 0.069 sec/batch)
2017-05-02 15:14:35.237846: step 15920, loss = 0.85 (1609.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:14:36.022673: step 15930, loss = 0.95 (1630.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:14:36.820039: step 15940, loss = 0.81 (1605.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:14:37.612347: step 15950, loss = 0.85 (1615.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:38.408540: step 15960, loss = 0.96 (1607.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:14:39.196555: step 15970, loss = 0.59 (1624.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:39.978414: step 15980, loss = 0.70 (1637.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:14:40.774257: step 15990, loss = 0.97 (1608.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:14:41.662841: step 16000, loss = 0.89 (1440.5 examples/sec; 0.089 sec/batch)
2017-05-02 15:14:42.359303: step 16010, loss = 0.98 (1837.9 examples/sec; 0.070 sec/batch)
2017-05-02 15:14:43.149499: step 16020, loss = 0.81 (1619.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:43.931792: step 16030, loss = 0.80 (1636.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:14:44.723496: step 16040, loss = 0.88 (1616.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:45.513067: step 16050, loss = 0.85 (1621.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:46.305816: step 16060, loss = 0.88 (1614.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:47.101152: step 16070, loss = 1.02 (1609.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:14:47.894654: step 16080, loss = 0.74 (1613.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:48.689650: step 16090, loss = 0.68 (1610.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:49.572902: step 16100, loss = 0.86 (1449.2 examples/sec; 0.088 sec/batch)
2017-05-02 15:14:50.269544: step 16110, loss = 0.92 (1837.3 examples/sec; 0.070 sec/batch)
2017-05-02 15:14:51.064121: step 16120, loss = 0.94 (1610.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:51.843339: step 16130, loss = 0.85 (1642.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:14:52.631790: step 16140, loss = 0.87 (1623.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:53.424216: step 16150, loss = 0.94 (1615.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:54.215955: step 16160, loss = 0.81 (1616.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:55.005312: step 16170, loss = 0.87 (1621.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:55.787587: step 16180, loss = 0.73 (1636.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:14:56.579601: step 16190, loss = 0.73 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:14:57.475918: step 16200, loss = 0.86 (1428.1 examples/sec; 0.090 sec/batch)
2017-05-02 15:14:58.170376: step 16210, loss = 0.93 (1843.2 examples/sec; 0.069 sec/batch)
2017-05-02 15:14:58.965958: step 16220, loss = 0.72 (1608.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:14:59.746019: step 16230, loss = 0.91 (1640.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:15:00.534701: step 16240, loss = 0.73 (1623.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:01.342796: step 16250, loss = 0.76 (1584.0 examples/sec; 0.081 sec/batch)
2017-05-02 15:15:02.138413: step 16260, loss = 0.90 (1608.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:15:02.933603: step 16270, loss = 0.74 (1609.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:15:03.718853: step 16280, loss = 0.76 (1630.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:04.506927: step 16290, loss = 0.72 (1624.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:05.396190: step 16300, loss = 0.72 (1439.4 examples/sec; 0.089 sec/batch)
2017-05-02 15:15:06.089411: step 16310, loss = 0.88 (1846.5 examples/sec; 0.069 sec/batch)
2017-05-02 15:15:06.884109: step 16320, loss = 0.94 (1610.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:07.665944: step 16330, loss = 0.76 (1637.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:15:08.456226: step 16340, loss = 0.68 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:09.246630: step 16350, loss = 0.68 (1619.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:10.039745: step 16360, loss = 0.95 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:10.824785: step 16370, loss = 0.81 (1630.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:11.614971: step 16380, loss = 0.71 (1619.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:12.409827: step 16390, loss = 0.92 (1610.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:13.321045: step 16400, loss = 0.78 (1404.7 examples/sec; 0.091 sec/batch)
2017-05-02 15:15:13.993494: step 16410, loss = 0.78 (1903.5 examples/sec; 0.067 sec/batch)
2017-05-02 15:15:14.797556: step 16420, loss = 1.03 (1591.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:15:15.571407: step 16430, loss = 0.87 (1654.1 examples/sec; 0.077 sec/batch)
2017-05-02 15:15:16.356248: step 16440, loss = 0.75 (1630.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:15:17.145398: step 16450, loss = 0.82 (1622.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:17.941174: step 16460, loss = 0.88 (1608.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:15:18.744128: step 16470, loss = 0.79 (1594.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:15:19.530009: step 16480, loss = 0.80 (1628.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:20.318569: step 16490, loss = 0.69 (1623.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:21.203785: step 16500, loss = 1.00 (1446.0 examples/sec; 0.089 sec/batch)
2017-05-02 15:15:21.897235: step 16510, loss = 0.94 (1845.9 examples/sec; 0.069 sec/batch)
2017-05-02 15:15:22.691848: step 16520, loss = 0.88 (1610.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:23.480618: step 16530, loss = 0.82 (1622.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:24.264472: step 16540, loss = 0.87 (1633.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:15:25.053621: step 16550, loss = 0.94 (1622.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:25.845593: step 16560, loss = 0.91 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:26.635196: step 16570, loss = 0.86 (1621.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:27.423554: step 16580, loss = 0.81 (1623.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:28.202918: step 16590, loss = 0.82 (1642.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:15:29.090458: step 16600, loss = 0.75 (1442.2 examples/sec; 0.089 sec/batch)
2017-05-02 15:15:29.783813: step 16610, loss = 0.87 (1846.1 examples/sec; 0.069 sec/batch)
2017-05-02 15:15:30.574124: step 16620, loss = 1.01 (1619.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:31.362798: step 16630, loss = 0.84 (1623.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:32.152788: step 16640, loss = 0.77 (1620.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:32.945036: step 16650, loss = 0.84 (1615.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:33.737005: step 16660, loss = 0.76 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:34.527160: step 16670, loss = 0.70 (1619.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:35.320039: step 16680, loss = 0.84 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:36.100402: step 16690, loss = 0.78 (1640.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:15:36.992969: step 16700, loss = 0.79 (1434.1 examples/sec; 0.089 sec/batch)
2017-05-02 15:15:37.684011: step 16710, loss = 1.00 (1852.3 examples/sec; 0.069 sec/batch)
2017-05-02 15:15:38.477249: step 16720, loss = 0.76 (1613.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:39.266211: step 16730, loss = 1.14 (1622.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:40.054754: step 16740, loss = 0.75 (1623.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:40.847241: step 16750, loss = 0.75 (1615.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:41.641213: step 16760, loss = 0.93 (1612.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:42.440086: step 16770, loss = 0.90 (1602.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:15:43.227010: step 16780, loss = 0.76 (1626.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:44.010387: step 16790, loss = 0.74 (1634.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:15:44.905782: step 16800, loss = 0.81 (1429.5 examples/sec; 0.090 sec/batch)
2017-05-02 15:15:45.605483: step 16810, loss = 0.93 (1829.4 examples/sec; 0.070 sec/batch)
2017-05-02 15:15:46.403193: step 16820, loss = 0.87 (1604.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:15:47.203350: step 16830, loss = 1.04 (1599.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:15:47.984452: step 16840, loss = 0.88 (1638.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:15:48.779558: step 16850, loss = 0.79 (1609.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:15:49.564795: step 16860, loss = 0.82 (1630.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:50.361482: step 16870, loss = 0.71 (1606.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:15:51.147538: step 16880, loss = 0.83 (1628.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:51.931621: step 16890, loss = 0.84 (1632.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:15:52.826662: step 16900, loss = 0.87 (1430.1 examples/sec; 0.090 sec/batch)
2017-05-02 15:15:53.524339: step 16910, loss = 0.79 (1834.7 examples/sec; 0.070 sec/batch)
2017-05-02 15:15:54.322402: step 16920, loss = 0.75 (1603.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:15:55.117905: step 16930, loss = 1.01 (1609.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:15:55.898868: step 16940, loss = 0.76 (1639.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:15:56.701932: step 16950, loss = 1.09 (1593.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:15:57.496125: step 16960, loss = 0.88 (1611.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:58.288699: step 16970, loss = 0.83 (1615.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:59.078690: step 16980, loss = 0.97 (1620.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:15:59.862816: step 16990, loss = 0.86 (1632.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:16:00.759871: step 17000, loss = 0.97 (1426.9 examples/sec; 0.090 sec/batch)
2017-05-02 15:16:01.442547: step 17010, loss = 0.72 (1875.0 examples/sec; 0.068 sec/batch)
2017-05-02 15:16:02.235991: step 17020, loss = 0.84 (1613.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:03.031107: step 17030, loss = 0.88 (1609.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:16:03.812413: step 17040, loss = 0.89 (1638.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:16:04.606538: step 17050, loss = 0.81 (1611.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:05.403564: step 17060, loss = 0.90 (1606.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:16:06.196160: step 17070, loss = 0.95 (1615.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:06.989220: step 17080, loss = 0.94 (1614.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:07.764838: step 17090, loss = 0.93 (1650.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:16:08.663306: step 17100, loss = 0.80 (1424.6 examples/sec; 0.090 sec/batch)
2017-05-02 15:16:09.347787: step 17110, loss = 0.92 (1870.0 examples/sec; 0.068 sec/batch)
2017-05-02 15:16:10.141364: step 17120, loss = 0.80 (1613.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:10.935304: step 17130, loss = 0.87 (1612.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:11.713218: step 17140, loss = 0.80 (1645.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:16:12.506957: step 17150, loss = 0.72 (1612.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:13.300136: step 17160, loss = 0.94 (1613.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:14.097833: step 17170, loss = 0.81 (1604.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:16:14.893185: step 17180, loss = 1.01 (1609.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:16:15.679010: step 17190, loss = 0.71 (1628.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:16.566992: step 17200, loss = 0.76 (1441.5 examples/sec; 0.089 sec/batch)
2017-05-02 15:16:17.263058: step 17210, loss = 0.83 (1838.9 examples/sec; 0.070 sec/batch)
2017-05-02 15:16:18.049821: step 17220, loss = 0.90 (1626.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:18.841945: step 17230, loss = 0.89 (1615.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:19.620746: step 17240, loss = 0.90 (1643.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:16:20.421105: step 17250, loss = 0.86 (1599.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:16:21.212619: step 17260, loss = 0.86 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:22.005363: step 17270, loss = 0.80 (1614.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:22.800120: step 17280, loss = 0.80 (1610.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:23.582118: step 17290, loss = 0.79 (1636.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:16:24.482207: step 17300, loss = 0.86 (1422.1 examples/sec; 0.090 sec/batch)
2017-05-02 15:16:25.175478: step 17310, loss = 0.90 (1846.3 examples/sec; 0.069 sec/batch)
2017-05-02 15:16:25.965559: step 17320, loss = 0.89 (1620.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:26.762243: step 17330, loss = 0.77 (1606.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:16:27.542248: step 17340, loss = 0.89 (1641.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:16:28.331187: step 17350, loss = 0.72 (1622.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:29.131043: step 17360, loss = 0.86 (1600.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:16:29.920375: step 17370, loss = 0.68 (1621.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:30.718485: step 17380, loss = 0.87 (1603.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:16:31.507777: step 17390, loss = 0.95 (1621.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:32.404433: step 17400, loss = 0.90 (1427.5 examples/sec; 0.090 sec/batch)
2017-05-02 15:16:33.092514: step 17410, loss = 0.95 (1860.3 examples/sec; 0.069 sec/batch)
2017-05-02 15:16:33.890045: step 17420, loss = 0.77 (1604.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:16:34.675150: step 17430, loss = 0.87 (1630.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:35.460425: step 17440, loss = 0.78 (1630.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:36.251498: step 17450, loss = 0.81 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:37.041449: step 17460, loss = 0.80 (1620.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:37.833097: step 17470, loss = 0.84 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:38.622195: step 17480, loss = 0.83 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:39.414234: step 17490, loss = 0.74 (1616.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:40.306701: step 17500, loss = 0.84 (1434.2 examples/sec; 0.089 sec/batch)
2017-05-02 15:16:40.999477: step 17510, loss = 0.91 (1847.6 examples/sec; 0.069 sec/batch)
2017-05-02 15:16:41.791125: step 17520, loss = 0.85 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:42.586475: step 17530, loss = 0.80 (1609.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:16:43.375189: step 17540, loss = 0.75 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:44.161425: step 17550, loss = 0.79 (1628.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:44.953409: step 17560, loss = 0.67 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:45.749945: step 17570, loss = 0.76 (1607.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:16:46.537125: step 17580, loss = 0.82 (1626.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:47.319920: step 17590, loss = 1.01 (1635.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:16:48.216803: step 17600, loss = 0.78 (1427.2 examples/sec; 0.090 sec/batch)
2017-05-02 15:16:48.896854: step 17610, loss = 0.78 (1882.2 examples/sec; 0.068 sec/batch)
2017-05-02 15:16:49.693625: step 17620, loss = 0.84 (1606.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:16:50.485822: step 17630, loss = 0.85 (1615.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:51.286761: step 17640, loss = 0.76 (1598.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:16:52.069499: step 17650, loss = 0.76 (1635.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:16:52.864076: step 17660, loss = 0.73 (1610.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:53.653490: step 17670, loss = 0.77 (1621.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:54.450410: step 17680, loss = 0.72 (1606.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:16:55.244705: step 17690, loss = 0.80 (1611.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:56.125639: step 17700, loss = 0.72 (1453.0 examples/sec; 0.088 sec/batch)
2017-05-02 15:16:56.810498: step 17710, loss = 0.95 (1869.0 examples/sec; 0.068 sec/batch)
2017-05-02 15:16:57.600843: step 17720, loss = 0.77 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:58.403348: step 17730, loss = 0.73 (1595.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:16:59.197950: step 17740, loss = 0.90 (1610.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:16:59.974236: step 17750, loss = 0.86 (1648.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:17:00.774564: step 17760, loss = 0.66 (1599.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:17:01.559869: step 17770, loss = 0.83 (1629.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:02.355989: step 17780, loss = 0.80 (1607.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:17:03.146586: step 17790, loss = 0.86 (1619.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:04.031316: step 17800, loss = 0.64 (1446.8 examples/sec; 0.088 sec/batch)
2017-05-02 15:17:04.727968: step 17810, loss = 0.89 (1837.4 examples/sec; 0.070 sec/batch)
2017-05-02 15:17:05.519174: step 17820, loss = 0.82 (1617.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:06.312665: step 17830, loss = 0.95 (1613.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:07.102428: step 17840, loss = 0.88 (1620.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:07.883356: step 17850, loss = 0.71 (1639.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:17:08.678499: step 17860, loss = 0.83 (1609.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:17:09.467925: step 17870, loss = 0.79 (1621.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:10.257469: step 17880, loss = 0.98 (1621.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:11.047492: step 17890, loss = 0.81 (1620.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:11.932654: step 17900, loss = 0.86 (1446.1 examples/sec; 0.089 sec/batch)
2017-05-02 15:17:12.619760: step 17910, loss = 0.76 (1862.9 examples/sec; 0.069 sec/batch)
2017-05-02 15:17:13.420004: step 17920, loss = 0.91 (1599.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:17:14.220365: step 17930, loss = 0.75 (1599.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:17:15.010791: step 17940, loss = 0.75 (1619.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:15.789923: step 17950, loss = 0.85 (1642.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:17:16.579209: step 17960, loss = 0.71 (1621.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:17.372597: step 17970, loss = 0.90 (1613.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:18.164740: step 17980, loss = 0.70 (1615.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:18.954382: step 17990, loss = 0.87 (1621.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:19.840297: step 18000, loss = 0.79 (1444.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:17:20.526917: step 18010, loss = 0.82 (1864.3 examples/sec; 0.069 sec/batch)
2017-05-02 15:17:21.330867: step 18020, loss = 0.84 (1592.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:17:22.130986: step 18030, loss = 1.04 (1599.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:17:22.927368: step 18040, loss = 0.73 (1607.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:17:23.704730: step 18050, loss = 0.66 (1646.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:17:24.498024: step 18060, loss = 0.91 (1613.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:25.287594: step 18070, loss = 0.96 (1621.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:26.075175: step 18080, loss = 0.82 (1625.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:26.869384: step 18090, loss = 0.85 (1611.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:27.758841: step 18100, loss = 0.90 (1439.1 examples/sec; 0.089 sec/batch)
2017-05-02 15:17:28.445732: step 18110, loss = 0.87 (1863.5 examples/sec; 0.069 sec/batch)
2017-05-02 15:17:29.234078: step 18120, loss = 0.69 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:30.026247: step 18130, loss = 0.82 (1615.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:30.825231: step 18140, loss = 0.81 (1602.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:17:31.610418: step 18150, loss = 0.75 (1630.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:32.400660: step 18160, loss = 0.89 (1619.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:33.186711: step 18170, loss = 0.73 (1628.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:33.981979: step 18180, loss = 0.83 (1609.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:17:34.778068: step 18190, loss = 1.00 (1607.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:17:35.673157: step 18200, loss = 0.84 (1430.0 examples/sec; 0.090 sec/batch)
2017-05-02 15:17:36.364242: step 18210, loss = 0.89 (1852.2 examples/sec; 0.069 sec/batch)
2017-05-02 15:17:37.152306: step 18220, loss = 1.00 (1624.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:37.947803: step 18230, loss = 0.78 (1609.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:17:38.739066: step 18240, loss = 0.63 (1617.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:39.521212: step 18250, loss = 0.74 (1636.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:17:40.305547: step 18260, loss = 0.69 (1631.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:17:41.099151: step 18270, loss = 0.89 (1612.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:41.888365: step 18280, loss = 0.92 (1621.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:42.683773: step 18290, loss = 0.89 (1609.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:17:43.569770: step 18300, loss = 0.77 (1444.7 examples/sec; 0.089 sec/batch)
2017-05-02 15:17:44.264893: step 18310, loss = 0.68 (1841.4 examples/sec; 0.070 sec/batch)
2017-05-02 15:17:45.061065: step 18320, loss = 0.71 (1607.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:17:45.861953: step 18330, loss = 0.83 (1598.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:17:46.655010: step 18340, loss = 0.78 (1614.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:47.444960: step 18350, loss = 0.91 (1620.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:48.234790: step 18360, loss = 0.83 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:49.020708: step 18370, loss = 0.80 (1628.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:49.814761: step 18380, loss = 0.89 (1612.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:50.605227: step 18390, loss = 0.69 (1619.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:51.495309: step 18400, loss = 0.71 (1438.1 examples/sec; 0.089 sec/batch)
2017-05-02 15:17:52.180008: step 18410, loss = 0.83 (1869.4 examples/sec; 0.068 sec/batch)
2017-05-02 15:17:52.976357: step 18420, loss = 1.06 (1607.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:17:53.768266: step 18430, loss = 0.63 (1616.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:54.561018: step 18440, loss = 0.82 (1614.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:55.355445: step 18450, loss = 0.85 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:56.145532: step 18460, loss = 0.89 (1620.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:56.936047: step 18470, loss = 0.89 (1619.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:57.730749: step 18480, loss = 0.87 (1610.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:58.520360: step 18490, loss = 0.80 (1621.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:17:59.415742: step 18500, loss = 0.96 (1429.6 examples/sec; 0.090 sec/batch)
2017-05-02 15:18:00.099299: step 18510, loss = 0.96 (1872.6 examples/sec; 0.068 sec/batch)
2017-05-02 15:18:00.885120: step 18520, loss = 0.71 (1628.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:01.673737: step 18530, loss = 1.01 (1623.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:02.463394: step 18540, loss = 0.75 (1621.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:03.249503: step 18550, loss = 0.81 (1628.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:04.032983: step 18560, loss = 0.65 (1633.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:18:04.822463: step 18570, loss = 0.83 (1621.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:05.612422: step 18580, loss = 0.97 (1620.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:06.399090: step 18590, loss = 0.97 (1627.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:07.300815: step 18600, loss = 0.74 (1419.5 examples/sec; 0.090 sec/batch)
2017-05-02 15:18:07.974368: step 18610, loss = 0.89 (1900.4 examples/sec; 0.067 sec/batch)
2017-05-02 15:18:08.769554: step 18620, loss = 0.97 (1609.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:18:09.560798: step 18630, loss = 0.85 (1617.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:10.357263: step 18640, loss = 0.73 (1607.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:18:11.167515: step 18650, loss = 0.87 (1579.8 examples/sec; 0.081 sec/batch)
2017-05-02 15:18:11.943059: step 18660, loss = 0.93 (1650.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:18:12.727483: step 18670, loss = 0.83 (1631.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:18:13.519656: step 18680, loss = 0.82 (1615.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:14.314211: step 18690, loss = 0.78 (1611.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:15.208281: step 18700, loss = 0.85 (1431.7 examples/sec; 0.089 sec/batch)
2017-05-02 15:18:15.892342: step 18710, loss = 0.96 (1871.2 examples/sec; 0.068 sec/batch)
2017-05-02 15:18:16.682707: step 18720, loss = 0.80 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:17.475703: step 18730, loss = 0.77 (1614.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:18.269851: step 18740, loss = 0.83 (1611.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:19.055274: step 18750, loss = 0.83 (1629.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:19.837676: step 18760, loss = 0.88 (1636.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:18:20.630372: step 18770, loss = 0.95 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:21.426850: step 18780, loss = 0.93 (1607.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:18:22.221981: step 18790, loss = 0.69 (1609.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:18:23.123226: step 18800, loss = 0.84 (1420.3 examples/sec; 0.090 sec/batch)
2017-05-02 15:18:23.805909: step 18810, loss = 0.90 (1875.0 examples/sec; 0.068 sec/batch)
2017-05-02 15:18:24.599150: step 18820, loss = 0.94 (1613.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:25.388595: step 18830, loss = 0.76 (1621.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:26.176093: step 18840, loss = 0.79 (1625.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:26.981093: step 18850, loss = 0.93 (1590.1 examples/sec; 0.081 sec/batch)
2017-05-02 15:18:27.763503: step 18860, loss = 0.90 (1636.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:18:28.551947: step 18870, loss = 0.82 (1623.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:29.347339: step 18880, loss = 0.84 (1609.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:18:30.137204: step 18890, loss = 0.92 (1620.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:31.036201: step 18900, loss = 0.77 (1423.8 examples/sec; 0.090 sec/batch)
2017-05-02 15:18:31.731592: step 18910, loss = 0.85 (1840.7 examples/sec; 0.070 sec/batch)
2017-05-02 15:18:32.523349: step 18920, loss = 0.93 (1616.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:33.307916: step 18930, loss = 0.90 (1631.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:18:34.098561: step 18940, loss = 0.91 (1618.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:34.889253: step 18950, loss = 0.91 (1618.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:35.668982: step 18960, loss = 0.82 (1641.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:18:36.464595: step 18970, loss = 0.92 (1608.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:18:37.258955: step 18980, loss = 0.76 (1611.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:38.052834: step 18990, loss = 0.91 (1612.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:38.974296: step 19000, loss = 0.73 (1389.1 examples/sec; 0.092 sec/batch)
2017-05-02 15:18:39.642354: step 19010, loss = 0.93 (1916.0 examples/sec; 0.067 sec/batch)
2017-05-02 15:18:40.431585: step 19020, loss = 0.78 (1621.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:41.222376: step 19030, loss = 0.82 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:42.006098: step 19040, loss = 0.88 (1633.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:18:42.799056: step 19050, loss = 0.83 (1614.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:43.585017: step 19060, loss = 0.84 (1628.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:44.379105: step 19070, loss = 0.85 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:45.174443: step 19080, loss = 0.87 (1609.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:18:45.967165: step 19090, loss = 0.77 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:46.861128: step 19100, loss = 0.92 (1431.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:18:47.553708: step 19110, loss = 0.87 (1848.2 examples/sec; 0.069 sec/batch)
2017-05-02 15:18:48.342818: step 19120, loss = 0.81 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:49.135353: step 19130, loss = 0.79 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:49.931772: step 19140, loss = 0.76 (1607.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:18:50.732059: step 19150, loss = 0.93 (1599.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:18:51.512304: step 19160, loss = 0.79 (1640.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:18:52.298876: step 19170, loss = 0.93 (1627.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:53.097848: step 19180, loss = 0.73 (1602.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:18:53.885182: step 19190, loss = 0.93 (1625.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:54.783213: step 19200, loss = 1.05 (1425.3 examples/sec; 0.090 sec/batch)
2017-05-02 15:18:55.468598: step 19210, loss = 0.70 (1867.6 examples/sec; 0.069 sec/batch)
2017-05-02 15:18:56.257020: step 19220, loss = 0.87 (1623.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:57.050495: step 19230, loss = 0.86 (1613.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:57.838847: step 19240, loss = 0.95 (1623.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:58.627024: step 19250, loss = 0.70 (1624.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:18:59.408818: step 19260, loss = 0.84 (1637.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:19:00.192601: step 19270, loss = 0.90 (1633.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:19:00.989392: step 19280, loss = 0.75 (1606.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:19:01.771644: step 19290, loss = 0.84 (1636.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:19:02.663639: step 19300, loss = 0.76 (1435.0 examples/sec; 0.089 sec/batch)
2017-05-02 15:19:03.361993: step 19310, loss = 0.81 (1832.9 examples/sec; 0.070 sec/batch)
2017-05-02 15:19:04.146188: step 19320, loss = 0.92 (1632.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:19:04.936947: step 19330, loss = 0.97 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:19:05.734569: step 19340, loss = 0.89 (1604.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:19:06.532152: step 19350, loss = 0.71 (1604.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:19:07.328105: step 19360, loss = 0.84 (1608.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:19:08.108708: step 19370, loss = 0.77 (1639.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:19:08.905968: step 19380, loss = 0.70 (1605.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:19:09.695089: step 19390, loss = 0.77 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:19:10.594482: step 19400, loss = 0.85 (1423.2 examples/sec; 0.090 sec/batch)
2017-05-02 15:19:11.292913: step 19410, loss = 0.80 (1832.7 examples/sec; 0.070 sec/batch)
2017-05-02 15:19:12.076297: step 19420, loss = 0.72 (1633.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:19:12.869897: step 19430, loss = 0.86 (1612.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:19:13.657782: step 19440, loss = 0.83 (1624.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:19:14.453938: step 19450, loss = 0.68 (1607.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:19:15.249946: step 19460, loss = 0.74 (1608.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:19:16.033609: step 19470, loss = 0.67 (1633.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:19:16.824114: step 19480, loss = 0.78 (1619.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:19:17.618365: step 19490, loss = 0.87 (1611.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:19:18.507647: step 19500, loss = 0.91 (1439.4 examples/sec; 0.089 sec/batch)
2017-05-02 15:19:19.208143: step 19510, loss = 0.71 (1827.3 examples/sec; 0.070 sec/batch)
2017-05-02 15:19:19.987664: step 19520, loss = 0.93 (1642.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:19:20.781465: step 19530, loss = 0.87 (1612.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:19:21.582653: step 19540, loss = 0.92 (1597.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:19:22.383951: step 19550, loss = 0.68 (1597.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:19:23.177583: step 19560, loss = 0.85 (1612.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:19:23.960756: step 19570, loss = 0.94 (1634.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:19:24.760620: step 19580, loss = 0.65 (1600.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:19:25.548892: step 19590, loss = 0.76 (1623.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:19:26.458808: step 19600, loss = 0.91 (1406.7 examples/sec; 0.091 sec/batch)
2017-05-02 15:19:27.132230: step 19610, loss = 0.77 (1900.7 examples/sec; 0.067 sec/batch)
2017-05-02 15:19:27.916502: step 19620, loss = 0.85 (1632.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:19:28.706620: step 19630, loss = 0.81 (1620.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:19:29.500724: step 19640, loss = 0.73 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:19:30.297244: step 19650, loss = 0.79 (1607.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:19:31.090801: step 19660, loss = 0.83 (1613.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:19:31.877062: step 19670, loss = 0.73 (1628.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:19:32.672670: step 19680, loss = 0.86 (1608.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:19:33.469830: step 19690, loss = 0.84 (1605.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:19:34.350294: step 19700, loss = 0.73 (1453.8 examples/sec; 0.088 sec/batch)
2017-05-02 15:19:35.036454: step 19710, loss = 0.72 (1865.5 examples/sec; 0.069 sec/batch)
2017-05-02 15:19:35.818535: step 19720, loss = 0.82 (1636.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:19:36.611038: step 19730, loss = 0.85 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:19:37.409335: step 19740, loss = 0.66 (1603.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:19:38.199373: step 19750, loss = 0.91 (1620.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:19:38.989908: step 19760, loss = 0.92 (1619.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:19:39.767611: step 19770, loss = 0.80 (1645.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:19:40.564987: step 19780, loss = 0.80 (1605.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:19:41.367256: step 19790, loss = 0.74 (1595.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:19:42.267929: step 19800, loss = 0.84 (1421.2 examples/sec; 0.090 sec/batch)
2017-05-02 15:19:42.962396: step 19810, loss = 0.80 (1843.1 examples/sec; 0.069 sec/batch)
2017-05-02 15:19:43.747804: step 19820, loss = 0.68 (1629.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:19:44.540663: step 19830, loss = 0.71 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:19:45.332114: step 19840, loss = 0.78 (1617.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:19:46.121653: step 19850, loss = 0.82 (1621.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:19:46.907521: step 19860, loss = 0.86 (1628.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:19:47.690752: step 19870, loss = 0.75 (1634.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:19:48.477973: step 19880, loss = 0.95 (1626.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:19:49.271206: step 19890, loss = 0.83 (1613.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:19:50.164850: step 19900, loss = 0.68 (1432.3 examples/sec; 0.089 sec/batch)
2017-05-02 15:19:50.861257: step 19910, loss = 0.81 (1838.0 examples/sec; 0.070 sec/batch)
2017-05-02 15:19:51.642684: step 19920, loss = 0.76 (1638.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:19:52.435167: step 19930, loss = 0.84 (1615.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:19:53.226819: step 19940, loss = 0.75 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:19:54.017568: step 19950, loss = 0.87 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:19:54.815947: step 19960, loss = 0.90 (1603.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:19:55.598842: step 19970, loss = 0.84 (1635.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:19:56.386874: step 19980, loss = 0.84 (1624.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:19:57.183457: step 19990, loss = 0.75 (1606.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:19:58.064601: step 20000, loss = 0.85 (1452.7 examples/sec; 0.088 sec/batch)
2017-05-02 15:19:58.756531: step 20010, loss = 0.91 (1849.9 examples/sec; 0.069 sec/batch)
2017-05-02 15:19:59.540081: step 20020, loss = 0.86 (1633.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:20:00.326116: step 20030, loss = 0.77 (1628.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:01.123602: step 20040, loss = 0.79 (1605.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:20:01.925820: step 20050, loss = 0.67 (1595.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:20:02.719965: step 20060, loss = 1.03 (1611.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:03.506191: step 20070, loss = 0.60 (1628.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:04.289932: step 20080, loss = 0.83 (1633.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:20:05.077960: step 20090, loss = 0.81 (1624.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:05.975219: step 20100, loss = 0.76 (1426.6 examples/sec; 0.090 sec/batch)
2017-05-02 15:20:06.666698: step 20110, loss = 0.79 (1851.1 examples/sec; 0.069 sec/batch)
2017-05-02 15:20:07.456330: step 20120, loss = 0.93 (1621.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:08.240315: step 20130, loss = 0.78 (1632.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:20:09.036872: step 20140, loss = 0.71 (1606.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:20:09.824129: step 20150, loss = 0.93 (1625.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:10.617211: step 20160, loss = 0.93 (1614.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:11.405937: step 20170, loss = 0.82 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:12.204130: step 20180, loss = 0.70 (1603.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:20:12.990597: step 20190, loss = 1.00 (1627.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:13.887670: step 20200, loss = 0.84 (1426.9 examples/sec; 0.090 sec/batch)
2017-05-02 15:20:14.582832: step 20210, loss = 0.90 (1841.3 examples/sec; 0.070 sec/batch)
2017-05-02 15:20:15.372586: step 20220, loss = 0.91 (1620.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:16.154281: step 20230, loss = 0.90 (1637.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:20:16.944956: step 20240, loss = 0.80 (1618.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:17.733086: step 20250, loss = 0.83 (1624.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:18.524269: step 20260, loss = 0.70 (1617.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:19.315445: step 20270, loss = 0.66 (1617.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:20.097358: step 20280, loss = 0.79 (1637.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:20:20.894926: step 20290, loss = 0.71 (1604.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:20:21.785300: step 20300, loss = 0.91 (1437.6 examples/sec; 0.089 sec/batch)
2017-05-02 15:20:22.479494: step 20310, loss = 0.85 (1843.9 examples/sec; 0.069 sec/batch)
2017-05-02 15:20:23.267082: step 20320, loss = 0.85 (1625.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:24.049827: step 20330, loss = 0.89 (1635.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:20:24.836540: step 20340, loss = 0.88 (1627.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:25.640653: step 20350, loss = 1.02 (1591.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:20:26.436436: step 20360, loss = 0.89 (1608.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:20:27.227312: step 20370, loss = 0.75 (1618.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:28.012508: step 20380, loss = 0.77 (1630.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:28.804714: step 20390, loss = 0.81 (1615.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:29.694215: step 20400, loss = 0.80 (1439.0 examples/sec; 0.089 sec/batch)
2017-05-02 15:20:30.387631: step 20410, loss = 0.72 (1845.9 examples/sec; 0.069 sec/batch)
2017-05-02 15:20:31.177773: step 20420, loss = 0.69 (1620.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:31.959628: step 20430, loss = 1.02 (1637.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:20:32.754298: step 20440, loss = 0.80 (1610.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:33.544820: step 20450, loss = 0.67 (1619.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:34.336491: step 20460, loss = 0.62 (1616.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:35.130120: step 20470, loss = 0.70 (1612.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:35.914631: step 20480, loss = 0.85 (1631.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:20:36.710952: step 20490, loss = 0.78 (1607.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:20:37.601046: step 20500, loss = 0.90 (1438.1 examples/sec; 0.089 sec/batch)
2017-05-02 15:20:38.293025: step 20510, loss = 0.71 (1849.8 examples/sec; 0.069 sec/batch)
2017-05-02 15:20:39.092210: step 20520, loss = 0.76 (1601.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:20:39.873109: step 20530, loss = 0.78 (1639.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:20:40.668226: step 20540, loss = 0.89 (1609.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:20:41.453831: step 20550, loss = 0.77 (1629.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:42.245996: step 20560, loss = 0.68 (1615.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:43.040502: step 20570, loss = 0.76 (1611.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:43.831839: step 20580, loss = 0.78 (1617.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:44.622672: step 20590, loss = 0.79 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:45.511653: step 20600, loss = 0.79 (1439.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:20:46.207266: step 20610, loss = 0.75 (1840.1 examples/sec; 0.070 sec/batch)
2017-05-02 15:20:46.995248: step 20620, loss = 1.00 (1624.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:47.775406: step 20630, loss = 0.74 (1640.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:20:48.572121: step 20640, loss = 1.11 (1606.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:20:49.359242: step 20650, loss = 0.76 (1626.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:50.154742: step 20660, loss = 0.92 (1609.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:20:50.942833: step 20670, loss = 0.78 (1624.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:51.727064: step 20680, loss = 0.76 (1632.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:20:52.527722: step 20690, loss = 0.75 (1598.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:20:53.426058: step 20700, loss = 0.80 (1424.9 examples/sec; 0.090 sec/batch)
2017-05-02 15:20:54.124766: step 20710, loss = 0.79 (1832.0 examples/sec; 0.070 sec/batch)
2017-05-02 15:20:54.920012: step 20720, loss = 0.83 (1609.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:20:55.708769: step 20730, loss = 0.72 (1622.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:56.508049: step 20740, loss = 0.90 (1601.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:20:57.304981: step 20750, loss = 0.91 (1606.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:20:58.094536: step 20760, loss = 0.73 (1621.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:58.886585: step 20770, loss = 0.82 (1616.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:20:59.670486: step 20780, loss = 0.70 (1632.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:21:00.463064: step 20790, loss = 0.82 (1615.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:01.363018: step 20800, loss = 0.70 (1422.3 examples/sec; 0.090 sec/batch)
2017-05-02 15:21:02.045540: step 20810, loss = 1.01 (1875.4 examples/sec; 0.068 sec/batch)
2017-05-02 15:21:02.836200: step 20820, loss = 0.78 (1618.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:03.623037: step 20830, loss = 0.88 (1626.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:04.412853: step 20840, loss = 0.84 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:05.199321: step 20850, loss = 0.76 (1627.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:05.989491: step 20860, loss = 0.75 (1619.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:06.777320: step 20870, loss = 0.64 (1624.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:07.558946: step 20880, loss = 0.90 (1637.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:21:08.355052: step 20890, loss = 0.94 (1607.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:21:09.234806: step 20900, loss = 0.77 (1455.0 examples/sec; 0.088 sec/batch)
2017-05-02 15:21:09.927435: step 20910, loss = 0.89 (1848.0 examples/sec; 0.069 sec/batch)
2017-05-02 15:21:10.726487: step 20920, loss = 0.74 (1601.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:21:11.510573: step 20930, loss = 0.79 (1632.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:21:12.302757: step 20940, loss = 0.75 (1615.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:13.093434: step 20950, loss = 0.76 (1618.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:13.883764: step 20960, loss = 0.83 (1619.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:14.670614: step 20970, loss = 0.88 (1626.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:15.463602: step 20980, loss = 0.66 (1614.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:16.248886: step 20990, loss = 0.76 (1630.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:17.142406: step 21000, loss = 1.02 (1432.5 examples/sec; 0.089 sec/batch)
2017-05-02 15:21:17.840852: step 21010, loss = 0.98 (1832.7 examples/sec; 0.070 sec/batch)
2017-05-02 15:21:18.637172: step 21020, loss = 0.77 (1607.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:21:19.420288: step 21030, loss = 0.89 (1634.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:21:20.208013: step 21040, loss = 0.90 (1625.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:20.999814: step 21050, loss = 0.68 (1616.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:21.791755: step 21060, loss = 0.86 (1616.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:22.582713: step 21070, loss = 0.85 (1618.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:23.373723: step 21080, loss = 0.70 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:24.165595: step 21090, loss = 1.07 (1616.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:25.062856: step 21100, loss = 0.82 (1426.6 examples/sec; 0.090 sec/batch)
2017-05-02 15:21:25.755383: step 21110, loss = 0.69 (1848.3 examples/sec; 0.069 sec/batch)
2017-05-02 15:21:26.544329: step 21120, loss = 0.74 (1622.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:27.332521: step 21130, loss = 0.83 (1624.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:28.105625: step 21140, loss = 0.74 (1655.7 examples/sec; 0.077 sec/batch)
2017-05-02 15:21:28.898696: step 21150, loss = 0.71 (1614.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:29.689129: step 21160, loss = 0.78 (1619.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:30.485091: step 21170, loss = 0.87 (1608.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:21:31.272000: step 21180, loss = 0.97 (1626.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:32.059798: step 21190, loss = 0.75 (1624.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:32.949416: step 21200, loss = 0.83 (1438.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:21:33.641659: step 21210, loss = 0.78 (1849.0 examples/sec; 0.069 sec/batch)
2017-05-02 15:21:34.432446: step 21220, loss = 0.86 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:35.220581: step 21230, loss = 0.78 (1624.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:36.015107: step 21240, loss = 0.86 (1611.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:36.809145: step 21250, loss = 0.79 (1612.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:37.606561: step 21260, loss = 0.66 (1605.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:21:38.400370: step 21270, loss = 0.97 (1612.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:39.190528: step 21280, loss = 0.89 (1619.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:39.968018: step 21290, loss = 0.72 (1646.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:21:40.864752: step 21300, loss = 0.68 (1427.4 examples/sec; 0.090 sec/batch)
2017-05-02 15:21:41.560918: step 21310, loss = 0.84 (1838.6 examples/sec; 0.070 sec/batch)
2017-05-02 15:21:42.352595: step 21320, loss = 0.86 (1616.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:43.155372: step 21330, loss = 0.79 (1594.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:21:43.936651: step 21340, loss = 0.91 (1638.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:21:44.724966: step 21350, loss = 0.80 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:45.523742: step 21360, loss = 0.73 (1602.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:21:46.314321: step 21370, loss = 0.88 (1619.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:47.111725: step 21380, loss = 0.79 (1605.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:21:47.888828: step 21390, loss = 0.82 (1647.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:21:48.787307: step 21400, loss = 0.93 (1424.6 examples/sec; 0.090 sec/batch)
2017-05-02 15:21:49.475793: step 21410, loss = 0.73 (1859.1 examples/sec; 0.069 sec/batch)
2017-05-02 15:21:50.257770: step 21420, loss = 0.77 (1636.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:21:51.060082: step 21430, loss = 0.95 (1595.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:21:51.836913: step 21440, loss = 0.95 (1647.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:21:52.628979: step 21450, loss = 0.90 (1616.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:53.424826: step 21460, loss = 0.80 (1608.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:21:54.214706: step 21470, loss = 0.90 (1620.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:55.020350: step 21480, loss = 0.96 (1588.8 examples/sec; 0.081 sec/batch)
2017-05-02 15:21:55.796132: step 21490, loss = 0.74 (1650.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:21:56.691586: step 21500, loss = 0.75 (1429.4 examples/sec; 0.090 sec/batch)
2017-05-02 15:21:57.384169: step 21510, loss = 0.79 (1848.2 examples/sec; 0.069 sec/batch)
2017-05-02 15:21:58.172946: step 21520, loss = 0.69 (1622.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:58.963749: step 21530, loss = 0.88 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:21:59.744023: step 21540, loss = 0.94 (1640.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:22:00.542499: step 21550, loss = 0.69 (1603.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:22:01.325853: step 21560, loss = 0.87 (1634.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:22:02.117340: step 21570, loss = 0.71 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:02.911752: step 21580, loss = 0.79 (1611.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:03.690129: step 21590, loss = 0.80 (1644.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:22:04.581263: step 21600, loss = 0.88 (1436.4 examples/sec; 0.089 sec/batch)
2017-05-02 15:22:05.278855: step 21610, loss = 0.71 (1834.9 examples/sec; 0.070 sec/batch)
2017-05-02 15:22:06.077855: step 21620, loss = 0.82 (1602.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:22:06.866919: step 21630, loss = 0.82 (1622.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:07.649823: step 21640, loss = 0.75 (1634.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:22:08.438688: step 21650, loss = 0.83 (1622.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:09.227133: step 21660, loss = 0.75 (1623.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:10.012541: step 21670, loss = 0.82 (1629.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:10.807012: step 21680, loss = 0.75 (1611.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:11.589407: step 21690, loss = 0.63 (1636.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:22:12.485081: step 21700, loss = 0.70 (1429.1 examples/sec; 0.090 sec/batch)
2017-05-02 15:22:13.187256: step 21710, loss = 0.94 (1822.9 examples/sec; 0.070 sec/batch)
2017-05-02 15:22:13.982496: step 21720, loss = 0.89 (1609.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:22:14.776763: step 21730, loss = 0.74 (1611.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:15.559324: step 21740, loss = 0.86 (1635.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:22:16.352094: step 21750, loss = 0.82 (1614.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:17.144327: step 21760, loss = 0.68 (1615.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:17.943834: step 21770, loss = 0.76 (1601.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:22:18.734569: step 21780, loss = 0.71 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:19.527181: step 21790, loss = 0.90 (1614.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:20.421515: step 21800, loss = 0.57 (1431.2 examples/sec; 0.089 sec/batch)
2017-05-02 15:22:21.109987: step 21810, loss = 0.77 (1859.2 examples/sec; 0.069 sec/batch)
2017-05-02 15:22:21.905090: step 21820, loss = 0.76 (1609.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:22:22.697443: step 21830, loss = 0.84 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:23.489151: step 21840, loss = 0.92 (1616.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:24.277318: step 21850, loss = 0.81 (1624.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:25.065916: step 21860, loss = 0.85 (1623.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:25.860977: step 21870, loss = 0.71 (1610.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:22:26.661544: step 21880, loss = 0.98 (1598.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:22:27.457202: step 21890, loss = 0.88 (1608.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:22:28.341142: step 21900, loss = 0.74 (1448.1 examples/sec; 0.088 sec/batch)
2017-05-02 15:22:29.042252: step 21910, loss = 0.67 (1825.7 examples/sec; 0.070 sec/batch)
2017-05-02 15:22:29.833299: step 21920, loss = 0.85 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:30.626242: step 21930, loss = 0.67 (1614.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:31.420339: step 21940, loss = 0.73 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:32.210351: step 21950, loss = 0.73 (1620.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:33.003378: step 21960, loss = 0.81 (1614.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:33.795555: step 21970, loss = 0.89 (1615.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:34.585416: step 21980, loss = 0.83 (1620.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:35.379587: step 21990, loss = 0.72 (1611.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:36.260717: step 22000, loss = 0.74 (1452.7 examples/sec; 0.088 sec/batch)
2017-05-02 15:22:36.956345: step 22010, loss = 0.75 (1840.1 examples/sec; 0.070 sec/batch)
2017-05-02 15:22:37.744308: step 22020, loss = 0.85 (1624.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:38.546451: step 22030, loss = 0.78 (1595.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:22:39.342194: step 22040, loss = 0.88 (1608.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:22:40.123705: step 22050, loss = 0.78 (1637.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:22:40.915176: step 22060, loss = 0.78 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:41.703284: step 22070, loss = 0.73 (1624.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:42.498408: step 22080, loss = 0.81 (1609.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:22:43.291312: step 22090, loss = 0.72 (1614.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:44.177844: step 22100, loss = 0.68 (1443.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:22:44.869062: step 22110, loss = 0.87 (1851.8 examples/sec; 0.069 sec/batch)
2017-05-02 15:22:45.668542: step 22120, loss = 0.70 (1601.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:22:46.460177: step 22130, loss = 0.92 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:47.251065: step 22140, loss = 0.82 (1618.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:48.040816: step 22150, loss = 0.85 (1620.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:48.828020: step 22160, loss = 0.79 (1626.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:49.618500: step 22170, loss = 0.81 (1619.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:50.409908: step 22180, loss = 0.66 (1617.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:51.201870: step 22190, loss = 0.79 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:52.088509: step 22200, loss = 0.84 (1443.7 examples/sec; 0.089 sec/batch)
2017-05-02 15:22:52.778104: step 22210, loss = 0.65 (1856.1 examples/sec; 0.069 sec/batch)
2017-05-02 15:22:53.577585: step 22220, loss = 0.83 (1601.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:22:54.369953: step 22230, loss = 0.95 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:55.156255: step 22240, loss = 0.69 (1627.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:55.951821: step 22250, loss = 0.82 (1608.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:22:56.745405: step 22260, loss = 0.78 (1612.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:22:57.546631: step 22270, loss = 0.65 (1597.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:22:58.344255: step 22280, loss = 0.80 (1604.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:22:59.132477: step 22290, loss = 0.86 (1623.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:00.009328: step 22300, loss = 0.88 (1459.8 examples/sec; 0.088 sec/batch)
2017-05-02 15:23:00.703126: step 22310, loss = 0.71 (1844.9 examples/sec; 0.069 sec/batch)
2017-05-02 15:23:01.496514: step 22320, loss = 0.71 (1613.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:02.296324: step 22330, loss = 0.82 (1600.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:23:03.091802: step 22340, loss = 0.71 (1609.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:23:03.871997: step 22350, loss = 0.92 (1640.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:23:04.667360: step 22360, loss = 0.95 (1609.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:23:05.466185: step 22370, loss = 0.93 (1602.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:23:06.254903: step 22380, loss = 0.87 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:07.049086: step 22390, loss = 0.78 (1611.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:07.939206: step 22400, loss = 0.75 (1438.0 examples/sec; 0.089 sec/batch)
2017-05-02 15:23:08.620575: step 22410, loss = 0.68 (1878.6 examples/sec; 0.068 sec/batch)
2017-05-02 15:23:09.411531: step 22420, loss = 1.04 (1618.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:10.205145: step 22430, loss = 0.71 (1612.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:10.994819: step 22440, loss = 0.74 (1620.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:11.777788: step 22450, loss = 0.84 (1634.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:23:12.569653: step 22460, loss = 0.89 (1616.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:13.365995: step 22470, loss = 0.85 (1607.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:23:14.149623: step 22480, loss = 0.79 (1633.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:23:14.942088: step 22490, loss = 0.77 (1615.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:15.825171: step 22500, loss = 0.78 (1449.5 examples/sec; 0.088 sec/batch)
2017-05-02 15:23:16.516850: step 22510, loss = 0.85 (1850.6 examples/sec; 0.069 sec/batch)
2017-05-02 15:23:17.316827: step 22520, loss = 0.89 (1600.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:23:18.105363: step 22530, loss = 0.74 (1623.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:18.900157: step 22540, loss = 0.65 (1610.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:19.683832: step 22550, loss = 0.94 (1633.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:23:20.488395: step 22560, loss = 0.74 (1590.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:23:21.283708: step 22570, loss = 0.81 (1609.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:23:22.077767: step 22580, loss = 0.83 (1612.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:22.872220: step 22590, loss = 0.70 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:23.747911: step 22600, loss = 0.76 (1461.8 examples/sec; 0.088 sec/batch)
2017-05-02 15:23:24.449958: step 22610, loss = 0.74 (1823.2 examples/sec; 0.070 sec/batch)
2017-05-02 15:23:25.237791: step 22620, loss = 0.98 (1624.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:26.033842: step 22630, loss = 0.82 (1607.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:23:26.831404: step 22640, loss = 0.76 (1604.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:23:27.613440: step 22650, loss = 0.79 (1636.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:23:28.399756: step 22660, loss = 0.78 (1627.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:29.187722: step 22670, loss = 0.91 (1624.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:30.032340: step 22680, loss = 0.71 (1515.5 examples/sec; 0.084 sec/batch)
2017-05-02 15:23:30.826783: step 22690, loss = 0.76 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:31.728360: step 22700, loss = 0.79 (1419.7 examples/sec; 0.090 sec/batch)
2017-05-02 15:23:32.413394: step 22710, loss = 0.77 (1868.5 examples/sec; 0.069 sec/batch)
2017-05-02 15:23:33.205508: step 22720, loss = 0.73 (1615.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:33.996070: step 22730, loss = 0.77 (1619.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:34.786267: step 22740, loss = 0.81 (1619.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:35.575678: step 22750, loss = 0.73 (1621.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:36.369569: step 22760, loss = 0.72 (1612.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:37.159352: step 22770, loss = 0.91 (1620.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:37.950936: step 22780, loss = 0.90 (1617.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:38.743747: step 22790, loss = 0.86 (1614.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:39.631030: step 22800, loss = 1.07 (1442.6 examples/sec; 0.089 sec/batch)
2017-05-02 15:23:40.322034: step 22810, loss = 0.93 (1852.4 examples/sec; 0.069 sec/batch)
2017-05-02 15:23:41.119640: step 22820, loss = 0.80 (1604.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:23:41.911564: step 22830, loss = 0.92 (1616.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:42.701778: step 22840, loss = 0.79 (1619.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:43.491024: step 22850, loss = 0.99 (1621.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:44.276182: step 22860, loss = 0.85 (1630.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:45.068586: step 22870, loss = 0.87 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:45.864154: step 22880, loss = 0.82 (1608.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:23:46.662721: step 22890, loss = 0.73 (1602.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:23:47.552488: step 22900, loss = 0.75 (1438.6 examples/sec; 0.089 sec/batch)
2017-05-02 15:23:48.244271: step 22910, loss = 1.00 (1850.3 examples/sec; 0.069 sec/batch)
2017-05-02 15:23:49.032710: step 22920, loss = 0.88 (1623.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:49.823488: step 22930, loss = 0.73 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:50.620231: step 22940, loss = 0.75 (1606.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:23:51.407965: step 22950, loss = 0.74 (1624.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:52.191531: step 22960, loss = 0.93 (1633.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:23:52.975366: step 22970, loss = 0.95 (1633.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:23:53.771777: step 22980, loss = 0.72 (1607.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:23:54.566734: step 22990, loss = 0.69 (1610.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:55.456022: step 23000, loss = 0.74 (1439.4 examples/sec; 0.089 sec/batch)
2017-05-02 15:23:56.155541: step 23010, loss = 0.86 (1829.8 examples/sec; 0.070 sec/batch)
2017-05-02 15:23:56.948306: step 23020, loss = 0.93 (1614.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:57.741658: step 23030, loss = 0.88 (1613.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:23:58.537804: step 23040, loss = 0.63 (1607.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:23:59.338100: step 23050, loss = 0.69 (1599.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:24:00.130995: step 23060, loss = 0.55 (1614.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:24:00.930226: step 23070, loss = 0.75 (1601.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:24:01.723698: step 23080, loss = 0.82 (1613.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:24:02.518657: step 23090, loss = 0.81 (1610.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:24:03.429074: step 23100, loss = 0.86 (1405.9 examples/sec; 0.091 sec/batch)
2017-05-02 15:24:04.105321: step 23110, loss = 0.73 (1892.8 examples/sec; 0.068 sec/batch)
2017-05-02 15:24:04.902759: step 23120, loss = 0.79 (1605.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:24:05.699121: step 23130, loss = 0.81 (1607.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:24:06.484372: step 23140, loss = 0.77 (1630.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:24:07.277573: step 23150, loss = 0.94 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:24:08.064009: step 23160, loss = 0.88 (1627.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:24:08.856218: step 23170, loss = 0.82 (1615.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:24:09.651703: step 23180, loss = 0.75 (1609.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:24:10.447070: step 23190, loss = 0.82 (1609.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:24:11.340585: step 23200, loss = 0.83 (1432.5 examples/sec; 0.089 sec/batch)
2017-05-02 15:24:12.025667: step 23210, loss = 0.84 (1868.4 examples/sec; 0.069 sec/batch)
2017-05-02 15:24:12.818153: step 23220, loss = 0.59 (1615.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:24:13.615210: step 23230, loss = 0.71 (1605.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:24:14.402719: step 23240, loss = 0.73 (1625.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:24:15.197101: step 23250, loss = 0.77 (1611.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:24:15.980580: step 23260, loss = 0.73 (1633.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:24:16.772025: step 23270, loss = 0.84 (1617.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:24:17.562348: step 23280, loss = 0.93 (1619.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:24:18.359334: step 23290, loss = 0.78 (1606.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:24:19.268505: step 23300, loss = 0.88 (1407.9 examples/sec; 0.091 sec/batch)
2017-05-02 15:24:19.940005: step 23310, loss = 0.81 (1906.2 examples/sec; 0.067 sec/batch)
2017-05-02 15:24:20.730338: step 23320, loss = 0.72 (1619.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:24:21.523684: step 23330, loss = 0.76 (1613.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:24:22.312795: step 23340, loss = 0.73 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:24:23.099206: step 23350, loss = 0.97 (1627.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:24:23.880389: step 23360, loss = 0.86 (1638.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:24:24.681243: step 23370, loss = 0.83 (1598.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:24:25.480863: step 23380, loss = 0.83 (1600.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:24:26.274043: step 23390, loss = 0.80 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:24:27.176683: step 23400, loss = 0.82 (1418.1 examples/sec; 0.090 sec/batch)
2017-05-02 15:24:27.866510: step 23410, loss = 0.71 (1855.5 examples/sec; 0.069 sec/batch)
2017-05-02 15:24:28.672364: step 23420, loss = 0.89 (1588.4 examples/sec; 0.081 sec/batch)
2017-05-02 15:24:29.469450: step 23430, loss = 0.74 (1605.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:24:30.258636: step 23440, loss = 0.82 (1621.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:24:31.056468: step 23450, loss = 1.06 (1604.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:24:31.846660: step 23460, loss = 0.80 (1619.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:24:32.647584: step 23470, loss = 0.76 (1598.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:24:33.434282: step 23480, loss = 0.71 (1627.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:24:34.230425: step 23490, loss = 0.91 (1607.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:24:35.135223: step 23500, loss = 1.03 (1414.7 examples/sec; 0.090 sec/batch)
2017-05-02 15:24:35.826201: step 23510, loss = 0.77 (1852.4 examples/sec; 0.069 sec/batch)
2017-05-02 15:24:36.618955: step 23520, loss = 0.98 (1614.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:24:37.411683: step 23530, loss = 0.87 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:24:38.209429: step 23540, loss = 0.79 (1604.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:24:39.009955: step 23550, loss = 0.92 (1598.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:24:39.789476: step 23560, loss = 0.84 (1642.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:24:40.586929: step 23570, loss = 0.90 (1605.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:24:41.382632: step 23580, loss = 0.67 (1608.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:24:42.173104: step 23590, loss = 0.79 (1619.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:24:43.075300: step 23600, loss = 0.71 (1418.8 examples/sec; 0.090 sec/batch)
2017-05-02 15:24:43.755495: step 23610, loss = 1.08 (1881.8 examples/sec; 0.068 sec/batch)
2017-05-02 15:24:44.554060: step 23620, loss = 0.83 (1602.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:24:45.347176: step 23630, loss = 0.73 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:24:46.139730: step 23640, loss = 0.99 (1615.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:24:46.937874: step 23650, loss = 0.82 (1603.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:24:47.719238: step 23660, loss = 0.75 (1638.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:24:48.516544: step 23670, loss = 0.82 (1605.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:24:49.308030: step 23680, loss = 0.78 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:24:50.100314: step 23690, loss = 0.83 (1615.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:24:50.994487: step 23700, loss = 0.85 (1431.5 examples/sec; 0.089 sec/batch)
2017-05-02 15:24:51.683959: step 23710, loss = 0.79 (1856.5 examples/sec; 0.069 sec/batch)
2017-05-02 15:24:52.480426: step 23720, loss = 1.00 (1607.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:24:53.272143: step 23730, loss = 0.78 (1616.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:24:54.067085: step 23740, loss = 0.79 (1610.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:24:54.866107: step 23750, loss = 0.66 (1602.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:24:55.648523: step 23760, loss = 0.83 (1636.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:24:56.445944: step 23770, loss = 1.01 (1605.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:24:57.246704: step 23780, loss = 0.89 (1598.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:24:58.044744: step 23790, loss = 0.91 (1603.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:24:58.934341: step 23800, loss = 0.97 (1438.9 examples/sec; 0.089 sec/batch)
2017-05-02 15:24:59.627310: step 23810, loss = 0.83 (1847.1 examples/sec; 0.069 sec/batch)
2017-05-02 15:25:00.421075: step 23820, loss = 0.78 (1612.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:01.219969: step 23830, loss = 0.86 (1602.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:25:02.027285: step 23840, loss = 0.92 (1585.5 examples/sec; 0.081 sec/batch)
2017-05-02 15:25:02.832449: step 23850, loss = 0.80 (1589.7 examples/sec; 0.081 sec/batch)
2017-05-02 15:25:03.623174: step 23860, loss = 0.81 (1618.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:04.417307: step 23870, loss = 0.72 (1611.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:05.216040: step 23880, loss = 0.79 (1602.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:25:06.022274: step 23890, loss = 0.88 (1587.6 examples/sec; 0.081 sec/batch)
2017-05-02 15:25:06.916818: step 23900, loss = 0.73 (1430.9 examples/sec; 0.089 sec/batch)
2017-05-02 15:25:07.608058: step 23910, loss = 0.73 (1851.8 examples/sec; 0.069 sec/batch)
2017-05-02 15:25:08.400286: step 23920, loss = 0.85 (1615.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:09.192957: step 23930, loss = 0.71 (1614.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:09.996721: step 23940, loss = 0.74 (1592.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:25:10.797204: step 23950, loss = 0.77 (1599.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:25:11.581129: step 23960, loss = 0.81 (1632.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:25:12.372572: step 23970, loss = 0.73 (1617.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:13.163180: step 23980, loss = 0.96 (1619.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:13.959766: step 23990, loss = 0.72 (1606.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:25:14.854869: step 24000, loss = 0.87 (1430.0 examples/sec; 0.090 sec/batch)
2017-05-02 15:25:15.549931: step 24010, loss = 0.89 (1841.6 examples/sec; 0.070 sec/batch)
2017-05-02 15:25:16.344660: step 24020, loss = 0.89 (1610.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:17.140819: step 24030, loss = 0.87 (1607.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:25:17.937637: step 24040, loss = 0.84 (1606.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:25:18.730608: step 24050, loss = 0.84 (1614.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:19.511198: step 24060, loss = 0.80 (1639.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:25:20.306045: step 24070, loss = 0.78 (1610.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:21.096348: step 24080, loss = 0.80 (1619.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:21.892923: step 24090, loss = 0.77 (1606.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:25:22.800077: step 24100, loss = 0.88 (1411.0 examples/sec; 0.091 sec/batch)
2017-05-02 15:25:23.494313: step 24110, loss = 0.85 (1843.7 examples/sec; 0.069 sec/batch)
2017-05-02 15:25:24.285477: step 24120, loss = 0.72 (1617.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:25.076277: step 24130, loss = 0.75 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:25.864525: step 24140, loss = 0.88 (1623.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:26.648275: step 24150, loss = 0.77 (1633.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:25:27.440060: step 24160, loss = 0.73 (1616.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:28.229573: step 24170, loss = 0.70 (1621.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:29.027034: step 24180, loss = 0.73 (1605.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:25:29.813666: step 24190, loss = 0.78 (1627.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:30.708683: step 24200, loss = 0.81 (1430.1 examples/sec; 0.090 sec/batch)
2017-05-02 15:25:31.394411: step 24210, loss = 0.87 (1866.6 examples/sec; 0.069 sec/batch)
2017-05-02 15:25:32.185946: step 24220, loss = 0.67 (1617.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:32.979934: step 24230, loss = 0.70 (1612.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:33.777090: step 24240, loss = 0.76 (1605.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:25:34.567077: step 24250, loss = 0.76 (1620.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:35.359217: step 24260, loss = 0.82 (1615.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:36.140654: step 24270, loss = 0.75 (1638.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:25:36.927511: step 24280, loss = 0.93 (1626.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:37.721597: step 24290, loss = 0.75 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:38.623591: step 24300, loss = 0.93 (1419.1 examples/sec; 0.090 sec/batch)
2017-05-02 15:25:39.310559: step 24310, loss = 0.76 (1863.3 examples/sec; 0.069 sec/batch)
2017-05-02 15:25:40.090092: step 24320, loss = 0.91 (1642.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:25:40.878852: step 24330, loss = 0.83 (1622.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:41.666214: step 24340, loss = 0.69 (1625.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:42.457931: step 24350, loss = 0.75 (1616.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:43.265571: step 24360, loss = 0.82 (1584.9 examples/sec; 0.081 sec/batch)
2017-05-02 15:25:44.037730: step 24370, loss = 0.92 (1657.7 examples/sec; 0.077 sec/batch)
2017-05-02 15:25:44.828070: step 24380, loss = 0.93 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:45.625023: step 24390, loss = 0.72 (1606.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:25:46.519522: step 24400, loss = 0.77 (1431.0 examples/sec; 0.089 sec/batch)
2017-05-02 15:25:47.214636: step 24410, loss = 0.70 (1841.4 examples/sec; 0.070 sec/batch)
2017-05-02 15:25:48.002759: step 24420, loss = 0.90 (1624.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:48.796028: step 24430, loss = 0.83 (1613.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:49.592102: step 24440, loss = 1.00 (1607.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:25:50.388921: step 24450, loss = 0.79 (1606.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:25:51.179850: step 24460, loss = 0.86 (1618.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:51.964281: step 24470, loss = 0.83 (1631.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:25:52.758350: step 24480, loss = 0.74 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:53.550257: step 24490, loss = 0.72 (1616.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:54.441434: step 24500, loss = 0.66 (1436.3 examples/sec; 0.089 sec/batch)
2017-05-02 15:25:55.137836: step 24510, loss = 0.84 (1838.0 examples/sec; 0.070 sec/batch)
2017-05-02 15:25:55.921316: step 24520, loss = 0.76 (1633.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:25:56.713055: step 24530, loss = 0.88 (1616.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:57.506950: step 24540, loss = 0.73 (1612.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:58.299503: step 24550, loss = 0.72 (1615.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:59.091291: step 24560, loss = 0.83 (1616.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:25:59.873286: step 24570, loss = 0.77 (1636.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:26:00.666174: step 24580, loss = 0.81 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:01.457487: step 24590, loss = 0.78 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:02.353669: step 24600, loss = 0.97 (1428.3 examples/sec; 0.090 sec/batch)
2017-05-02 15:26:03.042748: step 24610, loss = 0.76 (1857.6 examples/sec; 0.069 sec/batch)
2017-05-02 15:26:03.823715: step 24620, loss = 0.81 (1639.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:26:04.620528: step 24630, loss = 0.90 (1606.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:26:05.413183: step 24640, loss = 0.75 (1614.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:06.207577: step 24650, loss = 0.76 (1611.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:07.001054: step 24660, loss = 0.92 (1613.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:07.777230: step 24670, loss = 0.87 (1649.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:26:08.567576: step 24680, loss = 0.73 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:09.359466: step 24690, loss = 0.84 (1616.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:10.254161: step 24700, loss = 0.82 (1430.7 examples/sec; 0.089 sec/batch)
2017-05-02 15:26:10.947674: step 24710, loss = 0.77 (1845.7 examples/sec; 0.069 sec/batch)
2017-05-02 15:26:11.732410: step 24720, loss = 0.79 (1631.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:26:12.528696: step 24730, loss = 0.89 (1607.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:26:13.319935: step 24740, loss = 0.79 (1617.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:14.104929: step 24750, loss = 0.77 (1630.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:26:14.897680: step 24760, loss = 0.88 (1614.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:15.682271: step 24770, loss = 0.80 (1631.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:26:16.469600: step 24780, loss = 0.79 (1625.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:17.262891: step 24790, loss = 0.84 (1613.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:18.149659: step 24800, loss = 0.94 (1443.4 examples/sec; 0.089 sec/batch)
2017-05-02 15:26:18.850090: step 24810, loss = 0.67 (1827.5 examples/sec; 0.070 sec/batch)
2017-05-02 15:26:19.627264: step 24820, loss = 0.73 (1647.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:26:20.421929: step 24830, loss = 0.84 (1610.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:21.211332: step 24840, loss = 0.79 (1621.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:22.001367: step 24850, loss = 0.80 (1620.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:22.800729: step 24860, loss = 0.99 (1601.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:26:23.578004: step 24870, loss = 0.75 (1646.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:26:24.367500: step 24880, loss = 1.00 (1621.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:25.158694: step 24890, loss = 0.80 (1617.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:26.041694: step 24900, loss = 0.77 (1449.6 examples/sec; 0.088 sec/batch)
2017-05-02 15:26:26.740766: step 24910, loss = 0.67 (1831.0 examples/sec; 0.070 sec/batch)
2017-05-02 15:26:27.523805: step 24920, loss = 0.78 (1634.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:26:28.311638: step 24930, loss = 0.78 (1624.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:29.104977: step 24940, loss = 0.82 (1613.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:29.893782: step 24950, loss = 0.70 (1622.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:30.687201: step 24960, loss = 0.67 (1613.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:31.466750: step 24970, loss = 0.94 (1642.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:26:32.263442: step 24980, loss = 0.84 (1606.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:26:33.057224: step 24990, loss = 0.76 (1612.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:33.969003: step 25000, loss = 1.00 (1403.9 examples/sec; 0.091 sec/batch)
2017-05-02 15:26:34.646786: step 25010, loss = 0.87 (1888.5 examples/sec; 0.068 sec/batch)
2017-05-02 15:26:35.430923: step 25020, loss = 0.83 (1632.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:26:36.221620: step 25030, loss = 0.85 (1618.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:37.010547: step 25040, loss = 0.75 (1622.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:37.803746: step 25050, loss = 0.81 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:38.597193: step 25060, loss = 0.83 (1613.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:39.382733: step 25070, loss = 0.86 (1629.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:40.168974: step 25080, loss = 0.76 (1628.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:40.961585: step 25090, loss = 0.68 (1614.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:41.852704: step 25100, loss = 0.82 (1436.4 examples/sec; 0.089 sec/batch)
2017-05-02 15:26:42.558802: step 25110, loss = 0.92 (1812.8 examples/sec; 0.071 sec/batch)
2017-05-02 15:26:43.344135: step 25120, loss = 1.00 (1629.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:44.128289: step 25130, loss = 0.76 (1632.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:26:44.921125: step 25140, loss = 0.84 (1614.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:45.712010: step 25150, loss = 0.83 (1618.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:46.506714: step 25160, loss = 0.80 (1610.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:47.292034: step 25170, loss = 0.89 (1629.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:48.075515: step 25180, loss = 0.88 (1633.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:26:48.864146: step 25190, loss = 0.77 (1623.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:49.759074: step 25200, loss = 0.79 (1430.3 examples/sec; 0.089 sec/batch)
2017-05-02 15:26:50.450442: step 25210, loss = 0.69 (1851.4 examples/sec; 0.069 sec/batch)
2017-05-02 15:26:51.241709: step 25220, loss = 0.88 (1617.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:52.017820: step 25230, loss = 0.90 (1649.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:26:52.810306: step 25240, loss = 0.92 (1615.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:53.604962: step 25250, loss = 0.74 (1610.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:54.390096: step 25260, loss = 0.66 (1630.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:55.184395: step 25270, loss = 0.79 (1611.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:55.975563: step 25280, loss = 0.83 (1617.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:56.765399: step 25290, loss = 0.76 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:57.657275: step 25300, loss = 0.69 (1435.2 examples/sec; 0.089 sec/batch)
2017-05-02 15:26:58.357558: step 25310, loss = 0.89 (1827.8 examples/sec; 0.070 sec/batch)
2017-05-02 15:26:59.149216: step 25320, loss = 0.64 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:26:59.922842: step 25330, loss = 0.82 (1654.5 examples/sec; 0.077 sec/batch)
2017-05-02 15:27:00.718011: step 25340, loss = 0.81 (1609.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:27:01.508106: step 25350, loss = 0.83 (1620.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:27:02.304918: step 25360, loss = 0.80 (1606.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:27:03.092124: step 25370, loss = 0.93 (1626.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:27:03.879502: step 25380, loss = 0.81 (1625.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:27:04.663922: step 25390, loss = 0.79 (1631.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:27:05.584751: step 25400, loss = 0.83 (1390.1 examples/sec; 0.092 sec/batch)
2017-05-02 15:27:06.250928: step 25410, loss = 0.88 (1921.4 examples/sec; 0.067 sec/batch)
2017-05-02 15:27:07.041788: step 25420, loss = 0.88 (1618.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:27:07.824533: step 25430, loss = 0.81 (1635.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:27:08.613870: step 25440, loss = 0.83 (1621.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:27:09.397296: step 25450, loss = 0.72 (1633.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:27:10.189059: step 25460, loss = 0.85 (1616.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:27:10.983286: step 25470, loss = 0.90 (1611.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:27:11.764996: step 25480, loss = 1.04 (1637.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:27:12.552190: step 25490, loss = 0.74 (1626.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:27:13.447006: step 25500, loss = 0.79 (1430.5 examples/sec; 0.089 sec/batch)
2017-05-02 15:27:14.145771: step 25510, loss = 0.88 (1831.8 examples/sec; 0.070 sec/batch)
2017-05-02 15:27:14.938917: step 25520, loss = 0.75 (1613.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:27:15.718098: step 25530, loss = 0.70 (1642.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:27:16.514639: step 25540, loss = 0.79 (1606.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:27:17.307556: step 25550, loss = 0.81 (1614.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:27:18.105187: step 25560, loss = 0.83 (1604.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:27:18.892969: step 25570, loss = 0.73 (1624.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:27:19.671595: step 25580, loss = 0.95 (1643.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:27:20.466152: step 25590, loss = 0.76 (1611.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:27:21.364201: step 25600, loss = 0.76 (1425.3 examples/sec; 0.090 sec/batch)
2017-05-02 15:27:22.062175: step 25610, loss = 0.88 (1833.9 examples/sec; 0.070 sec/batch)
2017-05-02 15:27:22.853898: step 25620, loss = 0.78 (1616.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:27:23.635014: step 25630, loss = 0.92 (1638.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:27:24.424871: step 25640, loss = 0.77 (1620.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:27:25.221149: step 25650, loss = 0.82 (1607.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:27:26.015950: step 25660, loss = 0.96 (1610.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:27:26.800324: step 25670, loss = 0.87 (1631.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:27:27.581490: step 25680, loss = 0.88 (1638.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:27:28.367310: step 25690, loss = 0.77 (1628.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:27:29.254246: step 25700, loss = 0.83 (1443.2 examples/sec; 0.089 sec/batch)
2017-05-02 15:27:29.954442: step 25710, loss = 0.76 (1828.1 examples/sec; 0.070 sec/batch)
2017-05-02 15:27:30.748627: step 25720, loss = 0.79 (1611.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:27:31.526007: step 25730, loss = 0.75 (1646.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:27:32.321845: step 25740, loss = 0.85 (1608.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:27:33.110874: step 25750, loss = 0.85 (1622.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:27:33.902162: step 25760, loss = 0.90 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:27:34.690450: step 25770, loss = 0.66 (1623.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:27:35.470597: step 25780, loss = 0.79 (1640.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:27:36.262374: step 25790, loss = 0.79 (1616.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:27:37.156033: step 25800, loss = 0.76 (1432.3 examples/sec; 0.089 sec/batch)
2017-05-02 15:27:37.846912: step 25810, loss = 0.67 (1852.7 examples/sec; 0.069 sec/batch)
2017-05-02 15:27:38.635999: step 25820, loss = 0.78 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:27:39.420358: step 25830, loss = 0.69 (1631.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:27:40.201652: step 25840, loss = 0.67 (1638.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:27:40.986709: step 25850, loss = 0.68 (1630.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:27:41.775117: step 25860, loss = 0.79 (1623.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:27:42.558912: step 25870, loss = 0.83 (1633.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:27:43.350062: step 25880, loss = 0.92 (1617.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:27:44.133067: step 25890, loss = 0.73 (1634.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:27:45.019274: step 25900, loss = 0.79 (1444.4 examples/sec; 0.089 sec/batch)
2017-05-02 15:27:45.719617: step 25910, loss = 0.74 (1827.7 examples/sec; 0.070 sec/batch)
2017-05-02 15:27:46.518717: step 25920, loss = 0.77 (1601.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:27:47.310821: step 25930, loss = 0.84 (1616.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:27:48.097695: step 25940, loss = 0.69 (1626.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:27:48.889642: step 25950, loss = 0.83 (1616.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:27:49.682532: step 25960, loss = 0.83 (1614.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:27:50.470073: step 25970, loss = 0.72 (1625.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:27:51.265280: step 25980, loss = 1.04 (1609.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:27:52.048806: step 25990, loss = 0.86 (1633.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:27:52.948184: step 26000, loss = 0.81 (1423.2 examples/sec; 0.090 sec/batch)
2017-05-02 15:27:53.635670: step 26010, loss = 0.91 (1861.9 examples/sec; 0.069 sec/batch)
2017-05-02 15:27:54.432667: step 26020, loss = 1.00 (1606.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:27:55.223631: step 26030, loss = 0.89 (1618.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:27:56.004126: step 26040, loss = 0.81 (1640.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:27:56.803420: step 26050, loss = 0.75 (1601.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:27:57.604436: step 26060, loss = 0.85 (1598.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:27:58.403195: step 26070, loss = 0.88 (1602.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:27:59.204948: step 26080, loss = 0.83 (1596.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:27:59.983026: step 26090, loss = 0.82 (1645.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:28:00.878602: step 26100, loss = 0.83 (1429.2 examples/sec; 0.090 sec/batch)
2017-05-02 15:28:01.599298: step 26110, loss = 0.86 (1776.1 examples/sec; 0.072 sec/batch)
2017-05-02 15:28:02.379646: step 26120, loss = 0.81 (1640.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:28:03.174520: step 26130, loss = 1.09 (1610.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:03.962408: step 26140, loss = 0.78 (1624.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:04.757377: step 26150, loss = 0.85 (1610.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:05.560179: step 26160, loss = 0.82 (1594.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:28:06.355360: step 26170, loss = 0.92 (1609.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:28:07.145281: step 26180, loss = 0.78 (1620.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:07.926402: step 26190, loss = 0.74 (1638.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:28:08.814239: step 26200, loss = 0.81 (1441.7 examples/sec; 0.089 sec/batch)
2017-05-02 15:28:09.515368: step 26210, loss = 0.88 (1825.6 examples/sec; 0.070 sec/batch)
2017-05-02 15:28:10.321612: step 26220, loss = 0.73 (1587.6 examples/sec; 0.081 sec/batch)
2017-05-02 15:28:11.111551: step 26230, loss = 0.88 (1620.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:11.890253: step 26240, loss = 0.92 (1643.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:28:12.678388: step 26250, loss = 0.83 (1624.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:13.477515: step 26260, loss = 0.83 (1601.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:28:14.264631: step 26270, loss = 0.70 (1626.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:15.058478: step 26280, loss = 0.91 (1612.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:15.841152: step 26290, loss = 0.78 (1635.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:28:16.737059: step 26300, loss = 0.86 (1428.7 examples/sec; 0.090 sec/batch)
2017-05-02 15:28:17.430384: step 26310, loss = 0.87 (1846.2 examples/sec; 0.069 sec/batch)
2017-05-02 15:28:18.224138: step 26320, loss = 0.86 (1612.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:19.010408: step 26330, loss = 0.69 (1627.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:19.783501: step 26340, loss = 0.88 (1655.7 examples/sec; 0.077 sec/batch)
2017-05-02 15:28:20.577090: step 26350, loss = 0.81 (1612.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:21.366741: step 26360, loss = 0.77 (1621.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:22.158007: step 26370, loss = 0.75 (1617.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:22.949575: step 26380, loss = 0.76 (1617.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:23.733495: step 26390, loss = 0.88 (1632.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:28:24.621651: step 26400, loss = 0.74 (1441.2 examples/sec; 0.089 sec/batch)
2017-05-02 15:28:25.323154: step 26410, loss = 0.73 (1824.7 examples/sec; 0.070 sec/batch)
2017-05-02 15:28:26.113977: step 26420, loss = 0.65 (1618.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:26.902454: step 26430, loss = 0.66 (1623.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:27.678953: step 26440, loss = 0.80 (1648.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:28:28.474751: step 26450, loss = 0.83 (1608.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:28:29.265620: step 26460, loss = 0.90 (1618.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:30.053392: step 26470, loss = 0.73 (1624.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:30.842743: step 26480, loss = 0.78 (1621.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:31.619198: step 26490, loss = 0.77 (1648.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:28:32.508570: step 26500, loss = 0.74 (1439.2 examples/sec; 0.089 sec/batch)
2017-05-02 15:28:33.206112: step 26510, loss = 0.76 (1835.0 examples/sec; 0.070 sec/batch)
2017-05-02 15:28:33.994677: step 26520, loss = 1.09 (1623.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:34.781944: step 26530, loss = 0.74 (1625.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:35.561574: step 26540, loss = 0.82 (1641.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:28:36.356551: step 26550, loss = 0.87 (1610.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:37.145468: step 26560, loss = 1.02 (1622.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:37.935287: step 26570, loss = 0.70 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:38.726330: step 26580, loss = 0.73 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:39.510061: step 26590, loss = 0.73 (1633.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:28:40.399718: step 26600, loss = 0.80 (1438.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:28:41.091238: step 26610, loss = 0.62 (1851.0 examples/sec; 0.069 sec/batch)
2017-05-02 15:28:41.887133: step 26620, loss = 0.80 (1608.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:28:42.684214: step 26630, loss = 0.89 (1605.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:28:43.467545: step 26640, loss = 0.67 (1634.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:28:44.254531: step 26650, loss = 0.81 (1626.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:45.041144: step 26660, loss = 0.73 (1627.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:45.829406: step 26670, loss = 0.94 (1623.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:46.620676: step 26680, loss = 0.79 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:47.413554: step 26690, loss = 0.79 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:48.297690: step 26700, loss = 0.81 (1447.7 examples/sec; 0.088 sec/batch)
2017-05-02 15:28:48.984145: step 26710, loss = 0.79 (1864.6 examples/sec; 0.069 sec/batch)
2017-05-02 15:28:49.778300: step 26720, loss = 0.79 (1611.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:50.574490: step 26730, loss = 0.73 (1607.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:28:51.363004: step 26740, loss = 0.76 (1623.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:52.151859: step 26750, loss = 0.79 (1622.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:52.948689: step 26760, loss = 0.89 (1606.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:28:53.740334: step 26770, loss = 0.70 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:54.529100: step 26780, loss = 0.77 (1622.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:55.318921: step 26790, loss = 0.93 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:56.210387: step 26800, loss = 0.71 (1435.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:28:56.896350: step 26810, loss = 0.74 (1866.0 examples/sec; 0.069 sec/batch)
2017-05-02 15:28:57.682611: step 26820, loss = 0.72 (1628.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:28:58.480134: step 26830, loss = 0.85 (1605.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:28:59.267532: step 26840, loss = 0.83 (1625.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:00.051378: step 26850, loss = 0.80 (1633.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:29:00.843805: step 26860, loss = 0.86 (1615.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:01.626712: step 26870, loss = 0.74 (1634.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:29:02.422509: step 26880, loss = 0.78 (1608.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:29:03.215486: step 26890, loss = 0.97 (1614.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:04.102500: step 26900, loss = 0.80 (1443.0 examples/sec; 0.089 sec/batch)
2017-05-02 15:29:04.805842: step 26910, loss = 0.73 (1819.9 examples/sec; 0.070 sec/batch)
2017-05-02 15:29:05.593391: step 26920, loss = 0.74 (1625.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:06.392609: step 26930, loss = 0.86 (1601.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:29:07.185842: step 26940, loss = 0.92 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:07.976266: step 26950, loss = 1.05 (1619.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:08.769165: step 26960, loss = 0.78 (1614.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:09.562626: step 26970, loss = 0.93 (1613.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:10.357038: step 26980, loss = 0.74 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:11.155031: step 26990, loss = 0.77 (1604.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:29:12.041564: step 27000, loss = 0.88 (1443.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:29:12.732955: step 27010, loss = 0.80 (1851.4 examples/sec; 0.069 sec/batch)
2017-05-02 15:29:13.522763: step 27020, loss = 0.76 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:14.314012: step 27030, loss = 0.96 (1617.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:15.101096: step 27040, loss = 0.73 (1626.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:15.881922: step 27050, loss = 0.76 (1639.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:29:16.673234: step 27060, loss = 0.71 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:17.466014: step 27070, loss = 0.68 (1614.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:18.262038: step 27080, loss = 0.82 (1608.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:29:19.050887: step 27090, loss = 0.82 (1622.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:19.938677: step 27100, loss = 0.78 (1441.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:29:20.637823: step 27110, loss = 0.73 (1830.8 examples/sec; 0.070 sec/batch)
2017-05-02 15:29:21.422901: step 27120, loss = 0.77 (1630.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:22.210940: step 27130, loss = 0.84 (1624.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:23.006361: step 27140, loss = 0.91 (1609.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:29:23.791742: step 27150, loss = 0.84 (1629.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:24.590392: step 27160, loss = 0.67 (1602.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:29:25.388811: step 27170, loss = 0.88 (1603.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:29:26.181081: step 27180, loss = 0.81 (1615.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:26.968023: step 27190, loss = 0.91 (1626.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:27.847272: step 27200, loss = 0.90 (1455.8 examples/sec; 0.088 sec/batch)
2017-05-02 15:29:28.536325: step 27210, loss = 0.92 (1857.6 examples/sec; 0.069 sec/batch)
2017-05-02 15:29:29.336080: step 27220, loss = 0.72 (1600.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:29:30.127272: step 27230, loss = 0.69 (1617.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:30.918491: step 27240, loss = 0.77 (1617.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:31.700190: step 27250, loss = 0.79 (1637.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:29:32.495029: step 27260, loss = 0.71 (1610.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:33.291299: step 27270, loss = 0.79 (1607.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:29:34.078465: step 27280, loss = 0.90 (1626.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:34.883255: step 27290, loss = 0.76 (1590.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:29:35.762643: step 27300, loss = 0.84 (1455.5 examples/sec; 0.088 sec/batch)
2017-05-02 15:29:36.460184: step 27310, loss = 0.77 (1835.0 examples/sec; 0.070 sec/batch)
2017-05-02 15:29:37.262911: step 27320, loss = 0.96 (1594.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:29:38.054925: step 27330, loss = 0.93 (1616.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:38.844245: step 27340, loss = 0.73 (1621.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:39.625705: step 27350, loss = 0.69 (1638.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:29:40.415491: step 27360, loss = 0.70 (1620.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:41.208383: step 27370, loss = 0.65 (1614.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:42.007317: step 27380, loss = 0.68 (1602.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:29:42.797373: step 27390, loss = 0.91 (1620.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:43.671626: step 27400, loss = 0.90 (1464.1 examples/sec; 0.087 sec/batch)
2017-05-02 15:29:44.367326: step 27410, loss = 0.59 (1839.9 examples/sec; 0.070 sec/batch)
2017-05-02 15:29:45.157781: step 27420, loss = 0.95 (1619.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:45.948317: step 27430, loss = 0.82 (1619.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:46.739690: step 27440, loss = 0.93 (1617.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:47.521226: step 27450, loss = 0.95 (1637.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:29:48.307566: step 27460, loss = 0.87 (1627.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:49.101299: step 27470, loss = 0.87 (1612.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:49.885405: step 27480, loss = 0.89 (1632.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:29:50.677188: step 27490, loss = 0.80 (1616.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:51.561970: step 27500, loss = 0.67 (1446.7 examples/sec; 0.088 sec/batch)
2017-05-02 15:29:52.266711: step 27510, loss = 0.95 (1816.3 examples/sec; 0.070 sec/batch)
2017-05-02 15:29:53.054287: step 27520, loss = 0.73 (1625.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:53.848275: step 27530, loss = 0.90 (1612.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:54.657420: step 27540, loss = 0.81 (1581.9 examples/sec; 0.081 sec/batch)
2017-05-02 15:29:55.450313: step 27550, loss = 0.59 (1614.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:56.249623: step 27560, loss = 0.74 (1601.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:29:57.035481: step 27570, loss = 0.84 (1628.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:57.824640: step 27580, loss = 0.85 (1622.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:58.615968: step 27590, loss = 0.84 (1617.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:29:59.501010: step 27600, loss = 0.77 (1446.3 examples/sec; 0.089 sec/batch)
2017-05-02 15:30:00.198977: step 27610, loss = 0.67 (1833.9 examples/sec; 0.070 sec/batch)
2017-05-02 15:30:00.980306: step 27620, loss = 0.90 (1638.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:30:01.776495: step 27630, loss = 0.78 (1607.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:30:02.573007: step 27640, loss = 0.87 (1607.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:30:03.354208: step 27650, loss = 0.82 (1638.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:30:04.149693: step 27660, loss = 0.74 (1609.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:30:04.929944: step 27670, loss = 0.97 (1640.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:30:05.717510: step 27680, loss = 0.73 (1625.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:06.513680: step 27690, loss = 0.83 (1607.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:30:07.394417: step 27700, loss = 0.75 (1453.3 examples/sec; 0.088 sec/batch)
2017-05-02 15:30:08.092174: step 27710, loss = 0.93 (1834.4 examples/sec; 0.070 sec/batch)
2017-05-02 15:30:08.884323: step 27720, loss = 0.74 (1615.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:09.671348: step 27730, loss = 0.72 (1626.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:10.471494: step 27740, loss = 0.84 (1599.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:30:11.266900: step 27750, loss = 0.85 (1609.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:30:12.047343: step 27760, loss = 0.70 (1640.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:30:12.844112: step 27770, loss = 0.94 (1606.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:30:13.634594: step 27780, loss = 0.75 (1619.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:14.429729: step 27790, loss = 0.77 (1609.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:30:15.323100: step 27800, loss = 0.90 (1432.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:30:16.011042: step 27810, loss = 0.74 (1860.6 examples/sec; 0.069 sec/batch)
2017-05-02 15:30:16.799602: step 27820, loss = 0.68 (1623.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:17.590874: step 27830, loss = 0.77 (1617.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:18.380155: step 27840, loss = 0.86 (1621.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:19.171765: step 27850, loss = 0.88 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:19.951117: step 27860, loss = 0.80 (1642.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:30:20.741915: step 27870, loss = 0.88 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:21.534211: step 27880, loss = 0.80 (1615.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:22.328233: step 27890, loss = 0.98 (1612.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:23.223272: step 27900, loss = 0.88 (1430.1 examples/sec; 0.090 sec/batch)
2017-05-02 15:30:23.902358: step 27910, loss = 0.80 (1884.9 examples/sec; 0.068 sec/batch)
2017-05-02 15:30:24.689847: step 27920, loss = 0.72 (1625.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:25.478067: step 27930, loss = 0.80 (1623.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:26.272800: step 27940, loss = 0.85 (1610.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:27.070328: step 27950, loss = 0.79 (1604.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:30:27.852237: step 27960, loss = 0.83 (1637.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:30:28.644604: step 27970, loss = 0.85 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:29.433775: step 27980, loss = 0.94 (1622.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:30.224043: step 27990, loss = 0.93 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:31.109339: step 28000, loss = 0.73 (1445.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:30:31.803234: step 28010, loss = 0.95 (1844.7 examples/sec; 0.069 sec/batch)
2017-05-02 15:30:32.593184: step 28020, loss = 0.91 (1620.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:33.385323: step 28030, loss = 0.70 (1615.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:34.180069: step 28040, loss = 0.79 (1610.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:34.966592: step 28050, loss = 0.74 (1627.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:35.745092: step 28060, loss = 0.77 (1644.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:30:36.534484: step 28070, loss = 0.66 (1621.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:37.328321: step 28080, loss = 0.71 (1612.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:38.117694: step 28090, loss = 0.92 (1621.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:39.013839: step 28100, loss = 0.86 (1428.3 examples/sec; 0.090 sec/batch)
2017-05-02 15:30:39.708657: step 28110, loss = 0.76 (1842.2 examples/sec; 0.069 sec/batch)
2017-05-02 15:30:40.499114: step 28120, loss = 0.89 (1619.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:41.292057: step 28130, loss = 0.71 (1614.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:42.085472: step 28140, loss = 0.73 (1613.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:42.881045: step 28150, loss = 0.60 (1608.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:30:43.665775: step 28160, loss = 0.86 (1631.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:30:44.459317: step 28170, loss = 0.62 (1613.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:45.249454: step 28180, loss = 0.70 (1620.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:46.043987: step 28190, loss = 0.83 (1611.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:46.936064: step 28200, loss = 0.74 (1434.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:30:47.616566: step 28210, loss = 0.69 (1881.0 examples/sec; 0.068 sec/batch)
2017-05-02 15:30:48.414829: step 28220, loss = 0.82 (1603.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:30:49.212787: step 28230, loss = 0.70 (1604.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:30:50.008784: step 28240, loss = 0.72 (1608.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:30:50.805918: step 28250, loss = 0.68 (1605.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:30:51.589994: step 28260, loss = 0.70 (1632.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:30:52.383200: step 28270, loss = 0.77 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:53.172098: step 28280, loss = 0.82 (1622.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:53.967566: step 28290, loss = 0.83 (1609.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:30:54.877104: step 28300, loss = 0.78 (1407.3 examples/sec; 0.091 sec/batch)
2017-05-02 15:30:55.557241: step 28310, loss = 0.72 (1882.0 examples/sec; 0.068 sec/batch)
2017-05-02 15:30:56.352277: step 28320, loss = 0.90 (1610.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:30:57.155249: step 28330, loss = 0.70 (1594.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:30:57.956741: step 28340, loss = 0.82 (1597.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:30:58.750301: step 28350, loss = 0.89 (1613.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:30:59.542547: step 28360, loss = 0.80 (1615.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:00.336937: step 28370, loss = 1.00 (1611.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:01.124723: step 28380, loss = 0.79 (1624.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:01.910250: step 28390, loss = 0.88 (1629.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:02.802567: step 28400, loss = 0.86 (1434.5 examples/sec; 0.089 sec/batch)
2017-05-02 15:31:03.494405: step 28410, loss = 0.75 (1850.1 examples/sec; 0.069 sec/batch)
2017-05-02 15:31:04.278291: step 28420, loss = 0.73 (1632.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:31:05.066132: step 28430, loss = 0.69 (1624.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:05.855728: step 28440, loss = 0.88 (1621.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:06.646509: step 28450, loss = 0.71 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:07.435967: step 28460, loss = 0.72 (1621.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:08.223846: step 28470, loss = 0.71 (1624.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:09.020719: step 28480, loss = 0.70 (1606.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:31:09.813028: step 28490, loss = 0.73 (1615.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:10.695557: step 28500, loss = 0.94 (1450.4 examples/sec; 0.088 sec/batch)
2017-05-02 15:31:11.390976: step 28510, loss = 0.84 (1840.6 examples/sec; 0.070 sec/batch)
2017-05-02 15:31:12.172926: step 28520, loss = 0.89 (1636.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:31:12.970571: step 28530, loss = 0.86 (1604.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:31:13.761710: step 28540, loss = 0.90 (1617.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:14.541360: step 28550, loss = 0.76 (1641.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:31:15.329385: step 28560, loss = 0.76 (1624.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:16.105375: step 28570, loss = 0.70 (1649.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:31:16.897676: step 28580, loss = 0.82 (1615.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:17.688432: step 28590, loss = 0.70 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:18.570891: step 28600, loss = 0.80 (1450.5 examples/sec; 0.088 sec/batch)
2017-05-02 15:31:19.260950: step 28610, loss = 0.72 (1854.9 examples/sec; 0.069 sec/batch)
2017-05-02 15:31:20.039062: step 28620, loss = 0.83 (1645.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:31:20.824508: step 28630, loss = 0.68 (1629.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:21.619642: step 28640, loss = 0.80 (1609.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:31:22.408255: step 28650, loss = 0.85 (1623.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:23.206060: step 28660, loss = 0.65 (1604.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:31:23.985789: step 28670, loss = 0.81 (1641.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:31:24.774965: step 28680, loss = 0.85 (1621.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:25.569419: step 28690, loss = 0.86 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:26.466072: step 28700, loss = 0.87 (1427.5 examples/sec; 0.090 sec/batch)
2017-05-02 15:31:27.151836: step 28710, loss = 0.70 (1866.5 examples/sec; 0.069 sec/batch)
2017-05-02 15:31:27.937399: step 28720, loss = 0.78 (1629.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:28.729563: step 28730, loss = 0.70 (1615.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:29.515537: step 28740, loss = 0.81 (1628.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:30.310038: step 28750, loss = 0.95 (1611.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:31.092826: step 28760, loss = 0.96 (1635.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:31:31.882618: step 28770, loss = 1.06 (1620.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:32.683896: step 28780, loss = 0.73 (1597.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:31:33.476193: step 28790, loss = 0.80 (1615.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:34.375096: step 28800, loss = 0.70 (1424.0 examples/sec; 0.090 sec/batch)
2017-05-02 15:31:35.064728: step 28810, loss = 0.78 (1856.1 examples/sec; 0.069 sec/batch)
2017-05-02 15:31:35.842490: step 28820, loss = 0.67 (1645.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:31:36.631142: step 28830, loss = 0.87 (1623.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:37.423496: step 28840, loss = 0.73 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:38.214773: step 28850, loss = 0.77 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:39.006743: step 28860, loss = 0.81 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:39.786400: step 28870, loss = 0.83 (1641.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:31:40.577698: step 28880, loss = 0.68 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:41.369106: step 28890, loss = 0.77 (1617.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:42.252154: step 28900, loss = 0.78 (1449.5 examples/sec; 0.088 sec/batch)
2017-05-02 15:31:42.951615: step 28910, loss = 0.91 (1830.0 examples/sec; 0.070 sec/batch)
2017-05-02 15:31:43.731541: step 28920, loss = 0.87 (1641.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:31:44.520800: step 28930, loss = 0.86 (1621.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:45.322423: step 28940, loss = 0.81 (1596.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:31:46.107957: step 28950, loss = 0.82 (1629.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:46.900728: step 28960, loss = 0.74 (1614.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:47.682158: step 28970, loss = 0.87 (1638.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:31:48.469874: step 28980, loss = 0.69 (1624.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:49.262145: step 28990, loss = 0.73 (1615.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:50.172661: step 29000, loss = 0.86 (1405.8 examples/sec; 0.091 sec/batch)
2017-05-02 15:31:50.842393: step 29010, loss = 0.73 (1911.2 examples/sec; 0.067 sec/batch)
2017-05-02 15:31:51.628309: step 29020, loss = 0.86 (1628.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:52.412775: step 29030, loss = 0.76 (1631.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:31:53.208133: step 29040, loss = 0.71 (1609.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:31:54.004448: step 29050, loss = 0.86 (1607.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:31:54.794422: step 29060, loss = 0.83 (1620.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:55.572674: step 29070, loss = 0.72 (1644.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:31:56.362163: step 29080, loss = 0.75 (1621.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:57.155425: step 29090, loss = 0.77 (1613.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:31:58.050332: step 29100, loss = 0.88 (1430.3 examples/sec; 0.089 sec/batch)
2017-05-02 15:31:58.743619: step 29110, loss = 0.81 (1846.3 examples/sec; 0.069 sec/batch)
2017-05-02 15:31:59.530473: step 29120, loss = 0.62 (1626.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:00.318679: step 29130, loss = 0.70 (1623.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:01.107691: step 29140, loss = 0.79 (1622.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:01.893535: step 29150, loss = 0.74 (1628.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:02.696161: step 29160, loss = 0.81 (1594.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:32:03.481511: step 29170, loss = 0.83 (1629.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:04.278325: step 29180, loss = 0.77 (1606.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:32:05.075614: step 29190, loss = 0.81 (1605.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:32:05.959571: step 29200, loss = 0.77 (1448.0 examples/sec; 0.088 sec/batch)
2017-05-02 15:32:06.656045: step 29210, loss = 0.93 (1837.8 examples/sec; 0.070 sec/batch)
2017-05-02 15:32:07.446705: step 29220, loss = 0.86 (1618.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:08.235291: step 29230, loss = 0.88 (1623.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:09.013475: step 29240, loss = 0.88 (1644.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:32:09.797381: step 29250, loss = 0.72 (1632.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:32:10.596168: step 29260, loss = 0.72 (1602.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:32:11.382841: step 29270, loss = 0.83 (1627.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:12.166522: step 29280, loss = 0.90 (1633.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:32:12.957407: step 29290, loss = 0.77 (1618.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:13.843285: step 29300, loss = 0.77 (1444.9 examples/sec; 0.089 sec/batch)
2017-05-02 15:32:14.538235: step 29310, loss = 0.77 (1841.9 examples/sec; 0.069 sec/batch)
2017-05-02 15:32:15.321239: step 29320, loss = 0.74 (1634.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:32:16.108175: step 29330, loss = 0.75 (1626.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:16.907584: step 29340, loss = 0.92 (1601.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:32:17.698534: step 29350, loss = 0.76 (1618.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:18.481061: step 29360, loss = 0.87 (1635.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:32:19.274197: step 29370, loss = 0.80 (1613.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:20.053159: step 29380, loss = 0.70 (1643.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:32:20.846587: step 29390, loss = 0.69 (1613.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:21.736494: step 29400, loss = 0.81 (1438.4 examples/sec; 0.089 sec/batch)
2017-05-02 15:32:22.432247: step 29410, loss = 0.81 (1839.7 examples/sec; 0.070 sec/batch)
2017-05-02 15:32:23.221904: step 29420, loss = 0.87 (1621.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:24.003553: step 29430, loss = 0.73 (1637.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:32:24.791045: step 29440, loss = 0.91 (1625.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:25.585630: step 29450, loss = 0.77 (1610.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:26.370107: step 29460, loss = 0.64 (1631.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:32:27.159900: step 29470, loss = 0.68 (1620.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:27.952198: step 29480, loss = 1.01 (1615.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:28.744635: step 29490, loss = 0.72 (1615.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:29.643914: step 29500, loss = 0.73 (1423.4 examples/sec; 0.090 sec/batch)
2017-05-02 15:32:30.330982: step 29510, loss = 0.80 (1862.9 examples/sec; 0.069 sec/batch)
2017-05-02 15:32:31.127361: step 29520, loss = 0.87 (1607.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:32:31.903530: step 29530, loss = 0.92 (1649.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:32:32.691477: step 29540, loss = 0.74 (1624.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:33.475346: step 29550, loss = 0.77 (1632.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:32:34.263336: step 29560, loss = 0.71 (1624.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:35.052935: step 29570, loss = 0.78 (1621.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:35.836265: step 29580, loss = 0.82 (1634.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:32:36.632452: step 29590, loss = 0.94 (1607.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:32:37.514102: step 29600, loss = 0.75 (1451.8 examples/sec; 0.088 sec/batch)
2017-05-02 15:32:38.210512: step 29610, loss = 0.81 (1838.0 examples/sec; 0.070 sec/batch)
2017-05-02 15:32:38.996728: step 29620, loss = 0.98 (1628.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:39.782502: step 29630, loss = 0.78 (1629.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:40.581997: step 29640, loss = 0.73 (1601.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:32:41.371189: step 29650, loss = 0.81 (1621.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:42.165154: step 29660, loss = 0.81 (1612.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:42.958130: step 29670, loss = 0.89 (1614.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:43.737498: step 29680, loss = 0.76 (1642.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:32:44.533028: step 29690, loss = 0.68 (1609.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:32:45.419552: step 29700, loss = 0.75 (1443.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:32:46.111332: step 29710, loss = 0.87 (1850.3 examples/sec; 0.069 sec/batch)
2017-05-02 15:32:46.910264: step 29720, loss = 0.89 (1602.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:32:47.688561: step 29730, loss = 0.74 (1644.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:32:48.481400: step 29740, loss = 0.64 (1614.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:49.280390: step 29750, loss = 0.83 (1602.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:32:50.069502: step 29760, loss = 0.84 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:50.861756: step 29770, loss = 0.70 (1615.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:51.643953: step 29780, loss = 0.67 (1636.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:32:52.434030: step 29790, loss = 1.07 (1620.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:53.325829: step 29800, loss = 0.77 (1435.3 examples/sec; 0.089 sec/batch)
2017-05-02 15:32:54.027326: step 29810, loss = 0.76 (1824.7 examples/sec; 0.070 sec/batch)
2017-05-02 15:32:54.821976: step 29820, loss = 0.89 (1610.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:55.593404: step 29830, loss = 0.78 (1659.3 examples/sec; 0.077 sec/batch)
2017-05-02 15:32:56.388761: step 29840, loss = 0.74 (1609.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:32:57.171218: step 29850, loss = 0.77 (1635.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:32:57.960322: step 29860, loss = 0.76 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:58.751241: step 29870, loss = 0.87 (1618.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:32:59.531395: step 29880, loss = 0.81 (1640.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:33:00.315407: step 29890, loss = 0.87 (1632.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:33:01.205914: step 29900, loss = 0.73 (1437.4 examples/sec; 0.089 sec/batch)
2017-05-02 15:33:01.898105: step 29910, loss = 0.89 (1849.2 examples/sec; 0.069 sec/batch)
2017-05-02 15:33:02.685367: step 29920, loss = 0.70 (1625.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:03.469773: step 29930, loss = 0.77 (1631.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:33:04.256312: step 29940, loss = 0.80 (1627.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:05.053854: step 29950, loss = 0.82 (1604.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:33:05.845168: step 29960, loss = 0.93 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:06.629975: step 29970, loss = 0.77 (1631.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:33:07.415895: step 29980, loss = 0.82 (1628.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:08.205738: step 29990, loss = 0.81 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:09.102059: step 30000, loss = 0.69 (1428.1 examples/sec; 0.090 sec/batch)
2017-05-02 15:33:09.790541: step 30010, loss = 0.70 (1859.2 examples/sec; 0.069 sec/batch)
2017-05-02 15:33:10.585259: step 30020, loss = 0.68 (1610.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:11.382131: step 30030, loss = 0.66 (1606.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:33:12.164238: step 30040, loss = 0.78 (1636.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:33:12.955730: step 30050, loss = 0.84 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:13.739917: step 30060, loss = 0.88 (1632.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:33:14.525322: step 30070, loss = 0.85 (1629.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:15.319755: step 30080, loss = 0.81 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:16.097635: step 30090, loss = 0.97 (1645.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:33:16.986853: step 30100, loss = 0.88 (1439.5 examples/sec; 0.089 sec/batch)
2017-05-02 15:33:17.684898: step 30110, loss = 0.73 (1833.7 examples/sec; 0.070 sec/batch)
2017-05-02 15:33:18.478414: step 30120, loss = 0.94 (1613.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:19.268050: step 30130, loss = 0.84 (1621.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:20.051656: step 30140, loss = 0.86 (1633.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:33:20.841394: step 30150, loss = 0.79 (1620.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:21.632140: step 30160, loss = 0.85 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:22.422578: step 30170, loss = 0.76 (1619.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:23.216297: step 30180, loss = 0.80 (1612.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:23.994026: step 30190, loss = 0.76 (1645.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:33:24.886433: step 30200, loss = 0.66 (1434.3 examples/sec; 0.089 sec/batch)
2017-05-02 15:33:25.577075: step 30210, loss = 0.89 (1853.4 examples/sec; 0.069 sec/batch)
2017-05-02 15:33:26.371213: step 30220, loss = 0.93 (1611.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:27.167977: step 30230, loss = 0.74 (1606.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:33:27.949946: step 30240, loss = 0.81 (1636.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:33:28.744584: step 30250, loss = 0.84 (1610.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:29.531438: step 30260, loss = 0.75 (1626.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:30.354389: step 30270, loss = 0.86 (1555.4 examples/sec; 0.082 sec/batch)
2017-05-02 15:33:31.143999: step 30280, loss = 0.93 (1621.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:31.925582: step 30290, loss = 0.68 (1637.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:33:32.816521: step 30300, loss = 0.79 (1436.7 examples/sec; 0.089 sec/batch)
2017-05-02 15:33:33.504375: step 30310, loss = 0.84 (1860.9 examples/sec; 0.069 sec/batch)
2017-05-02 15:33:34.297771: step 30320, loss = 0.84 (1613.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:35.093138: step 30330, loss = 0.89 (1609.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:33:35.864989: step 30340, loss = 0.93 (1658.4 examples/sec; 0.077 sec/batch)
2017-05-02 15:33:36.659092: step 30350, loss = 0.75 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:37.458222: step 30360, loss = 0.97 (1601.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:33:38.251521: step 30370, loss = 0.74 (1613.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:39.043335: step 30380, loss = 0.88 (1616.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:39.822504: step 30390, loss = 0.70 (1642.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:33:40.718332: step 30400, loss = 0.83 (1428.9 examples/sec; 0.090 sec/batch)
2017-05-02 15:33:41.410652: step 30410, loss = 0.78 (1848.9 examples/sec; 0.069 sec/batch)
2017-05-02 15:33:42.206993: step 30420, loss = 0.77 (1607.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:33:43.002160: step 30430, loss = 0.80 (1609.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:33:43.787778: step 30440, loss = 0.88 (1629.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:44.575107: step 30450, loss = 0.83 (1625.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:45.372636: step 30460, loss = 0.86 (1605.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:33:46.161077: step 30470, loss = 0.72 (1623.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:46.953083: step 30480, loss = 0.73 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:47.733964: step 30490, loss = 0.69 (1639.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:33:48.620946: step 30500, loss = 0.72 (1443.1 examples/sec; 0.089 sec/batch)
2017-05-02 15:33:49.313867: step 30510, loss = 0.83 (1847.3 examples/sec; 0.069 sec/batch)
2017-05-02 15:33:50.102748: step 30520, loss = 0.84 (1622.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:50.898858: step 30530, loss = 0.78 (1607.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:33:51.683785: step 30540, loss = 0.68 (1630.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:33:52.472162: step 30550, loss = 0.73 (1623.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:53.266681: step 30560, loss = 0.70 (1611.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:54.055501: step 30570, loss = 0.79 (1622.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:54.841520: step 30580, loss = 0.67 (1628.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:55.619942: step 30590, loss = 1.03 (1644.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:33:56.505853: step 30600, loss = 0.75 (1444.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:33:57.202354: step 30610, loss = 0.67 (1837.8 examples/sec; 0.070 sec/batch)
2017-05-02 15:33:57.997109: step 30620, loss = 0.98 (1610.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:33:58.793181: step 30630, loss = 0.84 (1607.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:33:59.576825: step 30640, loss = 0.85 (1633.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:34:00.365611: step 30650, loss = 0.90 (1622.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:01.160012: step 30660, loss = 0.96 (1611.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:01.944097: step 30670, loss = 0.61 (1632.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:34:02.740379: step 30680, loss = 0.82 (1607.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:34:03.522540: step 30690, loss = 0.89 (1636.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:34:04.409008: step 30700, loss = 0.76 (1443.9 examples/sec; 0.089 sec/batch)
2017-05-02 15:34:05.103665: step 30710, loss = 0.65 (1842.6 examples/sec; 0.069 sec/batch)
2017-05-02 15:34:05.896599: step 30720, loss = 0.76 (1614.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:06.682699: step 30730, loss = 0.64 (1628.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:07.470562: step 30740, loss = 0.74 (1624.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:08.254368: step 30750, loss = 0.75 (1633.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:34:09.049808: step 30760, loss = 0.90 (1609.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:34:09.845738: step 30770, loss = 0.70 (1608.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:34:10.629896: step 30780, loss = 0.73 (1632.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:34:11.415740: step 30790, loss = 0.70 (1628.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:12.301693: step 30800, loss = 0.80 (1444.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:34:12.996328: step 30810, loss = 0.76 (1842.7 examples/sec; 0.069 sec/batch)
2017-05-02 15:34:13.785786: step 30820, loss = 0.70 (1621.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:14.577739: step 30830, loss = 0.82 (1616.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:15.368830: step 30840, loss = 0.74 (1618.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:16.158325: step 30850, loss = 0.86 (1621.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:16.946106: step 30860, loss = 0.74 (1624.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:17.737216: step 30870, loss = 0.86 (1618.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:18.522052: step 30880, loss = 0.75 (1630.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:34:19.306934: step 30890, loss = 0.68 (1630.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:34:20.187998: step 30900, loss = 0.80 (1452.8 examples/sec; 0.088 sec/batch)
2017-05-02 15:34:20.880416: step 30910, loss = 0.95 (1848.6 examples/sec; 0.069 sec/batch)
2017-05-02 15:34:21.668615: step 30920, loss = 0.83 (1624.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:22.460391: step 30930, loss = 0.91 (1616.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:23.249152: step 30940, loss = 0.69 (1622.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:24.036802: step 30950, loss = 0.79 (1625.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:24.827489: step 30960, loss = 0.83 (1618.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:25.621556: step 30970, loss = 0.65 (1612.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:26.413490: step 30980, loss = 0.72 (1616.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:27.202200: step 30990, loss = 0.87 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:28.095433: step 31000, loss = 0.73 (1433.0 examples/sec; 0.089 sec/batch)
2017-05-02 15:34:28.785337: step 31010, loss = 0.59 (1855.3 examples/sec; 0.069 sec/batch)
2017-05-02 15:34:29.568955: step 31020, loss = 0.70 (1633.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:34:30.369424: step 31030, loss = 0.81 (1599.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:34:31.162742: step 31040, loss = 0.79 (1613.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:31.947325: step 31050, loss = 1.00 (1631.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:34:32.736231: step 31060, loss = 0.83 (1622.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:33.522301: step 31070, loss = 0.76 (1628.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:34.313924: step 31080, loss = 0.88 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:35.103398: step 31090, loss = 0.83 (1621.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:35.979740: step 31100, loss = 0.76 (1460.6 examples/sec; 0.088 sec/batch)
2017-05-02 15:34:36.677245: step 31110, loss = 0.83 (1835.1 examples/sec; 0.070 sec/batch)
2017-05-02 15:34:37.471461: step 31120, loss = 0.92 (1611.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:38.264564: step 31130, loss = 0.65 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:39.053578: step 31140, loss = 0.66 (1622.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:39.827849: step 31150, loss = 0.73 (1653.2 examples/sec; 0.077 sec/batch)
2017-05-02 15:34:40.624427: step 31160, loss = 0.80 (1606.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:34:41.419822: step 31170, loss = 0.99 (1609.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:34:42.208114: step 31180, loss = 0.92 (1623.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:42.993021: step 31190, loss = 0.87 (1630.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:34:43.877255: step 31200, loss = 0.75 (1447.6 examples/sec; 0.088 sec/batch)
2017-05-02 15:34:44.569367: step 31210, loss = 0.71 (1849.4 examples/sec; 0.069 sec/batch)
2017-05-02 15:34:45.357308: step 31220, loss = 0.75 (1624.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:46.149390: step 31230, loss = 0.76 (1616.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:46.936080: step 31240, loss = 0.68 (1627.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:47.715858: step 31250, loss = 0.84 (1641.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:34:48.505454: step 31260, loss = 0.77 (1621.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:49.297584: step 31270, loss = 0.78 (1615.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:50.081953: step 31280, loss = 0.72 (1631.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:34:50.873430: step 31290, loss = 0.69 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:51.754120: step 31300, loss = 0.91 (1453.4 examples/sec; 0.088 sec/batch)
2017-05-02 15:34:52.438243: step 31310, loss = 0.83 (1871.0 examples/sec; 0.068 sec/batch)
2017-05-02 15:34:53.231493: step 31320, loss = 0.65 (1613.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:54.017169: step 31330, loss = 0.65 (1629.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:54.812172: step 31340, loss = 0.79 (1610.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:34:55.595470: step 31350, loss = 0.74 (1634.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:34:56.386380: step 31360, loss = 0.94 (1618.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:34:57.171289: step 31370, loss = 0.81 (1630.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:34:57.966297: step 31380, loss = 0.75 (1610.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:34:58.762352: step 31390, loss = 0.89 (1607.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:34:59.651843: step 31400, loss = 0.87 (1439.0 examples/sec; 0.089 sec/batch)
2017-05-02 15:35:00.340864: step 31410, loss = 0.71 (1857.7 examples/sec; 0.069 sec/batch)
2017-05-02 15:35:01.125584: step 31420, loss = 0.73 (1631.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:35:01.925915: step 31430, loss = 0.61 (1599.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:35:02.710429: step 31440, loss = 0.80 (1631.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:35:03.489790: step 31450, loss = 0.67 (1642.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:35:04.281003: step 31460, loss = 0.82 (1617.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:05.068356: step 31470, loss = 0.80 (1625.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:05.866344: step 31480, loss = 0.69 (1604.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:35:06.654204: step 31490, loss = 0.87 (1624.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:07.547827: step 31500, loss = 0.86 (1432.4 examples/sec; 0.089 sec/batch)
2017-05-02 15:35:08.241543: step 31510, loss = 0.72 (1845.1 examples/sec; 0.069 sec/batch)
2017-05-02 15:35:09.037799: step 31520, loss = 0.70 (1607.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:35:09.833075: step 31530, loss = 0.76 (1609.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:35:10.631247: step 31540, loss = 0.70 (1603.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:35:11.425177: step 31550, loss = 0.77 (1612.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:12.214854: step 31560, loss = 0.87 (1620.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:13.007661: step 31570, loss = 0.73 (1614.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:13.807124: step 31580, loss = 0.81 (1601.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:35:14.598115: step 31590, loss = 0.80 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:15.482422: step 31600, loss = 0.77 (1447.5 examples/sec; 0.088 sec/batch)
2017-05-02 15:35:16.182588: step 31610, loss = 0.76 (1828.1 examples/sec; 0.070 sec/batch)
2017-05-02 15:35:16.969813: step 31620, loss = 0.77 (1626.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:17.760934: step 31630, loss = 0.94 (1618.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:18.550703: step 31640, loss = 0.64 (1620.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:19.339189: step 31650, loss = 0.68 (1623.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:20.122078: step 31660, loss = 0.80 (1635.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:35:20.920795: step 31670, loss = 0.70 (1602.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:35:21.714533: step 31680, loss = 0.70 (1612.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:22.507587: step 31690, loss = 0.73 (1614.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:23.404975: step 31700, loss = 0.75 (1426.4 examples/sec; 0.090 sec/batch)
2017-05-02 15:35:24.105898: step 31710, loss = 0.75 (1826.2 examples/sec; 0.070 sec/batch)
2017-05-02 15:35:24.897790: step 31720, loss = 0.75 (1616.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:25.688075: step 31730, loss = 0.78 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:26.479173: step 31740, loss = 0.78 (1618.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:27.267117: step 31750, loss = 0.60 (1624.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:28.051418: step 31760, loss = 0.76 (1632.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:35:28.845352: step 31770, loss = 0.75 (1612.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:29.633089: step 31780, loss = 0.90 (1624.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:30.423701: step 31790, loss = 0.80 (1619.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:31.312439: step 31800, loss = 1.01 (1440.2 examples/sec; 0.089 sec/batch)
2017-05-02 15:35:31.996159: step 31810, loss = 0.67 (1872.1 examples/sec; 0.068 sec/batch)
2017-05-02 15:35:32.791704: step 31820, loss = 0.95 (1609.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:35:33.578162: step 31830, loss = 0.87 (1627.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:34.362395: step 31840, loss = 0.84 (1632.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:35:35.154300: step 31850, loss = 0.86 (1616.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:35.930560: step 31860, loss = 0.85 (1648.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:35:36.721603: step 31870, loss = 0.77 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:37.511451: step 31880, loss = 0.76 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:38.304554: step 31890, loss = 0.75 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:39.190512: step 31900, loss = 0.76 (1444.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:35:39.879668: step 31910, loss = 0.73 (1857.3 examples/sec; 0.069 sec/batch)
2017-05-02 15:35:40.668327: step 31920, loss = 0.71 (1623.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:41.460185: step 31930, loss = 0.79 (1616.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:42.253993: step 31940, loss = 0.84 (1612.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:43.038849: step 31950, loss = 0.84 (1630.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:35:43.820569: step 31960, loss = 0.90 (1637.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:35:44.611692: step 31970, loss = 0.79 (1618.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:45.400627: step 31980, loss = 0.73 (1622.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:46.194457: step 31990, loss = 0.72 (1612.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:47.090388: step 32000, loss = 0.78 (1428.7 examples/sec; 0.090 sec/batch)
2017-05-02 15:35:47.772819: step 32010, loss = 0.74 (1875.7 examples/sec; 0.068 sec/batch)
2017-05-02 15:35:48.561438: step 32020, loss = 0.76 (1623.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:49.357295: step 32030, loss = 0.74 (1608.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:35:50.145453: step 32040, loss = 0.80 (1624.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:50.937989: step 32050, loss = 0.75 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:51.716267: step 32060, loss = 0.71 (1644.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:35:52.503647: step 32070, loss = 0.76 (1625.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:53.289697: step 32080, loss = 0.80 (1628.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:54.082652: step 32090, loss = 0.74 (1614.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:54.966459: step 32100, loss = 0.72 (1448.3 examples/sec; 0.088 sec/batch)
2017-05-02 15:35:55.663830: step 32110, loss = 0.77 (1835.5 examples/sec; 0.070 sec/batch)
2017-05-02 15:35:56.449296: step 32120, loss = 0.87 (1629.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:57.243640: step 32130, loss = 0.73 (1611.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:58.036844: step 32140, loss = 0.69 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:58.828087: step 32150, loss = 0.67 (1617.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:35:59.612949: step 32160, loss = 0.80 (1630.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:36:00.403537: step 32170, loss = 0.84 (1619.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:01.194374: step 32180, loss = 0.70 (1618.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:01.978247: step 32190, loss = 0.74 (1632.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:36:02.884612: step 32200, loss = 0.77 (1412.2 examples/sec; 0.091 sec/batch)
2017-05-02 15:36:03.554587: step 32210, loss = 0.96 (1910.5 examples/sec; 0.067 sec/batch)
2017-05-02 15:36:04.343854: step 32220, loss = 0.83 (1621.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:05.135788: step 32230, loss = 0.77 (1616.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:05.920148: step 32240, loss = 0.88 (1631.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:36:06.718215: step 32250, loss = 0.70 (1603.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:36:07.502340: step 32260, loss = 0.70 (1632.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:36:08.290085: step 32270, loss = 0.66 (1624.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:09.084020: step 32280, loss = 0.98 (1612.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:09.872407: step 32290, loss = 0.83 (1623.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:10.757210: step 32300, loss = 0.78 (1446.6 examples/sec; 0.088 sec/batch)
2017-05-02 15:36:11.445970: step 32310, loss = 0.67 (1858.4 examples/sec; 0.069 sec/batch)
2017-05-02 15:36:12.233867: step 32320, loss = 0.82 (1624.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:13.023975: step 32330, loss = 0.70 (1620.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:13.813743: step 32340, loss = 0.67 (1620.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:14.603203: step 32350, loss = 0.74 (1621.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:15.387793: step 32360, loss = 0.76 (1631.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:36:16.177541: step 32370, loss = 0.69 (1620.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:16.970840: step 32380, loss = 0.68 (1613.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:17.767579: step 32390, loss = 0.90 (1606.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:36:18.658478: step 32400, loss = 0.62 (1436.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:36:19.346839: step 32410, loss = 0.63 (1859.5 examples/sec; 0.069 sec/batch)
2017-05-02 15:36:20.127369: step 32420, loss = 1.07 (1639.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:36:20.921163: step 32430, loss = 0.75 (1612.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:21.706310: step 32440, loss = 0.79 (1630.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:22.505648: step 32450, loss = 0.84 (1601.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:36:23.296953: step 32460, loss = 0.71 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:24.080378: step 32470, loss = 0.74 (1633.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:36:24.870913: step 32480, loss = 0.77 (1619.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:25.660982: step 32490, loss = 0.95 (1620.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:26.571862: step 32500, loss = 1.08 (1405.2 examples/sec; 0.091 sec/batch)
2017-05-02 15:36:27.249946: step 32510, loss = 0.68 (1887.7 examples/sec; 0.068 sec/batch)
2017-05-02 15:36:28.034686: step 32520, loss = 0.79 (1631.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:36:28.823201: step 32530, loss = 0.81 (1623.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:29.609403: step 32540, loss = 0.96 (1628.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:30.402065: step 32550, loss = 0.76 (1614.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:31.197463: step 32560, loss = 0.82 (1609.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:36:31.972294: step 32570, loss = 0.80 (1652.0 examples/sec; 0.077 sec/batch)
2017-05-02 15:36:32.755664: step 32580, loss = 0.83 (1634.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:36:33.548627: step 32590, loss = 0.79 (1614.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:34.460995: step 32600, loss = 0.80 (1402.9 examples/sec; 0.091 sec/batch)
2017-05-02 15:36:35.126298: step 32610, loss = 0.81 (1923.9 examples/sec; 0.067 sec/batch)
2017-05-02 15:36:35.910728: step 32620, loss = 0.81 (1631.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:36:36.706235: step 32630, loss = 0.69 (1609.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:36:37.497404: step 32640, loss = 0.77 (1617.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:38.291521: step 32650, loss = 0.82 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:39.075294: step 32660, loss = 0.69 (1633.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:36:39.853490: step 32670, loss = 0.74 (1644.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:36:40.648271: step 32680, loss = 0.77 (1610.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:41.439290: step 32690, loss = 0.76 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:42.328767: step 32700, loss = 0.87 (1439.0 examples/sec; 0.089 sec/batch)
2017-05-02 15:36:43.017264: step 32710, loss = 0.72 (1859.1 examples/sec; 0.069 sec/batch)
2017-05-02 15:36:43.801517: step 32720, loss = 0.74 (1632.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:36:44.594242: step 32730, loss = 0.80 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:45.386942: step 32740, loss = 0.86 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:46.176525: step 32750, loss = 0.67 (1621.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:46.967131: step 32760, loss = 0.99 (1619.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:47.749577: step 32770, loss = 0.60 (1635.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:36:48.536769: step 32780, loss = 0.69 (1626.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:49.328914: step 32790, loss = 0.76 (1615.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:50.209457: step 32800, loss = 0.64 (1453.6 examples/sec; 0.088 sec/batch)
2017-05-02 15:36:50.902512: step 32810, loss = 0.78 (1846.9 examples/sec; 0.069 sec/batch)
2017-05-02 15:36:51.685304: step 32820, loss = 0.77 (1635.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:36:52.483335: step 32830, loss = 0.71 (1603.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:36:53.275721: step 32840, loss = 0.63 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:54.062479: step 32850, loss = 0.84 (1626.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:54.854015: step 32860, loss = 0.88 (1617.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:36:55.634826: step 32870, loss = 0.79 (1639.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:36:56.418934: step 32880, loss = 0.72 (1632.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:36:57.217893: step 32890, loss = 0.58 (1602.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:36:58.109858: step 32900, loss = 0.81 (1435.0 examples/sec; 0.089 sec/batch)
2017-05-02 15:36:58.798458: step 32910, loss = 0.92 (1858.9 examples/sec; 0.069 sec/batch)
2017-05-02 15:36:59.580310: step 32920, loss = 0.77 (1637.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:37:00.367631: step 32930, loss = 0.71 (1625.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:01.162912: step 32940, loss = 0.70 (1609.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:37:01.951441: step 32950, loss = 0.92 (1623.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:02.749288: step 32960, loss = 0.88 (1604.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:37:03.525172: step 32970, loss = 0.80 (1649.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:37:04.308717: step 32980, loss = 0.68 (1633.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:37:05.098201: step 32990, loss = 0.69 (1621.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:05.989280: step 33000, loss = 0.79 (1436.5 examples/sec; 0.089 sec/batch)
2017-05-02 15:37:06.684322: step 33010, loss = 0.78 (1841.6 examples/sec; 0.070 sec/batch)
2017-05-02 15:37:07.470524: step 33020, loss = 0.69 (1628.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:08.256555: step 33030, loss = 0.74 (1628.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:09.054746: step 33040, loss = 0.82 (1603.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:37:09.847975: step 33050, loss = 0.86 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:10.637784: step 33060, loss = 0.68 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:11.425147: step 33070, loss = 0.76 (1625.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:12.210931: step 33080, loss = 0.59 (1629.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:13.007069: step 33090, loss = 0.82 (1607.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:37:13.902551: step 33100, loss = 0.87 (1429.4 examples/sec; 0.090 sec/batch)
2017-05-02 15:37:14.586082: step 33110, loss = 0.66 (1872.6 examples/sec; 0.068 sec/batch)
2017-05-02 15:37:15.376819: step 33120, loss = 0.76 (1618.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:16.159903: step 33130, loss = 0.86 (1634.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:37:16.958781: step 33140, loss = 0.79 (1602.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:37:17.750470: step 33150, loss = 0.66 (1616.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:18.546993: step 33160, loss = 0.86 (1607.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:37:19.338028: step 33170, loss = 0.85 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:20.115503: step 33180, loss = 0.73 (1646.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:37:20.900364: step 33190, loss = 0.85 (1630.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:37:21.789108: step 33200, loss = 0.66 (1440.2 examples/sec; 0.089 sec/batch)
2017-05-02 15:37:22.487598: step 33210, loss = 0.72 (1832.5 examples/sec; 0.070 sec/batch)
2017-05-02 15:37:23.277590: step 33220, loss = 0.86 (1620.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:24.075313: step 33230, loss = 0.76 (1604.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:37:24.867653: step 33240, loss = 0.70 (1615.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:25.665269: step 33250, loss = 0.72 (1604.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:37:26.451441: step 33260, loss = 0.69 (1628.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:27.243228: step 33270, loss = 0.80 (1616.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:28.027268: step 33280, loss = 0.71 (1632.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:37:28.813371: step 33290, loss = 0.85 (1628.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:29.703482: step 33300, loss = 0.67 (1438.0 examples/sec; 0.089 sec/batch)
2017-05-02 15:37:30.402822: step 33310, loss = 0.78 (1830.3 examples/sec; 0.070 sec/batch)
2017-05-02 15:37:31.194343: step 33320, loss = 0.60 (1617.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:31.976225: step 33330, loss = 0.85 (1637.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:37:32.771340: step 33340, loss = 0.91 (1609.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:37:33.558290: step 33350, loss = 0.80 (1626.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:34.351096: step 33360, loss = 0.79 (1614.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:35.141840: step 33370, loss = 0.77 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:35.919241: step 33380, loss = 0.75 (1646.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:37:36.712854: step 33390, loss = 0.90 (1612.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:37.599374: step 33400, loss = 0.74 (1443.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:37:38.304299: step 33410, loss = 0.74 (1815.8 examples/sec; 0.070 sec/batch)
2017-05-02 15:37:39.093588: step 33420, loss = 0.70 (1621.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:39.872291: step 33430, loss = 0.77 (1643.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:37:40.666411: step 33440, loss = 0.73 (1611.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:41.457516: step 33450, loss = 0.69 (1618.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:42.256388: step 33460, loss = 0.78 (1602.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:37:43.047744: step 33470, loss = 0.82 (1617.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:43.826669: step 33480, loss = 0.77 (1643.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:37:44.618135: step 33490, loss = 0.79 (1617.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:45.506679: step 33500, loss = 0.87 (1440.6 examples/sec; 0.089 sec/batch)
2017-05-02 15:37:46.211570: step 33510, loss = 0.71 (1815.9 examples/sec; 0.070 sec/batch)
2017-05-02 15:37:47.005158: step 33520, loss = 0.76 (1612.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:47.787567: step 33530, loss = 0.77 (1636.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:37:48.572862: step 33540, loss = 0.70 (1630.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:49.359923: step 33550, loss = 0.80 (1626.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:50.147140: step 33560, loss = 0.71 (1626.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:50.942080: step 33570, loss = 0.71 (1610.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:51.717235: step 33580, loss = 0.81 (1651.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:37:52.509620: step 33590, loss = 0.66 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:53.396500: step 33600, loss = 0.77 (1443.3 examples/sec; 0.089 sec/batch)
2017-05-02 15:37:54.101089: step 33610, loss = 0.98 (1816.7 examples/sec; 0.070 sec/batch)
2017-05-02 15:37:54.893931: step 33620, loss = 0.86 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:55.671454: step 33630, loss = 0.64 (1646.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:37:56.464825: step 33640, loss = 0.65 (1613.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:57.260083: step 33650, loss = 0.65 (1609.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:37:58.049222: step 33660, loss = 0.82 (1622.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:37:58.847466: step 33670, loss = 0.66 (1603.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:37:59.631194: step 33680, loss = 0.83 (1633.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:38:00.431645: step 33690, loss = 0.81 (1599.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:38:01.321853: step 33700, loss = 0.70 (1437.9 examples/sec; 0.089 sec/batch)
2017-05-02 15:38:02.015705: step 33710, loss = 0.76 (1844.8 examples/sec; 0.069 sec/batch)
2017-05-02 15:38:02.803179: step 33720, loss = 0.91 (1625.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:03.586734: step 33730, loss = 0.84 (1633.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:38:04.378536: step 33740, loss = 0.62 (1616.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:05.172136: step 33750, loss = 0.78 (1612.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:05.971317: step 33760, loss = 0.69 (1601.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:38:06.757458: step 33770, loss = 0.87 (1628.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:07.544580: step 33780, loss = 0.84 (1626.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:08.333548: step 33790, loss = 0.72 (1622.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:09.227571: step 33800, loss = 0.96 (1431.7 examples/sec; 0.089 sec/batch)
2017-05-02 15:38:09.922644: step 33810, loss = 0.88 (1841.5 examples/sec; 0.070 sec/batch)
2017-05-02 15:38:10.714970: step 33820, loss = 0.93 (1615.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:11.505346: step 33830, loss = 0.73 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:12.295188: step 33840, loss = 0.66 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:13.083304: step 33850, loss = 0.95 (1624.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:13.874759: step 33860, loss = 0.65 (1617.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:14.664944: step 33870, loss = 0.82 (1619.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:15.449540: step 33880, loss = 0.75 (1631.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:38:16.235153: step 33890, loss = 0.81 (1629.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:17.132970: step 33900, loss = 0.83 (1425.7 examples/sec; 0.090 sec/batch)
2017-05-02 15:38:17.819077: step 33910, loss = 0.77 (1865.6 examples/sec; 0.069 sec/batch)
2017-05-02 15:38:18.605928: step 33920, loss = 0.62 (1626.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:19.399073: step 33930, loss = 0.80 (1613.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:20.177590: step 33940, loss = 0.84 (1644.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:38:20.962312: step 33950, loss = 0.92 (1631.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:38:21.756382: step 33960, loss = 0.74 (1612.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:22.545450: step 33970, loss = 0.83 (1622.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:23.335290: step 33980, loss = 0.72 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:24.113457: step 33990, loss = 0.86 (1644.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:38:25.013721: step 34000, loss = 0.68 (1421.8 examples/sec; 0.090 sec/batch)
2017-05-02 15:38:25.711123: step 34010, loss = 0.66 (1835.4 examples/sec; 0.070 sec/batch)
2017-05-02 15:38:26.507640: step 34020, loss = 0.89 (1607.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:38:27.294825: step 34030, loss = 0.98 (1626.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:28.075107: step 34040, loss = 0.67 (1640.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:38:28.867770: step 34050, loss = 0.80 (1614.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:29.659602: step 34060, loss = 0.70 (1616.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:30.449801: step 34070, loss = 0.77 (1619.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:31.240384: step 34080, loss = 0.67 (1619.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:32.019080: step 34090, loss = 0.72 (1643.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:38:32.921569: step 34100, loss = 0.88 (1418.3 examples/sec; 0.090 sec/batch)
2017-05-02 15:38:33.617083: step 34110, loss = 0.97 (1840.4 examples/sec; 0.070 sec/batch)
2017-05-02 15:38:34.404205: step 34120, loss = 0.82 (1626.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:35.196631: step 34130, loss = 0.88 (1615.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:35.981188: step 34140, loss = 0.81 (1631.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:38:36.775253: step 34150, loss = 0.81 (1612.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:37.571833: step 34160, loss = 0.78 (1606.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:38:38.363380: step 34170, loss = 0.77 (1617.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:39.151984: step 34180, loss = 0.78 (1623.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:39.930405: step 34190, loss = 0.75 (1644.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:38:40.820122: step 34200, loss = 0.76 (1438.7 examples/sec; 0.089 sec/batch)
2017-05-02 15:38:41.518639: step 34210, loss = 0.83 (1832.5 examples/sec; 0.070 sec/batch)
2017-05-02 15:38:42.307790: step 34220, loss = 0.80 (1622.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:43.099330: step 34230, loss = 0.84 (1617.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:43.878171: step 34240, loss = 0.83 (1643.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:38:44.682402: step 34250, loss = 0.75 (1591.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:38:45.477814: step 34260, loss = 0.86 (1609.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:38:46.276650: step 34270, loss = 0.69 (1602.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:38:47.064156: step 34280, loss = 0.86 (1625.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:47.851423: step 34290, loss = 0.79 (1625.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:48.752602: step 34300, loss = 0.80 (1420.4 examples/sec; 0.090 sec/batch)
2017-05-02 15:38:49.438064: step 34310, loss = 0.86 (1867.3 examples/sec; 0.069 sec/batch)
2017-05-02 15:38:50.225691: step 34320, loss = 0.71 (1625.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:51.023373: step 34330, loss = 0.77 (1604.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:38:51.806275: step 34340, loss = 0.68 (1634.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:38:52.604777: step 34350, loss = 0.67 (1603.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:38:53.398900: step 34360, loss = 0.80 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:54.185594: step 34370, loss = 0.86 (1627.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:54.981694: step 34380, loss = 0.63 (1607.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:38:55.770694: step 34390, loss = 0.77 (1622.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:56.674791: step 34400, loss = 0.78 (1415.8 examples/sec; 0.090 sec/batch)
2017-05-02 15:38:57.367611: step 34410, loss = 0.72 (1847.5 examples/sec; 0.069 sec/batch)
2017-05-02 15:38:58.161280: step 34420, loss = 0.78 (1612.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:58.955603: step 34430, loss = 0.91 (1611.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:38:59.737241: step 34440, loss = 0.88 (1637.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:39:00.534095: step 34450, loss = 0.72 (1606.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:39:01.331658: step 34460, loss = 0.79 (1604.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:39:02.119982: step 34470, loss = 0.82 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:02.914575: step 34480, loss = 0.61 (1610.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:03.699323: step 34490, loss = 0.80 (1631.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:39:04.599619: step 34500, loss = 0.67 (1421.7 examples/sec; 0.090 sec/batch)
2017-05-02 15:39:05.291476: step 34510, loss = 0.66 (1850.1 examples/sec; 0.069 sec/batch)
2017-05-02 15:39:06.085640: step 34520, loss = 0.71 (1611.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:06.882670: step 34530, loss = 0.74 (1606.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:39:07.661207: step 34540, loss = 0.73 (1644.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:39:08.448025: step 34550, loss = 0.83 (1626.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:09.233054: step 34560, loss = 0.69 (1630.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:10.022574: step 34570, loss = 0.95 (1621.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:10.817816: step 34580, loss = 0.86 (1609.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:39:11.607353: step 34590, loss = 0.68 (1621.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:12.507642: step 34600, loss = 0.69 (1421.8 examples/sec; 0.090 sec/batch)
2017-05-02 15:39:13.209640: step 34610, loss = 0.68 (1823.4 examples/sec; 0.070 sec/batch)
2017-05-02 15:39:13.996351: step 34620, loss = 0.72 (1627.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:14.784657: step 34630, loss = 0.90 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:15.570178: step 34640, loss = 0.71 (1629.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:16.362593: step 34650, loss = 0.98 (1615.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:17.153308: step 34660, loss = 0.95 (1618.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:17.946393: step 34670, loss = 0.70 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:18.745459: step 34680, loss = 0.83 (1601.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:39:19.535393: step 34690, loss = 0.80 (1620.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:20.423218: step 34700, loss = 0.90 (1441.7 examples/sec; 0.089 sec/batch)
2017-05-02 15:39:21.120850: step 34710, loss = 0.95 (1834.8 examples/sec; 0.070 sec/batch)
2017-05-02 15:39:21.910997: step 34720, loss = 0.65 (1619.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:22.705017: step 34730, loss = 0.73 (1612.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:23.495337: step 34740, loss = 0.69 (1619.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:24.284862: step 34750, loss = 0.84 (1621.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:25.075471: step 34760, loss = 0.75 (1619.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:25.867981: step 34770, loss = 0.75 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:26.657796: step 34780, loss = 0.71 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:27.446206: step 34790, loss = 0.65 (1623.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:28.355966: step 34800, loss = 0.86 (1407.0 examples/sec; 0.091 sec/batch)
2017-05-02 15:39:29.026396: step 34810, loss = 0.76 (1909.2 examples/sec; 0.067 sec/batch)
2017-05-02 15:39:29.810912: step 34820, loss = 0.86 (1631.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:39:30.601961: step 34830, loss = 0.79 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:31.396115: step 34840, loss = 0.87 (1611.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:32.186313: step 34850, loss = 0.75 (1619.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:32.978448: step 34860, loss = 0.76 (1615.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:33.772394: step 34870, loss = 0.93 (1612.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:34.565826: step 34880, loss = 0.72 (1613.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:35.356215: step 34890, loss = 0.69 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:36.245837: step 34900, loss = 0.68 (1438.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:39:36.932803: step 34910, loss = 0.88 (1863.3 examples/sec; 0.069 sec/batch)
2017-05-02 15:39:37.729472: step 34920, loss = 0.84 (1606.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:39:38.526857: step 34930, loss = 0.67 (1605.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:39:39.318310: step 34940, loss = 0.81 (1617.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:40.102579: step 34950, loss = 0.62 (1632.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:39:40.900380: step 34960, loss = 0.70 (1604.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:39:41.692092: step 34970, loss = 0.84 (1616.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:42.483561: step 34980, loss = 0.74 (1617.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:43.276793: step 34990, loss = 0.68 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:44.163296: step 35000, loss = 0.78 (1443.9 examples/sec; 0.089 sec/batch)
2017-05-02 15:39:44.853552: step 35010, loss = 0.73 (1854.4 examples/sec; 0.069 sec/batch)
2017-05-02 15:39:45.647924: step 35020, loss = 0.81 (1611.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:46.437431: step 35030, loss = 1.01 (1621.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:47.226847: step 35040, loss = 0.74 (1621.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:48.002870: step 35050, loss = 0.76 (1649.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:39:48.789994: step 35060, loss = 0.78 (1626.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:49.587760: step 35070, loss = 0.76 (1604.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:39:50.381235: step 35080, loss = 0.97 (1613.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:51.170758: step 35090, loss = 0.68 (1621.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:52.052150: step 35100, loss = 0.85 (1452.3 examples/sec; 0.088 sec/batch)
2017-05-02 15:39:52.750467: step 35110, loss = 0.60 (1833.0 examples/sec; 0.070 sec/batch)
2017-05-02 15:39:53.534251: step 35120, loss = 0.72 (1633.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:39:54.327178: step 35130, loss = 0.79 (1614.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:55.119641: step 35140, loss = 0.70 (1615.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:55.895143: step 35150, loss = 0.85 (1650.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:39:56.686182: step 35160, loss = 0.86 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:57.481473: step 35170, loss = 0.72 (1609.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:39:58.278815: step 35180, loss = 0.73 (1605.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:39:59.072264: step 35190, loss = 0.84 (1613.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:39:59.958763: step 35200, loss = 0.89 (1443.9 examples/sec; 0.089 sec/batch)
2017-05-02 15:40:00.651177: step 35210, loss = 0.99 (1848.6 examples/sec; 0.069 sec/batch)
2017-05-02 15:40:01.441028: step 35220, loss = 0.81 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:02.232017: step 35230, loss = 0.90 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:03.025582: step 35240, loss = 0.70 (1613.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:03.810197: step 35250, loss = 0.84 (1631.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:40:04.597866: step 35260, loss = 0.81 (1625.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:05.386400: step 35270, loss = 0.84 (1623.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:06.176849: step 35280, loss = 0.72 (1619.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:06.969636: step 35290, loss = 0.77 (1614.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:07.852366: step 35300, loss = 0.62 (1450.0 examples/sec; 0.088 sec/batch)
2017-05-02 15:40:08.547763: step 35310, loss = 0.59 (1840.7 examples/sec; 0.070 sec/batch)
2017-05-02 15:40:09.343710: step 35320, loss = 0.92 (1608.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:40:10.143579: step 35330, loss = 0.86 (1600.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:40:10.939114: step 35340, loss = 0.65 (1609.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:40:11.722281: step 35350, loss = 0.80 (1634.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:40:12.513437: step 35360, loss = 0.89 (1617.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:13.306619: step 35370, loss = 0.89 (1613.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:14.094694: step 35380, loss = 0.82 (1624.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:14.887063: step 35390, loss = 0.73 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:15.779005: step 35400, loss = 0.67 (1435.1 examples/sec; 0.089 sec/batch)
2017-05-02 15:40:16.472353: step 35410, loss = 0.95 (1846.1 examples/sec; 0.069 sec/batch)
2017-05-02 15:40:17.261241: step 35420, loss = 0.76 (1622.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:18.044575: step 35430, loss = 0.78 (1634.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:40:18.838751: step 35440, loss = 0.80 (1611.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:19.622467: step 35450, loss = 0.82 (1633.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:40:20.414040: step 35460, loss = 0.71 (1617.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:21.201994: step 35470, loss = 0.73 (1624.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:21.991356: step 35480, loss = 0.78 (1621.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:22.779582: step 35490, loss = 0.84 (1623.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:23.662196: step 35500, loss = 0.83 (1450.2 examples/sec; 0.088 sec/batch)
2017-05-02 15:40:24.365071: step 35510, loss = 0.93 (1821.1 examples/sec; 0.070 sec/batch)
2017-05-02 15:40:25.148937: step 35520, loss = 0.83 (1632.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:40:25.937913: step 35530, loss = 0.83 (1622.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:26.727121: step 35540, loss = 0.66 (1621.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:27.512106: step 35550, loss = 0.91 (1630.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:40:28.299967: step 35560, loss = 0.76 (1624.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:29.090696: step 35570, loss = 0.64 (1618.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:29.882936: step 35580, loss = 0.91 (1615.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:30.675594: step 35590, loss = 0.91 (1614.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:31.585048: step 35600, loss = 0.87 (1407.4 examples/sec; 0.091 sec/batch)
2017-05-02 15:40:32.258467: step 35610, loss = 0.80 (1900.7 examples/sec; 0.067 sec/batch)
2017-05-02 15:40:33.051337: step 35620, loss = 0.71 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:33.843885: step 35630, loss = 0.77 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:34.630521: step 35640, loss = 0.72 (1627.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:35.416191: step 35650, loss = 0.87 (1629.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:36.206215: step 35660, loss = 0.90 (1620.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:36.997248: step 35670, loss = 0.83 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:37.785611: step 35680, loss = 0.79 (1623.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:38.579493: step 35690, loss = 0.71 (1612.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:39.473164: step 35700, loss = 0.76 (1432.3 examples/sec; 0.089 sec/batch)
2017-05-02 15:40:40.159227: step 35710, loss = 0.63 (1865.7 examples/sec; 0.069 sec/batch)
2017-05-02 15:40:40.949354: step 35720, loss = 0.77 (1620.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:41.739028: step 35730, loss = 0.76 (1620.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:42.529551: step 35740, loss = 0.64 (1619.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:43.319257: step 35750, loss = 0.86 (1620.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:44.105032: step 35760, loss = 0.85 (1629.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:44.898793: step 35770, loss = 0.71 (1612.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:45.685432: step 35780, loss = 0.64 (1627.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:46.480046: step 35790, loss = 0.91 (1610.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:47.377220: step 35800, loss = 0.82 (1426.7 examples/sec; 0.090 sec/batch)
2017-05-02 15:40:48.063473: step 35810, loss = 0.73 (1865.2 examples/sec; 0.069 sec/batch)
2017-05-02 15:40:48.850349: step 35820, loss = 0.67 (1626.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:49.643599: step 35830, loss = 0.81 (1613.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:50.434724: step 35840, loss = 0.87 (1617.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:51.220617: step 35850, loss = 0.64 (1628.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:51.997473: step 35860, loss = 0.85 (1647.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:40:52.798330: step 35870, loss = 0.80 (1598.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:40:53.594243: step 35880, loss = 0.86 (1608.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:40:54.386748: step 35890, loss = 0.67 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:55.277086: step 35900, loss = 0.71 (1437.7 examples/sec; 0.089 sec/batch)
2017-05-02 15:40:55.956162: step 35910, loss = 0.68 (1884.9 examples/sec; 0.068 sec/batch)
2017-05-02 15:40:56.747772: step 35920, loss = 0.92 (1617.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:57.542772: step 35930, loss = 0.78 (1610.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:58.332044: step 35940, loss = 0.78 (1621.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:59.124865: step 35950, loss = 0.71 (1614.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:40:59.898008: step 35960, loss = 0.87 (1655.6 examples/sec; 0.077 sec/batch)
2017-05-02 15:41:00.698102: step 35970, loss = 0.67 (1599.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:41:01.484420: step 35980, loss = 0.98 (1627.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:02.274776: step 35990, loss = 0.98 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:03.175697: step 36000, loss = 0.76 (1420.8 examples/sec; 0.090 sec/batch)
2017-05-02 15:41:03.853004: step 36010, loss = 0.76 (1889.9 examples/sec; 0.068 sec/batch)
2017-05-02 15:41:04.640343: step 36020, loss = 0.78 (1625.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:05.435918: step 36030, loss = 0.76 (1609.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:41:06.226316: step 36040, loss = 0.73 (1619.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:07.014419: step 36050, loss = 0.76 (1624.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:07.803781: step 36060, loss = 0.78 (1621.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:08.600318: step 36070, loss = 0.77 (1607.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:41:09.400400: step 36080, loss = 0.83 (1599.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:41:10.194078: step 36090, loss = 0.68 (1612.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:11.086724: step 36100, loss = 0.73 (1433.9 examples/sec; 0.089 sec/batch)
2017-05-02 15:41:11.774351: step 36110, loss = 0.76 (1861.5 examples/sec; 0.069 sec/batch)
2017-05-02 15:41:12.565188: step 36120, loss = 0.67 (1618.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:13.360427: step 36130, loss = 0.78 (1609.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:41:14.155447: step 36140, loss = 0.80 (1610.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:41:14.938514: step 36150, loss = 0.92 (1634.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:41:15.733479: step 36160, loss = 0.86 (1610.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:16.523661: step 36170, loss = 0.76 (1619.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:17.314849: step 36180, loss = 0.71 (1617.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:18.105786: step 36190, loss = 0.90 (1618.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:18.992362: step 36200, loss = 0.79 (1443.7 examples/sec; 0.089 sec/batch)
2017-05-02 15:41:19.682294: step 36210, loss = 0.88 (1855.3 examples/sec; 0.069 sec/batch)
2017-05-02 15:41:20.475714: step 36220, loss = 0.75 (1613.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:21.266653: step 36230, loss = 0.75 (1618.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:22.063093: step 36240, loss = 0.74 (1607.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:41:22.853620: step 36250, loss = 0.82 (1619.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:23.636008: step 36260, loss = 0.94 (1636.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:41:24.426363: step 36270, loss = 0.77 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:25.220825: step 36280, loss = 0.90 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:26.017208: step 36290, loss = 0.59 (1607.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:41:26.910364: step 36300, loss = 0.65 (1433.1 examples/sec; 0.089 sec/batch)
2017-05-02 15:41:27.598905: step 36310, loss = 0.90 (1859.0 examples/sec; 0.069 sec/batch)
2017-05-02 15:41:28.392101: step 36320, loss = 0.57 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:29.179548: step 36330, loss = 0.74 (1625.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:29.969527: step 36340, loss = 0.91 (1620.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:30.759438: step 36350, loss = 0.76 (1620.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:31.543597: step 36360, loss = 0.73 (1632.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:41:32.333690: step 36370, loss = 0.77 (1620.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:33.132703: step 36380, loss = 0.80 (1602.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:41:33.920969: step 36390, loss = 0.80 (1623.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:34.806084: step 36400, loss = 0.82 (1446.2 examples/sec; 0.089 sec/batch)
2017-05-02 15:41:35.494556: step 36410, loss = 0.64 (1859.2 examples/sec; 0.069 sec/batch)
2017-05-02 15:41:36.285760: step 36420, loss = 0.73 (1617.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:37.086675: step 36430, loss = 0.67 (1598.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:41:37.878720: step 36440, loss = 0.69 (1616.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:38.667724: step 36450, loss = 0.68 (1622.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:39.455773: step 36460, loss = 0.85 (1624.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:40.243154: step 36470, loss = 0.75 (1625.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:41.042978: step 36480, loss = 0.75 (1600.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:41:41.825067: step 36490, loss = 0.79 (1636.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:41:42.718317: step 36500, loss = 0.74 (1433.0 examples/sec; 0.089 sec/batch)
2017-05-02 15:41:43.415100: step 36510, loss = 0.85 (1837.0 examples/sec; 0.070 sec/batch)
2017-05-02 15:41:44.201551: step 36520, loss = 0.58 (1627.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:44.994288: step 36530, loss = 0.77 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:45.789003: step 36540, loss = 0.84 (1610.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:46.586904: step 36550, loss = 0.72 (1604.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:41:47.373567: step 36560, loss = 0.83 (1627.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:48.161612: step 36570, loss = 0.83 (1624.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:48.955110: step 36580, loss = 0.84 (1613.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:49.745926: step 36590, loss = 0.78 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:50.644111: step 36600, loss = 0.98 (1425.1 examples/sec; 0.090 sec/batch)
2017-05-02 15:41:51.330272: step 36610, loss = 0.80 (1865.5 examples/sec; 0.069 sec/batch)
2017-05-02 15:41:52.110734: step 36620, loss = 0.74 (1640.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:41:52.907473: step 36630, loss = 0.70 (1606.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:41:53.702437: step 36640, loss = 0.77 (1610.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:54.493382: step 36650, loss = 0.69 (1618.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:55.283613: step 36660, loss = 0.75 (1619.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:56.062944: step 36670, loss = 0.69 (1642.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:41:56.854788: step 36680, loss = 0.79 (1616.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:41:57.657175: step 36690, loss = 0.82 (1595.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:41:58.542934: step 36700, loss = 0.73 (1445.1 examples/sec; 0.089 sec/batch)
2017-05-02 15:41:59.239815: step 36710, loss = 0.90 (1836.8 examples/sec; 0.070 sec/batch)
2017-05-02 15:42:00.030128: step 36720, loss = 0.67 (1619.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:00.822179: step 36730, loss = 0.82 (1616.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:01.607558: step 36740, loss = 0.75 (1629.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:02.397536: step 36750, loss = 0.71 (1620.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:03.187921: step 36760, loss = 0.80 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:03.973734: step 36770, loss = 0.86 (1628.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:04.762314: step 36780, loss = 0.77 (1623.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:05.562581: step 36790, loss = 0.77 (1599.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:42:06.453455: step 36800, loss = 0.88 (1436.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:42:07.147955: step 36810, loss = 0.91 (1843.0 examples/sec; 0.069 sec/batch)
2017-05-02 15:42:07.929805: step 36820, loss = 0.81 (1637.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:42:08.723280: step 36830, loss = 0.77 (1613.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:09.509126: step 36840, loss = 0.75 (1628.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:10.306414: step 36850, loss = 0.74 (1605.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:42:11.098943: step 36860, loss = 0.74 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:11.878077: step 36870, loss = 0.72 (1642.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:42:12.676656: step 36880, loss = 0.78 (1602.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:42:13.460007: step 36890, loss = 0.69 (1634.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:42:14.355792: step 36900, loss = 0.77 (1428.9 examples/sec; 0.090 sec/batch)
2017-05-02 15:42:15.052142: step 36910, loss = 0.75 (1838.1 examples/sec; 0.070 sec/batch)
2017-05-02 15:42:15.835362: step 36920, loss = 0.85 (1634.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:42:16.626157: step 36930, loss = 0.84 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:17.419680: step 36940, loss = 0.54 (1613.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:18.209431: step 36950, loss = 0.71 (1620.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:18.998353: step 36960, loss = 0.90 (1622.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:19.784107: step 36970, loss = 0.63 (1629.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:20.577561: step 36980, loss = 0.84 (1613.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:21.363285: step 36990, loss = 0.74 (1629.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:22.257105: step 37000, loss = 0.79 (1432.1 examples/sec; 0.089 sec/batch)
2017-05-02 15:42:22.943051: step 37010, loss = 0.88 (1866.0 examples/sec; 0.069 sec/batch)
2017-05-02 15:42:23.728038: step 37020, loss = 0.61 (1630.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:42:24.516000: step 37030, loss = 0.72 (1624.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:25.313392: step 37040, loss = 0.81 (1605.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:42:26.104472: step 37050, loss = 0.97 (1618.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:26.903279: step 37060, loss = 0.65 (1602.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:42:27.690864: step 37070, loss = 0.73 (1625.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:28.478303: step 37080, loss = 0.85 (1625.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:29.278167: step 37090, loss = 0.84 (1600.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:42:30.173188: step 37100, loss = 0.94 (1430.1 examples/sec; 0.090 sec/batch)
2017-05-02 15:42:30.862215: step 37110, loss = 0.84 (1857.7 examples/sec; 0.069 sec/batch)
2017-05-02 15:42:31.644070: step 37120, loss = 0.74 (1637.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:42:32.434739: step 37130, loss = 0.83 (1618.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:33.228110: step 37140, loss = 0.77 (1613.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:34.020067: step 37150, loss = 0.88 (1616.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:34.805078: step 37160, loss = 0.85 (1630.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:35.583757: step 37170, loss = 0.67 (1643.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:42:36.369406: step 37180, loss = 0.81 (1629.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:37.167296: step 37190, loss = 0.77 (1604.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:42:38.052452: step 37200, loss = 0.68 (1446.1 examples/sec; 0.089 sec/batch)
2017-05-02 15:42:38.752946: step 37210, loss = 0.72 (1827.3 examples/sec; 0.070 sec/batch)
2017-05-02 15:42:39.535970: step 37220, loss = 0.74 (1634.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:42:40.325619: step 37230, loss = 0.78 (1621.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:41.116104: step 37240, loss = 0.75 (1619.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:41.910192: step 37250, loss = 0.79 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:42.705735: step 37260, loss = 0.75 (1609.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:42:43.491188: step 37270, loss = 0.78 (1629.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:44.282590: step 37280, loss = 0.65 (1617.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:45.069308: step 37290, loss = 0.68 (1627.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:45.955509: step 37300, loss = 0.65 (1444.4 examples/sec; 0.089 sec/batch)
2017-05-02 15:42:46.654386: step 37310, loss = 0.79 (1831.5 examples/sec; 0.070 sec/batch)
2017-05-02 15:42:47.447625: step 37320, loss = 0.80 (1613.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:48.237169: step 37330, loss = 0.68 (1621.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:49.025394: step 37340, loss = 0.74 (1623.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:49.820530: step 37350, loss = 0.85 (1609.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:42:50.603866: step 37360, loss = 0.72 (1634.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:42:51.390163: step 37370, loss = 0.78 (1627.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:52.176776: step 37380, loss = 0.72 (1627.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:52.968843: step 37390, loss = 0.62 (1616.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:53.860955: step 37400, loss = 0.87 (1434.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:42:54.550718: step 37410, loss = 0.74 (1855.7 examples/sec; 0.069 sec/batch)
2017-05-02 15:42:55.341304: step 37420, loss = 0.80 (1619.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:56.132023: step 37430, loss = 0.63 (1618.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:56.927194: step 37440, loss = 0.77 (1609.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:42:57.720251: step 37450, loss = 0.72 (1614.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:58.508986: step 37460, loss = 1.00 (1622.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:42:59.296965: step 37470, loss = 0.72 (1624.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:00.079974: step 37480, loss = 0.69 (1634.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:43:00.875232: step 37490, loss = 0.94 (1609.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:43:01.772496: step 37500, loss = 0.78 (1426.6 examples/sec; 0.090 sec/batch)
2017-05-02 15:43:02.467258: step 37510, loss = 0.89 (1842.4 examples/sec; 0.069 sec/batch)
2017-05-02 15:43:03.268387: step 37520, loss = 0.84 (1597.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:43:04.054362: step 37530, loss = 0.68 (1628.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:04.846650: step 37540, loss = 0.83 (1615.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:05.635368: step 37550, loss = 0.79 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:06.436214: step 37560, loss = 0.74 (1598.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:43:07.226524: step 37570, loss = 0.72 (1619.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:08.011032: step 37580, loss = 0.81 (1631.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:43:08.801160: step 37590, loss = 0.86 (1620.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:09.704008: step 37600, loss = 0.61 (1417.7 examples/sec; 0.090 sec/batch)
2017-05-02 15:43:10.397442: step 37610, loss = 0.69 (1845.9 examples/sec; 0.069 sec/batch)
2017-05-02 15:43:11.195629: step 37620, loss = 0.86 (1603.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:43:11.978140: step 37630, loss = 0.89 (1635.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:43:12.763622: step 37640, loss = 0.90 (1629.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:13.558389: step 37650, loss = 0.70 (1610.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:14.344262: step 37660, loss = 0.78 (1628.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:15.132508: step 37670, loss = 0.78 (1623.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:15.914709: step 37680, loss = 0.86 (1636.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:43:16.709060: step 37690, loss = 0.85 (1611.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:17.600024: step 37700, loss = 0.83 (1436.6 examples/sec; 0.089 sec/batch)
2017-05-02 15:43:18.299992: step 37710, loss = 0.70 (1828.7 examples/sec; 0.070 sec/batch)
2017-05-02 15:43:19.088967: step 37720, loss = 0.54 (1622.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:19.866345: step 37730, loss = 0.87 (1646.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:43:20.661123: step 37740, loss = 0.73 (1610.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:21.453977: step 37750, loss = 0.75 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:22.243610: step 37760, loss = 0.80 (1621.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:23.035477: step 37770, loss = 0.73 (1616.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:23.823109: step 37780, loss = 0.76 (1625.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:24.624928: step 37790, loss = 0.81 (1596.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:43:25.522997: step 37800, loss = 0.68 (1425.3 examples/sec; 0.090 sec/batch)
2017-05-02 15:43:26.219220: step 37810, loss = 0.72 (1838.5 examples/sec; 0.070 sec/batch)
2017-05-02 15:43:27.011618: step 37820, loss = 0.69 (1615.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:27.790552: step 37830, loss = 0.85 (1643.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:43:28.577994: step 37840, loss = 0.78 (1625.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:29.370590: step 37850, loss = 0.73 (1614.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:30.189840: step 37860, loss = 0.69 (1562.4 examples/sec; 0.082 sec/batch)
2017-05-02 15:43:30.993743: step 37870, loss = 0.68 (1592.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:43:31.776185: step 37880, loss = 0.79 (1635.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:43:32.561785: step 37890, loss = 0.72 (1629.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:33.455931: step 37900, loss = 0.61 (1431.5 examples/sec; 0.089 sec/batch)
2017-05-02 15:43:34.147359: step 37910, loss = 0.65 (1851.2 examples/sec; 0.069 sec/batch)
2017-05-02 15:43:34.929742: step 37920, loss = 0.89 (1636.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:43:35.713222: step 37930, loss = 0.80 (1633.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:43:36.501036: step 37940, loss = 0.78 (1624.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:37.290067: step 37950, loss = 0.82 (1622.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:38.077170: step 37960, loss = 0.65 (1626.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:38.869091: step 37970, loss = 0.69 (1616.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:39.646464: step 37980, loss = 0.64 (1646.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:43:40.447417: step 37990, loss = 0.73 (1598.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:43:41.340540: step 38000, loss = 0.81 (1433.2 examples/sec; 0.089 sec/batch)
2017-05-02 15:43:42.017649: step 38010, loss = 0.72 (1890.4 examples/sec; 0.068 sec/batch)
2017-05-02 15:43:42.812009: step 38020, loss = 0.85 (1611.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:43.595939: step 38030, loss = 0.84 (1632.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:43:44.389369: step 38040, loss = 0.79 (1613.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:45.188374: step 38050, loss = 0.84 (1602.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:43:45.987535: step 38060, loss = 0.80 (1601.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:43:46.783026: step 38070, loss = 0.72 (1609.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:43:47.572333: step 38080, loss = 0.65 (1621.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:48.366819: step 38090, loss = 0.57 (1611.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:49.260277: step 38100, loss = 0.75 (1432.6 examples/sec; 0.089 sec/batch)
2017-05-02 15:43:49.947716: step 38110, loss = 0.83 (1862.0 examples/sec; 0.069 sec/batch)
2017-05-02 15:43:50.741052: step 38120, loss = 0.71 (1613.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:51.529430: step 38130, loss = 0.71 (1623.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:52.317871: step 38140, loss = 0.69 (1623.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:53.110360: step 38150, loss = 0.73 (1615.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:53.900709: step 38160, loss = 0.72 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:54.695040: step 38170, loss = 0.89 (1611.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:55.485161: step 38180, loss = 0.73 (1620.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:56.271403: step 38190, loss = 0.78 (1628.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:57.171784: step 38200, loss = 0.67 (1421.6 examples/sec; 0.090 sec/batch)
2017-05-02 15:43:57.858083: step 38210, loss = 0.93 (1865.1 examples/sec; 0.069 sec/batch)
2017-05-02 15:43:58.646112: step 38220, loss = 0.68 (1624.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:43:59.431245: step 38230, loss = 0.76 (1630.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:00.210972: step 38240, loss = 0.84 (1641.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:44:01.001400: step 38250, loss = 0.73 (1619.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:01.788992: step 38260, loss = 0.78 (1625.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:02.581489: step 38270, loss = 0.72 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:03.368200: step 38280, loss = 0.84 (1627.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:04.151767: step 38290, loss = 0.66 (1633.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:44:05.049107: step 38300, loss = 0.78 (1426.4 examples/sec; 0.090 sec/batch)
2017-05-02 15:44:05.742029: step 38310, loss = 0.83 (1847.2 examples/sec; 0.069 sec/batch)
2017-05-02 15:44:06.536312: step 38320, loss = 0.79 (1611.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:07.323372: step 38330, loss = 0.63 (1626.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:08.113184: step 38340, loss = 0.72 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:08.897220: step 38350, loss = 0.78 (1632.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:44:09.686161: step 38360, loss = 0.79 (1622.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:10.466195: step 38370, loss = 0.78 (1641.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:44:11.257259: step 38380, loss = 0.72 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:12.048980: step 38390, loss = 0.96 (1616.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:12.938698: step 38400, loss = 0.87 (1438.7 examples/sec; 0.089 sec/batch)
2017-05-02 15:44:13.635312: step 38410, loss = 0.67 (1837.5 examples/sec; 0.070 sec/batch)
2017-05-02 15:44:14.429764: step 38420, loss = 0.77 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:15.219946: step 38430, loss = 0.96 (1619.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:15.997765: step 38440, loss = 0.67 (1645.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:44:16.791044: step 38450, loss = 0.73 (1613.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:17.587358: step 38460, loss = 0.74 (1607.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:44:18.378556: step 38470, loss = 0.78 (1617.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:19.168392: step 38480, loss = 0.80 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:19.949055: step 38490, loss = 0.83 (1639.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:44:20.833696: step 38500, loss = 0.76 (1446.9 examples/sec; 0.088 sec/batch)
2017-05-02 15:44:21.530264: step 38510, loss = 0.80 (1837.6 examples/sec; 0.070 sec/batch)
2017-05-02 15:44:22.329409: step 38520, loss = 0.75 (1601.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:44:23.120760: step 38530, loss = 0.70 (1617.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:23.902524: step 38540, loss = 0.64 (1637.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:44:24.697565: step 38550, loss = 0.72 (1610.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:44:25.487036: step 38560, loss = 0.70 (1621.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:26.280909: step 38570, loss = 0.85 (1612.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:27.076587: step 38580, loss = 0.89 (1608.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:44:27.853198: step 38590, loss = 0.92 (1648.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:44:28.745772: step 38600, loss = 0.73 (1434.1 examples/sec; 0.089 sec/batch)
2017-05-02 15:44:29.450959: step 38610, loss = 0.86 (1815.1 examples/sec; 0.071 sec/batch)
2017-05-02 15:44:30.234680: step 38620, loss = 0.75 (1633.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:44:31.032105: step 38630, loss = 0.72 (1605.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:44:31.809980: step 38640, loss = 0.70 (1645.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:44:32.598530: step 38650, loss = 0.76 (1623.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:33.388344: step 38660, loss = 0.94 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:34.176618: step 38670, loss = 0.77 (1623.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:34.971163: step 38680, loss = 0.83 (1611.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:35.749110: step 38690, loss = 0.82 (1645.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:44:36.649364: step 38700, loss = 0.86 (1421.8 examples/sec; 0.090 sec/batch)
2017-05-02 15:44:37.338935: step 38710, loss = 0.69 (1856.2 examples/sec; 0.069 sec/batch)
2017-05-02 15:44:38.132774: step 38720, loss = 0.89 (1612.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:38.923824: step 38730, loss = 0.79 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:39.706873: step 38740, loss = 0.73 (1634.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:44:40.504744: step 38750, loss = 0.76 (1604.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:44:41.298654: step 38760, loss = 0.70 (1612.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:42.090272: step 38770, loss = 0.72 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:42.882123: step 38780, loss = 0.73 (1616.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:43.664352: step 38790, loss = 0.64 (1636.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:44:44.553991: step 38800, loss = 0.88 (1438.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:44:45.261786: step 38810, loss = 0.80 (1808.4 examples/sec; 0.071 sec/batch)
2017-05-02 15:44:46.052473: step 38820, loss = 0.77 (1618.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:46.842707: step 38830, loss = 0.71 (1619.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:47.626234: step 38840, loss = 0.62 (1633.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:44:48.417374: step 38850, loss = 0.65 (1617.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:49.209790: step 38860, loss = 0.80 (1615.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:49.995493: step 38870, loss = 0.79 (1629.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:50.790918: step 38880, loss = 0.64 (1609.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:44:51.572718: step 38890, loss = 0.83 (1637.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:44:52.457937: step 38900, loss = 0.71 (1446.0 examples/sec; 0.089 sec/batch)
2017-05-02 15:44:53.153315: step 38910, loss = 0.70 (1840.7 examples/sec; 0.070 sec/batch)
2017-05-02 15:44:53.946634: step 38920, loss = 0.74 (1613.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:54.741143: step 38930, loss = 0.70 (1611.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:55.522370: step 38940, loss = 0.79 (1638.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:44:56.311942: step 38950, loss = 0.61 (1621.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:57.107361: step 38960, loss = 0.87 (1609.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:44:57.897060: step 38970, loss = 0.84 (1620.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:58.686527: step 38980, loss = 0.86 (1621.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:44:59.478454: step 38990, loss = 0.67 (1616.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:00.359918: step 39000, loss = 0.72 (1452.1 examples/sec; 0.088 sec/batch)
2017-05-02 15:45:01.072166: step 39010, loss = 0.86 (1797.1 examples/sec; 0.071 sec/batch)
2017-05-02 15:45:01.869143: step 39020, loss = 0.66 (1606.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:45:02.661450: step 39030, loss = 0.60 (1615.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:03.448562: step 39040, loss = 0.70 (1626.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:04.232561: step 39050, loss = 0.68 (1632.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:45:05.023062: step 39060, loss = 0.69 (1619.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:05.814998: step 39070, loss = 0.71 (1616.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:06.609110: step 39080, loss = 0.72 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:07.399897: step 39090, loss = 0.81 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:08.281339: step 39100, loss = 0.71 (1452.1 examples/sec; 0.088 sec/batch)
2017-05-02 15:45:08.983513: step 39110, loss = 0.77 (1822.9 examples/sec; 0.070 sec/batch)
2017-05-02 15:45:09.780466: step 39120, loss = 0.80 (1606.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:45:10.577503: step 39130, loss = 0.83 (1606.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:45:11.364216: step 39140, loss = 0.63 (1627.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:12.152227: step 39150, loss = 0.85 (1624.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:12.946363: step 39160, loss = 0.76 (1611.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:13.738917: step 39170, loss = 0.86 (1615.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:14.526430: step 39180, loss = 0.71 (1625.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:15.316270: step 39190, loss = 0.58 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:16.200457: step 39200, loss = 0.78 (1447.7 examples/sec; 0.088 sec/batch)
2017-05-02 15:45:16.885565: step 39210, loss = 0.74 (1868.3 examples/sec; 0.069 sec/batch)
2017-05-02 15:45:17.685612: step 39220, loss = 0.73 (1599.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:45:18.478117: step 39230, loss = 0.74 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:19.261429: step 39240, loss = 0.69 (1634.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:45:20.044383: step 39250, loss = 0.73 (1634.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:45:20.833535: step 39260, loss = 0.90 (1622.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:21.622916: step 39270, loss = 0.98 (1621.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:22.414351: step 39280, loss = 0.71 (1617.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:23.203683: step 39290, loss = 0.72 (1621.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:24.089811: step 39300, loss = 0.81 (1444.5 examples/sec; 0.089 sec/batch)
2017-05-02 15:45:24.777147: step 39310, loss = 0.93 (1862.3 examples/sec; 0.069 sec/batch)
2017-05-02 15:45:25.570611: step 39320, loss = 0.62 (1613.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:26.365155: step 39330, loss = 0.95 (1611.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:27.163190: step 39340, loss = 0.72 (1603.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:45:27.945545: step 39350, loss = 0.96 (1636.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:45:28.736632: step 39360, loss = 0.81 (1618.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:29.544050: step 39370, loss = 0.86 (1585.3 examples/sec; 0.081 sec/batch)
2017-05-02 15:45:30.333910: step 39380, loss = 0.75 (1620.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:31.136188: step 39390, loss = 0.76 (1595.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:45:32.015994: step 39400, loss = 0.71 (1454.9 examples/sec; 0.088 sec/batch)
2017-05-02 15:45:32.715782: step 39410, loss = 0.84 (1829.1 examples/sec; 0.070 sec/batch)
2017-05-02 15:45:33.509266: step 39420, loss = 0.71 (1613.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:34.296774: step 39430, loss = 0.68 (1625.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:35.094435: step 39440, loss = 0.82 (1604.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:45:35.870216: step 39450, loss = 0.81 (1650.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:45:36.676451: step 39460, loss = 0.71 (1587.6 examples/sec; 0.081 sec/batch)
2017-05-02 15:45:37.466770: step 39470, loss = 0.70 (1619.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:38.258918: step 39480, loss = 0.63 (1615.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:39.044425: step 39490, loss = 0.82 (1629.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:39.930601: step 39500, loss = 0.76 (1444.4 examples/sec; 0.089 sec/batch)
2017-05-02 15:45:40.623417: step 39510, loss = 0.68 (1847.5 examples/sec; 0.069 sec/batch)
2017-05-02 15:45:41.412925: step 39520, loss = 0.82 (1621.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:42.201332: step 39530, loss = 0.89 (1623.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:42.992928: step 39540, loss = 0.78 (1617.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:43.770811: step 39550, loss = 0.76 (1645.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:45:44.562243: step 39560, loss = 0.82 (1617.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:45.353324: step 39570, loss = 0.81 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:46.143774: step 39580, loss = 0.84 (1619.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:46.935039: step 39590, loss = 0.79 (1617.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:47.824663: step 39600, loss = 0.81 (1438.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:45:48.506841: step 39610, loss = 0.73 (1876.4 examples/sec; 0.068 sec/batch)
2017-05-02 15:45:49.297680: step 39620, loss = 0.80 (1618.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:50.088034: step 39630, loss = 0.84 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:50.882635: step 39640, loss = 0.64 (1610.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:51.658229: step 39650, loss = 0.75 (1650.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:45:52.450014: step 39660, loss = 0.81 (1616.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:53.246833: step 39670, loss = 0.83 (1606.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:45:54.046548: step 39680, loss = 1.02 (1600.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:45:54.847833: step 39690, loss = 0.79 (1597.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:45:55.738907: step 39700, loss = 0.86 (1436.5 examples/sec; 0.089 sec/batch)
2017-05-02 15:45:56.427932: step 39710, loss = 0.71 (1857.6 examples/sec; 0.069 sec/batch)
2017-05-02 15:45:57.215559: step 39720, loss = 0.75 (1625.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:58.012704: step 39730, loss = 0.75 (1605.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:45:58.803294: step 39740, loss = 0.61 (1619.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:45:59.586193: step 39750, loss = 0.72 (1635.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:46:00.377298: step 39760, loss = 0.76 (1618.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:01.163996: step 39770, loss = 0.77 (1627.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:01.950809: step 39780, loss = 0.73 (1626.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:02.742874: step 39790, loss = 0.90 (1616.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:03.623527: step 39800, loss = 0.89 (1453.5 examples/sec; 0.088 sec/batch)
2017-05-02 15:46:04.312510: step 39810, loss = 0.75 (1857.8 examples/sec; 0.069 sec/batch)
2017-05-02 15:46:05.101303: step 39820, loss = 0.71 (1622.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:05.902195: step 39830, loss = 0.75 (1598.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:46:06.692359: step 39840, loss = 0.69 (1619.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:07.476819: step 39850, loss = 0.82 (1631.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:46:08.267145: step 39860, loss = 0.76 (1619.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:09.063533: step 39870, loss = 0.88 (1607.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:46:09.853951: step 39880, loss = 0.71 (1619.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:10.648593: step 39890, loss = 0.82 (1610.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:11.540385: step 39900, loss = 0.61 (1435.3 examples/sec; 0.089 sec/batch)
2017-05-02 15:46:12.228787: step 39910, loss = 0.74 (1859.4 examples/sec; 0.069 sec/batch)
2017-05-02 15:46:13.033290: step 39920, loss = 0.83 (1591.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:46:13.819753: step 39930, loss = 0.89 (1627.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:14.610801: step 39940, loss = 0.69 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:15.399319: step 39950, loss = 0.91 (1623.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:16.186698: step 39960, loss = 0.73 (1625.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:16.976583: step 39970, loss = 0.71 (1620.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:17.770668: step 39980, loss = 0.68 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:18.569322: step 39990, loss = 0.76 (1602.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:46:19.463748: step 40000, loss = 0.77 (1431.1 examples/sec; 0.089 sec/batch)
2017-05-02 15:46:20.140301: step 40010, loss = 0.59 (1892.0 examples/sec; 0.068 sec/batch)
2017-05-02 15:46:20.933966: step 40020, loss = 0.79 (1612.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:21.722701: step 40030, loss = 0.72 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:22.517249: step 40040, loss = 0.76 (1611.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:23.307925: step 40050, loss = 0.68 (1618.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:24.087479: step 40060, loss = 0.69 (1642.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:46:24.878504: step 40070, loss = 0.83 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:25.671074: step 40080, loss = 0.88 (1615.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:26.458842: step 40090, loss = 0.82 (1624.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:27.347390: step 40100, loss = 0.80 (1440.5 examples/sec; 0.089 sec/batch)
2017-05-02 15:46:28.036325: step 40110, loss = 0.81 (1857.9 examples/sec; 0.069 sec/batch)
2017-05-02 15:46:28.832252: step 40120, loss = 0.65 (1608.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:46:29.617420: step 40130, loss = 0.83 (1630.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:30.411676: step 40140, loss = 0.84 (1611.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:31.201152: step 40150, loss = 0.73 (1621.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:31.984500: step 40160, loss = 0.76 (1634.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:46:32.776851: step 40170, loss = 0.82 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:33.565260: step 40180, loss = 0.69 (1623.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:34.359908: step 40190, loss = 0.89 (1610.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:35.248253: step 40200, loss = 0.68 (1440.9 examples/sec; 0.089 sec/batch)
2017-05-02 15:46:35.928985: step 40210, loss = 0.56 (1880.3 examples/sec; 0.068 sec/batch)
2017-05-02 15:46:36.730180: step 40220, loss = 0.86 (1597.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:46:37.533739: step 40230, loss = 0.77 (1592.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:46:38.335062: step 40240, loss = 0.75 (1597.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:46:39.126112: step 40250, loss = 0.96 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:39.907050: step 40260, loss = 0.87 (1639.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:46:40.694379: step 40270, loss = 0.69 (1625.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:41.489382: step 40280, loss = 0.81 (1610.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:46:42.281567: step 40290, loss = 0.76 (1615.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:43.167705: step 40300, loss = 0.64 (1444.5 examples/sec; 0.089 sec/batch)
2017-05-02 15:46:43.859073: step 40310, loss = 0.73 (1851.4 examples/sec; 0.069 sec/batch)
2017-05-02 15:46:44.647636: step 40320, loss = 0.87 (1623.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:45.440176: step 40330, loss = 0.77 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:46.236756: step 40340, loss = 0.72 (1606.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:46:47.027790: step 40350, loss = 0.67 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:47.806909: step 40360, loss = 0.79 (1642.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:46:48.593597: step 40370, loss = 0.73 (1627.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:49.383970: step 40380, loss = 0.74 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:50.180367: step 40390, loss = 0.87 (1607.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:46:51.067617: step 40400, loss = 0.66 (1442.7 examples/sec; 0.089 sec/batch)
2017-05-02 15:46:51.748853: step 40410, loss = 0.59 (1878.9 examples/sec; 0.068 sec/batch)
2017-05-02 15:46:52.544567: step 40420, loss = 0.68 (1608.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:46:53.338761: step 40430, loss = 0.96 (1611.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:54.129540: step 40440, loss = 0.79 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:54.919590: step 40450, loss = 0.52 (1620.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:55.694134: step 40460, loss = 0.70 (1652.6 examples/sec; 0.077 sec/batch)
2017-05-02 15:46:56.490218: step 40470, loss = 0.92 (1607.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:46:57.283728: step 40480, loss = 0.71 (1613.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:58.072439: step 40490, loss = 0.73 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:46:58.963874: step 40500, loss = 0.70 (1435.9 examples/sec; 0.089 sec/batch)
2017-05-02 15:46:59.642809: step 40510, loss = 0.90 (1885.3 examples/sec; 0.068 sec/batch)
2017-05-02 15:47:00.433596: step 40520, loss = 0.75 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:01.220096: step 40530, loss = 0.63 (1627.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:02.010722: step 40540, loss = 0.73 (1619.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:02.795003: step 40550, loss = 0.73 (1632.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:47:03.580187: step 40560, loss = 0.65 (1630.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:04.368953: step 40570, loss = 0.82 (1622.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:05.158759: step 40580, loss = 0.81 (1620.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:05.952363: step 40590, loss = 0.76 (1612.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:06.843086: step 40600, loss = 0.63 (1437.0 examples/sec; 0.089 sec/batch)
2017-05-02 15:47:07.525164: step 40610, loss = 0.68 (1876.6 examples/sec; 0.068 sec/batch)
2017-05-02 15:47:08.312626: step 40620, loss = 0.74 (1625.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:09.106772: step 40630, loss = 0.73 (1611.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:09.892379: step 40640, loss = 0.90 (1629.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:10.682432: step 40650, loss = 0.71 (1620.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:11.474567: step 40660, loss = 0.78 (1615.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:12.265555: step 40670, loss = 0.78 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:13.060948: step 40680, loss = 0.84 (1609.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:47:13.860449: step 40690, loss = 1.03 (1601.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:47:14.760638: step 40700, loss = 0.85 (1421.9 examples/sec; 0.090 sec/batch)
2017-05-02 15:47:15.447855: step 40710, loss = 0.77 (1862.6 examples/sec; 0.069 sec/batch)
2017-05-02 15:47:16.239232: step 40720, loss = 0.86 (1617.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:17.033017: step 40730, loss = 0.65 (1612.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:17.834938: step 40740, loss = 0.80 (1596.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:47:18.625397: step 40750, loss = 0.91 (1619.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:19.417156: step 40760, loss = 0.71 (1616.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:20.201508: step 40770, loss = 0.64 (1631.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:47:20.994216: step 40780, loss = 0.78 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:21.786687: step 40790, loss = 0.91 (1615.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:22.676926: step 40800, loss = 0.73 (1437.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:47:23.371171: step 40810, loss = 0.86 (1843.7 examples/sec; 0.069 sec/batch)
2017-05-02 15:47:24.156482: step 40820, loss = 0.74 (1629.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:24.945152: step 40830, loss = 0.81 (1623.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:25.738491: step 40840, loss = 0.67 (1613.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:26.528555: step 40850, loss = 0.63 (1620.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:27.320000: step 40860, loss = 0.78 (1617.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:28.107219: step 40870, loss = 0.83 (1625.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:28.895472: step 40880, loss = 0.73 (1623.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:29.688597: step 40890, loss = 0.74 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:30.571455: step 40900, loss = 0.81 (1449.8 examples/sec; 0.088 sec/batch)
2017-05-02 15:47:31.269100: step 40910, loss = 0.85 (1834.7 examples/sec; 0.070 sec/batch)
2017-05-02 15:47:32.045351: step 40920, loss = 0.72 (1649.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:47:32.841844: step 40930, loss = 0.86 (1607.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:47:33.637447: step 40940, loss = 0.79 (1608.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:47:34.429782: step 40950, loss = 0.73 (1615.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:35.215278: step 40960, loss = 0.70 (1629.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:35.996835: step 40970, loss = 0.77 (1637.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:47:36.783281: step 40980, loss = 0.78 (1627.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:37.577445: step 40990, loss = 0.62 (1611.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:38.470533: step 41000, loss = 0.67 (1433.2 examples/sec; 0.089 sec/batch)
2017-05-02 15:47:39.164397: step 41010, loss = 0.69 (1844.7 examples/sec; 0.069 sec/batch)
2017-05-02 15:47:39.949448: step 41020, loss = 0.75 (1630.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:40.735921: step 41030, loss = 1.05 (1627.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:41.529966: step 41040, loss = 0.78 (1612.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:42.321410: step 41050, loss = 0.78 (1617.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:43.112078: step 41060, loss = 0.89 (1618.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:43.889567: step 41070, loss = 0.73 (1646.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:47:44.679187: step 41080, loss = 0.72 (1621.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:45.469600: step 41090, loss = 0.79 (1619.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:46.355585: step 41100, loss = 0.74 (1444.7 examples/sec; 0.089 sec/batch)
2017-05-02 15:47:47.049622: step 41110, loss = 0.79 (1844.3 examples/sec; 0.069 sec/batch)
2017-05-02 15:47:47.828520: step 41120, loss = 0.69 (1643.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:47:48.621064: step 41130, loss = 0.65 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:49.411958: step 41140, loss = 0.78 (1618.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:50.210053: step 41150, loss = 0.73 (1603.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:47:51.001212: step 41160, loss = 0.79 (1617.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:51.781931: step 41170, loss = 0.66 (1639.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:47:52.577990: step 41180, loss = 0.86 (1607.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:47:53.376966: step 41190, loss = 1.02 (1602.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:47:54.288845: step 41200, loss = 0.83 (1403.7 examples/sec; 0.091 sec/batch)
2017-05-02 15:47:54.979861: step 41210, loss = 0.70 (1852.3 examples/sec; 0.069 sec/batch)
2017-05-02 15:47:55.762586: step 41220, loss = 0.69 (1635.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:47:56.550344: step 41230, loss = 0.72 (1624.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:57.339966: step 41240, loss = 0.73 (1621.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:58.130947: step 41250, loss = 0.77 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:47:58.930411: step 41260, loss = 0.62 (1601.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:47:59.709232: step 41270, loss = 0.72 (1643.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:48:00.502751: step 41280, loss = 0.75 (1613.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:48:01.288235: step 41290, loss = 0.79 (1629.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:48:02.172604: step 41300, loss = 0.76 (1447.4 examples/sec; 0.088 sec/batch)
2017-05-02 15:48:02.870139: step 41310, loss = 0.86 (1835.0 examples/sec; 0.070 sec/batch)
2017-05-02 15:48:03.650837: step 41320, loss = 0.80 (1639.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:48:04.446017: step 41330, loss = 0.87 (1609.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:48:05.242478: step 41340, loss = 0.85 (1607.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:48:06.029010: step 41350, loss = 0.86 (1627.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:48:06.826946: step 41360, loss = 0.80 (1604.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:48:07.611791: step 41370, loss = 0.75 (1630.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:48:08.400497: step 41380, loss = 0.91 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:48:09.194589: step 41390, loss = 0.61 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:48:10.089005: step 41400, loss = 0.81 (1431.1 examples/sec; 0.089 sec/batch)
2017-05-02 15:48:10.778042: step 41410, loss = 0.74 (1857.7 examples/sec; 0.069 sec/batch)
2017-05-02 15:48:11.565276: step 41420, loss = 0.79 (1625.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:48:12.361044: step 41430, loss = 0.97 (1608.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:48:13.158154: step 41440, loss = 0.80 (1605.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:48:13.953352: step 41450, loss = 0.69 (1609.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:48:14.748628: step 41460, loss = 0.81 (1609.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:48:15.541139: step 41470, loss = 0.85 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:48:16.346170: step 41480, loss = 0.85 (1590.0 examples/sec; 0.081 sec/batch)
2017-05-02 15:48:17.139315: step 41490, loss = 0.74 (1613.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:48:18.043221: step 41500, loss = 0.70 (1416.1 examples/sec; 0.090 sec/batch)
2017-05-02 15:48:18.727057: step 41510, loss = 0.92 (1871.8 examples/sec; 0.068 sec/batch)
2017-05-02 15:48:19.515264: step 41520, loss = 0.60 (1624.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:48:20.307549: step 41530, loss = 0.86 (1615.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:48:21.095399: step 41540, loss = 0.74 (1624.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:48:21.891674: step 41550, loss = 0.73 (1607.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:48:22.694411: step 41560, loss = 0.84 (1594.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:48:23.491352: step 41570, loss = 0.85 (1606.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:48:24.281799: step 41580, loss = 0.91 (1619.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:48:25.078731: step 41590, loss = 0.71 (1606.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:48:25.972042: step 41600, loss = 0.68 (1432.9 examples/sec; 0.089 sec/batch)
2017-05-02 15:48:26.665592: step 41610, loss = 0.74 (1845.6 examples/sec; 0.069 sec/batch)
2017-05-02 15:48:27.451894: step 41620, loss = 0.70 (1627.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:48:28.248993: step 41630, loss = 0.92 (1605.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:48:29.045805: step 41640, loss = 0.59 (1606.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:48:29.849135: step 41650, loss = 0.93 (1593.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:48:30.646976: step 41660, loss = 0.83 (1604.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:48:31.437819: step 41670, loss = 0.80 (1618.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:48:32.228254: step 41680, loss = 0.78 (1619.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:48:33.017874: step 41690, loss = 0.69 (1621.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:48:33.906135: step 41700, loss = 0.91 (1441.0 examples/sec; 0.089 sec/batch)
2017-05-02 15:48:34.601578: step 41710, loss = 0.75 (1840.5 examples/sec; 0.070 sec/batch)
2017-05-02 15:48:35.395708: step 41720, loss = 0.77 (1611.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:48:36.183189: step 41730, loss = 0.67 (1625.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:48:36.975717: step 41740, loss = 0.78 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:48:37.772630: step 41750, loss = 0.75 (1606.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:48:38.568114: step 41760, loss = 0.73 (1609.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:48:39.358190: step 41770, loss = 0.78 (1620.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:48:40.141631: step 41780, loss = 0.61 (1633.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:48:40.934141: step 41790, loss = 0.73 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:48:41.846444: step 41800, loss = 0.84 (1403.0 examples/sec; 0.091 sec/batch)
2017-05-02 15:48:42.524138: step 41810, loss = 0.80 (1888.8 examples/sec; 0.068 sec/batch)
2017-05-02 15:48:43.314688: step 41820, loss = 0.70 (1619.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:48:44.098941: step 41830, loss = 0.83 (1632.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:48:44.886637: step 41840, loss = 0.89 (1625.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:48:45.676229: step 41850, loss = 0.80 (1621.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:48:46.473444: step 41860, loss = 0.81 (1605.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:48:47.263737: step 41870, loss = 0.73 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:48:48.043289: step 41880, loss = 0.78 (1642.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:48:48.848528: step 41890, loss = 0.77 (1589.6 examples/sec; 0.081 sec/batch)
2017-05-02 15:48:49.734477: step 41900, loss = 0.74 (1444.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:48:50.419502: step 41910, loss = 0.80 (1868.5 examples/sec; 0.069 sec/batch)
2017-05-02 15:48:51.215052: step 41920, loss = 0.70 (1609.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:48:51.987473: step 41930, loss = 0.76 (1657.1 examples/sec; 0.077 sec/batch)
2017-05-02 15:48:52.781327: step 41940, loss = 0.57 (1612.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:48:53.577586: step 41950, loss = 0.68 (1607.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:48:54.361078: step 41960, loss = 0.63 (1633.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:48:55.153198: step 41970, loss = 0.68 (1615.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:48:55.928042: step 41980, loss = 0.63 (1651.9 examples/sec; 0.077 sec/batch)
2017-05-02 15:48:56.723350: step 41990, loss = 0.70 (1609.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:48:57.637304: step 42000, loss = 0.67 (1400.5 examples/sec; 0.091 sec/batch)
2017-05-02 15:48:58.310507: step 42010, loss = 0.71 (1901.3 examples/sec; 0.067 sec/batch)
2017-05-02 15:48:59.096640: step 42020, loss = 0.67 (1628.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:48:59.875761: step 42030, loss = 0.89 (1642.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:49:00.682702: step 42040, loss = 0.64 (1586.2 examples/sec; 0.081 sec/batch)
2017-05-02 15:49:01.475823: step 42050, loss = 0.69 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:02.263988: step 42060, loss = 0.85 (1624.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:03.054861: step 42070, loss = 0.74 (1618.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:03.844086: step 42080, loss = 0.59 (1621.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:04.629989: step 42090, loss = 0.73 (1628.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:05.525768: step 42100, loss = 0.76 (1428.9 examples/sec; 0.090 sec/batch)
2017-05-02 15:49:06.215584: step 42110, loss = 0.84 (1855.6 examples/sec; 0.069 sec/batch)
2017-05-02 15:49:07.010665: step 42120, loss = 0.60 (1609.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:49:07.791073: step 42130, loss = 0.66 (1640.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:49:08.584299: step 42140, loss = 0.61 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:09.384256: step 42150, loss = 0.72 (1600.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:49:10.171890: step 42160, loss = 0.71 (1625.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:10.970938: step 42170, loss = 0.79 (1601.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:49:11.746130: step 42180, loss = 0.72 (1651.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:49:12.537790: step 42190, loss = 0.78 (1616.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:13.445655: step 42200, loss = 0.83 (1409.9 examples/sec; 0.091 sec/batch)
2017-05-02 15:49:14.119385: step 42210, loss = 0.65 (1899.9 examples/sec; 0.067 sec/batch)
2017-05-02 15:49:14.907930: step 42220, loss = 0.79 (1623.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:15.687384: step 42230, loss = 0.82 (1642.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:49:16.480602: step 42240, loss = 0.65 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:17.275939: step 42250, loss = 0.74 (1609.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:49:18.065579: step 42260, loss = 0.91 (1621.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:18.855716: step 42270, loss = 0.77 (1620.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:19.638413: step 42280, loss = 0.82 (1635.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:49:20.422453: step 42290, loss = 0.69 (1632.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:49:21.320271: step 42300, loss = 0.70 (1425.7 examples/sec; 0.090 sec/batch)
2017-05-02 15:49:22.010317: step 42310, loss = 0.62 (1854.9 examples/sec; 0.069 sec/batch)
2017-05-02 15:49:22.805115: step 42320, loss = 0.90 (1610.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:23.587483: step 42330, loss = 0.71 (1636.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:49:24.377765: step 42340, loss = 0.87 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:25.167603: step 42350, loss = 0.63 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:25.957803: step 42360, loss = 0.70 (1619.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:26.750614: step 42370, loss = 0.83 (1614.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:27.539309: step 42380, loss = 0.80 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:28.333075: step 42390, loss = 0.72 (1612.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:29.228543: step 42400, loss = 0.98 (1429.4 examples/sec; 0.090 sec/batch)
2017-05-02 15:49:29.928597: step 42410, loss = 0.73 (1828.5 examples/sec; 0.070 sec/batch)
2017-05-02 15:49:30.729865: step 42420, loss = 0.90 (1597.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:49:31.519020: step 42430, loss = 0.71 (1622.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:32.314782: step 42440, loss = 0.72 (1608.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:49:33.111252: step 42450, loss = 0.56 (1607.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:49:33.901108: step 42460, loss = 0.87 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:34.699178: step 42470, loss = 0.79 (1603.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:49:35.489557: step 42480, loss = 0.73 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:36.269108: step 42490, loss = 0.73 (1642.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:49:37.158716: step 42500, loss = 0.85 (1438.9 examples/sec; 0.089 sec/batch)
2017-05-02 15:49:37.854260: step 42510, loss = 0.78 (1840.3 examples/sec; 0.070 sec/batch)
2017-05-02 15:49:38.645969: step 42520, loss = 0.86 (1616.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:39.430163: step 42530, loss = 0.77 (1632.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:49:40.218193: step 42540, loss = 0.80 (1624.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:41.007936: step 42550, loss = 0.68 (1620.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:41.791751: step 42560, loss = 0.67 (1632.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:49:42.584300: step 42570, loss = 0.73 (1615.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:43.372055: step 42580, loss = 0.65 (1624.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:44.155131: step 42590, loss = 0.73 (1634.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:49:45.066129: step 42600, loss = 0.92 (1405.1 examples/sec; 0.091 sec/batch)
2017-05-02 15:49:45.737559: step 42610, loss = 0.63 (1906.5 examples/sec; 0.067 sec/batch)
2017-05-02 15:49:46.530261: step 42620, loss = 0.88 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:47.318989: step 42630, loss = 0.97 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:48.104818: step 42640, loss = 0.75 (1628.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:48.903061: step 42650, loss = 0.71 (1603.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:49:49.693355: step 42660, loss = 0.88 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:50.489271: step 42670, loss = 0.76 (1608.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:49:51.285532: step 42680, loss = 0.59 (1607.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:49:52.068700: step 42690, loss = 0.83 (1634.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:49:52.959500: step 42700, loss = 0.71 (1436.9 examples/sec; 0.089 sec/batch)
2017-05-02 15:49:53.650263: step 42710, loss = 0.92 (1853.0 examples/sec; 0.069 sec/batch)
2017-05-02 15:49:54.439581: step 42720, loss = 0.82 (1621.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:55.236201: step 42730, loss = 0.90 (1606.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:49:56.023546: step 42740, loss = 0.84 (1625.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:56.818438: step 42750, loss = 0.84 (1610.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:57.610188: step 42760, loss = 0.62 (1616.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:58.397762: step 42770, loss = 0.75 (1625.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:59.186641: step 42780, loss = 0.80 (1622.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:49:59.968555: step 42790, loss = 0.85 (1637.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:50:00.864430: step 42800, loss = 0.85 (1428.8 examples/sec; 0.090 sec/batch)
2017-05-02 15:50:01.565268: step 42810, loss = 0.78 (1826.4 examples/sec; 0.070 sec/batch)
2017-05-02 15:50:02.353247: step 42820, loss = 0.84 (1624.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:50:03.150806: step 42830, loss = 0.67 (1604.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:50:03.924557: step 42840, loss = 0.91 (1654.3 examples/sec; 0.077 sec/batch)
2017-05-02 15:50:04.726092: step 42850, loss = 0.80 (1597.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:50:05.523223: step 42860, loss = 0.73 (1605.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:50:06.315115: step 42870, loss = 0.85 (1616.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:50:07.108096: step 42880, loss = 0.81 (1614.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:50:07.887921: step 42890, loss = 0.75 (1641.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:50:08.776594: step 42900, loss = 0.60 (1440.4 examples/sec; 0.089 sec/batch)
2017-05-02 15:50:09.476364: step 42910, loss = 0.71 (1829.2 examples/sec; 0.070 sec/batch)
2017-05-02 15:50:10.264934: step 42920, loss = 0.75 (1623.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:50:11.052410: step 42930, loss = 0.84 (1625.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:50:11.839477: step 42940, loss = 0.75 (1626.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:50:12.645448: step 42950, loss = 0.90 (1588.1 examples/sec; 0.081 sec/batch)
2017-05-02 15:50:13.448256: step 42960, loss = 0.65 (1594.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:50:14.239133: step 42970, loss = 0.80 (1618.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:50:15.020967: step 42980, loss = 0.81 (1637.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:50:15.799337: step 42990, loss = 0.72 (1644.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:50:16.699457: step 43000, loss = 0.69 (1422.0 examples/sec; 0.090 sec/batch)
2017-05-02 15:50:17.378904: step 43010, loss = 0.85 (1883.9 examples/sec; 0.068 sec/batch)
2017-05-02 15:50:18.165361: step 43020, loss = 0.63 (1627.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:50:18.967929: step 43030, loss = 0.91 (1594.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:50:19.752989: step 43040, loss = 0.81 (1630.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:50:20.550257: step 43050, loss = 0.86 (1605.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:50:21.335189: step 43060, loss = 0.79 (1630.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:50:22.126441: step 43070, loss = 0.79 (1617.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:50:22.926341: step 43080, loss = 0.68 (1600.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:50:23.714083: step 43090, loss = 0.81 (1624.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:50:24.600281: step 43100, loss = 0.83 (1444.4 examples/sec; 0.089 sec/batch)
2017-05-02 15:50:25.302714: step 43110, loss = 0.61 (1822.3 examples/sec; 0.070 sec/batch)
2017-05-02 15:50:26.092935: step 43120, loss = 0.76 (1619.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:50:26.885113: step 43130, loss = 0.87 (1615.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:50:27.665538: step 43140, loss = 0.76 (1640.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:50:28.461520: step 43150, loss = 0.86 (1608.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:50:29.253349: step 43160, loss = 0.60 (1616.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:50:30.035075: step 43170, loss = 0.83 (1637.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:50:30.824752: step 43180, loss = 0.65 (1620.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:50:31.603702: step 43190, loss = 0.77 (1643.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:50:32.496999: step 43200, loss = 0.83 (1432.9 examples/sec; 0.089 sec/batch)
2017-05-02 15:50:33.193197: step 43210, loss = 0.71 (1838.6 examples/sec; 0.070 sec/batch)
2017-05-02 15:50:33.986198: step 43220, loss = 0.92 (1614.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:50:34.778225: step 43230, loss = 0.73 (1616.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:50:35.564502: step 43240, loss = 0.66 (1627.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:50:36.356941: step 43250, loss = 0.77 (1615.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:50:37.145071: step 43260, loss = 0.81 (1624.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:50:37.942319: step 43270, loss = 0.73 (1605.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:50:38.730378: step 43280, loss = 0.90 (1624.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:50:39.512615: step 43290, loss = 0.81 (1636.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:50:40.397083: step 43300, loss = 0.71 (1447.2 examples/sec; 0.088 sec/batch)
2017-05-02 15:50:41.086101: step 43310, loss = 0.74 (1857.7 examples/sec; 0.069 sec/batch)
2017-05-02 15:50:41.879672: step 43320, loss = 0.73 (1613.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:50:42.677896: step 43330, loss = 0.79 (1603.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:50:43.473695: step 43340, loss = 0.72 (1608.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:50:44.258266: step 43350, loss = 0.69 (1631.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:50:45.052159: step 43360, loss = 0.78 (1612.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:50:45.838286: step 43370, loss = 0.82 (1628.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:50:46.632933: step 43380, loss = 0.81 (1610.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:50:47.417814: step 43390, loss = 0.73 (1630.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:50:48.306774: step 43400, loss = 0.63 (1439.9 examples/sec; 0.089 sec/batch)
2017-05-02 15:50:48.994688: step 43410, loss = 0.90 (1860.7 examples/sec; 0.069 sec/batch)
2017-05-02 15:50:49.783447: step 43420, loss = 0.81 (1622.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:50:50.572673: step 43430, loss = 0.93 (1621.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:50:51.366061: step 43440, loss = 0.80 (1613.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:50:52.148379: step 43450, loss = 0.71 (1636.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:50:52.934203: step 43460, loss = 0.91 (1628.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:50:53.724076: step 43470, loss = 0.77 (1620.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:50:54.520434: step 43480, loss = 0.74 (1607.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:50:55.316361: step 43490, loss = 0.62 (1608.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:50:56.199252: step 43500, loss = 0.88 (1449.8 examples/sec; 0.088 sec/batch)
2017-05-02 15:50:56.891362: step 43510, loss = 0.72 (1849.4 examples/sec; 0.069 sec/batch)
2017-05-02 15:50:57.687758: step 43520, loss = 0.75 (1607.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:50:58.474746: step 43530, loss = 0.68 (1626.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:50:59.267008: step 43540, loss = 0.70 (1615.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:00.042327: step 43550, loss = 0.64 (1650.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:51:00.839570: step 43560, loss = 0.57 (1605.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:51:01.624233: step 43570, loss = 0.85 (1631.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:51:02.414077: step 43580, loss = 0.85 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:03.207177: step 43590, loss = 0.68 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:04.083020: step 43600, loss = 0.62 (1461.4 examples/sec; 0.088 sec/batch)
2017-05-02 15:51:04.785292: step 43610, loss = 0.60 (1822.7 examples/sec; 0.070 sec/batch)
2017-05-02 15:51:05.577202: step 43620, loss = 0.69 (1616.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:06.366007: step 43630, loss = 0.74 (1622.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:07.164026: step 43640, loss = 0.75 (1604.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:51:07.934772: step 43650, loss = 0.69 (1660.7 examples/sec; 0.077 sec/batch)
2017-05-02 15:51:08.731689: step 43660, loss = 0.68 (1606.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:51:09.522311: step 43670, loss = 0.73 (1619.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:10.308084: step 43680, loss = 0.68 (1629.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:11.103835: step 43690, loss = 0.72 (1608.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:51:11.990713: step 43700, loss = 0.81 (1443.3 examples/sec; 0.089 sec/batch)
2017-05-02 15:51:12.684403: step 43710, loss = 0.88 (1845.2 examples/sec; 0.069 sec/batch)
2017-05-02 15:51:13.479351: step 43720, loss = 0.66 (1610.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:14.267068: step 43730, loss = 0.81 (1624.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:15.056176: step 43740, loss = 0.76 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:15.838100: step 43750, loss = 0.71 (1637.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:51:16.630288: step 43760, loss = 0.77 (1615.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:17.429229: step 43770, loss = 0.67 (1602.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:51:18.222708: step 43780, loss = 0.84 (1613.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:19.009493: step 43790, loss = 0.61 (1626.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:19.885584: step 43800, loss = 0.85 (1461.0 examples/sec; 0.088 sec/batch)
2017-05-02 15:51:20.573600: step 43810, loss = 0.68 (1860.4 examples/sec; 0.069 sec/batch)
2017-05-02 15:51:21.368198: step 43820, loss = 0.68 (1610.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:22.165295: step 43830, loss = 0.90 (1605.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:51:22.957656: step 43840, loss = 0.82 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:23.738994: step 43850, loss = 0.86 (1638.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:51:24.531987: step 43860, loss = 0.82 (1614.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:25.322234: step 43870, loss = 0.75 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:26.113339: step 43880, loss = 0.77 (1618.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:26.910146: step 43890, loss = 0.73 (1606.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:51:27.786511: step 43900, loss = 0.63 (1460.6 examples/sec; 0.088 sec/batch)
2017-05-02 15:51:28.482537: step 43910, loss = 0.62 (1839.0 examples/sec; 0.070 sec/batch)
2017-05-02 15:51:29.276550: step 43920, loss = 0.58 (1612.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:30.066114: step 43930, loss = 0.79 (1621.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:30.871219: step 43940, loss = 0.76 (1589.9 examples/sec; 0.081 sec/batch)
2017-05-02 15:51:31.657447: step 43950, loss = 0.64 (1628.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:32.453594: step 43960, loss = 0.82 (1607.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:51:33.255532: step 43970, loss = 0.68 (1596.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:51:34.051994: step 43980, loss = 0.62 (1607.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:51:34.844372: step 43990, loss = 0.77 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:35.718188: step 44000, loss = 0.83 (1464.8 examples/sec; 0.087 sec/batch)
2017-05-02 15:51:36.423592: step 44010, loss = 0.72 (1814.6 examples/sec; 0.071 sec/batch)
2017-05-02 15:51:37.221719: step 44020, loss = 0.82 (1603.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:51:38.013404: step 44030, loss = 0.93 (1616.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:38.807743: step 44040, loss = 0.86 (1611.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:39.596335: step 44050, loss = 0.75 (1623.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:40.387931: step 44060, loss = 0.77 (1617.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:41.180454: step 44070, loss = 0.90 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:41.974647: step 44080, loss = 0.90 (1611.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:42.767326: step 44090, loss = 0.75 (1614.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:43.659289: step 44100, loss = 0.75 (1435.0 examples/sec; 0.089 sec/batch)
2017-05-02 15:51:44.347090: step 44110, loss = 0.80 (1861.0 examples/sec; 0.069 sec/batch)
2017-05-02 15:51:45.141520: step 44120, loss = 0.85 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:45.931275: step 44130, loss = 0.68 (1620.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:46.724138: step 44140, loss = 0.78 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:47.509359: step 44150, loss = 0.82 (1630.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:48.298631: step 44160, loss = 0.61 (1621.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:49.090246: step 44170, loss = 0.76 (1617.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:49.881711: step 44180, loss = 0.75 (1617.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:50.671504: step 44190, loss = 0.78 (1620.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:51.556543: step 44200, loss = 0.84 (1446.3 examples/sec; 0.089 sec/batch)
2017-05-02 15:51:52.254816: step 44210, loss = 0.77 (1833.1 examples/sec; 0.070 sec/batch)
2017-05-02 15:51:53.048858: step 44220, loss = 0.89 (1612.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:53.840905: step 44230, loss = 0.70 (1616.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:54.629431: step 44240, loss = 0.69 (1623.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:55.418276: step 44250, loss = 0.84 (1622.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:56.204172: step 44260, loss = 0.91 (1628.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:56.998926: step 44270, loss = 0.82 (1610.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:57.793760: step 44280, loss = 0.77 (1610.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:51:58.591734: step 44290, loss = 0.66 (1604.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:51:59.467844: step 44300, loss = 0.64 (1461.0 examples/sec; 0.088 sec/batch)
2017-05-02 15:52:00.163087: step 44310, loss = 0.89 (1841.1 examples/sec; 0.070 sec/batch)
2017-05-02 15:52:00.950451: step 44320, loss = 0.88 (1625.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:01.731535: step 44330, loss = 0.75 (1638.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:52:02.526413: step 44340, loss = 0.62 (1610.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:03.315797: step 44350, loss = 0.77 (1621.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:04.093110: step 44360, loss = 0.62 (1646.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:52:04.885681: step 44370, loss = 0.76 (1615.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:05.684006: step 44380, loss = 0.88 (1603.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:52:06.477730: step 44390, loss = 0.63 (1612.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:07.376733: step 44400, loss = 0.77 (1423.8 examples/sec; 0.090 sec/batch)
2017-05-02 15:52:08.049201: step 44410, loss = 0.78 (1903.4 examples/sec; 0.067 sec/batch)
2017-05-02 15:52:08.845493: step 44420, loss = 0.67 (1607.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:52:09.633423: step 44430, loss = 0.75 (1624.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:10.428286: step 44440, loss = 0.85 (1610.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:11.224845: step 44450, loss = 0.71 (1606.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:52:12.000268: step 44460, loss = 0.72 (1650.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:52:12.796256: step 44470, loss = 0.65 (1608.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:52:13.588485: step 44480, loss = 0.74 (1615.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:14.380510: step 44490, loss = 0.81 (1616.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:15.274081: step 44500, loss = 0.65 (1432.5 examples/sec; 0.089 sec/batch)
2017-05-02 15:52:15.958086: step 44510, loss = 0.66 (1871.3 examples/sec; 0.068 sec/batch)
2017-05-02 15:52:16.761210: step 44520, loss = 0.80 (1593.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:52:17.549972: step 44530, loss = 0.77 (1622.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:18.342949: step 44540, loss = 0.85 (1614.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:19.132031: step 44550, loss = 0.87 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:19.921423: step 44560, loss = 0.74 (1621.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:20.726497: step 44570, loss = 0.66 (1589.9 examples/sec; 0.081 sec/batch)
2017-05-02 15:52:21.526311: step 44580, loss = 0.91 (1600.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:52:22.321937: step 44590, loss = 0.71 (1608.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:52:23.215397: step 44600, loss = 0.82 (1432.6 examples/sec; 0.089 sec/batch)
2017-05-02 15:52:23.901790: step 44610, loss = 0.71 (1864.8 examples/sec; 0.069 sec/batch)
2017-05-02 15:52:24.694355: step 44620, loss = 0.72 (1615.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:25.486638: step 44630, loss = 0.78 (1615.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:26.279164: step 44640, loss = 0.83 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:27.067527: step 44650, loss = 0.74 (1623.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:27.849379: step 44660, loss = 0.73 (1637.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:52:28.643053: step 44670, loss = 0.61 (1612.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:29.440033: step 44680, loss = 0.75 (1606.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:52:30.229196: step 44690, loss = 0.80 (1622.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:31.130323: step 44700, loss = 0.68 (1420.4 examples/sec; 0.090 sec/batch)
2017-05-02 15:52:31.807668: step 44710, loss = 0.72 (1889.7 examples/sec; 0.068 sec/batch)
2017-05-02 15:52:32.599994: step 44720, loss = 0.65 (1615.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:33.390651: step 44730, loss = 0.91 (1618.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:34.175901: step 44740, loss = 0.81 (1630.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:34.969067: step 44750, loss = 1.02 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:35.757873: step 44760, loss = 0.90 (1622.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:36.549295: step 44770, loss = 0.88 (1617.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:37.340824: step 44780, loss = 0.79 (1617.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:38.134832: step 44790, loss = 0.68 (1612.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:39.029120: step 44800, loss = 0.88 (1431.3 examples/sec; 0.089 sec/batch)
2017-05-02 15:52:39.711735: step 44810, loss = 0.88 (1875.1 examples/sec; 0.068 sec/batch)
2017-05-02 15:52:40.507365: step 44820, loss = 0.76 (1608.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:52:41.303225: step 44830, loss = 0.71 (1608.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:52:42.088236: step 44840, loss = 0.70 (1630.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:42.879308: step 44850, loss = 0.64 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:43.666107: step 44860, loss = 0.76 (1626.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:44.456787: step 44870, loss = 0.61 (1618.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:45.249430: step 44880, loss = 0.67 (1614.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:46.038011: step 44890, loss = 0.67 (1623.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:46.926870: step 44900, loss = 0.78 (1440.1 examples/sec; 0.089 sec/batch)
2017-05-02 15:52:47.615620: step 44910, loss = 0.61 (1858.4 examples/sec; 0.069 sec/batch)
2017-05-02 15:52:48.404496: step 44920, loss = 0.75 (1622.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:49.191106: step 44930, loss = 0.63 (1627.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:49.979458: step 44940, loss = 0.69 (1623.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:50.772770: step 44950, loss = 0.61 (1613.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:51.554499: step 44960, loss = 0.92 (1637.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:52:52.342499: step 44970, loss = 0.76 (1624.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:53.129595: step 44980, loss = 0.64 (1626.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:53.924961: step 44990, loss = 0.68 (1609.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:52:54.821530: step 45000, loss = 0.94 (1427.7 examples/sec; 0.090 sec/batch)
2017-05-02 15:52:55.508320: step 45010, loss = 0.88 (1863.7 examples/sec; 0.069 sec/batch)
2017-05-02 15:52:56.292171: step 45020, loss = 0.82 (1633.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:52:57.078799: step 45030, loss = 0.84 (1627.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:57.875911: step 45040, loss = 0.84 (1605.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:52:58.667096: step 45050, loss = 0.63 (1617.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:52:59.452930: step 45060, loss = 0.59 (1628.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:00.234586: step 45070, loss = 0.73 (1637.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:53:01.026186: step 45080, loss = 0.79 (1617.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:01.816081: step 45090, loss = 0.76 (1620.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:02.702500: step 45100, loss = 0.75 (1444.0 examples/sec; 0.089 sec/batch)
2017-05-02 15:53:03.398902: step 45110, loss = 0.76 (1838.1 examples/sec; 0.070 sec/batch)
2017-05-02 15:53:04.190047: step 45120, loss = 0.82 (1617.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:04.980535: step 45130, loss = 0.76 (1619.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:05.787837: step 45140, loss = 0.66 (1585.5 examples/sec; 0.081 sec/batch)
2017-05-02 15:53:06.586354: step 45150, loss = 0.89 (1603.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:53:07.371873: step 45160, loss = 0.81 (1629.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:08.153686: step 45170, loss = 0.73 (1637.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:53:08.952219: step 45180, loss = 0.71 (1602.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:53:09.740232: step 45190, loss = 0.49 (1624.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:10.630750: step 45200, loss = 0.79 (1437.4 examples/sec; 0.089 sec/batch)
2017-05-02 15:53:11.322709: step 45210, loss = 0.66 (1849.8 examples/sec; 0.069 sec/batch)
2017-05-02 15:53:12.097871: step 45220, loss = 0.83 (1651.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:53:12.888669: step 45230, loss = 0.75 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:13.677507: step 45240, loss = 0.82 (1622.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:14.467804: step 45250, loss = 0.63 (1619.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:15.261712: step 45260, loss = 0.62 (1612.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:16.037860: step 45270, loss = 0.88 (1649.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:53:16.824713: step 45280, loss = 0.78 (1626.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:17.615317: step 45290, loss = 0.78 (1619.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:18.502237: step 45300, loss = 0.66 (1443.2 examples/sec; 0.089 sec/batch)
2017-05-02 15:53:19.204624: step 45310, loss = 0.86 (1822.4 examples/sec; 0.070 sec/batch)
2017-05-02 15:53:19.987149: step 45320, loss = 0.75 (1635.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:53:20.780477: step 45330, loss = 0.67 (1613.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:21.567805: step 45340, loss = 0.69 (1625.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:22.357044: step 45350, loss = 0.89 (1621.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:23.150849: step 45360, loss = 0.67 (1612.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:23.930350: step 45370, loss = 0.89 (1642.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:53:24.718539: step 45380, loss = 0.84 (1624.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:25.507523: step 45390, loss = 0.84 (1622.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:26.403232: step 45400, loss = 0.85 (1429.0 examples/sec; 0.090 sec/batch)
2017-05-02 15:53:27.102423: step 45410, loss = 0.84 (1830.7 examples/sec; 0.070 sec/batch)
2017-05-02 15:53:27.880478: step 45420, loss = 0.75 (1645.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:53:28.667627: step 45430, loss = 0.65 (1626.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:29.463464: step 45440, loss = 0.94 (1608.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:53:30.295083: step 45450, loss = 0.74 (1539.2 examples/sec; 0.083 sec/batch)
2017-05-02 15:53:31.077459: step 45460, loss = 0.80 (1636.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:53:31.860677: step 45470, loss = 0.61 (1634.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:53:32.663843: step 45480, loss = 0.70 (1593.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:53:33.455678: step 45490, loss = 0.79 (1616.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:34.339798: step 45500, loss = 0.82 (1447.8 examples/sec; 0.088 sec/batch)
2017-05-02 15:53:35.039865: step 45510, loss = 0.77 (1828.4 examples/sec; 0.070 sec/batch)
2017-05-02 15:53:35.831727: step 45520, loss = 0.67 (1616.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:36.612801: step 45530, loss = 0.82 (1638.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:53:37.405389: step 45540, loss = 0.74 (1615.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:38.193786: step 45550, loss = 0.83 (1623.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:38.987334: step 45560, loss = 0.92 (1613.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:39.767221: step 45570, loss = 0.82 (1641.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:53:40.559040: step 45580, loss = 0.74 (1616.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:41.344861: step 45590, loss = 0.73 (1628.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:42.226321: step 45600, loss = 0.71 (1452.1 examples/sec; 0.088 sec/batch)
2017-05-02 15:53:42.927873: step 45610, loss = 0.65 (1824.5 examples/sec; 0.070 sec/batch)
2017-05-02 15:53:43.708725: step 45620, loss = 0.74 (1639.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:53:44.495494: step 45630, loss = 0.65 (1626.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:45.289106: step 45640, loss = 0.78 (1612.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:46.075030: step 45650, loss = 0.74 (1628.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:46.871161: step 45660, loss = 0.98 (1607.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:53:47.646077: step 45670, loss = 0.80 (1651.8 examples/sec; 0.077 sec/batch)
2017-05-02 15:53:48.448845: step 45680, loss = 0.65 (1594.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:53:49.245611: step 45690, loss = 0.82 (1606.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:53:50.133173: step 45700, loss = 0.71 (1442.2 examples/sec; 0.089 sec/batch)
2017-05-02 15:53:50.829327: step 45710, loss = 0.55 (1838.7 examples/sec; 0.070 sec/batch)
2017-05-02 15:53:51.608565: step 45720, loss = 0.82 (1642.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:53:52.399861: step 45730, loss = 0.81 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:53.194714: step 45740, loss = 0.71 (1610.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:53.985504: step 45750, loss = 0.73 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:54.790052: step 45760, loss = 0.63 (1591.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:53:55.574931: step 45770, loss = 0.70 (1630.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:53:56.363637: step 45780, loss = 1.05 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:57.155609: step 45790, loss = 0.80 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:53:58.049230: step 45800, loss = 0.78 (1432.4 examples/sec; 0.089 sec/batch)
2017-05-02 15:53:58.739096: step 45810, loss = 0.66 (1855.4 examples/sec; 0.069 sec/batch)
2017-05-02 15:53:59.524041: step 45820, loss = 0.84 (1630.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:54:00.307448: step 45830, loss = 0.69 (1633.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:54:01.095595: step 45840, loss = 0.76 (1624.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:01.891456: step 45850, loss = 0.80 (1608.3 examples/sec; 0.080 sec/batch)
2017-05-02 15:54:02.682932: step 45860, loss = 0.70 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:03.474222: step 45870, loss = 0.64 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:04.255791: step 45880, loss = 0.79 (1637.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:54:05.044491: step 45890, loss = 0.75 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:05.929499: step 45900, loss = 0.76 (1446.3 examples/sec; 0.089 sec/batch)
2017-05-02 15:54:06.631486: step 45910, loss = 0.90 (1823.4 examples/sec; 0.070 sec/batch)
2017-05-02 15:54:07.422794: step 45920, loss = 0.63 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:08.217093: step 45930, loss = 0.86 (1611.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:09.006000: step 45940, loss = 0.91 (1622.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:09.792597: step 45950, loss = 0.89 (1627.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:10.588724: step 45960, loss = 0.70 (1607.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:54:11.372983: step 45970, loss = 0.84 (1632.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:54:12.155206: step 45980, loss = 0.68 (1636.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:54:12.948368: step 45990, loss = 0.84 (1613.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:13.840718: step 46000, loss = 0.79 (1434.4 examples/sec; 0.089 sec/batch)
2017-05-02 15:54:14.529424: step 46010, loss = 0.73 (1858.6 examples/sec; 0.069 sec/batch)
2017-05-02 15:54:15.316489: step 46020, loss = 0.64 (1626.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:16.094544: step 46030, loss = 0.75 (1645.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:54:16.884751: step 46040, loss = 0.71 (1619.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:17.681316: step 46050, loss = 0.68 (1606.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:54:18.471661: step 46060, loss = 0.80 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:19.262423: step 46070, loss = 0.68 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:20.038417: step 46080, loss = 0.75 (1649.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:54:20.828319: step 46090, loss = 0.81 (1620.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:21.715641: step 46100, loss = 0.80 (1442.5 examples/sec; 0.089 sec/batch)
2017-05-02 15:54:22.413888: step 46110, loss = 0.61 (1833.2 examples/sec; 0.070 sec/batch)
2017-05-02 15:54:23.206008: step 46120, loss = 0.79 (1615.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:23.986099: step 46130, loss = 0.88 (1640.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:54:24.771721: step 46140, loss = 0.83 (1629.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:25.561425: step 46150, loss = 0.74 (1620.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:26.356018: step 46160, loss = 0.73 (1610.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:27.145845: step 46170, loss = 0.62 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:27.922327: step 46180, loss = 0.82 (1648.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:54:28.710121: step 46190, loss = 0.71 (1624.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:29.612097: step 46200, loss = 0.73 (1419.1 examples/sec; 0.090 sec/batch)
2017-05-02 15:54:30.298261: step 46210, loss = 0.75 (1865.4 examples/sec; 0.069 sec/batch)
2017-05-02 15:54:31.090123: step 46220, loss = 0.71 (1616.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:31.873403: step 46230, loss = 0.79 (1634.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:54:32.664025: step 46240, loss = 0.82 (1619.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:33.454007: step 46250, loss = 0.88 (1620.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:34.244400: step 46260, loss = 0.85 (1619.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:35.036837: step 46270, loss = 0.78 (1615.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:35.813277: step 46280, loss = 0.88 (1648.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:54:36.603519: step 46290, loss = 0.79 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:37.504000: step 46300, loss = 0.88 (1421.5 examples/sec; 0.090 sec/batch)
2017-05-02 15:54:38.189208: step 46310, loss = 0.75 (1868.0 examples/sec; 0.069 sec/batch)
2017-05-02 15:54:38.976715: step 46320, loss = 0.83 (1625.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:39.752551: step 46330, loss = 0.81 (1649.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:54:40.549527: step 46340, loss = 0.75 (1606.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:54:41.338615: step 46350, loss = 0.67 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:42.123591: step 46360, loss = 0.83 (1630.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:54:42.912973: step 46370, loss = 0.76 (1621.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:43.695801: step 46380, loss = 0.67 (1635.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:54:44.487326: step 46390, loss = 0.77 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:45.366826: step 46400, loss = 0.78 (1455.4 examples/sec; 0.088 sec/batch)
2017-05-02 15:54:46.063524: step 46410, loss = 0.65 (1837.2 examples/sec; 0.070 sec/batch)
2017-05-02 15:54:46.852466: step 46420, loss = 0.82 (1622.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:47.630863: step 46430, loss = 0.84 (1644.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:54:48.421793: step 46440, loss = 0.70 (1618.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:49.211037: step 46450, loss = 0.69 (1621.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:50.000517: step 46460, loss = 0.72 (1621.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:50.793118: step 46470, loss = 0.74 (1614.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:51.575456: step 46480, loss = 0.78 (1636.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:54:52.364638: step 46490, loss = 0.82 (1621.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:53.244348: step 46500, loss = 0.88 (1455.0 examples/sec; 0.088 sec/batch)
2017-05-02 15:54:53.947441: step 46510, loss = 0.93 (1820.5 examples/sec; 0.070 sec/batch)
2017-05-02 15:54:54.747259: step 46520, loss = 0.83 (1600.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:54:55.529192: step 46530, loss = 0.65 (1637.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:54:56.313736: step 46540, loss = 0.75 (1631.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:54:57.112251: step 46550, loss = 0.72 (1603.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:54:57.901076: step 46560, loss = 0.64 (1622.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:58.690027: step 46570, loss = 0.67 (1622.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:54:59.469201: step 46580, loss = 0.70 (1642.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:55:00.255365: step 46590, loss = 0.68 (1628.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:01.150380: step 46600, loss = 0.76 (1430.1 examples/sec; 0.090 sec/batch)
2017-05-02 15:55:01.858314: step 46610, loss = 0.80 (1808.1 examples/sec; 0.071 sec/batch)
2017-05-02 15:55:02.656268: step 46620, loss = 0.90 (1604.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:55:03.446068: step 46630, loss = 0.85 (1620.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:04.232993: step 46640, loss = 0.76 (1626.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:05.022522: step 46650, loss = 0.82 (1621.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:05.813845: step 46660, loss = 0.81 (1617.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:06.600526: step 46670, loss = 0.87 (1627.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:07.383158: step 46680, loss = 0.90 (1635.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:55:08.171427: step 46690, loss = 0.63 (1623.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:09.053687: step 46700, loss = 0.80 (1450.8 examples/sec; 0.088 sec/batch)
2017-05-02 15:55:09.743580: step 46710, loss = 0.86 (1855.4 examples/sec; 0.069 sec/batch)
2017-05-02 15:55:10.537722: step 46720, loss = 0.78 (1611.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:11.326115: step 46730, loss = 0.60 (1623.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:12.104811: step 46740, loss = 0.82 (1643.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:55:12.910050: step 46750, loss = 0.88 (1589.6 examples/sec; 0.081 sec/batch)
2017-05-02 15:55:13.710858: step 46760, loss = 0.82 (1598.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:55:14.503908: step 46770, loss = 0.81 (1614.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:15.299317: step 46780, loss = 0.85 (1609.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:55:16.081056: step 46790, loss = 0.78 (1637.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:55:16.964981: step 46800, loss = 0.84 (1448.1 examples/sec; 0.088 sec/batch)
2017-05-02 15:55:17.666252: step 46810, loss = 0.59 (1825.3 examples/sec; 0.070 sec/batch)
2017-05-02 15:55:18.456635: step 46820, loss = 0.68 (1619.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:19.246413: step 46830, loss = 0.75 (1620.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:20.027168: step 46840, loss = 0.67 (1639.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:55:20.818057: step 46850, loss = 0.73 (1618.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:21.612709: step 46860, loss = 0.78 (1610.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:22.407856: step 46870, loss = 0.69 (1609.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:55:23.200192: step 46880, loss = 0.84 (1615.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:23.972355: step 46890, loss = 0.88 (1657.7 examples/sec; 0.077 sec/batch)
2017-05-02 15:55:24.859567: step 46900, loss = 0.78 (1442.7 examples/sec; 0.089 sec/batch)
2017-05-02 15:55:25.558078: step 46910, loss = 0.72 (1832.5 examples/sec; 0.070 sec/batch)
2017-05-02 15:55:26.339661: step 46920, loss = 0.91 (1637.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:55:27.129404: step 46930, loss = 0.58 (1620.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:27.909063: step 46940, loss = 0.57 (1641.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:55:28.700069: step 46950, loss = 0.67 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:29.497454: step 46960, loss = 0.71 (1605.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:55:30.283707: step 46970, loss = 0.77 (1628.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:31.075327: step 46980, loss = 0.78 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:31.856114: step 46990, loss = 0.72 (1639.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:55:32.744657: step 47000, loss = 0.79 (1440.6 examples/sec; 0.089 sec/batch)
2017-05-02 15:55:33.438604: step 47010, loss = 0.92 (1844.5 examples/sec; 0.069 sec/batch)
2017-05-02 15:55:34.231476: step 47020, loss = 0.73 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:35.021762: step 47030, loss = 0.74 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:35.805909: step 47040, loss = 0.63 (1632.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:55:36.589835: step 47050, loss = 0.72 (1632.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:55:37.379221: step 47060, loss = 0.85 (1621.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:38.166588: step 47070, loss = 0.72 (1625.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:38.955329: step 47080, loss = 0.82 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:39.741354: step 47090, loss = 0.73 (1628.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:40.634743: step 47100, loss = 0.85 (1432.7 examples/sec; 0.089 sec/batch)
2017-05-02 15:55:41.322264: step 47110, loss = 0.72 (1861.8 examples/sec; 0.069 sec/batch)
2017-05-02 15:55:42.116367: step 47120, loss = 0.71 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:42.907114: step 47130, loss = 0.71 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:43.691263: step 47140, loss = 0.80 (1632.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:55:44.480521: step 47150, loss = 0.78 (1621.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:45.272157: step 47160, loss = 0.84 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:46.064202: step 47170, loss = 0.79 (1616.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:46.851448: step 47180, loss = 0.69 (1625.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:47.644153: step 47190, loss = 0.83 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:48.530322: step 47200, loss = 0.80 (1444.4 examples/sec; 0.089 sec/batch)
2017-05-02 15:55:49.227199: step 47210, loss = 0.78 (1836.7 examples/sec; 0.070 sec/batch)
2017-05-02 15:55:50.013606: step 47220, loss = 0.65 (1627.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:50.804227: step 47230, loss = 0.88 (1619.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:51.585968: step 47240, loss = 0.66 (1637.4 examples/sec; 0.078 sec/batch)
2017-05-02 15:55:52.370868: step 47250, loss = 0.83 (1630.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:55:53.162945: step 47260, loss = 0.57 (1616.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:53.954169: step 47270, loss = 0.76 (1617.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:54.740189: step 47280, loss = 0.78 (1628.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:55.525200: step 47290, loss = 0.78 (1630.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:56.422372: step 47300, loss = 0.74 (1426.7 examples/sec; 0.090 sec/batch)
2017-05-02 15:55:57.120273: step 47310, loss = 0.73 (1834.1 examples/sec; 0.070 sec/batch)
2017-05-02 15:55:57.919821: step 47320, loss = 0.65 (1600.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:55:58.714198: step 47330, loss = 0.71 (1611.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:55:59.494299: step 47340, loss = 0.76 (1640.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:56:00.283140: step 47350, loss = 0.75 (1622.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:01.072647: step 47360, loss = 0.78 (1621.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:01.867111: step 47370, loss = 0.77 (1611.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:02.665813: step 47380, loss = 0.72 (1602.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:56:03.456829: step 47390, loss = 0.85 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:04.346809: step 47400, loss = 0.60 (1438.2 examples/sec; 0.089 sec/batch)
2017-05-02 15:56:05.042038: step 47410, loss = 0.75 (1841.1 examples/sec; 0.070 sec/batch)
2017-05-02 15:56:05.832293: step 47420, loss = 0.85 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:06.630308: step 47430, loss = 0.73 (1604.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:56:07.420649: step 47440, loss = 0.76 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:08.212995: step 47450, loss = 0.68 (1615.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:09.000168: step 47460, loss = 0.67 (1626.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:09.791729: step 47470, loss = 0.72 (1617.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:10.583475: step 47480, loss = 0.74 (1616.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:11.373165: step 47490, loss = 0.72 (1620.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:12.259744: step 47500, loss = 0.71 (1443.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:56:12.954270: step 47510, loss = 0.85 (1843.0 examples/sec; 0.069 sec/batch)
2017-05-02 15:56:13.748119: step 47520, loss = 0.64 (1612.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:14.541852: step 47530, loss = 0.73 (1612.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:15.329476: step 47540, loss = 0.94 (1625.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:16.111351: step 47550, loss = 0.90 (1637.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:56:16.902432: step 47560, loss = 0.66 (1618.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:17.691921: step 47570, loss = 0.82 (1621.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:18.482204: step 47580, loss = 0.76 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:19.273782: step 47590, loss = 0.77 (1617.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:20.154007: step 47600, loss = 0.85 (1454.2 examples/sec; 0.088 sec/batch)
2017-05-02 15:56:20.849078: step 47610, loss = 0.83 (1841.5 examples/sec; 0.070 sec/batch)
2017-05-02 15:56:21.639604: step 47620, loss = 0.91 (1619.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:22.429459: step 47630, loss = 0.79 (1620.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:23.228885: step 47640, loss = 0.84 (1601.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:56:24.009620: step 47650, loss = 0.78 (1639.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:56:24.800461: step 47660, loss = 0.67 (1618.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:25.591260: step 47670, loss = 0.70 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:26.381286: step 47680, loss = 0.74 (1620.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:27.172726: step 47690, loss = 0.84 (1617.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:28.049414: step 47700, loss = 0.62 (1460.0 examples/sec; 0.088 sec/batch)
2017-05-02 15:56:28.749007: step 47710, loss = 0.76 (1829.6 examples/sec; 0.070 sec/batch)
2017-05-02 15:56:29.544305: step 47720, loss = 0.64 (1609.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:56:30.340366: step 47730, loss = 0.73 (1607.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:56:31.124113: step 47740, loss = 0.82 (1633.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:56:31.907487: step 47750, loss = 0.78 (1634.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:56:32.699913: step 47760, loss = 0.63 (1615.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:33.493557: step 47770, loss = 0.71 (1612.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:34.290226: step 47780, loss = 0.84 (1606.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:56:35.077520: step 47790, loss = 0.72 (1625.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:35.956135: step 47800, loss = 0.75 (1456.8 examples/sec; 0.088 sec/batch)
2017-05-02 15:56:36.652500: step 47810, loss = 0.84 (1838.1 examples/sec; 0.070 sec/batch)
2017-05-02 15:56:37.447005: step 47820, loss = 0.78 (1611.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:38.246135: step 47830, loss = 0.71 (1601.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:56:39.035198: step 47840, loss = 0.77 (1622.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:39.821077: step 47850, loss = 0.75 (1628.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:40.620055: step 47860, loss = 0.87 (1602.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:56:41.419094: step 47870, loss = 0.69 (1601.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:56:42.217210: step 47880, loss = 0.72 (1603.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:56:43.011682: step 47890, loss = 0.73 (1611.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:43.894006: step 47900, loss = 0.85 (1450.7 examples/sec; 0.088 sec/batch)
2017-05-02 15:56:44.588371: step 47910, loss = 0.77 (1843.4 examples/sec; 0.069 sec/batch)
2017-05-02 15:56:45.382536: step 47920, loss = 0.83 (1611.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:46.170341: step 47930, loss = 0.71 (1624.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:46.960076: step 47940, loss = 0.76 (1620.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:47.749475: step 47950, loss = 0.84 (1621.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:48.539683: step 47960, loss = 0.72 (1619.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:49.332050: step 47970, loss = 0.86 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:50.118813: step 47980, loss = 0.72 (1626.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:50.913319: step 47990, loss = 0.62 (1611.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:51.800974: step 48000, loss = 0.83 (1442.0 examples/sec; 0.089 sec/batch)
2017-05-02 15:56:52.497171: step 48010, loss = 0.63 (1838.6 examples/sec; 0.070 sec/batch)
2017-05-02 15:56:53.292355: step 48020, loss = 0.71 (1609.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:56:54.085569: step 48030, loss = 0.89 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:54.882770: step 48040, loss = 0.58 (1605.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:56:55.665096: step 48050, loss = 0.81 (1636.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:56:56.459660: step 48060, loss = 0.74 (1611.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:57.251560: step 48070, loss = 0.63 (1616.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:58.043106: step 48080, loss = 0.83 (1617.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:58.832238: step 48090, loss = 0.75 (1622.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:56:59.714533: step 48100, loss = 0.70 (1450.8 examples/sec; 0.088 sec/batch)
2017-05-02 15:57:00.408159: step 48110, loss = 0.80 (1845.4 examples/sec; 0.069 sec/batch)
2017-05-02 15:57:01.206313: step 48120, loss = 0.76 (1603.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:57:01.994872: step 48130, loss = 0.64 (1623.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:02.786155: step 48140, loss = 0.70 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:03.563665: step 48150, loss = 0.69 (1646.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:57:04.358984: step 48160, loss = 0.64 (1609.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:57:05.159961: step 48170, loss = 0.80 (1598.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:57:05.953673: step 48180, loss = 0.77 (1612.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:06.747594: step 48190, loss = 0.90 (1612.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:07.643535: step 48200, loss = 0.70 (1428.7 examples/sec; 0.090 sec/batch)
2017-05-02 15:57:08.330751: step 48210, loss = 0.66 (1862.6 examples/sec; 0.069 sec/batch)
2017-05-02 15:57:09.125758: step 48220, loss = 0.74 (1610.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:57:09.916088: step 48230, loss = 0.74 (1619.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:10.717340: step 48240, loss = 0.75 (1597.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:57:11.504109: step 48250, loss = 0.76 (1626.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:12.287356: step 48260, loss = 0.93 (1634.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:57:13.077421: step 48270, loss = 0.81 (1620.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:13.868676: step 48280, loss = 0.78 (1617.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:14.657732: step 48290, loss = 0.80 (1622.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:15.544023: step 48300, loss = 0.80 (1444.2 examples/sec; 0.089 sec/batch)
2017-05-02 15:57:16.234410: step 48310, loss = 0.92 (1854.0 examples/sec; 0.069 sec/batch)
2017-05-02 15:57:17.026832: step 48320, loss = 0.67 (1615.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:17.823110: step 48330, loss = 0.86 (1607.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:57:18.615831: step 48340, loss = 0.67 (1614.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:19.405842: step 48350, loss = 0.71 (1620.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:20.193516: step 48360, loss = 0.79 (1625.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:20.975959: step 48370, loss = 0.77 (1635.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:57:21.777504: step 48380, loss = 0.74 (1596.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:57:22.567429: step 48390, loss = 0.77 (1620.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:23.460713: step 48400, loss = 0.86 (1432.9 examples/sec; 0.089 sec/batch)
2017-05-02 15:57:24.141674: step 48410, loss = 0.76 (1879.7 examples/sec; 0.068 sec/batch)
2017-05-02 15:57:24.935259: step 48420, loss = 0.64 (1612.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:25.726964: step 48430, loss = 0.76 (1616.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:26.522089: step 48440, loss = 0.65 (1609.8 examples/sec; 0.080 sec/batch)
2017-05-02 15:57:27.311689: step 48450, loss = 0.84 (1621.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:28.093196: step 48460, loss = 0.70 (1637.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:57:28.881335: step 48470, loss = 0.66 (1624.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:29.674563: step 48480, loss = 0.71 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:30.461956: step 48490, loss = 0.85 (1625.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:31.347928: step 48500, loss = 0.73 (1444.7 examples/sec; 0.089 sec/batch)
2017-05-02 15:57:32.046946: step 48510, loss = 0.67 (1831.1 examples/sec; 0.070 sec/batch)
2017-05-02 15:57:32.839581: step 48520, loss = 0.81 (1614.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:33.629539: step 48530, loss = 0.77 (1620.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:34.422708: step 48540, loss = 0.83 (1613.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:35.205705: step 48550, loss = 0.84 (1634.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:57:35.986273: step 48560, loss = 0.66 (1639.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:57:36.781714: step 48570, loss = 0.65 (1609.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:57:37.575961: step 48580, loss = 0.81 (1611.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:38.371257: step 48590, loss = 0.64 (1609.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:57:39.262763: step 48600, loss = 0.64 (1435.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:57:39.942564: step 48610, loss = 0.75 (1882.9 examples/sec; 0.068 sec/batch)
2017-05-02 15:57:40.733472: step 48620, loss = 0.63 (1618.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:41.524749: step 48630, loss = 0.91 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:42.316055: step 48640, loss = 0.71 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:43.110548: step 48650, loss = 0.70 (1611.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:43.888441: step 48660, loss = 0.68 (1645.5 examples/sec; 0.078 sec/batch)
2017-05-02 15:57:44.682077: step 48670, loss = 0.98 (1612.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:45.470189: step 48680, loss = 0.69 (1624.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:46.262309: step 48690, loss = 0.92 (1615.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:47.159438: step 48700, loss = 0.82 (1426.8 examples/sec; 0.090 sec/batch)
2017-05-02 15:57:47.839367: step 48710, loss = 0.85 (1882.6 examples/sec; 0.068 sec/batch)
2017-05-02 15:57:48.631234: step 48720, loss = 0.72 (1616.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:49.424528: step 48730, loss = 0.78 (1613.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:50.223279: step 48740, loss = 0.75 (1602.5 examples/sec; 0.080 sec/batch)
2017-05-02 15:57:51.010821: step 48750, loss = 0.80 (1625.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:51.795960: step 48760, loss = 0.91 (1630.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:52.581179: step 48770, loss = 0.58 (1630.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:53.374122: step 48780, loss = 0.85 (1614.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:54.160364: step 48790, loss = 0.89 (1628.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:55.065045: step 48800, loss = 0.80 (1414.9 examples/sec; 0.090 sec/batch)
2017-05-02 15:57:55.745198: step 48810, loss = 0.73 (1881.9 examples/sec; 0.068 sec/batch)
2017-05-02 15:57:56.526605: step 48820, loss = 0.72 (1638.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:57:57.312317: step 48830, loss = 0.91 (1629.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:58.104822: step 48840, loss = 0.66 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:58.895636: step 48850, loss = 0.64 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:57:59.681864: step 48860, loss = 0.60 (1628.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:00.474486: step 48870, loss = 0.69 (1614.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:01.263329: step 48880, loss = 0.74 (1622.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:02.077332: step 48890, loss = 0.90 (1572.5 examples/sec; 0.081 sec/batch)
2017-05-02 15:58:02.966681: step 48900, loss = 0.74 (1439.3 examples/sec; 0.089 sec/batch)
2017-05-02 15:58:03.645007: step 48910, loss = 0.68 (1887.0 examples/sec; 0.068 sec/batch)
2017-05-02 15:58:04.447931: step 48920, loss = 0.71 (1594.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:58:05.244141: step 48930, loss = 0.78 (1607.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:58:06.038476: step 48940, loss = 0.61 (1611.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:06.836005: step 48950, loss = 0.68 (1605.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:58:07.621191: step 48960, loss = 0.67 (1630.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:08.417410: step 48970, loss = 0.70 (1607.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:58:09.209989: step 48980, loss = 0.64 (1615.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:10.001352: step 48990, loss = 0.72 (1617.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:10.903649: step 49000, loss = 0.78 (1418.6 examples/sec; 0.090 sec/batch)
2017-05-02 15:58:11.585513: step 49010, loss = 0.86 (1877.2 examples/sec; 0.068 sec/batch)
2017-05-02 15:58:12.377863: step 49020, loss = 0.93 (1615.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:13.172047: step 49030, loss = 0.69 (1611.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:13.967348: step 49040, loss = 0.74 (1609.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:58:14.759363: step 49050, loss = 0.74 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:15.544425: step 49060, loss = 0.65 (1630.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:16.334517: step 49070, loss = 0.81 (1620.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:17.125314: step 49080, loss = 0.74 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:17.913674: step 49090, loss = 0.69 (1623.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:18.804442: step 49100, loss = 0.72 (1437.0 examples/sec; 0.089 sec/batch)
2017-05-02 15:58:19.497132: step 49110, loss = 0.62 (1847.9 examples/sec; 0.069 sec/batch)
2017-05-02 15:58:20.283607: step 49120, loss = 0.69 (1627.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:21.079081: step 49130, loss = 0.75 (1609.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:58:21.871746: step 49140, loss = 0.77 (1614.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:22.668729: step 49150, loss = 0.66 (1606.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:58:23.448830: step 49160, loss = 0.60 (1640.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:58:24.233623: step 49170, loss = 0.80 (1631.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:58:25.026337: step 49180, loss = 0.58 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:25.816077: step 49190, loss = 0.68 (1620.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:26.700603: step 49200, loss = 0.66 (1447.1 examples/sec; 0.088 sec/batch)
2017-05-02 15:58:27.391964: step 49210, loss = 0.72 (1851.4 examples/sec; 0.069 sec/batch)
2017-05-02 15:58:28.182675: step 49220, loss = 0.76 (1618.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:28.975223: step 49230, loss = 0.66 (1615.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:29.764565: step 49240, loss = 0.86 (1621.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:30.552084: step 49250, loss = 0.80 (1625.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:31.341987: step 49260, loss = 0.72 (1620.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:32.122871: step 49270, loss = 0.90 (1639.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:58:32.920071: step 49280, loss = 0.81 (1605.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:58:33.714913: step 49290, loss = 0.61 (1610.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:34.603150: step 49300, loss = 0.81 (1441.1 examples/sec; 0.089 sec/batch)
2017-05-02 15:58:35.296379: step 49310, loss = 0.67 (1846.4 examples/sec; 0.069 sec/batch)
2017-05-02 15:58:36.074189: step 49320, loss = 0.78 (1645.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:58:36.855317: step 49330, loss = 0.81 (1638.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:58:37.648604: step 49340, loss = 0.64 (1613.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:38.442425: step 49350, loss = 0.74 (1612.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:39.229423: step 49360, loss = 0.89 (1626.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:40.012408: step 49370, loss = 0.67 (1634.8 examples/sec; 0.078 sec/batch)
2017-05-02 15:58:40.798347: step 49380, loss = 0.62 (1628.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:41.588088: step 49390, loss = 0.54 (1620.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:42.481475: step 49400, loss = 0.80 (1432.7 examples/sec; 0.089 sec/batch)
2017-05-02 15:58:43.179690: step 49410, loss = 0.80 (1833.3 examples/sec; 0.070 sec/batch)
2017-05-02 15:58:43.958937: step 49420, loss = 0.80 (1642.6 examples/sec; 0.078 sec/batch)
2017-05-02 15:58:44.754466: step 49430, loss = 0.74 (1609.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:58:45.547784: step 49440, loss = 0.78 (1613.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:46.341284: step 49450, loss = 0.81 (1613.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:47.139795: step 49460, loss = 0.75 (1603.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:58:47.917835: step 49470, loss = 0.78 (1645.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:58:48.708744: step 49480, loss = 0.55 (1618.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:49.505545: step 49490, loss = 0.91 (1606.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:58:50.400140: step 49500, loss = 0.74 (1430.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:58:51.097772: step 49510, loss = 0.75 (1834.8 examples/sec; 0.070 sec/batch)
2017-05-02 15:58:51.892606: step 49520, loss = 0.73 (1610.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:52.690507: step 49530, loss = 0.82 (1604.2 examples/sec; 0.080 sec/batch)
2017-05-02 15:58:53.474503: step 49540, loss = 0.69 (1632.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:58:54.266994: step 49550, loss = 0.66 (1615.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:55.057302: step 49560, loss = 0.74 (1619.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:55.836218: step 49570, loss = 0.80 (1643.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:58:56.625467: step 49580, loss = 0.68 (1621.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:58:57.431403: step 49590, loss = 0.70 (1588.2 examples/sec; 0.081 sec/batch)
2017-05-02 15:58:58.341968: step 49600, loss = 0.72 (1405.7 examples/sec; 0.091 sec/batch)
2017-05-02 15:58:59.029557: step 49610, loss = 0.64 (1861.6 examples/sec; 0.069 sec/batch)
2017-05-02 15:58:59.810984: step 49620, loss = 0.80 (1638.0 examples/sec; 0.078 sec/batch)
2017-05-02 15:59:00.607923: step 49630, loss = 0.69 (1606.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:59:01.404213: step 49640, loss = 0.75 (1607.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:59:02.193718: step 49650, loss = 0.76 (1621.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:02.991799: step 49660, loss = 0.71 (1603.9 examples/sec; 0.080 sec/batch)
2017-05-02 15:59:03.772616: step 49670, loss = 0.81 (1639.3 examples/sec; 0.078 sec/batch)
2017-05-02 15:59:04.569815: step 49680, loss = 0.81 (1605.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:59:05.363440: step 49690, loss = 0.83 (1612.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:06.246975: step 49700, loss = 0.68 (1448.7 examples/sec; 0.088 sec/batch)
2017-05-02 15:59:06.944436: step 49710, loss = 0.58 (1835.2 examples/sec; 0.070 sec/batch)
2017-05-02 15:59:07.726987: step 49720, loss = 0.73 (1635.7 examples/sec; 0.078 sec/batch)
2017-05-02 15:59:08.525432: step 49730, loss = 0.68 (1603.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:59:09.313607: step 49740, loss = 0.80 (1624.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:10.098929: step 49750, loss = 0.65 (1629.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:10.893064: step 49760, loss = 0.80 (1611.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:11.674426: step 49770, loss = 0.79 (1638.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:59:12.464980: step 49780, loss = 0.81 (1619.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:13.257786: step 49790, loss = 0.82 (1614.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:14.146169: step 49800, loss = 0.85 (1440.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:59:14.846556: step 49810, loss = 0.77 (1827.6 examples/sec; 0.070 sec/batch)
2017-05-02 15:59:15.634750: step 49820, loss = 0.64 (1624.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:16.422324: step 49830, loss = 0.83 (1625.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:17.212792: step 49840, loss = 0.81 (1619.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:18.003804: step 49850, loss = 0.75 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:18.797266: step 49860, loss = 0.64 (1613.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:19.580990: step 49870, loss = 0.70 (1633.2 examples/sec; 0.078 sec/batch)
2017-05-02 15:59:20.372221: step 49880, loss = 0.70 (1617.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:21.164647: step 49890, loss = 0.85 (1615.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:22.053612: step 49900, loss = 0.77 (1439.9 examples/sec; 0.089 sec/batch)
2017-05-02 15:59:22.758531: step 49910, loss = 0.66 (1815.8 examples/sec; 0.070 sec/batch)
2017-05-02 15:59:23.536143: step 49920, loss = 0.85 (1646.1 examples/sec; 0.078 sec/batch)
2017-05-02 15:59:24.328204: step 49930, loss = 0.58 (1616.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:25.114640: step 49940, loss = 0.53 (1627.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:25.898519: step 49950, loss = 0.72 (1632.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:59:26.686269: step 49960, loss = 0.89 (1624.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:27.471723: step 49970, loss = 0.69 (1629.6 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:28.257283: step 49980, loss = 0.75 (1629.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:29.050460: step 49990, loss = 0.76 (1613.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:29.947425: step 50000, loss = 0.83 (1427.0 examples/sec; 0.090 sec/batch)
2017-05-02 15:59:30.646532: step 50010, loss = 0.66 (1830.9 examples/sec; 0.070 sec/batch)
2017-05-02 15:59:31.439706: step 50020, loss = 0.78 (1613.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:32.232728: step 50030, loss = 0.73 (1614.1 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:33.033854: step 50040, loss = 0.79 (1597.7 examples/sec; 0.080 sec/batch)
2017-05-02 15:59:33.825079: step 50050, loss = 0.76 (1617.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:34.622759: step 50060, loss = 0.72 (1604.6 examples/sec; 0.080 sec/batch)
2017-05-02 15:59:35.407110: step 50070, loss = 0.60 (1631.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:59:36.194218: step 50080, loss = 0.73 (1626.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:36.979401: step 50090, loss = 0.75 (1630.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:37.869814: step 50100, loss = 0.73 (1437.5 examples/sec; 0.089 sec/batch)
2017-05-02 15:59:38.558986: step 50110, loss = 0.81 (1857.3 examples/sec; 0.069 sec/batch)
2017-05-02 15:59:39.351206: step 50120, loss = 0.80 (1615.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:40.138518: step 50130, loss = 0.89 (1625.8 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:40.930962: step 50140, loss = 0.67 (1615.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:41.717841: step 50150, loss = 0.64 (1626.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:42.511542: step 50160, loss = 0.74 (1612.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:43.304157: step 50170, loss = 0.88 (1614.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:44.085185: step 50180, loss = 0.79 (1638.9 examples/sec; 0.078 sec/batch)
2017-05-02 15:59:44.876181: step 50190, loss = 0.70 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:45.757364: step 50200, loss = 0.65 (1452.6 examples/sec; 0.088 sec/batch)
2017-05-02 15:59:46.455627: step 50210, loss = 0.66 (1833.1 examples/sec; 0.070 sec/batch)
2017-05-02 15:59:47.252137: step 50220, loss = 0.83 (1607.0 examples/sec; 0.080 sec/batch)
2017-05-02 15:59:48.040944: step 50230, loss = 1.01 (1622.7 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:48.830816: step 50240, loss = 0.62 (1620.5 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:49.623298: step 50250, loss = 0.77 (1615.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:50.416848: step 50260, loss = 0.92 (1613.0 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:51.218132: step 50270, loss = 0.62 (1597.4 examples/sec; 0.080 sec/batch)
2017-05-02 15:59:51.992586: step 50280, loss = 0.84 (1652.8 examples/sec; 0.077 sec/batch)
2017-05-02 15:59:52.783741: step 50290, loss = 0.66 (1617.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:53.674641: step 50300, loss = 0.94 (1436.8 examples/sec; 0.089 sec/batch)
2017-05-02 15:59:54.379137: step 50310, loss = 0.67 (1816.9 examples/sec; 0.070 sec/batch)
2017-05-02 15:59:55.164796: step 50320, loss = 0.78 (1629.2 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:55.952790: step 50330, loss = 0.65 (1624.4 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:56.741495: step 50340, loss = 0.78 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:57.537459: step 50350, loss = 0.83 (1608.1 examples/sec; 0.080 sec/batch)
2017-05-02 15:59:58.324234: step 50360, loss = 0.84 (1626.9 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:59.114694: step 50370, loss = 0.92 (1619.3 examples/sec; 0.079 sec/batch)
2017-05-02 15:59:59.894747: step 50380, loss = 0.78 (1640.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:00:00.686645: step 50390, loss = 0.81 (1616.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:01.590123: step 50400, loss = 0.84 (1416.7 examples/sec; 0.090 sec/batch)
2017-05-02 16:00:02.269173: step 50410, loss = 0.70 (1885.0 examples/sec; 0.068 sec/batch)
2017-05-02 16:00:03.057586: step 50420, loss = 0.85 (1623.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:03.837148: step 50430, loss = 0.80 (1641.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:00:04.624590: step 50440, loss = 0.70 (1625.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:05.416451: step 50450, loss = 0.78 (1616.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:06.205787: step 50460, loss = 0.72 (1621.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:06.995821: step 50470, loss = 0.71 (1620.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:07.783005: step 50480, loss = 0.81 (1626.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:08.570955: step 50490, loss = 0.83 (1624.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:09.457021: step 50500, loss = 0.84 (1444.6 examples/sec; 0.089 sec/batch)
2017-05-02 16:00:10.153014: step 50510, loss = 0.71 (1839.1 examples/sec; 0.070 sec/batch)
2017-05-02 16:00:10.942272: step 50520, loss = 0.91 (1621.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:11.724778: step 50530, loss = 0.87 (1635.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:00:12.522461: step 50540, loss = 0.74 (1604.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:00:13.314774: step 50550, loss = 0.84 (1615.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:14.099798: step 50560, loss = 0.54 (1630.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:14.890206: step 50570, loss = 0.67 (1619.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:15.673224: step 50580, loss = 0.63 (1634.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:00:16.479342: step 50590, loss = 0.58 (1587.9 examples/sec; 0.081 sec/batch)
2017-05-02 16:00:17.369412: step 50600, loss = 0.83 (1438.1 examples/sec; 0.089 sec/batch)
2017-05-02 16:00:18.071600: step 50610, loss = 0.76 (1822.9 examples/sec; 0.070 sec/batch)
2017-05-02 16:00:18.875982: step 50620, loss = 0.62 (1591.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:00:19.662448: step 50630, loss = 0.79 (1627.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:20.460100: step 50640, loss = 0.78 (1604.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:00:21.249565: step 50650, loss = 0.80 (1621.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:22.043110: step 50660, loss = 0.79 (1613.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:22.840109: step 50670, loss = 0.83 (1606.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:00:23.622452: step 50680, loss = 0.77 (1636.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:00:24.414863: step 50690, loss = 0.72 (1615.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:25.306867: step 50700, loss = 0.73 (1435.0 examples/sec; 0.089 sec/batch)
2017-05-02 16:00:26.004797: step 50710, loss = 0.78 (1834.0 examples/sec; 0.070 sec/batch)
2017-05-02 16:00:26.796542: step 50720, loss = 0.79 (1616.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:27.579339: step 50730, loss = 0.67 (1635.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:00:28.380459: step 50740, loss = 0.60 (1597.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:00:29.169354: step 50750, loss = 0.69 (1622.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:29.958872: step 50760, loss = 0.67 (1621.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:30.748852: step 50770, loss = 0.83 (1620.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:31.532055: step 50780, loss = 0.68 (1634.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:00:32.315542: step 50790, loss = 0.70 (1633.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:00:33.202831: step 50800, loss = 0.70 (1442.6 examples/sec; 0.089 sec/batch)
2017-05-02 16:00:33.898225: step 50810, loss = 0.65 (1840.7 examples/sec; 0.070 sec/batch)
2017-05-02 16:00:34.690989: step 50820, loss = 0.77 (1614.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:35.480828: step 50830, loss = 0.58 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:36.267908: step 50840, loss = 0.82 (1626.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:37.057628: step 50850, loss = 0.60 (1620.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:37.853315: step 50860, loss = 0.67 (1608.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:00:38.648619: step 50870, loss = 0.69 (1609.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:00:39.437133: step 50880, loss = 0.79 (1623.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:40.225364: step 50890, loss = 0.65 (1623.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:41.121956: step 50900, loss = 0.73 (1427.6 examples/sec; 0.090 sec/batch)
2017-05-02 16:00:41.810053: step 50910, loss = 0.65 (1860.2 examples/sec; 0.069 sec/batch)
2017-05-02 16:00:42.598760: step 50920, loss = 0.68 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:43.390785: step 50930, loss = 0.73 (1616.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:44.180456: step 50940, loss = 0.68 (1620.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:44.966738: step 50950, loss = 0.72 (1627.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:45.759227: step 50960, loss = 0.83 (1615.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:46.547197: step 50970, loss = 0.76 (1624.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:47.334595: step 50980, loss = 0.74 (1625.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:48.118977: step 50990, loss = 0.62 (1631.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:00:49.009554: step 51000, loss = 0.86 (1437.3 examples/sec; 0.089 sec/batch)
2017-05-02 16:00:49.709431: step 51010, loss = 0.70 (1828.9 examples/sec; 0.070 sec/batch)
2017-05-02 16:00:50.503805: step 51020, loss = 0.88 (1611.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:51.299036: step 51030, loss = 0.83 (1609.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:00:52.081185: step 51040, loss = 0.94 (1636.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:00:52.873554: step 51050, loss = 0.92 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:53.677307: step 51060, loss = 0.86 (1592.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:00:54.464662: step 51070, loss = 0.84 (1625.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:00:55.260570: step 51080, loss = 0.85 (1608.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:00:56.036645: step 51090, loss = 0.66 (1649.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:00:56.929028: step 51100, loss = 0.66 (1434.4 examples/sec; 0.089 sec/batch)
2017-05-02 16:00:57.631109: step 51110, loss = 0.87 (1823.2 examples/sec; 0.070 sec/batch)
2017-05-02 16:00:58.429022: step 51120, loss = 0.78 (1604.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:00:59.232067: step 51130, loss = 0.84 (1593.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:01:00.016145: step 51140, loss = 0.67 (1632.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:01:00.810904: step 51150, loss = 0.75 (1610.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:01:01.613224: step 51160, loss = 0.74 (1595.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:01:02.403201: step 51170, loss = 0.84 (1620.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:01:03.188097: step 51180, loss = 0.64 (1630.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:01:03.975021: step 51190, loss = 0.78 (1626.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:01:04.860786: step 51200, loss = 0.75 (1445.1 examples/sec; 0.089 sec/batch)
2017-05-02 16:01:05.562531: step 51210, loss = 0.86 (1824.0 examples/sec; 0.070 sec/batch)
2017-05-02 16:01:06.358568: step 51220, loss = 0.72 (1608.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:01:07.153305: step 51230, loss = 0.73 (1610.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:01:07.929314: step 51240, loss = 0.81 (1649.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:01:08.726623: step 51250, loss = 0.78 (1605.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:01:09.521769: step 51260, loss = 0.69 (1609.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:01:10.315820: step 51270, loss = 0.73 (1612.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:01:11.111411: step 51280, loss = 0.73 (1608.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:01:11.900554: step 51290, loss = 0.73 (1622.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:01:12.792861: step 51300, loss = 0.70 (1434.5 examples/sec; 0.089 sec/batch)
2017-05-02 16:01:13.484739: step 51310, loss = 0.60 (1850.0 examples/sec; 0.069 sec/batch)
2017-05-02 16:01:14.279198: step 51320, loss = 0.64 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:01:15.074296: step 51330, loss = 0.81 (1609.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:01:15.855629: step 51340, loss = 0.85 (1638.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:01:16.647133: step 51350, loss = 0.65 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:01:17.436845: step 51360, loss = 0.72 (1620.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:01:18.238107: step 51370, loss = 0.77 (1597.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:01:19.031704: step 51380, loss = 0.70 (1612.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:01:19.818641: step 51390, loss = 0.78 (1626.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:01:20.718730: step 51400, loss = 0.82 (1422.1 examples/sec; 0.090 sec/batch)
2017-05-02 16:01:21.415369: step 51410, loss = 0.74 (1837.4 examples/sec; 0.070 sec/batch)
2017-05-02 16:01:22.210385: step 51420, loss = 0.70 (1610.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:01:23.003292: step 51430, loss = 0.71 (1614.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:01:23.785625: step 51440, loss = 0.69 (1636.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:01:24.580939: step 51450, loss = 0.78 (1609.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:01:25.377674: step 51460, loss = 0.74 (1606.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:01:26.170403: step 51470, loss = 0.67 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:01:26.966474: step 51480, loss = 0.77 (1607.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:01:27.753607: step 51490, loss = 0.70 (1626.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:01:28.650426: step 51500, loss = 0.56 (1427.3 examples/sec; 0.090 sec/batch)
2017-05-02 16:01:29.344743: step 51510, loss = 0.75 (1843.5 examples/sec; 0.069 sec/batch)
2017-05-02 16:01:30.143039: step 51520, loss = 0.57 (1603.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:01:30.944406: step 51530, loss = 0.73 (1597.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:01:31.727028: step 51540, loss = 0.74 (1635.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:01:32.519763: step 51550, loss = 0.66 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:01:33.317470: step 51560, loss = 0.77 (1604.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:01:34.113842: step 51570, loss = 0.66 (1607.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:01:34.902451: step 51580, loss = 0.80 (1623.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:01:35.692205: step 51590, loss = 0.79 (1620.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:01:36.595574: step 51600, loss = 0.75 (1416.9 examples/sec; 0.090 sec/batch)
2017-05-02 16:01:37.295463: step 51610, loss = 0.91 (1828.9 examples/sec; 0.070 sec/batch)
2017-05-02 16:01:38.099083: step 51620, loss = 0.74 (1592.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:01:38.891735: step 51630, loss = 0.90 (1614.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:01:39.678674: step 51640, loss = 0.88 (1626.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:01:40.472939: step 51650, loss = 0.69 (1611.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:01:41.272305: step 51660, loss = 0.72 (1601.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:01:42.067343: step 51670, loss = 0.69 (1610.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:01:42.865626: step 51680, loss = 0.74 (1603.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:01:43.640925: step 51690, loss = 0.81 (1651.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:01:44.539089: step 51700, loss = 0.84 (1425.1 examples/sec; 0.090 sec/batch)
2017-05-02 16:01:45.237935: step 51710, loss = 0.67 (1831.6 examples/sec; 0.070 sec/batch)
2017-05-02 16:01:46.032053: step 51720, loss = 0.74 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:01:46.824551: step 51730, loss = 0.54 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:01:47.607597: step 51740, loss = 0.71 (1634.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:01:48.403916: step 51750, loss = 1.00 (1607.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:01:49.197014: step 51760, loss = 0.77 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:01:49.982183: step 51770, loss = 0.78 (1630.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:01:50.767376: step 51780, loss = 0.73 (1630.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:01:51.549523: step 51790, loss = 0.64 (1636.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:01:52.438051: step 51800, loss = 0.79 (1440.6 examples/sec; 0.089 sec/batch)
2017-05-02 16:01:53.134128: step 51810, loss = 0.64 (1838.9 examples/sec; 0.070 sec/batch)
2017-05-02 16:01:53.925960: step 51820, loss = 0.56 (1616.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:01:54.717269: step 51830, loss = 0.74 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:01:55.500859: step 51840, loss = 0.84 (1633.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:01:56.289776: step 51850, loss = 0.80 (1622.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:01:57.078425: step 51860, loss = 0.89 (1623.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:01:57.874548: step 51870, loss = 0.62 (1607.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:01:58.664392: step 51880, loss = 0.56 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:01:59.446353: step 51890, loss = 0.70 (1636.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:02:00.346041: step 51900, loss = 0.69 (1422.7 examples/sec; 0.090 sec/batch)
2017-05-02 16:02:01.010539: step 51910, loss = 0.61 (1926.3 examples/sec; 0.066 sec/batch)
2017-05-02 16:02:01.809464: step 51920, loss = 0.73 (1602.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:02:02.599741: step 51930, loss = 0.67 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:03.391677: step 51940, loss = 0.67 (1616.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:04.177496: step 51950, loss = 0.82 (1628.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:04.962782: step 51960, loss = 0.71 (1630.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:05.749444: step 51970, loss = 0.79 (1627.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:06.536828: step 51980, loss = 0.69 (1625.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:07.328317: step 51990, loss = 0.76 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:08.214639: step 52000, loss = 0.74 (1444.2 examples/sec; 0.089 sec/batch)
2017-05-02 16:02:08.905281: step 52010, loss = 0.66 (1853.4 examples/sec; 0.069 sec/batch)
2017-05-02 16:02:09.690246: step 52020, loss = 0.74 (1630.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:02:10.483870: step 52030, loss = 0.77 (1612.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:11.276964: step 52040, loss = 0.86 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:12.057335: step 52050, loss = 0.70 (1640.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:02:12.853638: step 52060, loss = 0.77 (1607.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:02:13.649999: step 52070, loss = 0.76 (1607.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:02:14.437738: step 52080, loss = 0.82 (1624.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:15.227541: step 52090, loss = 0.87 (1620.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:16.099238: step 52100, loss = 0.64 (1468.4 examples/sec; 0.087 sec/batch)
2017-05-02 16:02:16.799469: step 52110, loss = 0.78 (1828.0 examples/sec; 0.070 sec/batch)
2017-05-02 16:02:17.592635: step 52120, loss = 0.74 (1613.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:18.387589: step 52130, loss = 0.85 (1610.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:19.184574: step 52140, loss = 0.74 (1606.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:02:19.962803: step 52150, loss = 0.73 (1644.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:02:20.762771: step 52160, loss = 0.84 (1600.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:02:21.552987: step 52170, loss = 0.81 (1619.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:22.344494: step 52180, loss = 0.77 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:23.135669: step 52190, loss = 0.69 (1617.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:24.012167: step 52200, loss = 0.76 (1460.4 examples/sec; 0.088 sec/batch)
2017-05-02 16:02:24.711956: step 52210, loss = 1.03 (1829.1 examples/sec; 0.070 sec/batch)
2017-05-02 16:02:25.514087: step 52220, loss = 0.65 (1595.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:02:26.303003: step 52230, loss = 0.73 (1622.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:27.094813: step 52240, loss = 0.74 (1616.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:27.878480: step 52250, loss = 0.71 (1633.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:02:28.669962: step 52260, loss = 0.72 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:29.458634: step 52270, loss = 0.65 (1623.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:30.253504: step 52280, loss = 0.88 (1610.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:31.045295: step 52290, loss = 0.72 (1616.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:31.934396: step 52300, loss = 0.85 (1439.7 examples/sec; 0.089 sec/batch)
2017-05-02 16:02:32.632626: step 52310, loss = 0.83 (1833.2 examples/sec; 0.070 sec/batch)
2017-05-02 16:02:33.418831: step 52320, loss = 0.75 (1628.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:34.213268: step 52330, loss = 0.72 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:35.005437: step 52340, loss = 0.93 (1615.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:35.781257: step 52350, loss = 0.72 (1649.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:02:36.574377: step 52360, loss = 0.76 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:37.367222: step 52370, loss = 0.76 (1614.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:38.162745: step 52380, loss = 0.84 (1609.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:02:38.951120: step 52390, loss = 0.61 (1623.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:39.825665: step 52400, loss = 0.65 (1463.6 examples/sec; 0.087 sec/batch)
2017-05-02 16:02:40.519419: step 52410, loss = 0.82 (1845.1 examples/sec; 0.069 sec/batch)
2017-05-02 16:02:41.317732: step 52420, loss = 0.78 (1603.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:02:42.102338: step 52430, loss = 0.76 (1631.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:02:42.897057: step 52440, loss = 0.63 (1610.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:43.677827: step 52450, loss = 0.80 (1639.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:02:44.466101: step 52460, loss = 0.74 (1623.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:45.252449: step 52470, loss = 0.78 (1627.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:46.043195: step 52480, loss = 0.58 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:46.836117: step 52490, loss = 1.00 (1614.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:47.713711: step 52500, loss = 0.67 (1458.5 examples/sec; 0.088 sec/batch)
2017-05-02 16:02:48.410806: step 52510, loss = 0.71 (1836.2 examples/sec; 0.070 sec/batch)
2017-05-02 16:02:49.204446: step 52520, loss = 0.68 (1612.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:49.993743: step 52530, loss = 0.81 (1621.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:50.783752: step 52540, loss = 0.62 (1620.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:51.566160: step 52550, loss = 0.59 (1636.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:02:52.362840: step 52560, loss = 0.80 (1606.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:02:53.153056: step 52570, loss = 0.84 (1619.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:53.938243: step 52580, loss = 0.65 (1630.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:54.728633: step 52590, loss = 0.76 (1619.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:55.615278: step 52600, loss = 0.80 (1443.6 examples/sec; 0.089 sec/batch)
2017-05-02 16:02:56.314581: step 52610, loss = 0.78 (1830.4 examples/sec; 0.070 sec/batch)
2017-05-02 16:02:57.108847: step 52620, loss = 1.01 (1611.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:57.904786: step 52630, loss = 0.89 (1608.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:02:58.692665: step 52640, loss = 0.74 (1624.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:02:59.480549: step 52650, loss = 0.68 (1624.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:00.269432: step 52660, loss = 0.82 (1622.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:01.058642: step 52670, loss = 0.94 (1621.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:01.854430: step 52680, loss = 0.81 (1608.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:03:02.646165: step 52690, loss = 0.89 (1616.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:03.537941: step 52700, loss = 0.67 (1435.3 examples/sec; 0.089 sec/batch)
2017-05-02 16:03:04.226387: step 52710, loss = 0.91 (1859.3 examples/sec; 0.069 sec/batch)
2017-05-02 16:03:05.022504: step 52720, loss = 0.79 (1607.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:03:05.818095: step 52730, loss = 0.82 (1608.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:03:06.609157: step 52740, loss = 0.86 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:07.401088: step 52750, loss = 0.82 (1616.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:08.188177: step 52760, loss = 0.82 (1626.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:08.977116: step 52770, loss = 0.78 (1622.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:09.771374: step 52780, loss = 0.79 (1611.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:10.566033: step 52790, loss = 0.60 (1610.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:11.464707: step 52800, loss = 0.73 (1424.3 examples/sec; 0.090 sec/batch)
2017-05-02 16:03:12.140340: step 52810, loss = 0.92 (1894.5 examples/sec; 0.068 sec/batch)
2017-05-02 16:03:12.929508: step 52820, loss = 0.67 (1622.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:13.722373: step 52830, loss = 0.63 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:14.520427: step 52840, loss = 0.82 (1603.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:03:15.316207: step 52850, loss = 0.62 (1608.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:03:16.098495: step 52860, loss = 0.60 (1636.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:03:16.895437: step 52870, loss = 0.70 (1606.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:03:17.690993: step 52880, loss = 0.73 (1608.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:03:18.476623: step 52890, loss = 0.73 (1629.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:19.385436: step 52900, loss = 0.76 (1408.4 examples/sec; 0.091 sec/batch)
2017-05-02 16:03:20.064701: step 52910, loss = 0.67 (1884.4 examples/sec; 0.068 sec/batch)
2017-05-02 16:03:20.865271: step 52920, loss = 0.80 (1598.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:03:21.659371: step 52930, loss = 0.69 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:22.454558: step 52940, loss = 0.62 (1609.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:03:23.245860: step 52950, loss = 0.65 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:24.025560: step 52960, loss = 0.70 (1641.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:03:24.818386: step 52970, loss = 0.66 (1614.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:25.613055: step 52980, loss = 1.04 (1610.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:26.410021: step 52990, loss = 0.67 (1606.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:03:27.304374: step 53000, loss = 0.75 (1431.2 examples/sec; 0.089 sec/batch)
2017-05-02 16:03:27.983662: step 53010, loss = 0.61 (1884.3 examples/sec; 0.068 sec/batch)
2017-05-02 16:03:28.774605: step 53020, loss = 0.75 (1618.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:29.561838: step 53030, loss = 0.71 (1626.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:30.384643: step 53040, loss = 0.77 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-02 16:03:31.172731: step 53050, loss = 0.73 (1624.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:31.952043: step 53060, loss = 0.66 (1642.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:03:32.752437: step 53070, loss = 0.86 (1599.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:03:33.546159: step 53080, loss = 0.75 (1612.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:34.332083: step 53090, loss = 0.78 (1628.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:35.229711: step 53100, loss = 0.88 (1426.0 examples/sec; 0.090 sec/batch)
2017-05-02 16:03:35.911132: step 53110, loss = 0.69 (1878.4 examples/sec; 0.068 sec/batch)
2017-05-02 16:03:36.708104: step 53120, loss = 0.74 (1606.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:03:37.501025: step 53130, loss = 0.81 (1614.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:38.293428: step 53140, loss = 0.80 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:39.085949: step 53150, loss = 0.82 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:39.863707: step 53160, loss = 0.75 (1645.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:03:40.647839: step 53170, loss = 0.79 (1632.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:03:41.443577: step 53180, loss = 0.76 (1608.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:03:42.237657: step 53190, loss = 0.78 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:43.128349: step 53200, loss = 0.65 (1437.1 examples/sec; 0.089 sec/batch)
2017-05-02 16:03:43.815728: step 53210, loss = 0.59 (1862.1 examples/sec; 0.069 sec/batch)
2017-05-02 16:03:44.604687: step 53220, loss = 0.79 (1622.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:45.399910: step 53230, loss = 0.70 (1609.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:03:46.188099: step 53240, loss = 0.70 (1624.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:46.981930: step 53250, loss = 0.76 (1612.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:47.766696: step 53260, loss = 0.62 (1631.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:03:48.559421: step 53270, loss = 0.73 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:49.349439: step 53280, loss = 0.84 (1620.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:50.140132: step 53290, loss = 0.68 (1618.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:51.040774: step 53300, loss = 0.79 (1421.2 examples/sec; 0.090 sec/batch)
2017-05-02 16:03:51.727903: step 53310, loss = 0.68 (1862.8 examples/sec; 0.069 sec/batch)
2017-05-02 16:03:52.512866: step 53320, loss = 0.77 (1630.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:03:53.307544: step 53330, loss = 0.66 (1610.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:54.101013: step 53340, loss = 0.82 (1613.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:54.895288: step 53350, loss = 0.81 (1611.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:55.686515: step 53360, loss = 0.63 (1617.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:56.479298: step 53370, loss = 0.62 (1614.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:57.268206: step 53380, loss = 0.68 (1622.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:58.054816: step 53390, loss = 0.83 (1627.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:03:58.942035: step 53400, loss = 0.80 (1442.7 examples/sec; 0.089 sec/batch)
2017-05-02 16:03:59.623743: step 53410, loss = 0.71 (1877.6 examples/sec; 0.068 sec/batch)
2017-05-02 16:04:00.417946: step 53420, loss = 0.58 (1611.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:01.207323: step 53430, loss = 0.73 (1621.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:01.989802: step 53440, loss = 0.74 (1635.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:04:02.788212: step 53450, loss = 0.62 (1603.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:04:03.574675: step 53460, loss = 0.84 (1627.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:04.359024: step 53470, loss = 0.63 (1631.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:04:05.145445: step 53480, loss = 0.75 (1627.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:05.943590: step 53490, loss = 0.83 (1603.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:04:06.825539: step 53500, loss = 0.75 (1451.3 examples/sec; 0.088 sec/batch)
2017-05-02 16:04:07.524752: step 53510, loss = 0.72 (1830.6 examples/sec; 0.070 sec/batch)
2017-05-02 16:04:08.308010: step 53520, loss = 0.69 (1634.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:04:09.099285: step 53530, loss = 0.67 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:09.887068: step 53540, loss = 0.66 (1624.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:10.678126: step 53550, loss = 0.86 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:11.470840: step 53560, loss = 0.56 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:12.256181: step 53570, loss = 0.81 (1629.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:13.042617: step 53580, loss = 0.68 (1627.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:13.834377: step 53590, loss = 0.70 (1616.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:14.721779: step 53600, loss = 0.82 (1442.4 examples/sec; 0.089 sec/batch)
2017-05-02 16:04:15.419838: step 53610, loss = 0.77 (1833.6 examples/sec; 0.070 sec/batch)
2017-05-02 16:04:16.209131: step 53620, loss = 0.94 (1621.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:16.997097: step 53630, loss = 0.63 (1624.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:17.786592: step 53640, loss = 0.72 (1621.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:18.573008: step 53650, loss = 0.74 (1627.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:19.356323: step 53660, loss = 0.79 (1634.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:04:20.141651: step 53670, loss = 0.85 (1629.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:20.932549: step 53680, loss = 0.76 (1618.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:21.722116: step 53690, loss = 0.72 (1621.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:22.604208: step 53700, loss = 0.78 (1451.1 examples/sec; 0.088 sec/batch)
2017-05-02 16:04:23.305286: step 53710, loss = 0.87 (1825.8 examples/sec; 0.070 sec/batch)
2017-05-02 16:04:24.085997: step 53720, loss = 0.91 (1639.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:04:24.878156: step 53730, loss = 0.77 (1615.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:25.674131: step 53740, loss = 0.68 (1608.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:04:26.458965: step 53750, loss = 0.82 (1630.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:04:27.253458: step 53760, loss = 0.73 (1611.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:28.026793: step 53770, loss = 0.74 (1655.2 examples/sec; 0.077 sec/batch)
2017-05-02 16:04:28.817156: step 53780, loss = 0.73 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:29.614798: step 53790, loss = 0.72 (1604.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:04:30.511760: step 53800, loss = 0.78 (1427.0 examples/sec; 0.090 sec/batch)
2017-05-02 16:04:31.201285: step 53810, loss = 0.77 (1856.4 examples/sec; 0.069 sec/batch)
2017-05-02 16:04:31.978478: step 53820, loss = 0.55 (1646.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:04:32.774788: step 53830, loss = 0.76 (1607.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:04:33.566026: step 53840, loss = 0.68 (1617.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:34.362224: step 53850, loss = 0.80 (1607.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:04:35.157438: step 53860, loss = 0.91 (1609.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:04:35.943329: step 53870, loss = 0.86 (1628.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:36.733134: step 53880, loss = 0.72 (1620.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:37.533642: step 53890, loss = 0.75 (1598.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:04:38.434730: step 53900, loss = 0.85 (1420.5 examples/sec; 0.090 sec/batch)
2017-05-02 16:04:39.120302: step 53910, loss = 0.88 (1867.1 examples/sec; 0.069 sec/batch)
2017-05-02 16:04:39.902215: step 53920, loss = 1.04 (1637.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:04:40.700201: step 53930, loss = 0.73 (1604.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:04:41.493942: step 53940, loss = 0.66 (1612.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:42.293926: step 53950, loss = 0.70 (1600.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:04:43.083706: step 53960, loss = 0.76 (1620.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:43.866264: step 53970, loss = 0.58 (1635.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:04:44.650335: step 53980, loss = 0.62 (1632.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:04:45.444066: step 53990, loss = 0.70 (1612.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:46.339555: step 54000, loss = 0.78 (1429.4 examples/sec; 0.090 sec/batch)
2017-05-02 16:04:47.040578: step 54010, loss = 0.75 (1825.9 examples/sec; 0.070 sec/batch)
2017-05-02 16:04:47.818704: step 54020, loss = 0.78 (1645.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:04:48.611312: step 54030, loss = 0.74 (1614.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:49.403957: step 54040, loss = 0.66 (1614.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:50.189163: step 54050, loss = 0.63 (1630.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:50.976727: step 54060, loss = 0.72 (1625.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:51.762011: step 54070, loss = 0.75 (1630.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:52.548999: step 54080, loss = 0.78 (1626.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:53.347086: step 54090, loss = 0.68 (1603.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:04:54.234675: step 54100, loss = 0.76 (1442.1 examples/sec; 0.089 sec/batch)
2017-05-02 16:04:54.934349: step 54110, loss = 0.97 (1829.4 examples/sec; 0.070 sec/batch)
2017-05-02 16:04:55.714190: step 54120, loss = 0.73 (1641.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:04:56.504812: step 54130, loss = 0.69 (1619.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:57.299923: step 54140, loss = 0.78 (1609.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:04:58.091572: step 54150, loss = 0.63 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:58.885329: step 54160, loss = 0.68 (1612.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:04:59.664290: step 54170, loss = 0.65 (1643.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:05:00.460174: step 54180, loss = 0.60 (1608.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:05:01.257360: step 54190, loss = 0.71 (1605.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:05:02.147218: step 54200, loss = 0.72 (1438.4 examples/sec; 0.089 sec/batch)
2017-05-02 16:05:02.841103: step 54210, loss = 0.76 (1844.7 examples/sec; 0.069 sec/batch)
2017-05-02 16:05:03.629913: step 54220, loss = 0.60 (1622.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:05:04.413151: step 54230, loss = 0.70 (1634.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:05:05.211931: step 54240, loss = 0.78 (1602.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:05:06.002994: step 54250, loss = 0.81 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:05:06.791027: step 54260, loss = 0.66 (1624.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:05:07.573146: step 54270, loss = 0.79 (1636.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:05:08.357789: step 54280, loss = 0.64 (1631.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:05:09.154337: step 54290, loss = 0.79 (1606.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:05:10.044419: step 54300, loss = 0.72 (1438.1 examples/sec; 0.089 sec/batch)
2017-05-02 16:05:10.747395: step 54310, loss = 0.78 (1820.8 examples/sec; 0.070 sec/batch)
2017-05-02 16:05:11.530845: step 54320, loss = 0.75 (1633.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:05:12.321901: step 54330, loss = 0.94 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:05:13.109331: step 54340, loss = 0.79 (1625.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:05:13.897144: step 54350, loss = 0.70 (1624.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:05:14.682783: step 54360, loss = 0.74 (1629.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:05:15.471574: step 54370, loss = 0.70 (1622.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:05:16.250847: step 54380, loss = 0.67 (1642.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:05:17.042290: step 54390, loss = 0.69 (1617.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:05:17.941309: step 54400, loss = 0.73 (1423.8 examples/sec; 0.090 sec/batch)
2017-05-02 16:05:18.631029: step 54410, loss = 0.79 (1855.8 examples/sec; 0.069 sec/batch)
2017-05-02 16:05:19.414654: step 54420, loss = 0.65 (1633.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:05:20.200204: step 54430, loss = 0.61 (1629.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:05:20.991843: step 54440, loss = 0.80 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:05:21.779332: step 54450, loss = 0.76 (1625.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:05:22.579157: step 54460, loss = 0.79 (1600.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:05:23.365697: step 54470, loss = 0.72 (1627.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:05:24.150186: step 54480, loss = 0.71 (1631.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:05:24.942677: step 54490, loss = 0.74 (1615.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:05:25.837387: step 54500, loss = 0.83 (1430.6 examples/sec; 0.089 sec/batch)
2017-05-02 16:05:26.532970: step 54510, loss = 0.64 (1840.2 examples/sec; 0.070 sec/batch)
2017-05-02 16:05:27.319797: step 54520, loss = 0.94 (1626.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:05:28.104483: step 54530, loss = 0.73 (1631.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:05:28.908303: step 54540, loss = 0.68 (1592.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:05:29.699813: step 54550, loss = 0.78 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:05:30.479544: step 54560, loss = 0.61 (1641.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:05:31.266073: step 54570, loss = 0.61 (1627.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:05:32.043282: step 54580, loss = 0.66 (1646.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:05:32.836036: step 54590, loss = 0.86 (1614.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:05:33.735762: step 54600, loss = 0.74 (1422.7 examples/sec; 0.090 sec/batch)
2017-05-02 16:05:34.426290: step 54610, loss = 0.71 (1853.6 examples/sec; 0.069 sec/batch)
2017-05-02 16:05:35.214252: step 54620, loss = 0.81 (1624.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:05:35.999388: step 54630, loss = 0.69 (1630.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:05:36.795400: step 54640, loss = 0.84 (1608.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:05:37.589025: step 54650, loss = 0.55 (1612.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:05:38.391514: step 54660, loss = 0.60 (1595.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:05:39.176453: step 54670, loss = 0.86 (1630.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:05:39.955842: step 54680, loss = 0.82 (1642.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:05:40.751322: step 54690, loss = 0.91 (1609.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:05:41.633555: step 54700, loss = 0.66 (1450.9 examples/sec; 0.088 sec/batch)
2017-05-02 16:05:42.339122: step 54710, loss = 0.59 (1814.1 examples/sec; 0.071 sec/batch)
2017-05-02 16:05:43.133280: step 54720, loss = 0.69 (1611.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:05:43.911447: step 54730, loss = 0.82 (1644.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:05:44.701312: step 54740, loss = 0.71 (1620.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:05:45.496312: step 54750, loss = 0.88 (1610.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:05:46.281133: step 54760, loss = 0.82 (1631.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:05:47.074816: step 54770, loss = 0.60 (1612.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:05:47.850045: step 54780, loss = 0.76 (1651.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:05:48.638150: step 54790, loss = 0.61 (1624.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:05:49.538081: step 54800, loss = 0.92 (1422.3 examples/sec; 0.090 sec/batch)
2017-05-02 16:05:50.217408: step 54810, loss = 0.88 (1884.2 examples/sec; 0.068 sec/batch)
2017-05-02 16:05:51.009206: step 54820, loss = 0.60 (1616.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:05:51.799346: step 54830, loss = 0.82 (1620.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:05:52.598673: step 54840, loss = 0.74 (1601.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:05:53.391283: step 54850, loss = 0.72 (1614.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:05:54.183145: step 54860, loss = 0.79 (1616.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:05:54.969135: step 54870, loss = 0.73 (1628.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:05:55.748394: step 54880, loss = 0.74 (1642.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:05:56.541501: step 54890, loss = 0.81 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:05:57.432823: step 54900, loss = 0.83 (1436.1 examples/sec; 0.089 sec/batch)
2017-05-02 16:05:58.139961: step 54910, loss = 0.74 (1810.1 examples/sec; 0.071 sec/batch)
2017-05-02 16:05:58.928300: step 54920, loss = 0.82 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:05:59.715456: step 54930, loss = 0.73 (1626.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:00.514442: step 54940, loss = 0.89 (1602.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:06:01.307733: step 54950, loss = 0.69 (1613.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:02.099911: step 54960, loss = 0.73 (1615.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:02.896413: step 54970, loss = 0.93 (1607.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:06:03.674681: step 54980, loss = 0.65 (1644.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:06:04.464539: step 54990, loss = 0.71 (1620.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:05.357460: step 55000, loss = 0.68 (1433.5 examples/sec; 0.089 sec/batch)
2017-05-02 16:06:06.052069: step 55010, loss = 0.73 (1842.8 examples/sec; 0.069 sec/batch)
2017-05-02 16:06:06.842718: step 55020, loss = 0.68 (1618.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:07.626156: step 55030, loss = 0.80 (1633.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:06:08.419990: step 55040, loss = 0.85 (1612.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:09.209725: step 55050, loss = 0.72 (1620.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:10.003422: step 55060, loss = 0.69 (1612.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:10.787860: step 55070, loss = 0.71 (1631.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:06:11.575484: step 55080, loss = 0.72 (1625.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:12.365853: step 55090, loss = 0.71 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:13.259406: step 55100, loss = 0.94 (1432.5 examples/sec; 0.089 sec/batch)
2017-05-02 16:06:13.957363: step 55110, loss = 0.71 (1833.9 examples/sec; 0.070 sec/batch)
2017-05-02 16:06:14.749337: step 55120, loss = 0.83 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:15.530773: step 55130, loss = 0.70 (1638.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:06:16.320050: step 55140, loss = 0.71 (1621.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:17.111333: step 55150, loss = 0.54 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:17.905903: step 55160, loss = 0.83 (1610.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:18.702370: step 55170, loss = 0.74 (1607.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:06:19.486666: step 55180, loss = 0.71 (1632.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:06:20.276160: step 55190, loss = 0.66 (1621.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:21.171199: step 55200, loss = 0.70 (1430.1 examples/sec; 0.090 sec/batch)
2017-05-02 16:06:21.871174: step 55210, loss = 0.76 (1828.6 examples/sec; 0.070 sec/batch)
2017-05-02 16:06:22.664251: step 55220, loss = 0.81 (1614.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:23.453813: step 55230, loss = 0.69 (1621.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:24.240655: step 55240, loss = 0.74 (1626.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:25.029054: step 55250, loss = 0.83 (1623.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:25.817121: step 55260, loss = 0.72 (1624.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:26.617930: step 55270, loss = 0.94 (1598.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:06:27.412917: step 55280, loss = 0.72 (1610.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:28.190911: step 55290, loss = 0.83 (1645.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:06:29.088750: step 55300, loss = 0.77 (1425.6 examples/sec; 0.090 sec/batch)
2017-05-02 16:06:29.774259: step 55310, loss = 0.76 (1867.2 examples/sec; 0.069 sec/batch)
2017-05-02 16:06:30.564497: step 55320, loss = 0.82 (1619.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:31.347966: step 55330, loss = 0.70 (1633.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:06:32.130666: step 55340, loss = 0.62 (1635.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:06:32.923053: step 55350, loss = 0.69 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:33.718368: step 55360, loss = 0.78 (1609.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:06:34.509087: step 55370, loss = 0.81 (1618.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:35.297574: step 55380, loss = 0.74 (1623.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:36.083699: step 55390, loss = 0.79 (1628.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:36.980134: step 55400, loss = 0.77 (1427.9 examples/sec; 0.090 sec/batch)
2017-05-02 16:06:37.674211: step 55410, loss = 0.68 (1844.2 examples/sec; 0.069 sec/batch)
2017-05-02 16:06:38.466478: step 55420, loss = 0.73 (1615.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:39.264540: step 55430, loss = 0.63 (1603.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:06:40.045201: step 55440, loss = 0.72 (1639.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:06:40.837489: step 55450, loss = 0.73 (1615.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:41.624671: step 55460, loss = 0.90 (1626.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:42.417581: step 55470, loss = 0.65 (1614.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:43.208295: step 55480, loss = 0.74 (1618.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:43.984262: step 55490, loss = 0.62 (1649.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:06:44.875325: step 55500, loss = 0.70 (1436.5 examples/sec; 0.089 sec/batch)
2017-05-02 16:06:45.572107: step 55510, loss = 0.74 (1837.0 examples/sec; 0.070 sec/batch)
2017-05-02 16:06:46.363933: step 55520, loss = 0.85 (1616.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:47.153154: step 55530, loss = 0.84 (1621.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:47.937622: step 55540, loss = 0.84 (1631.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:06:48.723657: step 55550, loss = 0.82 (1628.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:49.511526: step 55560, loss = 0.61 (1624.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:50.313395: step 55570, loss = 0.82 (1596.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:06:51.108609: step 55580, loss = 0.70 (1609.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:06:51.888186: step 55590, loss = 0.63 (1641.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:06:52.779995: step 55600, loss = 0.70 (1435.3 examples/sec; 0.089 sec/batch)
2017-05-02 16:06:53.478198: step 55610, loss = 0.79 (1833.3 examples/sec; 0.070 sec/batch)
2017-05-02 16:06:54.266816: step 55620, loss = 0.66 (1623.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:55.053216: step 55630, loss = 0.76 (1627.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:55.835846: step 55640, loss = 0.70 (1635.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:06:56.631534: step 55650, loss = 0.84 (1608.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:06:57.422293: step 55660, loss = 0.73 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:58.225790: step 55670, loss = 0.72 (1593.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:06:59.011860: step 55680, loss = 0.68 (1628.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:06:59.792318: step 55690, loss = 0.72 (1640.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:07:00.675321: step 55700, loss = 0.74 (1449.6 examples/sec; 0.088 sec/batch)
2017-05-02 16:07:01.371430: step 55710, loss = 0.66 (1838.8 examples/sec; 0.070 sec/batch)
2017-05-02 16:07:02.166015: step 55720, loss = 0.78 (1610.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:02.948353: step 55730, loss = 0.89 (1636.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:07:03.727638: step 55740, loss = 0.77 (1642.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:07:04.516371: step 55750, loss = 0.61 (1622.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:05.315014: step 55760, loss = 0.77 (1602.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:07:06.116068: step 55770, loss = 0.69 (1597.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:07:06.905095: step 55780, loss = 0.63 (1622.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:07.692943: step 55790, loss = 0.70 (1624.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:08.589692: step 55800, loss = 0.78 (1427.4 examples/sec; 0.090 sec/batch)
2017-05-02 16:07:09.280208: step 55810, loss = 0.80 (1853.7 examples/sec; 0.069 sec/batch)
2017-05-02 16:07:10.075933: step 55820, loss = 0.67 (1608.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:07:10.866306: step 55830, loss = 0.80 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:11.645819: step 55840, loss = 0.80 (1642.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:07:12.438390: step 55850, loss = 0.61 (1615.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:13.223515: step 55860, loss = 0.71 (1630.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:14.014872: step 55870, loss = 0.71 (1617.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:14.807163: step 55880, loss = 0.90 (1615.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:15.594314: step 55890, loss = 0.67 (1626.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:16.476763: step 55900, loss = 0.82 (1450.5 examples/sec; 0.088 sec/batch)
2017-05-02 16:07:17.176121: step 55910, loss = 0.82 (1830.3 examples/sec; 0.070 sec/batch)
2017-05-02 16:07:17.963725: step 55920, loss = 0.75 (1625.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:18.757941: step 55930, loss = 0.59 (1611.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:19.539804: step 55940, loss = 0.76 (1637.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:07:20.329348: step 55950, loss = 0.77 (1621.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:21.117476: step 55960, loss = 0.82 (1624.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:21.902341: step 55970, loss = 0.78 (1630.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:07:22.697819: step 55980, loss = 0.70 (1609.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:07:23.480692: step 55990, loss = 0.73 (1635.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:07:24.369203: step 56000, loss = 0.81 (1440.6 examples/sec; 0.089 sec/batch)
2017-05-02 16:07:25.069962: step 56010, loss = 0.72 (1826.6 examples/sec; 0.070 sec/batch)
2017-05-02 16:07:25.864662: step 56020, loss = 0.74 (1610.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:26.669485: step 56030, loss = 0.80 (1590.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:07:27.457074: step 56040, loss = 0.65 (1625.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:28.244297: step 56050, loss = 0.64 (1626.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:29.032288: step 56060, loss = 0.73 (1624.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:29.818661: step 56070, loss = 0.70 (1627.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:30.614511: step 56080, loss = 0.57 (1608.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:07:31.397541: step 56090, loss = 0.93 (1634.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:07:32.279455: step 56100, loss = 0.79 (1451.4 examples/sec; 0.088 sec/batch)
2017-05-02 16:07:32.983668: step 56110, loss = 0.71 (1817.6 examples/sec; 0.070 sec/batch)
2017-05-02 16:07:33.777751: step 56120, loss = 0.68 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:34.570548: step 56130, loss = 0.63 (1614.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:35.361460: step 56140, loss = 0.70 (1618.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:36.149036: step 56150, loss = 0.75 (1625.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:36.940199: step 56160, loss = 0.79 (1617.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:37.731366: step 56170, loss = 0.66 (1617.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:38.516791: step 56180, loss = 0.91 (1629.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:39.306607: step 56190, loss = 0.81 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:40.195630: step 56200, loss = 0.65 (1439.8 examples/sec; 0.089 sec/batch)
2017-05-02 16:07:40.883435: step 56210, loss = 0.72 (1861.0 examples/sec; 0.069 sec/batch)
2017-05-02 16:07:41.674795: step 56220, loss = 0.65 (1617.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:42.463240: step 56230, loss = 0.78 (1623.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:43.256601: step 56240, loss = 0.58 (1613.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:44.039020: step 56250, loss = 0.78 (1636.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:07:44.828023: step 56260, loss = 0.78 (1622.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:45.617697: step 56270, loss = 0.62 (1620.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:46.410675: step 56280, loss = 0.74 (1614.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:47.210828: step 56290, loss = 0.68 (1599.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:07:48.087238: step 56300, loss = 0.62 (1460.5 examples/sec; 0.088 sec/batch)
2017-05-02 16:07:48.784553: step 56310, loss = 0.84 (1835.6 examples/sec; 0.070 sec/batch)
2017-05-02 16:07:49.577841: step 56320, loss = 0.81 (1613.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:50.373159: step 56330, loss = 0.60 (1609.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:07:51.161985: step 56340, loss = 0.71 (1622.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:51.940873: step 56350, loss = 0.71 (1643.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:07:52.736614: step 56360, loss = 0.76 (1608.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:07:53.532168: step 56370, loss = 0.81 (1608.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:07:54.325289: step 56380, loss = 0.79 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:55.121654: step 56390, loss = 0.77 (1607.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:07:56.002481: step 56400, loss = 0.85 (1453.2 examples/sec; 0.088 sec/batch)
2017-05-02 16:07:56.700115: step 56410, loss = 0.75 (1834.8 examples/sec; 0.070 sec/batch)
2017-05-02 16:07:57.488383: step 56420, loss = 0.73 (1623.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:58.280516: step 56430, loss = 0.70 (1615.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:07:59.063099: step 56440, loss = 0.50 (1635.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:07:59.854001: step 56450, loss = 0.78 (1618.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:00.646063: step 56460, loss = 0.77 (1616.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:01.434219: step 56470, loss = 0.66 (1624.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:02.226360: step 56480, loss = 0.74 (1615.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:03.010021: step 56490, loss = 0.84 (1633.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:08:03.900614: step 56500, loss = 0.89 (1437.2 examples/sec; 0.089 sec/batch)
2017-05-02 16:08:04.590361: step 56510, loss = 0.64 (1855.8 examples/sec; 0.069 sec/batch)
2017-05-02 16:08:05.379232: step 56520, loss = 0.78 (1622.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:06.176176: step 56530, loss = 0.64 (1606.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:08:06.970946: step 56540, loss = 0.72 (1610.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:07.755234: step 56550, loss = 0.66 (1632.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:08:08.554419: step 56560, loss = 0.69 (1601.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:08:09.351776: step 56570, loss = 0.72 (1605.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:08:10.144473: step 56580, loss = 0.66 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:10.951158: step 56590, loss = 0.74 (1586.7 examples/sec; 0.081 sec/batch)
2017-05-02 16:08:11.831791: step 56600, loss = 0.77 (1453.5 examples/sec; 0.088 sec/batch)
2017-05-02 16:08:12.513672: step 56610, loss = 0.64 (1877.1 examples/sec; 0.068 sec/batch)
2017-05-02 16:08:13.305599: step 56620, loss = 0.81 (1616.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:14.090063: step 56630, loss = 0.58 (1631.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:08:14.884745: step 56640, loss = 0.66 (1610.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:15.662101: step 56650, loss = 0.64 (1646.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:08:16.465379: step 56660, loss = 0.70 (1593.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:08:17.252264: step 56670, loss = 0.91 (1626.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:18.043612: step 56680, loss = 0.74 (1617.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:18.839156: step 56690, loss = 0.53 (1609.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:08:19.741745: step 56700, loss = 0.66 (1418.1 examples/sec; 0.090 sec/batch)
2017-05-02 16:08:20.410141: step 56710, loss = 0.84 (1915.0 examples/sec; 0.067 sec/batch)
2017-05-02 16:08:21.198266: step 56720, loss = 0.66 (1624.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:21.984342: step 56730, loss = 0.74 (1628.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:22.779134: step 56740, loss = 0.75 (1610.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:23.562292: step 56750, loss = 0.67 (1634.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:08:24.345261: step 56760, loss = 0.65 (1634.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:08:25.134999: step 56770, loss = 0.79 (1620.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:25.932526: step 56780, loss = 0.63 (1605.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:08:26.722541: step 56790, loss = 0.80 (1620.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:27.628586: step 56800, loss = 0.74 (1412.7 examples/sec; 0.091 sec/batch)
2017-05-02 16:08:28.295321: step 56810, loss = 0.79 (1919.8 examples/sec; 0.067 sec/batch)
2017-05-02 16:08:29.082780: step 56820, loss = 0.76 (1625.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:29.880262: step 56830, loss = 0.79 (1605.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:08:30.668153: step 56840, loss = 0.77 (1624.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:31.454701: step 56850, loss = 0.73 (1627.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:32.246150: step 56860, loss = 0.65 (1617.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:33.039273: step 56870, loss = 0.67 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:33.830296: step 56880, loss = 0.87 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:34.619927: step 56890, loss = 0.71 (1621.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:35.508956: step 56900, loss = 0.75 (1439.8 examples/sec; 0.089 sec/batch)
2017-05-02 16:08:36.211724: step 56910, loss = 0.67 (1821.4 examples/sec; 0.070 sec/batch)
2017-05-02 16:08:36.997513: step 56920, loss = 0.74 (1628.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:37.791518: step 56930, loss = 0.66 (1612.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:38.581119: step 56940, loss = 0.63 (1621.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:39.362012: step 56950, loss = 0.72 (1639.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:08:40.151630: step 56960, loss = 0.73 (1621.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:40.938934: step 56970, loss = 0.71 (1625.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:41.729995: step 56980, loss = 0.80 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:42.516198: step 56990, loss = 0.57 (1628.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:43.430588: step 57000, loss = 0.66 (1399.8 examples/sec; 0.091 sec/batch)
2017-05-02 16:08:44.089779: step 57010, loss = 0.77 (1941.8 examples/sec; 0.066 sec/batch)
2017-05-02 16:08:44.877462: step 57020, loss = 0.80 (1625.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:45.672053: step 57030, loss = 0.74 (1610.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:46.467591: step 57040, loss = 0.81 (1609.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:08:47.256403: step 57050, loss = 0.61 (1622.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:48.040200: step 57060, loss = 0.69 (1633.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:08:48.831827: step 57070, loss = 0.65 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:49.626250: step 57080, loss = 0.73 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:50.415699: step 57090, loss = 0.62 (1621.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:51.305654: step 57100, loss = 0.76 (1438.3 examples/sec; 0.089 sec/batch)
2017-05-02 16:08:51.993375: step 57110, loss = 0.74 (1861.2 examples/sec; 0.069 sec/batch)
2017-05-02 16:08:52.789861: step 57120, loss = 0.68 (1607.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:08:53.573807: step 57130, loss = 0.74 (1632.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:08:54.366147: step 57140, loss = 0.59 (1615.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:55.154359: step 57150, loss = 0.64 (1623.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:55.930204: step 57160, loss = 0.75 (1649.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:08:56.723498: step 57170, loss = 0.68 (1613.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:57.510159: step 57180, loss = 0.84 (1627.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:58.301531: step 57190, loss = 0.71 (1617.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:08:59.213332: step 57200, loss = 0.69 (1403.8 examples/sec; 0.091 sec/batch)
2017-05-02 16:08:59.878108: step 57210, loss = 0.84 (1925.5 examples/sec; 0.066 sec/batch)
2017-05-02 16:09:00.676299: step 57220, loss = 0.76 (1603.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:09:01.473952: step 57230, loss = 0.73 (1604.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:09:02.272759: step 57240, loss = 0.85 (1602.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:09:03.073134: step 57250, loss = 0.76 (1599.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:09:03.853451: step 57260, loss = 0.71 (1640.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:09:04.641422: step 57270, loss = 0.68 (1624.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:05.425406: step 57280, loss = 0.68 (1632.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:09:06.216979: step 57290, loss = 0.69 (1617.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:07.106236: step 57300, loss = 0.90 (1439.4 examples/sec; 0.089 sec/batch)
2017-05-02 16:09:07.790485: step 57310, loss = 0.81 (1870.7 examples/sec; 0.068 sec/batch)
2017-05-02 16:09:08.574174: step 57320, loss = 0.71 (1633.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:09:09.365450: step 57330, loss = 0.67 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:10.162556: step 57340, loss = 0.77 (1605.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:09:10.949236: step 57350, loss = 0.89 (1627.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:11.728401: step 57360, loss = 0.69 (1642.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:09:12.518401: step 57370, loss = 0.70 (1620.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:13.315073: step 57380, loss = 0.76 (1606.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:09:14.102620: step 57390, loss = 0.77 (1625.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:14.991463: step 57400, loss = 0.70 (1440.1 examples/sec; 0.089 sec/batch)
2017-05-02 16:09:15.681824: step 57410, loss = 0.89 (1854.1 examples/sec; 0.069 sec/batch)
2017-05-02 16:09:16.472671: step 57420, loss = 0.70 (1618.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:17.277514: step 57430, loss = 0.76 (1590.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:09:18.066959: step 57440, loss = 0.73 (1621.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:18.852731: step 57450, loss = 0.89 (1629.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:19.632979: step 57460, loss = 0.76 (1640.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:09:20.422394: step 57470, loss = 0.77 (1621.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:21.212355: step 57480, loss = 0.78 (1620.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:22.006457: step 57490, loss = 0.71 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:22.885696: step 57500, loss = 0.75 (1455.8 examples/sec; 0.088 sec/batch)
2017-05-02 16:09:23.580511: step 57510, loss = 0.72 (1842.2 examples/sec; 0.069 sec/batch)
2017-05-02 16:09:24.373717: step 57520, loss = 0.71 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:25.158650: step 57530, loss = 0.75 (1630.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:09:25.946803: step 57540, loss = 0.75 (1624.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:26.732230: step 57550, loss = 0.90 (1629.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:27.519088: step 57560, loss = 0.76 (1626.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:28.301704: step 57570, loss = 0.84 (1635.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:09:29.095200: step 57580, loss = 0.84 (1613.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:29.895858: step 57590, loss = 0.64 (1598.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:09:30.787428: step 57600, loss = 0.70 (1435.7 examples/sec; 0.089 sec/batch)
2017-05-02 16:09:31.470392: step 57610, loss = 0.78 (1874.2 examples/sec; 0.068 sec/batch)
2017-05-02 16:09:32.260194: step 57620, loss = 0.68 (1620.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:33.055010: step 57630, loss = 0.86 (1610.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:33.846385: step 57640, loss = 0.68 (1617.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:34.636063: step 57650, loss = 0.71 (1620.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:35.421516: step 57660, loss = 0.76 (1629.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:36.209788: step 57670, loss = 0.77 (1623.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:36.998095: step 57680, loss = 0.84 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:37.796710: step 57690, loss = 0.73 (1602.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:09:38.685676: step 57700, loss = 0.75 (1439.9 examples/sec; 0.089 sec/batch)
2017-05-02 16:09:39.376541: step 57710, loss = 0.66 (1852.7 examples/sec; 0.069 sec/batch)
2017-05-02 16:09:40.163556: step 57720, loss = 0.63 (1626.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:40.950544: step 57730, loss = 0.78 (1626.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:41.737696: step 57740, loss = 0.81 (1626.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:42.531139: step 57750, loss = 0.79 (1613.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:43.323462: step 57760, loss = 0.61 (1615.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:44.106509: step 57770, loss = 0.75 (1634.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:09:44.895271: step 57780, loss = 0.68 (1622.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:45.686674: step 57790, loss = 0.81 (1617.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:46.583035: step 57800, loss = 0.66 (1428.0 examples/sec; 0.090 sec/batch)
2017-05-02 16:09:47.279512: step 57810, loss = 0.86 (1837.8 examples/sec; 0.070 sec/batch)
2017-05-02 16:09:48.060677: step 57820, loss = 0.68 (1638.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:09:48.852827: step 57830, loss = 0.67 (1615.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:49.640449: step 57840, loss = 0.78 (1625.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:50.438445: step 57850, loss = 0.60 (1604.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:09:51.222560: step 57860, loss = 0.69 (1632.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:09:52.005083: step 57870, loss = 0.61 (1635.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:09:52.795770: step 57880, loss = 0.80 (1618.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:53.588930: step 57890, loss = 0.67 (1613.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:54.479141: step 57900, loss = 0.76 (1437.9 examples/sec; 0.089 sec/batch)
2017-05-02 16:09:55.176359: step 57910, loss = 0.65 (1835.9 examples/sec; 0.070 sec/batch)
2017-05-02 16:09:55.956346: step 57920, loss = 0.73 (1641.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:09:56.746528: step 57930, loss = 0.67 (1619.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:57.539409: step 57940, loss = 0.69 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:58.329832: step 57950, loss = 0.68 (1619.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:59.116526: step 57960, loss = 0.76 (1627.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:09:59.896750: step 57970, loss = 0.89 (1640.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:10:00.687494: step 57980, loss = 0.78 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:01.483182: step 57990, loss = 0.59 (1608.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:10:02.392466: step 58000, loss = 0.73 (1407.7 examples/sec; 0.091 sec/batch)
2017-05-02 16:10:03.057789: step 58010, loss = 0.70 (1923.9 examples/sec; 0.067 sec/batch)
2017-05-02 16:10:03.840911: step 58020, loss = 0.65 (1634.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:10:04.621851: step 58030, loss = 0.65 (1639.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:10:05.420819: step 58040, loss = 0.70 (1602.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:10:06.219239: step 58050, loss = 0.85 (1603.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:10:07.005455: step 58060, loss = 0.55 (1628.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:07.788566: step 58070, loss = 0.72 (1634.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:10:08.576426: step 58080, loss = 0.76 (1624.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:09.367211: step 58090, loss = 0.52 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:10.253841: step 58100, loss = 0.78 (1443.7 examples/sec; 0.089 sec/batch)
2017-05-02 16:10:10.944448: step 58110, loss = 0.70 (1853.4 examples/sec; 0.069 sec/batch)
2017-05-02 16:10:11.726719: step 58120, loss = 0.89 (1636.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:10:12.517473: step 58130, loss = 0.83 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:13.311016: step 58140, loss = 0.81 (1613.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:14.092903: step 58150, loss = 0.64 (1637.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:10:14.885508: step 58160, loss = 0.73 (1614.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:15.669277: step 58170, loss = 0.84 (1633.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:10:16.463729: step 58180, loss = 0.72 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:17.249006: step 58190, loss = 0.60 (1630.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:18.138523: step 58200, loss = 0.75 (1439.0 examples/sec; 0.089 sec/batch)
2017-05-02 16:10:18.839118: step 58210, loss = 0.75 (1827.0 examples/sec; 0.070 sec/batch)
2017-05-02 16:10:19.627933: step 58220, loss = 0.62 (1622.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:20.405602: step 58230, loss = 0.80 (1645.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:10:21.197807: step 58240, loss = 0.59 (1615.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:21.990068: step 58250, loss = 0.63 (1615.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:22.781912: step 58260, loss = 0.75 (1616.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:23.564778: step 58270, loss = 0.67 (1635.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:10:24.346709: step 58280, loss = 0.57 (1637.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:10:25.136251: step 58290, loss = 0.73 (1621.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:26.023772: step 58300, loss = 0.56 (1442.2 examples/sec; 0.089 sec/batch)
2017-05-02 16:10:26.723041: step 58310, loss = 0.69 (1830.5 examples/sec; 0.070 sec/batch)
2017-05-02 16:10:27.512851: step 58320, loss = 0.76 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:28.305272: step 58330, loss = 0.82 (1615.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:29.092057: step 58340, loss = 0.73 (1626.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:29.880374: step 58350, loss = 0.61 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:30.675593: step 58360, loss = 0.65 (1609.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:10:31.457131: step 58370, loss = 0.70 (1637.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:10:32.244457: step 58380, loss = 0.78 (1625.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:33.035508: step 58390, loss = 0.69 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:33.938204: step 58400, loss = 0.69 (1418.0 examples/sec; 0.090 sec/batch)
2017-05-02 16:10:34.616365: step 58410, loss = 0.63 (1887.4 examples/sec; 0.068 sec/batch)
2017-05-02 16:10:35.407513: step 58420, loss = 0.63 (1617.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:36.187531: step 58430, loss = 0.76 (1641.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:10:36.978615: step 58440, loss = 0.91 (1618.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:37.775531: step 58450, loss = 0.74 (1606.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:10:38.568348: step 58460, loss = 0.79 (1614.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:39.351990: step 58470, loss = 0.76 (1633.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:10:40.135646: step 58480, loss = 0.83 (1633.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:10:40.929268: step 58490, loss = 0.72 (1612.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:41.818460: step 58500, loss = 0.75 (1439.5 examples/sec; 0.089 sec/batch)
2017-05-02 16:10:42.511130: step 58510, loss = 0.59 (1847.9 examples/sec; 0.069 sec/batch)
2017-05-02 16:10:43.299836: step 58520, loss = 0.74 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:44.087066: step 58530, loss = 0.64 (1626.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:44.874725: step 58540, loss = 0.67 (1625.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:45.658399: step 58550, loss = 0.71 (1633.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:10:46.451123: step 58560, loss = 0.63 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:47.243818: step 58570, loss = 0.75 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:48.021680: step 58580, loss = 0.63 (1645.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:10:48.810157: step 58590, loss = 0.76 (1623.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:49.698703: step 58600, loss = 0.63 (1440.6 examples/sec; 0.089 sec/batch)
2017-05-02 16:10:50.396973: step 58610, loss = 0.79 (1833.1 examples/sec; 0.070 sec/batch)
2017-05-02 16:10:51.189774: step 58620, loss = 0.86 (1614.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:51.967297: step 58630, loss = 0.79 (1646.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:10:52.758099: step 58640, loss = 0.81 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:53.554822: step 58650, loss = 0.62 (1606.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:10:54.346487: step 58660, loss = 0.83 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:55.139980: step 58670, loss = 0.88 (1613.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:55.918090: step 58680, loss = 0.61 (1645.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:10:56.705395: step 58690, loss = 0.75 (1625.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:57.606325: step 58700, loss = 0.79 (1420.8 examples/sec; 0.090 sec/batch)
2017-05-02 16:10:58.285098: step 58710, loss = 0.71 (1885.7 examples/sec; 0.068 sec/batch)
2017-05-02 16:10:59.074563: step 58720, loss = 0.74 (1621.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:10:59.856775: step 58730, loss = 0.75 (1636.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:11:00.642590: step 58740, loss = 0.60 (1629.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:01.435701: step 58750, loss = 0.72 (1613.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:02.234158: step 58760, loss = 0.73 (1603.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:11:03.025688: step 58770, loss = 0.67 (1617.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:03.802784: step 58780, loss = 0.71 (1647.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:11:04.590286: step 58790, loss = 0.82 (1625.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:05.486091: step 58800, loss = 0.77 (1428.9 examples/sec; 0.090 sec/batch)
2017-05-02 16:11:06.170566: step 58810, loss = 0.93 (1870.0 examples/sec; 0.068 sec/batch)
2017-05-02 16:11:06.968943: step 58820, loss = 0.63 (1603.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:11:07.748097: step 58830, loss = 0.70 (1642.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:11:08.541855: step 58840, loss = 0.67 (1612.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:09.332640: step 58850, loss = 0.63 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:10.124266: step 58860, loss = 0.75 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:10.919154: step 58870, loss = 0.68 (1610.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:11.693620: step 58880, loss = 0.71 (1652.8 examples/sec; 0.077 sec/batch)
2017-05-02 16:11:12.478337: step 58890, loss = 0.83 (1631.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:11:13.370142: step 58900, loss = 0.58 (1435.3 examples/sec; 0.089 sec/batch)
2017-05-02 16:11:14.069357: step 58910, loss = 0.81 (1830.6 examples/sec; 0.070 sec/batch)
2017-05-02 16:11:14.863750: step 58920, loss = 0.74 (1611.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:15.646085: step 58930, loss = 0.78 (1636.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:11:16.435761: step 58940, loss = 0.67 (1620.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:17.235340: step 58950, loss = 0.67 (1600.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:11:18.017288: step 58960, loss = 0.65 (1636.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:11:18.810023: step 58970, loss = 0.63 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:19.598764: step 58980, loss = 0.58 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:20.387731: step 58990, loss = 0.67 (1622.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:21.284270: step 59000, loss = 0.88 (1427.7 examples/sec; 0.090 sec/batch)
2017-05-02 16:11:21.974011: step 59010, loss = 0.83 (1855.8 examples/sec; 0.069 sec/batch)
2017-05-02 16:11:22.759413: step 59020, loss = 0.66 (1629.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:23.544367: step 59030, loss = 0.80 (1630.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:11:24.333238: step 59040, loss = 0.70 (1622.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:25.124631: step 59050, loss = 0.91 (1617.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:25.909125: step 59060, loss = 0.74 (1631.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:11:26.699576: step 59070, loss = 0.83 (1619.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:27.479158: step 59080, loss = 0.65 (1641.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:11:28.266293: step 59090, loss = 0.73 (1626.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:29.145407: step 59100, loss = 0.69 (1456.0 examples/sec; 0.088 sec/batch)
2017-05-02 16:11:29.858047: step 59110, loss = 0.77 (1796.1 examples/sec; 0.071 sec/batch)
2017-05-02 16:11:30.643126: step 59120, loss = 0.77 (1630.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:31.431531: step 59130, loss = 0.69 (1623.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:32.212276: step 59140, loss = 0.75 (1639.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:11:33.003851: step 59150, loss = 0.54 (1617.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:33.798114: step 59160, loss = 0.78 (1611.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:34.594060: step 59170, loss = 0.71 (1608.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:11:35.378319: step 59180, loss = 0.76 (1632.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:11:36.167606: step 59190, loss = 0.74 (1621.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:37.068199: step 59200, loss = 0.80 (1421.3 examples/sec; 0.090 sec/batch)
2017-05-02 16:11:37.761313: step 59210, loss = 0.79 (1846.7 examples/sec; 0.069 sec/batch)
2017-05-02 16:11:38.547759: step 59220, loss = 0.73 (1627.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:39.340131: step 59230, loss = 0.71 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:40.123964: step 59240, loss = 0.71 (1633.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:11:40.910802: step 59250, loss = 0.69 (1626.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:41.705233: step 59260, loss = 0.69 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:42.482323: step 59270, loss = 0.65 (1647.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:11:43.272710: step 59280, loss = 0.91 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:44.054773: step 59290, loss = 0.69 (1636.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:11:44.955015: step 59300, loss = 0.69 (1421.9 examples/sec; 0.090 sec/batch)
2017-05-02 16:11:45.652219: step 59310, loss = 0.75 (1835.9 examples/sec; 0.070 sec/batch)
2017-05-02 16:11:46.450740: step 59320, loss = 0.77 (1603.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:11:47.242054: step 59330, loss = 0.71 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:48.023121: step 59340, loss = 0.64 (1638.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:11:48.814324: step 59350, loss = 0.76 (1617.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:49.602649: step 59360, loss = 0.63 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:50.395208: step 59370, loss = 0.72 (1615.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:51.181333: step 59380, loss = 0.67 (1628.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:51.970207: step 59390, loss = 0.67 (1622.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:52.851995: step 59400, loss = 0.70 (1451.6 examples/sec; 0.088 sec/batch)
2017-05-02 16:11:53.552581: step 59410, loss = 0.84 (1827.0 examples/sec; 0.070 sec/batch)
2017-05-02 16:11:54.342103: step 59420, loss = 0.68 (1621.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:55.127682: step 59430, loss = 0.85 (1629.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:55.907300: step 59440, loss = 0.73 (1641.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:11:56.701541: step 59450, loss = 0.65 (1611.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:57.496103: step 59460, loss = 0.77 (1611.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:58.285091: step 59470, loss = 0.80 (1622.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:59.074145: step 59480, loss = 0.64 (1622.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:11:59.850338: step 59490, loss = 0.77 (1649.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:12:00.730106: step 59500, loss = 0.77 (1454.9 examples/sec; 0.088 sec/batch)
2017-05-02 16:12:01.425957: step 59510, loss = 0.73 (1839.5 examples/sec; 0.070 sec/batch)
2017-05-02 16:12:02.220258: step 59520, loss = 0.69 (1611.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:03.011602: step 59530, loss = 0.73 (1617.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:03.792844: step 59540, loss = 0.68 (1638.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:12:04.587483: step 59550, loss = 0.73 (1610.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:05.379857: step 59560, loss = 0.75 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:06.170661: step 59570, loss = 0.81 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:06.962604: step 59580, loss = 0.83 (1616.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:07.740306: step 59590, loss = 0.70 (1645.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:12:08.626031: step 59600, loss = 0.80 (1445.1 examples/sec; 0.089 sec/batch)
2017-05-02 16:12:09.332441: step 59610, loss = 0.67 (1812.0 examples/sec; 0.071 sec/batch)
2017-05-02 16:12:10.119380: step 59620, loss = 0.72 (1626.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:10.909581: step 59630, loss = 0.74 (1619.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:11.688691: step 59640, loss = 0.71 (1642.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:12:12.483734: step 59650, loss = 0.63 (1610.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:12:13.273280: step 59660, loss = 0.74 (1621.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:14.062477: step 59670, loss = 0.78 (1621.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:14.855000: step 59680, loss = 0.70 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:15.631836: step 59690, loss = 0.77 (1647.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:12:16.522414: step 59700, loss = 0.63 (1437.3 examples/sec; 0.089 sec/batch)
2017-05-02 16:12:17.216459: step 59710, loss = 0.59 (1844.3 examples/sec; 0.069 sec/batch)
2017-05-02 16:12:18.008244: step 59720, loss = 0.79 (1616.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:18.800854: step 59730, loss = 0.78 (1614.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:19.588406: step 59740, loss = 0.58 (1625.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:20.378562: step 59750, loss = 0.66 (1619.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:21.165304: step 59760, loss = 0.66 (1627.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:21.956293: step 59770, loss = 0.69 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:22.754157: step 59780, loss = 0.57 (1604.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:12:23.546728: step 59790, loss = 0.63 (1615.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:24.429868: step 59800, loss = 0.72 (1449.4 examples/sec; 0.088 sec/batch)
2017-05-02 16:12:25.122160: step 59810, loss = 0.86 (1848.9 examples/sec; 0.069 sec/batch)
2017-05-02 16:12:25.909395: step 59820, loss = 0.68 (1625.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:26.697597: step 59830, loss = 0.74 (1623.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:27.479906: step 59840, loss = 0.78 (1636.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:12:28.269687: step 59850, loss = 0.81 (1620.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:29.057037: step 59860, loss = 0.86 (1625.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:29.848724: step 59870, loss = 0.73 (1616.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:30.641386: step 59880, loss = 0.80 (1614.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:31.428301: step 59890, loss = 0.79 (1626.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:32.318296: step 59900, loss = 0.72 (1438.2 examples/sec; 0.089 sec/batch)
2017-05-02 16:12:33.016937: step 59910, loss = 0.59 (1832.1 examples/sec; 0.070 sec/batch)
2017-05-02 16:12:33.813481: step 59920, loss = 0.79 (1606.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:12:34.606747: step 59930, loss = 0.70 (1613.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:35.393101: step 59940, loss = 0.58 (1627.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:36.181761: step 59950, loss = 0.95 (1623.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:36.974416: step 59960, loss = 0.71 (1614.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:37.771649: step 59970, loss = 0.66 (1605.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:12:38.561606: step 59980, loss = 0.69 (1620.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:39.352060: step 59990, loss = 0.67 (1619.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:40.235561: step 60000, loss = 0.63 (1448.8 examples/sec; 0.088 sec/batch)
2017-05-02 16:12:40.932528: step 60010, loss = 0.67 (1836.5 examples/sec; 0.070 sec/batch)
2017-05-02 16:12:41.728571: step 60020, loss = 0.73 (1608.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:12:42.523557: step 60030, loss = 0.75 (1610.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:43.319266: step 60040, loss = 0.76 (1608.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:12:44.103594: step 60050, loss = 0.83 (1632.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:12:44.891403: step 60060, loss = 0.72 (1624.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:45.681767: step 60070, loss = 0.59 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:46.470311: step 60080, loss = 0.69 (1623.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:47.260163: step 60090, loss = 0.59 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:48.136596: step 60100, loss = 0.86 (1460.5 examples/sec; 0.088 sec/batch)
2017-05-02 16:12:48.834002: step 60110, loss = 0.65 (1835.4 examples/sec; 0.070 sec/batch)
2017-05-02 16:12:49.628458: step 60120, loss = 0.64 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:50.429027: step 60130, loss = 0.73 (1598.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:12:51.224386: step 60140, loss = 0.73 (1609.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:12:52.004592: step 60150, loss = 0.64 (1640.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:12:52.791657: step 60160, loss = 0.90 (1626.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:53.586240: step 60170, loss = 0.64 (1610.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:54.383502: step 60180, loss = 0.76 (1605.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:12:55.172120: step 60190, loss = 0.79 (1623.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:56.053201: step 60200, loss = 0.76 (1452.8 examples/sec; 0.088 sec/batch)
2017-05-02 16:12:56.753434: step 60210, loss = 0.77 (1828.0 examples/sec; 0.070 sec/batch)
2017-05-02 16:12:57.547452: step 60220, loss = 0.67 (1612.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:58.336250: step 60230, loss = 0.68 (1622.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:59.128539: step 60240, loss = 0.65 (1615.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:12:59.907078: step 60250, loss = 0.74 (1644.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:13:00.694234: step 60260, loss = 0.67 (1626.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:01.479524: step 60270, loss = 0.73 (1630.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:02.276169: step 60280, loss = 0.72 (1606.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:13:03.066399: step 60290, loss = 0.92 (1619.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:03.943616: step 60300, loss = 0.68 (1459.2 examples/sec; 0.088 sec/batch)
2017-05-02 16:13:04.640826: step 60310, loss = 0.67 (1835.9 examples/sec; 0.070 sec/batch)
2017-05-02 16:13:05.443110: step 60320, loss = 0.86 (1595.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:13:06.224955: step 60330, loss = 0.75 (1637.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:13:07.035656: step 60340, loss = 0.66 (1578.9 examples/sec; 0.081 sec/batch)
2017-05-02 16:13:07.818589: step 60350, loss = 0.89 (1634.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:13:08.607519: step 60360, loss = 0.84 (1622.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:09.402156: step 60370, loss = 0.86 (1610.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:10.193112: step 60380, loss = 0.86 (1618.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:10.987323: step 60390, loss = 0.58 (1611.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:11.866986: step 60400, loss = 0.75 (1455.1 examples/sec; 0.088 sec/batch)
2017-05-02 16:13:12.561436: step 60410, loss = 0.84 (1843.2 examples/sec; 0.069 sec/batch)
2017-05-02 16:13:13.349029: step 60420, loss = 0.60 (1625.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:14.143505: step 60430, loss = 0.64 (1611.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:14.944990: step 60440, loss = 0.73 (1597.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:13:15.725570: step 60450, loss = 0.79 (1639.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:13:16.521928: step 60460, loss = 0.70 (1607.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:13:17.314500: step 60470, loss = 0.78 (1615.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:18.104749: step 60480, loss = 0.72 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:18.892821: step 60490, loss = 0.89 (1624.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:19.762934: step 60500, loss = 0.69 (1471.1 examples/sec; 0.087 sec/batch)
2017-05-02 16:13:20.464699: step 60510, loss = 0.58 (1824.0 examples/sec; 0.070 sec/batch)
2017-05-02 16:13:21.249160: step 60520, loss = 0.73 (1631.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:13:22.041974: step 60530, loss = 0.65 (1614.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:22.834466: step 60540, loss = 0.66 (1615.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:23.617337: step 60550, loss = 0.67 (1635.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:13:24.406311: step 60560, loss = 0.86 (1622.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:25.201260: step 60570, loss = 0.62 (1610.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:25.993521: step 60580, loss = 0.63 (1615.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:26.786339: step 60590, loss = 0.65 (1614.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:27.689410: step 60600, loss = 0.78 (1417.4 examples/sec; 0.090 sec/batch)
2017-05-02 16:13:28.361248: step 60610, loss = 0.76 (1905.2 examples/sec; 0.067 sec/batch)
2017-05-02 16:13:29.149941: step 60620, loss = 0.87 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:30.000329: step 60630, loss = 0.84 (1505.2 examples/sec; 0.085 sec/batch)
2017-05-02 16:13:30.774812: step 60640, loss = 0.75 (1652.7 examples/sec; 0.077 sec/batch)
2017-05-02 16:13:31.556608: step 60650, loss = 0.90 (1637.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:13:32.348562: step 60660, loss = 0.78 (1616.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:33.141692: step 60670, loss = 0.66 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:33.936066: step 60680, loss = 0.67 (1611.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:34.726746: step 60690, loss = 0.74 (1618.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:35.620458: step 60700, loss = 0.76 (1432.2 examples/sec; 0.089 sec/batch)
2017-05-02 16:13:36.316942: step 60710, loss = 0.81 (1837.8 examples/sec; 0.070 sec/batch)
2017-05-02 16:13:37.105696: step 60720, loss = 0.65 (1622.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:37.896867: step 60730, loss = 0.78 (1617.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:38.693703: step 60740, loss = 0.61 (1606.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:13:39.477045: step 60750, loss = 0.62 (1634.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:13:40.261651: step 60760, loss = 0.75 (1631.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:13:41.051725: step 60770, loss = 0.74 (1620.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:41.839763: step 60780, loss = 0.77 (1624.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:42.630531: step 60790, loss = 0.72 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:43.521818: step 60800, loss = 0.83 (1436.1 examples/sec; 0.089 sec/batch)
2017-05-02 16:13:44.218054: step 60810, loss = 0.90 (1838.5 examples/sec; 0.070 sec/batch)
2017-05-02 16:13:45.003738: step 60820, loss = 0.58 (1629.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:45.800745: step 60830, loss = 0.87 (1606.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:13:46.592737: step 60840, loss = 0.75 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:47.385015: step 60850, loss = 0.75 (1615.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:48.170030: step 60860, loss = 0.78 (1630.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:48.965005: step 60870, loss = 0.71 (1610.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:49.757872: step 60880, loss = 0.72 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:50.548829: step 60890, loss = 0.78 (1618.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:51.431475: step 60900, loss = 0.73 (1450.2 examples/sec; 0.088 sec/batch)
2017-05-02 16:13:52.126432: step 60910, loss = 0.80 (1841.8 examples/sec; 0.069 sec/batch)
2017-05-02 16:13:52.925974: step 60920, loss = 0.75 (1600.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:13:53.718704: step 60930, loss = 0.74 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:54.513439: step 60940, loss = 0.75 (1610.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:55.306819: step 60950, loss = 0.72 (1613.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:56.086919: step 60960, loss = 0.82 (1640.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:13:56.882579: step 60970, loss = 0.73 (1608.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:13:57.676663: step 60980, loss = 0.71 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:58.465016: step 60990, loss = 0.70 (1623.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:13:59.351420: step 61000, loss = 0.72 (1444.0 examples/sec; 0.089 sec/batch)
2017-05-02 16:14:00.046342: step 61010, loss = 0.75 (1842.0 examples/sec; 0.069 sec/batch)
2017-05-02 16:14:00.837934: step 61020, loss = 0.79 (1617.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:01.638279: step 61030, loss = 0.75 (1599.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:14:02.434237: step 61040, loss = 0.79 (1608.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:14:03.225401: step 61050, loss = 0.67 (1617.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:04.008777: step 61060, loss = 0.73 (1634.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:14:04.798321: step 61070, loss = 0.70 (1621.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:05.591339: step 61080, loss = 0.70 (1614.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:06.386768: step 61090, loss = 0.68 (1609.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:14:07.275812: step 61100, loss = 0.68 (1439.7 examples/sec; 0.089 sec/batch)
2017-05-02 16:14:07.957467: step 61110, loss = 0.69 (1877.8 examples/sec; 0.068 sec/batch)
2017-05-02 16:14:08.747240: step 61120, loss = 0.62 (1620.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:09.537541: step 61130, loss = 0.82 (1619.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:10.325398: step 61140, loss = 0.61 (1624.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:11.120248: step 61150, loss = 0.68 (1610.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:11.902777: step 61160, loss = 0.77 (1635.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:14:12.696392: step 61170, loss = 0.70 (1612.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:13.489582: step 61180, loss = 0.75 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:14.277662: step 61190, loss = 0.82 (1624.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:15.179089: step 61200, loss = 0.71 (1420.0 examples/sec; 0.090 sec/batch)
2017-05-02 16:14:15.873337: step 61210, loss = 0.69 (1843.7 examples/sec; 0.069 sec/batch)
2017-05-02 16:14:16.661503: step 61220, loss = 0.69 (1624.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:17.454222: step 61230, loss = 0.77 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:18.244390: step 61240, loss = 0.68 (1619.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:19.033775: step 61250, loss = 0.71 (1621.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:19.815508: step 61260, loss = 0.68 (1637.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:14:20.612402: step 61270, loss = 0.73 (1606.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:14:21.405208: step 61280, loss = 0.67 (1614.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:22.192429: step 61290, loss = 0.86 (1626.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:23.081504: step 61300, loss = 0.76 (1439.7 examples/sec; 0.089 sec/batch)
2017-05-02 16:14:23.772253: step 61310, loss = 0.66 (1853.1 examples/sec; 0.069 sec/batch)
2017-05-02 16:14:24.567445: step 61320, loss = 0.65 (1609.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:14:25.359796: step 61330, loss = 0.70 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:26.146912: step 61340, loss = 0.80 (1626.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:26.942770: step 61350, loss = 0.72 (1608.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:14:27.725165: step 61360, loss = 0.84 (1636.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:14:28.518323: step 61370, loss = 0.80 (1613.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:29.311029: step 61380, loss = 0.66 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:30.108291: step 61390, loss = 0.79 (1605.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:14:30.995296: step 61400, loss = 0.55 (1443.1 examples/sec; 0.089 sec/batch)
2017-05-02 16:14:31.683765: step 61410, loss = 0.77 (1859.2 examples/sec; 0.069 sec/batch)
2017-05-02 16:14:32.475214: step 61420, loss = 0.71 (1617.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:33.268224: step 61430, loss = 0.81 (1614.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:34.063131: step 61440, loss = 0.72 (1610.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:34.853142: step 61450, loss = 0.80 (1620.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:35.635929: step 61460, loss = 0.78 (1635.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:14:36.430854: step 61470, loss = 0.58 (1610.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:37.226368: step 61480, loss = 0.70 (1609.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:14:38.012813: step 61490, loss = 0.70 (1627.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:38.909391: step 61500, loss = 0.88 (1427.7 examples/sec; 0.090 sec/batch)
2017-05-02 16:14:39.588818: step 61510, loss = 0.63 (1883.9 examples/sec; 0.068 sec/batch)
2017-05-02 16:14:40.383772: step 61520, loss = 0.73 (1610.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:41.176826: step 61530, loss = 0.61 (1614.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:41.965628: step 61540, loss = 0.61 (1622.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:42.758494: step 61550, loss = 0.73 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:43.546834: step 61560, loss = 0.63 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:44.335585: step 61570, loss = 0.93 (1622.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:45.123722: step 61580, loss = 0.72 (1624.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:45.918252: step 61590, loss = 0.93 (1611.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:46.818770: step 61600, loss = 0.61 (1421.4 examples/sec; 0.090 sec/batch)
2017-05-02 16:14:47.504525: step 61610, loss = 0.81 (1866.6 examples/sec; 0.069 sec/batch)
2017-05-02 16:14:48.291618: step 61620, loss = 0.82 (1626.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:49.082784: step 61630, loss = 0.62 (1617.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:49.872727: step 61640, loss = 0.63 (1620.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:50.652659: step 61650, loss = 0.64 (1641.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:14:51.440334: step 61660, loss = 0.75 (1625.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:52.231954: step 61670, loss = 0.84 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:53.022755: step 61680, loss = 0.63 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:53.818346: step 61690, loss = 0.74 (1608.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:14:54.706196: step 61700, loss = 0.76 (1441.7 examples/sec; 0.089 sec/batch)
2017-05-02 16:14:55.400724: step 61710, loss = 0.70 (1843.0 examples/sec; 0.069 sec/batch)
2017-05-02 16:14:56.177463: step 61720, loss = 0.82 (1647.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:14:56.974569: step 61730, loss = 0.61 (1605.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:14:57.764063: step 61740, loss = 0.76 (1621.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:58.553663: step 61750, loss = 0.78 (1621.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:14:59.341305: step 61760, loss = 0.72 (1625.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:00.126891: step 61770, loss = 0.64 (1629.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:00.917625: step 61780, loss = 0.76 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:01.716701: step 61790, loss = 0.66 (1601.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:15:02.604152: step 61800, loss = 0.66 (1442.3 examples/sec; 0.089 sec/batch)
2017-05-02 16:15:03.298586: step 61810, loss = 0.66 (1843.3 examples/sec; 0.069 sec/batch)
2017-05-02 16:15:04.081961: step 61820, loss = 0.72 (1633.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:15:04.878584: step 61830, loss = 0.75 (1606.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:15:05.668408: step 61840, loss = 0.66 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:06.462836: step 61850, loss = 0.66 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:07.254296: step 61860, loss = 0.79 (1617.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:08.040778: step 61870, loss = 0.78 (1627.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:08.839422: step 61880, loss = 0.78 (1602.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:15:09.635224: step 61890, loss = 0.71 (1608.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:15:10.529463: step 61900, loss = 0.62 (1431.4 examples/sec; 0.089 sec/batch)
2017-05-02 16:15:11.230244: step 61910, loss = 0.75 (1826.5 examples/sec; 0.070 sec/batch)
2017-05-02 16:15:12.014137: step 61920, loss = 0.69 (1632.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:15:12.805203: step 61930, loss = 0.68 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:13.597912: step 61940, loss = 0.65 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:14.387386: step 61950, loss = 0.85 (1621.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:15.179210: step 61960, loss = 0.84 (1616.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:15.961559: step 61970, loss = 0.74 (1636.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:15:16.746172: step 61980, loss = 0.78 (1631.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:15:17.540288: step 61990, loss = 0.83 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:18.433227: step 62000, loss = 0.76 (1433.5 examples/sec; 0.089 sec/batch)
2017-05-02 16:15:19.127965: step 62010, loss = 0.91 (1842.4 examples/sec; 0.069 sec/batch)
2017-05-02 16:15:19.924061: step 62020, loss = 0.84 (1607.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:15:20.714620: step 62030, loss = 0.73 (1619.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:21.507345: step 62040, loss = 0.60 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:22.299323: step 62050, loss = 0.72 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:23.101012: step 62060, loss = 0.54 (1596.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:15:23.881651: step 62070, loss = 0.70 (1639.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:15:24.672891: step 62080, loss = 0.75 (1617.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:25.478533: step 62090, loss = 0.62 (1588.8 examples/sec; 0.081 sec/batch)
2017-05-02 16:15:26.362253: step 62100, loss = 0.91 (1448.4 examples/sec; 0.088 sec/batch)
2017-05-02 16:15:27.055494: step 62110, loss = 0.65 (1846.4 examples/sec; 0.069 sec/batch)
2017-05-02 16:15:27.835773: step 62120, loss = 0.77 (1640.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:15:28.628537: step 62130, loss = 0.77 (1614.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:29.427690: step 62140, loss = 0.84 (1601.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:15:30.210643: step 62150, loss = 0.74 (1634.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:15:31.002800: step 62160, loss = 0.83 (1615.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:31.782302: step 62170, loss = 0.68 (1642.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:15:32.576412: step 62180, loss = 0.64 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:33.365762: step 62190, loss = 0.75 (1621.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:34.257398: step 62200, loss = 0.79 (1435.6 examples/sec; 0.089 sec/batch)
2017-05-02 16:15:34.951231: step 62210, loss = 0.65 (1844.8 examples/sec; 0.069 sec/batch)
2017-05-02 16:15:35.738558: step 62220, loss = 0.74 (1625.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:36.528312: step 62230, loss = 0.77 (1620.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:37.323053: step 62240, loss = 0.76 (1610.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:38.110381: step 62250, loss = 0.90 (1625.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:38.899807: step 62260, loss = 0.70 (1621.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:39.682836: step 62270, loss = 0.68 (1634.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:15:40.483063: step 62280, loss = 0.64 (1599.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:15:41.272032: step 62290, loss = 0.61 (1622.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:42.172707: step 62300, loss = 0.70 (1421.2 examples/sec; 0.090 sec/batch)
2017-05-02 16:15:42.861375: step 62310, loss = 0.88 (1858.7 examples/sec; 0.069 sec/batch)
2017-05-02 16:15:43.643493: step 62320, loss = 0.70 (1636.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:15:44.442091: step 62330, loss = 0.71 (1602.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:15:45.239545: step 62340, loss = 0.81 (1605.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:15:46.031631: step 62350, loss = 0.74 (1616.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:46.832035: step 62360, loss = 0.66 (1599.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:15:47.620134: step 62370, loss = 0.75 (1624.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:48.410092: step 62380, loss = 0.60 (1620.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:49.197213: step 62390, loss = 0.65 (1626.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:50.091688: step 62400, loss = 0.70 (1431.0 examples/sec; 0.089 sec/batch)
2017-05-02 16:15:50.794257: step 62410, loss = 0.83 (1821.9 examples/sec; 0.070 sec/batch)
2017-05-02 16:15:51.583748: step 62420, loss = 0.86 (1621.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:52.374136: step 62430, loss = 0.60 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:53.166187: step 62440, loss = 0.63 (1616.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:53.964152: step 62450, loss = 0.79 (1604.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:15:54.758644: step 62460, loss = 0.62 (1611.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:55.547175: step 62470, loss = 0.78 (1623.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:56.336062: step 62480, loss = 0.75 (1622.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:15:57.142041: step 62490, loss = 0.79 (1588.1 examples/sec; 0.081 sec/batch)
2017-05-02 16:15:58.058856: step 62500, loss = 0.85 (1396.1 examples/sec; 0.092 sec/batch)
2017-05-02 16:15:58.734736: step 62510, loss = 0.77 (1893.8 examples/sec; 0.068 sec/batch)
2017-05-02 16:15:59.523261: step 62520, loss = 0.76 (1623.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:00.318370: step 62530, loss = 0.65 (1609.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:16:01.112614: step 62540, loss = 0.73 (1611.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:01.909542: step 62550, loss = 0.66 (1606.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:16:02.711926: step 62560, loss = 0.63 (1595.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:16:03.501600: step 62570, loss = 0.71 (1620.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:04.289557: step 62580, loss = 0.66 (1624.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:05.084694: step 62590, loss = 0.66 (1609.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:16:05.974512: step 62600, loss = 0.74 (1438.5 examples/sec; 0.089 sec/batch)
2017-05-02 16:16:06.670506: step 62610, loss = 0.85 (1839.1 examples/sec; 0.070 sec/batch)
2017-05-02 16:16:07.457983: step 62620, loss = 0.74 (1625.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:08.240599: step 62630, loss = 0.74 (1635.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:16:09.032985: step 62640, loss = 0.72 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:09.820428: step 62650, loss = 0.80 (1625.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:10.613383: step 62660, loss = 0.71 (1614.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:11.400939: step 62670, loss = 0.69 (1625.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:12.179495: step 62680, loss = 0.82 (1644.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:16:12.970025: step 62690, loss = 0.78 (1619.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:13.856806: step 62700, loss = 0.83 (1443.4 examples/sec; 0.089 sec/batch)
2017-05-02 16:16:14.550771: step 62710, loss = 0.72 (1844.5 examples/sec; 0.069 sec/batch)
2017-05-02 16:16:15.330295: step 62720, loss = 0.76 (1642.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:16:16.112498: step 62730, loss = 0.66 (1636.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:16:16.907049: step 62740, loss = 0.67 (1611.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:17.703542: step 62750, loss = 0.63 (1607.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:16:18.492094: step 62760, loss = 0.75 (1623.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:19.278469: step 62770, loss = 0.81 (1627.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:20.068616: step 62780, loss = 0.81 (1620.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:20.866871: step 62790, loss = 0.75 (1603.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:16:21.769651: step 62800, loss = 0.71 (1417.8 examples/sec; 0.090 sec/batch)
2017-05-02 16:16:22.442311: step 62810, loss = 0.83 (1903.8 examples/sec; 0.067 sec/batch)
2017-05-02 16:16:23.232118: step 62820, loss = 0.68 (1620.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:24.007333: step 62830, loss = 0.70 (1651.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:16:24.795197: step 62840, loss = 0.79 (1624.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:25.591387: step 62850, loss = 0.67 (1607.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:16:26.389681: step 62860, loss = 0.68 (1603.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:16:27.174697: step 62870, loss = 0.61 (1630.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:27.965954: step 62880, loss = 0.83 (1617.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:28.757311: step 62890, loss = 0.81 (1617.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:29.655868: step 62900, loss = 0.81 (1424.5 examples/sec; 0.090 sec/batch)
2017-05-02 16:16:30.350350: step 62910, loss = 0.82 (1843.1 examples/sec; 0.069 sec/batch)
2017-05-02 16:16:31.144662: step 62920, loss = 0.66 (1611.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:31.925537: step 62930, loss = 0.63 (1639.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:16:32.714932: step 62940, loss = 0.73 (1621.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:33.505617: step 62950, loss = 0.80 (1618.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:34.293812: step 62960, loss = 0.80 (1624.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:35.083927: step 62970, loss = 0.87 (1620.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:35.868867: step 62980, loss = 0.87 (1630.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:16:36.663823: step 62990, loss = 0.62 (1610.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:37.562085: step 63000, loss = 0.63 (1425.0 examples/sec; 0.090 sec/batch)
2017-05-02 16:16:38.245922: step 63010, loss = 0.74 (1871.8 examples/sec; 0.068 sec/batch)
2017-05-02 16:16:39.043497: step 63020, loss = 0.70 (1604.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:16:39.823502: step 63030, loss = 0.64 (1641.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:16:40.621703: step 63040, loss = 0.84 (1603.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:16:41.407437: step 63050, loss = 0.72 (1629.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:42.200131: step 63060, loss = 0.76 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:42.989066: step 63070, loss = 0.79 (1622.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:43.770411: step 63080, loss = 0.61 (1638.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:16:44.564566: step 63090, loss = 0.79 (1611.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:45.467317: step 63100, loss = 0.65 (1417.9 examples/sec; 0.090 sec/batch)
2017-05-02 16:16:46.166167: step 63110, loss = 0.68 (1831.6 examples/sec; 0.070 sec/batch)
2017-05-02 16:16:46.960479: step 63120, loss = 0.71 (1611.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:47.743282: step 63130, loss = 0.68 (1635.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:16:48.535682: step 63140, loss = 0.70 (1615.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:49.324425: step 63150, loss = 0.69 (1622.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:50.116188: step 63160, loss = 0.72 (1616.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:50.909956: step 63170, loss = 0.67 (1612.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:51.694299: step 63180, loss = 0.73 (1631.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:16:52.484727: step 63190, loss = 0.78 (1619.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:53.382775: step 63200, loss = 0.62 (1425.3 examples/sec; 0.090 sec/batch)
2017-05-02 16:16:54.082313: step 63210, loss = 0.78 (1829.8 examples/sec; 0.070 sec/batch)
2017-05-02 16:16:54.872567: step 63220, loss = 0.55 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:55.652686: step 63230, loss = 0.79 (1640.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:16:56.448848: step 63240, loss = 0.68 (1607.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:16:57.244269: step 63250, loss = 0.77 (1609.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:16:58.039589: step 63260, loss = 0.63 (1609.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:16:58.833431: step 63270, loss = 0.77 (1612.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:16:59.618784: step 63280, loss = 0.73 (1629.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:00.402300: step 63290, loss = 0.71 (1633.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:17:01.306654: step 63300, loss = 0.79 (1415.4 examples/sec; 0.090 sec/batch)
2017-05-02 16:17:01.998407: step 63310, loss = 0.71 (1850.4 examples/sec; 0.069 sec/batch)
2017-05-02 16:17:02.792124: step 63320, loss = 0.67 (1612.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:03.579586: step 63330, loss = 0.77 (1625.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:04.368561: step 63340, loss = 0.65 (1622.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:05.170314: step 63350, loss = 0.75 (1596.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:17:05.962319: step 63360, loss = 0.72 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:06.759645: step 63370, loss = 0.78 (1605.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:17:07.546266: step 63380, loss = 0.79 (1627.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:08.341740: step 63390, loss = 0.65 (1609.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:17:09.229114: step 63400, loss = 0.67 (1442.5 examples/sec; 0.089 sec/batch)
2017-05-02 16:17:09.918720: step 63410, loss = 0.74 (1856.1 examples/sec; 0.069 sec/batch)
2017-05-02 16:17:10.709585: step 63420, loss = 0.87 (1618.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:11.495482: step 63430, loss = 0.71 (1628.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:12.282759: step 63440, loss = 0.80 (1625.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:13.067745: step 63450, loss = 0.77 (1630.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:17:13.860977: step 63460, loss = 0.77 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:14.648800: step 63470, loss = 0.62 (1624.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:15.444844: step 63480, loss = 0.74 (1607.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:17:16.227794: step 63490, loss = 0.71 (1634.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:17:17.120089: step 63500, loss = 0.71 (1434.5 examples/sec; 0.089 sec/batch)
2017-05-02 16:17:17.814648: step 63510, loss = 0.65 (1842.9 examples/sec; 0.069 sec/batch)
2017-05-02 16:17:18.609577: step 63520, loss = 0.73 (1610.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:19.400867: step 63530, loss = 0.75 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:20.187148: step 63540, loss = 0.81 (1627.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:20.975425: step 63550, loss = 0.82 (1623.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:21.767118: step 63560, loss = 0.90 (1616.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:22.554647: step 63570, loss = 0.92 (1625.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:23.343594: step 63580, loss = 0.58 (1622.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:24.134803: step 63590, loss = 0.72 (1617.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:25.024818: step 63600, loss = 0.70 (1438.2 examples/sec; 0.089 sec/batch)
2017-05-02 16:17:25.722496: step 63610, loss = 0.60 (1834.7 examples/sec; 0.070 sec/batch)
2017-05-02 16:17:26.519648: step 63620, loss = 0.70 (1605.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:17:27.310816: step 63630, loss = 0.79 (1617.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:28.090570: step 63640, loss = 0.57 (1641.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:17:28.882893: step 63650, loss = 0.66 (1615.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:29.682670: step 63660, loss = 0.65 (1600.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:17:30.481381: step 63670, loss = 0.98 (1602.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:17:31.272855: step 63680, loss = 0.77 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:32.056021: step 63690, loss = 0.68 (1634.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:17:32.936009: step 63700, loss = 0.90 (1454.6 examples/sec; 0.088 sec/batch)
2017-05-02 16:17:33.630978: step 63710, loss = 0.70 (1841.8 examples/sec; 0.069 sec/batch)
2017-05-02 16:17:34.424090: step 63720, loss = 0.80 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:35.215019: step 63730, loss = 0.74 (1618.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:35.998045: step 63740, loss = 0.84 (1634.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:17:36.790202: step 63750, loss = 0.62 (1615.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:37.580645: step 63760, loss = 0.61 (1619.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:38.374193: step 63770, loss = 0.76 (1613.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:39.163808: step 63780, loss = 0.73 (1621.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:39.937334: step 63790, loss = 0.76 (1654.8 examples/sec; 0.077 sec/batch)
2017-05-02 16:17:40.832764: step 63800, loss = 0.73 (1429.5 examples/sec; 0.090 sec/batch)
2017-05-02 16:17:41.523706: step 63810, loss = 0.83 (1852.6 examples/sec; 0.069 sec/batch)
2017-05-02 16:17:42.322623: step 63820, loss = 0.77 (1602.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:17:43.113563: step 63830, loss = 0.86 (1618.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:43.898249: step 63840, loss = 0.71 (1631.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:17:44.689387: step 63850, loss = 0.82 (1617.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:45.482555: step 63860, loss = 0.81 (1613.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:46.273474: step 63870, loss = 0.64 (1618.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:47.071740: step 63880, loss = 0.79 (1603.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:17:47.849239: step 63890, loss = 0.86 (1646.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:17:48.750531: step 63900, loss = 0.55 (1420.2 examples/sec; 0.090 sec/batch)
2017-05-02 16:17:49.446650: step 63910, loss = 0.66 (1838.8 examples/sec; 0.070 sec/batch)
2017-05-02 16:17:50.242698: step 63920, loss = 0.56 (1607.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:17:51.030322: step 63930, loss = 0.78 (1625.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:51.811270: step 63940, loss = 0.77 (1639.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:17:52.609589: step 63950, loss = 0.84 (1603.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:17:53.396424: step 63960, loss = 0.77 (1626.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:54.186988: step 63970, loss = 0.66 (1619.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:54.974912: step 63980, loss = 0.64 (1624.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:55.763117: step 63990, loss = 0.61 (1623.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:56.648458: step 64000, loss = 0.63 (1445.8 examples/sec; 0.089 sec/batch)
2017-05-02 16:17:57.346271: step 64010, loss = 0.76 (1834.3 examples/sec; 0.070 sec/batch)
2017-05-02 16:17:58.131534: step 64020, loss = 0.72 (1630.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:58.924764: step 64030, loss = 0.85 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:17:59.707959: step 64040, loss = 0.81 (1634.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:18:00.501622: step 64050, loss = 0.68 (1612.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:01.286006: step 64060, loss = 0.55 (1631.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:18:02.073438: step 64070, loss = 0.69 (1625.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:02.873288: step 64080, loss = 0.77 (1600.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:18:03.663559: step 64090, loss = 0.86 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:04.546201: step 64100, loss = 0.70 (1450.2 examples/sec; 0.088 sec/batch)
2017-05-02 16:18:05.240903: step 64110, loss = 0.71 (1842.5 examples/sec; 0.069 sec/batch)
2017-05-02 16:18:06.031344: step 64120, loss = 0.69 (1619.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:06.825972: step 64130, loss = 0.61 (1610.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:07.611847: step 64140, loss = 0.65 (1628.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:08.404237: step 64150, loss = 0.81 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:09.202837: step 64160, loss = 0.76 (1602.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:18:09.993796: step 64170, loss = 0.91 (1618.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:10.789957: step 64180, loss = 0.87 (1607.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:18:11.579254: step 64190, loss = 0.70 (1621.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:12.493188: step 64200, loss = 0.64 (1400.5 examples/sec; 0.091 sec/batch)
2017-05-02 16:18:13.163003: step 64210, loss = 0.79 (1911.0 examples/sec; 0.067 sec/batch)
2017-05-02 16:18:13.956835: step 64220, loss = 0.66 (1612.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:14.750681: step 64230, loss = 0.66 (1612.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:15.529210: step 64240, loss = 0.73 (1644.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:18:16.315353: step 64250, loss = 0.68 (1628.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:17.102949: step 64260, loss = 0.70 (1625.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:17.894638: step 64270, loss = 0.60 (1616.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:18.686135: step 64280, loss = 0.72 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:19.470332: step 64290, loss = 0.69 (1632.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:18:20.362571: step 64300, loss = 0.65 (1434.6 examples/sec; 0.089 sec/batch)
2017-05-02 16:18:21.056793: step 64310, loss = 0.57 (1843.8 examples/sec; 0.069 sec/batch)
2017-05-02 16:18:21.846425: step 64320, loss = 0.80 (1621.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:22.638193: step 64330, loss = 0.71 (1616.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:23.421842: step 64340, loss = 0.68 (1633.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:18:24.209061: step 64350, loss = 0.70 (1626.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:24.999961: step 64360, loss = 0.75 (1618.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:25.794183: step 64370, loss = 0.71 (1611.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:26.586365: step 64380, loss = 0.86 (1615.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:27.367400: step 64390, loss = 0.88 (1638.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:18:28.252661: step 64400, loss = 0.74 (1445.9 examples/sec; 0.089 sec/batch)
2017-05-02 16:18:28.946118: step 64410, loss = 0.71 (1845.8 examples/sec; 0.069 sec/batch)
2017-05-02 16:18:29.738997: step 64420, loss = 0.75 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:30.534140: step 64430, loss = 0.58 (1609.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:18:31.329007: step 64440, loss = 0.66 (1610.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:32.115846: step 64450, loss = 0.77 (1626.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:32.903702: step 64460, loss = 0.77 (1624.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:33.694376: step 64470, loss = 0.74 (1618.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:34.481255: step 64480, loss = 0.81 (1626.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:35.268037: step 64490, loss = 0.68 (1626.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:36.143786: step 64500, loss = 0.75 (1461.6 examples/sec; 0.088 sec/batch)
2017-05-02 16:18:36.837099: step 64510, loss = 0.70 (1846.2 examples/sec; 0.069 sec/batch)
2017-05-02 16:18:37.636035: step 64520, loss = 0.96 (1602.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:18:38.430498: step 64530, loss = 0.85 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:39.224719: step 64540, loss = 0.70 (1611.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:40.004187: step 64550, loss = 0.74 (1642.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:18:40.796114: step 64560, loss = 0.79 (1616.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:41.590426: step 64570, loss = 0.72 (1611.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:42.375370: step 64580, loss = 0.57 (1630.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:18:43.165950: step 64590, loss = 0.66 (1619.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:44.055042: step 64600, loss = 0.81 (1439.7 examples/sec; 0.089 sec/batch)
2017-05-02 16:18:44.741314: step 64610, loss = 0.63 (1865.2 examples/sec; 0.069 sec/batch)
2017-05-02 16:18:45.534254: step 64620, loss = 0.63 (1614.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:46.324825: step 64630, loss = 0.64 (1619.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:47.110514: step 64640, loss = 0.60 (1629.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:47.893618: step 64650, loss = 0.65 (1634.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:18:48.691147: step 64660, loss = 0.68 (1605.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:18:49.476311: step 64670, loss = 0.70 (1630.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:50.270235: step 64680, loss = 0.67 (1612.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:51.054085: step 64690, loss = 0.69 (1632.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:18:51.926993: step 64700, loss = 0.74 (1466.4 examples/sec; 0.087 sec/batch)
2017-05-02 16:18:52.631412: step 64710, loss = 0.69 (1817.1 examples/sec; 0.070 sec/batch)
2017-05-02 16:18:53.427937: step 64720, loss = 0.89 (1607.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:18:54.227210: step 64730, loss = 0.64 (1601.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:18:55.024540: step 64740, loss = 0.80 (1605.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:18:55.804692: step 64750, loss = 0.76 (1640.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:18:56.586305: step 64760, loss = 0.67 (1637.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:18:57.382831: step 64770, loss = 0.74 (1607.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:18:58.180788: step 64780, loss = 0.71 (1604.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:18:58.969537: step 64790, loss = 0.58 (1622.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:18:59.846479: step 64800, loss = 0.68 (1459.6 examples/sec; 0.088 sec/batch)
2017-05-02 16:19:00.545630: step 64810, loss = 0.68 (1830.8 examples/sec; 0.070 sec/batch)
2017-05-02 16:19:01.331017: step 64820, loss = 0.79 (1629.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:02.123866: step 64830, loss = 0.89 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:02.917865: step 64840, loss = 0.79 (1612.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:03.698428: step 64850, loss = 0.81 (1639.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:19:04.491590: step 64860, loss = 0.73 (1613.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:05.286670: step 64870, loss = 0.71 (1609.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:19:06.080647: step 64880, loss = 0.65 (1612.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:06.876080: step 64890, loss = 0.61 (1609.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:19:07.753808: step 64900, loss = 0.70 (1458.3 examples/sec; 0.088 sec/batch)
2017-05-02 16:19:08.445396: step 64910, loss = 0.64 (1850.8 examples/sec; 0.069 sec/batch)
2017-05-02 16:19:09.240850: step 64920, loss = 0.87 (1609.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:19:10.027821: step 64930, loss = 0.98 (1626.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:10.819103: step 64940, loss = 0.68 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:11.607507: step 64950, loss = 0.83 (1623.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:12.403708: step 64960, loss = 0.56 (1607.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:19:13.187469: step 64970, loss = 0.69 (1633.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:19:13.981486: step 64980, loss = 0.63 (1612.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:14.775128: step 64990, loss = 0.70 (1612.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:15.654861: step 65000, loss = 0.87 (1455.0 examples/sec; 0.088 sec/batch)
2017-05-02 16:19:16.347932: step 65010, loss = 0.63 (1846.8 examples/sec; 0.069 sec/batch)
2017-05-02 16:19:17.132384: step 65020, loss = 0.77 (1631.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:19:17.920793: step 65030, loss = 0.60 (1623.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:18.710003: step 65040, loss = 0.56 (1621.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:19.495699: step 65050, loss = 0.69 (1629.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:20.284104: step 65060, loss = 0.77 (1623.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:21.076492: step 65070, loss = 0.67 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:21.873068: step 65080, loss = 0.79 (1606.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:19:22.664171: step 65090, loss = 0.54 (1618.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:23.548902: step 65100, loss = 0.61 (1446.8 examples/sec; 0.088 sec/batch)
2017-05-02 16:19:24.244225: step 65110, loss = 0.65 (1840.8 examples/sec; 0.070 sec/batch)
2017-05-02 16:19:25.037607: step 65120, loss = 0.65 (1613.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:25.826394: step 65130, loss = 0.69 (1622.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:26.620165: step 65140, loss = 0.85 (1612.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:27.408215: step 65150, loss = 0.64 (1624.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:28.194370: step 65160, loss = 0.66 (1628.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:28.979084: step 65170, loss = 0.53 (1631.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:19:29.774974: step 65180, loss = 0.85 (1608.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:19:30.572818: step 65190, loss = 0.63 (1604.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:19:31.466609: step 65200, loss = 0.83 (1432.1 examples/sec; 0.089 sec/batch)
2017-05-02 16:19:32.148452: step 65210, loss = 0.66 (1877.3 examples/sec; 0.068 sec/batch)
2017-05-02 16:19:32.937655: step 65220, loss = 0.70 (1621.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:33.730255: step 65230, loss = 0.69 (1614.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:34.520308: step 65240, loss = 0.72 (1620.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:35.311950: step 65250, loss = 0.59 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:36.099766: step 65260, loss = 0.74 (1624.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:36.898463: step 65270, loss = 0.74 (1602.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:19:37.704590: step 65280, loss = 0.67 (1587.8 examples/sec; 0.081 sec/batch)
2017-05-02 16:19:38.499182: step 65290, loss = 0.63 (1610.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:39.388125: step 65300, loss = 0.66 (1439.9 examples/sec; 0.089 sec/batch)
2017-05-02 16:19:40.074934: step 65310, loss = 0.74 (1863.7 examples/sec; 0.069 sec/batch)
2017-05-02 16:19:40.870291: step 65320, loss = 0.60 (1609.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:19:41.660507: step 65330, loss = 0.83 (1619.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:42.456189: step 65340, loss = 0.77 (1608.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:19:43.245767: step 65350, loss = 0.79 (1621.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:44.035016: step 65360, loss = 0.73 (1621.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:44.825514: step 65370, loss = 0.72 (1619.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:45.617232: step 65380, loss = 0.70 (1616.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:46.409623: step 65390, loss = 0.77 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:47.299135: step 65400, loss = 0.86 (1439.0 examples/sec; 0.089 sec/batch)
2017-05-02 16:19:47.981800: step 65410, loss = 0.83 (1875.0 examples/sec; 0.068 sec/batch)
2017-05-02 16:19:48.779159: step 65420, loss = 0.83 (1605.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:19:49.568114: step 65430, loss = 0.84 (1622.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:50.355719: step 65440, loss = 0.61 (1625.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:51.148028: step 65450, loss = 0.57 (1615.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:51.926848: step 65460, loss = 0.68 (1643.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:19:52.721435: step 65470, loss = 0.62 (1610.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:53.512151: step 65480, loss = 0.75 (1618.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:54.297390: step 65490, loss = 0.74 (1630.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:55.189512: step 65500, loss = 0.74 (1434.8 examples/sec; 0.089 sec/batch)
2017-05-02 16:19:55.889109: step 65510, loss = 0.61 (1829.6 examples/sec; 0.070 sec/batch)
2017-05-02 16:19:56.683719: step 65520, loss = 0.74 (1610.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:57.471350: step 65530, loss = 0.62 (1625.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:58.259028: step 65540, loss = 0.68 (1625.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:59.045514: step 65550, loss = 0.69 (1627.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:19:59.827276: step 65560, loss = 0.66 (1637.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:20:00.606930: step 65570, loss = 0.84 (1641.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:20:01.392980: step 65580, loss = 0.75 (1628.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:02.195153: step 65590, loss = 0.75 (1595.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:20:03.109132: step 65600, loss = 0.77 (1400.5 examples/sec; 0.091 sec/batch)
2017-05-02 16:20:03.776319: step 65610, loss = 0.89 (1918.5 examples/sec; 0.067 sec/batch)
2017-05-02 16:20:04.560972: step 65620, loss = 0.90 (1631.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:20:05.354720: step 65630, loss = 0.61 (1612.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:06.148266: step 65640, loss = 0.67 (1613.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:06.943136: step 65650, loss = 0.76 (1610.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:07.724609: step 65660, loss = 0.58 (1637.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:20:08.516517: step 65670, loss = 0.79 (1616.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:09.314539: step 65680, loss = 0.74 (1604.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:20:10.102239: step 65690, loss = 0.75 (1625.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:10.989122: step 65700, loss = 0.72 (1443.3 examples/sec; 0.089 sec/batch)
2017-05-02 16:20:11.682674: step 65710, loss = 0.59 (1845.6 examples/sec; 0.069 sec/batch)
2017-05-02 16:20:12.474330: step 65720, loss = 0.70 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:13.265800: step 65730, loss = 0.82 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:14.055623: step 65740, loss = 0.68 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:14.848208: step 65750, loss = 0.78 (1615.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:15.633892: step 65760, loss = 0.64 (1629.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:16.424792: step 65770, loss = 0.67 (1618.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:17.214237: step 65780, loss = 0.71 (1621.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:18.007220: step 65790, loss = 0.71 (1614.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:18.893587: step 65800, loss = 0.74 (1444.1 examples/sec; 0.089 sec/batch)
2017-05-02 16:20:19.586414: step 65810, loss = 0.58 (1847.5 examples/sec; 0.069 sec/batch)
2017-05-02 16:20:20.383603: step 65820, loss = 0.66 (1605.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:20:21.189785: step 65830, loss = 0.73 (1587.7 examples/sec; 0.081 sec/batch)
2017-05-02 16:20:21.983343: step 65840, loss = 0.78 (1613.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:22.779197: step 65850, loss = 0.73 (1608.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:20:23.556051: step 65860, loss = 0.72 (1647.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:20:24.345618: step 65870, loss = 0.75 (1621.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:25.137155: step 65880, loss = 0.66 (1617.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:25.925763: step 65890, loss = 0.63 (1623.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:26.818318: step 65900, loss = 0.80 (1434.1 examples/sec; 0.089 sec/batch)
2017-05-02 16:20:27.508188: step 65910, loss = 0.73 (1855.4 examples/sec; 0.069 sec/batch)
2017-05-02 16:20:28.303327: step 65920, loss = 0.74 (1609.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:20:29.093238: step 65930, loss = 0.80 (1620.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:29.884183: step 65940, loss = 0.70 (1618.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:30.679739: step 65950, loss = 0.68 (1608.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:20:31.467070: step 65960, loss = 0.65 (1625.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:32.251227: step 65970, loss = 0.71 (1632.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:20:33.041561: step 65980, loss = 0.77 (1619.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:33.834681: step 65990, loss = 0.70 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:34.717275: step 66000, loss = 0.75 (1450.3 examples/sec; 0.088 sec/batch)
2017-05-02 16:20:35.417621: step 66010, loss = 0.58 (1827.7 examples/sec; 0.070 sec/batch)
2017-05-02 16:20:36.197873: step 66020, loss = 0.64 (1640.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:20:36.988143: step 66030, loss = 0.75 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:37.777463: step 66040, loss = 0.63 (1621.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:38.567957: step 66050, loss = 0.65 (1619.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:39.365260: step 66060, loss = 0.78 (1605.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:20:40.151232: step 66070, loss = 0.66 (1628.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:40.937737: step 66080, loss = 0.71 (1627.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:41.723104: step 66090, loss = 0.62 (1629.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:42.617775: step 66100, loss = 0.78 (1430.7 examples/sec; 0.089 sec/batch)
2017-05-02 16:20:43.317949: step 66110, loss = 0.78 (1828.1 examples/sec; 0.070 sec/batch)
2017-05-02 16:20:44.103753: step 66120, loss = 0.66 (1628.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:44.893028: step 66130, loss = 0.76 (1621.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:45.685572: step 66140, loss = 0.66 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:46.474802: step 66150, loss = 0.94 (1621.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:47.267337: step 66160, loss = 0.59 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:48.043489: step 66170, loss = 0.74 (1649.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:20:48.832717: step 66180, loss = 0.62 (1621.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:49.627326: step 66190, loss = 0.70 (1610.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:50.512199: step 66200, loss = 0.57 (1446.5 examples/sec; 0.088 sec/batch)
2017-05-02 16:20:51.212296: step 66210, loss = 0.81 (1828.3 examples/sec; 0.070 sec/batch)
2017-05-02 16:20:51.985079: step 66220, loss = 0.72 (1656.4 examples/sec; 0.077 sec/batch)
2017-05-02 16:20:52.777029: step 66230, loss = 0.69 (1616.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:53.563686: step 66240, loss = 0.77 (1627.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:54.355207: step 66250, loss = 0.74 (1617.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:55.146258: step 66260, loss = 0.68 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:55.930542: step 66270, loss = 0.82 (1632.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:20:56.722402: step 66280, loss = 0.70 (1616.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:57.515646: step 66290, loss = 0.80 (1613.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:20:58.420172: step 66300, loss = 0.72 (1415.1 examples/sec; 0.090 sec/batch)
2017-05-02 16:20:59.100947: step 66310, loss = 0.80 (1880.2 examples/sec; 0.068 sec/batch)
2017-05-02 16:20:59.878862: step 66320, loss = 0.71 (1645.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:21:00.667517: step 66330, loss = 0.67 (1623.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:01.459234: step 66340, loss = 0.77 (1616.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:02.241733: step 66350, loss = 0.80 (1635.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:21:03.039277: step 66360, loss = 0.77 (1604.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:21:03.818500: step 66370, loss = 0.60 (1642.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:21:04.611603: step 66380, loss = 0.68 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:05.406935: step 66390, loss = 0.61 (1609.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:21:06.285824: step 66400, loss = 0.65 (1456.4 examples/sec; 0.088 sec/batch)
2017-05-02 16:21:06.988263: step 66410, loss = 0.69 (1822.2 examples/sec; 0.070 sec/batch)
2017-05-02 16:21:07.774260: step 66420, loss = 0.66 (1628.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:08.563796: step 66430, loss = 0.70 (1621.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:09.355812: step 66440, loss = 0.83 (1616.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:10.147213: step 66450, loss = 0.64 (1617.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:10.939931: step 66460, loss = 0.70 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:11.716598: step 66470, loss = 0.73 (1648.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:21:12.510700: step 66480, loss = 0.71 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:13.302411: step 66490, loss = 0.62 (1616.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:14.197413: step 66500, loss = 0.77 (1430.2 examples/sec; 0.090 sec/batch)
2017-05-02 16:21:14.882683: step 66510, loss = 0.89 (1867.9 examples/sec; 0.069 sec/batch)
2017-05-02 16:21:15.669894: step 66520, loss = 0.58 (1626.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:16.465915: step 66530, loss = 0.63 (1608.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:21:17.259707: step 66540, loss = 0.59 (1612.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:18.047106: step 66550, loss = 0.59 (1625.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:18.829410: step 66560, loss = 0.72 (1636.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:21:19.611515: step 66570, loss = 0.60 (1636.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:21:20.397052: step 66580, loss = 0.66 (1629.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:21.187190: step 66590, loss = 0.72 (1620.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:22.075860: step 66600, loss = 0.75 (1440.4 examples/sec; 0.089 sec/batch)
2017-05-02 16:21:22.770579: step 66610, loss = 0.67 (1842.5 examples/sec; 0.069 sec/batch)
2017-05-02 16:21:23.552363: step 66620, loss = 0.70 (1637.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:21:24.344558: step 66630, loss = 0.56 (1615.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:25.144381: step 66640, loss = 0.67 (1600.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:21:25.936185: step 66650, loss = 0.75 (1616.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:26.725261: step 66660, loss = 0.69 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:27.510412: step 66670, loss = 0.70 (1630.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:28.296687: step 66680, loss = 0.64 (1627.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:29.087381: step 66690, loss = 0.63 (1618.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:29.980254: step 66700, loss = 0.80 (1433.6 examples/sec; 0.089 sec/batch)
2017-05-02 16:21:30.672417: step 66710, loss = 0.74 (1849.3 examples/sec; 0.069 sec/batch)
2017-05-02 16:21:31.453633: step 66720, loss = 0.71 (1638.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:21:32.241725: step 66730, loss = 0.85 (1624.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:33.038252: step 66740, loss = 0.75 (1607.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:21:33.828394: step 66750, loss = 0.81 (1620.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:34.621391: step 66760, loss = 0.93 (1614.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:35.407169: step 66770, loss = 0.79 (1628.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:36.181734: step 66780, loss = 0.68 (1652.5 examples/sec; 0.077 sec/batch)
2017-05-02 16:21:36.974268: step 66790, loss = 0.69 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:37.861160: step 66800, loss = 0.61 (1443.2 examples/sec; 0.089 sec/batch)
2017-05-02 16:21:38.552718: step 66810, loss = 0.75 (1850.9 examples/sec; 0.069 sec/batch)
2017-05-02 16:21:39.336980: step 66820, loss = 0.72 (1632.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:21:40.122009: step 66830, loss = 0.68 (1630.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:40.910344: step 66840, loss = 0.71 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:41.698676: step 66850, loss = 0.62 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:42.490286: step 66860, loss = 0.73 (1617.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:43.292290: step 66870, loss = 0.61 (1596.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:21:44.080892: step 66880, loss = 0.73 (1623.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:44.872869: step 66890, loss = 0.78 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:45.762141: step 66900, loss = 0.86 (1439.4 examples/sec; 0.089 sec/batch)
2017-05-02 16:21:46.454754: step 66910, loss = 0.71 (1848.1 examples/sec; 0.069 sec/batch)
2017-05-02 16:21:47.242906: step 66920, loss = 0.70 (1624.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:48.029753: step 66930, loss = 0.78 (1626.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:48.816905: step 66940, loss = 0.69 (1626.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:49.614152: step 66950, loss = 0.90 (1605.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:21:50.402743: step 66960, loss = 0.71 (1623.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:51.188185: step 66970, loss = 0.71 (1629.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:51.969573: step 66980, loss = 0.92 (1638.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:21:52.767290: step 66990, loss = 0.75 (1604.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:21:53.654544: step 67000, loss = 0.67 (1442.7 examples/sec; 0.089 sec/batch)
2017-05-02 16:21:54.341502: step 67010, loss = 0.69 (1863.3 examples/sec; 0.069 sec/batch)
2017-05-02 16:21:55.126948: step 67020, loss = 0.69 (1629.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:55.903630: step 67030, loss = 0.66 (1648.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:21:56.698032: step 67040, loss = 0.69 (1611.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:57.489050: step 67050, loss = 0.79 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:58.275631: step 67060, loss = 0.75 (1627.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:59.066927: step 67070, loss = 0.64 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:21:59.848501: step 67080, loss = 0.78 (1637.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:22:00.643031: step 67090, loss = 0.59 (1611.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:01.524386: step 67100, loss = 0.88 (1452.3 examples/sec; 0.088 sec/batch)
2017-05-02 16:22:02.218867: step 67110, loss = 0.73 (1843.1 examples/sec; 0.069 sec/batch)
2017-05-02 16:22:03.011921: step 67120, loss = 0.73 (1614.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:03.793218: step 67130, loss = 0.78 (1638.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:22:04.587297: step 67140, loss = 0.58 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:05.386698: step 67150, loss = 0.69 (1601.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:22:06.172700: step 67160, loss = 0.79 (1628.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:06.960001: step 67170, loss = 0.69 (1625.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:07.745154: step 67180, loss = 0.71 (1630.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:08.532902: step 67190, loss = 0.70 (1624.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:09.415124: step 67200, loss = 0.79 (1450.9 examples/sec; 0.088 sec/batch)
2017-05-02 16:22:10.113059: step 67210, loss = 0.77 (1834.0 examples/sec; 0.070 sec/batch)
2017-05-02 16:22:10.910098: step 67220, loss = 0.81 (1606.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:22:11.695232: step 67230, loss = 0.65 (1630.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:12.487222: step 67240, loss = 0.78 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:13.273601: step 67250, loss = 0.75 (1627.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:14.058717: step 67260, loss = 0.69 (1630.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:14.851116: step 67270, loss = 0.77 (1615.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:15.636937: step 67280, loss = 0.76 (1628.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:16.426252: step 67290, loss = 0.69 (1621.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:17.310096: step 67300, loss = 0.66 (1448.2 examples/sec; 0.088 sec/batch)
2017-05-02 16:22:17.999185: step 67310, loss = 0.70 (1857.5 examples/sec; 0.069 sec/batch)
2017-05-02 16:22:18.791644: step 67320, loss = 0.83 (1615.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:19.576446: step 67330, loss = 0.62 (1631.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:22:20.363715: step 67340, loss = 0.85 (1625.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:21.157832: step 67350, loss = 0.70 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:21.948191: step 67360, loss = 0.75 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:22.744380: step 67370, loss = 0.88 (1607.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:22:23.528109: step 67380, loss = 0.75 (1633.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:22:24.321176: step 67390, loss = 0.69 (1614.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:25.215068: step 67400, loss = 0.74 (1431.9 examples/sec; 0.089 sec/batch)
2017-05-02 16:22:25.917284: step 67410, loss = 0.75 (1822.8 examples/sec; 0.070 sec/batch)
2017-05-02 16:22:26.705509: step 67420, loss = 0.67 (1623.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:27.491184: step 67430, loss = 0.88 (1629.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:28.278131: step 67440, loss = 0.74 (1626.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:29.075229: step 67450, loss = 0.72 (1605.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:22:29.866008: step 67460, loss = 0.81 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:30.656044: step 67470, loss = 0.72 (1620.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:31.453065: step 67480, loss = 0.64 (1606.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:22:32.240031: step 67490, loss = 0.68 (1626.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:33.138520: step 67500, loss = 0.80 (1424.6 examples/sec; 0.090 sec/batch)
2017-05-02 16:22:33.832801: step 67510, loss = 0.73 (1843.6 examples/sec; 0.069 sec/batch)
2017-05-02 16:22:34.620632: step 67520, loss = 0.80 (1624.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:35.408197: step 67530, loss = 0.78 (1625.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:36.194709: step 67540, loss = 0.82 (1627.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:36.986831: step 67550, loss = 0.68 (1615.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:37.775742: step 67560, loss = 0.77 (1622.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:38.576643: step 67570, loss = 0.75 (1598.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:22:39.364660: step 67580, loss = 0.80 (1624.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:40.148262: step 67590, loss = 0.86 (1633.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:22:41.039571: step 67600, loss = 0.60 (1436.1 examples/sec; 0.089 sec/batch)
2017-05-02 16:22:41.737032: step 67610, loss = 0.62 (1835.2 examples/sec; 0.070 sec/batch)
2017-05-02 16:22:42.530234: step 67620, loss = 0.71 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:43.315344: step 67630, loss = 0.66 (1630.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:44.100594: step 67640, loss = 0.67 (1630.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:44.890582: step 67650, loss = 0.96 (1620.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:45.680253: step 67660, loss = 0.70 (1620.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:46.479128: step 67670, loss = 0.64 (1602.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:22:47.264532: step 67680, loss = 0.73 (1629.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:48.048914: step 67690, loss = 0.65 (1631.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:22:48.944234: step 67700, loss = 0.54 (1429.7 examples/sec; 0.090 sec/batch)
2017-05-02 16:22:49.645807: step 67710, loss = 0.88 (1824.5 examples/sec; 0.070 sec/batch)
2017-05-02 16:22:50.436640: step 67720, loss = 0.77 (1618.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:51.231063: step 67730, loss = 0.69 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:52.007335: step 67740, loss = 0.79 (1648.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:22:52.797867: step 67750, loss = 0.82 (1619.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:53.588535: step 67760, loss = 0.85 (1618.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:54.383179: step 67770, loss = 0.63 (1610.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:55.181323: step 67780, loss = 0.76 (1603.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:22:55.962544: step 67790, loss = 0.68 (1638.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:22:56.853116: step 67800, loss = 0.82 (1437.3 examples/sec; 0.089 sec/batch)
2017-05-02 16:22:57.553466: step 67810, loss = 0.83 (1827.6 examples/sec; 0.070 sec/batch)
2017-05-02 16:22:58.341611: step 67820, loss = 0.60 (1624.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:59.132120: step 67830, loss = 0.77 (1619.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:22:59.909676: step 67840, loss = 0.68 (1646.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:23:00.703272: step 67850, loss = 0.60 (1612.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:01.491429: step 67860, loss = 0.87 (1624.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:02.287723: step 67870, loss = 0.74 (1607.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:23:03.078941: step 67880, loss = 0.76 (1617.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:03.860248: step 67890, loss = 0.80 (1638.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:23:04.749931: step 67900, loss = 0.76 (1438.7 examples/sec; 0.089 sec/batch)
2017-05-02 16:23:05.445181: step 67910, loss = 0.73 (1841.1 examples/sec; 0.070 sec/batch)
2017-05-02 16:23:06.239040: step 67920, loss = 0.77 (1612.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:07.026116: step 67930, loss = 0.85 (1626.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:07.808133: step 67940, loss = 0.98 (1636.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:23:08.597663: step 67950, loss = 0.77 (1621.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:09.388295: step 67960, loss = 0.67 (1619.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:10.178681: step 67970, loss = 0.51 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:10.969547: step 67980, loss = 0.81 (1618.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:11.759931: step 67990, loss = 0.54 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:12.681463: step 68000, loss = 0.72 (1389.0 examples/sec; 0.092 sec/batch)
2017-05-02 16:23:13.357008: step 68010, loss = 0.58 (1894.8 examples/sec; 0.068 sec/batch)
2017-05-02 16:23:14.158675: step 68020, loss = 0.78 (1596.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:23:14.945566: step 68030, loss = 0.69 (1626.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:15.730812: step 68040, loss = 0.74 (1630.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:16.520384: step 68050, loss = 0.73 (1621.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:17.314714: step 68060, loss = 0.65 (1611.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:18.108784: step 68070, loss = 0.59 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:18.899403: step 68080, loss = 0.70 (1619.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:19.677502: step 68090, loss = 0.81 (1645.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:23:20.575508: step 68100, loss = 0.66 (1425.4 examples/sec; 0.090 sec/batch)
2017-05-02 16:23:21.270749: step 68110, loss = 0.85 (1841.1 examples/sec; 0.070 sec/batch)
2017-05-02 16:23:22.052382: step 68120, loss = 0.82 (1637.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:23:22.849083: step 68130, loss = 0.73 (1606.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:23:23.628464: step 68140, loss = 0.66 (1642.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:23:24.423035: step 68150, loss = 0.64 (1611.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:25.214331: step 68160, loss = 0.69 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:26.006759: step 68170, loss = 0.74 (1615.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:26.797312: step 68180, loss = 0.70 (1619.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:27.584557: step 68190, loss = 0.88 (1625.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:28.476392: step 68200, loss = 0.60 (1435.2 examples/sec; 0.089 sec/batch)
2017-05-02 16:23:29.172201: step 68210, loss = 0.70 (1839.6 examples/sec; 0.070 sec/batch)
2017-05-02 16:23:30.000906: step 68220, loss = 0.66 (1544.6 examples/sec; 0.083 sec/batch)
2017-05-02 16:23:30.800378: step 68230, loss = 0.60 (1601.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:23:31.589727: step 68240, loss = 0.77 (1621.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:32.379662: step 68250, loss = 0.70 (1620.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:33.171580: step 68260, loss = 0.80 (1616.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:33.956294: step 68270, loss = 0.73 (1631.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:23:34.744771: step 68280, loss = 0.77 (1623.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:35.535547: step 68290, loss = 0.74 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:36.428211: step 68300, loss = 0.74 (1433.9 examples/sec; 0.089 sec/batch)
2017-05-02 16:23:37.123735: step 68310, loss = 0.79 (1840.3 examples/sec; 0.070 sec/batch)
2017-05-02 16:23:37.909573: step 68320, loss = 0.74 (1628.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:38.698521: step 68330, loss = 0.64 (1622.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:39.488109: step 68340, loss = 0.70 (1621.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:40.275668: step 68350, loss = 0.70 (1625.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:41.065377: step 68360, loss = 0.73 (1620.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:41.863753: step 68370, loss = 0.68 (1603.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:23:42.654426: step 68380, loss = 0.92 (1618.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:43.438873: step 68390, loss = 0.68 (1631.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:23:44.324284: step 68400, loss = 0.69 (1445.7 examples/sec; 0.089 sec/batch)
2017-05-02 16:23:45.018508: step 68410, loss = 0.82 (1843.7 examples/sec; 0.069 sec/batch)
2017-05-02 16:23:45.808916: step 68420, loss = 0.76 (1619.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:46.604446: step 68430, loss = 0.61 (1609.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:23:47.381674: step 68440, loss = 0.67 (1646.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:23:48.162658: step 68450, loss = 0.63 (1639.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:23:48.947449: step 68460, loss = 0.58 (1631.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:23:49.731299: step 68470, loss = 0.90 (1633.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:23:50.522077: step 68480, loss = 0.75 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:51.315314: step 68490, loss = 0.77 (1613.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:52.189228: step 68500, loss = 0.64 (1464.7 examples/sec; 0.087 sec/batch)
2017-05-02 16:23:52.888182: step 68510, loss = 0.68 (1831.3 examples/sec; 0.070 sec/batch)
2017-05-02 16:23:53.680303: step 68520, loss = 0.72 (1615.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:54.469450: step 68530, loss = 0.84 (1622.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:55.264780: step 68540, loss = 0.77 (1609.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:23:56.050028: step 68550, loss = 0.83 (1630.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:56.847179: step 68560, loss = 0.73 (1605.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:23:57.645291: step 68570, loss = 0.70 (1603.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:23:58.433865: step 68580, loss = 0.76 (1623.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:23:59.224032: step 68590, loss = 0.76 (1619.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:00.111830: step 68600, loss = 0.75 (1441.8 examples/sec; 0.089 sec/batch)
2017-05-02 16:24:00.793249: step 68610, loss = 0.80 (1878.4 examples/sec; 0.068 sec/batch)
2017-05-02 16:24:01.584248: step 68620, loss = 0.74 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:02.377072: step 68630, loss = 0.66 (1614.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:03.174050: step 68640, loss = 0.91 (1606.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:24:03.950387: step 68650, loss = 0.68 (1648.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:24:04.738305: step 68660, loss = 0.67 (1624.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:05.527008: step 68670, loss = 0.82 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:06.316325: step 68680, loss = 0.72 (1621.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:07.110835: step 68690, loss = 0.89 (1611.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:07.988922: step 68700, loss = 0.79 (1457.7 examples/sec; 0.088 sec/batch)
2017-05-02 16:24:08.703041: step 68710, loss = 0.79 (1792.4 examples/sec; 0.071 sec/batch)
2017-05-02 16:24:09.491653: step 68720, loss = 0.61 (1623.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:10.277626: step 68730, loss = 0.74 (1628.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:11.069095: step 68740, loss = 0.58 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:11.850018: step 68750, loss = 0.83 (1639.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:24:12.641343: step 68760, loss = 0.72 (1617.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:13.431200: step 68770, loss = 0.71 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:14.228060: step 68780, loss = 0.65 (1606.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:24:15.022042: step 68790, loss = 0.62 (1612.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:15.895577: step 68800, loss = 0.68 (1465.3 examples/sec; 0.087 sec/batch)
2017-05-02 16:24:16.587822: step 68810, loss = 0.75 (1849.1 examples/sec; 0.069 sec/batch)
2017-05-02 16:24:17.375060: step 68820, loss = 0.94 (1625.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:18.161513: step 68830, loss = 0.75 (1627.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:18.947465: step 68840, loss = 0.91 (1628.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:19.731738: step 68850, loss = 0.60 (1632.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:24:20.522933: step 68860, loss = 0.62 (1617.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:21.315270: step 68870, loss = 0.75 (1615.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:22.106475: step 68880, loss = 0.67 (1617.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:22.896577: step 68890, loss = 0.80 (1620.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:23.793277: step 68900, loss = 0.75 (1427.5 examples/sec; 0.090 sec/batch)
2017-05-02 16:24:24.477691: step 68910, loss = 0.78 (1870.2 examples/sec; 0.068 sec/batch)
2017-05-02 16:24:25.268738: step 68920, loss = 0.78 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:26.066939: step 68930, loss = 0.73 (1603.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:24:26.856915: step 68940, loss = 0.71 (1620.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:27.639770: step 68950, loss = 0.73 (1635.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:24:28.432607: step 68960, loss = 0.64 (1614.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:29.220036: step 68970, loss = 0.77 (1625.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:30.007035: step 68980, loss = 0.69 (1626.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:30.793426: step 68990, loss = 0.78 (1627.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:31.679833: step 69000, loss = 0.74 (1444.1 examples/sec; 0.089 sec/batch)
2017-05-02 16:24:32.374196: step 69010, loss = 0.78 (1843.4 examples/sec; 0.069 sec/batch)
2017-05-02 16:24:33.166717: step 69020, loss = 0.63 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:33.959524: step 69030, loss = 0.85 (1614.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:34.749824: step 69040, loss = 0.76 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:35.532733: step 69050, loss = 0.78 (1634.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:24:36.318984: step 69060, loss = 0.80 (1628.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:37.104804: step 69070, loss = 0.91 (1628.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:37.895746: step 69080, loss = 0.59 (1618.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:38.683819: step 69090, loss = 0.79 (1624.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:39.570205: step 69100, loss = 0.66 (1444.1 examples/sec; 0.089 sec/batch)
2017-05-02 16:24:40.258322: step 69110, loss = 0.86 (1860.1 examples/sec; 0.069 sec/batch)
2017-05-02 16:24:41.049860: step 69120, loss = 0.66 (1617.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:41.842934: step 69130, loss = 0.65 (1614.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:42.630972: step 69140, loss = 0.76 (1624.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:43.417932: step 69150, loss = 0.69 (1626.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:44.198590: step 69160, loss = 0.65 (1639.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:24:44.991189: step 69170, loss = 0.82 (1614.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:45.776281: step 69180, loss = 0.72 (1630.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:46.572240: step 69190, loss = 0.78 (1608.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:24:47.457399: step 69200, loss = 0.81 (1446.1 examples/sec; 0.089 sec/batch)
2017-05-02 16:24:48.150385: step 69210, loss = 0.76 (1847.0 examples/sec; 0.069 sec/batch)
2017-05-02 16:24:48.941232: step 69220, loss = 0.79 (1618.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:49.727224: step 69230, loss = 0.60 (1628.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:50.518624: step 69240, loss = 0.72 (1617.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:51.306258: step 69250, loss = 0.74 (1625.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:52.091909: step 69260, loss = 0.88 (1629.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:52.882481: step 69270, loss = 0.88 (1619.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:53.672725: step 69280, loss = 0.73 (1619.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:54.465242: step 69290, loss = 0.65 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:55.355394: step 69300, loss = 0.70 (1438.0 examples/sec; 0.089 sec/batch)
2017-05-02 16:24:56.032405: step 69310, loss = 0.70 (1890.7 examples/sec; 0.068 sec/batch)
2017-05-02 16:24:56.819598: step 69320, loss = 0.82 (1626.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:57.608920: step 69330, loss = 0.71 (1621.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:58.400093: step 69340, loss = 0.67 (1617.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:59.192199: step 69350, loss = 0.71 (1615.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:24:59.974065: step 69360, loss = 0.71 (1637.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:25:00.758863: step 69370, loss = 0.63 (1631.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:25:01.548138: step 69380, loss = 0.80 (1621.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:02.340515: step 69390, loss = 0.59 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:03.244236: step 69400, loss = 0.79 (1416.4 examples/sec; 0.090 sec/batch)
2017-05-02 16:25:03.925463: step 69410, loss = 0.82 (1879.0 examples/sec; 0.068 sec/batch)
2017-05-02 16:25:04.713588: step 69420, loss = 0.83 (1624.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:05.504153: step 69430, loss = 0.81 (1619.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:06.293051: step 69440, loss = 0.71 (1622.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:07.085790: step 69450, loss = 0.61 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:07.860329: step 69460, loss = 0.78 (1652.6 examples/sec; 0.077 sec/batch)
2017-05-02 16:25:08.653812: step 69470, loss = 0.70 (1613.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:09.446842: step 69480, loss = 0.85 (1614.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:10.235940: step 69490, loss = 0.71 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:11.123048: step 69500, loss = 0.87 (1442.9 examples/sec; 0.089 sec/batch)
2017-05-02 16:25:11.805774: step 69510, loss = 0.85 (1874.9 examples/sec; 0.068 sec/batch)
2017-05-02 16:25:12.599364: step 69520, loss = 0.77 (1612.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:13.387043: step 69530, loss = 0.64 (1625.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:14.173842: step 69540, loss = 0.66 (1626.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:14.961840: step 69550, loss = 0.59 (1624.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:15.743045: step 69560, loss = 0.71 (1638.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:25:16.529781: step 69570, loss = 0.66 (1627.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:17.325700: step 69580, loss = 0.67 (1608.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:25:18.112066: step 69590, loss = 0.64 (1627.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:19.002404: step 69600, loss = 0.71 (1437.7 examples/sec; 0.089 sec/batch)
2017-05-02 16:25:19.684796: step 69610, loss = 0.66 (1875.7 examples/sec; 0.068 sec/batch)
2017-05-02 16:25:20.482481: step 69620, loss = 0.71 (1604.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:25:21.271165: step 69630, loss = 0.63 (1623.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:22.074350: step 69640, loss = 0.86 (1593.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:25:22.872081: step 69650, loss = 0.54 (1604.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:25:23.652597: step 69660, loss = 0.88 (1639.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:25:24.444461: step 69670, loss = 0.89 (1616.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:25.240371: step 69680, loss = 0.90 (1608.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:25:26.033941: step 69690, loss = 0.62 (1613.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:26.934287: step 69700, loss = 0.72 (1421.7 examples/sec; 0.090 sec/batch)
2017-05-02 16:25:27.612325: step 69710, loss = 0.68 (1887.8 examples/sec; 0.068 sec/batch)
2017-05-02 16:25:28.402114: step 69720, loss = 0.65 (1620.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:29.192070: step 69730, loss = 0.64 (1620.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:29.986854: step 69740, loss = 0.68 (1610.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:30.779263: step 69750, loss = 0.75 (1615.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:31.559169: step 69760, loss = 0.62 (1641.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:25:32.339303: step 69770, loss = 0.78 (1640.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:25:33.130040: step 69780, loss = 0.70 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:33.918046: step 69790, loss = 0.85 (1624.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:34.841326: step 69800, loss = 1.04 (1386.4 examples/sec; 0.092 sec/batch)
2017-05-02 16:25:35.509746: step 69810, loss = 0.82 (1915.0 examples/sec; 0.067 sec/batch)
2017-05-02 16:25:36.299780: step 69820, loss = 0.68 (1620.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:37.084381: step 69830, loss = 0.68 (1631.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:25:37.874733: step 69840, loss = 0.72 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:38.662427: step 69850, loss = 0.67 (1625.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:39.446632: step 69860, loss = 0.65 (1632.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:25:40.239056: step 69870, loss = 0.84 (1615.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:41.026054: step 69880, loss = 0.65 (1626.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:41.818516: step 69890, loss = 0.63 (1615.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:42.710431: step 69900, loss = 0.80 (1435.1 examples/sec; 0.089 sec/batch)
2017-05-02 16:25:43.400520: step 69910, loss = 0.67 (1854.8 examples/sec; 0.069 sec/batch)
2017-05-02 16:25:44.185184: step 69920, loss = 0.66 (1631.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:25:44.978672: step 69930, loss = 0.73 (1613.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:45.766362: step 69940, loss = 0.72 (1625.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:46.550895: step 69950, loss = 0.65 (1631.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:25:47.335450: step 69960, loss = 0.78 (1631.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:25:48.116324: step 69970, loss = 0.70 (1639.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:25:48.906827: step 69980, loss = 0.68 (1619.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:49.702529: step 69990, loss = 0.84 (1608.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:25:50.597345: step 70000, loss = 0.65 (1430.5 examples/sec; 0.089 sec/batch)
2017-05-02 16:25:51.286975: step 70010, loss = 0.70 (1856.1 examples/sec; 0.069 sec/batch)
2017-05-02 16:25:52.074484: step 70020, loss = 0.70 (1625.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:52.871824: step 70030, loss = 0.65 (1605.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:25:53.659139: step 70040, loss = 0.78 (1625.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:54.458500: step 70050, loss = 0.71 (1601.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:25:55.248965: step 70060, loss = 0.76 (1619.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:56.022617: step 70070, loss = 0.75 (1654.5 examples/sec; 0.077 sec/batch)
2017-05-02 16:25:56.816289: step 70080, loss = 0.72 (1612.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:57.604241: step 70090, loss = 0.79 (1624.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:25:58.494377: step 70100, loss = 0.71 (1438.0 examples/sec; 0.089 sec/batch)
2017-05-02 16:25:59.193472: step 70110, loss = 0.68 (1830.9 examples/sec; 0.070 sec/batch)
2017-05-02 16:25:59.979871: step 70120, loss = 0.63 (1627.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:00.764168: step 70130, loss = 0.73 (1632.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:26:01.552098: step 70140, loss = 0.63 (1624.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:02.340313: step 70150, loss = 0.70 (1623.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:03.130296: step 70160, loss = 0.81 (1620.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:03.913316: step 70170, loss = 0.68 (1634.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:26:04.705631: step 70180, loss = 0.69 (1615.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:05.505487: step 70190, loss = 0.66 (1600.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:26:06.406747: step 70200, loss = 0.69 (1420.2 examples/sec; 0.090 sec/batch)
2017-05-02 16:26:07.095995: step 70210, loss = 0.81 (1857.1 examples/sec; 0.069 sec/batch)
2017-05-02 16:26:07.882938: step 70220, loss = 0.63 (1626.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:08.672016: step 70230, loss = 0.71 (1622.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:09.461143: step 70240, loss = 0.75 (1622.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:10.255153: step 70250, loss = 0.75 (1612.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:11.043816: step 70260, loss = 0.80 (1623.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:11.824606: step 70270, loss = 0.87 (1639.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:26:12.619054: step 70280, loss = 0.70 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:13.410319: step 70290, loss = 0.67 (1617.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:14.315646: step 70300, loss = 0.71 (1413.8 examples/sec; 0.091 sec/batch)
2017-05-02 16:26:14.992563: step 70310, loss = 0.73 (1891.0 examples/sec; 0.068 sec/batch)
2017-05-02 16:26:15.777953: step 70320, loss = 0.56 (1629.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:16.569768: step 70330, loss = 0.62 (1616.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:17.359504: step 70340, loss = 0.81 (1620.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:18.145837: step 70350, loss = 0.77 (1627.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:18.941000: step 70360, loss = 0.68 (1609.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:26:19.718128: step 70370, loss = 0.69 (1647.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:26:20.510616: step 70380, loss = 0.68 (1615.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:21.305693: step 70390, loss = 0.68 (1609.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:26:22.193845: step 70400, loss = 0.75 (1441.2 examples/sec; 0.089 sec/batch)
2017-05-02 16:26:22.888574: step 70410, loss = 0.63 (1842.4 examples/sec; 0.069 sec/batch)
2017-05-02 16:26:23.668084: step 70420, loss = 0.65 (1642.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:26:24.462199: step 70430, loss = 0.63 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:25.259769: step 70440, loss = 0.79 (1604.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:26:26.055266: step 70450, loss = 0.58 (1609.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:26:26.850203: step 70460, loss = 0.72 (1610.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:27.630148: step 70470, loss = 0.69 (1641.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:26:28.425762: step 70480, loss = 0.66 (1608.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:26:29.217045: step 70490, loss = 0.55 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:30.102008: step 70500, loss = 0.75 (1446.4 examples/sec; 0.088 sec/batch)
2017-05-02 16:26:30.805525: step 70510, loss = 0.71 (1819.4 examples/sec; 0.070 sec/batch)
2017-05-02 16:26:31.585229: step 70520, loss = 0.71 (1641.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:26:32.373136: step 70530, loss = 0.57 (1624.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:33.167711: step 70540, loss = 0.91 (1610.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:33.959370: step 70550, loss = 0.93 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:34.748407: step 70560, loss = 0.74 (1622.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:35.534938: step 70570, loss = 0.85 (1627.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:36.325601: step 70580, loss = 0.67 (1618.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:37.122306: step 70590, loss = 0.68 (1606.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:26:38.016706: step 70600, loss = 0.59 (1431.1 examples/sec; 0.089 sec/batch)
2017-05-02 16:26:38.707979: step 70610, loss = 0.72 (1851.7 examples/sec; 0.069 sec/batch)
2017-05-02 16:26:39.492967: step 70620, loss = 0.71 (1630.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:26:40.279684: step 70630, loss = 0.68 (1627.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:41.073535: step 70640, loss = 0.63 (1612.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:41.860088: step 70650, loss = 0.87 (1627.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:42.651369: step 70660, loss = 0.59 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:43.442607: step 70670, loss = 0.82 (1617.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:44.232161: step 70680, loss = 0.57 (1621.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:45.019212: step 70690, loss = 0.77 (1626.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:45.911289: step 70700, loss = 0.61 (1434.8 examples/sec; 0.089 sec/batch)
2017-05-02 16:26:46.606126: step 70710, loss = 0.62 (1842.2 examples/sec; 0.069 sec/batch)
2017-05-02 16:26:47.392411: step 70720, loss = 0.66 (1627.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:48.182859: step 70730, loss = 0.71 (1619.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:48.980054: step 70740, loss = 0.81 (1605.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:26:49.775276: step 70750, loss = 0.67 (1609.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:26:50.570455: step 70760, loss = 0.69 (1609.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:26:51.357127: step 70770, loss = 0.80 (1627.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:52.144332: step 70780, loss = 0.71 (1626.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:52.932707: step 70790, loss = 0.84 (1623.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:53.818355: step 70800, loss = 0.73 (1445.3 examples/sec; 0.089 sec/batch)
2017-05-02 16:26:54.515673: step 70810, loss = 0.77 (1835.6 examples/sec; 0.070 sec/batch)
2017-05-02 16:26:55.300143: step 70820, loss = 0.71 (1631.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:26:56.100653: step 70830, loss = 0.73 (1599.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:26:56.891312: step 70840, loss = 0.63 (1618.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:57.682218: step 70850, loss = 0.78 (1618.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:58.471151: step 70860, loss = 0.79 (1622.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:26:59.259231: step 70870, loss = 0.81 (1624.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:00.038119: step 70880, loss = 0.74 (1643.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:27:00.834642: step 70890, loss = 0.57 (1607.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:27:01.720300: step 70900, loss = 0.82 (1445.3 examples/sec; 0.089 sec/batch)
2017-05-02 16:27:02.410797: step 70910, loss = 0.77 (1853.7 examples/sec; 0.069 sec/batch)
2017-05-02 16:27:03.211587: step 70920, loss = 0.86 (1598.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:27:03.986973: step 70930, loss = 0.75 (1650.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:27:04.775900: step 70940, loss = 0.71 (1622.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:05.566819: step 70950, loss = 0.71 (1618.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:06.359661: step 70960, loss = 0.61 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:07.149187: step 70970, loss = 0.75 (1621.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:07.930752: step 70980, loss = 0.67 (1637.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:27:08.715459: step 70990, loss = 0.74 (1631.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:27:09.605527: step 71000, loss = 0.66 (1438.1 examples/sec; 0.089 sec/batch)
2017-05-02 16:27:10.295463: step 71010, loss = 0.83 (1855.2 examples/sec; 0.069 sec/batch)
2017-05-02 16:27:11.087118: step 71020, loss = 0.81 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:11.875450: step 71030, loss = 0.66 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:12.673607: step 71040, loss = 0.70 (1603.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:27:13.472786: step 71050, loss = 0.82 (1601.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:27:14.269665: step 71060, loss = 0.98 (1606.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:27:15.055359: step 71070, loss = 0.71 (1629.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:15.832154: step 71080, loss = 0.75 (1647.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:27:16.624752: step 71090, loss = 0.72 (1614.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:17.529150: step 71100, loss = 0.70 (1415.3 examples/sec; 0.090 sec/batch)
2017-05-02 16:27:18.215316: step 71110, loss = 0.64 (1865.4 examples/sec; 0.069 sec/batch)
2017-05-02 16:27:19.001781: step 71120, loss = 0.75 (1627.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:19.786701: step 71130, loss = 0.81 (1630.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:27:20.578157: step 71140, loss = 0.81 (1617.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:21.366491: step 71150, loss = 0.84 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:22.157297: step 71160, loss = 0.80 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:22.947349: step 71170, loss = 0.59 (1620.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:23.729225: step 71180, loss = 0.72 (1637.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:27:24.522532: step 71190, loss = 0.78 (1613.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:25.427958: step 71200, loss = 0.87 (1413.7 examples/sec; 0.091 sec/batch)
2017-05-02 16:27:26.113122: step 71210, loss = 0.80 (1868.2 examples/sec; 0.069 sec/batch)
2017-05-02 16:27:26.903946: step 71220, loss = 0.71 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:27.690990: step 71230, loss = 0.65 (1626.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:28.474825: step 71240, loss = 0.61 (1633.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:27:29.266129: step 71250, loss = 0.79 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:30.056094: step 71260, loss = 0.82 (1620.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:30.844445: step 71270, loss = 0.73 (1623.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:31.622010: step 71280, loss = 0.70 (1646.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:27:32.412542: step 71290, loss = 0.67 (1619.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:33.298906: step 71300, loss = 0.84 (1444.1 examples/sec; 0.089 sec/batch)
2017-05-02 16:27:33.995673: step 71310, loss = 0.76 (1837.0 examples/sec; 0.070 sec/batch)
2017-05-02 16:27:34.797829: step 71320, loss = 0.83 (1595.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:27:35.577798: step 71330, loss = 0.62 (1641.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:27:36.363965: step 71340, loss = 0.72 (1628.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:37.154335: step 71350, loss = 0.72 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:37.939470: step 71360, loss = 0.63 (1630.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:38.732706: step 71370, loss = 0.83 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:39.517641: step 71380, loss = 0.79 (1630.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:27:40.300250: step 71390, loss = 0.60 (1635.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:27:41.185246: step 71400, loss = 0.76 (1446.3 examples/sec; 0.088 sec/batch)
2017-05-02 16:27:41.882491: step 71410, loss = 0.63 (1835.8 examples/sec; 0.070 sec/batch)
2017-05-02 16:27:42.675832: step 71420, loss = 0.60 (1613.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:43.458370: step 71430, loss = 0.70 (1635.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:27:44.237972: step 71440, loss = 0.53 (1641.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:27:45.029456: step 71450, loss = 0.73 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:45.821461: step 71460, loss = 0.61 (1616.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:46.612206: step 71470, loss = 0.66 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:47.403178: step 71480, loss = 0.68 (1618.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:48.187399: step 71490, loss = 0.67 (1632.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:27:49.075803: step 71500, loss = 0.65 (1440.8 examples/sec; 0.089 sec/batch)
2017-05-02 16:27:49.777199: step 71510, loss = 0.80 (1824.9 examples/sec; 0.070 sec/batch)
2017-05-02 16:27:50.576152: step 71520, loss = 0.81 (1602.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:27:51.360117: step 71530, loss = 0.74 (1632.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:27:52.146266: step 71540, loss = 0.75 (1628.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:52.944327: step 71550, loss = 0.91 (1603.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:27:53.733003: step 71560, loss = 0.55 (1623.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:54.527540: step 71570, loss = 0.68 (1611.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:55.315251: step 71580, loss = 0.76 (1625.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:27:56.098818: step 71590, loss = 0.81 (1633.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:27:57.005602: step 71600, loss = 0.64 (1411.6 examples/sec; 0.091 sec/batch)
2017-05-02 16:27:57.680717: step 71610, loss = 0.74 (1896.0 examples/sec; 0.068 sec/batch)
2017-05-02 16:27:58.477277: step 71620, loss = 0.90 (1606.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:27:59.257921: step 71630, loss = 0.81 (1639.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:28:00.033554: step 71640, loss = 0.72 (1650.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:28:00.830824: step 71650, loss = 0.68 (1605.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:28:01.627088: step 71660, loss = 0.71 (1607.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:28:02.429148: step 71670, loss = 0.68 (1595.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:28:03.230270: step 71680, loss = 0.77 (1597.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:28:04.014625: step 71690, loss = 0.70 (1631.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:28:04.902966: step 71700, loss = 0.74 (1440.9 examples/sec; 0.089 sec/batch)
2017-05-02 16:28:05.607920: step 71710, loss = 0.75 (1815.7 examples/sec; 0.070 sec/batch)
2017-05-02 16:28:06.396846: step 71720, loss = 0.69 (1622.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:07.188402: step 71730, loss = 0.70 (1617.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:07.968415: step 71740, loss = 0.69 (1641.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:28:08.758792: step 71750, loss = 0.67 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:09.551297: step 71760, loss = 0.74 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:10.343530: step 71770, loss = 0.68 (1615.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:11.139029: step 71780, loss = 0.66 (1609.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:28:11.917652: step 71790, loss = 0.74 (1643.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:28:12.817979: step 71800, loss = 0.63 (1421.7 examples/sec; 0.090 sec/batch)
2017-05-02 16:28:13.513339: step 71810, loss = 0.63 (1840.8 examples/sec; 0.070 sec/batch)
2017-05-02 16:28:14.310092: step 71820, loss = 0.69 (1606.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:28:15.101223: step 71830, loss = 0.73 (1617.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:15.882155: step 71840, loss = 0.74 (1639.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:28:16.680672: step 71850, loss = 0.70 (1603.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:28:17.479544: step 71860, loss = 0.58 (1602.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:28:18.264475: step 71870, loss = 0.72 (1630.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:28:19.049859: step 71880, loss = 0.49 (1629.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:19.824926: step 71890, loss = 0.84 (1651.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:28:20.724210: step 71900, loss = 0.74 (1423.4 examples/sec; 0.090 sec/batch)
2017-05-02 16:28:21.413027: step 71910, loss = 0.65 (1858.2 examples/sec; 0.069 sec/batch)
2017-05-02 16:28:22.207149: step 71920, loss = 0.93 (1611.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:22.993858: step 71930, loss = 0.73 (1627.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:23.767978: step 71940, loss = 0.77 (1653.5 examples/sec; 0.077 sec/batch)
2017-05-02 16:28:24.561249: step 71950, loss = 0.76 (1613.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:25.355764: step 71960, loss = 0.74 (1611.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:26.144007: step 71970, loss = 0.58 (1623.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:26.938625: step 71980, loss = 0.76 (1610.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:27.723527: step 71990, loss = 0.77 (1630.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:28:28.630311: step 72000, loss = 0.68 (1411.6 examples/sec; 0.091 sec/batch)
2017-05-02 16:28:29.300742: step 72010, loss = 0.68 (1909.2 examples/sec; 0.067 sec/batch)
2017-05-02 16:28:30.092027: step 72020, loss = 0.79 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:30.885150: step 72030, loss = 0.75 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:31.662160: step 72040, loss = 0.78 (1647.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:28:32.452185: step 72050, loss = 0.77 (1620.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:33.248302: step 72060, loss = 0.62 (1607.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:28:34.034856: step 72070, loss = 0.79 (1627.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:34.825051: step 72080, loss = 0.74 (1619.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:35.606951: step 72090, loss = 0.77 (1637.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:28:36.515853: step 72100, loss = 0.71 (1408.3 examples/sec; 0.091 sec/batch)
2017-05-02 16:28:37.188785: step 72110, loss = 0.73 (1902.1 examples/sec; 0.067 sec/batch)
2017-05-02 16:28:37.983020: step 72120, loss = 0.79 (1611.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:38.777342: step 72130, loss = 0.87 (1611.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:39.554276: step 72140, loss = 0.85 (1647.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:28:40.343098: step 72150, loss = 0.76 (1622.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:41.136901: step 72160, loss = 0.71 (1612.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:41.922715: step 72170, loss = 0.75 (1628.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:42.717428: step 72180, loss = 0.72 (1610.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:43.503509: step 72190, loss = 0.66 (1628.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:44.399339: step 72200, loss = 0.69 (1428.8 examples/sec; 0.090 sec/batch)
2017-05-02 16:28:45.092753: step 72210, loss = 0.67 (1845.9 examples/sec; 0.069 sec/batch)
2017-05-02 16:28:45.884807: step 72220, loss = 0.59 (1616.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:46.673825: step 72230, loss = 0.67 (1622.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:47.465157: step 72240, loss = 0.70 (1617.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:48.247116: step 72250, loss = 0.75 (1636.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:28:49.040127: step 72260, loss = 0.83 (1614.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:49.831299: step 72270, loss = 0.72 (1617.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:50.623797: step 72280, loss = 0.71 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:51.406492: step 72290, loss = 0.78 (1635.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:28:52.305060: step 72300, loss = 0.80 (1424.5 examples/sec; 0.090 sec/batch)
2017-05-02 16:28:52.977300: step 72310, loss = 0.76 (1904.1 examples/sec; 0.067 sec/batch)
2017-05-02 16:28:53.765846: step 72320, loss = 0.64 (1623.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:54.560382: step 72330, loss = 0.70 (1611.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:55.349465: step 72340, loss = 0.68 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:56.137729: step 72350, loss = 0.69 (1623.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:56.936720: step 72360, loss = 0.73 (1602.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:28:57.735286: step 72370, loss = 0.67 (1602.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:28:58.529829: step 72380, loss = 0.75 (1611.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:28:59.319059: step 72390, loss = 0.66 (1621.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:00.195447: step 72400, loss = 0.85 (1460.5 examples/sec; 0.088 sec/batch)
2017-05-02 16:29:00.886894: step 72410, loss = 0.65 (1851.2 examples/sec; 0.069 sec/batch)
2017-05-02 16:29:01.673376: step 72420, loss = 0.76 (1627.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:02.462469: step 72430, loss = 0.58 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:03.252145: step 72440, loss = 0.76 (1620.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:04.038362: step 72450, loss = 0.75 (1628.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:04.819016: step 72460, loss = 0.79 (1639.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:29:05.604642: step 72470, loss = 0.82 (1629.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:06.392961: step 72480, loss = 0.63 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:07.175001: step 72490, loss = 0.63 (1636.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:29:08.057132: step 72500, loss = 0.78 (1451.0 examples/sec; 0.088 sec/batch)
2017-05-02 16:29:08.758975: step 72510, loss = 0.67 (1823.8 examples/sec; 0.070 sec/batch)
2017-05-02 16:29:09.543589: step 72520, loss = 0.65 (1631.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:29:10.330274: step 72530, loss = 0.76 (1627.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:11.118956: step 72540, loss = 0.73 (1623.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:11.896029: step 72550, loss = 0.73 (1647.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:29:12.689941: step 72560, loss = 0.78 (1612.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:13.479229: step 72570, loss = 0.68 (1621.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:14.274943: step 72580, loss = 0.72 (1608.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:29:15.070118: step 72590, loss = 0.75 (1609.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:29:15.944487: step 72600, loss = 0.78 (1463.9 examples/sec; 0.087 sec/batch)
2017-05-02 16:29:16.648494: step 72610, loss = 0.66 (1818.2 examples/sec; 0.070 sec/batch)
2017-05-02 16:29:17.441483: step 72620, loss = 0.94 (1614.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:18.228254: step 72630, loss = 0.90 (1626.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:19.016744: step 72640, loss = 0.82 (1623.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:19.792373: step 72650, loss = 0.71 (1650.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:29:20.581195: step 72660, loss = 0.67 (1622.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:21.372422: step 72670, loss = 0.70 (1617.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:22.161146: step 72680, loss = 0.77 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:22.956346: step 72690, loss = 0.84 (1609.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:29:23.836067: step 72700, loss = 0.82 (1455.0 examples/sec; 0.088 sec/batch)
2017-05-02 16:29:24.521652: step 72710, loss = 0.61 (1867.0 examples/sec; 0.069 sec/batch)
2017-05-02 16:29:25.311487: step 72720, loss = 0.76 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:26.098323: step 72730, loss = 0.73 (1626.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:26.884795: step 72740, loss = 0.63 (1627.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:27.661606: step 72750, loss = 0.66 (1647.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:29:28.447600: step 72760, loss = 0.78 (1628.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:29.240326: step 72770, loss = 0.76 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:30.027441: step 72780, loss = 0.63 (1626.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:30.817238: step 72790, loss = 0.74 (1620.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:31.721539: step 72800, loss = 0.62 (1415.5 examples/sec; 0.090 sec/batch)
2017-05-02 16:29:32.392178: step 72810, loss = 0.66 (1908.6 examples/sec; 0.067 sec/batch)
2017-05-02 16:29:33.186889: step 72820, loss = 0.84 (1610.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:33.974179: step 72830, loss = 0.73 (1625.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:34.759989: step 72840, loss = 0.73 (1628.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:35.543405: step 72850, loss = 0.78 (1633.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:29:36.328611: step 72860, loss = 0.70 (1630.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:37.116731: step 72870, loss = 0.71 (1624.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:37.910071: step 72880, loss = 0.69 (1613.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:38.703107: step 72890, loss = 0.66 (1614.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:39.582104: step 72900, loss = 0.75 (1456.2 examples/sec; 0.088 sec/batch)
2017-05-02 16:29:40.283049: step 72910, loss = 0.72 (1826.1 examples/sec; 0.070 sec/batch)
2017-05-02 16:29:41.071542: step 72920, loss = 0.72 (1623.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:41.871361: step 72930, loss = 0.64 (1600.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:29:42.675561: step 72940, loss = 0.62 (1591.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:29:43.460222: step 72950, loss = 0.71 (1631.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:29:44.246309: step 72960, loss = 0.71 (1628.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:45.032451: step 72970, loss = 0.77 (1628.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:45.821814: step 72980, loss = 0.76 (1621.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:46.613590: step 72990, loss = 0.55 (1616.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:47.505573: step 73000, loss = 0.57 (1435.0 examples/sec; 0.089 sec/batch)
2017-05-02 16:29:48.188933: step 73010, loss = 0.73 (1873.1 examples/sec; 0.068 sec/batch)
2017-05-02 16:29:48.984960: step 73020, loss = 0.71 (1608.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:29:49.775348: step 73030, loss = 0.82 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:50.569404: step 73040, loss = 0.69 (1612.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:51.358527: step 73050, loss = 0.84 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:52.143448: step 73060, loss = 0.61 (1630.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:29:52.936788: step 73070, loss = 0.67 (1613.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:53.726604: step 73080, loss = 0.55 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:54.513469: step 73090, loss = 0.94 (1626.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:55.400537: step 73100, loss = 0.74 (1443.0 examples/sec; 0.089 sec/batch)
2017-05-02 16:29:56.087389: step 73110, loss = 0.68 (1863.6 examples/sec; 0.069 sec/batch)
2017-05-02 16:29:56.882915: step 73120, loss = 0.62 (1609.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:29:57.683334: step 73130, loss = 0.67 (1599.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:29:58.469096: step 73140, loss = 0.69 (1629.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:29:59.260204: step 73150, loss = 0.75 (1618.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:00.040148: step 73160, loss = 0.79 (1641.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:30:00.827829: step 73170, loss = 0.75 (1625.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:01.615716: step 73180, loss = 0.70 (1624.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:02.415312: step 73190, loss = 0.62 (1600.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:30:03.308674: step 73200, loss = 0.61 (1432.8 examples/sec; 0.089 sec/batch)
2017-05-02 16:30:03.999901: step 73210, loss = 0.65 (1851.8 examples/sec; 0.069 sec/batch)
2017-05-02 16:30:04.788375: step 73220, loss = 0.61 (1623.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:05.579460: step 73230, loss = 0.70 (1618.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:06.369412: step 73240, loss = 0.69 (1620.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:07.158560: step 73250, loss = 0.61 (1622.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:07.938227: step 73260, loss = 0.69 (1641.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:30:08.736512: step 73270, loss = 0.71 (1603.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:30:09.526970: step 73280, loss = 0.84 (1619.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:10.319840: step 73290, loss = 0.74 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:11.229841: step 73300, loss = 0.74 (1406.6 examples/sec; 0.091 sec/batch)
2017-05-02 16:30:11.914557: step 73310, loss = 0.65 (1869.4 examples/sec; 0.068 sec/batch)
2017-05-02 16:30:12.704122: step 73320, loss = 0.66 (1621.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:13.499657: step 73330, loss = 0.72 (1609.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:30:14.303788: step 73340, loss = 0.62 (1591.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:30:15.097418: step 73350, loss = 0.96 (1612.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:15.877922: step 73360, loss = 0.77 (1640.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:30:16.667542: step 73370, loss = 0.76 (1621.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:17.463020: step 73380, loss = 0.75 (1609.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:30:18.259515: step 73390, loss = 0.71 (1607.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:30:19.152185: step 73400, loss = 0.74 (1433.9 examples/sec; 0.089 sec/batch)
2017-05-02 16:30:19.841117: step 73410, loss = 0.79 (1857.9 examples/sec; 0.069 sec/batch)
2017-05-02 16:30:20.635364: step 73420, loss = 0.53 (1611.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:21.426102: step 73430, loss = 0.74 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:22.212298: step 73440, loss = 0.76 (1628.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:23.004943: step 73450, loss = 0.52 (1614.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:23.791689: step 73460, loss = 0.60 (1627.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:24.584603: step 73470, loss = 0.70 (1614.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:25.390319: step 73480, loss = 0.81 (1588.7 examples/sec; 0.081 sec/batch)
2017-05-02 16:30:26.180661: step 73490, loss = 0.68 (1619.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:27.063602: step 73500, loss = 0.68 (1449.7 examples/sec; 0.088 sec/batch)
2017-05-02 16:30:27.758124: step 73510, loss = 0.66 (1843.0 examples/sec; 0.069 sec/batch)
2017-05-02 16:30:28.555214: step 73520, loss = 0.79 (1605.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:30:29.345964: step 73530, loss = 0.80 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:30.138984: step 73540, loss = 0.85 (1614.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:30.929538: step 73550, loss = 0.77 (1619.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:31.712867: step 73560, loss = 0.70 (1634.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:30:32.504633: step 73570, loss = 0.54 (1616.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:33.299482: step 73580, loss = 0.74 (1610.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:34.092974: step 73590, loss = 0.76 (1613.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:34.986357: step 73600, loss = 0.66 (1432.8 examples/sec; 0.089 sec/batch)
2017-05-02 16:30:35.693389: step 73610, loss = 0.71 (1810.4 examples/sec; 0.071 sec/batch)
2017-05-02 16:30:36.481291: step 73620, loss = 0.72 (1624.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:37.269119: step 73630, loss = 0.72 (1624.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:38.059733: step 73640, loss = 0.70 (1619.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:38.849519: step 73650, loss = 0.59 (1620.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:39.637867: step 73660, loss = 0.66 (1623.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:40.429844: step 73670, loss = 0.66 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:41.217617: step 73680, loss = 0.70 (1624.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:42.006521: step 73690, loss = 0.85 (1622.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:42.901029: step 73700, loss = 0.68 (1431.0 examples/sec; 0.089 sec/batch)
2017-05-02 16:30:43.587521: step 73710, loss = 0.74 (1864.5 examples/sec; 0.069 sec/batch)
2017-05-02 16:30:44.379135: step 73720, loss = 0.66 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:45.172049: step 73730, loss = 0.76 (1614.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:45.962206: step 73740, loss = 0.74 (1619.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:46.757710: step 73750, loss = 0.85 (1609.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:30:47.545693: step 73760, loss = 0.63 (1624.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:48.332582: step 73770, loss = 0.62 (1626.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:49.124668: step 73780, loss = 0.71 (1616.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:49.915849: step 73790, loss = 0.74 (1617.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:50.798914: step 73800, loss = 0.75 (1449.5 examples/sec; 0.088 sec/batch)
2017-05-02 16:30:51.493250: step 73810, loss = 0.64 (1843.5 examples/sec; 0.069 sec/batch)
2017-05-02 16:30:52.279316: step 73820, loss = 0.64 (1628.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:53.071106: step 73830, loss = 0.60 (1616.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:53.860336: step 73840, loss = 0.72 (1621.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:54.650115: step 73850, loss = 0.77 (1620.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:55.442998: step 73860, loss = 0.76 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:56.221526: step 73870, loss = 0.76 (1644.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:30:57.012560: step 73880, loss = 0.63 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:57.806999: step 73890, loss = 0.81 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:30:58.703613: step 73900, loss = 0.77 (1427.6 examples/sec; 0.090 sec/batch)
2017-05-02 16:30:59.394708: step 73910, loss = 0.77 (1852.1 examples/sec; 0.069 sec/batch)
2017-05-02 16:31:00.174258: step 73920, loss = 0.68 (1642.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:31:00.968961: step 73930, loss = 0.60 (1610.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:31:01.760502: step 73940, loss = 0.58 (1617.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:31:02.555997: step 73950, loss = 0.74 (1609.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:31:03.339258: step 73960, loss = 0.66 (1634.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:31:04.121911: step 73970, loss = 0.66 (1635.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:31:04.915697: step 73980, loss = 0.83 (1612.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:31:05.712684: step 73990, loss = 0.61 (1606.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:31:06.606901: step 74000, loss = 0.69 (1431.5 examples/sec; 0.089 sec/batch)
2017-05-02 16:31:07.307807: step 74010, loss = 0.80 (1826.2 examples/sec; 0.070 sec/batch)
2017-05-02 16:31:08.090075: step 74020, loss = 0.76 (1636.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:31:08.887320: step 74030, loss = 0.70 (1605.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:31:09.684120: step 74040, loss = 0.77 (1606.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:31:10.478861: step 74050, loss = 0.61 (1610.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:31:11.271889: step 74060, loss = 0.61 (1614.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:31:12.054277: step 74070, loss = 0.58 (1636.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:31:12.852301: step 74080, loss = 0.74 (1604.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:31:13.643635: step 74090, loss = 0.66 (1617.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:31:14.553099: step 74100, loss = 0.74 (1407.4 examples/sec; 0.091 sec/batch)
2017-05-02 16:31:15.225270: step 74110, loss = 0.65 (1904.3 examples/sec; 0.067 sec/batch)
2017-05-02 16:31:16.007325: step 74120, loss = 0.80 (1636.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:31:16.795716: step 74130, loss = 0.69 (1623.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:31:17.588803: step 74140, loss = 0.62 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:31:18.385041: step 74150, loss = 0.71 (1607.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:31:19.173144: step 74160, loss = 0.87 (1624.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:31:19.953737: step 74170, loss = 0.73 (1639.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:31:20.749619: step 74180, loss = 0.81 (1608.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:31:21.549402: step 74190, loss = 0.63 (1600.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:31:22.458593: step 74200, loss = 0.70 (1407.8 examples/sec; 0.091 sec/batch)
2017-05-02 16:31:23.139450: step 74210, loss = 0.62 (1880.0 examples/sec; 0.068 sec/batch)
2017-05-02 16:31:23.921447: step 74220, loss = 0.62 (1636.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:31:24.723344: step 74230, loss = 0.64 (1596.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:31:25.510583: step 74240, loss = 0.71 (1625.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:31:26.311201: step 74250, loss = 0.70 (1598.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:31:27.104768: step 74260, loss = 0.81 (1613.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:31:27.883226: step 74270, loss = 0.68 (1644.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:31:28.671761: step 74280, loss = 0.66 (1623.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:31:29.459955: step 74290, loss = 0.69 (1624.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:31:30.350848: step 74300, loss = 0.74 (1436.8 examples/sec; 0.089 sec/batch)
2017-05-02 16:31:31.045691: step 74310, loss = 0.76 (1842.1 examples/sec; 0.069 sec/batch)
2017-05-02 16:31:31.821574: step 74320, loss = 0.82 (1649.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:31:32.613627: step 74330, loss = 0.73 (1616.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:31:33.401155: step 74340, loss = 0.71 (1625.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:31:34.193653: step 74350, loss = 0.73 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:31:34.986135: step 74360, loss = 0.63 (1615.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:31:35.768185: step 74370, loss = 0.76 (1636.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:31:36.559896: step 74380, loss = 0.75 (1616.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:31:37.353281: step 74390, loss = 0.77 (1613.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:31:38.242336: step 74400, loss = 0.65 (1439.7 examples/sec; 0.089 sec/batch)
2017-05-02 16:31:38.937703: step 74410, loss = 0.71 (1840.7 examples/sec; 0.070 sec/batch)
2017-05-02 16:31:39.719368: step 74420, loss = 0.81 (1637.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:31:40.513964: step 74430, loss = 0.59 (1610.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:31:41.320080: step 74440, loss = 0.71 (1587.9 examples/sec; 0.081 sec/batch)
2017-05-02 16:31:42.108650: step 74450, loss = 0.77 (1623.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:31:42.908129: step 74460, loss = 0.84 (1601.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:31:43.693352: step 74470, loss = 0.75 (1630.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:31:44.478138: step 74480, loss = 0.58 (1631.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:31:45.272276: step 74490, loss = 0.61 (1611.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:31:46.155433: step 74500, loss = 0.79 (1449.3 examples/sec; 0.088 sec/batch)
2017-05-02 16:31:46.854152: step 74510, loss = 0.71 (1831.9 examples/sec; 0.070 sec/batch)
2017-05-02 16:31:47.642452: step 74520, loss = 0.62 (1623.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:31:48.430379: step 74530, loss = 0.57 (1624.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:31:49.222748: step 74540, loss = 0.61 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:31:50.021297: step 74550, loss = 0.62 (1602.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:31:50.818081: step 74560, loss = 0.79 (1606.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:31:51.605916: step 74570, loss = 0.73 (1624.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:31:52.393189: step 74580, loss = 0.60 (1625.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:31:53.192497: step 74590, loss = 0.68 (1601.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:31:54.079081: step 74600, loss = 0.69 (1443.8 examples/sec; 0.089 sec/batch)
2017-05-02 16:31:54.784048: step 74610, loss = 0.71 (1815.7 examples/sec; 0.070 sec/batch)
2017-05-02 16:31:55.568254: step 74620, loss = 0.74 (1632.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:31:56.360404: step 74630, loss = 0.63 (1615.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:31:57.146502: step 74640, loss = 0.67 (1628.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:31:57.942751: step 74650, loss = 0.79 (1607.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:31:58.734661: step 74660, loss = 0.78 (1616.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:31:59.520847: step 74670, loss = 0.83 (1628.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:32:00.314856: step 74680, loss = 0.72 (1612.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:32:01.103860: step 74690, loss = 0.88 (1622.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:32:01.989458: step 74700, loss = 0.64 (1445.4 examples/sec; 0.089 sec/batch)
2017-05-02 16:32:02.693823: step 74710, loss = 0.77 (1817.2 examples/sec; 0.070 sec/batch)
2017-05-02 16:32:03.481481: step 74720, loss = 0.90 (1625.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:32:04.269260: step 74730, loss = 0.78 (1624.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:32:05.062200: step 74740, loss = 0.62 (1614.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:32:05.851173: step 74750, loss = 0.71 (1622.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:32:06.639386: step 74760, loss = 0.67 (1623.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:32:07.425292: step 74770, loss = 0.74 (1628.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:32:08.213564: step 74780, loss = 0.74 (1623.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:32:09.004980: step 74790, loss = 0.79 (1617.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:32:09.910199: step 74800, loss = 0.67 (1414.0 examples/sec; 0.091 sec/batch)
2017-05-02 16:32:10.608248: step 74810, loss = 0.88 (1833.7 examples/sec; 0.070 sec/batch)
2017-05-02 16:32:11.399608: step 74820, loss = 0.68 (1617.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:32:12.184366: step 74830, loss = 0.76 (1631.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:32:12.972923: step 74840, loss = 0.65 (1623.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:32:13.766388: step 74850, loss = 0.57 (1613.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:32:14.561009: step 74860, loss = 0.86 (1610.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:32:15.349982: step 74870, loss = 0.66 (1622.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:32:16.131443: step 74880, loss = 0.64 (1638.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:32:16.925086: step 74890, loss = 0.70 (1612.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:32:17.814451: step 74900, loss = 0.82 (1439.2 examples/sec; 0.089 sec/batch)
2017-05-02 16:32:18.512029: step 74910, loss = 0.68 (1834.9 examples/sec; 0.070 sec/batch)
2017-05-02 16:32:19.303032: step 74920, loss = 0.71 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:32:20.083918: step 74930, loss = 0.68 (1639.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:32:20.865854: step 74940, loss = 0.64 (1637.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:32:21.658223: step 74950, loss = 0.65 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:32:22.448629: step 74960, loss = 0.63 (1619.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:32:23.235573: step 74970, loss = 0.60 (1626.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:32:24.020122: step 74980, loss = 0.66 (1631.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:32:24.811786: step 74990, loss = 0.74 (1616.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:32:25.696649: step 75000, loss = 0.75 (1446.6 examples/sec; 0.088 sec/batch)
2017-05-02 16:32:26.397583: step 75010, loss = 0.87 (1826.1 examples/sec; 0.070 sec/batch)
2017-05-02 16:32:27.196304: step 75020, loss = 0.74 (1602.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:32:27.989867: step 75030, loss = 0.61 (1613.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:32:28.793432: step 75040, loss = 0.62 (1592.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:32:29.595401: step 75050, loss = 0.70 (1596.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:32:30.400676: step 75060, loss = 0.67 (1589.5 examples/sec; 0.081 sec/batch)
2017-05-02 16:32:31.203640: step 75070, loss = 0.81 (1594.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:32:31.995516: step 75080, loss = 0.62 (1616.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:32:32.800132: step 75090, loss = 0.67 (1590.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:32:33.698708: step 75100, loss = 0.65 (1424.5 examples/sec; 0.090 sec/batch)
2017-05-02 16:32:34.406643: step 75110, loss = 0.80 (1808.1 examples/sec; 0.071 sec/batch)
2017-05-02 16:32:35.218920: step 75120, loss = 0.66 (1575.8 examples/sec; 0.081 sec/batch)
2017-05-02 16:32:36.012385: step 75130, loss = 0.75 (1613.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:32:36.824495: step 75140, loss = 0.73 (1576.1 examples/sec; 0.081 sec/batch)
2017-05-02 16:32:37.624732: step 75150, loss = 0.68 (1599.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:32:38.423026: step 75160, loss = 0.73 (1603.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:32:39.226919: step 75170, loss = 0.68 (1592.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:32:40.021493: step 75180, loss = 0.81 (1610.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:32:40.826094: step 75190, loss = 0.73 (1590.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:32:41.728205: step 75200, loss = 0.62 (1418.9 examples/sec; 0.090 sec/batch)
2017-05-02 16:32:42.422735: step 75210, loss = 0.78 (1843.0 examples/sec; 0.069 sec/batch)
2017-05-02 16:32:43.227165: step 75220, loss = 0.70 (1591.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:32:44.016513: step 75230, loss = 0.70 (1621.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:32:44.821927: step 75240, loss = 0.63 (1589.3 examples/sec; 0.081 sec/batch)
2017-05-02 16:32:45.626938: step 75250, loss = 0.62 (1590.0 examples/sec; 0.081 sec/batch)
2017-05-02 16:32:46.419396: step 75260, loss = 0.85 (1615.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:32:47.220342: step 75270, loss = 0.78 (1598.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:32:48.015693: step 75280, loss = 0.68 (1609.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:32:48.817798: step 75290, loss = 0.75 (1595.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:32:49.719557: step 75300, loss = 0.67 (1419.5 examples/sec; 0.090 sec/batch)
2017-05-02 16:32:50.427350: step 75310, loss = 0.61 (1808.4 examples/sec; 0.071 sec/batch)
2017-05-02 16:32:51.226423: step 75320, loss = 0.64 (1601.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:32:52.016285: step 75330, loss = 0.82 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:32:52.817540: step 75340, loss = 0.69 (1597.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:32:53.619033: step 75350, loss = 0.68 (1597.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:32:54.422949: step 75360, loss = 0.65 (1592.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:32:55.224560: step 75370, loss = 0.68 (1596.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:32:56.020219: step 75380, loss = 0.58 (1608.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:32:56.821125: step 75390, loss = 0.72 (1598.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:32:57.733595: step 75400, loss = 0.84 (1402.8 examples/sec; 0.091 sec/batch)
2017-05-02 16:32:58.427598: step 75410, loss = 0.66 (1844.4 examples/sec; 0.069 sec/batch)
2017-05-02 16:32:59.227085: step 75420, loss = 0.68 (1601.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:33:00.023698: step 75430, loss = 0.69 (1606.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:33:00.825832: step 75440, loss = 0.71 (1595.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:33:01.616214: step 75450, loss = 0.70 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:33:02.420050: step 75460, loss = 0.66 (1592.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:33:03.226701: step 75470, loss = 0.79 (1586.8 examples/sec; 0.081 sec/batch)
2017-05-02 16:33:04.010773: step 75480, loss = 0.73 (1632.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:33:04.813281: step 75490, loss = 0.74 (1595.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:33:05.706363: step 75500, loss = 0.81 (1433.2 examples/sec; 0.089 sec/batch)
2017-05-02 16:33:06.418850: step 75510, loss = 0.62 (1796.5 examples/sec; 0.071 sec/batch)
2017-05-02 16:33:07.218570: step 75520, loss = 0.72 (1600.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:33:08.004451: step 75530, loss = 0.91 (1628.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:33:08.803026: step 75540, loss = 0.74 (1602.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:33:09.605562: step 75550, loss = 0.68 (1594.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:33:10.409706: step 75560, loss = 0.85 (1591.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:33:11.203336: step 75570, loss = 0.76 (1612.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:33:11.990281: step 75580, loss = 0.63 (1626.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:33:12.780711: step 75590, loss = 0.77 (1619.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:33:13.691370: step 75600, loss = 0.69 (1405.6 examples/sec; 0.091 sec/batch)
2017-05-02 16:33:14.386329: step 75610, loss = 0.70 (1841.8 examples/sec; 0.069 sec/batch)
2017-05-02 16:33:15.191669: step 75620, loss = 0.63 (1589.4 examples/sec; 0.081 sec/batch)
2017-05-02 16:33:15.990720: step 75630, loss = 0.65 (1601.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:33:16.805477: step 75640, loss = 0.69 (1571.0 examples/sec; 0.081 sec/batch)
2017-05-02 16:33:17.609852: step 75650, loss = 0.73 (1591.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:33:18.414515: step 75660, loss = 0.74 (1590.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:33:19.201257: step 75670, loss = 0.76 (1627.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:33:19.980624: step 75680, loss = 0.74 (1642.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:33:20.774819: step 75690, loss = 0.60 (1611.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:33:21.660942: step 75700, loss = 0.66 (1444.5 examples/sec; 0.089 sec/batch)
2017-05-02 16:33:22.359512: step 75710, loss = 0.80 (1832.3 examples/sec; 0.070 sec/batch)
2017-05-02 16:33:23.148753: step 75720, loss = 0.58 (1621.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:33:23.928788: step 75730, loss = 0.69 (1641.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:33:24.718665: step 75740, loss = 0.67 (1620.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:33:25.505111: step 75750, loss = 0.62 (1627.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:33:26.296101: step 75760, loss = 0.84 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:33:27.086028: step 75770, loss = 0.53 (1620.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:33:27.870545: step 75780, loss = 0.54 (1631.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:33:28.665988: step 75790, loss = 0.77 (1609.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:33:29.574489: step 75800, loss = 0.70 (1408.9 examples/sec; 0.091 sec/batch)
2017-05-02 16:33:30.283779: step 75810, loss = 0.71 (1804.6 examples/sec; 0.071 sec/batch)
2017-05-02 16:33:31.076601: step 75820, loss = 0.60 (1614.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:33:31.853743: step 75830, loss = 0.64 (1647.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:33:32.648176: step 75840, loss = 0.91 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:33:33.442675: step 75850, loss = 0.74 (1611.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:33:34.233717: step 75860, loss = 0.76 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:33:35.028144: step 75870, loss = 0.78 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:33:35.804435: step 75880, loss = 0.77 (1648.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:33:36.604583: step 75890, loss = 0.62 (1599.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:33:37.498699: step 75900, loss = 0.62 (1431.6 examples/sec; 0.089 sec/batch)
2017-05-02 16:33:38.202452: step 75910, loss = 0.63 (1818.8 examples/sec; 0.070 sec/batch)
2017-05-02 16:33:38.991561: step 75920, loss = 0.70 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:33:39.774423: step 75930, loss = 0.72 (1635.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:33:40.570318: step 75940, loss = 0.68 (1608.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:33:41.360260: step 75950, loss = 0.68 (1620.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:33:42.156341: step 75960, loss = 0.72 (1607.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:33:42.945574: step 75970, loss = 0.71 (1621.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:33:43.724661: step 75980, loss = 0.77 (1642.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:33:44.522595: step 75990, loss = 0.76 (1604.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:33:45.407813: step 76000, loss = 0.60 (1446.0 examples/sec; 0.089 sec/batch)
2017-05-02 16:33:46.108751: step 76010, loss = 0.75 (1826.1 examples/sec; 0.070 sec/batch)
2017-05-02 16:33:46.901105: step 76020, loss = 0.76 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:33:47.681502: step 76030, loss = 0.74 (1640.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:33:48.485646: step 76040, loss = 0.64 (1591.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:33:49.274957: step 76050, loss = 0.59 (1621.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:33:50.057735: step 76060, loss = 0.67 (1635.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:33:50.849062: step 76070, loss = 0.86 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:33:51.625325: step 76080, loss = 0.73 (1648.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:33:52.414173: step 76090, loss = 0.78 (1622.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:33:53.315352: step 76100, loss = 0.78 (1420.4 examples/sec; 0.090 sec/batch)
2017-05-02 16:33:54.008242: step 76110, loss = 0.68 (1847.3 examples/sec; 0.069 sec/batch)
2017-05-02 16:33:54.803241: step 76120, loss = 0.82 (1610.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:33:55.583184: step 76130, loss = 0.74 (1641.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:33:56.380365: step 76140, loss = 0.67 (1605.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:33:57.169059: step 76150, loss = 0.80 (1623.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:33:57.960708: step 76160, loss = 0.73 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:33:58.751264: step 76170, loss = 0.66 (1619.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:33:59.540230: step 76180, loss = 0.66 (1622.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:00.324075: step 76190, loss = 0.64 (1633.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:34:01.206780: step 76200, loss = 0.71 (1450.1 examples/sec; 0.088 sec/batch)
2017-05-02 16:34:01.902304: step 76210, loss = 0.79 (1840.3 examples/sec; 0.070 sec/batch)
2017-05-02 16:34:02.694626: step 76220, loss = 0.89 (1615.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:03.475934: step 76230, loss = 0.75 (1638.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:34:04.269085: step 76240, loss = 0.73 (1613.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:05.059837: step 76250, loss = 0.72 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:05.849584: step 76260, loss = 0.77 (1620.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:06.641233: step 76270, loss = 0.68 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:07.425304: step 76280, loss = 0.78 (1632.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:34:08.214064: step 76290, loss = 0.70 (1622.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:09.098240: step 76300, loss = 0.78 (1447.7 examples/sec; 0.088 sec/batch)
2017-05-02 16:34:09.788709: step 76310, loss = 0.60 (1853.8 examples/sec; 0.069 sec/batch)
2017-05-02 16:34:10.585082: step 76320, loss = 0.72 (1607.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:34:11.371595: step 76330, loss = 0.71 (1627.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:12.157539: step 76340, loss = 0.69 (1628.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:12.946870: step 76350, loss = 0.73 (1621.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:13.738614: step 76360, loss = 0.68 (1616.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:14.529421: step 76370, loss = 0.72 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:15.318791: step 76380, loss = 0.78 (1621.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:16.104698: step 76390, loss = 0.89 (1628.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:17.008818: step 76400, loss = 0.70 (1415.7 examples/sec; 0.090 sec/batch)
2017-05-02 16:34:17.699838: step 76410, loss = 0.70 (1852.3 examples/sec; 0.069 sec/batch)
2017-05-02 16:34:18.494872: step 76420, loss = 0.80 (1610.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:34:19.278161: step 76430, loss = 0.69 (1634.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:34:20.056321: step 76440, loss = 0.60 (1644.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:34:20.844419: step 76450, loss = 0.68 (1624.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:21.649110: step 76460, loss = 0.81 (1590.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:34:22.441083: step 76470, loss = 0.66 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:23.239031: step 76480, loss = 0.75 (1604.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:34:24.022562: step 76490, loss = 0.74 (1633.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:34:24.952297: step 76500, loss = 0.67 (1376.7 examples/sec; 0.093 sec/batch)
2017-05-02 16:34:25.632214: step 76510, loss = 0.70 (1882.6 examples/sec; 0.068 sec/batch)
2017-05-02 16:34:26.431924: step 76520, loss = 0.70 (1600.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:34:27.230197: step 76530, loss = 0.74 (1603.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:34:28.012312: step 76540, loss = 0.71 (1636.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:34:28.805769: step 76550, loss = 0.79 (1613.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:29.602093: step 76560, loss = 0.68 (1607.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:34:30.394101: step 76570, loss = 0.92 (1616.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:31.191828: step 76580, loss = 0.80 (1604.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:34:31.972440: step 76590, loss = 0.73 (1639.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:34:32.870925: step 76600, loss = 0.77 (1424.6 examples/sec; 0.090 sec/batch)
2017-05-02 16:34:33.564419: step 76610, loss = 0.78 (1845.7 examples/sec; 0.069 sec/batch)
2017-05-02 16:34:34.356986: step 76620, loss = 0.65 (1615.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:35.149113: step 76630, loss = 0.76 (1615.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:35.930518: step 76640, loss = 0.62 (1638.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:34:36.719186: step 76650, loss = 0.63 (1623.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:37.505267: step 76660, loss = 0.60 (1628.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:38.295629: step 76670, loss = 0.70 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:39.092338: step 76680, loss = 0.92 (1606.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:34:39.876079: step 76690, loss = 0.70 (1633.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:34:40.771616: step 76700, loss = 0.71 (1429.3 examples/sec; 0.090 sec/batch)
2017-05-02 16:34:41.477054: step 76710, loss = 0.59 (1814.5 examples/sec; 0.071 sec/batch)
2017-05-02 16:34:42.278328: step 76720, loss = 0.87 (1597.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:34:43.065522: step 76730, loss = 0.63 (1626.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:43.846832: step 76740, loss = 0.83 (1638.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:34:44.636866: step 76750, loss = 0.68 (1620.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:45.428748: step 76760, loss = 0.69 (1616.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:46.220642: step 76770, loss = 0.77 (1616.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:47.012131: step 76780, loss = 0.80 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:47.799450: step 76790, loss = 0.75 (1625.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:48.693414: step 76800, loss = 0.69 (1431.8 examples/sec; 0.089 sec/batch)
2017-05-02 16:34:49.392081: step 76810, loss = 0.70 (1832.0 examples/sec; 0.070 sec/batch)
2017-05-02 16:34:50.184081: step 76820, loss = 0.75 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:50.978913: step 76830, loss = 0.83 (1610.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:51.765776: step 76840, loss = 0.82 (1626.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:52.554942: step 76850, loss = 0.69 (1622.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:53.347601: step 76860, loss = 0.66 (1614.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:54.143231: step 76870, loss = 0.77 (1608.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:34:54.927212: step 76880, loss = 0.72 (1632.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:34:55.710803: step 76890, loss = 0.77 (1633.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:34:56.598008: step 76900, loss = 0.65 (1442.7 examples/sec; 0.089 sec/batch)
2017-05-02 16:34:57.292242: step 76910, loss = 0.75 (1843.8 examples/sec; 0.069 sec/batch)
2017-05-02 16:34:58.086076: step 76920, loss = 0.65 (1612.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:58.872749: step 76930, loss = 0.74 (1627.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:34:59.657639: step 76940, loss = 0.77 (1630.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:35:00.443105: step 76950, loss = 0.70 (1629.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:01.237489: step 76960, loss = 0.78 (1611.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:02.032520: step 76970, loss = 0.73 (1610.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:35:02.822800: step 76980, loss = 0.75 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:03.606231: step 76990, loss = 0.84 (1633.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:35:04.490828: step 77000, loss = 0.60 (1447.0 examples/sec; 0.088 sec/batch)
2017-05-02 16:35:05.187629: step 77010, loss = 0.82 (1837.0 examples/sec; 0.070 sec/batch)
2017-05-02 16:35:05.980460: step 77020, loss = 0.82 (1614.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:06.771495: step 77030, loss = 0.72 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:07.556033: step 77040, loss = 0.67 (1631.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:35:08.343342: step 77050, loss = 0.94 (1625.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:09.138325: step 77060, loss = 0.80 (1610.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:09.923774: step 77070, loss = 0.71 (1629.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:10.714800: step 77080, loss = 0.58 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:11.499790: step 77090, loss = 0.85 (1630.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:35:12.393367: step 77100, loss = 0.71 (1432.4 examples/sec; 0.089 sec/batch)
2017-05-02 16:35:13.097287: step 77110, loss = 0.75 (1818.4 examples/sec; 0.070 sec/batch)
2017-05-02 16:35:13.885393: step 77120, loss = 0.70 (1624.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:14.677015: step 77130, loss = 0.75 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:15.462580: step 77140, loss = 0.81 (1629.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:16.251134: step 77150, loss = 0.63 (1623.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:17.043612: step 77160, loss = 0.65 (1615.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:17.833620: step 77170, loss = 0.56 (1620.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:18.619907: step 77180, loss = 0.66 (1628.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:19.405056: step 77190, loss = 0.69 (1630.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:20.290826: step 77200, loss = 0.84 (1445.1 examples/sec; 0.089 sec/batch)
2017-05-02 16:35:20.988100: step 77210, loss = 0.70 (1835.7 examples/sec; 0.070 sec/batch)
2017-05-02 16:35:21.783779: step 77220, loss = 0.67 (1608.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:35:22.578464: step 77230, loss = 0.76 (1610.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:23.367002: step 77240, loss = 0.86 (1623.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:24.152564: step 77250, loss = 0.67 (1629.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:24.951277: step 77260, loss = 0.65 (1602.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:35:25.746137: step 77270, loss = 0.64 (1610.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:26.538095: step 77280, loss = 0.68 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:27.328970: step 77290, loss = 0.75 (1618.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:28.212030: step 77300, loss = 0.68 (1449.5 examples/sec; 0.088 sec/batch)
2017-05-02 16:35:28.898804: step 77310, loss = 0.75 (1863.8 examples/sec; 0.069 sec/batch)
2017-05-02 16:35:29.694754: step 77320, loss = 0.59 (1608.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:35:30.488578: step 77330, loss = 0.64 (1612.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:31.279582: step 77340, loss = 0.75 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:32.067485: step 77350, loss = 0.73 (1624.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:32.860513: step 77360, loss = 0.67 (1614.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:33.649500: step 77370, loss = 0.64 (1622.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:34.446650: step 77380, loss = 0.75 (1605.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:35:35.232076: step 77390, loss = 0.62 (1629.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:36.108908: step 77400, loss = 0.76 (1459.8 examples/sec; 0.088 sec/batch)
2017-05-02 16:35:36.807152: step 77410, loss = 0.84 (1833.1 examples/sec; 0.070 sec/batch)
2017-05-02 16:35:37.599782: step 77420, loss = 0.67 (1614.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:38.392160: step 77430, loss = 0.50 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:39.188081: step 77440, loss = 0.67 (1608.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:35:39.969541: step 77450, loss = 0.69 (1638.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:35:40.757498: step 77460, loss = 0.76 (1624.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:41.548715: step 77470, loss = 0.72 (1617.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:42.340454: step 77480, loss = 0.75 (1616.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:43.131823: step 77490, loss = 0.79 (1617.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:44.008497: step 77500, loss = 0.80 (1460.1 examples/sec; 0.088 sec/batch)
2017-05-02 16:35:44.707429: step 77510, loss = 0.66 (1831.4 examples/sec; 0.070 sec/batch)
2017-05-02 16:35:45.496254: step 77520, loss = 0.66 (1622.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:46.285909: step 77530, loss = 0.91 (1621.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:47.072324: step 77540, loss = 0.68 (1627.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:47.853762: step 77550, loss = 0.98 (1638.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:35:48.642099: step 77560, loss = 0.79 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:49.436441: step 77570, loss = 0.71 (1611.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:50.222763: step 77580, loss = 0.79 (1627.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:51.014056: step 77590, loss = 0.73 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:51.893360: step 77600, loss = 0.62 (1455.7 examples/sec; 0.088 sec/batch)
2017-05-02 16:35:52.595485: step 77610, loss = 0.90 (1823.0 examples/sec; 0.070 sec/batch)
2017-05-02 16:35:53.387299: step 77620, loss = 0.70 (1616.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:54.171409: step 77630, loss = 0.68 (1632.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:35:54.969692: step 77640, loss = 0.57 (1603.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:35:55.748180: step 77650, loss = 0.82 (1644.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:35:56.546851: step 77660, loss = 0.82 (1602.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:35:57.333072: step 77670, loss = 0.85 (1628.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:58.129407: step 77680, loss = 0.86 (1607.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:35:58.915338: step 77690, loss = 0.57 (1628.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:35:59.794315: step 77700, loss = 0.60 (1456.2 examples/sec; 0.088 sec/batch)
2017-05-02 16:36:00.493671: step 77710, loss = 0.72 (1830.2 examples/sec; 0.070 sec/batch)
2017-05-02 16:36:01.283627: step 77720, loss = 0.80 (1620.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:02.077153: step 77730, loss = 0.88 (1613.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:02.872103: step 77740, loss = 0.70 (1610.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:03.658022: step 77750, loss = 0.64 (1628.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:04.437991: step 77760, loss = 0.58 (1641.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:36:05.231190: step 77770, loss = 0.73 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:06.021973: step 77780, loss = 0.68 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:06.810796: step 77790, loss = 0.75 (1622.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:07.715127: step 77800, loss = 0.68 (1415.4 examples/sec; 0.090 sec/batch)
2017-05-02 16:36:08.390139: step 77810, loss = 0.70 (1896.3 examples/sec; 0.068 sec/batch)
2017-05-02 16:36:09.179350: step 77820, loss = 0.80 (1621.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:09.969543: step 77830, loss = 0.67 (1619.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:10.764277: step 77840, loss = 0.70 (1610.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:11.545676: step 77850, loss = 0.62 (1638.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:36:12.333634: step 77860, loss = 0.62 (1624.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:13.115864: step 77870, loss = 0.74 (1636.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:36:13.906165: step 77880, loss = 0.87 (1619.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:14.692329: step 77890, loss = 0.71 (1628.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:15.579337: step 77900, loss = 0.82 (1443.1 examples/sec; 0.089 sec/batch)
2017-05-02 16:36:16.269052: step 77910, loss = 0.58 (1855.8 examples/sec; 0.069 sec/batch)
2017-05-02 16:36:17.059091: step 77920, loss = 0.60 (1620.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:17.842698: step 77930, loss = 0.84 (1633.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:36:18.630427: step 77940, loss = 0.74 (1624.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:19.418744: step 77950, loss = 0.68 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:20.203648: step 77960, loss = 0.63 (1630.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:36:20.996853: step 77970, loss = 0.73 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:21.787357: step 77980, loss = 0.64 (1619.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:22.586180: step 77990, loss = 0.66 (1602.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:36:23.469390: step 78000, loss = 0.79 (1449.3 examples/sec; 0.088 sec/batch)
2017-05-02 16:36:24.160677: step 78010, loss = 0.67 (1851.6 examples/sec; 0.069 sec/batch)
2017-05-02 16:36:24.949036: step 78020, loss = 0.64 (1623.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:25.742820: step 78030, loss = 0.67 (1612.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:26.542621: step 78040, loss = 0.81 (1600.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:36:27.329566: step 78050, loss = 0.64 (1626.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:28.123698: step 78060, loss = 0.68 (1611.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:28.914656: step 78070, loss = 0.73 (1618.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:29.705376: step 78080, loss = 0.69 (1618.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:30.493147: step 78090, loss = 0.63 (1624.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:31.400480: step 78100, loss = 0.65 (1410.7 examples/sec; 0.091 sec/batch)
2017-05-02 16:36:32.062738: step 78110, loss = 0.76 (1932.8 examples/sec; 0.066 sec/batch)
2017-05-02 16:36:32.856335: step 78120, loss = 0.67 (1612.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:33.641576: step 78130, loss = 0.71 (1630.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:34.430954: step 78140, loss = 0.73 (1621.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:35.221045: step 78150, loss = 0.69 (1620.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:36.001688: step 78160, loss = 0.62 (1639.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:36:36.785102: step 78170, loss = 0.88 (1633.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:36:37.575006: step 78180, loss = 0.79 (1620.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:38.364010: step 78190, loss = 0.66 (1622.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:39.276614: step 78200, loss = 0.72 (1402.6 examples/sec; 0.091 sec/batch)
2017-05-02 16:36:39.949399: step 78210, loss = 0.69 (1902.5 examples/sec; 0.067 sec/batch)
2017-05-02 16:36:40.740693: step 78220, loss = 0.68 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:41.531381: step 78230, loss = 0.68 (1618.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:42.319828: step 78240, loss = 0.76 (1623.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:43.113860: step 78250, loss = 0.60 (1612.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:43.890380: step 78260, loss = 0.74 (1648.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:36:44.687059: step 78270, loss = 0.52 (1606.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:36:45.480648: step 78280, loss = 0.63 (1612.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:46.270925: step 78290, loss = 0.65 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:47.170063: step 78300, loss = 0.65 (1423.6 examples/sec; 0.090 sec/batch)
2017-05-02 16:36:47.841250: step 78310, loss = 0.95 (1907.1 examples/sec; 0.067 sec/batch)
2017-05-02 16:36:48.631110: step 78320, loss = 0.71 (1620.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:49.428277: step 78330, loss = 0.64 (1605.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:36:50.217609: step 78340, loss = 0.67 (1621.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:51.012036: step 78350, loss = 0.64 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:51.794888: step 78360, loss = 0.78 (1635.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:36:52.590945: step 78370, loss = 0.66 (1607.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:36:53.381298: step 78380, loss = 0.72 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:54.165542: step 78390, loss = 0.78 (1632.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:36:55.069718: step 78400, loss = 0.69 (1415.6 examples/sec; 0.090 sec/batch)
2017-05-02 16:36:55.743085: step 78410, loss = 0.71 (1900.9 examples/sec; 0.067 sec/batch)
2017-05-02 16:36:56.531085: step 78420, loss = 0.72 (1624.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:57.319584: step 78430, loss = 0.66 (1623.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:58.110391: step 78440, loss = 0.74 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:36:58.906574: step 78450, loss = 0.74 (1607.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:36:59.685315: step 78460, loss = 0.62 (1643.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:37:00.470467: step 78470, loss = 0.72 (1630.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:01.268946: step 78480, loss = 0.74 (1603.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:37:02.054127: step 78490, loss = 0.72 (1630.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:02.939273: step 78500, loss = 0.74 (1446.1 examples/sec; 0.089 sec/batch)
2017-05-02 16:37:03.631170: step 78510, loss = 0.74 (1850.0 examples/sec; 0.069 sec/batch)
2017-05-02 16:37:04.417710: step 78520, loss = 0.63 (1627.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:05.205997: step 78530, loss = 0.75 (1623.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:05.989462: step 78540, loss = 0.76 (1633.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:37:06.780421: step 78550, loss = 0.83 (1618.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:07.565958: step 78560, loss = 0.71 (1629.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:08.354888: step 78570, loss = 0.88 (1622.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:09.145107: step 78580, loss = 0.75 (1619.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:09.940076: step 78590, loss = 0.86 (1610.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:10.855019: step 78600, loss = 0.67 (1399.0 examples/sec; 0.091 sec/batch)
2017-05-02 16:37:11.523138: step 78610, loss = 0.80 (1915.8 examples/sec; 0.067 sec/batch)
2017-05-02 16:37:12.314640: step 78620, loss = 0.61 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:13.107627: step 78630, loss = 0.72 (1614.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:13.900869: step 78640, loss = 0.66 (1613.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:14.689637: step 78650, loss = 0.64 (1622.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:15.468833: step 78660, loss = 0.76 (1642.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:37:16.254645: step 78670, loss = 0.78 (1628.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:17.049962: step 78680, loss = 0.63 (1609.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:37:17.842640: step 78690, loss = 0.82 (1614.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:18.730704: step 78700, loss = 0.76 (1441.3 examples/sec; 0.089 sec/batch)
2017-05-02 16:37:19.416743: step 78710, loss = 0.72 (1865.8 examples/sec; 0.069 sec/batch)
2017-05-02 16:37:20.203393: step 78720, loss = 0.70 (1627.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:20.993418: step 78730, loss = 0.65 (1620.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:21.785274: step 78740, loss = 0.64 (1616.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:22.586188: step 78750, loss = 0.64 (1598.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:37:23.373697: step 78760, loss = 0.54 (1625.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:24.158701: step 78770, loss = 0.72 (1630.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:24.955955: step 78780, loss = 0.70 (1605.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:37:25.743653: step 78790, loss = 0.63 (1625.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:26.647281: step 78800, loss = 0.60 (1416.5 examples/sec; 0.090 sec/batch)
2017-05-02 16:37:27.338166: step 78810, loss = 0.91 (1852.7 examples/sec; 0.069 sec/batch)
2017-05-02 16:37:28.127718: step 78820, loss = 0.57 (1621.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:28.915795: step 78830, loss = 0.80 (1624.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:29.701290: step 78840, loss = 0.67 (1629.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:30.494158: step 78850, loss = 0.82 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:31.287054: step 78860, loss = 0.69 (1614.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:32.063816: step 78870, loss = 0.79 (1647.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:37:32.865522: step 78880, loss = 0.74 (1596.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:37:33.658570: step 78890, loss = 0.78 (1614.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:34.547451: step 78900, loss = 0.77 (1440.0 examples/sec; 0.089 sec/batch)
2017-05-02 16:37:35.248379: step 78910, loss = 0.76 (1826.2 examples/sec; 0.070 sec/batch)
2017-05-02 16:37:36.034437: step 78920, loss = 0.94 (1628.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:36.824507: step 78930, loss = 0.76 (1620.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:37.617088: step 78940, loss = 0.75 (1615.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:38.411274: step 78950, loss = 0.65 (1611.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:39.206061: step 78960, loss = 0.60 (1610.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:39.988080: step 78970, loss = 0.70 (1636.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:37:40.781778: step 78980, loss = 0.66 (1612.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:41.571245: step 78990, loss = 0.53 (1621.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:42.465807: step 79000, loss = 0.55 (1430.9 examples/sec; 0.089 sec/batch)
2017-05-02 16:37:43.154108: step 79010, loss = 0.66 (1859.6 examples/sec; 0.069 sec/batch)
2017-05-02 16:37:43.929465: step 79020, loss = 0.80 (1650.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:37:44.723105: step 79030, loss = 0.69 (1612.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:45.517834: step 79040, loss = 0.76 (1610.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:46.302941: step 79050, loss = 0.75 (1630.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:47.098784: step 79060, loss = 0.70 (1608.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:37:47.878846: step 79070, loss = 0.51 (1640.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:37:48.663749: step 79080, loss = 0.86 (1630.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:37:49.451116: step 79090, loss = 0.70 (1625.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:50.342565: step 79100, loss = 0.74 (1435.9 examples/sec; 0.089 sec/batch)
2017-05-02 16:37:51.040956: step 79110, loss = 0.81 (1832.8 examples/sec; 0.070 sec/batch)
2017-05-02 16:37:51.823087: step 79120, loss = 0.79 (1636.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:37:52.614634: step 79130, loss = 0.68 (1617.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:53.408785: step 79140, loss = 0.84 (1611.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:54.187230: step 79150, loss = 0.74 (1644.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:37:54.977359: step 79160, loss = 0.87 (1620.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:55.759383: step 79170, loss = 0.61 (1636.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:37:56.551688: step 79180, loss = 0.68 (1615.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:57.341396: step 79190, loss = 0.71 (1620.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:37:58.236857: step 79200, loss = 0.70 (1429.4 examples/sec; 0.090 sec/batch)
2017-05-02 16:37:58.939097: step 79210, loss = 0.76 (1822.7 examples/sec; 0.070 sec/batch)
2017-05-02 16:37:59.720510: step 79220, loss = 0.78 (1638.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:38:00.509027: step 79230, loss = 0.79 (1623.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:01.303362: step 79240, loss = 0.60 (1611.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:02.088698: step 79250, loss = 0.71 (1629.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:02.875582: step 79260, loss = 0.78 (1626.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:03.657850: step 79270, loss = 0.86 (1636.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:38:04.448663: step 79280, loss = 0.71 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:05.243598: step 79290, loss = 0.63 (1610.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:06.132841: step 79300, loss = 0.60 (1439.4 examples/sec; 0.089 sec/batch)
2017-05-02 16:38:06.832338: step 79310, loss = 0.75 (1829.9 examples/sec; 0.070 sec/batch)
2017-05-02 16:38:07.612818: step 79320, loss = 0.68 (1640.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:38:08.396961: step 79330, loss = 0.79 (1632.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:38:09.189819: step 79340, loss = 0.66 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:09.977570: step 79350, loss = 0.66 (1624.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:10.772735: step 79360, loss = 0.73 (1609.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:38:11.561329: step 79370, loss = 0.70 (1623.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:12.362155: step 79380, loss = 0.66 (1598.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:38:13.148498: step 79390, loss = 0.74 (1627.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:14.026478: step 79400, loss = 0.71 (1457.9 examples/sec; 0.088 sec/batch)
2017-05-02 16:38:14.726347: step 79410, loss = 0.89 (1828.9 examples/sec; 0.070 sec/batch)
2017-05-02 16:38:15.511589: step 79420, loss = 0.99 (1630.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:16.299038: step 79430, loss = 0.76 (1625.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:17.090568: step 79440, loss = 0.70 (1617.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:17.890687: step 79450, loss = 0.82 (1599.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:38:18.682442: step 79460, loss = 0.62 (1616.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:19.470235: step 79470, loss = 0.57 (1624.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:20.261783: step 79480, loss = 0.65 (1617.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:21.051691: step 79490, loss = 0.63 (1620.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:21.936513: step 79500, loss = 0.70 (1446.6 examples/sec; 0.088 sec/batch)
2017-05-02 16:38:22.627465: step 79510, loss = 0.66 (1852.5 examples/sec; 0.069 sec/batch)
2017-05-02 16:38:23.416492: step 79520, loss = 0.69 (1622.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:24.200982: step 79530, loss = 0.69 (1631.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:38:24.989995: step 79540, loss = 0.74 (1622.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:25.783358: step 79550, loss = 0.85 (1613.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:26.575546: step 79560, loss = 0.66 (1615.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:27.360812: step 79570, loss = 0.64 (1630.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:28.150359: step 79580, loss = 0.63 (1621.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:28.946193: step 79590, loss = 0.73 (1608.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:38:29.836103: step 79600, loss = 0.70 (1438.3 examples/sec; 0.089 sec/batch)
2017-05-02 16:38:30.540817: step 79610, loss = 0.75 (1816.3 examples/sec; 0.070 sec/batch)
2017-05-02 16:38:31.323282: step 79620, loss = 0.78 (1635.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:38:32.103163: step 79630, loss = 0.57 (1641.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:38:32.895783: step 79640, loss = 0.74 (1614.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:33.684257: step 79650, loss = 0.74 (1623.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:34.474862: step 79660, loss = 0.69 (1619.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:35.263632: step 79670, loss = 0.85 (1622.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:36.044387: step 79680, loss = 0.67 (1639.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:38:36.837103: step 79690, loss = 0.72 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:37.727308: step 79700, loss = 0.75 (1437.9 examples/sec; 0.089 sec/batch)
2017-05-02 16:38:38.419748: step 79710, loss = 0.74 (1848.5 examples/sec; 0.069 sec/batch)
2017-05-02 16:38:39.211236: step 79720, loss = 0.73 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:39.988449: step 79730, loss = 0.69 (1646.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:38:40.779858: step 79740, loss = 0.92 (1617.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:41.580474: step 79750, loss = 0.64 (1598.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:38:42.379688: step 79760, loss = 0.74 (1601.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:38:43.171046: step 79770, loss = 0.79 (1617.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:43.947178: step 79780, loss = 0.79 (1649.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:38:44.735413: step 79790, loss = 0.82 (1623.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:45.624626: step 79800, loss = 0.68 (1439.5 examples/sec; 0.089 sec/batch)
2017-05-02 16:38:46.317116: step 79810, loss = 0.58 (1848.4 examples/sec; 0.069 sec/batch)
2017-05-02 16:38:47.102625: step 79820, loss = 0.61 (1629.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:47.881503: step 79830, loss = 0.62 (1643.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:38:48.674188: step 79840, loss = 0.74 (1614.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:49.465716: step 79850, loss = 0.66 (1617.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:50.256542: step 79860, loss = 0.76 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:51.050951: step 79870, loss = 0.68 (1611.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:51.825329: step 79880, loss = 0.84 (1652.9 examples/sec; 0.077 sec/batch)
2017-05-02 16:38:52.618832: step 79890, loss = 0.86 (1613.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:53.513496: step 79900, loss = 0.67 (1430.7 examples/sec; 0.089 sec/batch)
2017-05-02 16:38:54.212220: step 79910, loss = 0.75 (1831.9 examples/sec; 0.070 sec/batch)
2017-05-02 16:38:55.001293: step 79920, loss = 0.65 (1622.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:55.786930: step 79930, loss = 0.76 (1629.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:56.582455: step 79940, loss = 0.67 (1609.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:38:57.373805: step 79950, loss = 0.59 (1617.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:58.168078: step 79960, loss = 0.75 (1611.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:58.954425: step 79970, loss = 0.63 (1627.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:38:59.743136: step 79980, loss = 0.93 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:00.530959: step 79990, loss = 0.59 (1624.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:01.417480: step 80000, loss = 0.89 (1443.9 examples/sec; 0.089 sec/batch)
2017-05-02 16:39:02.109692: step 80010, loss = 0.56 (1849.2 examples/sec; 0.069 sec/batch)
2017-05-02 16:39:02.904149: step 80020, loss = 0.74 (1611.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:03.692502: step 80030, loss = 0.79 (1623.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:04.483610: step 80040, loss = 0.64 (1618.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:05.277982: step 80050, loss = 0.68 (1611.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:06.069468: step 80060, loss = 0.71 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:06.862333: step 80070, loss = 0.89 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:07.641811: step 80080, loss = 0.79 (1642.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:39:08.434650: step 80090, loss = 0.73 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:09.326201: step 80100, loss = 0.82 (1435.7 examples/sec; 0.089 sec/batch)
2017-05-02 16:39:10.022779: step 80110, loss = 0.69 (1837.6 examples/sec; 0.070 sec/batch)
2017-05-02 16:39:10.819082: step 80120, loss = 0.72 (1607.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:39:11.606379: step 80130, loss = 0.72 (1625.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:12.396010: step 80140, loss = 0.57 (1621.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:13.194660: step 80150, loss = 0.61 (1602.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:39:13.985642: step 80160, loss = 0.76 (1618.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:14.774968: step 80170, loss = 0.60 (1621.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:15.563222: step 80180, loss = 0.67 (1623.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:16.353572: step 80190, loss = 0.69 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:17.241856: step 80200, loss = 0.70 (1441.0 examples/sec; 0.089 sec/batch)
2017-05-02 16:39:17.933857: step 80210, loss = 0.68 (1849.7 examples/sec; 0.069 sec/batch)
2017-05-02 16:39:18.718947: step 80220, loss = 0.79 (1630.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:19.507575: step 80230, loss = 0.61 (1623.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:20.297409: step 80240, loss = 0.76 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:21.083662: step 80250, loss = 0.70 (1628.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:21.875403: step 80260, loss = 0.74 (1616.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:22.667548: step 80270, loss = 0.65 (1615.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:23.457112: step 80280, loss = 0.69 (1621.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:24.242462: step 80290, loss = 0.67 (1629.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:25.138678: step 80300, loss = 0.68 (1428.2 examples/sec; 0.090 sec/batch)
2017-05-02 16:39:25.825814: step 80310, loss = 0.60 (1862.8 examples/sec; 0.069 sec/batch)
2017-05-02 16:39:26.618534: step 80320, loss = 0.54 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:27.407027: step 80330, loss = 0.64 (1623.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:28.188392: step 80340, loss = 0.85 (1638.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:39:28.974329: step 80350, loss = 0.67 (1628.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:29.767455: step 80360, loss = 0.64 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:30.559106: step 80370, loss = 0.59 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:31.349140: step 80380, loss = 0.73 (1620.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:32.131960: step 80390, loss = 0.71 (1635.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:39:33.022725: step 80400, loss = 0.65 (1437.0 examples/sec; 0.089 sec/batch)
2017-05-02 16:39:33.710281: step 80410, loss = 0.62 (1861.7 examples/sec; 0.069 sec/batch)
2017-05-02 16:39:34.505874: step 80420, loss = 0.79 (1608.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:39:35.295258: step 80430, loss = 0.69 (1621.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:36.088460: step 80440, loss = 0.73 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:36.887159: step 80450, loss = 0.73 (1602.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:39:37.680091: step 80460, loss = 0.65 (1614.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:38.474968: step 80470, loss = 0.60 (1610.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:39.260477: step 80480, loss = 0.72 (1629.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:40.043189: step 80490, loss = 0.64 (1635.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:39:40.931091: step 80500, loss = 0.72 (1441.6 examples/sec; 0.089 sec/batch)
2017-05-02 16:39:41.638646: step 80510, loss = 0.78 (1809.0 examples/sec; 0.071 sec/batch)
2017-05-02 16:39:42.428402: step 80520, loss = 0.67 (1620.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:43.222478: step 80530, loss = 0.68 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:44.004613: step 80540, loss = 0.86 (1636.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:39:44.800278: step 80550, loss = 0.75 (1608.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:39:45.593504: step 80560, loss = 0.74 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:46.385141: step 80570, loss = 0.65 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:47.181630: step 80580, loss = 0.79 (1607.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:39:47.958111: step 80590, loss = 0.76 (1648.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:39:48.844478: step 80600, loss = 0.78 (1444.1 examples/sec; 0.089 sec/batch)
2017-05-02 16:39:49.542959: step 80610, loss = 0.65 (1832.5 examples/sec; 0.070 sec/batch)
2017-05-02 16:39:50.332721: step 80620, loss = 0.75 (1620.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:51.118814: step 80630, loss = 0.60 (1628.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:51.906907: step 80640, loss = 0.71 (1624.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:52.701586: step 80650, loss = 0.66 (1610.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:53.497392: step 80660, loss = 0.63 (1608.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:39:54.287112: step 80670, loss = 0.72 (1620.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:55.081824: step 80680, loss = 0.65 (1610.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:55.859994: step 80690, loss = 0.71 (1644.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:39:56.770643: step 80700, loss = 0.59 (1405.6 examples/sec; 0.091 sec/batch)
2017-05-02 16:39:57.443134: step 80710, loss = 0.63 (1903.4 examples/sec; 0.067 sec/batch)
2017-05-02 16:39:58.231627: step 80720, loss = 0.59 (1623.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:59.023177: step 80730, loss = 0.74 (1617.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:39:59.804899: step 80740, loss = 0.72 (1637.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:40:00.592637: step 80750, loss = 0.87 (1624.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:01.388213: step 80760, loss = 0.69 (1608.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:40:02.187315: step 80770, loss = 0.68 (1601.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:40:02.980508: step 80780, loss = 0.70 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:03.755502: step 80790, loss = 0.82 (1651.6 examples/sec; 0.077 sec/batch)
2017-05-02 16:40:04.646436: step 80800, loss = 0.68 (1436.7 examples/sec; 0.089 sec/batch)
2017-05-02 16:40:05.340487: step 80810, loss = 0.68 (1844.3 examples/sec; 0.069 sec/batch)
2017-05-02 16:40:06.124974: step 80820, loss = 0.61 (1631.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:40:06.930222: step 80830, loss = 0.52 (1589.6 examples/sec; 0.081 sec/batch)
2017-05-02 16:40:07.715813: step 80840, loss = 0.69 (1629.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:08.514345: step 80850, loss = 0.60 (1602.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:40:09.308045: step 80860, loss = 0.68 (1612.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:10.099103: step 80870, loss = 0.60 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:10.899704: step 80880, loss = 0.71 (1598.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:40:11.684336: step 80890, loss = 0.77 (1631.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:40:12.575031: step 80900, loss = 0.68 (1437.1 examples/sec; 0.089 sec/batch)
2017-05-02 16:40:13.272557: step 80910, loss = 0.67 (1835.1 examples/sec; 0.070 sec/batch)
2017-05-02 16:40:14.059024: step 80920, loss = 0.88 (1627.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:14.850023: step 80930, loss = 0.63 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:15.637262: step 80940, loss = 0.78 (1625.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:16.427698: step 80950, loss = 0.60 (1619.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:17.219294: step 80960, loss = 0.92 (1617.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:18.011127: step 80970, loss = 0.71 (1616.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:18.793781: step 80980, loss = 0.74 (1635.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:40:19.579188: step 80990, loss = 0.66 (1629.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:20.459019: step 81000, loss = 0.69 (1454.8 examples/sec; 0.088 sec/batch)
2017-05-02 16:40:21.165927: step 81010, loss = 0.86 (1810.7 examples/sec; 0.071 sec/batch)
2017-05-02 16:40:21.952639: step 81020, loss = 0.71 (1627.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:22.741904: step 81030, loss = 0.61 (1621.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:23.527738: step 81040, loss = 0.75 (1628.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:24.321651: step 81050, loss = 0.89 (1612.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:25.115561: step 81060, loss = 0.77 (1612.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:25.915647: step 81070, loss = 0.71 (1599.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:40:26.711575: step 81080, loss = 0.85 (1608.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:40:27.502200: step 81090, loss = 0.73 (1619.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:28.389456: step 81100, loss = 0.79 (1442.6 examples/sec; 0.089 sec/batch)
2017-05-02 16:40:29.093221: step 81110, loss = 0.52 (1818.8 examples/sec; 0.070 sec/batch)
2017-05-02 16:40:29.876804: step 81120, loss = 0.74 (1633.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:40:30.668181: step 81130, loss = 0.80 (1617.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:31.449653: step 81140, loss = 0.75 (1637.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:40:32.236600: step 81150, loss = 0.68 (1626.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:33.032524: step 81160, loss = 0.62 (1608.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:40:33.829335: step 81170, loss = 0.79 (1606.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:40:34.622444: step 81180, loss = 0.79 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:35.408114: step 81190, loss = 0.67 (1629.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:36.298484: step 81200, loss = 0.73 (1437.6 examples/sec; 0.089 sec/batch)
2017-05-02 16:40:36.990009: step 81210, loss = 0.66 (1851.0 examples/sec; 0.069 sec/batch)
2017-05-02 16:40:37.782626: step 81220, loss = 0.65 (1614.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:38.572178: step 81230, loss = 0.66 (1621.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:39.363151: step 81240, loss = 0.60 (1618.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:40.149156: step 81250, loss = 0.89 (1628.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:40.937451: step 81260, loss = 0.61 (1623.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:41.725203: step 81270, loss = 0.71 (1624.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:42.518399: step 81280, loss = 0.72 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:43.312372: step 81290, loss = 0.79 (1612.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:44.192090: step 81300, loss = 0.67 (1455.0 examples/sec; 0.088 sec/batch)
2017-05-02 16:40:44.889525: step 81310, loss = 0.67 (1835.3 examples/sec; 0.070 sec/batch)
2017-05-02 16:40:45.682982: step 81320, loss = 0.90 (1613.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:46.471936: step 81330, loss = 0.66 (1622.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:47.265731: step 81340, loss = 0.73 (1612.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:48.052405: step 81350, loss = 0.72 (1627.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:48.840051: step 81360, loss = 0.75 (1625.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:49.626985: step 81370, loss = 0.67 (1626.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:50.417215: step 81380, loss = 0.63 (1619.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:51.217733: step 81390, loss = 0.80 (1599.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:40:52.101042: step 81400, loss = 0.66 (1449.1 examples/sec; 0.088 sec/batch)
2017-05-02 16:40:52.792330: step 81410, loss = 0.67 (1851.6 examples/sec; 0.069 sec/batch)
2017-05-02 16:40:53.587270: step 81420, loss = 0.73 (1610.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:54.375987: step 81430, loss = 0.66 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:55.174447: step 81440, loss = 0.67 (1603.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:40:55.955929: step 81450, loss = 0.64 (1637.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:40:56.746550: step 81460, loss = 0.74 (1619.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:57.540683: step 81470, loss = 0.64 (1611.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:58.334191: step 81480, loss = 0.59 (1613.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:59.120333: step 81490, loss = 0.63 (1628.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:40:59.996294: step 81500, loss = 0.69 (1461.3 examples/sec; 0.088 sec/batch)
2017-05-02 16:41:00.687799: step 81510, loss = 0.73 (1851.0 examples/sec; 0.069 sec/batch)
2017-05-02 16:41:01.480126: step 81520, loss = 0.80 (1615.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:02.262640: step 81530, loss = 0.80 (1635.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:41:03.060684: step 81540, loss = 0.60 (1603.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:41:03.836759: step 81550, loss = 0.75 (1649.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:41:04.626452: step 81560, loss = 0.71 (1620.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:05.422036: step 81570, loss = 0.77 (1608.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:41:06.214500: step 81580, loss = 0.85 (1615.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:07.002300: step 81590, loss = 0.77 (1624.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:07.886791: step 81600, loss = 0.71 (1447.1 examples/sec; 0.088 sec/batch)
2017-05-02 16:41:08.587324: step 81610, loss = 0.72 (1827.2 examples/sec; 0.070 sec/batch)
2017-05-02 16:41:09.379518: step 81620, loss = 0.62 (1615.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:10.177658: step 81630, loss = 0.72 (1603.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:41:10.969325: step 81640, loss = 0.68 (1616.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:11.751494: step 81650, loss = 0.81 (1636.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:41:12.545788: step 81660, loss = 0.82 (1611.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:13.340269: step 81670, loss = 0.78 (1611.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:14.131649: step 81680, loss = 0.76 (1617.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:14.926192: step 81690, loss = 0.66 (1611.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:15.824278: step 81700, loss = 0.64 (1425.3 examples/sec; 0.090 sec/batch)
2017-05-02 16:41:16.511840: step 81710, loss = 0.76 (1861.6 examples/sec; 0.069 sec/batch)
2017-05-02 16:41:17.305142: step 81720, loss = 0.79 (1613.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:18.089162: step 81730, loss = 0.71 (1632.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:41:18.877677: step 81740, loss = 0.68 (1623.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:19.660276: step 81750, loss = 0.72 (1635.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:41:20.444452: step 81760, loss = 0.76 (1632.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:41:21.236431: step 81770, loss = 0.68 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:22.022696: step 81780, loss = 0.77 (1627.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:22.822963: step 81790, loss = 0.72 (1599.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:41:23.730934: step 81800, loss = 0.64 (1409.7 examples/sec; 0.091 sec/batch)
2017-05-02 16:41:24.402670: step 81810, loss = 0.81 (1905.5 examples/sec; 0.067 sec/batch)
2017-05-02 16:41:25.193548: step 81820, loss = 0.64 (1618.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:25.983096: step 81830, loss = 0.63 (1621.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:26.773658: step 81840, loss = 0.60 (1619.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:27.560797: step 81850, loss = 0.75 (1626.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:28.344648: step 81860, loss = 0.70 (1633.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:41:29.133360: step 81870, loss = 0.65 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:29.926910: step 81880, loss = 0.84 (1613.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:30.717611: step 81890, loss = 0.61 (1618.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:31.600054: step 81900, loss = 0.55 (1450.5 examples/sec; 0.088 sec/batch)
2017-05-02 16:41:32.290863: step 81910, loss = 0.65 (1852.9 examples/sec; 0.069 sec/batch)
2017-05-02 16:41:33.079112: step 81920, loss = 0.63 (1623.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:33.871746: step 81930, loss = 0.59 (1614.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:34.672498: step 81940, loss = 0.68 (1598.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:41:35.464658: step 81950, loss = 0.59 (1615.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:36.254371: step 81960, loss = 0.69 (1620.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:37.042699: step 81970, loss = 0.68 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:37.826936: step 81980, loss = 0.64 (1632.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:41:38.613918: step 81990, loss = 0.63 (1626.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:39.498539: step 82000, loss = 0.75 (1446.9 examples/sec; 0.088 sec/batch)
2017-05-02 16:41:40.180409: step 82010, loss = 0.61 (1877.2 examples/sec; 0.068 sec/batch)
2017-05-02 16:41:40.975470: step 82020, loss = 0.72 (1609.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:41:41.778649: step 82030, loss = 0.69 (1593.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:41:42.571113: step 82040, loss = 0.64 (1615.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:43.358083: step 82050, loss = 0.67 (1626.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:44.139302: step 82060, loss = 0.64 (1638.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:41:44.930243: step 82070, loss = 0.60 (1618.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:45.724982: step 82080, loss = 0.81 (1610.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:46.514824: step 82090, loss = 0.73 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:47.397320: step 82100, loss = 0.69 (1450.4 examples/sec; 0.088 sec/batch)
2017-05-02 16:41:48.087893: step 82110, loss = 0.85 (1853.6 examples/sec; 0.069 sec/batch)
2017-05-02 16:41:48.874966: step 82120, loss = 0.66 (1626.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:49.662726: step 82130, loss = 0.70 (1624.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:50.452315: step 82140, loss = 0.79 (1621.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:51.247573: step 82150, loss = 0.57 (1609.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:41:52.030007: step 82160, loss = 0.55 (1635.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:41:52.829555: step 82170, loss = 0.88 (1600.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:41:53.629297: step 82180, loss = 0.79 (1600.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:41:54.419203: step 82190, loss = 0.60 (1620.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:55.310078: step 82200, loss = 0.72 (1436.8 examples/sec; 0.089 sec/batch)
2017-05-02 16:41:55.989807: step 82210, loss = 0.66 (1883.1 examples/sec; 0.068 sec/batch)
2017-05-02 16:41:56.787384: step 82220, loss = 0.74 (1604.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:41:57.571969: step 82230, loss = 0.80 (1631.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:41:58.367472: step 82240, loss = 0.83 (1609.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:41:59.158574: step 82250, loss = 0.85 (1618.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:41:59.937554: step 82260, loss = 0.67 (1643.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:42:00.728625: step 82270, loss = 0.58 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:01.518778: step 82280, loss = 0.70 (1619.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:02.300627: step 82290, loss = 0.76 (1637.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:42:03.205333: step 82300, loss = 0.67 (1414.8 examples/sec; 0.090 sec/batch)
2017-05-02 16:42:03.880467: step 82310, loss = 0.57 (1895.9 examples/sec; 0.068 sec/batch)
2017-05-02 16:42:04.674153: step 82320, loss = 0.75 (1612.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:05.459366: step 82330, loss = 0.67 (1630.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:06.249683: step 82340, loss = 0.60 (1619.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:07.042627: step 82350, loss = 0.76 (1614.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:07.824956: step 82360, loss = 0.83 (1636.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:42:08.618211: step 82370, loss = 0.75 (1613.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:09.412828: step 82380, loss = 0.76 (1610.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:10.215141: step 82390, loss = 0.57 (1595.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:42:11.102104: step 82400, loss = 0.78 (1443.1 examples/sec; 0.089 sec/batch)
2017-05-02 16:42:11.785115: step 82410, loss = 0.58 (1874.1 examples/sec; 0.068 sec/batch)
2017-05-02 16:42:12.581458: step 82420, loss = 0.60 (1607.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:42:13.373015: step 82430, loss = 0.59 (1617.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:14.161298: step 82440, loss = 0.68 (1623.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:14.949099: step 82450, loss = 0.62 (1624.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:15.734599: step 82460, loss = 0.70 (1629.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:16.522369: step 82470, loss = 0.72 (1624.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:17.307088: step 82480, loss = 0.74 (1631.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:42:18.093389: step 82490, loss = 0.71 (1627.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:18.984628: step 82500, loss = 0.58 (1436.2 examples/sec; 0.089 sec/batch)
2017-05-02 16:42:19.669712: step 82510, loss = 0.62 (1868.4 examples/sec; 0.069 sec/batch)
2017-05-02 16:42:20.455674: step 82520, loss = 0.73 (1628.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:21.250552: step 82530, loss = 0.73 (1610.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:22.053496: step 82540, loss = 0.66 (1594.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:42:22.842764: step 82550, loss = 0.72 (1621.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:23.624273: step 82560, loss = 0.72 (1637.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:42:24.416370: step 82570, loss = 0.68 (1616.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:25.206673: step 82580, loss = 0.81 (1619.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:26.000735: step 82590, loss = 0.68 (1612.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:26.897400: step 82600, loss = 0.69 (1427.5 examples/sec; 0.090 sec/batch)
2017-05-02 16:42:27.585370: step 82610, loss = 0.85 (1860.5 examples/sec; 0.069 sec/batch)
2017-05-02 16:42:28.374719: step 82620, loss = 0.80 (1621.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:29.169716: step 82630, loss = 0.77 (1610.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:42:29.957205: step 82640, loss = 0.78 (1625.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:30.748364: step 82650, loss = 0.65 (1617.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:31.532122: step 82660, loss = 0.69 (1633.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:42:32.321428: step 82670, loss = 0.71 (1621.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:33.116156: step 82680, loss = 0.67 (1610.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:33.908570: step 82690, loss = 0.75 (1615.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:34.798796: step 82700, loss = 0.75 (1437.9 examples/sec; 0.089 sec/batch)
2017-05-02 16:42:35.485301: step 82710, loss = 0.69 (1864.5 examples/sec; 0.069 sec/batch)
2017-05-02 16:42:36.279222: step 82720, loss = 0.65 (1612.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:37.069189: step 82730, loss = 0.65 (1620.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:37.862552: step 82740, loss = 0.90 (1613.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:38.654788: step 82750, loss = 0.71 (1615.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:39.440045: step 82760, loss = 0.78 (1630.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:40.226639: step 82770, loss = 0.80 (1627.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:41.018473: step 82780, loss = 0.64 (1616.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:41.811841: step 82790, loss = 0.63 (1613.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:42.698637: step 82800, loss = 0.85 (1443.4 examples/sec; 0.089 sec/batch)
2017-05-02 16:42:43.388711: step 82810, loss = 0.57 (1854.9 examples/sec; 0.069 sec/batch)
2017-05-02 16:42:44.175401: step 82820, loss = 0.70 (1627.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:44.964913: step 82830, loss = 0.68 (1621.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:45.747066: step 82840, loss = 0.99 (1636.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:42:46.542661: step 82850, loss = 0.74 (1608.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:42:47.334315: step 82860, loss = 0.77 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:48.118593: step 82870, loss = 0.65 (1632.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:42:48.916126: step 82880, loss = 0.87 (1604.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:42:49.701794: step 82890, loss = 0.77 (1629.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:50.598649: step 82900, loss = 0.83 (1427.2 examples/sec; 0.090 sec/batch)
2017-05-02 16:42:51.283289: step 82910, loss = 0.81 (1869.6 examples/sec; 0.068 sec/batch)
2017-05-02 16:42:52.066137: step 82920, loss = 0.76 (1635.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:42:52.858299: step 82930, loss = 0.65 (1615.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:53.653071: step 82940, loss = 0.77 (1610.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:54.439689: step 82950, loss = 0.65 (1627.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:55.232169: step 82960, loss = 0.62 (1615.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:56.013484: step 82970, loss = 0.84 (1638.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:42:56.797963: step 82980, loss = 0.61 (1631.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:42:57.591314: step 82990, loss = 0.72 (1613.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:42:58.499381: step 83000, loss = 0.65 (1409.6 examples/sec; 0.091 sec/batch)
2017-05-02 16:42:59.177456: step 83010, loss = 0.81 (1887.7 examples/sec; 0.068 sec/batch)
2017-05-02 16:42:59.950497: step 83020, loss = 0.88 (1655.8 examples/sec; 0.077 sec/batch)
2017-05-02 16:43:00.744312: step 83030, loss = 0.68 (1612.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:01.535959: step 83040, loss = 0.71 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:02.324269: step 83050, loss = 0.74 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:03.111525: step 83060, loss = 0.70 (1625.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:03.894086: step 83070, loss = 0.76 (1635.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:43:04.683771: step 83080, loss = 0.55 (1620.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:05.473866: step 83090, loss = 0.63 (1620.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:06.361288: step 83100, loss = 0.72 (1442.4 examples/sec; 0.089 sec/batch)
2017-05-02 16:43:07.057160: step 83110, loss = 0.74 (1839.4 examples/sec; 0.070 sec/batch)
2017-05-02 16:43:07.829736: step 83120, loss = 0.81 (1656.8 examples/sec; 0.077 sec/batch)
2017-05-02 16:43:08.625818: step 83130, loss = 0.64 (1607.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:43:09.417924: step 83140, loss = 0.69 (1616.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:10.205533: step 83150, loss = 0.66 (1625.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:10.998204: step 83160, loss = 0.70 (1614.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:11.773768: step 83170, loss = 0.82 (1650.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:43:12.560126: step 83180, loss = 0.76 (1627.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:13.350860: step 83190, loss = 0.74 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:14.241485: step 83200, loss = 0.76 (1437.2 examples/sec; 0.089 sec/batch)
2017-05-02 16:43:14.938712: step 83210, loss = 0.60 (1835.8 examples/sec; 0.070 sec/batch)
2017-05-02 16:43:15.717321: step 83220, loss = 0.77 (1644.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:43:16.512109: step 83230, loss = 0.91 (1610.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:17.306435: step 83240, loss = 0.65 (1611.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:18.104033: step 83250, loss = 0.67 (1604.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:43:18.898097: step 83260, loss = 0.85 (1612.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:19.682097: step 83270, loss = 0.68 (1632.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:43:20.475300: step 83280, loss = 0.72 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:21.266167: step 83290, loss = 0.87 (1618.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:22.149027: step 83300, loss = 0.76 (1449.8 examples/sec; 0.088 sec/batch)
2017-05-02 16:43:22.843032: step 83310, loss = 0.68 (1844.4 examples/sec; 0.069 sec/batch)
2017-05-02 16:43:23.620517: step 83320, loss = 0.81 (1646.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:43:24.413251: step 83330, loss = 0.70 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:25.205736: step 83340, loss = 0.74 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:25.994730: step 83350, loss = 0.76 (1622.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:26.795682: step 83360, loss = 0.68 (1598.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:43:27.570460: step 83370, loss = 0.78 (1652.1 examples/sec; 0.077 sec/batch)
2017-05-02 16:43:28.358793: step 83380, loss = 0.78 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:29.148703: step 83390, loss = 0.70 (1620.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:30.128023: step 83400, loss = 0.78 (1307.0 examples/sec; 0.098 sec/batch)
2017-05-02 16:43:30.775229: step 83410, loss = 0.61 (1977.8 examples/sec; 0.065 sec/batch)
2017-05-02 16:43:31.558079: step 83420, loss = 0.54 (1635.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:43:32.353025: step 83430, loss = 0.74 (1610.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:33.138088: step 83440, loss = 0.75 (1630.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:33.935464: step 83450, loss = 0.73 (1605.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:43:34.724715: step 83460, loss = 0.69 (1621.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:35.512163: step 83470, loss = 0.62 (1625.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:36.301414: step 83480, loss = 0.69 (1621.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:37.093985: step 83490, loss = 0.69 (1615.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:37.986295: step 83500, loss = 0.70 (1434.5 examples/sec; 0.089 sec/batch)
2017-05-02 16:43:38.675775: step 83510, loss = 0.62 (1856.5 examples/sec; 0.069 sec/batch)
2017-05-02 16:43:39.454792: step 83520, loss = 0.70 (1643.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:43:40.242487: step 83530, loss = 0.69 (1625.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:41.028187: step 83540, loss = 0.68 (1629.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:41.816666: step 83550, loss = 0.64 (1623.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:42.608962: step 83560, loss = 0.77 (1615.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:43.393926: step 83570, loss = 0.68 (1630.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:43:44.182766: step 83580, loss = 0.65 (1622.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:44.969608: step 83590, loss = 0.62 (1626.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:45.870304: step 83600, loss = 0.70 (1421.1 examples/sec; 0.090 sec/batch)
2017-05-02 16:43:46.565965: step 83610, loss = 0.63 (1840.0 examples/sec; 0.070 sec/batch)
2017-05-02 16:43:47.355119: step 83620, loss = 0.65 (1622.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:48.137137: step 83630, loss = 0.73 (1636.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:43:48.926082: step 83640, loss = 0.81 (1622.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:49.717796: step 83650, loss = 0.65 (1616.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:50.512809: step 83660, loss = 0.64 (1610.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:43:51.308716: step 83670, loss = 0.80 (1608.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:43:52.091825: step 83680, loss = 0.64 (1634.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:43:52.878989: step 83690, loss = 0.70 (1626.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:53.770625: step 83700, loss = 0.61 (1435.6 examples/sec; 0.089 sec/batch)
2017-05-02 16:43:54.458082: step 83710, loss = 0.64 (1861.9 examples/sec; 0.069 sec/batch)
2017-05-02 16:43:55.249965: step 83720, loss = 0.72 (1616.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:56.026485: step 83730, loss = 0.86 (1648.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:43:56.822835: step 83740, loss = 0.88 (1607.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:43:57.614451: step 83750, loss = 0.81 (1617.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:58.401113: step 83760, loss = 0.74 (1627.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:43:59.202093: step 83770, loss = 0.64 (1598.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:43:59.984084: step 83780, loss = 0.84 (1636.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:44:00.785343: step 83790, loss = 0.64 (1597.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:44:01.686321: step 83800, loss = 0.79 (1420.7 examples/sec; 0.090 sec/batch)
2017-05-02 16:44:02.380173: step 83810, loss = 0.61 (1844.8 examples/sec; 0.069 sec/batch)
2017-05-02 16:44:03.162174: step 83820, loss = 0.77 (1636.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:44:03.947422: step 83830, loss = 0.66 (1630.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:04.732161: step 83840, loss = 0.72 (1631.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:44:05.523625: step 83850, loss = 0.79 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:06.317060: step 83860, loss = 0.70 (1613.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:07.109992: step 83870, loss = 0.69 (1614.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:07.891565: step 83880, loss = 0.62 (1637.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:44:08.685473: step 83890, loss = 0.80 (1612.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:09.569729: step 83900, loss = 0.78 (1447.5 examples/sec; 0.088 sec/batch)
2017-05-02 16:44:10.272485: step 83910, loss = 0.74 (1821.4 examples/sec; 0.070 sec/batch)
2017-05-02 16:44:11.069551: step 83920, loss = 0.76 (1605.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:44:11.846566: step 83930, loss = 0.62 (1647.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:44:12.637318: step 83940, loss = 0.73 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:13.423504: step 83950, loss = 0.69 (1628.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:14.211677: step 83960, loss = 0.70 (1624.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:14.998915: step 83970, loss = 0.73 (1626.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:15.777663: step 83980, loss = 0.78 (1643.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:44:16.569537: step 83990, loss = 0.70 (1616.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:17.464571: step 84000, loss = 0.65 (1430.1 examples/sec; 0.090 sec/batch)
2017-05-02 16:44:18.162401: step 84010, loss = 0.71 (1834.3 examples/sec; 0.070 sec/batch)
2017-05-02 16:44:18.948633: step 84020, loss = 0.79 (1628.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:19.731953: step 84030, loss = 0.63 (1634.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:44:20.519236: step 84040, loss = 0.65 (1625.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:21.311957: step 84050, loss = 0.78 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:22.097462: step 84060, loss = 0.63 (1629.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:22.886286: step 84070, loss = 0.63 (1622.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:23.661394: step 84080, loss = 0.78 (1651.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:44:24.450188: step 84090, loss = 0.68 (1622.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:25.341528: step 84100, loss = 0.82 (1436.0 examples/sec; 0.089 sec/batch)
2017-05-02 16:44:26.042578: step 84110, loss = 0.64 (1825.9 examples/sec; 0.070 sec/batch)
2017-05-02 16:44:26.845501: step 84120, loss = 0.78 (1594.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:44:27.623724: step 84130, loss = 0.64 (1644.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:44:28.410612: step 84140, loss = 0.81 (1626.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:29.206365: step 84150, loss = 0.68 (1608.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:44:29.993231: step 84160, loss = 0.69 (1626.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:30.785117: step 84170, loss = 0.65 (1616.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:31.569047: step 84180, loss = 0.86 (1632.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:44:32.354478: step 84190, loss = 0.69 (1629.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:33.255478: step 84200, loss = 0.73 (1420.7 examples/sec; 0.090 sec/batch)
2017-05-02 16:44:33.946746: step 84210, loss = 0.68 (1851.7 examples/sec; 0.069 sec/batch)
2017-05-02 16:44:34.741597: step 84220, loss = 0.60 (1610.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:35.520020: step 84230, loss = 0.75 (1644.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:44:36.309165: step 84240, loss = 0.50 (1622.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:37.102369: step 84250, loss = 0.70 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:37.899192: step 84260, loss = 0.59 (1606.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:44:38.689471: step 84270, loss = 0.71 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:39.476148: step 84280, loss = 0.64 (1627.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:40.255300: step 84290, loss = 0.94 (1642.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:44:41.148249: step 84300, loss = 0.83 (1433.4 examples/sec; 0.089 sec/batch)
2017-05-02 16:44:41.846972: step 84310, loss = 0.74 (1831.9 examples/sec; 0.070 sec/batch)
2017-05-02 16:44:42.640418: step 84320, loss = 0.70 (1613.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:43.419124: step 84330, loss = 0.67 (1643.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:44:44.203031: step 84340, loss = 0.67 (1632.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:44:45.001995: step 84350, loss = 0.77 (1602.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:44:45.800612: step 84360, loss = 0.68 (1602.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:44:46.586307: step 84370, loss = 0.55 (1629.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:47.365455: step 84380, loss = 0.83 (1642.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:44:48.144918: step 84390, loss = 0.85 (1642.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:44:49.051929: step 84400, loss = 0.82 (1411.2 examples/sec; 0.091 sec/batch)
2017-05-02 16:44:49.746568: step 84410, loss = 0.63 (1842.7 examples/sec; 0.069 sec/batch)
2017-05-02 16:44:50.540452: step 84420, loss = 0.57 (1612.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:51.326424: step 84430, loss = 0.67 (1628.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:52.114757: step 84440, loss = 0.62 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:52.906872: step 84450, loss = 0.74 (1615.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:53.698114: step 84460, loss = 0.75 (1617.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:54.488627: step 84470, loss = 0.69 (1619.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:55.277607: step 84480, loss = 0.55 (1622.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:56.057120: step 84490, loss = 0.75 (1642.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:44:56.951602: step 84500, loss = 0.64 (1431.0 examples/sec; 0.089 sec/batch)
2017-05-02 16:44:57.638398: step 84510, loss = 0.66 (1863.7 examples/sec; 0.069 sec/batch)
2017-05-02 16:44:58.429237: step 84520, loss = 0.72 (1618.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:59.216632: step 84530, loss = 0.57 (1625.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:44:59.993536: step 84540, loss = 0.77 (1647.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:45:00.781091: step 84550, loss = 0.67 (1625.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:01.569186: step 84560, loss = 0.72 (1624.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:02.366723: step 84570, loss = 0.72 (1605.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:45:03.153540: step 84580, loss = 0.71 (1626.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:03.927842: step 84590, loss = 0.72 (1653.1 examples/sec; 0.077 sec/batch)
2017-05-02 16:45:04.818715: step 84600, loss = 0.72 (1436.8 examples/sec; 0.089 sec/batch)
2017-05-02 16:45:05.506023: step 84610, loss = 0.68 (1862.3 examples/sec; 0.069 sec/batch)
2017-05-02 16:45:06.301115: step 84620, loss = 0.62 (1609.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:45:07.086243: step 84630, loss = 0.73 (1630.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:07.871407: step 84640, loss = 0.72 (1630.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:08.658116: step 84650, loss = 0.78 (1627.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:09.451271: step 84660, loss = 0.67 (1613.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:10.247042: step 84670, loss = 0.63 (1608.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:45:11.035799: step 84680, loss = 0.81 (1622.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:11.813214: step 84690, loss = 0.61 (1646.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:45:12.702153: step 84700, loss = 0.65 (1439.9 examples/sec; 0.089 sec/batch)
2017-05-02 16:45:13.400957: step 84710, loss = 0.67 (1831.7 examples/sec; 0.070 sec/batch)
2017-05-02 16:45:14.186731: step 84720, loss = 0.63 (1629.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:14.975185: step 84730, loss = 0.82 (1623.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:15.756609: step 84740, loss = 0.63 (1638.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:45:16.549413: step 84750, loss = 0.75 (1614.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:17.341174: step 84760, loss = 0.76 (1616.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:18.132293: step 84770, loss = 0.72 (1618.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:18.916908: step 84780, loss = 0.64 (1631.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:45:19.696318: step 84790, loss = 0.76 (1642.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:45:20.587211: step 84800, loss = 0.76 (1436.8 examples/sec; 0.089 sec/batch)
2017-05-02 16:45:21.273354: step 84810, loss = 0.68 (1865.5 examples/sec; 0.069 sec/batch)
2017-05-02 16:45:22.060656: step 84820, loss = 0.62 (1625.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:22.851215: step 84830, loss = 0.73 (1619.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:23.628512: step 84840, loss = 0.63 (1646.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:45:24.422648: step 84850, loss = 0.71 (1611.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:25.215400: step 84860, loss = 0.67 (1614.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:26.003329: step 84870, loss = 0.63 (1624.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:26.793033: step 84880, loss = 0.54 (1620.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:27.572522: step 84890, loss = 0.68 (1642.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:45:28.463168: step 84900, loss = 0.69 (1437.2 examples/sec; 0.089 sec/batch)
2017-05-02 16:45:29.159900: step 84910, loss = 0.86 (1837.2 examples/sec; 0.070 sec/batch)
2017-05-02 16:45:29.951378: step 84920, loss = 0.79 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:30.743247: step 84930, loss = 0.75 (1616.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:31.531483: step 84940, loss = 0.73 (1623.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:32.318430: step 84950, loss = 0.72 (1626.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:33.116994: step 84960, loss = 0.71 (1602.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:45:33.908894: step 84970, loss = 0.76 (1616.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:34.701284: step 84980, loss = 0.61 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:35.490166: step 84990, loss = 0.64 (1622.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:36.381028: step 85000, loss = 0.69 (1436.8 examples/sec; 0.089 sec/batch)
2017-05-02 16:45:37.080941: step 85010, loss = 0.69 (1828.8 examples/sec; 0.070 sec/batch)
2017-05-02 16:45:37.881078: step 85020, loss = 0.75 (1599.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:45:38.676619: step 85030, loss = 0.73 (1609.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:45:39.468159: step 85040, loss = 0.73 (1617.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:40.256783: step 85050, loss = 0.63 (1623.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:41.052633: step 85060, loss = 0.56 (1608.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:45:41.851774: step 85070, loss = 0.89 (1601.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:45:42.647068: step 85080, loss = 0.65 (1609.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:45:43.431240: step 85090, loss = 0.77 (1632.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:45:44.317908: step 85100, loss = 0.79 (1443.6 examples/sec; 0.089 sec/batch)
2017-05-02 16:45:45.015728: step 85110, loss = 0.82 (1834.3 examples/sec; 0.070 sec/batch)
2017-05-02 16:45:45.812641: step 85120, loss = 0.61 (1606.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:45:46.607681: step 85130, loss = 0.61 (1610.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:45:47.397635: step 85140, loss = 0.57 (1620.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:48.186153: step 85150, loss = 0.75 (1623.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:48.982452: step 85160, loss = 0.67 (1607.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:45:49.781151: step 85170, loss = 0.56 (1602.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:45:50.572579: step 85180, loss = 0.60 (1617.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:51.366104: step 85190, loss = 0.72 (1613.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:52.252086: step 85200, loss = 0.56 (1444.7 examples/sec; 0.089 sec/batch)
2017-05-02 16:45:52.946897: step 85210, loss = 0.58 (1842.2 examples/sec; 0.069 sec/batch)
2017-05-02 16:45:53.741869: step 85220, loss = 0.70 (1610.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:54.536514: step 85230, loss = 0.58 (1610.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:55.325564: step 85240, loss = 0.82 (1622.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:56.115530: step 85250, loss = 0.55 (1620.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:56.912816: step 85260, loss = 0.62 (1605.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:45:57.709274: step 85270, loss = 0.64 (1607.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:45:58.500559: step 85280, loss = 0.71 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:45:59.298356: step 85290, loss = 0.82 (1604.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:46:00.185759: step 85300, loss = 0.64 (1442.4 examples/sec; 0.089 sec/batch)
2017-05-02 16:46:00.879394: step 85310, loss = 0.75 (1845.3 examples/sec; 0.069 sec/batch)
2017-05-02 16:46:01.675340: step 85320, loss = 0.80 (1608.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:46:02.469426: step 85330, loss = 0.64 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:46:03.262641: step 85340, loss = 0.65 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:46:04.042833: step 85350, loss = 0.71 (1640.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:46:04.835085: step 85360, loss = 0.76 (1615.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:46:05.631362: step 85370, loss = 0.80 (1607.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:46:06.423529: step 85380, loss = 0.75 (1615.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:46:07.221789: step 85390, loss = 0.79 (1603.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:46:08.109582: step 85400, loss = 0.69 (1441.8 examples/sec; 0.089 sec/batch)
2017-05-02 16:46:08.811673: step 85410, loss = 0.69 (1823.2 examples/sec; 0.070 sec/batch)
2017-05-02 16:46:09.608143: step 85420, loss = 0.86 (1607.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:46:10.418722: step 85430, loss = 0.66 (1579.1 examples/sec; 0.081 sec/batch)
2017-05-02 16:46:11.219963: step 85440, loss = 0.68 (1597.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:46:12.012821: step 85450, loss = 0.77 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:46:12.807523: step 85460, loss = 0.69 (1610.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:46:13.603892: step 85470, loss = 0.57 (1607.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:46:14.394524: step 85480, loss = 0.71 (1618.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:46:15.192577: step 85490, loss = 0.66 (1603.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:46:16.078985: step 85500, loss = 0.75 (1444.0 examples/sec; 0.089 sec/batch)
2017-05-02 16:46:16.780067: step 85510, loss = 0.64 (1825.7 examples/sec; 0.070 sec/batch)
2017-05-02 16:46:17.571313: step 85520, loss = 0.78 (1617.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:46:18.360818: step 85530, loss = 0.65 (1621.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:46:19.149163: step 85540, loss = 0.86 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:46:19.933544: step 85550, loss = 0.71 (1631.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:46:20.728916: step 85560, loss = 0.67 (1609.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:46:21.523407: step 85570, loss = 0.64 (1611.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:46:22.318328: step 85580, loss = 0.72 (1610.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:46:23.112358: step 85590, loss = 0.70 (1612.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:46:23.998003: step 85600, loss = 0.73 (1445.3 examples/sec; 0.089 sec/batch)
2017-05-02 16:46:24.690078: step 85610, loss = 0.59 (1849.5 examples/sec; 0.069 sec/batch)
2017-05-02 16:46:25.491676: step 85620, loss = 0.68 (1596.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:46:26.288364: step 85630, loss = 0.59 (1606.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:46:27.090192: step 85640, loss = 0.87 (1596.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:46:27.875400: step 85650, loss = 0.74 (1630.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:46:28.671800: step 85660, loss = 0.65 (1607.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:46:29.472707: step 85670, loss = 0.60 (1598.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:46:30.260233: step 85680, loss = 0.73 (1625.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:46:31.052210: step 85690, loss = 0.70 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:46:31.940727: step 85700, loss = 0.62 (1440.6 examples/sec; 0.089 sec/batch)
2017-05-02 16:46:32.631228: step 85710, loss = 0.74 (1853.7 examples/sec; 0.069 sec/batch)
2017-05-02 16:46:33.426572: step 85720, loss = 0.75 (1609.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:46:34.218655: step 85730, loss = 0.68 (1616.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:46:35.016293: step 85740, loss = 0.70 (1604.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:46:35.799739: step 85750, loss = 0.67 (1633.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:46:36.597202: step 85760, loss = 0.70 (1605.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:46:37.399858: step 85770, loss = 0.64 (1594.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:46:38.189686: step 85780, loss = 0.79 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:46:38.974457: step 85790, loss = 0.65 (1631.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:46:39.879370: step 85800, loss = 0.51 (1414.5 examples/sec; 0.090 sec/batch)
2017-05-02 16:46:40.556194: step 85810, loss = 0.63 (1891.2 examples/sec; 0.068 sec/batch)
2017-05-02 16:46:41.354865: step 85820, loss = 0.74 (1602.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:46:42.140914: step 85830, loss = 0.79 (1628.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:46:42.928248: step 85840, loss = 0.71 (1625.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:46:43.710629: step 85850, loss = 0.65 (1636.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:46:44.511952: step 85860, loss = 0.65 (1597.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:46:45.301389: step 85870, loss = 0.83 (1621.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:46:46.095119: step 85880, loss = 0.66 (1612.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:46:46.886076: step 85890, loss = 0.65 (1618.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:46:47.789745: step 85900, loss = 0.73 (1416.4 examples/sec; 0.090 sec/batch)
2017-05-02 16:46:48.468684: step 85910, loss = 0.82 (1885.3 examples/sec; 0.068 sec/batch)
2017-05-02 16:46:49.260657: step 85920, loss = 0.61 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:46:50.050282: step 85930, loss = 0.79 (1621.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:46:50.842969: step 85940, loss = 0.68 (1614.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:46:51.628613: step 85950, loss = 0.73 (1629.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:46:52.422971: step 85960, loss = 0.72 (1611.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:46:53.225414: step 85970, loss = 0.64 (1595.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:46:54.017257: step 85980, loss = 0.80 (1616.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:46:54.811199: step 85990, loss = 0.73 (1612.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:46:55.722401: step 86000, loss = 0.67 (1404.8 examples/sec; 0.091 sec/batch)
2017-05-02 16:46:56.394055: step 86010, loss = 0.54 (1905.7 examples/sec; 0.067 sec/batch)
2017-05-02 16:46:57.198206: step 86020, loss = 0.79 (1591.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:46:57.988223: step 86030, loss = 0.76 (1620.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:46:58.779559: step 86040, loss = 0.70 (1617.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:46:59.564348: step 86050, loss = 0.58 (1631.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:47:00.353668: step 86060, loss = 0.81 (1621.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:01.146764: step 86070, loss = 0.70 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:01.935888: step 86080, loss = 0.65 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:02.727532: step 86090, loss = 0.68 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:03.604728: step 86100, loss = 0.74 (1459.2 examples/sec; 0.088 sec/batch)
2017-05-02 16:47:04.300448: step 86110, loss = 0.64 (1839.8 examples/sec; 0.070 sec/batch)
2017-05-02 16:47:05.091050: step 86120, loss = 0.60 (1619.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:05.883913: step 86130, loss = 0.65 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:06.684508: step 86140, loss = 0.75 (1598.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:47:07.472737: step 86150, loss = 0.66 (1623.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:08.261062: step 86160, loss = 0.67 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:09.048841: step 86170, loss = 0.73 (1624.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:09.834568: step 86180, loss = 0.75 (1629.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:10.628787: step 86190, loss = 0.68 (1611.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:11.523865: step 86200, loss = 0.77 (1430.0 examples/sec; 0.090 sec/batch)
2017-05-02 16:47:12.207196: step 86210, loss = 0.68 (1873.2 examples/sec; 0.068 sec/batch)
2017-05-02 16:47:13.001434: step 86220, loss = 0.84 (1611.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:13.796831: step 86230, loss = 0.81 (1609.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:47:14.587434: step 86240, loss = 0.61 (1619.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:15.378479: step 86250, loss = 0.80 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:16.160660: step 86260, loss = 0.59 (1636.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:47:16.957020: step 86270, loss = 0.78 (1607.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:47:17.743481: step 86280, loss = 0.74 (1627.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:18.533584: step 86290, loss = 0.70 (1620.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:19.432756: step 86300, loss = 0.76 (1423.5 examples/sec; 0.090 sec/batch)
2017-05-02 16:47:20.099078: step 86310, loss = 0.63 (1921.0 examples/sec; 0.067 sec/batch)
2017-05-02 16:47:20.886122: step 86320, loss = 0.76 (1626.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:21.677492: step 86330, loss = 0.61 (1617.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:22.462750: step 86340, loss = 0.67 (1630.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:23.255766: step 86350, loss = 0.67 (1614.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:24.032817: step 86360, loss = 0.68 (1647.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:47:24.825629: step 86370, loss = 0.76 (1614.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:25.617305: step 86380, loss = 0.69 (1616.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:26.412459: step 86390, loss = 0.63 (1609.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:47:27.315737: step 86400, loss = 0.65 (1417.1 examples/sec; 0.090 sec/batch)
2017-05-02 16:47:28.000746: step 86410, loss = 0.66 (1868.6 examples/sec; 0.069 sec/batch)
2017-05-02 16:47:28.793065: step 86420, loss = 0.67 (1615.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:29.582020: step 86430, loss = 0.77 (1622.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:30.374978: step 86440, loss = 0.70 (1614.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:31.166610: step 86450, loss = 0.73 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:31.948845: step 86460, loss = 0.71 (1636.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:47:32.733100: step 86470, loss = 0.76 (1632.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:47:33.534511: step 86480, loss = 0.69 (1597.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:47:34.327633: step 86490, loss = 0.70 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:35.211167: step 86500, loss = 0.68 (1448.7 examples/sec; 0.088 sec/batch)
2017-05-02 16:47:35.901098: step 86510, loss = 0.70 (1855.3 examples/sec; 0.069 sec/batch)
2017-05-02 16:47:36.698669: step 86520, loss = 0.69 (1604.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:47:37.491509: step 86530, loss = 0.93 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:38.284177: step 86540, loss = 0.68 (1614.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:39.083579: step 86550, loss = 0.68 (1601.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:47:39.867078: step 86560, loss = 0.72 (1633.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:47:40.667215: step 86570, loss = 0.62 (1599.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:47:41.458647: step 86580, loss = 0.64 (1617.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:42.254661: step 86590, loss = 0.68 (1608.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:47:43.146706: step 86600, loss = 0.69 (1434.9 examples/sec; 0.089 sec/batch)
2017-05-02 16:47:43.833674: step 86610, loss = 0.75 (1863.3 examples/sec; 0.069 sec/batch)
2017-05-02 16:47:44.631461: step 86620, loss = 0.82 (1604.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:47:45.428158: step 86630, loss = 0.61 (1606.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:47:46.224535: step 86640, loss = 0.61 (1607.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:47:47.021374: step 86650, loss = 0.74 (1606.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:47:47.806587: step 86660, loss = 0.72 (1630.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:48.603132: step 86670, loss = 0.68 (1606.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:47:49.400739: step 86680, loss = 0.79 (1604.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:47:50.193782: step 86690, loss = 0.65 (1614.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:51.089567: step 86700, loss = 0.69 (1428.9 examples/sec; 0.090 sec/batch)
2017-05-02 16:47:51.782781: step 86710, loss = 0.74 (1846.5 examples/sec; 0.069 sec/batch)
2017-05-02 16:47:52.572053: step 86720, loss = 0.57 (1621.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:53.363222: step 86730, loss = 0.62 (1617.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:54.158259: step 86740, loss = 0.59 (1610.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:47:54.944444: step 86750, loss = 0.69 (1628.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:55.719700: step 86760, loss = 0.68 (1651.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:47:56.512613: step 86770, loss = 0.79 (1614.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:57.312525: step 86780, loss = 0.78 (1600.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:47:58.104632: step 86790, loss = 0.65 (1615.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:47:58.989579: step 86800, loss = 0.81 (1446.4 examples/sec; 0.088 sec/batch)
2017-05-02 16:47:59.684357: step 86810, loss = 0.78 (1842.3 examples/sec; 0.069 sec/batch)
2017-05-02 16:48:00.464743: step 86820, loss = 0.78 (1640.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:48:01.255972: step 86830, loss = 0.75 (1617.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:02.046906: step 86840, loss = 0.67 (1618.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:02.837977: step 86850, loss = 0.72 (1618.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:03.623604: step 86860, loss = 0.66 (1629.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:04.417855: step 86870, loss = 0.76 (1611.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:05.218648: step 86880, loss = 0.65 (1598.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:48:06.015245: step 86890, loss = 0.66 (1606.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:48:06.899408: step 86900, loss = 0.77 (1447.7 examples/sec; 0.088 sec/batch)
2017-05-02 16:48:07.591102: step 86910, loss = 0.75 (1850.5 examples/sec; 0.069 sec/batch)
2017-05-02 16:48:08.383367: step 86920, loss = 0.93 (1615.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:09.170978: step 86930, loss = 0.78 (1625.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:09.966164: step 86940, loss = 0.71 (1609.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:48:10.758648: step 86950, loss = 0.77 (1615.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:11.542418: step 86960, loss = 0.62 (1633.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:48:12.327218: step 86970, loss = 0.75 (1631.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:48:13.119463: step 86980, loss = 0.71 (1615.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:13.915938: step 86990, loss = 0.74 (1607.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:48:14.829443: step 87000, loss = 0.67 (1401.2 examples/sec; 0.091 sec/batch)
2017-05-02 16:48:15.495214: step 87010, loss = 0.69 (1922.6 examples/sec; 0.067 sec/batch)
2017-05-02 16:48:16.277607: step 87020, loss = 0.65 (1636.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:48:17.072457: step 87030, loss = 0.69 (1610.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:17.871185: step 87040, loss = 0.63 (1602.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:48:18.667547: step 87050, loss = 0.72 (1607.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:48:19.454053: step 87060, loss = 0.73 (1627.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:20.251374: step 87070, loss = 0.74 (1605.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:48:21.044965: step 87080, loss = 0.70 (1612.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:21.835980: step 87090, loss = 0.74 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:22.736245: step 87100, loss = 0.85 (1421.8 examples/sec; 0.090 sec/batch)
2017-05-02 16:48:23.416116: step 87110, loss = 0.76 (1882.7 examples/sec; 0.068 sec/batch)
2017-05-02 16:48:24.197100: step 87120, loss = 0.69 (1639.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:48:24.987703: step 87130, loss = 0.83 (1619.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:25.786971: step 87140, loss = 0.75 (1601.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:48:26.575993: step 87150, loss = 0.70 (1622.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:27.364057: step 87160, loss = 0.69 (1624.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:28.153648: step 87170, loss = 0.63 (1621.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:28.947895: step 87180, loss = 0.71 (1611.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:29.746325: step 87190, loss = 0.88 (1603.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:48:30.630864: step 87200, loss = 0.75 (1447.1 examples/sec; 0.088 sec/batch)
2017-05-02 16:48:31.329592: step 87210, loss = 0.73 (1831.9 examples/sec; 0.070 sec/batch)
2017-05-02 16:48:32.111433: step 87220, loss = 0.75 (1637.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:48:32.903531: step 87230, loss = 0.63 (1616.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:33.694076: step 87240, loss = 0.75 (1619.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:34.492555: step 87250, loss = 0.75 (1603.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:48:35.278892: step 87260, loss = 0.65 (1627.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:36.061839: step 87270, loss = 0.64 (1634.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:48:36.858342: step 87280, loss = 0.65 (1607.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:48:37.655726: step 87290, loss = 0.66 (1605.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:48:38.547476: step 87300, loss = 0.69 (1435.4 examples/sec; 0.089 sec/batch)
2017-05-02 16:48:39.248429: step 87310, loss = 0.75 (1826.1 examples/sec; 0.070 sec/batch)
2017-05-02 16:48:40.030003: step 87320, loss = 0.74 (1637.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:48:40.823839: step 87330, loss = 0.69 (1612.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:41.623794: step 87340, loss = 0.71 (1600.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:48:42.414711: step 87350, loss = 0.76 (1618.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:43.205264: step 87360, loss = 0.67 (1619.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:43.979681: step 87370, loss = 0.58 (1652.8 examples/sec; 0.077 sec/batch)
2017-05-02 16:48:44.774362: step 87380, loss = 0.64 (1610.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:45.566980: step 87390, loss = 0.68 (1614.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:46.454345: step 87400, loss = 0.78 (1442.5 examples/sec; 0.089 sec/batch)
2017-05-02 16:48:47.151480: step 87410, loss = 0.70 (1836.1 examples/sec; 0.070 sec/batch)
2017-05-02 16:48:47.932391: step 87420, loss = 0.65 (1639.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:48:48.721154: step 87430, loss = 0.65 (1622.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:49.522598: step 87440, loss = 0.76 (1597.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:48:50.311746: step 87450, loss = 0.78 (1622.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:51.102017: step 87460, loss = 0.83 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:51.886650: step 87470, loss = 0.65 (1631.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:48:52.667059: step 87480, loss = 0.89 (1640.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:48:53.461082: step 87490, loss = 0.61 (1612.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:54.344672: step 87500, loss = 0.80 (1448.6 examples/sec; 0.088 sec/batch)
2017-05-02 16:48:55.044961: step 87510, loss = 0.59 (1827.8 examples/sec; 0.070 sec/batch)
2017-05-02 16:48:55.833256: step 87520, loss = 0.68 (1623.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:56.623486: step 87530, loss = 0.84 (1619.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:57.410838: step 87540, loss = 0.73 (1625.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:58.201685: step 87550, loss = 0.94 (1618.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:48:58.986619: step 87560, loss = 0.62 (1630.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:48:59.770592: step 87570, loss = 0.75 (1632.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:49:00.562268: step 87580, loss = 0.79 (1616.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:01.350301: step 87590, loss = 0.73 (1624.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:02.232070: step 87600, loss = 0.69 (1451.6 examples/sec; 0.088 sec/batch)
2017-05-02 16:49:02.940830: step 87610, loss = 0.55 (1806.0 examples/sec; 0.071 sec/batch)
2017-05-02 16:49:03.725443: step 87620, loss = 0.64 (1631.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:49:04.515910: step 87630, loss = 0.64 (1619.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:05.307838: step 87640, loss = 0.69 (1616.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:06.095041: step 87650, loss = 0.72 (1626.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:06.889714: step 87660, loss = 0.76 (1610.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:07.667689: step 87670, loss = 0.76 (1645.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:49:08.458836: step 87680, loss = 0.69 (1617.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:09.243643: step 87690, loss = 0.69 (1631.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:49:10.138686: step 87700, loss = 0.65 (1430.1 examples/sec; 0.090 sec/batch)
2017-05-02 16:49:10.834787: step 87710, loss = 0.76 (1838.8 examples/sec; 0.070 sec/batch)
2017-05-02 16:49:11.612903: step 87720, loss = 0.70 (1645.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:49:12.407381: step 87730, loss = 0.67 (1611.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:13.197675: step 87740, loss = 0.60 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:13.982149: step 87750, loss = 0.68 (1631.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:49:14.767238: step 87760, loss = 0.76 (1630.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:15.546723: step 87770, loss = 0.71 (1642.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:49:16.332344: step 87780, loss = 0.67 (1629.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:17.128229: step 87790, loss = 0.67 (1608.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:49:18.015727: step 87800, loss = 0.78 (1442.3 examples/sec; 0.089 sec/batch)
2017-05-02 16:49:18.711390: step 87810, loss = 0.78 (1840.0 examples/sec; 0.070 sec/batch)
2017-05-02 16:49:19.497239: step 87820, loss = 0.70 (1628.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:20.287222: step 87830, loss = 0.66 (1620.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:21.080895: step 87840, loss = 0.85 (1612.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:21.871205: step 87850, loss = 0.70 (1619.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:22.662646: step 87860, loss = 0.75 (1617.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:23.446826: step 87870, loss = 0.80 (1632.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:49:24.229936: step 87880, loss = 0.73 (1634.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:49:25.028602: step 87890, loss = 0.66 (1602.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:49:25.917417: step 87900, loss = 0.76 (1440.1 examples/sec; 0.089 sec/batch)
2017-05-02 16:49:26.606620: step 87910, loss = 0.65 (1857.2 examples/sec; 0.069 sec/batch)
2017-05-02 16:49:27.393945: step 87920, loss = 0.83 (1625.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:28.185029: step 87930, loss = 0.70 (1618.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:28.977051: step 87940, loss = 0.66 (1616.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:29.766451: step 87950, loss = 0.73 (1621.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:30.561278: step 87960, loss = 0.67 (1610.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:31.352186: step 87970, loss = 0.70 (1618.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:32.132253: step 87980, loss = 0.78 (1640.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:49:32.932381: step 87990, loss = 0.55 (1599.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:49:33.825775: step 88000, loss = 0.68 (1432.7 examples/sec; 0.089 sec/batch)
2017-05-02 16:49:34.520448: step 88010, loss = 0.90 (1842.6 examples/sec; 0.069 sec/batch)
2017-05-02 16:49:35.312873: step 88020, loss = 0.70 (1615.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:36.092760: step 88030, loss = 0.84 (1641.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:49:36.879096: step 88040, loss = 0.66 (1627.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:37.675345: step 88050, loss = 0.72 (1607.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:49:38.471180: step 88060, loss = 0.74 (1608.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:49:39.261841: step 88070, loss = 0.65 (1618.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:40.040470: step 88080, loss = 0.71 (1643.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:49:40.835863: step 88090, loss = 0.75 (1609.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:49:41.724902: step 88100, loss = 0.64 (1439.8 examples/sec; 0.089 sec/batch)
2017-05-02 16:49:42.420311: step 88110, loss = 0.65 (1840.6 examples/sec; 0.070 sec/batch)
2017-05-02 16:49:43.209036: step 88120, loss = 0.70 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:43.988445: step 88130, loss = 0.77 (1642.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:49:44.794754: step 88140, loss = 0.68 (1587.5 examples/sec; 0.081 sec/batch)
2017-05-02 16:49:45.589037: step 88150, loss = 0.73 (1611.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:46.382442: step 88160, loss = 0.68 (1613.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:47.184432: step 88170, loss = 0.81 (1596.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:49:47.969596: step 88180, loss = 0.63 (1630.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:48.762124: step 88190, loss = 0.69 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:49.653748: step 88200, loss = 0.61 (1435.6 examples/sec; 0.089 sec/batch)
2017-05-02 16:49:50.341595: step 88210, loss = 0.82 (1860.9 examples/sec; 0.069 sec/batch)
2017-05-02 16:49:51.137537: step 88220, loss = 0.73 (1608.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:49:51.915837: step 88230, loss = 0.63 (1644.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:49:52.706737: step 88240, loss = 0.70 (1618.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:53.496121: step 88250, loss = 0.76 (1621.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:54.297162: step 88260, loss = 0.72 (1597.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:49:55.081355: step 88270, loss = 0.72 (1632.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:49:55.859520: step 88280, loss = 0.60 (1644.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:49:56.653539: step 88290, loss = 0.67 (1612.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:57.544180: step 88300, loss = 0.84 (1437.2 examples/sec; 0.089 sec/batch)
2017-05-02 16:49:58.237720: step 88310, loss = 0.63 (1845.6 examples/sec; 0.069 sec/batch)
2017-05-02 16:49:59.028239: step 88320, loss = 0.52 (1619.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:49:59.809178: step 88330, loss = 0.80 (1639.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:50:00.602373: step 88340, loss = 0.78 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:01.400402: step 88350, loss = 0.83 (1603.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:50:02.205562: step 88360, loss = 0.67 (1589.7 examples/sec; 0.081 sec/batch)
2017-05-02 16:50:02.992015: step 88370, loss = 0.77 (1627.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:03.771012: step 88380, loss = 0.71 (1643.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:50:04.561675: step 88390, loss = 0.74 (1618.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:05.457826: step 88400, loss = 0.74 (1428.3 examples/sec; 0.090 sec/batch)
2017-05-02 16:50:06.155170: step 88410, loss = 0.78 (1835.5 examples/sec; 0.070 sec/batch)
2017-05-02 16:50:06.948939: step 88420, loss = 0.69 (1612.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:07.729823: step 88430, loss = 0.51 (1639.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:50:08.521531: step 88440, loss = 0.75 (1616.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:09.311952: step 88450, loss = 0.79 (1619.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:10.107482: step 88460, loss = 0.77 (1609.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:50:10.897774: step 88470, loss = 0.70 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:11.680346: step 88480, loss = 0.70 (1635.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:50:12.472035: step 88490, loss = 0.56 (1616.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:13.366333: step 88500, loss = 0.73 (1431.3 examples/sec; 0.089 sec/batch)
2017-05-02 16:50:14.065202: step 88510, loss = 0.79 (1831.5 examples/sec; 0.070 sec/batch)
2017-05-02 16:50:14.853487: step 88520, loss = 0.66 (1623.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:15.636056: step 88530, loss = 0.60 (1635.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:50:16.427029: step 88540, loss = 0.71 (1618.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:17.222262: step 88550, loss = 0.71 (1609.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:50:18.012567: step 88560, loss = 0.69 (1619.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:18.807968: step 88570, loss = 0.57 (1609.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:50:19.594415: step 88580, loss = 0.60 (1627.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:20.386218: step 88590, loss = 0.84 (1616.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:21.272674: step 88600, loss = 0.64 (1444.0 examples/sec; 0.089 sec/batch)
2017-05-02 16:50:21.975942: step 88610, loss = 0.68 (1820.1 examples/sec; 0.070 sec/batch)
2017-05-02 16:50:22.773243: step 88620, loss = 0.69 (1605.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:50:23.553444: step 88630, loss = 0.71 (1640.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:50:24.346818: step 88640, loss = 0.71 (1613.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:25.134053: step 88650, loss = 0.73 (1625.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:25.925206: step 88660, loss = 0.70 (1617.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:26.721916: step 88670, loss = 0.89 (1606.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:50:27.500484: step 88680, loss = 0.83 (1644.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:50:28.286783: step 88690, loss = 0.70 (1627.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:29.177350: step 88700, loss = 0.69 (1437.3 examples/sec; 0.089 sec/batch)
2017-05-02 16:50:29.878481: step 88710, loss = 0.59 (1825.6 examples/sec; 0.070 sec/batch)
2017-05-02 16:50:30.674403: step 88720, loss = 0.65 (1608.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:50:31.462515: step 88730, loss = 0.80 (1624.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:32.249681: step 88740, loss = 0.66 (1626.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:33.054185: step 88750, loss = 0.61 (1591.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:50:33.854043: step 88760, loss = 0.75 (1600.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:50:34.644365: step 88770, loss = 0.66 (1619.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:35.432804: step 88780, loss = 0.63 (1623.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:36.222817: step 88790, loss = 0.79 (1620.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:37.113812: step 88800, loss = 0.76 (1436.6 examples/sec; 0.089 sec/batch)
2017-05-02 16:50:37.808543: step 88810, loss = 0.81 (1842.4 examples/sec; 0.069 sec/batch)
2017-05-02 16:50:38.607292: step 88820, loss = 0.65 (1602.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:50:39.388334: step 88830, loss = 0.82 (1638.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:50:40.176133: step 88840, loss = 0.64 (1624.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:40.965407: step 88850, loss = 0.79 (1621.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:41.757173: step 88860, loss = 0.63 (1616.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:42.548365: step 88870, loss = 0.83 (1617.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:43.338791: step 88880, loss = 0.53 (1619.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:44.125032: step 88890, loss = 0.71 (1628.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:45.009592: step 88900, loss = 0.78 (1447.1 examples/sec; 0.088 sec/batch)
2017-05-02 16:50:45.709703: step 88910, loss = 0.65 (1828.3 examples/sec; 0.070 sec/batch)
2017-05-02 16:50:46.502551: step 88920, loss = 0.60 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:47.291424: step 88930, loss = 0.73 (1622.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:48.071176: step 88940, loss = 0.65 (1641.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:50:48.860840: step 88950, loss = 0.65 (1620.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:49.647510: step 88960, loss = 0.66 (1627.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:50.434986: step 88970, loss = 0.61 (1625.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:51.233590: step 88980, loss = 0.63 (1602.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:50:52.013388: step 88990, loss = 0.73 (1641.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:50:52.903680: step 89000, loss = 0.69 (1437.7 examples/sec; 0.089 sec/batch)
2017-05-02 16:50:53.601395: step 89010, loss = 0.60 (1834.5 examples/sec; 0.070 sec/batch)
2017-05-02 16:50:54.392922: step 89020, loss = 0.68 (1617.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:55.188493: step 89030, loss = 0.66 (1608.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:50:55.966586: step 89040, loss = 0.82 (1645.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:50:56.759939: step 89050, loss = 0.75 (1613.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:57.565171: step 89060, loss = 0.77 (1589.6 examples/sec; 0.081 sec/batch)
2017-05-02 16:50:58.353822: step 89070, loss = 0.73 (1623.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:59.147047: step 89080, loss = 0.75 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:50:59.923650: step 89090, loss = 0.69 (1648.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:51:00.815270: step 89100, loss = 0.81 (1435.6 examples/sec; 0.089 sec/batch)
2017-05-02 16:51:01.507552: step 89110, loss = 0.72 (1849.0 examples/sec; 0.069 sec/batch)
2017-05-02 16:51:02.303561: step 89120, loss = 0.71 (1608.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:51:03.092668: step 89130, loss = 0.71 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:03.874595: step 89140, loss = 0.76 (1637.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:51:04.676336: step 89150, loss = 0.68 (1596.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:51:05.462055: step 89160, loss = 0.76 (1629.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:06.247783: step 89170, loss = 0.61 (1629.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:07.039253: step 89180, loss = 0.69 (1617.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:07.828232: step 89190, loss = 0.92 (1622.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:08.716291: step 89200, loss = 0.86 (1441.3 examples/sec; 0.089 sec/batch)
2017-05-02 16:51:09.419127: step 89210, loss = 0.92 (1821.2 examples/sec; 0.070 sec/batch)
2017-05-02 16:51:10.213428: step 89220, loss = 0.67 (1611.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:11.001742: step 89230, loss = 0.61 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:11.785035: step 89240, loss = 0.71 (1634.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:51:12.576111: step 89250, loss = 0.74 (1618.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:13.367019: step 89260, loss = 0.88 (1618.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:14.164246: step 89270, loss = 0.87 (1605.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:51:14.951254: step 89280, loss = 0.50 (1626.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:15.732546: step 89290, loss = 0.66 (1638.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:51:16.612591: step 89300, loss = 0.78 (1454.5 examples/sec; 0.088 sec/batch)
2017-05-02 16:51:17.312123: step 89310, loss = 0.73 (1829.8 examples/sec; 0.070 sec/batch)
2017-05-02 16:51:18.102278: step 89320, loss = 0.73 (1619.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:18.890673: step 89330, loss = 0.63 (1623.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:19.669717: step 89340, loss = 0.71 (1643.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:51:20.458290: step 89350, loss = 0.71 (1623.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:21.254392: step 89360, loss = 0.70 (1607.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:51:22.039376: step 89370, loss = 0.71 (1630.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:51:22.836229: step 89380, loss = 0.63 (1606.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:51:23.612584: step 89390, loss = 0.72 (1648.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:51:24.498996: step 89400, loss = 0.72 (1444.0 examples/sec; 0.089 sec/batch)
2017-05-02 16:51:25.200827: step 89410, loss = 0.72 (1823.8 examples/sec; 0.070 sec/batch)
2017-05-02 16:51:25.996122: step 89420, loss = 0.65 (1609.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:51:26.787856: step 89430, loss = 0.90 (1616.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:27.575575: step 89440, loss = 0.77 (1624.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:28.361153: step 89450, loss = 0.85 (1629.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:29.153923: step 89460, loss = 0.64 (1614.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:29.961852: step 89470, loss = 0.82 (1584.3 examples/sec; 0.081 sec/batch)
2017-05-02 16:51:30.755341: step 89480, loss = 0.70 (1613.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:31.537381: step 89490, loss = 0.66 (1636.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:51:32.426563: step 89500, loss = 0.66 (1439.5 examples/sec; 0.089 sec/batch)
2017-05-02 16:51:33.120062: step 89510, loss = 0.75 (1845.7 examples/sec; 0.069 sec/batch)
2017-05-02 16:51:33.908404: step 89520, loss = 0.78 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:34.704077: step 89530, loss = 0.65 (1608.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:51:35.495469: step 89540, loss = 0.75 (1617.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:36.283808: step 89550, loss = 0.59 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:37.073085: step 89560, loss = 0.90 (1621.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:37.861980: step 89570, loss = 0.63 (1622.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:38.660354: step 89580, loss = 0.79 (1603.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:51:39.448188: step 89590, loss = 0.59 (1624.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:40.340870: step 89600, loss = 0.73 (1433.9 examples/sec; 0.089 sec/batch)
2017-05-02 16:51:41.040622: step 89610, loss = 0.78 (1829.2 examples/sec; 0.070 sec/batch)
2017-05-02 16:51:41.836988: step 89620, loss = 0.62 (1607.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:51:42.627250: step 89630, loss = 0.62 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:43.415520: step 89640, loss = 0.68 (1623.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:44.203004: step 89650, loss = 0.63 (1625.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:44.992192: step 89660, loss = 0.67 (1621.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:45.784188: step 89670, loss = 0.61 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:46.581395: step 89680, loss = 0.65 (1605.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:51:47.363966: step 89690, loss = 0.76 (1635.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:51:48.241477: step 89700, loss = 0.75 (1458.7 examples/sec; 0.088 sec/batch)
2017-05-02 16:51:48.937507: step 89710, loss = 0.80 (1839.0 examples/sec; 0.070 sec/batch)
2017-05-02 16:51:49.726647: step 89720, loss = 0.68 (1622.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:50.518181: step 89730, loss = 0.71 (1617.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:51.305310: step 89740, loss = 0.59 (1626.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:52.090952: step 89750, loss = 0.69 (1629.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:52.892838: step 89760, loss = 0.79 (1596.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:51:53.677661: step 89770, loss = 0.67 (1631.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:51:54.473376: step 89780, loss = 0.65 (1608.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:51:55.263701: step 89790, loss = 0.71 (1619.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:56.145782: step 89800, loss = 0.75 (1451.1 examples/sec; 0.088 sec/batch)
2017-05-02 16:51:56.848521: step 89810, loss = 0.64 (1821.4 examples/sec; 0.070 sec/batch)
2017-05-02 16:51:57.640262: step 89820, loss = 0.59 (1616.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:51:58.436544: step 89830, loss = 0.68 (1607.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:51:59.226258: step 89840, loss = 0.85 (1620.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:00.001020: step 89850, loss = 0.67 (1652.1 examples/sec; 0.077 sec/batch)
2017-05-02 16:52:00.794057: step 89860, loss = 0.74 (1614.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:01.580815: step 89870, loss = 0.78 (1626.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:02.370649: step 89880, loss = 0.72 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:03.170469: step 89890, loss = 0.72 (1600.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:52:04.049361: step 89900, loss = 0.59 (1456.4 examples/sec; 0.088 sec/batch)
2017-05-02 16:52:04.747431: step 89910, loss = 0.83 (1833.6 examples/sec; 0.070 sec/batch)
2017-05-02 16:52:05.532985: step 89920, loss = 0.74 (1629.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:06.329082: step 89930, loss = 0.68 (1607.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:52:07.119265: step 89940, loss = 0.71 (1619.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:07.901489: step 89950, loss = 0.63 (1636.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:52:08.688057: step 89960, loss = 0.71 (1627.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:09.479901: step 89970, loss = 0.72 (1616.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:10.268526: step 89980, loss = 0.69 (1623.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:11.062051: step 89990, loss = 0.63 (1613.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:11.940705: step 90000, loss = 0.79 (1456.8 examples/sec; 0.088 sec/batch)
2017-05-02 16:52:12.639903: step 90010, loss = 0.66 (1830.7 examples/sec; 0.070 sec/batch)
2017-05-02 16:52:13.442158: step 90020, loss = 0.74 (1595.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:52:14.224193: step 90030, loss = 0.58 (1636.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:52:15.018300: step 90040, loss = 0.61 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:15.794832: step 90050, loss = 0.70 (1648.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:52:16.585287: step 90060, loss = 0.81 (1619.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:17.377963: step 90070, loss = 0.62 (1614.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:18.173409: step 90080, loss = 0.75 (1609.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:52:18.961582: step 90090, loss = 0.69 (1624.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:19.838628: step 90100, loss = 0.69 (1459.4 examples/sec; 0.088 sec/batch)
2017-05-02 16:52:20.534501: step 90110, loss = 0.72 (1839.4 examples/sec; 0.070 sec/batch)
2017-05-02 16:52:21.338202: step 90120, loss = 0.54 (1592.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:52:22.126142: step 90130, loss = 0.72 (1624.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:22.918405: step 90140, loss = 0.79 (1615.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:23.703953: step 90150, loss = 0.72 (1629.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:24.494525: step 90160, loss = 0.61 (1619.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:25.277774: step 90170, loss = 0.68 (1634.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:52:26.073235: step 90180, loss = 0.68 (1609.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:52:26.864246: step 90190, loss = 0.74 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:27.749874: step 90200, loss = 0.58 (1445.3 examples/sec; 0.089 sec/batch)
2017-05-02 16:52:28.443195: step 90210, loss = 0.71 (1846.2 examples/sec; 0.069 sec/batch)
2017-05-02 16:52:29.241097: step 90220, loss = 0.54 (1604.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:52:30.035096: step 90230, loss = 0.86 (1612.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:30.835689: step 90240, loss = 0.69 (1598.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:52:31.618549: step 90250, loss = 0.62 (1635.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:52:32.406973: step 90260, loss = 0.63 (1623.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:33.192862: step 90270, loss = 0.88 (1628.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:33.988163: step 90280, loss = 0.70 (1609.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:52:34.779182: step 90290, loss = 0.73 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:35.676711: step 90300, loss = 0.75 (1426.1 examples/sec; 0.090 sec/batch)
2017-05-02 16:52:36.362855: step 90310, loss = 0.70 (1865.5 examples/sec; 0.069 sec/batch)
2017-05-02 16:52:37.157470: step 90320, loss = 0.64 (1610.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:37.946586: step 90330, loss = 0.80 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:38.738358: step 90340, loss = 0.83 (1616.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:39.523029: step 90350, loss = 0.61 (1631.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:52:40.313108: step 90360, loss = 0.78 (1620.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:41.108040: step 90370, loss = 0.62 (1610.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:41.895860: step 90380, loss = 0.62 (1624.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:42.690559: step 90390, loss = 0.74 (1610.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:43.577221: step 90400, loss = 0.67 (1443.6 examples/sec; 0.089 sec/batch)
2017-05-02 16:52:44.268147: step 90410, loss = 0.75 (1852.6 examples/sec; 0.069 sec/batch)
2017-05-02 16:52:45.053554: step 90420, loss = 0.68 (1629.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:45.843777: step 90430, loss = 0.68 (1619.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:46.636781: step 90440, loss = 0.80 (1614.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:47.417513: step 90450, loss = 0.60 (1639.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:52:48.210644: step 90460, loss = 0.79 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:48.994010: step 90470, loss = 0.86 (1634.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:52:49.786835: step 90480, loss = 0.65 (1614.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:50.582192: step 90490, loss = 0.73 (1609.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:52:51.473092: step 90500, loss = 0.78 (1436.7 examples/sec; 0.089 sec/batch)
2017-05-02 16:52:52.151801: step 90510, loss = 0.89 (1885.9 examples/sec; 0.068 sec/batch)
2017-05-02 16:52:52.944243: step 90520, loss = 0.72 (1615.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:53.733242: step 90530, loss = 0.69 (1622.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:54.526671: step 90540, loss = 0.57 (1613.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:55.316225: step 90550, loss = 0.61 (1621.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:56.095424: step 90560, loss = 0.73 (1642.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:52:56.888518: step 90570, loss = 0.63 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:57.684058: step 90580, loss = 0.78 (1609.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:52:58.474682: step 90590, loss = 0.69 (1619.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:52:59.387041: step 90600, loss = 0.73 (1403.0 examples/sec; 0.091 sec/batch)
2017-05-02 16:53:00.059035: step 90610, loss = 0.62 (1904.8 examples/sec; 0.067 sec/batch)
2017-05-02 16:53:00.848449: step 90620, loss = 0.80 (1621.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:01.636092: step 90630, loss = 0.73 (1625.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:02.425806: step 90640, loss = 0.70 (1620.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:03.224431: step 90650, loss = 0.66 (1602.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:53:03.994512: step 90660, loss = 0.83 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-02 16:53:04.787060: step 90670, loss = 0.81 (1615.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:05.577614: step 90680, loss = 0.67 (1619.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:06.368601: step 90690, loss = 0.53 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:07.263675: step 90700, loss = 0.67 (1430.0 examples/sec; 0.090 sec/batch)
2017-05-02 16:53:07.942023: step 90710, loss = 0.84 (1886.9 examples/sec; 0.068 sec/batch)
2017-05-02 16:53:08.734454: step 90720, loss = 0.66 (1615.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:09.521776: step 90730, loss = 0.80 (1625.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:10.320115: step 90740, loss = 0.79 (1603.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:53:11.109264: step 90750, loss = 0.72 (1622.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:11.884319: step 90760, loss = 0.62 (1651.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:53:12.666796: step 90770, loss = 0.68 (1635.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:53:13.463539: step 90780, loss = 0.81 (1606.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:53:14.255315: step 90790, loss = 0.56 (1616.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:15.152996: step 90800, loss = 0.63 (1425.9 examples/sec; 0.090 sec/batch)
2017-05-02 16:53:15.832142: step 90810, loss = 0.64 (1884.7 examples/sec; 0.068 sec/batch)
2017-05-02 16:53:16.615769: step 90820, loss = 0.58 (1633.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:53:17.415522: step 90830, loss = 0.70 (1600.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:53:18.200174: step 90840, loss = 0.61 (1631.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:53:18.990740: step 90850, loss = 0.69 (1619.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:19.770711: step 90860, loss = 0.62 (1641.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:53:20.565600: step 90870, loss = 0.79 (1610.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:21.359404: step 90880, loss = 0.69 (1612.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:22.156826: step 90890, loss = 0.67 (1605.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:53:23.070244: step 90900, loss = 0.74 (1401.3 examples/sec; 0.091 sec/batch)
2017-05-02 16:53:23.741703: step 90910, loss = 0.66 (1906.3 examples/sec; 0.067 sec/batch)
2017-05-02 16:53:24.530280: step 90920, loss = 0.87 (1623.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:25.322537: step 90930, loss = 0.75 (1615.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:26.112210: step 90940, loss = 0.78 (1620.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:26.900745: step 90950, loss = 0.76 (1623.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:27.676116: step 90960, loss = 0.65 (1650.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:53:28.468975: step 90970, loss = 0.74 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:29.260000: step 90980, loss = 0.79 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:30.124353: step 90990, loss = 0.79 (1480.9 examples/sec; 0.086 sec/batch)
2017-05-02 16:53:30.987110: step 91000, loss = 0.66 (1483.6 examples/sec; 0.086 sec/batch)
2017-05-02 16:53:31.664703: step 91010, loss = 0.69 (1889.0 examples/sec; 0.068 sec/batch)
2017-05-02 16:53:32.462641: step 91020, loss = 0.75 (1604.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:53:33.253696: step 91030, loss = 0.76 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:34.047292: step 91040, loss = 0.72 (1612.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:34.834740: step 91050, loss = 0.58 (1625.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:35.615212: step 91060, loss = 0.64 (1640.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:53:36.403451: step 91070, loss = 0.67 (1623.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:37.196397: step 91080, loss = 0.76 (1614.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:37.987153: step 91090, loss = 0.65 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:38.884005: step 91100, loss = 0.72 (1427.2 examples/sec; 0.090 sec/batch)
2017-05-02 16:53:39.569499: step 91110, loss = 0.81 (1867.3 examples/sec; 0.069 sec/batch)
2017-05-02 16:53:40.355312: step 91120, loss = 0.59 (1628.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:41.146804: step 91130, loss = 0.89 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:41.933841: step 91140, loss = 0.55 (1626.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:42.728576: step 91150, loss = 0.77 (1610.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:43.513968: step 91160, loss = 0.84 (1629.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:44.304592: step 91170, loss = 0.71 (1619.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:45.092740: step 91180, loss = 0.73 (1624.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:45.875332: step 91190, loss = 0.69 (1635.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:53:46.767525: step 91200, loss = 0.83 (1434.7 examples/sec; 0.089 sec/batch)
2017-05-02 16:53:47.458390: step 91210, loss = 0.77 (1852.8 examples/sec; 0.069 sec/batch)
2017-05-02 16:53:48.246761: step 91220, loss = 0.83 (1623.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:49.040066: step 91230, loss = 0.65 (1613.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:49.832583: step 91240, loss = 0.57 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:50.623245: step 91250, loss = 0.80 (1618.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:51.410253: step 91260, loss = 0.62 (1626.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:52.193625: step 91270, loss = 0.77 (1634.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:53:52.981978: step 91280, loss = 0.59 (1623.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:53.771705: step 91290, loss = 0.77 (1620.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:54.656929: step 91300, loss = 0.76 (1446.0 examples/sec; 0.089 sec/batch)
2017-05-02 16:53:55.354821: step 91310, loss = 0.91 (1834.1 examples/sec; 0.070 sec/batch)
2017-05-02 16:53:56.144403: step 91320, loss = 0.69 (1621.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:56.934152: step 91330, loss = 0.90 (1620.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:57.728540: step 91340, loss = 0.69 (1611.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:58.519121: step 91350, loss = 0.66 (1619.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:53:59.300166: step 91360, loss = 0.60 (1638.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:54:00.083359: step 91370, loss = 0.76 (1634.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:54:00.875337: step 91380, loss = 0.84 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:01.659305: step 91390, loss = 0.78 (1632.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:54:02.548131: step 91400, loss = 0.63 (1440.1 examples/sec; 0.089 sec/batch)
2017-05-02 16:54:03.249204: step 91410, loss = 0.62 (1825.8 examples/sec; 0.070 sec/batch)
2017-05-02 16:54:04.034152: step 91420, loss = 0.73 (1630.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:54:04.828709: step 91430, loss = 0.65 (1611.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:05.623281: step 91440, loss = 0.71 (1610.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:06.408589: step 91450, loss = 0.73 (1629.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:07.201756: step 91460, loss = 0.67 (1613.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:07.974631: step 91470, loss = 0.82 (1656.1 examples/sec; 0.077 sec/batch)
2017-05-02 16:54:08.761939: step 91480, loss = 0.73 (1625.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:09.553309: step 91490, loss = 0.81 (1617.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:10.437671: step 91500, loss = 0.71 (1447.3 examples/sec; 0.088 sec/batch)
2017-05-02 16:54:11.133107: step 91510, loss = 0.70 (1840.6 examples/sec; 0.070 sec/batch)
2017-05-02 16:54:11.921670: step 91520, loss = 0.67 (1623.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:12.709806: step 91530, loss = 0.72 (1624.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:13.504295: step 91540, loss = 0.74 (1611.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:14.293633: step 91550, loss = 0.71 (1621.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:15.081840: step 91560, loss = 0.76 (1624.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:15.863422: step 91570, loss = 0.64 (1637.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:54:16.654739: step 91580, loss = 0.64 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:17.445115: step 91590, loss = 0.75 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:18.337910: step 91600, loss = 0.64 (1434.0 examples/sec; 0.089 sec/batch)
2017-05-02 16:54:19.029394: step 91610, loss = 0.69 (1850.6 examples/sec; 0.069 sec/batch)
2017-05-02 16:54:19.811073: step 91620, loss = 0.66 (1637.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:54:20.606791: step 91630, loss = 0.63 (1608.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:54:21.395871: step 91640, loss = 0.80 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:22.185654: step 91650, loss = 0.72 (1620.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:22.974947: step 91660, loss = 0.73 (1621.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:23.760318: step 91670, loss = 0.62 (1629.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:24.546128: step 91680, loss = 0.71 (1628.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:25.341335: step 91690, loss = 0.78 (1609.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:54:26.228275: step 91700, loss = 0.71 (1443.2 examples/sec; 0.089 sec/batch)
2017-05-02 16:54:26.922621: step 91710, loss = 0.53 (1843.5 examples/sec; 0.069 sec/batch)
2017-05-02 16:54:27.708593: step 91720, loss = 0.87 (1628.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:28.497310: step 91730, loss = 0.81 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:29.291092: step 91740, loss = 0.73 (1612.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:30.078938: step 91750, loss = 0.68 (1624.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:30.878622: step 91760, loss = 0.69 (1600.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:54:31.665262: step 91770, loss = 0.67 (1627.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:32.462911: step 91780, loss = 0.66 (1604.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:54:33.269547: step 91790, loss = 0.65 (1586.8 examples/sec; 0.081 sec/batch)
2017-05-02 16:54:34.166760: step 91800, loss = 0.67 (1426.6 examples/sec; 0.090 sec/batch)
2017-05-02 16:54:34.856031: step 91810, loss = 0.81 (1857.0 examples/sec; 0.069 sec/batch)
2017-05-02 16:54:35.641265: step 91820, loss = 0.82 (1630.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:36.434429: step 91830, loss = 0.80 (1613.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:37.234345: step 91840, loss = 0.77 (1600.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:54:38.022387: step 91850, loss = 0.70 (1624.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:38.807365: step 91860, loss = 0.72 (1630.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:54:39.591713: step 91870, loss = 0.77 (1631.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:54:40.381558: step 91880, loss = 0.79 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:41.173104: step 91890, loss = 0.71 (1617.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:42.064582: step 91900, loss = 0.88 (1435.8 examples/sec; 0.089 sec/batch)
2017-05-02 16:54:42.762176: step 91910, loss = 0.53 (1834.9 examples/sec; 0.070 sec/batch)
2017-05-02 16:54:43.548331: step 91920, loss = 0.69 (1628.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:44.343385: step 91930, loss = 0.72 (1610.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:54:45.133891: step 91940, loss = 0.58 (1619.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:45.915555: step 91950, loss = 0.74 (1637.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:54:46.712595: step 91960, loss = 0.82 (1605.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:54:47.499137: step 91970, loss = 0.80 (1627.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:48.299031: step 91980, loss = 0.63 (1600.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:54:49.090682: step 91990, loss = 0.71 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:49.972069: step 92000, loss = 0.64 (1452.3 examples/sec; 0.088 sec/batch)
2017-05-02 16:54:50.675134: step 92010, loss = 0.78 (1820.6 examples/sec; 0.070 sec/batch)
2017-05-02 16:54:51.473445: step 92020, loss = 0.55 (1603.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:54:52.259841: step 92030, loss = 0.84 (1627.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:53.054339: step 92040, loss = 0.73 (1611.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:53.845159: step 92050, loss = 0.62 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:54.640697: step 92060, loss = 0.74 (1609.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:54:55.427758: step 92070, loss = 0.67 (1626.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:56.223385: step 92080, loss = 0.60 (1608.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:54:57.016423: step 92090, loss = 0.67 (1614.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:54:57.931133: step 92100, loss = 0.66 (1399.3 examples/sec; 0.091 sec/batch)
2017-05-02 16:54:58.602423: step 92110, loss = 0.69 (1906.8 examples/sec; 0.067 sec/batch)
2017-05-02 16:54:59.392873: step 92120, loss = 0.64 (1619.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:00.181960: step 92130, loss = 0.59 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:00.978461: step 92140, loss = 0.66 (1607.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:55:01.771711: step 92150, loss = 0.83 (1613.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:02.580791: step 92160, loss = 0.79 (1582.1 examples/sec; 0.081 sec/batch)
2017-05-02 16:55:03.370775: step 92170, loss = 0.73 (1620.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:04.156068: step 92180, loss = 0.61 (1630.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:04.949216: step 92190, loss = 0.62 (1613.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:05.840279: step 92200, loss = 0.82 (1436.5 examples/sec; 0.089 sec/batch)
2017-05-02 16:55:06.536916: step 92210, loss = 0.73 (1837.4 examples/sec; 0.070 sec/batch)
2017-05-02 16:55:07.330085: step 92220, loss = 0.75 (1613.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:08.118784: step 92230, loss = 0.82 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:08.906428: step 92240, loss = 0.60 (1625.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:09.705501: step 92250, loss = 0.71 (1601.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:55:10.502819: step 92260, loss = 0.78 (1605.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:55:11.296405: step 92270, loss = 0.70 (1612.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:12.085093: step 92280, loss = 0.68 (1623.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:12.879246: step 92290, loss = 0.66 (1611.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:13.770458: step 92300, loss = 0.63 (1436.3 examples/sec; 0.089 sec/batch)
2017-05-02 16:55:14.468936: step 92310, loss = 0.66 (1832.6 examples/sec; 0.070 sec/batch)
2017-05-02 16:55:15.266721: step 92320, loss = 0.64 (1604.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:55:16.051500: step 92330, loss = 0.60 (1631.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:55:16.847722: step 92340, loss = 0.82 (1607.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:55:17.642174: step 92350, loss = 0.94 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:18.440578: step 92360, loss = 0.75 (1603.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:55:19.238065: step 92370, loss = 0.88 (1605.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:55:20.015576: step 92380, loss = 0.67 (1646.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:55:20.813596: step 92390, loss = 0.68 (1604.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:55:21.700623: step 92400, loss = 0.88 (1443.0 examples/sec; 0.089 sec/batch)
2017-05-02 16:55:22.399070: step 92410, loss = 0.66 (1832.6 examples/sec; 0.070 sec/batch)
2017-05-02 16:55:23.191162: step 92420, loss = 0.86 (1616.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:23.977291: step 92430, loss = 0.85 (1628.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:24.770004: step 92440, loss = 0.73 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:25.562548: step 92450, loss = 0.62 (1615.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:26.357647: step 92460, loss = 0.55 (1609.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:55:27.152016: step 92470, loss = 0.72 (1611.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:27.933870: step 92480, loss = 0.78 (1637.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:55:28.735287: step 92490, loss = 0.83 (1597.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:55:29.629673: step 92500, loss = 0.60 (1431.1 examples/sec; 0.089 sec/batch)
2017-05-02 16:55:30.336077: step 92510, loss = 0.83 (1812.0 examples/sec; 0.071 sec/batch)
2017-05-02 16:55:31.135175: step 92520, loss = 0.75 (1601.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:55:31.917342: step 92530, loss = 0.73 (1636.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:55:32.714919: step 92540, loss = 0.69 (1604.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:55:33.509062: step 92550, loss = 0.74 (1611.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:34.304480: step 92560, loss = 0.71 (1609.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:55:35.093708: step 92570, loss = 0.55 (1621.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:35.877346: step 92580, loss = 0.74 (1633.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:55:36.669440: step 92590, loss = 0.59 (1616.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:37.579756: step 92600, loss = 0.71 (1406.1 examples/sec; 0.091 sec/batch)
2017-05-02 16:55:38.262380: step 92610, loss = 0.63 (1875.1 examples/sec; 0.068 sec/batch)
2017-05-02 16:55:39.051673: step 92620, loss = 0.65 (1621.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:39.833717: step 92630, loss = 0.83 (1636.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:55:40.625935: step 92640, loss = 0.62 (1615.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:41.418533: step 92650, loss = 0.66 (1614.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:42.213344: step 92660, loss = 0.64 (1610.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:43.006024: step 92670, loss = 0.58 (1614.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:43.789641: step 92680, loss = 0.63 (1633.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:55:44.580296: step 92690, loss = 0.75 (1618.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:45.480320: step 92700, loss = 0.72 (1422.2 examples/sec; 0.090 sec/batch)
2017-05-02 16:55:46.169644: step 92710, loss = 0.62 (1856.9 examples/sec; 0.069 sec/batch)
2017-05-02 16:55:46.962249: step 92720, loss = 0.68 (1614.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:47.743370: step 92730, loss = 0.80 (1638.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:55:48.531674: step 92740, loss = 0.63 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:49.324933: step 92750, loss = 0.83 (1613.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:50.115785: step 92760, loss = 0.84 (1618.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:50.907117: step 92770, loss = 0.77 (1617.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:51.691316: step 92780, loss = 0.75 (1632.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:55:52.482864: step 92790, loss = 0.72 (1617.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:53.374262: step 92800, loss = 0.68 (1435.9 examples/sec; 0.089 sec/batch)
2017-05-02 16:55:54.072218: step 92810, loss = 0.77 (1833.9 examples/sec; 0.070 sec/batch)
2017-05-02 16:55:54.859501: step 92820, loss = 0.76 (1625.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:55.646351: step 92830, loss = 0.77 (1626.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:56.444439: step 92840, loss = 0.74 (1603.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:55:57.238641: step 92850, loss = 0.81 (1611.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:58.028610: step 92860, loss = 0.59 (1620.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:58.821384: step 92870, loss = 0.71 (1614.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:55:59.610979: step 92880, loss = 0.86 (1621.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:00.407144: step 92890, loss = 0.73 (1607.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:56:01.304130: step 92900, loss = 0.65 (1427.0 examples/sec; 0.090 sec/batch)
2017-05-02 16:56:01.997077: step 92910, loss = 0.72 (1847.2 examples/sec; 0.069 sec/batch)
2017-05-02 16:56:02.782515: step 92920, loss = 0.72 (1629.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:03.570310: step 92930, loss = 0.79 (1624.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:04.359728: step 92940, loss = 0.67 (1621.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:05.155015: step 92950, loss = 0.62 (1609.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:56:05.950201: step 92960, loss = 0.73 (1609.7 examples/sec; 0.080 sec/batch)
2017-05-02 16:56:06.743291: step 92970, loss = 0.66 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:07.527531: step 92980, loss = 0.72 (1632.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:56:08.317192: step 92990, loss = 0.58 (1620.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:09.231483: step 93000, loss = 0.68 (1400.0 examples/sec; 0.091 sec/batch)
2017-05-02 16:56:09.910468: step 93010, loss = 0.73 (1885.2 examples/sec; 0.068 sec/batch)
2017-05-02 16:56:10.704320: step 93020, loss = 0.62 (1612.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:11.493275: step 93030, loss = 0.84 (1622.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:12.280734: step 93040, loss = 0.93 (1625.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:13.074270: step 93050, loss = 0.75 (1613.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:13.878829: step 93060, loss = 0.84 (1590.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:56:14.671494: step 93070, loss = 1.02 (1614.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:15.458758: step 93080, loss = 0.84 (1625.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:16.247846: step 93090, loss = 0.70 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:17.133711: step 93100, loss = 0.80 (1444.9 examples/sec; 0.089 sec/batch)
2017-05-02 16:56:17.833708: step 93110, loss = 0.72 (1828.6 examples/sec; 0.070 sec/batch)
2017-05-02 16:56:18.625606: step 93120, loss = 0.77 (1616.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:19.414013: step 93130, loss = 0.61 (1623.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:20.195741: step 93140, loss = 0.70 (1637.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:56:20.983042: step 93150, loss = 0.77 (1625.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:21.779786: step 93160, loss = 0.80 (1606.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:56:22.567679: step 93170, loss = 0.59 (1624.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:23.350030: step 93180, loss = 0.75 (1636.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:56:24.134579: step 93190, loss = 0.80 (1631.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:56:25.025086: step 93200, loss = 0.67 (1437.4 examples/sec; 0.089 sec/batch)
2017-05-02 16:56:25.721645: step 93210, loss = 0.67 (1837.6 examples/sec; 0.070 sec/batch)
2017-05-02 16:56:26.511397: step 93220, loss = 0.65 (1620.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:27.300539: step 93230, loss = 0.65 (1622.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:28.076659: step 93240, loss = 0.59 (1649.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:56:28.869683: step 93250, loss = 0.70 (1614.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:29.664785: step 93260, loss = 0.56 (1609.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:56:30.458120: step 93270, loss = 0.70 (1613.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:31.252161: step 93280, loss = 0.73 (1612.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:32.029221: step 93290, loss = 0.72 (1647.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:56:32.923457: step 93300, loss = 0.62 (1431.4 examples/sec; 0.089 sec/batch)
2017-05-02 16:56:33.612079: step 93310, loss = 0.71 (1858.8 examples/sec; 0.069 sec/batch)
2017-05-02 16:56:34.398647: step 93320, loss = 0.71 (1627.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:35.187394: step 93330, loss = 0.66 (1622.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:35.971438: step 93340, loss = 0.65 (1632.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:56:36.757742: step 93350, loss = 0.67 (1627.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:37.543459: step 93360, loss = 0.79 (1629.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:38.332148: step 93370, loss = 0.64 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:39.127666: step 93380, loss = 0.77 (1609.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:56:39.913103: step 93390, loss = 0.63 (1629.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:40.805326: step 93400, loss = 0.81 (1434.6 examples/sec; 0.089 sec/batch)
2017-05-02 16:56:41.504335: step 93410, loss = 0.57 (1831.2 examples/sec; 0.070 sec/batch)
2017-05-02 16:56:42.295310: step 93420, loss = 0.62 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:43.085249: step 93430, loss = 0.73 (1620.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:43.869773: step 93440, loss = 0.77 (1631.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:56:44.663847: step 93450, loss = 0.82 (1611.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:45.455844: step 93460, loss = 0.65 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:46.246276: step 93470, loss = 0.76 (1619.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:47.036959: step 93480, loss = 0.70 (1618.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:47.819897: step 93490, loss = 0.63 (1634.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:56:48.730410: step 93500, loss = 0.80 (1405.8 examples/sec; 0.091 sec/batch)
2017-05-02 16:56:49.398050: step 93510, loss = 0.73 (1917.2 examples/sec; 0.067 sec/batch)
2017-05-02 16:56:50.192111: step 93520, loss = 0.70 (1612.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:50.984684: step 93530, loss = 0.70 (1615.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:51.763573: step 93540, loss = 0.61 (1643.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:56:52.551237: step 93550, loss = 0.67 (1625.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:53.345361: step 93560, loss = 0.75 (1611.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:54.136554: step 93570, loss = 0.64 (1617.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:54.926095: step 93580, loss = 0.64 (1621.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:55.715085: step 93590, loss = 0.74 (1622.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:56.606254: step 93600, loss = 0.56 (1436.3 examples/sec; 0.089 sec/batch)
2017-05-02 16:56:57.310325: step 93610, loss = 0.86 (1818.0 examples/sec; 0.070 sec/batch)
2017-05-02 16:56:58.095707: step 93620, loss = 0.75 (1629.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:58.882677: step 93630, loss = 0.80 (1626.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:56:59.664474: step 93640, loss = 0.59 (1637.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:57:00.452770: step 93650, loss = 0.80 (1623.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:01.243026: step 93660, loss = 0.86 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:02.025839: step 93670, loss = 0.69 (1635.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:57:02.812056: step 93680, loss = 0.74 (1628.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:03.593026: step 93690, loss = 0.78 (1639.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:57:04.484272: step 93700, loss = 0.67 (1436.2 examples/sec; 0.089 sec/batch)
2017-05-02 16:57:05.177821: step 93710, loss = 0.84 (1845.6 examples/sec; 0.069 sec/batch)
2017-05-02 16:57:05.970554: step 93720, loss = 0.71 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:06.765075: step 93730, loss = 0.72 (1611.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:07.547692: step 93740, loss = 0.59 (1635.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:57:08.339270: step 93750, loss = 0.72 (1617.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:09.130050: step 93760, loss = 0.89 (1618.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:09.919579: step 93770, loss = 0.68 (1621.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:10.705052: step 93780, loss = 0.70 (1629.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:11.488535: step 93790, loss = 0.67 (1633.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:57:12.375433: step 93800, loss = 0.70 (1443.2 examples/sec; 0.089 sec/batch)
2017-05-02 16:57:13.079644: step 93810, loss = 0.69 (1817.7 examples/sec; 0.070 sec/batch)
2017-05-02 16:57:13.864124: step 93820, loss = 0.70 (1631.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:57:14.657439: step 93830, loss = 0.70 (1613.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:15.448003: step 93840, loss = 0.61 (1619.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:16.236128: step 93850, loss = 0.72 (1624.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:17.023200: step 93860, loss = 0.65 (1626.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:17.816645: step 93870, loss = 0.79 (1613.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:18.610593: step 93880, loss = 0.79 (1612.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:19.399388: step 93890, loss = 0.78 (1622.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:20.276597: step 93900, loss = 0.68 (1459.2 examples/sec; 0.088 sec/batch)
2017-05-02 16:57:20.978412: step 93910, loss = 0.68 (1823.8 examples/sec; 0.070 sec/batch)
2017-05-02 16:57:21.764993: step 93920, loss = 0.77 (1627.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:22.554080: step 93930, loss = 0.68 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:23.342828: step 93940, loss = 0.63 (1622.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:24.117584: step 93950, loss = 0.59 (1652.1 examples/sec; 0.077 sec/batch)
2017-05-02 16:57:24.915444: step 93960, loss = 0.81 (1604.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:57:25.707531: step 93970, loss = 0.70 (1616.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:26.499145: step 93980, loss = 0.71 (1617.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:27.283628: step 93990, loss = 0.68 (1631.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:57:28.169700: step 94000, loss = 0.72 (1444.6 examples/sec; 0.089 sec/batch)
2017-05-02 16:57:28.869236: step 94010, loss = 0.61 (1829.8 examples/sec; 0.070 sec/batch)
2017-05-02 16:57:29.655115: step 94020, loss = 0.72 (1628.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:30.455154: step 94030, loss = 0.77 (1599.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:57:31.255477: step 94040, loss = 0.64 (1599.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:57:32.039073: step 94050, loss = 0.66 (1633.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:57:32.826710: step 94060, loss = 0.63 (1625.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:33.620511: step 94070, loss = 0.56 (1612.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:34.410664: step 94080, loss = 0.77 (1620.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:35.196064: step 94090, loss = 0.80 (1629.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:36.076983: step 94100, loss = 0.66 (1453.0 examples/sec; 0.088 sec/batch)
2017-05-02 16:57:36.770692: step 94110, loss = 0.62 (1845.1 examples/sec; 0.069 sec/batch)
2017-05-02 16:57:37.559909: step 94120, loss = 0.72 (1621.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:38.347139: step 94130, loss = 0.79 (1626.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:39.131827: step 94140, loss = 0.75 (1631.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:57:39.912414: step 94150, loss = 0.72 (1639.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:57:40.706474: step 94160, loss = 0.78 (1612.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:41.496130: step 94170, loss = 0.73 (1621.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:42.285468: step 94180, loss = 0.69 (1621.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:43.074551: step 94190, loss = 0.62 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:43.950589: step 94200, loss = 0.76 (1461.1 examples/sec; 0.088 sec/batch)
2017-05-02 16:57:44.650253: step 94210, loss = 0.73 (1829.5 examples/sec; 0.070 sec/batch)
2017-05-02 16:57:45.446197: step 94220, loss = 0.70 (1608.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:57:46.236120: step 94230, loss = 0.74 (1620.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:47.027186: step 94240, loss = 0.65 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:47.805557: step 94250, loss = 0.69 (1644.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:57:48.588788: step 94260, loss = 0.68 (1634.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:57:49.380379: step 94270, loss = 0.73 (1617.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:50.166011: step 94280, loss = 0.66 (1629.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:50.959976: step 94290, loss = 0.81 (1612.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:51.845604: step 94300, loss = 0.82 (1445.3 examples/sec; 0.089 sec/batch)
2017-05-02 16:57:52.538407: step 94310, loss = 0.63 (1847.6 examples/sec; 0.069 sec/batch)
2017-05-02 16:57:53.333275: step 94320, loss = 0.61 (1610.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:54.127468: step 94330, loss = 0.83 (1611.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:54.915114: step 94340, loss = 0.73 (1625.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:55.694316: step 94350, loss = 0.70 (1642.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:57:56.495329: step 94360, loss = 0.83 (1598.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:57:57.288948: step 94370, loss = 0.65 (1612.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:58.075532: step 94380, loss = 0.68 (1627.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:58.863503: step 94390, loss = 0.74 (1624.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:57:59.737836: step 94400, loss = 0.71 (1464.0 examples/sec; 0.087 sec/batch)
2017-05-02 16:58:00.432059: step 94410, loss = 0.69 (1843.8 examples/sec; 0.069 sec/batch)
2017-05-02 16:58:01.223864: step 94420, loss = 0.81 (1616.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:02.001861: step 94430, loss = 0.70 (1645.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:58:02.817782: step 94440, loss = 0.64 (1568.8 examples/sec; 0.082 sec/batch)
2017-05-02 16:58:03.601768: step 94450, loss = 0.60 (1632.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:58:04.389298: step 94460, loss = 0.69 (1625.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:05.179915: step 94470, loss = 0.69 (1619.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:05.975133: step 94480, loss = 0.76 (1609.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:58:06.770152: step 94490, loss = 0.59 (1610.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:58:07.661503: step 94500, loss = 0.78 (1436.0 examples/sec; 0.089 sec/batch)
2017-05-02 16:58:08.343755: step 94510, loss = 0.75 (1876.1 examples/sec; 0.068 sec/batch)
2017-05-02 16:58:09.132454: step 94520, loss = 0.71 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:09.921920: step 94530, loss = 0.75 (1621.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:10.710114: step 94540, loss = 0.76 (1624.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:11.500520: step 94550, loss = 0.79 (1619.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:12.286647: step 94560, loss = 0.83 (1628.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:13.077784: step 94570, loss = 0.52 (1617.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:13.873089: step 94580, loss = 0.73 (1609.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:58:14.657635: step 94590, loss = 0.84 (1631.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:58:15.553547: step 94600, loss = 0.85 (1428.7 examples/sec; 0.090 sec/batch)
2017-05-02 16:58:16.248121: step 94610, loss = 0.62 (1842.8 examples/sec; 0.069 sec/batch)
2017-05-02 16:58:17.039088: step 94620, loss = 0.75 (1618.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:17.835536: step 94630, loss = 0.70 (1607.1 examples/sec; 0.080 sec/batch)
2017-05-02 16:58:18.621467: step 94640, loss = 0.72 (1628.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:19.403626: step 94650, loss = 0.59 (1636.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:58:20.189373: step 94660, loss = 0.66 (1629.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:20.977566: step 94670, loss = 0.69 (1624.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:21.778158: step 94680, loss = 0.81 (1598.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:58:22.571963: step 94690, loss = 0.86 (1612.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:23.464595: step 94700, loss = 0.68 (1433.9 examples/sec; 0.089 sec/batch)
2017-05-02 16:58:24.143667: step 94710, loss = 0.59 (1884.9 examples/sec; 0.068 sec/batch)
2017-05-02 16:58:24.933942: step 94720, loss = 0.72 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:25.727892: step 94730, loss = 0.70 (1612.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:26.520272: step 94740, loss = 0.80 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:27.311218: step 94750, loss = 0.64 (1618.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:28.091837: step 94760, loss = 0.75 (1639.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:58:28.873464: step 94770, loss = 0.80 (1637.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:58:29.664003: step 94780, loss = 0.68 (1619.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:30.457539: step 94790, loss = 0.61 (1613.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:31.347134: step 94800, loss = 0.83 (1438.9 examples/sec; 0.089 sec/batch)
2017-05-02 16:58:32.031174: step 94810, loss = 0.69 (1871.2 examples/sec; 0.068 sec/batch)
2017-05-02 16:58:32.822571: step 94820, loss = 0.63 (1617.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:33.609939: step 94830, loss = 0.64 (1625.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:34.401706: step 94840, loss = 0.60 (1616.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:35.204956: step 94850, loss = 0.71 (1593.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:58:35.980540: step 94860, loss = 0.69 (1650.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:58:36.778751: step 94870, loss = 0.68 (1603.6 examples/sec; 0.080 sec/batch)
2017-05-02 16:58:37.567214: step 94880, loss = 0.70 (1623.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:38.355095: step 94890, loss = 0.73 (1624.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:39.257034: step 94900, loss = 0.58 (1419.2 examples/sec; 0.090 sec/batch)
2017-05-02 16:58:39.931003: step 94910, loss = 0.77 (1899.2 examples/sec; 0.067 sec/batch)
2017-05-02 16:58:40.718289: step 94920, loss = 0.68 (1625.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:41.513371: step 94930, loss = 0.69 (1609.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:58:42.310916: step 94940, loss = 0.67 (1605.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:58:43.097765: step 94950, loss = 0.74 (1626.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:43.879372: step 94960, loss = 0.54 (1637.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:58:44.664448: step 94970, loss = 0.61 (1630.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:45.452421: step 94980, loss = 0.68 (1624.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:46.243503: step 94990, loss = 0.55 (1618.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:47.132208: step 95000, loss = 0.70 (1440.3 examples/sec; 0.089 sec/batch)
2017-05-02 16:58:47.812353: step 95010, loss = 0.82 (1882.0 examples/sec; 0.068 sec/batch)
2017-05-02 16:58:48.611401: step 95020, loss = 0.65 (1601.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:58:49.405098: step 95030, loss = 0.65 (1612.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:50.206897: step 95040, loss = 0.76 (1596.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:58:50.997742: step 95050, loss = 0.57 (1618.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:51.776195: step 95060, loss = 0.74 (1644.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:58:52.571266: step 95070, loss = 0.68 (1609.9 examples/sec; 0.080 sec/batch)
2017-05-02 16:58:53.364494: step 95080, loss = 0.81 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:54.159511: step 95090, loss = 0.77 (1610.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:58:55.048207: step 95100, loss = 0.67 (1440.3 examples/sec; 0.089 sec/batch)
2017-05-02 16:58:55.727336: step 95110, loss = 0.69 (1884.8 examples/sec; 0.068 sec/batch)
2017-05-02 16:58:56.518019: step 95120, loss = 0.68 (1618.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:57.309761: step 95130, loss = 0.68 (1616.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:58.106281: step 95140, loss = 0.75 (1607.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:58:58.899380: step 95150, loss = 0.64 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:58:59.674848: step 95160, loss = 0.69 (1650.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:59:00.471180: step 95170, loss = 0.69 (1607.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:59:01.262074: step 95180, loss = 0.65 (1618.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:02.044033: step 95190, loss = 0.61 (1636.9 examples/sec; 0.078 sec/batch)
2017-05-02 16:59:02.934286: step 95200, loss = 0.72 (1437.8 examples/sec; 0.089 sec/batch)
2017-05-02 16:59:03.623949: step 95210, loss = 0.66 (1856.0 examples/sec; 0.069 sec/batch)
2017-05-02 16:59:04.416677: step 95220, loss = 0.65 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:05.211530: step 95230, loss = 0.71 (1610.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:06.005772: step 95240, loss = 0.72 (1611.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:06.801399: step 95250, loss = 0.80 (1608.8 examples/sec; 0.080 sec/batch)
2017-05-02 16:59:07.584528: step 95260, loss = 0.82 (1634.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:59:08.372491: step 95270, loss = 0.66 (1624.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:09.163836: step 95280, loss = 0.58 (1617.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:09.956250: step 95290, loss = 0.78 (1615.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:10.840739: step 95300, loss = 0.90 (1447.2 examples/sec; 0.088 sec/batch)
2017-05-02 16:59:11.534992: step 95310, loss = 0.74 (1843.7 examples/sec; 0.069 sec/batch)
2017-05-02 16:59:12.319424: step 95320, loss = 0.76 (1631.8 examples/sec; 0.078 sec/batch)
2017-05-02 16:59:13.111558: step 95330, loss = 0.84 (1615.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:13.903821: step 95340, loss = 0.83 (1615.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:14.699207: step 95350, loss = 0.80 (1609.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:59:15.487507: step 95360, loss = 0.62 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:16.271567: step 95370, loss = 0.65 (1632.5 examples/sec; 0.078 sec/batch)
2017-05-02 16:59:17.055720: step 95380, loss = 0.64 (1632.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:59:17.850241: step 95390, loss = 0.69 (1611.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:18.733571: step 95400, loss = 0.69 (1449.1 examples/sec; 0.088 sec/batch)
2017-05-02 16:59:19.428048: step 95410, loss = 0.82 (1843.1 examples/sec; 0.069 sec/batch)
2017-05-02 16:59:20.212301: step 95420, loss = 0.73 (1632.1 examples/sec; 0.078 sec/batch)
2017-05-02 16:59:21.003281: step 95430, loss = 0.57 (1618.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:21.793231: step 95440, loss = 0.63 (1620.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:22.584705: step 95450, loss = 0.76 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:23.366408: step 95460, loss = 0.83 (1637.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:59:24.156364: step 95470, loss = 0.62 (1620.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:24.939441: step 95480, loss = 0.72 (1634.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:59:25.733448: step 95490, loss = 0.67 (1612.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:26.611600: step 95500, loss = 0.66 (1457.6 examples/sec; 0.088 sec/batch)
2017-05-02 16:59:27.319034: step 95510, loss = 0.71 (1809.4 examples/sec; 0.071 sec/batch)
2017-05-02 16:59:28.099391: step 95520, loss = 0.70 (1640.3 examples/sec; 0.078 sec/batch)
2017-05-02 16:59:28.901740: step 95530, loss = 0.78 (1595.3 examples/sec; 0.080 sec/batch)
2017-05-02 16:59:29.693088: step 95540, loss = 0.82 (1617.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:30.486666: step 95550, loss = 0.77 (1612.9 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:31.275561: step 95560, loss = 0.69 (1622.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:32.059223: step 95570, loss = 0.75 (1633.4 examples/sec; 0.078 sec/batch)
2017-05-02 16:59:32.845570: step 95580, loss = 0.68 (1627.8 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:33.638616: step 95590, loss = 0.65 (1614.0 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:34.525123: step 95600, loss = 0.68 (1443.9 examples/sec; 0.089 sec/batch)
2017-05-02 16:59:35.221440: step 95610, loss = 0.57 (1838.3 examples/sec; 0.070 sec/batch)
2017-05-02 16:59:36.000810: step 95620, loss = 0.73 (1642.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:59:36.793430: step 95630, loss = 0.68 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:37.588199: step 95640, loss = 0.66 (1610.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:38.379994: step 95650, loss = 0.78 (1616.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:39.176279: step 95660, loss = 0.62 (1607.5 examples/sec; 0.080 sec/batch)
2017-05-02 16:59:39.952676: step 95670, loss = 0.63 (1648.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:59:40.750996: step 95680, loss = 0.67 (1603.4 examples/sec; 0.080 sec/batch)
2017-05-02 16:59:41.542233: step 95690, loss = 0.72 (1617.7 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:42.433291: step 95700, loss = 0.64 (1436.5 examples/sec; 0.089 sec/batch)
2017-05-02 16:59:43.123662: step 95710, loss = 0.78 (1854.1 examples/sec; 0.069 sec/batch)
2017-05-02 16:59:43.903327: step 95720, loss = 0.73 (1641.7 examples/sec; 0.078 sec/batch)
2017-05-02 16:59:44.692772: step 95730, loss = 0.65 (1621.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:45.481652: step 95740, loss = 0.69 (1622.6 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:46.270759: step 95750, loss = 0.68 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:47.066758: step 95760, loss = 0.60 (1608.0 examples/sec; 0.080 sec/batch)
2017-05-02 16:59:47.849510: step 95770, loss = 0.69 (1635.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:59:48.645910: step 95780, loss = 0.65 (1607.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:59:49.443827: step 95790, loss = 0.86 (1604.2 examples/sec; 0.080 sec/batch)
2017-05-02 16:59:50.336790: step 95800, loss = 0.61 (1433.4 examples/sec; 0.089 sec/batch)
2017-05-02 16:59:51.034027: step 95810, loss = 0.81 (1835.8 examples/sec; 0.070 sec/batch)
2017-05-02 16:59:51.813757: step 95820, loss = 0.68 (1641.6 examples/sec; 0.078 sec/batch)
2017-05-02 16:59:52.604157: step 95830, loss = 0.63 (1619.4 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:53.396102: step 95840, loss = 0.73 (1616.3 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:54.174235: step 95850, loss = 0.77 (1645.0 examples/sec; 0.078 sec/batch)
2017-05-02 16:59:54.962789: step 95860, loss = 0.71 (1623.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:55.741749: step 95870, loss = 0.63 (1643.2 examples/sec; 0.078 sec/batch)
2017-05-02 16:59:56.532096: step 95880, loss = 0.65 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:57.324060: step 95890, loss = 0.74 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 16:59:58.206362: step 95900, loss = 0.83 (1450.8 examples/sec; 0.088 sec/batch)
2017-05-02 16:59:58.904851: step 95910, loss = 0.71 (1832.5 examples/sec; 0.070 sec/batch)
2017-05-02 16:59:59.688641: step 95920, loss = 0.62 (1633.1 examples/sec; 0.078 sec/batch)
2017-05-02 17:00:00.486801: step 95930, loss = 0.67 (1603.7 examples/sec; 0.080 sec/batch)
2017-05-02 17:00:01.283260: step 95940, loss = 0.61 (1607.1 examples/sec; 0.080 sec/batch)
2017-05-02 17:00:02.084748: step 95950, loss = 0.64 (1597.0 examples/sec; 0.080 sec/batch)
2017-05-02 17:00:02.880734: step 95960, loss = 0.66 (1608.1 examples/sec; 0.080 sec/batch)
2017-05-02 17:00:03.668223: step 95970, loss = 0.75 (1625.4 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:04.462636: step 95980, loss = 0.84 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:05.251911: step 95990, loss = 0.59 (1621.7 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:06.138457: step 96000, loss = 0.76 (1443.8 examples/sec; 0.089 sec/batch)
2017-05-02 17:00:06.825032: step 96010, loss = 0.59 (1864.3 examples/sec; 0.069 sec/batch)
2017-05-02 17:00:07.609515: step 96020, loss = 0.71 (1631.6 examples/sec; 0.078 sec/batch)
2017-05-02 17:00:08.404330: step 96030, loss = 0.75 (1610.4 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:09.201672: step 96040, loss = 0.78 (1605.3 examples/sec; 0.080 sec/batch)
2017-05-02 17:00:09.996138: step 96050, loss = 0.68 (1611.1 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:10.786405: step 96060, loss = 0.60 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:11.567028: step 96070, loss = 0.63 (1639.7 examples/sec; 0.078 sec/batch)
2017-05-02 17:00:12.353875: step 96080, loss = 0.60 (1626.8 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:13.144061: step 96090, loss = 0.83 (1619.8 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:14.042153: step 96100, loss = 0.67 (1425.2 examples/sec; 0.090 sec/batch)
2017-05-02 17:00:14.725268: step 96110, loss = 0.53 (1873.8 examples/sec; 0.068 sec/batch)
2017-05-02 17:00:15.509246: step 96120, loss = 0.63 (1632.7 examples/sec; 0.078 sec/batch)
2017-05-02 17:00:16.294433: step 96130, loss = 0.67 (1630.2 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:17.088086: step 96140, loss = 0.65 (1612.8 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:17.874957: step 96150, loss = 0.61 (1626.7 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:18.670351: step 96160, loss = 0.67 (1609.3 examples/sec; 0.080 sec/batch)
2017-05-02 17:00:19.450589: step 96170, loss = 0.67 (1640.6 examples/sec; 0.078 sec/batch)
2017-05-02 17:00:20.230401: step 96180, loss = 0.63 (1641.4 examples/sec; 0.078 sec/batch)
2017-05-02 17:00:21.019481: step 96190, loss = 0.74 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:21.903899: step 96200, loss = 0.67 (1447.3 examples/sec; 0.088 sec/batch)
2017-05-02 17:00:22.598320: step 96210, loss = 0.64 (1843.2 examples/sec; 0.069 sec/batch)
2017-05-02 17:00:23.384006: step 96220, loss = 0.81 (1629.2 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:24.171957: step 96230, loss = 0.62 (1624.5 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:24.961832: step 96240, loss = 0.81 (1620.5 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:25.753745: step 96250, loss = 0.69 (1616.3 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:26.544728: step 96260, loss = 0.73 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:27.334600: step 96270, loss = 0.63 (1620.5 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:28.113119: step 96280, loss = 0.83 (1644.1 examples/sec; 0.078 sec/batch)
2017-05-02 17:00:28.905190: step 96290, loss = 0.76 (1616.0 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:29.802388: step 96300, loss = 0.67 (1426.7 examples/sec; 0.090 sec/batch)
2017-05-02 17:00:30.495504: step 96310, loss = 0.83 (1846.7 examples/sec; 0.069 sec/batch)
2017-05-02 17:00:31.289511: step 96320, loss = 0.69 (1612.1 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:32.073645: step 96330, loss = 0.65 (1632.4 examples/sec; 0.078 sec/batch)
2017-05-02 17:00:32.870514: step 96340, loss = 0.62 (1606.3 examples/sec; 0.080 sec/batch)
2017-05-02 17:00:33.662546: step 96350, loss = 0.68 (1616.1 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:34.456417: step 96360, loss = 0.59 (1612.4 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:35.247447: step 96370, loss = 0.66 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:36.030059: step 96380, loss = 0.85 (1635.5 examples/sec; 0.078 sec/batch)
2017-05-02 17:00:36.818789: step 96390, loss = 0.83 (1622.9 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:37.705809: step 96400, loss = 0.66 (1443.0 examples/sec; 0.089 sec/batch)
2017-05-02 17:00:38.395018: step 96410, loss = 0.74 (1857.2 examples/sec; 0.069 sec/batch)
2017-05-02 17:00:39.183784: step 96420, loss = 0.65 (1622.8 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:39.957784: step 96430, loss = 0.62 (1653.7 examples/sec; 0.077 sec/batch)
2017-05-02 17:00:40.750480: step 96440, loss = 0.66 (1614.7 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:41.537751: step 96450, loss = 0.70 (1625.9 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:42.332180: step 96460, loss = 0.75 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:43.125551: step 96470, loss = 0.73 (1613.4 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:43.906477: step 96480, loss = 0.72 (1639.1 examples/sec; 0.078 sec/batch)
2017-05-02 17:00:44.701648: step 96490, loss = 0.67 (1609.7 examples/sec; 0.080 sec/batch)
2017-05-02 17:00:45.611067: step 96500, loss = 0.63 (1407.5 examples/sec; 0.091 sec/batch)
2017-05-02 17:00:46.291538: step 96510, loss = 0.79 (1881.1 examples/sec; 0.068 sec/batch)
2017-05-02 17:00:47.081456: step 96520, loss = 0.70 (1620.4 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:47.862524: step 96530, loss = 0.57 (1638.8 examples/sec; 0.078 sec/batch)
2017-05-02 17:00:48.654631: step 96540, loss = 0.60 (1615.9 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:49.448061: step 96550, loss = 0.69 (1613.2 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:50.242569: step 96560, loss = 0.62 (1611.1 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:51.033622: step 96570, loss = 0.62 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:51.818526: step 96580, loss = 0.74 (1630.8 examples/sec; 0.078 sec/batch)
2017-05-02 17:00:52.613812: step 96590, loss = 0.74 (1609.5 examples/sec; 0.080 sec/batch)
2017-05-02 17:00:53.507309: step 96600, loss = 0.86 (1432.6 examples/sec; 0.089 sec/batch)
2017-05-02 17:00:54.197243: step 96610, loss = 0.61 (1855.3 examples/sec; 0.069 sec/batch)
2017-05-02 17:00:54.975065: step 96620, loss = 0.80 (1645.6 examples/sec; 0.078 sec/batch)
2017-05-02 17:00:55.757317: step 96630, loss = 0.76 (1636.3 examples/sec; 0.078 sec/batch)
2017-05-02 17:00:56.553737: step 96640, loss = 0.66 (1607.2 examples/sec; 0.080 sec/batch)
2017-05-02 17:00:57.340830: step 96650, loss = 0.84 (1626.2 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:58.130348: step 96660, loss = 0.67 (1621.2 examples/sec; 0.079 sec/batch)
2017-05-02 17:00:58.926749: step 96670, loss = 0.84 (1607.2 examples/sec; 0.080 sec/batch)
2017-05-02 17:00:59.698820: step 96680, loss = 0.77 (1657.9 examples/sec; 0.077 sec/batch)
2017-05-02 17:01:00.492539: step 96690, loss = 0.83 (1612.7 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:01.377344: step 96700, loss = 0.66 (1446.6 examples/sec; 0.088 sec/batch)
2017-05-02 17:01:02.080622: step 96710, loss = 0.69 (1820.1 examples/sec; 0.070 sec/batch)
2017-05-02 17:01:02.870029: step 96720, loss = 0.69 (1621.5 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:03.659301: step 96730, loss = 0.55 (1621.7 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:04.449276: step 96740, loss = 0.71 (1620.3 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:05.239863: step 96750, loss = 0.52 (1619.1 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:06.024205: step 96760, loss = 0.74 (1631.9 examples/sec; 0.078 sec/batch)
2017-05-02 17:01:06.819534: step 96770, loss = 0.74 (1609.4 examples/sec; 0.080 sec/batch)
2017-05-02 17:01:07.600188: step 96780, loss = 0.65 (1639.7 examples/sec; 0.078 sec/batch)
2017-05-02 17:01:08.390838: step 96790, loss = 0.69 (1618.9 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:09.279180: step 96800, loss = 0.54 (1440.9 examples/sec; 0.089 sec/batch)
2017-05-02 17:01:09.963033: step 96810, loss = 0.85 (1871.8 examples/sec; 0.068 sec/batch)
2017-05-02 17:01:10.748439: step 96820, loss = 0.77 (1629.7 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:11.532139: step 96830, loss = 0.55 (1633.3 examples/sec; 0.078 sec/batch)
2017-05-02 17:01:12.319783: step 96840, loss = 0.66 (1625.1 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:13.116720: step 96850, loss = 0.74 (1606.2 examples/sec; 0.080 sec/batch)
2017-05-02 17:01:13.908279: step 96860, loss = 0.72 (1617.0 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:14.705694: step 96870, loss = 0.65 (1605.2 examples/sec; 0.080 sec/batch)
2017-05-02 17:01:15.491646: step 96880, loss = 0.68 (1628.6 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:16.279528: step 96890, loss = 0.66 (1624.6 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:17.160823: step 96900, loss = 0.75 (1452.4 examples/sec; 0.088 sec/batch)
2017-05-02 17:01:17.857235: step 96910, loss = 0.68 (1838.0 examples/sec; 0.070 sec/batch)
2017-05-02 17:01:18.639516: step 96920, loss = 0.72 (1636.2 examples/sec; 0.078 sec/batch)
2017-05-02 17:01:19.427087: step 96930, loss = 0.79 (1625.3 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:20.211387: step 96940, loss = 0.72 (1632.0 examples/sec; 0.078 sec/batch)
2017-05-02 17:01:21.006381: step 96950, loss = 0.76 (1610.1 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:21.789265: step 96960, loss = 0.68 (1635.0 examples/sec; 0.078 sec/batch)
2017-05-02 17:01:22.579821: step 96970, loss = 0.74 (1619.1 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:23.366529: step 96980, loss = 0.71 (1627.0 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:24.147210: step 96990, loss = 0.85 (1639.6 examples/sec; 0.078 sec/batch)
2017-05-02 17:01:25.037721: step 97000, loss = 0.70 (1437.4 examples/sec; 0.089 sec/batch)
2017-05-02 17:01:25.726238: step 97010, loss = 0.60 (1859.1 examples/sec; 0.069 sec/batch)
2017-05-02 17:01:26.510271: step 97020, loss = 0.74 (1632.6 examples/sec; 0.078 sec/batch)
2017-05-02 17:01:27.299508: step 97030, loss = 0.65 (1621.8 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:28.084153: step 97040, loss = 0.58 (1631.3 examples/sec; 0.078 sec/batch)
2017-05-02 17:01:28.871818: step 97050, loss = 0.79 (1625.1 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:29.662104: step 97060, loss = 0.65 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:30.451550: step 97070, loss = 0.55 (1621.4 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:31.243141: step 97080, loss = 0.63 (1617.0 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:32.030338: step 97090, loss = 0.73 (1626.0 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:32.930437: step 97100, loss = 0.72 (1422.1 examples/sec; 0.090 sec/batch)
2017-05-02 17:01:33.627488: step 97110, loss = 0.61 (1836.3 examples/sec; 0.070 sec/batch)
2017-05-02 17:01:34.414219: step 97120, loss = 0.62 (1627.0 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:35.207298: step 97130, loss = 0.75 (1614.0 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:35.986507: step 97140, loss = 0.94 (1642.7 examples/sec; 0.078 sec/batch)
2017-05-02 17:01:36.778370: step 97150, loss = 0.66 (1616.4 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:37.565959: step 97160, loss = 0.77 (1625.2 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:38.359250: step 97170, loss = 0.77 (1613.5 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:39.153465: step 97180, loss = 0.72 (1611.7 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:39.931445: step 97190, loss = 0.72 (1645.3 examples/sec; 0.078 sec/batch)
2017-05-02 17:01:40.826379: step 97200, loss = 0.61 (1430.3 examples/sec; 0.089 sec/batch)
2017-05-02 17:01:41.521852: step 97210, loss = 0.68 (1840.5 examples/sec; 0.070 sec/batch)
2017-05-02 17:01:42.317385: step 97220, loss = 0.60 (1609.0 examples/sec; 0.080 sec/batch)
2017-05-02 17:01:43.112178: step 97230, loss = 0.59 (1610.5 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:43.896712: step 97240, loss = 0.70 (1631.5 examples/sec; 0.078 sec/batch)
2017-05-02 17:01:44.682756: step 97250, loss = 0.71 (1628.4 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:45.470148: step 97260, loss = 0.67 (1625.6 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:46.256107: step 97270, loss = 0.70 (1628.6 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:47.045795: step 97280, loss = 0.66 (1620.9 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:47.825814: step 97290, loss = 0.82 (1641.0 examples/sec; 0.078 sec/batch)
2017-05-02 17:01:48.712129: step 97300, loss = 0.60 (1444.2 examples/sec; 0.089 sec/batch)
2017-05-02 17:01:49.416199: step 97310, loss = 0.67 (1818.0 examples/sec; 0.070 sec/batch)
2017-05-02 17:01:50.210971: step 97320, loss = 0.56 (1610.5 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:51.001242: step 97330, loss = 0.71 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:51.777957: step 97340, loss = 0.76 (1648.0 examples/sec; 0.078 sec/batch)
2017-05-02 17:01:52.569305: step 97350, loss = 0.70 (1617.5 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:53.355398: step 97360, loss = 0.62 (1628.3 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:54.143249: step 97370, loss = 0.75 (1624.7 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:54.940413: step 97380, loss = 0.71 (1605.7 examples/sec; 0.080 sec/batch)
2017-05-02 17:01:55.716284: step 97390, loss = 0.70 (1649.8 examples/sec; 0.078 sec/batch)
2017-05-02 17:01:56.619973: step 97400, loss = 0.75 (1416.4 examples/sec; 0.090 sec/batch)
2017-05-02 17:01:57.315201: step 97410, loss = 0.70 (1841.1 examples/sec; 0.070 sec/batch)
2017-05-02 17:01:58.102819: step 97420, loss = 0.69 (1625.2 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:58.895872: step 97430, loss = 0.74 (1614.0 examples/sec; 0.079 sec/batch)
2017-05-02 17:01:59.671284: step 97440, loss = 0.55 (1650.7 examples/sec; 0.078 sec/batch)
2017-05-02 17:02:00.467401: step 97450, loss = 0.68 (1607.8 examples/sec; 0.080 sec/batch)
2017-05-02 17:02:01.258645: step 97460, loss = 0.80 (1617.7 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:02.039306: step 97470, loss = 0.84 (1639.7 examples/sec; 0.078 sec/batch)
2017-05-02 17:02:02.827529: step 97480, loss = 0.91 (1623.9 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:03.608277: step 97490, loss = 0.73 (1639.5 examples/sec; 0.078 sec/batch)
2017-05-02 17:02:04.515671: step 97500, loss = 0.78 (1410.6 examples/sec; 0.091 sec/batch)
2017-05-02 17:02:05.182928: step 97510, loss = 0.86 (1918.3 examples/sec; 0.067 sec/batch)
2017-05-02 17:02:05.972768: step 97520, loss = 0.65 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:06.765738: step 97530, loss = 0.69 (1614.2 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:07.548930: step 97540, loss = 0.82 (1634.3 examples/sec; 0.078 sec/batch)
2017-05-02 17:02:08.344919: step 97550, loss = 0.66 (1608.1 examples/sec; 0.080 sec/batch)
2017-05-02 17:02:09.137288: step 97560, loss = 0.58 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:09.927192: step 97570, loss = 0.76 (1620.4 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:10.720827: step 97580, loss = 0.64 (1612.8 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:11.505125: step 97590, loss = 0.75 (1632.0 examples/sec; 0.078 sec/batch)
2017-05-02 17:02:12.391049: step 97600, loss = 0.75 (1444.8 examples/sec; 0.089 sec/batch)
2017-05-02 17:02:13.081252: step 97610, loss = 0.56 (1854.5 examples/sec; 0.069 sec/batch)
2017-05-02 17:02:13.870661: step 97620, loss = 0.70 (1621.5 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:14.664219: step 97630, loss = 0.62 (1613.0 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:15.443767: step 97640, loss = 0.71 (1642.0 examples/sec; 0.078 sec/batch)
2017-05-02 17:02:16.232074: step 97650, loss = 0.59 (1623.7 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:17.014313: step 97660, loss = 0.82 (1636.3 examples/sec; 0.078 sec/batch)
2017-05-02 17:02:17.806676: step 97670, loss = 0.70 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:18.592856: step 97680, loss = 0.71 (1628.1 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:19.377872: step 97690, loss = 0.73 (1630.5 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:20.254695: step 97700, loss = 0.56 (1459.8 examples/sec; 0.088 sec/batch)
2017-05-02 17:02:20.947332: step 97710, loss = 0.82 (1848.0 examples/sec; 0.069 sec/batch)
2017-05-02 17:02:21.739310: step 97720, loss = 0.71 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:22.529585: step 97730, loss = 0.63 (1619.7 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:23.315577: step 97740, loss = 0.67 (1628.5 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:24.106499: step 97750, loss = 0.59 (1618.4 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:24.898208: step 97760, loss = 0.73 (1616.8 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:25.692900: step 97770, loss = 0.69 (1610.7 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:26.485485: step 97780, loss = 0.61 (1615.0 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:27.268434: step 97790, loss = 0.56 (1634.9 examples/sec; 0.078 sec/batch)
2017-05-02 17:02:28.151786: step 97800, loss = 0.73 (1449.0 examples/sec; 0.088 sec/batch)
2017-05-02 17:02:28.843535: step 97810, loss = 0.80 (1850.4 examples/sec; 0.069 sec/batch)
2017-05-02 17:02:29.637955: step 97820, loss = 0.71 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:30.429789: step 97830, loss = 0.66 (1616.5 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:31.216668: step 97840, loss = 0.79 (1626.7 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:31.998111: step 97850, loss = 0.61 (1638.0 examples/sec; 0.078 sec/batch)
2017-05-02 17:02:32.780097: step 97860, loss = 0.63 (1636.9 examples/sec; 0.078 sec/batch)
2017-05-02 17:02:33.574608: step 97870, loss = 0.72 (1611.0 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:34.368817: step 97880, loss = 0.70 (1611.7 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:35.159976: step 97890, loss = 0.73 (1617.9 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:36.038034: step 97900, loss = 0.58 (1457.8 examples/sec; 0.088 sec/batch)
2017-05-02 17:02:36.734030: step 97910, loss = 0.70 (1839.1 examples/sec; 0.070 sec/batch)
2017-05-02 17:02:37.526933: step 97920, loss = 0.72 (1614.3 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:38.308595: step 97930, loss = 0.70 (1637.5 examples/sec; 0.078 sec/batch)
2017-05-02 17:02:39.095032: step 97940, loss = 0.67 (1627.6 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:39.876447: step 97950, loss = 0.64 (1638.1 examples/sec; 0.078 sec/batch)
2017-05-02 17:02:40.669534: step 97960, loss = 0.65 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:41.456450: step 97970, loss = 0.70 (1626.6 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:42.245386: step 97980, loss = 0.63 (1622.4 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:43.036332: step 97990, loss = 0.61 (1618.3 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:43.918660: step 98000, loss = 0.65 (1450.7 examples/sec; 0.088 sec/batch)
2017-05-02 17:02:44.613054: step 98010, loss = 0.81 (1843.3 examples/sec; 0.069 sec/batch)
2017-05-02 17:02:45.405459: step 98020, loss = 0.82 (1615.3 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:46.195695: step 98030, loss = 0.77 (1619.8 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:46.995352: step 98040, loss = 0.81 (1600.7 examples/sec; 0.080 sec/batch)
2017-05-02 17:02:47.772767: step 98050, loss = 0.59 (1646.5 examples/sec; 0.078 sec/batch)
2017-05-02 17:02:48.557786: step 98060, loss = 0.74 (1630.5 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:49.353022: step 98070, loss = 0.67 (1609.6 examples/sec; 0.080 sec/batch)
2017-05-02 17:02:50.136936: step 98080, loss = 0.68 (1632.8 examples/sec; 0.078 sec/batch)
2017-05-02 17:02:50.927302: step 98090, loss = 0.68 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:51.807442: step 98100, loss = 0.75 (1454.3 examples/sec; 0.088 sec/batch)
2017-05-02 17:02:52.497404: step 98110, loss = 0.76 (1855.2 examples/sec; 0.069 sec/batch)
2017-05-02 17:02:53.292626: step 98120, loss = 0.66 (1609.6 examples/sec; 0.080 sec/batch)
2017-05-02 17:02:54.080331: step 98130, loss = 0.62 (1625.0 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:54.871980: step 98140, loss = 0.60 (1616.9 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:55.648231: step 98150, loss = 0.85 (1649.0 examples/sec; 0.078 sec/batch)
2017-05-02 17:02:56.439714: step 98160, loss = 0.83 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:57.232976: step 98170, loss = 0.59 (1613.6 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:58.021370: step 98180, loss = 0.71 (1623.6 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:58.811713: step 98190, loss = 0.85 (1619.5 examples/sec; 0.079 sec/batch)
2017-05-02 17:02:59.694501: step 98200, loss = 0.66 (1450.0 examples/sec; 0.088 sec/batch)
2017-05-02 17:03:00.388507: step 98210, loss = 0.71 (1844.3 examples/sec; 0.069 sec/batch)
2017-05-02 17:03:01.184027: step 98220, loss = 0.69 (1609.0 examples/sec; 0.080 sec/batch)
2017-05-02 17:03:01.967722: step 98230, loss = 0.72 (1633.3 examples/sec; 0.078 sec/batch)
2017-05-02 17:03:02.755572: step 98240, loss = 0.75 (1624.7 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:03.536406: step 98250, loss = 0.61 (1639.3 examples/sec; 0.078 sec/batch)
2017-05-02 17:03:04.327135: step 98260, loss = 0.86 (1618.8 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:05.121899: step 98270, loss = 0.67 (1610.6 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:05.912914: step 98280, loss = 0.66 (1618.1 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:06.704667: step 98290, loss = 0.67 (1616.7 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:07.586789: step 98300, loss = 0.63 (1451.0 examples/sec; 0.088 sec/batch)
2017-05-02 17:03:08.276833: step 98310, loss = 0.62 (1854.9 examples/sec; 0.069 sec/batch)
2017-05-02 17:03:09.064150: step 98320, loss = 0.74 (1625.8 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:09.857787: step 98330, loss = 0.76 (1612.8 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:10.646422: step 98340, loss = 0.74 (1623.1 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:11.434875: step 98350, loss = 0.65 (1623.4 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:12.220813: step 98360, loss = 0.66 (1628.6 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:13.015658: step 98370, loss = 0.69 (1610.4 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:13.802075: step 98380, loss = 0.60 (1627.6 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:14.586239: step 98390, loss = 0.60 (1632.3 examples/sec; 0.078 sec/batch)
2017-05-02 17:03:15.484932: step 98400, loss = 0.72 (1424.3 examples/sec; 0.090 sec/batch)
2017-05-02 17:03:16.155262: step 98410, loss = 0.64 (1909.5 examples/sec; 0.067 sec/batch)
2017-05-02 17:03:16.940845: step 98420, loss = 0.95 (1629.4 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:17.730758: step 98430, loss = 0.70 (1620.4 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:18.533080: step 98440, loss = 0.85 (1595.4 examples/sec; 0.080 sec/batch)
2017-05-02 17:03:19.321193: step 98450, loss = 0.66 (1624.1 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:20.102063: step 98460, loss = 0.68 (1639.2 examples/sec; 0.078 sec/batch)
2017-05-02 17:03:20.899315: step 98470, loss = 0.76 (1605.5 examples/sec; 0.080 sec/batch)
2017-05-02 17:03:21.696564: step 98480, loss = 0.64 (1605.5 examples/sec; 0.080 sec/batch)
2017-05-02 17:03:22.487698: step 98490, loss = 0.62 (1617.9 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:23.390694: step 98500, loss = 0.60 (1417.5 examples/sec; 0.090 sec/batch)
2017-05-02 17:03:24.056978: step 98510, loss = 0.72 (1921.1 examples/sec; 0.067 sec/batch)
2017-05-02 17:03:24.852832: step 98520, loss = 0.55 (1608.3 examples/sec; 0.080 sec/batch)
2017-05-02 17:03:25.642949: step 98530, loss = 0.62 (1620.0 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:26.439117: step 98540, loss = 0.85 (1607.7 examples/sec; 0.080 sec/batch)
2017-05-02 17:03:27.231489: step 98550, loss = 0.71 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:28.013843: step 98560, loss = 0.74 (1636.1 examples/sec; 0.078 sec/batch)
2017-05-02 17:03:28.804750: step 98570, loss = 0.69 (1618.4 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:29.593650: step 98580, loss = 0.70 (1622.5 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:30.444949: step 98590, loss = 0.67 (1503.6 examples/sec; 0.085 sec/batch)
2017-05-02 17:03:31.346568: step 98600, loss = 0.78 (1419.7 examples/sec; 0.090 sec/batch)
2017-05-02 17:03:32.011126: step 98610, loss = 0.78 (1926.1 examples/sec; 0.066 sec/batch)
2017-05-02 17:03:32.802864: step 98620, loss = 0.77 (1616.7 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:33.603168: step 98630, loss = 0.71 (1599.4 examples/sec; 0.080 sec/batch)
2017-05-02 17:03:34.390990: step 98640, loss = 0.72 (1624.7 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:35.185505: step 98650, loss = 0.75 (1611.1 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:35.969591: step 98660, loss = 0.69 (1632.5 examples/sec; 0.078 sec/batch)
2017-05-02 17:03:36.759489: step 98670, loss = 0.68 (1620.5 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:37.560322: step 98680, loss = 0.78 (1598.3 examples/sec; 0.080 sec/batch)
2017-05-02 17:03:38.355699: step 98690, loss = 0.70 (1609.3 examples/sec; 0.080 sec/batch)
2017-05-02 17:03:39.246352: step 98700, loss = 0.77 (1437.1 examples/sec; 0.089 sec/batch)
2017-05-02 17:03:39.925530: step 98710, loss = 0.63 (1884.6 examples/sec; 0.068 sec/batch)
2017-05-02 17:03:40.714427: step 98720, loss = 0.73 (1622.5 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:41.501134: step 98730, loss = 0.80 (1627.0 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:42.293704: step 98740, loss = 0.84 (1615.0 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:43.083159: step 98750, loss = 0.73 (1621.4 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:43.861318: step 98760, loss = 0.67 (1644.9 examples/sec; 0.078 sec/batch)
2017-05-02 17:03:44.656467: step 98770, loss = 0.64 (1609.8 examples/sec; 0.080 sec/batch)
2017-05-02 17:03:45.443485: step 98780, loss = 0.67 (1626.4 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:46.240043: step 98790, loss = 0.69 (1606.9 examples/sec; 0.080 sec/batch)
2017-05-02 17:03:47.128084: step 98800, loss = 0.61 (1441.4 examples/sec; 0.089 sec/batch)
2017-05-02 17:03:47.808916: step 98810, loss = 0.73 (1880.1 examples/sec; 0.068 sec/batch)
2017-05-02 17:03:48.599158: step 98820, loss = 0.70 (1619.8 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:49.393542: step 98830, loss = 0.76 (1611.3 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:50.179632: step 98840, loss = 0.66 (1628.3 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:50.970538: step 98850, loss = 0.62 (1618.4 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:51.755794: step 98860, loss = 0.75 (1630.0 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:52.555143: step 98870, loss = 0.87 (1601.3 examples/sec; 0.080 sec/batch)
2017-05-02 17:03:53.350510: step 98880, loss = 0.74 (1609.3 examples/sec; 0.080 sec/batch)
2017-05-02 17:03:54.139579: step 98890, loss = 0.72 (1622.2 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:55.027937: step 98900, loss = 0.77 (1440.9 examples/sec; 0.089 sec/batch)
2017-05-02 17:03:55.714645: step 98910, loss = 0.69 (1864.0 examples/sec; 0.069 sec/batch)
2017-05-02 17:03:56.506121: step 98920, loss = 0.79 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:57.295554: step 98930, loss = 0.78 (1621.4 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:58.085721: step 98940, loss = 0.72 (1619.9 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:58.871501: step 98950, loss = 0.73 (1628.9 examples/sec; 0.079 sec/batch)
2017-05-02 17:03:59.655740: step 98960, loss = 0.69 (1632.2 examples/sec; 0.078 sec/batch)
2017-05-02 17:04:00.445133: step 98970, loss = 0.82 (1621.5 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:01.238226: step 98980, loss = 0.65 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:02.029984: step 98990, loss = 0.64 (1616.7 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:02.921179: step 99000, loss = 0.68 (1436.3 examples/sec; 0.089 sec/batch)
2017-05-02 17:04:03.607863: step 99010, loss = 0.86 (1864.0 examples/sec; 0.069 sec/batch)
2017-05-02 17:04:04.393812: step 99020, loss = 0.67 (1628.6 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:05.189654: step 99030, loss = 0.69 (1608.4 examples/sec; 0.080 sec/batch)
2017-05-02 17:04:05.978127: step 99040, loss = 0.69 (1623.4 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:06.774147: step 99050, loss = 0.70 (1608.0 examples/sec; 0.080 sec/batch)
2017-05-02 17:04:07.555496: step 99060, loss = 0.88 (1638.2 examples/sec; 0.078 sec/batch)
2017-05-02 17:04:08.353978: step 99070, loss = 0.77 (1603.0 examples/sec; 0.080 sec/batch)
2017-05-02 17:04:09.145556: step 99080, loss = 0.77 (1617.0 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:09.950241: step 99090, loss = 0.77 (1590.7 examples/sec; 0.080 sec/batch)
2017-05-02 17:04:10.847375: step 99100, loss = 0.68 (1426.8 examples/sec; 0.090 sec/batch)
2017-05-02 17:04:11.529975: step 99110, loss = 0.82 (1875.2 examples/sec; 0.068 sec/batch)
2017-05-02 17:04:12.316741: step 99120, loss = 0.68 (1626.9 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:13.112795: step 99130, loss = 0.82 (1607.9 examples/sec; 0.080 sec/batch)
2017-05-02 17:04:13.912405: step 99140, loss = 0.70 (1600.8 examples/sec; 0.080 sec/batch)
2017-05-02 17:04:14.710344: step 99150, loss = 0.58 (1604.1 examples/sec; 0.080 sec/batch)
2017-05-02 17:04:15.493312: step 99160, loss = 0.78 (1634.8 examples/sec; 0.078 sec/batch)
2017-05-02 17:04:16.275827: step 99170, loss = 0.83 (1635.8 examples/sec; 0.078 sec/batch)
2017-05-02 17:04:17.069034: step 99180, loss = 0.63 (1613.7 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:17.860645: step 99190, loss = 0.79 (1617.0 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:18.767996: step 99200, loss = 0.74 (1410.7 examples/sec; 0.091 sec/batch)
2017-05-02 17:04:19.436270: step 99210, loss = 0.77 (1915.4 examples/sec; 0.067 sec/batch)
2017-05-02 17:04:20.221928: step 99220, loss = 0.74 (1629.2 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:21.013404: step 99230, loss = 0.73 (1617.2 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:21.807462: step 99240, loss = 0.72 (1612.0 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:22.603988: step 99250, loss = 0.72 (1607.0 examples/sec; 0.080 sec/batch)
2017-05-02 17:04:23.387440: step 99260, loss = 0.59 (1633.8 examples/sec; 0.078 sec/batch)
2017-05-02 17:04:24.169870: step 99270, loss = 0.71 (1635.9 examples/sec; 0.078 sec/batch)
2017-05-02 17:04:24.959803: step 99280, loss = 0.70 (1620.4 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:25.752569: step 99290, loss = 0.56 (1614.6 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:26.641236: step 99300, loss = 0.66 (1440.4 examples/sec; 0.089 sec/batch)
2017-05-02 17:04:27.336499: step 99310, loss = 0.72 (1841.1 examples/sec; 0.070 sec/batch)
2017-05-02 17:04:28.123299: step 99320, loss = 0.69 (1626.8 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:28.918536: step 99330, loss = 0.76 (1609.6 examples/sec; 0.080 sec/batch)
2017-05-02 17:04:29.706988: step 99340, loss = 0.65 (1623.4 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:30.495852: step 99350, loss = 0.70 (1622.6 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:31.293940: step 99360, loss = 0.70 (1603.8 examples/sec; 0.080 sec/batch)
2017-05-02 17:04:32.077781: step 99370, loss = 0.85 (1633.0 examples/sec; 0.078 sec/batch)
2017-05-02 17:04:32.870909: step 99380, loss = 0.64 (1613.9 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:33.657013: step 99390, loss = 0.75 (1628.3 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:34.561857: step 99400, loss = 0.72 (1414.6 examples/sec; 0.090 sec/batch)
2017-05-02 17:04:35.242748: step 99410, loss = 0.89 (1879.9 examples/sec; 0.068 sec/batch)
2017-05-02 17:04:36.028168: step 99420, loss = 0.74 (1629.7 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:36.819683: step 99430, loss = 0.77 (1617.1 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:37.607731: step 99440, loss = 0.87 (1624.3 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:38.412824: step 99450, loss = 0.63 (1589.9 examples/sec; 0.081 sec/batch)
2017-05-02 17:04:39.204018: step 99460, loss = 0.63 (1617.8 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:39.985219: step 99470, loss = 0.79 (1638.5 examples/sec; 0.078 sec/batch)
2017-05-02 17:04:40.781557: step 99480, loss = 0.70 (1607.3 examples/sec; 0.080 sec/batch)
2017-05-02 17:04:41.571262: step 99490, loss = 0.66 (1620.9 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:42.462588: step 99500, loss = 0.67 (1436.1 examples/sec; 0.089 sec/batch)
2017-05-02 17:04:43.160718: step 99510, loss = 0.63 (1833.5 examples/sec; 0.070 sec/batch)
2017-05-02 17:04:43.942655: step 99520, loss = 0.77 (1637.0 examples/sec; 0.078 sec/batch)
2017-05-02 17:04:44.736645: step 99530, loss = 0.62 (1612.1 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:45.529085: step 99540, loss = 0.64 (1615.3 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:46.322774: step 99550, loss = 0.59 (1612.7 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:47.109680: step 99560, loss = 0.57 (1626.6 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:47.908693: step 99570, loss = 0.62 (1602.0 examples/sec; 0.080 sec/batch)
2017-05-02 17:04:48.710405: step 99580, loss = 0.74 (1596.6 examples/sec; 0.080 sec/batch)
2017-05-02 17:04:49.504787: step 99590, loss = 0.66 (1611.3 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:50.398414: step 99600, loss = 0.60 (1432.4 examples/sec; 0.089 sec/batch)
2017-05-02 17:04:51.100146: step 99610, loss = 0.60 (1824.1 examples/sec; 0.070 sec/batch)
2017-05-02 17:04:51.875381: step 99620, loss = 0.72 (1651.1 examples/sec; 0.078 sec/batch)
2017-05-02 17:04:52.670446: step 99630, loss = 0.59 (1609.9 examples/sec; 0.080 sec/batch)
2017-05-02 17:04:53.471162: step 99640, loss = 0.58 (1598.6 examples/sec; 0.080 sec/batch)
2017-05-02 17:04:54.262278: step 99650, loss = 0.75 (1618.0 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:55.053059: step 99660, loss = 0.71 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:55.832184: step 99670, loss = 0.70 (1642.9 examples/sec; 0.078 sec/batch)
2017-05-02 17:04:56.625449: step 99680, loss = 0.54 (1613.6 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:57.419187: step 99690, loss = 0.80 (1612.6 examples/sec; 0.079 sec/batch)
2017-05-02 17:04:58.309900: step 99700, loss = 0.60 (1437.1 examples/sec; 0.089 sec/batch)
2017-05-02 17:04:58.999849: step 99710, loss = 0.86 (1855.2 examples/sec; 0.069 sec/batch)
2017-05-02 17:04:59.785420: step 99720, loss = 0.74 (1629.4 examples/sec; 0.079 sec/batch)
2017-05-02 17:05:00.577397: step 99730, loss = 0.68 (1616.2 examples/sec; 0.079 sec/batch)
2017-05-02 17:05:01.371643: step 99740, loss = 0.79 (1611.6 examples/sec; 0.079 sec/batch)
2017-05-02 17:05:02.172105: step 99750, loss = 0.77 (1599.1 examples/sec; 0.080 sec/batch)
2017-05-02 17:05:02.964992: step 99760, loss = 0.51 (1614.4 examples/sec; 0.079 sec/batch)
2017-05-02 17:05:03.747072: step 99770, loss = 0.86 (1636.7 examples/sec; 0.078 sec/batch)
2017-05-02 17:05:04.537233: step 99780, loss = 0.62 (1619.9 examples/sec; 0.079 sec/batch)
2017-05-02 17:05:05.332149: step 99790, loss = 0.65 (1610.2 examples/sec; 0.079 sec/batch)
2017-05-02 17:05:06.246681: step 99800, loss = 0.61 (1399.6 examples/sec; 0.091 sec/batch)
2017-05-02 17:05:06.922478: step 99810, loss = 0.73 (1894.1 examples/sec; 0.068 sec/batch)
2017-05-02 17:05:07.707066: step 99820, loss = 0.67 (1631.4 examples/sec; 0.078 sec/batch)
2017-05-02 17:05:08.491813: step 99830, loss = 0.76 (1631.1 examples/sec; 0.078 sec/batch)
2017-05-02 17:05:09.279695: step 99840, loss = 0.58 (1624.6 examples/sec; 0.079 sec/batch)
2017-05-02 17:05:10.069732: step 99850, loss = 0.79 (1620.2 examples/sec; 0.079 sec/batch)
2017-05-02 17:05:10.870628: step 99860, loss = 0.63 (1598.2 examples/sec; 0.080 sec/batch)
2017-05-02 17:05:11.648309: step 99870, loss = 0.74 (1645.9 examples/sec; 0.078 sec/batch)
2017-05-02 17:05:12.443859: step 99880, loss = 0.74 (1608.9 examples/sec; 0.080 sec/batch)
2017-05-02 17:05:13.235961: step 99890, loss = 0.81 (1616.0 examples/sec; 0.079 sec/batch)
2017-05-02 17:05:14.119749: step 99900, loss = 0.72 (1448.3 examples/sec; 0.088 sec/batch)
2017-05-02 17:05:14.819092: step 99910, loss = 0.63 (1830.3 examples/sec; 0.070 sec/batch)
2017-05-02 17:05:15.609162: step 99920, loss = 0.76 (1620.1 examples/sec; 0.079 sec/batch)
2017-05-02 17:05:16.398786: step 99930, loss = 0.72 (1621.0 examples/sec; 0.079 sec/batch)
2017-05-02 17:05:17.189093: step 99940, loss = 0.68 (1619.6 examples/sec; 0.079 sec/batch)
2017-05-02 17:05:17.983489: step 99950, loss = 0.63 (1611.3 examples/sec; 0.079 sec/batch)
2017-05-02 17:05:18.768664: step 99960, loss = 0.68 (1630.2 examples/sec; 0.079 sec/batch)
2017-05-02 17:05:19.557772: step 99970, loss = 0.73 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-02 17:05:20.342558: step 99980, loss = 0.60 (1631.0 examples/sec; 0.078 sec/batch)
2017-05-02 17:05:21.140510: step 99990, loss = 0.67 (1604.1 examples/sec; 0.080 sec/batch)
2017-05-02 17:05:22.035791: step 100000, loss = 0.66 (1429.7 examples/sec; 0.090 sec/batch)
--- 7925.57946777 seconds ---
