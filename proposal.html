

    <section class="main-content">
      <h2>
<a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a><b>Proposal</b></h2>

<h3>Summary</h3>
 

  

<p>  We are going to implement a distributed training algorithm for high-accuracy deep neural networks using OpenCL/Cuda on a cluster of nodes using a parameter-server/reduction-tree architecture</p>

<h3>
<a id="designer-templates" class="anchor" href="#designer-templates" aria-hidden="true"><span class="octicon octicon-link"></span></a>Background</h3>
<p>Neural networks are a set of algorithms, modeled loosely after the human brain, that are designed to recognize patterns. They interpret sensory data through a kind of machine perception,
    labeling or clustering raw input. The patterns they recognize are numerical, contained in vectors, into which all real-world data, be it images, sound, text or time series, must be translated.
    Deep neural networks have more than a single hidden layer between the input and the output layer. Training the neural network is done through gradient descent and backpropagation. Deep neural networks
    with multiple convolution layers and followed by fully connected layers takes a long time to train due to the varying learning rates. To achieve high accuracy, the amount of data fed into these networks
    is extremely high, making the time for each epoch high. Training AlexNet on a single GPU(NVIDIA K20) takes about 100 epochs (6 days). Long training times for high-accuracy deep neural networks (DNNs) 
    impede research into new DNN architectures and slow the development of high-accuracy DNNs. Hence, we would like to explore the use of a cluster of GPU machines to accelerate the learning. 
    The speed and scalability of distributed algorithms is almost always limited by the overhead of communicating between servers; DNN training is not an exception to this rule.
    Therefore, the key consideration here is to reduce communication overhead wherever possible, while not degrading the accuracy of the DNN models that we train. Our approach has three key pillars.</p>


<h3>
<a id="rather-drive-stick" class="anchor" href="#rather-drive-stick" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Challenge</h3>
<ul style="list-style: none;">
  <li><h4>Problem complexity</h4>
    <p>SLAM algorithms are tailored to the available resources, and hence are not aimed at perfection, but at operational compliance. SLAM will always use several different types of sensors, and the powers and limits of various sensor types have been a major driver of new algorithms. Researchers and experts in artificial intelligence have struggled to solve the SLAM problem in practical settings: that is, it required a great deal of computational power to sense a sizable area and process the resulting data to both map and localize.  Almost all the current approaches cannot generate maps for large areas, mainly due to the increase of the computational cost and due to the uncertainties that become prohibitive when the scenario becomes larger 
    </p></li>
  <li><h4>Choice of camera</h4> 
  <p>RGB-D cameras provide both color images and per pixel depth estimates.  A modern widely used RGB-D sensor is the Kinect. Since the Kinect is not space rated, we have to use an alternate RGB-D sensor. The CubeRover has size and power limitations, due to which, a single monocular camera is prefered. In the future, the CubeRover team has tentative plans to mount a Lidar,  that will provide depth information. In the meantime, we only have a monocular camera to work with. Depth estimation with monocular vision is a project in itself. We plan to use depth maps generated from a Kinect to complete the localization and loop closure systems, and if time permits, address the issue of generating depth maps from a monocular camera. </p></li>
 <li><h4>Dense vs sparse SLAM</h4> 
 <p>At the highest level, there are two variations of SLAM. One involves extracting fixed features from each image frame, and comparing two images using these features (this is sparse SLAM). This is prone to errors mainly due to sparse, self similar features on the moon. The other variation uses all the pixels in two image frames when comparing them (this is dense SLAM). Although it has been shown (confirming suspicions) that dense SLAM is more accurate than sparse SLAM [<a href="#ref">3</a>] [<a href="#ref">4</a>], the dense SLAM problem is computationally much more expensive than the sparse version. For robotics applications, especially ones as sensitive as moon missions, it is imperative to have algorithms that are not only accurate, but also run in real-time.
</p></li>
</ul>

<h3>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span class="octicon octicon-link"></span></a>Scope for Parallelism</h3>

<ul> 
  <li> A lot of computation that is done on the images is inherently parallelizable, because they either involve operations at the pixel level, or they involve linear algebra operations such as matrix and vector arithmetic.</li>
  <li> The sections of the code for which parallelism is not trivial are those which use iterative algorithms to estimate certain parameters; being iterative, they aren’t intuitively parallelizable. A lot of research and thought has gone into this problem,[<a href="#ref">2</a>] and there are several papers which offer novel suggestions to best exploit avenues for parallelism.</li>
</ul>
In attempting to parallelize parts of SLAM, different problems will only become apparent when we try different approaches. Therefore it’s hard to comment on that aspect of the project at this stage

<!--<p>You can <a href="https://github.com/blog/821" class="user-mention">@mention</a> a GitHub username to generate a link to their profile. The resulting <code>&lt;a&gt;</code> element will link to the contributor's GitHub Profile. For example: In 2007, Chris Wanstrath (<a href="https://github.com/defunkt" class="user-mention">@defunkt</a>), PJ Hyett (<a href="https://github.com/pjhyett" class="user-mention">@pjhyett</a>), and Tom Preston-Werner (<a href="https://github.com/mojombo" class="user-mention">@mojombo</a>) founded GitHub.</p> -->

<h3>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span class="octicon octicon-link"></span></a>Evaluation</h3>
<ul style="list-style: none;">
<li><h4>Energy efficiency</h4> 
  <p>While working with the CubeRover, we need to keep in mind that its entire “mission” is to be done on a single charge. The Tegra K1 is a complete system-on-a-chip. In addition to the 192 CUDA cores, the K1 also has a quad-core ARM-Cortex CPU with SIMD. We think it would be a good idea to compare the performance and energy efficiency of parallelizing the system on the CPU cores and the GPU cores.
  </p></li>
<li><h4>Performance evaluation</h4> 
  <p>We will use <a target="_blank" href ="http://vision.in.tum.de/data/datasets/rgbd-dataset">TU Munich’s RGB-D SLAM benchmark</a> to evaluate the correctness of our tracking. <a target="_blank" href="https://github.com/tum-vision/dvo">TU Munich’s Dense Visual SLAM</a> is a state-of-the-art implementation of SLAM. It has been integrated into Robot Operating System (ROS) software libraries. It is open source, and we intend to benchmark our speed against it.
  </p></li>
</ul>

<h3>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span class="octicon octicon-link"></span></a>Stretch Goal</h3>
<p>As mentioned earlier, depth map estimation is a non-trivial problem [<a href="#ref">5</a>] [<a href="#ref">1</a>]. In the absence of an RGB-D sensor in the CubeRover, we would have to generate depth maps using monocular camera only. Our stretch goal would be to implement a depth map estimation algorithm and GPU/CPU parallelize it.</p>

<h3>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span class="octicon octicon-link"></span></a>Schedule</h3>

<table class="pure-table pure-table-horizontal">
    <thead>
        <tr bgcolor="#f0f0f0">
            <th>#</th>
            <th>Date</th>
            <th>Objective</th>
            <th>Description</th>
        </tr>
    </thead>

    <tbody>
        <tr>
            <td>1</td>
            <td nowrap= "nowrap">April 4</td>
            <td>Setup the working environment </td>
            <td>Set up the tegra and kinect. Install the required software packages</td>
        </tr>

        <tr bgcolor="#f0f0f0">
            <td>2</td>
            <td nowrap= "nowrap">April 6</td>
            <td>Initial testing of the benchmarked software</td>
            <td></td>
        </tr>

        <tr>
            <td>3*</td>
            <td nowrap="nowrap">April 20</td>
            <td>Visual Odometry GPU Implementation</td>
            <td></td>
        </tr>

        <tr bgcolor="#f0f0f0">
            <td>4*</td>
            <td nowrap="nowrap">April 25</td>
            <td>Visual Odometry CPU Implementation</td>
            <td></td>
        </tr>

        <tr>
            <td>5</td>
            <td nowrap="nowrap">April 26 - May 1</td>
            <td>Exam Preparation</td>
            <td></td>
        </tr>

        <tr bgcolor="#f0f0f0">
            <td>6</td>
            <td nowrap="nowrap">May 2 - May 5</td>
            <td>Integration</td>
            <td>Integrating all the modules and testing for correctness and performance. Also, testing on different test data sets</td>
        </tr>

        <tr>
            <td>7</td>
            <td nowrap="nowrap">May 6 - May 7</td>
            <td>Test for energy consumption</td>
            <td></td>
        </tr>

        <tr bgcolor="#f0f0f0">
            <td>8</td>
            <td nowrap="nowrap">May 7 - May 11</td>
            <td>Final Product</td>
            <td>Final report, presentation and an attempt at the stretch goal</td>
        </tr>
    </tbody>
</table>
<font size = "3"> <i> *Revised schedule</i></font>

<h3>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span class="octicon octicon-link"></span></a>References<a name="ref"></a></h3>
<ol type="1">
<li>Engel, J., Sturm, J., Cremers, D.: Semi-dense visual odometry for a monocular camera. In: Intl. Conf. on Computer Vision (ICCV) (2013)</li>
<li>T. Whelan, H. Johannsson, M. Kaess, J. Leonard, and J. McDonald, "Robust real-time visual odometry for dense RGB-D mapping," in IEEE Intl. Conf. on Robotics and Automation (ICRA), Karlsruhe, Germany, 2013.</li>
<li>F. Steinbrücker, J. Sturm, and D. Cremers, "Real-time visual odometry from dense RGB-D images," in ICCV Workshop on Live Dense Reconstruction with Moving Cameras, 2011</li>
<li>Kerl, C., Sturm, J., Cremers, D.: Dense visual SLAM for RGB-D cameras. In: Intl. Conf. on Intelligent Robot Systems (IROS) (2013)</li>
<li>J. Engel, T. Scho ̈ps, and D. Cremers. LSD-SLAM: Large-scale direct monocular SLAM. In European Conference on Computer Vision (ECCV), September 2014.</li>
</ol>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/uprasad/CubeRover">CubeRover</a> is maintained by <a href="https://github.com/uprasad">uprasad</a> & <a href="https://github.com/nikithashr">nikithashr</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>
