I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.7335
pciBusID 0000:04:00.0
Total memory: 7.92GiB
Free memory: 5.29GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x30e5140
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: 
name: Quadro K620
major: 5 minor: 0 memoryClockRate (GHz) 1.124
pciBusID 0000:03:00.0
Total memory: 1.95GiB
Free memory: 1.85GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:04:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:962] Ignoring gpu device (device: 1, name: Quadro K620, pci bus id: 0000:03:00.0) with Cuda multiprocessor count: 3. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.
Connecting to port  16001
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Received variable values from ps
2017-05-07 19:15:53.790054: step 0, loss = 4.67 (77.4 examples/sec; 1.654 sec/batch)
2017-05-07 19:15:56.355813: step 10, loss = 4.55 (498.9 examples/sec; 0.257 sec/batch)
2017-05-07 19:15:57.695612: step 20, loss = 4.53 (955.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:15:59.049578: step 30, loss = 4.48 (945.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:16:00.411930: step 40, loss = 4.29 (939.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:16:01.780268: step 50, loss = 4.36 (935.4 examples/sec; 0.137 sec/batch)
2017-05-07 19:16:03.112255: step 60, loss = 4.71 (961.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:16:04.460181: step 70, loss = 3.95 (949.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:16:05.800256: step 80, loss = 3.94 (955.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:16:07.143946: step 90, loss = 4.19 (952.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:16:08.597170: step 100, loss = 3.81 (880.8 examples/sec; 0.145 sec/batch)
2017-05-07 19:16:09.811024: step 110, loss = 3.74 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-07 19:16:11.119714: step 120, loss = 3.79 (978.1 examples/sec; 0.131 sec/batch)
2017-05-07 19:16:12.441974: step 130, loss = 3.80 (968.0 examples/sec; 0.132 sec/batch)
2017-05-07 19:16:13.788386: step 140, loss = 3.71 (950.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:16:15.130721: step 150, loss = 3.80 (953.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:16:16.482338: step 160, loss = 3.72 (947.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:16:17.810596: step 170, loss = 3.46 (963.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:16:19.156817: step 180, loss = 3.61 (950.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:16:20.509206: step 190, loss = 3.62 (946.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:16:21.961977: step 200, loss = 3.69 (881.1 examples/sec; 0.145 sec/batch)
2017-05-07 19:16:23.207554: step 210, loss = 3.34 (1027.6 examples/sec; 0.125 sec/batch)
2017-05-07 19:16:24.557722: step 220, loss = 3.39 (948.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:16:25.898904: step 230, loss = 3.68 (954.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:16:27.219793: step 240, loss = 3.37 (969.0 examples/sec; 0.132 sec/batch)
2017-05-07 19:16:28.569551: step 250, loss = 3.22 (948.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:16:29.912660: step 260, loss = 3.20 (953.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:16:31.265971: step 270, loss = 3.27 (945.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:16:32.608204: step 280, loss = 3.05 (953.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:16:33.930215: step 290, loss = 3.11 (968.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:16:35.389202: step 300, loss = 3.24 (877.3 examples/sec; 0.146 sec/batch)
2017-05-07 19:16:36.621738: step 310, loss = 3.27 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-07 19:16:37.963348: step 320, loss = 3.05 (954.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:16:39.316476: step 330, loss = 3.09 (946.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:16:40.672924: step 340, loss = 2.86 (943.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:16:42.025719: step 350, loss = 3.03 (946.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:16:43.386904: step 360, loss = 2.86 (940.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:16:44.704539: step 370, loss = 2.81 (971.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:16:46.065166: step 380, loss = 2.98 (940.7 examples/sec; 0.136 sec/batch)
2017-05-07 19:16:47.408748: step 390, loss = 2.83 (952.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:16:48.858016: step 400, loss = 3.51 (883.2 examples/sec; 0.145 sec/batch)
2017-05-07 19:16:50.120854: step 410, loss = 2.86 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-07 19:16:51.449756: step 420, loss = 2.61 (963.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:16:52.770600: step 430, loss = 2.99 (969.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:16:54.108706: step 440, loss = 2.68 (956.6 examples/sec; 0.134 seE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 21 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
c/batch)
2017-05-07 19:16:55.459120: step 450, loss = 2.52 (947.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:16:56.794108: step 460, loss = 2.80 (958.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:16:58.155745: step 470, loss = 2.57 (940.0 examples/sec; 0.136 sec/batch)
2017-05-07 19:16:59.468671: step 480, loss = 2.73 (974.9 examples/sec; 0.131 sec/batch)
2017-05-07 19:17:00.810955: step 490, loss = 2.68 (953.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:17:02.281658: step 500, loss = 2.50 (870.3 examples/sec; 0.147 sec/batch)
2017-05-07 19:17:03.522100: step 510, loss = 2.61 (1031.9 examples/sec; 0.124 sec/batch)
2017-05-07 19:17:04.869573: step 520, loss = 2.44 (949.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:17:06.217389: step 530, loss = 2.61 (949.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:17:07.572515: step 540, loss = 2.24 (944.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:17:08.898543: step 550, loss = 2.41 (965.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:17:10.258600: step 560, loss = 2.56 (941.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:17:11.597921: step 570, loss = 2.55 (955.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:17:12.950508: step 580, loss = 2.56 (946.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:17:14.293017: step 590, loss = 2.28 (953.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:17:15.738780: step 600, loss = 2.39 (885.4 examples/sec; 0.145 sec/batch)
2017-05-07 19:17:16.949263: step 610, loss = 2.39 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-07 19:17:18.310045: step 620, loss = 2.33 (940.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:17:19.651707: step 630, loss = 2.32 (954.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:17:20.992137: step 640, loss = 2.40 (954.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:17:22.363762: step 650, loss = 2.19 (933.2 examples/sec; 0.137 sec/batch)
2017-05-07 19:17:23.703845: step 660, loss = 2.36 (955.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:17:25.052925: step 670, loss = 2.06 (948.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:17:26.400551: step 680, loss = 2.20 (949.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:17:27.740685: step 690, loss = 2.39 (955.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:17:29.189900: step 700, loss = 2.19 (883.2 examples/sec; 0.145 sec/batch)
2017-05-07 19:17:30.440793: step 710, loss = 2.09 (1023.3 examples/sec; 0.125 sec/batch)
2017-05-07 19:17:31.760059: step 720, loss = 2.09 (970.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:17:33.096612: step 730, loss = 2.14 (957.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:17:34.442293: step 740, loss = 2.42 (951.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:17:35.797198: step 750, loss = 2.03 (944.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:17:37.126063: step 760, loss = 2.16 (963.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:17:38.461427: step 770, loss = 2.04 (958.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:17:39.819560: step 780, loss = 2.05 (942.5 examples/sec; 0.136 sec/batch)
2017-05-07 19:17:41.155554: step 790, loss = 2.10 (958.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:17:42.604707: step 800, loss = 2.01 (883.3 examples/sec; 0.145 sec/batch)
2017-05-07 19:17:43.861128: step 810, loss = 1.95 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-07 19:17:45.234078: step 820, loss = 2.06 (932.3 examples/sec; 0.137 sec/batch)
2017-05-07 19:17:46.565910: step 830, loss = 2.27 (961.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:17:47.904104: step 840, loss = 1.80 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:17:49.242201: step 850, loss = 1.96 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:17:50.576926: step 860, loss = 1.98 (959.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:17:51.947187: step 870, loss = 2.07 (934.1 examples/sec; 0.137 sec/batch)
2017-05-07 19:17:53.280591: step 880, loss = 1.86 (959.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:17:54.637598: step 890, loss = 1.79 (943.3 examples/sec; 0.136 sec/batch)
2017-05-07 19:17:56.162478: step 900, loss = 2.10 (839.4 examples/sec; 0.152 sec/batch)
2017-05-07 19:17:57.313668: step 910, loss = 1.88 (1111.9 examples/sec; 0.115 sec/batch)
2017-05-07 19:17:58.668834: step 920, loss = 2.12 (944.5 examples/sec; 0.136 sec/batch)
2017-05-07 19:18:00.004552: step 930, loss = 2.35 (958.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:18:01.350705: step 940, loss = 1.77 (950.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:18:02.690040: step 950, loss = 1.82 (955.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:18:04.003640: step 960, loss = 1.58 (974.4 examples/sec; 0.131 sec/batch)
2017-05-07 19:18:05.330412: step 970, loss = 1.96 (964.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:18:06.682880: step 980, loss = 1.81 (946.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:18:08.040218: step 990, loss = 1.73 (943.0 examples/sec; 0.136 sec/batch)
2017-05-07 19:18:09.498803: step 1000, loss = 1.54 (877.6 examples/sec; 0.146 sec/batch)
2017-05-07 19:18:10.760758: step 1010, loss = 1.69 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-07 19:18:12.106936: step 1020, loss = 1.67 (950.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:18:13.458027: step 1030, loss = 1.75 (947.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:18:14.778044: step 1040, loss = 1.54 (969.7 examples/sec; 0.132 sec/batch)
2017-05-07 19:18:16.152557: step 1050, loss = 1.64 (931.2 examples/sec; 0.137 sec/batch)
2017-05-07 19:18:17.470207: step 1060, loss = 1.78 (971.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:18:18.862196: step 1070, loss = 1.69 (919.5 examples/sec; 0.139 sec/batch)
2017-05-07 19:18:20.190218: step 1080, loss = 1.68 (963.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:18:21.513845: step 1090, loss = 1.46 (967.0 examples/sec; 0.132 sec/batch)
2017-05-07 19:18:22.964817: step 1100, loss = 1.61 (882.2 examples/sec; 0.145 sec/batch)
2017-05-07 19:18:24.240722: step 1110, loss = 1.72 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-07 19:18:25.590659: step 1120, loss = 1.74 (948.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:18:26.919982: step 1130, loss = 1.59 (962.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:18:28.256577: step 1140, loss = 1.59 (957.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:18:29.608571: step 1150, loss = 1.45 (946.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:18:30.949392: step 1160, loss = 1.61 (954.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:18:32.292303: step 1170, loss = 1.63 (953.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:18:33.645166: step 1180, loss = 1.42 (946.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:18:34.979127: step 1190, loss = 1.61 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:18:36.431109: step 1200, loss = 1.53 (881.6 examples/sec; 0.145 sec/batch)
2017-05-07 19:18:37.728783: step 1210, loss = 1.62 (986.4 examples/sec; 0.130 sec/batch)
2017-05-07 19:18:39.069417: step 1220, loss = 1.91 (954.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:18:40.410192: step 1230, loss = 1.49 (954.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:18:41.767076: step 1240, loss = 1.50 (943.3 examples/sec; 0.136 sec/batch)
2017-05-07 19:18:43.130547: step 1250, loss = 1.42 (938.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:18:44.490840: step 1260, loss = 1.43 (941.0 examples/sec; 0.136 sec/batch)
2017-05-07 19:18:45.820013: step 1270, loss = 1.47 (963.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:18:47.181553: step 1280, loss = 1.40 (940.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:18:48.524838: step 1290, loss = 1.66 (952.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:18:49.982922: step 1300, loss = 1.52 (877.9 examples/sec; 0.146 sec/batch)
2017-05-07 19:18:51.258725: step 1310, loss = 1.56 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-07 19:18:52.594734: step 1320, loss = 1.48 (958.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:18:53.947018: step 1330, loss = 1.41 (946.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:18:55.290928: step 1340, loss = 1.60 (952.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:18:56.633425: step 1350, loss = 1.56 (953.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:18:57.970399: step 1360, loss = 1.62 (957.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:18:59.333797: step 1370, losE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 39 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
s = 1.49 (938.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:19:00.669554: step 1380, loss = 1.50 (958.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:19:02.010612: step 1390, loss = 1.31 (954.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:19:03.451434: step 1400, loss = 1.27 (888.4 examples/sec; 0.144 sec/batch)
2017-05-07 19:19:04.687175: step 1410, loss = 1.41 (1035.8 examples/sec; 0.124 sec/batch)
2017-05-07 19:19:06.059038: step 1420, loss = 1.50 (933.0 examples/sec; 0.137 sec/batch)
2017-05-07 19:19:07.399521: step 1430, loss = 1.23 (954.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:19:08.722147: step 1440, loss = 1.71 (967.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:19:10.097257: step 1450, loss = 1.41 (930.8 examples/sec; 0.138 sec/batch)
2017-05-07 19:19:11.436543: step 1460, loss = 1.44 (955.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:19:12.782991: step 1470, loss = 1.45 (950.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:19:14.122953: step 1480, loss = 1.34 (955.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:19:15.485310: step 1490, loss = 1.47 (939.5 examples/sec; 0.136 sec/batch)
2017-05-07 19:19:16.947618: step 1500, loss = 1.57 (875.3 examples/sec; 0.146 sec/batch)
2017-05-07 19:19:18.156222: step 1510, loss = 1.37 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-07 19:19:19.501123: step 1520, loss = 1.27 (951.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:19:20.840527: step 1530, loss = 1.38 (955.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:19:22.200824: step 1540, loss = 1.41 (941.0 examples/sec; 0.136 sec/batch)
2017-05-07 19:19:23.546836: step 1550, loss = 1.40 (951.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:19:24.898799: step 1560, loss = 1.50 (946.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:19:26.216306: step 1570, loss = 1.48 (971.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:19:27.549948: step 1580, loss = 1.24 (959.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:19:28.890505: step 1590, loss = 1.51 (954.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:19:30.350381: step 1600, loss = 1.54 (876.8 examples/sec; 0.146 sec/batch)
2017-05-07 19:19:31.598990: step 1610, loss = 1.23 (1025.1 examples/sec; 0.125 sec/batch)
2017-05-07 19:19:32.921743: step 1620, loss = 1.32 (967.7 examples/sec; 0.132 sec/batch)
2017-05-07 19:19:34.270326: step 1630, loss = 1.23 (949.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:19:35.620073: step 1640, loss = 1.18 (948.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:19:36.964429: step 1650, loss = 1.54 (952.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:19:38.329518: step 1660, loss = 1.37 (937.7 examples/sec; 0.137 sec/batch)
2017-05-07 19:19:39.699003: step 1670, loss = 1.62 (934.7 examples/sec; 0.137 sec/batch)
2017-05-07 19:19:41.054238: step 1680, loss = 1.17 (944.5 examples/sec; 0.136 sec/batch)
2017-05-07 19:19:42.392884: step 1690, loss = 1.39 (956.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:19:43.849335: step 1700, loss = 1.38 (878.8 examples/sec; 0.146 sec/batch)
2017-05-07 19:19:45.100158: step 1710, loss = 1.28 (1023.3 examples/sec; 0.125 sec/batch)
2017-05-07 19:19:46.441845: step 1720, loss = 1.43 (954.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:19:47.785652: step 1730, loss = 1.24 (952.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:19:49.123388: step 1740, loss = 1.42 (956.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:19:50.491833: step 1750, loss = 1.22 (935.4 examples/sec; 0.137 sec/batch)
2017-05-07 19:19:51.815485: step 1760, loss = 1.17 (967.0 examples/sec; 0.132 sec/batch)
2017-05-07 19:19:53.164397: step 1770, loss = 1.19 (948.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:19:54.493283: step 1780, loss = 1.24 (963.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:19:55.848177: step 1790, loss = 1.26 (944.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:19:57.303956: step 1800, loss = 1.37 (879.3 examples/sec; 0.146 sec/batch)
2017-05-07 19:19:58.564503: step 1810, loss = 1.21 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-07 19:19:59.916180: step 1820, loss = 1.42 (947.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:20:01.243940: step 1830, loss = 1.49 (964.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:20:02.579321: step 1840, loss = 1.29 (958.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:20:03.950292: step 1850, loss = 1.29 (933.6 examples/sec; 0.137 sec/batch)
2017-05-07 19:20:05.285335: step 1860, loss = 1.25 (958.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:20:06.640092: step 1870, loss = 1.17 (944.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:20:07.978121: step 1880, loss = 1.27 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:20:09.326594: step 1890, loss = 1.21 (949.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:20:10.764516: step 1900, loss = 1.21 (890.2 examples/sec; 0.144 sec/batch)
2017-05-07 19:20:11.999117: step 1910, loss = 1.21 (1036.8 examples/sec; 0.123 sec/batch)
2017-05-07 19:20:13.364319: step 1920, loss = 1.16 (937.6 examples/sec; 0.137 sec/batch)
2017-05-07 19:20:14.738053: step 1930, loss = 1.21 (931.8 examples/sec; 0.137 sec/batch)
2017-05-07 19:20:16.103366: step 1940, loss = 1.16 (937.5 examples/sec; 0.137 sec/batch)
2017-05-07 19:20:17.460715: step 1950, loss = 1.12 (943.0 examples/sec; 0.136 sec/batch)
2017-05-07 19:20:18.812903: step 1960, loss = 1.15 (946.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:20:20.152262: step 1970, loss = 1.34 (955.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:20:21.484743: step 1980, loss = 1.41 (960.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:20:22.842337: step 1990, loss = 1.17 (942.9 examples/sec; 0.136 sec/batch)
2017-05-07 19:20:24.314087: step 2000, loss = 1.19 (869.7 examples/sec; 0.147 sec/batch)
2017-05-07 19:20:25.562110: step 2010, loss = 1.09 (1025.6 examples/sec; 0.125 sec/batch)
2017-05-07 19:20:26.931810: step 2020, loss = 1.23 (934.5 examples/sec; 0.137 sec/batch)
2017-05-07 19:20:28.344772: step 2030, loss = 1.02 (905.9 examples/sec; 0.141 sec/batch)
2017-05-07 19:20:29.710548: step 2040, loss = 1.18 (937.2 examples/sec; 0.137 sec/batch)
2017-05-07 19:20:31.086961: step 2050, loss = 1.20 (930.0 examples/sec; 0.138 sec/batch)
2017-05-07 19:20:32.503833: step 2060, loss = 1.13 (903.4 examples/sec; 0.142 sec/batch)
2017-05-07 19:20:33.847649: step 2070, loss = 1.07 (952.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:20:35.295035: step 2080, loss = 1.25 (884.4 examples/sec; 0.145 sec/batch)
2017-05-07 19:20:36.706550: step 2090, loss = 1.11 (906.8 examples/sec; 0.141 sec/batch)
2017-05-07 19:20:38.142842: step 2100, loss = 1.24 (891.2 examples/sec; 0.144 sec/batch)
2017-05-07 19:20:39.398181: step 2110, loss = 1.18 (1019.6 examples/sec; 0.126 sec/batch)
2017-05-07 19:20:40.776378: step 2120, loss = 1.07 (928.8 examples/sec; 0.138 sec/batch)
2017-05-07 19:20:42.187921: step 2130, loss = 1.25 (906.8 examples/sec; 0.141 sec/batch)
2017-05-07 19:20:43.536047: step 2140, loss = 1.05 (949.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:20:44.897192: step 2150, loss = 1.26 (940.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:20:46.264669: step 2160, loss = 1.05 (936.0 examples/sec; 0.137 sec/batch)
2017-05-07 19:20:47.611038: step 2170, loss = 1.18 (950.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:20:48.948426: step 2180, loss = 1.10 (957.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:20:50.301996: step 2190, loss = 1.44 (945.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:20:51.771140: step 2200, loss = 1.34 (871.3 examples/sec; 0.147 sec/batch)
2017-05-07 19:20:52.997247: step 2210, loss = 1.16 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-07 19:20:54.340433: step 2220, loss = 1.29 (953.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:20:55.669375: step 2230, loss = 1.08 (963.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:20:57.014118: step 2240, loss = 1.23 (951.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:20:58.379026: step 2250, loss = 1.04 (937.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:20:59.757082: step 2260, loss = 1.27 (928.9 examples/sec; 0.138 sec/batch)
2017-05-07 19:21:01.077393: step 2270, loss = 1.28 (969.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:21:02.429088: step 2280, loss = 1.31 (947.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:21:03.780912: step 2290E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 57 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
, loss = 1.16 (946.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:21:05.246395: step 2300, loss = 1.05 (873.4 examples/sec; 0.147 sec/batch)
2017-05-07 19:21:06.505737: step 2310, loss = 1.15 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-07 19:21:07.866253: step 2320, loss = 1.03 (940.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:21:09.225835: step 2330, loss = 1.20 (941.5 examples/sec; 0.136 sec/batch)
2017-05-07 19:21:10.579261: step 2340, loss = 1.33 (945.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:21:11.964608: step 2350, loss = 1.08 (924.0 examples/sec; 0.139 sec/batch)
2017-05-07 19:21:13.332270: step 2360, loss = 1.15 (935.9 examples/sec; 0.137 sec/batch)
2017-05-07 19:21:14.697278: step 2370, loss = 1.15 (937.7 examples/sec; 0.137 sec/batch)
2017-05-07 19:21:16.060430: step 2380, loss = 1.18 (939.0 examples/sec; 0.136 sec/batch)
2017-05-07 19:21:17.399772: step 2390, loss = 1.16 (955.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:21:18.844065: step 2400, loss = 1.07 (886.2 examples/sec; 0.144 sec/batch)
2017-05-07 19:21:20.071096: step 2410, loss = 1.00 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-07 19:21:21.408253: step 2420, loss = 1.12 (957.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:21:22.750657: step 2430, loss = 1.10 (953.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:21:24.112012: step 2440, loss = 1.15 (940.2 examples/sec; 0.136 sec/batch)
2017-05-07 19:21:25.509318: step 2450, loss = 0.93 (916.0 examples/sec; 0.140 sec/batch)
2017-05-07 19:21:26.868121: step 2460, loss = 1.12 (942.0 examples/sec; 0.136 sec/batch)
2017-05-07 19:21:28.220809: step 2470, loss = 0.99 (946.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:21:29.573127: step 2480, loss = 1.23 (946.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:21:30.904857: step 2490, loss = 1.00 (961.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:21:32.370977: step 2500, loss = 1.22 (873.0 examples/sec; 0.147 sec/batch)
2017-05-07 19:21:33.641279: step 2510, loss = 1.15 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-07 19:21:34.989342: step 2520, loss = 1.42 (949.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:21:36.336198: step 2530, loss = 1.30 (950.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:21:37.689983: step 2540, loss = 1.06 (945.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:21:39.072340: step 2550, loss = 0.96 (926.0 examples/sec; 0.138 sec/batch)
2017-05-07 19:21:40.456158: step 2560, loss = 1.04 (925.0 examples/sec; 0.138 sec/batch)
2017-05-07 19:21:41.826274: step 2570, loss = 1.04 (934.2 examples/sec; 0.137 sec/batch)
2017-05-07 19:21:43.200648: step 2580, loss = 1.24 (931.3 examples/sec; 0.137 sec/batch)
2017-05-07 19:21:44.551926: step 2590, loss = 1.14 (947.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:21:46.010188: step 2600, loss = 1.11 (877.8 examples/sec; 0.146 sec/batch)
2017-05-07 19:21:47.265728: step 2610, loss = 1.03 (1019.5 examples/sec; 0.126 sec/batch)
2017-05-07 19:21:48.585068: step 2620, loss = 1.12 (970.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:21:49.922284: step 2630, loss = 0.92 (957.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:21:51.272633: step 2640, loss = 1.03 (947.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:21:52.612436: step 2650, loss = 1.42 (955.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:21:53.968744: step 2660, loss = 1.04 (943.7 examples/sec; 0.136 sec/batch)
2017-05-07 19:21:55.325435: step 2670, loss = 0.99 (943.5 examples/sec; 0.136 sec/batch)
2017-05-07 19:21:56.662328: step 2680, loss = 1.09 (957.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:21:58.016342: step 2690, loss = 1.29 (945.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:21:59.467170: step 2700, loss = 1.47 (882.3 examples/sec; 0.145 sec/batch)
2017-05-07 19:22:00.705417: step 2710, loss = 1.04 (1033.7 examples/sec; 0.124 sec/batch)
2017-05-07 19:22:02.056423: step 2720, loss = 1.00 (947.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:22:03.419522: step 2730, loss = 1.19 (939.0 examples/sec; 0.136 sec/batch)
2017-05-07 19:22:04.751677: step 2740, loss = 1.02 (960.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:22:06.100407: step 2750, loss = 1.27 (949.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:22:07.431538: step 2760, loss = 1.21 (961.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:22:08.758272: step 2770, loss = 1.06 (964.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:22:10.118224: step 2780, loss = 0.98 (941.2 examples/sec; 0.136 sec/batch)
2017-05-07 19:22:11.468198: step 2790, loss = 1.38 (948.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:22:12.918319: step 2800, loss = 1.18 (882.7 examples/sec; 0.145 sec/batch)
2017-05-07 19:22:14.157732: step 2810, loss = 1.03 (1033.1 examples/sec; 0.124 sec/batch)
2017-05-07 19:22:15.498958: step 2820, loss = 0.95 (954.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:22:16.855698: step 2830, loss = 1.06 (943.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:22:18.212290: step 2840, loss = 1.01 (943.5 examples/sec; 0.136 sec/batch)
2017-05-07 19:22:19.571903: step 2850, loss = 1.09 (941.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:22:20.913883: step 2860, loss = 0.89 (953.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:22:22.240270: step 2870, loss = 1.00 (965.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:22:23.577620: step 2880, loss = 0.98 (957.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:22:24.927174: step 2890, loss = 1.08 (948.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:22:26.411947: step 2900, loss = 1.07 (862.1 examples/sec; 0.148 sec/batch)
2017-05-07 19:22:27.663234: step 2910, loss = 1.02 (1022.9 examples/sec; 0.125 sec/batch)
2017-05-07 19:22:28.997247: step 2920, loss = 1.21 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:22:30.311144: step 2930, loss = 0.91 (974.2 examples/sec; 0.131 sec/batch)
2017-05-07 19:22:31.663923: step 2940, loss = 0.81 (946.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:22:32.999124: step 2950, loss = 1.07 (958.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:22:34.367537: step 2960, loss = 1.01 (935.4 examples/sec; 0.137 sec/batch)
2017-05-07 19:22:35.702622: step 2970, loss = 0.98 (958.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:22:37.034050: step 2980, loss = 0.91 (961.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:22:38.384195: step 2990, loss = 1.01 (948.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:22:39.817111: step 3000, loss = 1.11 (893.3 examples/sec; 0.143 sec/batch)
2017-05-07 19:22:41.086944: step 3010, loss = 1.23 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-07 19:22:42.452831: step 3020, loss = 1.15 (937.1 examples/sec; 0.137 sec/batch)
2017-05-07 19:22:43.801325: step 3030, loss = 1.07 (949.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:22:45.144525: step 3040, loss = 1.09 (953.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:22:46.516061: step 3050, loss = 0.89 (933.3 examples/sec; 0.137 sec/batch)
2017-05-07 19:22:47.889075: step 3060, loss = 1.15 (932.3 examples/sec; 0.137 sec/batch)
2017-05-07 19:22:49.237693: step 3070, loss = 0.84 (949.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:22:50.599637: step 3080, loss = 1.02 (939.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:22:51.953653: step 3090, loss = 1.05 (945.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:22:53.411914: step 3100, loss = 1.23 (877.8 examples/sec; 0.146 sec/batch)
2017-05-07 19:22:54.653951: step 3110, loss = 0.88 (1030.6 examples/sec; 0.124 sec/batch)
2017-05-07 19:22:55.997881: step 3120, loss = 1.02 (952.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:22:57.347759: step 3130, loss = 1.10 (948.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:22:58.700242: step 3140, loss = 1.23 (946.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:23:00.059912: step 3150, loss = 1.06 (941.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:23:01.399133: step 3160, loss = 1.20 (955.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:23:02.724348: step 3170, loss = 0.96 (965.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:23:04.052930: step 3180, loss = 0.89 (963.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:23:05.399149: step 3190, loss = 1.00 (950.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:23:06.851481: step 3200, loss = 1.03 (881.4 examples/sec; 0.145 sec/batch)
2017-05-07 19:23:08.070658: stepE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 75 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
 3210, loss = 1.02 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-07 19:23:09.401835: step 3220, loss = 0.94 (961.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:23:10.705402: step 3230, loss = 1.01 (981.9 examples/sec; 0.130 sec/batch)
2017-05-07 19:23:12.022960: step 3240, loss = 0.99 (971.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:23:13.394984: step 3250, loss = 0.96 (932.9 examples/sec; 0.137 sec/batch)
2017-05-07 19:23:14.743880: step 3260, loss = 0.94 (948.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:23:16.077257: step 3270, loss = 1.09 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:23:17.403715: step 3280, loss = 1.02 (965.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:23:18.742326: step 3290, loss = 0.97 (956.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:23:20.192996: step 3300, loss = 1.22 (882.4 examples/sec; 0.145 sec/batch)
2017-05-07 19:23:21.451158: step 3310, loss = 0.89 (1017.4 examples/sec; 0.126 sec/batch)
2017-05-07 19:23:22.783852: step 3320, loss = 1.40 (960.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:23:24.120654: step 3330, loss = 0.83 (957.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:23:25.450907: step 3340, loss = 1.03 (962.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:23:26.813115: step 3350, loss = 1.00 (939.7 examples/sec; 0.136 sec/batch)
2017-05-07 19:23:28.162715: step 3360, loss = 0.96 (948.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:23:29.506139: step 3370, loss = 1.08 (952.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:23:30.843314: step 3380, loss = 1.00 (957.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:23:32.187069: step 3390, loss = 1.15 (952.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:23:33.621449: step 3400, loss = 1.00 (892.4 examples/sec; 0.143 sec/batch)
2017-05-07 19:23:34.891895: step 3410, loss = 1.03 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-07 19:23:36.209263: step 3420, loss = 0.85 (971.6 examples/sec; 0.132 sec/batch)
2017-05-07 19:23:37.551432: step 3430, loss = 1.13 (953.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:23:38.891024: step 3440, loss = 1.00 (955.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:23:40.238301: step 3450, loss = 0.88 (950.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:23:41.578043: step 3460, loss = 1.19 (955.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:23:42.911745: step 3470, loss = 1.06 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:23:44.269529: step 3480, loss = 0.92 (942.7 examples/sec; 0.136 sec/batch)
2017-05-07 19:23:45.597012: step 3490, loss = 1.00 (964.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:23:47.066030: step 3500, loss = 0.82 (871.3 examples/sec; 0.147 sec/batch)
2017-05-07 19:23:48.318834: step 3510, loss = 0.97 (1021.7 examples/sec; 0.125 sec/batch)
2017-05-07 19:23:49.654873: step 3520, loss = 1.00 (958.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:23:51.016043: step 3530, loss = 0.94 (940.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:23:52.372677: step 3540, loss = 0.95 (943.5 examples/sec; 0.136 sec/batch)
2017-05-07 19:23:53.736506: step 3550, loss = 0.96 (938.5 examples/sec; 0.136 sec/batch)
2017-05-07 19:23:55.065495: step 3560, loss = 0.84 (963.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:23:56.419313: step 3570, loss = 1.04 (945.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:23:57.784318: step 3580, loss = 1.03 (937.7 examples/sec; 0.137 sec/batch)
2017-05-07 19:23:59.106807: step 3590, loss = 1.02 (967.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:24:00.560295: step 3600, loss = 1.06 (880.6 examples/sec; 0.145 sec/batch)
2017-05-07 19:24:01.817949: step 3610, loss = 1.07 (1017.8 examples/sec; 0.126 sec/batch)
2017-05-07 19:24:03.154050: step 3620, loss = 1.13 (958.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:24:04.505333: step 3630, loss = 0.90 (947.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:24:05.838354: step 3640, loss = 1.00 (960.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:24:07.210964: step 3650, loss = 1.09 (932.5 examples/sec; 0.137 sec/batch)
2017-05-07 19:24:08.544580: step 3660, loss = 0.82 (959.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:24:09.895420: step 3670, loss = 1.07 (947.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:24:11.238079: step 3680, loss = 1.14 (953.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:24:12.582672: step 3690, loss = 1.00 (952.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:24:14.026549: step 3700, loss = 0.96 (886.5 examples/sec; 0.144 sec/batch)
2017-05-07 19:24:15.257571: step 3710, loss = 1.15 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-07 19:24:16.587985: step 3720, loss = 1.40 (962.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:24:17.927986: step 3730, loss = 1.04 (955.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:24:19.269805: step 3740, loss = 1.01 (953.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:24:20.630366: step 3750, loss = 0.95 (940.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:24:21.958492: step 3760, loss = 1.07 (963.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:24:23.302666: step 3770, loss = 1.04 (952.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:24:24.650082: step 3780, loss = 1.09 (950.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:24:25.987879: step 3790, loss = 1.05 (956.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:24:27.445307: step 3800, loss = 1.02 (878.3 examples/sec; 0.146 sec/batch)
2017-05-07 19:24:28.684810: step 3810, loss = 1.16 (1032.7 examples/sec; 0.124 sec/batch)
2017-05-07 19:24:30.026174: step 3820, loss = 1.02 (954.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:24:31.390735: step 3830, loss = 0.87 (938.0 examples/sec; 0.136 sec/batch)
2017-05-07 19:24:32.736397: step 3840, loss = 0.96 (951.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:24:34.097140: step 3850, loss = 0.88 (940.7 examples/sec; 0.136 sec/batch)
2017-05-07 19:24:35.431437: step 3860, loss = 0.94 (959.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:24:36.762017: step 3870, loss = 1.08 (962.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:24:38.098947: step 3880, loss = 0.92 (957.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:24:39.453602: step 3890, loss = 1.11 (944.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:24:40.907698: step 3900, loss = 0.88 (880.3 examples/sec; 0.145 sec/batch)
2017-05-07 19:24:42.155675: step 3910, loss = 0.91 (1025.7 examples/sec; 0.125 sec/batch)
2017-05-07 19:24:43.496616: step 3920, loss = 0.81 (954.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:24:44.832713: step 3930, loss = 0.92 (958.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:24:46.174924: step 3940, loss = 0.87 (953.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:24:47.540247: step 3950, loss = 0.99 (937.5 examples/sec; 0.137 sec/batch)
2017-05-07 19:24:48.876962: step 3960, loss = 0.91 (957.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:24:50.220501: step 3970, loss = 1.04 (952.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:24:51.571874: step 3980, loss = 0.91 (947.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:24:52.913999: step 3990, loss = 0.86 (953.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:24:54.361333: step 4000, loss = 0.97 (884.4 examples/sec; 0.145 sec/batch)
2017-05-07 19:24:55.608909: step 4010, loss = 0.97 (1026.0 examples/sec; 0.125 sec/batch)
2017-05-07 19:24:56.945250: step 4020, loss = 0.96 (957.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:24:58.277926: step 4030, loss = 1.06 (960.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:24:59.623103: step 4040, loss = 0.92 (951.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:25:00.980755: step 4050, loss = 0.82 (942.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:25:02.345157: step 4060, loss = 0.82 (938.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:25:03.698403: step 4070, loss = 0.98 (945.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:25:05.012194: step 4080, loss = 0.94 (974.3 examples/sec; 0.131 sec/batch)
2017-05-07 19:25:06.356618: step 4090, loss = 0.97 (952.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:25:07.796734: step 4100, loss = 0.89 (888.8 examples/sec; 0.144 sec/batch)
2017-05-07 19:25:09.034589: step 4110, loss = 0.99 (1034.1 examples/sec; 0.124 sec/batch)
2017-05-07 19:25:10.391055: step 4120, loss = 1.27 (943.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:25:11.723196E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 93 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
: step 4130, loss = 0.98 (960.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:25:13.073623: step 4140, loss = 1.09 (947.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:25:14.433222: step 4150, loss = 1.07 (941.5 examples/sec; 0.136 sec/batch)
2017-05-07 19:25:15.784947: step 4160, loss = 1.05 (946.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:25:17.120439: step 4170, loss = 1.04 (958.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:25:18.477777: step 4180, loss = 1.22 (943.0 examples/sec; 0.136 sec/batch)
2017-05-07 19:25:19.813041: step 4190, loss = 0.87 (958.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:25:21.267248: step 4200, loss = 0.88 (880.2 examples/sec; 0.145 sec/batch)
2017-05-07 19:25:22.495882: step 4210, loss = 1.04 (1041.8 examples/sec; 0.123 sec/batch)
2017-05-07 19:25:23.827437: step 4220, loss = 0.87 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:25:25.172169: step 4230, loss = 1.02 (951.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:25:26.512706: step 4240, loss = 1.01 (954.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:25:27.862301: step 4250, loss = 1.00 (948.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:25:29.196090: step 4260, loss = 0.75 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:25:30.537202: step 4270, loss = 0.92 (954.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:25:31.854384: step 4280, loss = 0.95 (971.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:25:33.210672: step 4290, loss = 1.13 (943.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:25:34.693606: step 4300, loss = 1.25 (863.1 examples/sec; 0.148 sec/batch)
2017-05-07 19:25:35.930606: step 4310, loss = 1.03 (1034.8 examples/sec; 0.124 sec/batch)
2017-05-07 19:25:37.262358: step 4320, loss = 0.85 (961.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:25:38.599899: step 4330, loss = 0.95 (957.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:25:39.930069: step 4340, loss = 1.08 (962.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:25:41.282578: step 4350, loss = 1.06 (946.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:25:42.623425: step 4360, loss = 0.90 (954.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:25:43.973953: step 4370, loss = 0.88 (947.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:25:45.308836: step 4380, loss = 1.00 (958.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:25:46.647591: step 4390, loss = 1.07 (956.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:25:48.096149: step 4400, loss = 0.88 (883.6 examples/sec; 0.145 sec/batch)
2017-05-07 19:25:49.361666: step 4410, loss = 1.08 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-07 19:25:50.685554: step 4420, loss = 0.98 (966.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:25:52.032356: step 4430, loss = 0.88 (950.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:25:53.383173: step 4440, loss = 1.09 (947.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:25:54.787543: step 4450, loss = 0.97 (911.4 examples/sec; 0.140 sec/batch)
2017-05-07 19:25:56.119124: step 4460, loss = 1.05 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:25:57.478762: step 4470, loss = 0.84 (941.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:25:58.803558: step 4480, loss = 0.71 (966.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:26:00.144269: step 4490, loss = 0.98 (954.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:26:01.567143: step 4500, loss = 1.21 (899.6 examples/sec; 0.142 sec/batch)
2017-05-07 19:26:02.825437: step 4510, loss = 0.93 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-07 19:26:04.163703: step 4520, loss = 0.90 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:26:05.514181: step 4530, loss = 1.02 (947.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:26:06.865389: step 4540, loss = 0.96 (947.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:26:08.229177: step 4550, loss = 0.87 (938.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:26:09.550929: step 4560, loss = 1.32 (968.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:26:10.888316: step 4570, loss = 0.91 (957.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:26:12.246208: step 4580, loss = 0.78 (942.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:26:13.569225: step 4590, loss = 0.81 (967.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:26:15.035659: step 4600, loss = 0.95 (872.9 examples/sec; 0.147 sec/batch)
2017-05-07 19:26:16.274001: step 4610, loss = 1.21 (1033.6 examples/sec; 0.124 sec/batch)
2017-05-07 19:26:17.612017: step 4620, loss = 0.98 (956.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:26:18.977912: step 4630, loss = 0.82 (937.1 examples/sec; 0.137 sec/batch)
2017-05-07 19:26:20.323880: step 4640, loss = 0.89 (951.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:26:21.682083: step 4650, loss = 0.97 (942.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:26:23.027142: step 4660, loss = 1.01 (951.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:26:24.378138: step 4670, loss = 0.85 (947.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:26:25.727810: step 4680, loss = 1.41 (948.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:26:27.079411: step 4690, loss = 0.82 (947.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:26:28.513792: step 4700, loss = 1.11 (892.4 examples/sec; 0.143 sec/batch)
2017-05-07 19:26:29.747135: step 4710, loss = 1.01 (1037.9 examples/sec; 0.123 sec/batch)
2017-05-07 19:26:31.080308: step 4720, loss = 0.87 (960.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:26:32.425171: step 4730, loss = 0.76 (951.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:26:33.763968: step 4740, loss = 0.90 (956.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:26:35.115506: step 4750, loss = 0.93 (947.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:26:36.456116: step 4760, loss = 1.03 (954.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:26:37.808154: step 4770, loss = 1.00 (946.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:26:39.162948: step 4780, loss = 0.95 (944.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:26:40.497765: step 4790, loss = 1.04 (958.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:26:41.934766: step 4800, loss = 0.87 (890.7 examples/sec; 0.144 sec/batch)
2017-05-07 19:26:43.156376: step 4810, loss = 1.07 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-07 19:26:44.516573: step 4820, loss = 0.79 (941.0 examples/sec; 0.136 sec/batch)
2017-05-07 19:26:45.855353: step 4830, loss = 0.93 (956.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:26:47.179410: step 4840, loss = 0.90 (966.7 examples/sec; 0.132 sec/batch)
2017-05-07 19:26:48.531140: step 4850, loss = 0.87 (946.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:26:49.884971: step 4860, loss = 0.85 (945.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:26:51.214209: step 4870, loss = 0.92 (963.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:26:52.555792: step 4880, loss = 0.99 (954.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:26:53.904656: step 4890, loss = 1.05 (948.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:26:55.353244: step 4900, loss = 0.90 (883.6 examples/sec; 0.145 sec/batch)
2017-05-07 19:26:56.606672: step 4910, loss = 0.94 (1021.2 examples/sec; 0.125 sec/batch)
2017-05-07 19:26:57.950053: step 4920, loss = 1.02 (952.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:26:59.301411: step 4930, loss = 0.91 (947.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:27:00.652831: step 4940, loss = 1.06 (947.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:27:02.012656: step 4950, loss = 0.90 (941.3 examples/sec; 0.136 sec/batch)
2017-05-07 19:27:03.350062: step 4960, loss = 0.97 (957.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:27:04.703992: step 4970, loss = 0.88 (945.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:27:06.058723: step 4980, loss = 0.82 (944.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:27:07.400718: step 4990, loss = 1.00 (953.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:27:08.859278: step 5000, loss = 0.87 (877.6 examples/sec; 0.146 sec/batch)
2017-05-07 19:27:10.081693: step 5010, loss = 0.90 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-07 19:27:11.446549: step 5020, loss = 1.04 (937.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:27:12.789577: step 5030, loss = 0.92 (953.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:27:14.129547: step 5040, loss = 0.89 (955.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:27:15.4E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 112 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
84974: step 5050, loss = 0.98 (944.3 examples/sec; 0.136 sec/batch)
2017-05-07 19:27:16.818140: step 5060, loss = 1.02 (960.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:27:18.146741: step 5070, loss = 1.00 (963.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:27:19.499945: step 5080, loss = 0.71 (945.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:27:20.823375: step 5090, loss = 0.96 (967.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:27:22.258674: step 5100, loss = 0.86 (891.8 examples/sec; 0.144 sec/batch)
2017-05-07 19:27:23.518215: step 5110, loss = 1.01 (1016.2 examples/sec; 0.126 sec/batch)
2017-05-07 19:27:24.867905: step 5120, loss = 1.06 (948.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:27:26.223387: step 5130, loss = 0.93 (944.3 examples/sec; 0.136 sec/batch)
2017-05-07 19:27:27.548028: step 5140, loss = 0.76 (966.3 examples/sec; 0.132 sec/batch)
2017-05-07 19:27:28.877088: step 5150, loss = 0.96 (963.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:27:30.232576: step 5160, loss = 0.89 (944.3 examples/sec; 0.136 sec/batch)
2017-05-07 19:27:31.572723: step 5170, loss = 0.83 (955.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:27:32.902444: step 5180, loss = 0.96 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:27:34.258293: step 5190, loss = 0.73 (944.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:27:35.724189: step 5200, loss = 1.13 (873.2 examples/sec; 0.147 sec/batch)
2017-05-07 19:27:36.946100: step 5210, loss = 0.87 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-07 19:27:38.299313: step 5220, loss = 1.21 (945.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:27:39.615574: step 5230, loss = 0.95 (972.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:27:40.965181: step 5240, loss = 1.00 (948.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:27:42.313240: step 5250, loss = 1.05 (949.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:27:43.673371: step 5260, loss = 1.04 (941.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:27:45.016224: step 5270, loss = 1.02 (953.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:27:46.367821: step 5280, loss = 0.87 (947.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:27:47.697416: step 5290, loss = 0.87 (962.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:27:49.146447: step 5300, loss = 0.90 (883.3 examples/sec; 0.145 sec/batch)
2017-05-07 19:27:50.380040: step 5310, loss = 0.96 (1037.6 examples/sec; 0.123 sec/batch)
2017-05-07 19:27:51.719621: step 5320, loss = 0.85 (955.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:27:53.061340: step 5330, loss = 0.94 (954.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:27:54.408832: step 5340, loss = 1.11 (949.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:27:55.770373: step 5350, loss = 0.86 (940.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:27:57.121096: step 5360, loss = 1.03 (947.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:27:58.450211: step 5370, loss = 0.91 (963.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:27:59.800435: step 5380, loss = 0.92 (948.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:28:01.141016: step 5390, loss = 0.98 (954.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:28:02.627361: step 5400, loss = 0.93 (861.2 examples/sec; 0.149 sec/batch)
2017-05-07 19:28:03.875955: step 5410, loss = 0.96 (1025.2 examples/sec; 0.125 sec/batch)
2017-05-07 19:28:05.241482: step 5420, loss = 0.84 (937.4 examples/sec; 0.137 sec/batch)
2017-05-07 19:28:06.588819: step 5430, loss = 1.04 (950.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:28:07.941765: step 5440, loss = 0.84 (946.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:28:09.282476: step 5450, loss = 1.00 (954.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:28:10.627629: step 5460, loss = 0.75 (951.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:28:11.978357: step 5470, loss = 0.84 (947.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:28:13.307885: step 5480, loss = 0.91 (962.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:28:14.653091: step 5490, loss = 1.04 (951.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:28:16.100552: step 5500, loss = 0.90 (884.3 examples/sec; 0.145 sec/batch)
2017-05-07 19:28:17.312364: step 5510, loss = 0.77 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-07 19:28:18.667037: step 5520, loss = 0.88 (944.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:28:19.988562: step 5530, loss = 1.05 (968.6 examples/sec; 0.132 sec/batch)
2017-05-07 19:28:21.337008: step 5540, loss = 0.92 (949.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:28:22.696427: step 5550, loss = 0.94 (941.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:28:24.034098: step 5560, loss = 1.20 (956.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:28:25.372816: step 5570, loss = 0.98 (956.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:28:26.722317: step 5580, loss = 1.02 (948.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:28:28.065311: step 5590, loss = 0.72 (953.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:28:29.512383: step 5600, loss = 1.11 (884.5 examples/sec; 0.145 sec/batch)
2017-05-07 19:28:30.768131: step 5610, loss = 0.84 (1019.3 examples/sec; 0.126 sec/batch)
2017-05-07 19:28:32.106349: step 5620, loss = 1.00 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:28:33.435755: step 5630, loss = 0.93 (962.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:28:34.789973: step 5640, loss = 0.83 (945.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:28:36.138654: step 5650, loss = 0.86 (949.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:28:37.488741: step 5660, loss = 1.07 (948.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:28:38.803961: step 5670, loss = 0.99 (973.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:28:40.136313: step 5680, loss = 0.95 (960.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:28:41.460258: step 5690, loss = 0.74 (966.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:28:42.908409: step 5700, loss = 0.98 (883.9 examples/sec; 0.145 sec/batch)
2017-05-07 19:28:44.139243: step 5710, loss = 0.89 (1039.9 examples/sec; 0.123 sec/batch)
2017-05-07 19:28:45.459116: step 5720, loss = 0.96 (969.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:28:46.808597: step 5730, loss = 0.82 (948.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:28:48.160783: step 5740, loss = 0.80 (946.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:28:49.519058: step 5750, loss = 0.90 (942.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:28:50.873670: step 5760, loss = 0.87 (944.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:28:52.216265: step 5770, loss = 0.81 (953.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:28:53.555499: step 5780, loss = 0.84 (955.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:28:54.902767: step 5790, loss = 1.13 (950.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:28:56.347654: step 5800, loss = 0.84 (885.9 examples/sec; 0.144 sec/batch)
2017-05-07 19:28:57.580070: step 5810, loss = 1.01 (1038.6 examples/sec; 0.123 sec/batch)
2017-05-07 19:28:58.936678: step 5820, loss = 0.95 (943.5 examples/sec; 0.136 sec/batch)
2017-05-07 19:29:00.271542: step 5830, loss = 0.88 (958.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:29:01.609655: step 5840, loss = 1.00 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:29:03.002198: step 5850, loss = 0.93 (919.2 examples/sec; 0.139 sec/batch)
2017-05-07 19:29:04.322035: step 5860, loss = 0.85 (969.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:29:05.657665: step 5870, loss = 0.88 (958.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:29:07.006639: step 5880, loss = 0.76 (948.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:29:08.342373: step 5890, loss = 0.99 (958.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:29:09.808684: step 5900, loss = 0.72 (872.9 examples/sec; 0.147 sec/batch)
2017-05-07 19:29:11.058436: step 5910, loss = 0.87 (1024.2 examples/sec; 0.125 sec/batch)
2017-05-07 19:29:12.418754: step 5920, loss = 0.87 (941.0 examples/sec; 0.136 sec/batch)
2017-05-07 19:29:13.752166: step 5930, loss = 0.95 (959.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:29:15.074344: step 5940, loss = 0.81 (968.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:29:16.427956: step 5950, loss = 0.81 (945.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:29:17.730776: step 5960, loss = 0.82 (982.5 examples/sec; 0.130 sec/batch)
2017-05-07 19:29E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 130 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
:19.086848: step 5970, loss = 0.78 (943.9 examples/sec; 0.136 sec/batch)
2017-05-07 19:29:20.424352: step 5980, loss = 0.79 (957.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:29:21.764720: step 5990, loss = 1.02 (955.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:29:23.239011: step 6000, loss = 0.91 (868.2 examples/sec; 0.147 sec/batch)
2017-05-07 19:29:24.473376: step 6010, loss = 0.94 (1037.0 examples/sec; 0.123 sec/batch)
2017-05-07 19:29:25.800049: step 6020, loss = 0.73 (964.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:29:27.132300: step 6030, loss = 1.05 (960.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:29:28.467265: step 6040, loss = 1.01 (958.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:29:29.850447: step 6050, loss = 0.80 (925.4 examples/sec; 0.138 sec/batch)
2017-05-07 19:29:31.209069: step 6060, loss = 0.82 (942.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:29:32.538509: step 6070, loss = 0.97 (962.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:29:33.868192: step 6080, loss = 0.76 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:29:35.210163: step 6090, loss = 0.97 (953.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:29:36.647086: step 6100, loss = 0.91 (890.8 examples/sec; 0.144 sec/batch)
2017-05-07 19:29:37.900785: step 6110, loss = 0.97 (1021.0 examples/sec; 0.125 sec/batch)
2017-05-07 19:29:39.224664: step 6120, loss = 0.93 (966.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:29:40.560196: step 6130, loss = 1.00 (958.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:29:41.894323: step 6140, loss = 0.79 (959.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:29:43.247507: step 6150, loss = 1.08 (945.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:29:44.562698: step 6160, loss = 0.80 (973.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:29:45.892083: step 6170, loss = 1.03 (962.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:29:47.234348: step 6180, loss = 0.97 (953.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:29:48.555415: step 6190, loss = 0.97 (968.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:29:49.991413: step 6200, loss = 0.94 (891.4 examples/sec; 0.144 sec/batch)
2017-05-07 19:29:51.244630: step 6210, loss = 0.96 (1021.4 examples/sec; 0.125 sec/batch)
2017-05-07 19:29:52.563568: step 6220, loss = 0.88 (970.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:29:53.893015: step 6230, loss = 1.26 (962.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:29:55.225837: step 6240, loss = 0.80 (960.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:29:56.580888: step 6250, loss = 0.82 (944.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:29:57.904669: step 6260, loss = 0.88 (966.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:29:59.238919: step 6270, loss = 0.98 (959.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:30:00.564539: step 6280, loss = 0.92 (965.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:30:01.907972: step 6290, loss = 0.88 (952.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:30:03.360337: step 6300, loss = 0.79 (881.3 examples/sec; 0.145 sec/batch)
2017-05-07 19:30:04.610406: step 6310, loss = 0.70 (1023.9 examples/sec; 0.125 sec/batch)
2017-05-07 19:30:05.927898: step 6320, loss = 0.84 (971.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:30:07.283114: step 6330, loss = 0.91 (944.5 examples/sec; 0.136 sec/batch)
2017-05-07 19:30:08.611550: step 6340, loss = 0.86 (963.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:30:09.975330: step 6350, loss = 0.86 (938.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:30:11.306760: step 6360, loss = 0.92 (961.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:30:12.626223: step 6370, loss = 0.78 (970.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:30:13.968989: step 6380, loss = 0.91 (953.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:30:15.330297: step 6390, loss = 0.83 (940.3 examples/sec; 0.136 sec/batch)
2017-05-07 19:30:16.763048: step 6400, loss = 0.94 (893.4 examples/sec; 0.143 sec/batch)
2017-05-07 19:30:18.015653: step 6410, loss = 0.82 (1021.9 examples/sec; 0.125 sec/batch)
2017-05-07 19:30:19.354000: step 6420, loss = 0.80 (956.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:30:20.686958: step 6430, loss = 0.85 (960.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:30:22.034671: step 6440, loss = 0.97 (949.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:30:23.378204: step 6450, loss = 0.92 (952.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:30:24.714543: step 6460, loss = 0.95 (957.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:30:26.073938: step 6470, loss = 0.93 (941.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:30:27.431613: step 6480, loss = 1.01 (942.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:30:28.747705: step 6490, loss = 0.93 (972.6 examples/sec; 0.132 sec/batch)
2017-05-07 19:30:30.205544: step 6500, loss = 1.02 (878.0 examples/sec; 0.146 sec/batch)
2017-05-07 19:30:31.434357: step 6510, loss = 1.04 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-07 19:30:32.767413: step 6520, loss = 0.97 (960.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:30:34.118998: step 6530, loss = 0.90 (947.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:30:35.459561: step 6540, loss = 1.01 (954.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:30:36.814001: step 6550, loss = 0.90 (945.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:30:38.126180: step 6560, loss = 0.74 (975.5 examples/sec; 0.131 sec/batch)
2017-05-07 19:30:39.451264: step 6570, loss = 0.95 (966.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:30:40.777787: step 6580, loss = 0.86 (964.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:30:42.117830: step 6590, loss = 1.00 (955.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:30:43.558287: step 6600, loss = 0.64 (888.6 examples/sec; 0.144 sec/batch)
2017-05-07 19:30:44.797108: step 6610, loss = 1.02 (1033.2 examples/sec; 0.124 sec/batch)
2017-05-07 19:30:46.148496: step 6620, loss = 0.89 (947.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:30:47.503692: step 6630, loss = 1.02 (944.5 examples/sec; 0.136 sec/batch)
2017-05-07 19:30:48.814730: step 6640, loss = 0.88 (976.3 examples/sec; 0.131 sec/batch)
2017-05-07 19:30:50.200090: step 6650, loss = 0.99 (923.9 examples/sec; 0.139 sec/batch)
2017-05-07 19:30:51.499905: step 6660, loss = 1.04 (984.8 examples/sec; 0.130 sec/batch)
2017-05-07 19:30:52.805562: step 6670, loss = 0.83 (980.3 examples/sec; 0.131 sec/batch)
2017-05-07 19:30:54.163993: step 6680, loss = 0.88 (942.3 examples/sec; 0.136 sec/batch)
2017-05-07 19:30:55.500608: step 6690, loss = 0.89 (957.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:30:56.973879: step 6700, loss = 0.86 (868.8 examples/sec; 0.147 sec/batch)
2017-05-07 19:30:58.201675: step 6710, loss = 1.02 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-07 19:30:59.514580: step 6720, loss = 0.68 (974.9 examples/sec; 0.131 sec/batch)
2017-05-07 19:31:00.869785: step 6730, loss = 0.91 (944.5 examples/sec; 0.136 sec/batch)
2017-05-07 19:31:02.193367: step 6740, loss = 0.91 (967.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:31:03.606959: step 6750, loss = 1.00 (905.5 examples/sec; 0.141 sec/batch)
2017-05-07 19:31:04.918072: step 6760, loss = 0.79 (976.3 examples/sec; 0.131 sec/batch)
2017-05-07 19:31:06.239687: step 6770, loss = 0.82 (968.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:31:07.595729: step 6780, loss = 0.88 (943.9 examples/sec; 0.136 sec/batch)
2017-05-07 19:31:08.951036: step 6790, loss = 0.95 (944.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:31:10.431025: step 6800, loss = 0.75 (864.9 examples/sec; 0.148 sec/batch)
2017-05-07 19:31:11.631786: step 6810, loss = 0.98 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-07 19:31:12.974091: step 6820, loss = 0.95 (953.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:31:14.319592: step 6830, loss = 0.94 (951.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:31:15.669949: step 6840, loss = 0.93 (947.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:31:17.052308: step 6850, loss = 0.94 (926.0 examples/sec; 0.138 sec/batch)
2017-05-07 19:31:18.372264: step 6860, loss = 0.90 (969.7 examples/sec; 0.132 sec/batch)
2017-05-07 19:31:19.727226: step 6870, loss = 0.81 (944.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:31:21.072325: step 6880, loss = 1.07 (951.6 examples/sec; 0.135 sec/batch)
2017-05-07 E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 148 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
19:31:22.390278: step 6890, loss = 0.99 (971.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:31:23.885907: step 6900, loss = 0.93 (855.8 examples/sec; 0.150 sec/batch)
2017-05-07 19:31:25.077776: step 6910, loss = 0.74 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-07 19:31:26.432780: step 6920, loss = 0.91 (944.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:31:27.762680: step 6930, loss = 0.95 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:31:29.096624: step 6940, loss = 0.77 (959.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:31:30.513059: step 6950, loss = 0.80 (903.7 examples/sec; 0.142 sec/batch)
2017-05-07 19:31:31.816499: step 6960, loss = 0.82 (982.0 examples/sec; 0.130 sec/batch)
2017-05-07 19:31:33.165322: step 6970, loss = 0.99 (948.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:31:34.532216: step 6980, loss = 0.94 (936.4 examples/sec; 0.137 sec/batch)
2017-05-07 19:31:35.877537: step 6990, loss = 0.87 (951.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:31:37.324330: step 7000, loss = 0.90 (884.7 examples/sec; 0.145 sec/batch)
2017-05-07 19:31:38.546964: step 7010, loss = 0.83 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-07 19:31:39.880062: step 7020, loss = 0.87 (960.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:31:41.242842: step 7030, loss = 0.84 (939.3 examples/sec; 0.136 sec/batch)
2017-05-07 19:31:42.616240: step 7040, loss = 0.91 (932.0 examples/sec; 0.137 sec/batch)
2017-05-07 19:31:43.991986: step 7050, loss = 0.81 (930.4 examples/sec; 0.138 sec/batch)
2017-05-07 19:31:45.288364: step 7060, loss = 0.92 (987.4 examples/sec; 0.130 sec/batch)
2017-05-07 19:31:46.642946: step 7070, loss = 0.93 (944.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:31:48.003473: step 7080, loss = 0.62 (940.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:31:49.338292: step 7090, loss = 0.84 (958.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:31:50.853061: step 7100, loss = 0.91 (845.0 examples/sec; 0.151 sec/batch)
2017-05-07 19:31:51.992984: step 7110, loss = 0.98 (1122.9 examples/sec; 0.114 sec/batch)
2017-05-07 19:31:53.323345: step 7120, loss = 0.91 (962.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:31:54.659142: step 7130, loss = 0.86 (958.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:31:56.008231: step 7140, loss = 0.72 (948.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:31:57.387790: step 7150, loss = 0.93 (927.8 examples/sec; 0.138 sec/batch)
2017-05-07 19:31:58.681108: step 7160, loss = 1.04 (989.7 examples/sec; 0.129 sec/batch)
2017-05-07 19:32:00.000725: step 7170, loss = 0.90 (970.0 examples/sec; 0.132 sec/batch)
2017-05-07 19:32:01.328573: step 7180, loss = 0.91 (964.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:32:02.687772: step 7190, loss = 0.87 (941.7 examples/sec; 0.136 sec/batch)
2017-05-07 19:32:04.185804: step 7200, loss = 0.91 (854.5 examples/sec; 0.150 sec/batch)
2017-05-07 19:32:05.418644: step 7210, loss = 0.95 (1038.3 examples/sec; 0.123 sec/batch)
2017-05-07 19:32:06.749849: step 7220, loss = 0.83 (961.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:32:08.123205: step 7230, loss = 0.86 (932.0 examples/sec; 0.137 sec/batch)
2017-05-07 19:32:09.452530: step 7240, loss = 0.81 (962.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:32:10.853416: step 7250, loss = 0.77 (913.7 examples/sec; 0.140 sec/batch)
2017-05-07 19:32:12.123216: step 7260, loss = 0.98 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-07 19:32:13.487719: step 7270, loss = 0.98 (938.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:32:14.842678: step 7280, loss = 1.03 (944.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:32:16.207263: step 7290, loss = 0.96 (938.0 examples/sec; 0.136 sec/batch)
2017-05-07 19:32:17.696242: step 7300, loss = 0.88 (859.6 examples/sec; 0.149 sec/batch)
2017-05-07 19:32:18.884510: step 7310, loss = 0.73 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-07 19:32:20.221708: step 7320, loss = 0.98 (957.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:32:21.555033: step 7330, loss = 0.84 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:32:22.869079: step 7340, loss = 0.96 (974.1 examples/sec; 0.131 sec/batch)
2017-05-07 19:32:24.256251: step 7350, loss = 0.84 (922.7 examples/sec; 0.139 sec/batch)
2017-05-07 19:32:25.552886: step 7360, loss = 0.76 (987.2 examples/sec; 0.130 sec/batch)
2017-05-07 19:32:26.881504: step 7370, loss = 0.86 (963.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:32:28.212554: step 7380, loss = 0.82 (961.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:32:29.540563: step 7390, loss = 0.87 (963.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:32:31.034132: step 7400, loss = 0.78 (857.0 examples/sec; 0.149 sec/batch)
2017-05-07 19:32:32.272540: step 7410, loss = 1.00 (1033.6 examples/sec; 0.124 sec/batch)
2017-05-07 19:32:33.596981: step 7420, loss = 0.90 (966.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:32:34.970769: step 7430, loss = 0.94 (931.7 examples/sec; 0.137 sec/batch)
2017-05-07 19:32:36.316332: step 7440, loss = 0.81 (951.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:32:37.709144: step 7450, loss = 0.98 (919.0 examples/sec; 0.139 sec/batch)
2017-05-07 19:32:39.028114: step 7460, loss = 0.84 (970.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:32:40.356656: step 7470, loss = 0.76 (963.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:32:41.715631: step 7480, loss = 0.90 (941.9 examples/sec; 0.136 sec/batch)
2017-05-07 19:32:43.080645: step 7490, loss = 0.89 (937.7 examples/sec; 0.137 sec/batch)
2017-05-07 19:32:44.557917: step 7500, loss = 1.03 (866.5 examples/sec; 0.148 sec/batch)
2017-05-07 19:32:45.775640: step 7510, loss = 0.94 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-07 19:32:47.123805: step 7520, loss = 0.81 (949.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:32:48.450850: step 7530, loss = 0.93 (964.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:32:49.793852: step 7540, loss = 0.85 (953.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:32:51.188499: step 7550, loss = 1.01 (917.8 examples/sec; 0.139 sec/batch)
2017-05-07 19:32:52.492240: step 7560, loss = 0.82 (981.8 examples/sec; 0.130 sec/batch)
2017-05-07 19:32:53.843627: step 7570, loss = 0.69 (947.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:32:55.183257: step 7580, loss = 1.03 (955.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:32:56.520559: step 7590, loss = 0.99 (957.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:32:57.999788: step 7600, loss = 0.91 (865.3 examples/sec; 0.148 sec/batch)
2017-05-07 19:32:59.210444: step 7610, loss = 1.01 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-07 19:33:00.550677: step 7620, loss = 0.85 (955.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:33:01.869924: step 7630, loss = 0.76 (970.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:33:03.215241: step 7640, loss = 0.95 (951.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:33:04.602583: step 7650, loss = 0.89 (922.6 examples/sec; 0.139 sec/batch)
2017-05-07 19:33:05.902650: step 7660, loss = 0.80 (984.6 examples/sec; 0.130 sec/batch)
2017-05-07 19:33:07.244245: step 7670, loss = 0.90 (954.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:33:08.577147: step 7680, loss = 0.78 (960.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:33:09.925292: step 7690, loss = 0.95 (949.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:33:11.410522: step 7700, loss = 0.90 (861.8 examples/sec; 0.149 sec/batch)
2017-05-07 19:33:12.616630: step 7710, loss = 0.91 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-07 19:33:13.974401: step 7720, loss = 0.87 (942.7 examples/sec; 0.136 sec/batch)
2017-05-07 19:33:15.321639: step 7730, loss = 0.90 (950.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:33:16.643648: step 7740, loss = 0.94 (968.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:33:18.040331: step 7750, loss = 0.83 (916.5 examples/sec; 0.140 sec/batch)
2017-05-07 19:33:19.340051: step 7760, loss = 0.79 (984.9 examples/sec; 0.130 sec/batch)
2017-05-07 19:33:20.673154: step 7770, loss = 0.84 (960.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:33:22.033147: step 7780, loss = 0.96 (941.2 examples/sec; 0.136 sec/batch)
2017-05-07 19:33:23.379841: step 7790, loss = 0.85 (950.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:33:24.856922: step 7800, loss = 1.06 (866.6 examples/sec; 0.148 sec/batch)
2017-E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 166 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
05-07 19:33:26.075993: step 7810, loss = 0.81 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-07 19:33:27.414040: step 7820, loss = 0.80 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:33:28.759787: step 7830, loss = 1.03 (951.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:33:30.114649: step 7840, loss = 0.75 (944.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:33:31.515314: step 7850, loss = 1.08 (913.9 examples/sec; 0.140 sec/batch)
2017-05-07 19:33:32.832834: step 7860, loss = 0.85 (971.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:33:34.181623: step 7870, loss = 1.00 (949.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:33:35.543073: step 7880, loss = 0.74 (940.2 examples/sec; 0.136 sec/batch)
2017-05-07 19:33:36.891351: step 7890, loss = 1.01 (949.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:33:38.376510: step 7900, loss = 1.00 (861.9 examples/sec; 0.149 sec/batch)
2017-05-07 19:33:39.569592: step 7910, loss = 1.13 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-07 19:33:40.920151: step 7920, loss = 1.17 (947.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:33:42.263413: step 7930, loss = 0.91 (952.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:33:43.600806: step 7940, loss = 0.85 (957.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:33:44.975896: step 7950, loss = 0.88 (930.8 examples/sec; 0.138 sec/batch)
2017-05-07 19:33:46.267588: step 7960, loss = 0.81 (991.0 examples/sec; 0.129 sec/batch)
2017-05-07 19:33:47.614200: step 7970, loss = 0.92 (950.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:33:48.928053: step 7980, loss = 0.87 (974.2 examples/sec; 0.131 sec/batch)
2017-05-07 19:33:50.249250: step 7990, loss = 0.88 (968.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:33:51.728163: step 8000, loss = 0.87 (865.5 examples/sec; 0.148 sec/batch)
2017-05-07 19:33:52.935316: step 8010, loss = 0.73 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-07 19:33:54.273099: step 8020, loss = 0.89 (956.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:33:55.610476: step 8030, loss = 0.86 (957.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:33:56.940171: step 8040, loss = 1.05 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:33:58.352844: step 8050, loss = 0.64 (906.1 examples/sec; 0.141 sec/batch)
2017-05-07 19:33:59.668551: step 8060, loss = 0.79 (972.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:34:01.013587: step 8070, loss = 0.91 (951.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:34:02.357543: step 8080, loss = 0.79 (952.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:34:03.691157: step 8090, loss = 0.89 (959.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:34:05.209809: step 8100, loss = 1.20 (842.9 examples/sec; 0.152 sec/batch)
2017-05-07 19:34:06.371394: step 8110, loss = 0.82 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-07 19:34:07.705194: step 8120, loss = 0.84 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:34:09.047159: step 8130, loss = 0.84 (953.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:34:10.421479: step 8140, loss = 0.92 (931.4 examples/sec; 0.137 sec/batch)
2017-05-07 19:34:11.813131: step 8150, loss = 0.89 (919.8 examples/sec; 0.139 sec/batch)
2017-05-07 19:34:13.092775: step 8160, loss = 0.73 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-07 19:34:14.447429: step 8170, loss = 0.85 (944.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:34:15.786573: step 8180, loss = 0.74 (955.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:34:17.124888: step 8190, loss = 0.94 (956.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:34:18.617705: step 8200, loss = 0.68 (857.4 examples/sec; 0.149 sec/batch)
2017-05-07 19:34:19.811317: step 8210, loss = 0.70 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-07 19:34:21.141569: step 8220, loss = 0.97 (962.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:34:22.522003: step 8230, loss = 0.93 (927.2 examples/sec; 0.138 sec/batch)
2017-05-07 19:34:23.867102: step 8240, loss = 0.87 (951.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:34:25.261747: step 8250, loss = 0.77 (917.8 examples/sec; 0.139 sec/batch)
2017-05-07 19:34:26.554258: step 8260, loss = 1.07 (990.3 examples/sec; 0.129 sec/batch)
2017-05-07 19:34:27.891155: step 8270, loss = 0.82 (957.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:34:29.226565: step 8280, loss = 0.90 (958.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:34:30.574891: step 8290, loss = 0.90 (949.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:34:32.051108: step 8300, loss = 0.85 (867.1 examples/sec; 0.148 sec/batch)
2017-05-07 19:34:33.257605: step 8310, loss = 0.97 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-07 19:34:34.600901: step 8320, loss = 0.95 (952.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:34:35.950536: step 8330, loss = 0.84 (948.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:34:37.290835: step 8340, loss = 0.81 (955.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:34:38.679803: step 8350, loss = 0.89 (921.6 examples/sec; 0.139 sec/batch)
2017-05-07 19:34:39.976743: step 8360, loss = 0.96 (986.9 examples/sec; 0.130 sec/batch)
2017-05-07 19:34:41.314074: step 8370, loss = 0.98 (957.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:34:42.676090: step 8380, loss = 1.01 (939.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:34:43.989996: step 8390, loss = 0.75 (974.2 examples/sec; 0.131 sec/batch)
2017-05-07 19:34:45.483721: step 8400, loss = 1.00 (856.9 examples/sec; 0.149 sec/batch)
2017-05-07 19:34:46.687985: step 8410, loss = 1.01 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-07 19:34:48.036507: step 8420, loss = 1.08 (949.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:34:49.402932: step 8430, loss = 0.87 (936.7 examples/sec; 0.137 sec/batch)
2017-05-07 19:34:50.728705: step 8440, loss = 0.78 (965.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:34:52.120174: step 8450, loss = 0.91 (919.9 examples/sec; 0.139 sec/batch)
2017-05-07 19:34:53.422829: step 8460, loss = 0.79 (982.6 examples/sec; 0.130 sec/batch)
2017-05-07 19:34:54.773181: step 8470, loss = 0.76 (947.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:34:56.121029: step 8480, loss = 0.88 (949.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:34:57.462173: step 8490, loss = 0.72 (954.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:34:58.959736: step 8500, loss = 0.94 (854.7 examples/sec; 0.150 sec/batch)
2017-05-07 19:35:00.178668: step 8510, loss = 0.86 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-07 19:35:01.507828: step 8520, loss = 0.94 (963.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:35:02.852951: step 8530, loss = 0.61 (951.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:35:04.226072: step 8540, loss = 0.95 (932.2 examples/sec; 0.137 sec/batch)
2017-05-07 19:35:05.617972: step 8550, loss = 0.96 (919.6 examples/sec; 0.139 sec/batch)
2017-05-07 19:35:06.929543: step 8560, loss = 0.89 (975.9 examples/sec; 0.131 sec/batch)
2017-05-07 19:35:08.286267: step 8570, loss = 0.90 (943.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:35:09.620333: step 8580, loss = 0.83 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:35:10.945685: step 8590, loss = 0.90 (965.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:35:12.440343: step 8600, loss = 0.73 (856.4 examples/sec; 0.149 sec/batch)
2017-05-07 19:35:13.639961: step 8610, loss = 0.95 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-07 19:35:14.965861: step 8620, loss = 0.73 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:35:16.306704: step 8630, loss = 0.93 (954.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:35:17.647586: step 8640, loss = 0.79 (954.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:35:19.044087: step 8650, loss = 0.89 (916.6 examples/sec; 0.140 sec/batch)
2017-05-07 19:35:20.339622: step 8660, loss = 0.82 (988.0 examples/sec; 0.130 sec/batch)
2017-05-07 19:35:21.685032: step 8670, loss = 0.85 (951.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:35:23.027631: step 8680, loss = 0.84 (953.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:35:24.356927: step 8690, loss = 1.07 (962.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:35:25.848860: step 8700, loss = 0.91 (857.9 examples/sec; 0.149 sec/batch)
2017-05-07 19:35:27.071131: step 8710, loss = 0.74 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-07 19:35:28.443791: step 8720, loss = 0.90 (932.5 examples/sec; 0.137 sec/batchE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 184 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
)
2017-05-07 19:35:29.782620: step 8730, loss = 0.78 (956.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:35:31.141027: step 8740, loss = 0.72 (942.3 examples/sec; 0.136 sec/batch)
2017-05-07 19:35:32.515186: step 8750, loss = 0.76 (931.5 examples/sec; 0.137 sec/batch)
2017-05-07 19:35:33.828262: step 8760, loss = 1.13 (974.8 examples/sec; 0.131 sec/batch)
2017-05-07 19:35:35.176245: step 8770, loss = 0.82 (949.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:35:36.515930: step 8780, loss = 0.92 (955.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:35:37.837798: step 8790, loss = 0.83 (968.3 examples/sec; 0.132 sec/batch)
2017-05-07 19:35:39.314787: step 8800, loss = 0.71 (866.6 examples/sec; 0.148 sec/batch)
2017-05-07 19:35:40.495163: step 8810, loss = 0.82 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-07 19:35:41.831433: step 8820, loss = 0.80 (957.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:35:43.157015: step 8830, loss = 0.99 (965.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:35:44.507898: step 8840, loss = 0.76 (947.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:35:45.897215: step 8850, loss = 0.87 (921.3 examples/sec; 0.139 sec/batch)
2017-05-07 19:35:47.195106: step 8860, loss = 0.86 (986.2 examples/sec; 0.130 sec/batch)
2017-05-07 19:35:48.530640: step 8870, loss = 0.89 (958.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:35:49.859260: step 8880, loss = 0.87 (963.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:35:51.209208: step 8890, loss = 0.87 (948.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:35:52.702321: step 8900, loss = 0.91 (857.3 examples/sec; 0.149 sec/batch)
2017-05-07 19:35:53.997910: step 8910, loss = 0.79 (988.0 examples/sec; 0.130 sec/batch)
2017-05-07 19:35:55.359250: step 8920, loss = 0.84 (940.2 examples/sec; 0.136 sec/batch)
2017-05-07 19:35:56.652223: step 8930, loss = 0.86 (990.0 examples/sec; 0.129 sec/batch)
2017-05-07 19:35:57.995138: step 8940, loss = 0.74 (953.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:35:59.417470: step 8950, loss = 0.74 (899.9 examples/sec; 0.142 sec/batch)
2017-05-07 19:36:00.706315: step 8960, loss = 0.92 (993.1 examples/sec; 0.129 sec/batch)
2017-05-07 19:36:02.062740: step 8970, loss = 0.72 (943.7 examples/sec; 0.136 sec/batch)
2017-05-07 19:36:03.398342: step 8980, loss = 0.99 (958.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:36:04.746862: step 8990, loss = 0.91 (949.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:36:06.267234: step 9000, loss = 0.88 (841.9 examples/sec; 0.152 sec/batch)
2017-05-07 19:36:07.457166: step 9010, loss = 0.79 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-07 19:36:08.785937: step 9020, loss = 1.01 (963.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:36:10.103276: step 9030, loss = 0.80 (971.7 examples/sec; 0.132 sec/batch)
2017-05-07 19:36:11.435862: step 9040, loss = 0.78 (960.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:36:12.848632: step 9050, loss = 0.67 (906.0 examples/sec; 0.141 sec/batch)
2017-05-07 19:36:14.189449: step 9060, loss = 0.75 (954.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:36:15.536996: step 9070, loss = 0.98 (949.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:36:16.855059: step 9080, loss = 1.18 (971.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:36:18.231475: step 9090, loss = 0.68 (930.0 examples/sec; 0.138 sec/batch)
2017-05-07 19:36:19.728432: step 9100, loss = 0.80 (855.1 examples/sec; 0.150 sec/batch)
2017-05-07 19:36:20.914969: step 9110, loss = 0.98 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-07 19:36:22.270896: step 9120, loss = 0.90 (944.0 examples/sec; 0.136 sec/batch)
2017-05-07 19:36:23.620522: step 9130, loss = 0.92 (948.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:36:24.970102: step 9140, loss = 0.85 (948.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:36:26.362620: step 9150, loss = 0.68 (919.2 examples/sec; 0.139 sec/batch)
2017-05-07 19:36:27.692731: step 9160, loss = 0.96 (962.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:36:29.033383: step 9170, loss = 0.93 (954.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:36:30.376129: step 9180, loss = 1.02 (953.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:36:31.708088: step 9190, loss = 0.78 (961.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:36:33.171881: step 9200, loss = 1.02 (874.4 examples/sec; 0.146 sec/batch)
2017-05-07 19:36:34.380051: step 9210, loss = 0.68 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-07 19:36:35.747214: step 9220, loss = 0.72 (936.2 examples/sec; 0.137 sec/batch)
2017-05-07 19:36:37.097645: step 9230, loss = 0.70 (947.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:36:38.436078: step 9240, loss = 0.99 (956.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:36:39.819534: step 9250, loss = 0.93 (925.2 examples/sec; 0.138 sec/batch)
2017-05-07 19:36:41.148415: step 9260, loss = 0.95 (963.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:36:42.475771: step 9270, loss = 0.99 (964.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:36:43.823541: step 9280, loss = 0.79 (949.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:36:45.140120: step 9290, loss = 1.07 (972.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:36:46.630905: step 9300, loss = 0.73 (858.6 examples/sec; 0.149 sec/batch)
2017-05-07 19:36:47.867719: step 9310, loss = 0.70 (1034.9 examples/sec; 0.124 sec/batch)
2017-05-07 19:36:49.195186: step 9320, loss = 0.84 (964.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:36:50.541308: step 9330, loss = 1.35 (950.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:36:51.891858: step 9340, loss = 0.91 (947.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:36:53.278962: step 9350, loss = 0.84 (922.8 examples/sec; 0.139 sec/batch)
2017-05-07 19:36:54.598683: step 9360, loss = 0.89 (969.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:36:55.933800: step 9370, loss = 0.89 (958.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:36:57.265166: step 9380, loss = 0.73 (961.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:36:58.633166: step 9390, loss = 0.82 (935.7 examples/sec; 0.137 sec/batch)
2017-05-07 19:37:00.129966: step 9400, loss = 1.13 (855.2 examples/sec; 0.150 sec/batch)
2017-05-07 19:37:01.337970: step 9410, loss = 0.85 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-07 19:37:02.693498: step 9420, loss = 0.84 (944.3 examples/sec; 0.136 sec/batch)
2017-05-07 19:37:04.038113: step 9430, loss = 0.97 (951.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:37:05.376330: step 9440, loss = 0.87 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:37:06.770624: step 9450, loss = 0.84 (918.0 examples/sec; 0.139 sec/batch)
2017-05-07 19:37:08.104807: step 9460, loss = 0.85 (959.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:37:09.433446: step 9470, loss = 0.80 (963.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:37:10.762786: step 9480, loss = 0.94 (962.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:37:12.113866: step 9490, loss = 0.77 (947.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:37:13.578077: step 9500, loss = 0.90 (874.2 examples/sec; 0.146 sec/batch)
2017-05-07 19:37:14.781398: step 9510, loss = 0.98 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-07 19:37:16.115092: step 9520, loss = 0.82 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:37:17.446797: step 9530, loss = 0.77 (961.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:37:18.795562: step 9540, loss = 0.81 (949.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:37:20.198506: step 9550, loss = 0.89 (912.4 examples/sec; 0.140 sec/batch)
2017-05-07 19:37:21.480910: step 9560, loss = 0.96 (998.1 examples/sec; 0.128 sec/batch)
2017-05-07 19:37:22.806560: step 9570, loss = 0.87 (965.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:37:24.169468: step 9580, loss = 0.97 (939.2 examples/sec; 0.136 sec/batch)
2017-05-07 19:37:25.486103: step 9590, loss = 0.93 (972.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:37:26.961745: step 9600, loss = 0.84 (867.4 examples/sec; 0.148 sec/batch)
2017-05-07 19:37:28.173776: step 9610, loss = 0.87 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-07 19:37:29.528343: step 9620, loss = 0.88 (945.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:37:30.853127: step 9630, loss = 1.01 (966.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:37:32.186410: step 9640, loss = 0.82 (960.0 examples/sec; 0.133 sec/bE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 203 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
atch)
2017-05-07 19:37:33.566493: step 9650, loss = 0.93 (927.5 examples/sec; 0.138 sec/batch)
2017-05-07 19:37:34.860927: step 9660, loss = 0.89 (988.9 examples/sec; 0.129 sec/batch)
2017-05-07 19:37:36.203071: step 9670, loss = 0.95 (953.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:37:37.549487: step 9680, loss = 0.86 (950.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:37:38.881021: step 9690, loss = 1.03 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:37:40.361963: step 9700, loss = 0.83 (864.3 examples/sec; 0.148 sec/batch)
2017-05-07 19:37:41.568553: step 9710, loss = 0.96 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-07 19:37:42.913528: step 9720, loss = 0.81 (951.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:37:44.256259: step 9730, loss = 0.79 (953.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:37:45.600605: step 9740, loss = 0.91 (952.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:37:46.986591: step 9750, loss = 0.88 (923.5 examples/sec; 0.139 sec/batch)
2017-05-07 19:37:48.268440: step 9760, loss = 0.99 (998.6 examples/sec; 0.128 sec/batch)
2017-05-07 19:37:49.607640: step 9770, loss = 0.73 (955.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:37:50.943757: step 9780, loss = 0.93 (958.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:37:52.282938: step 9790, loss = 0.94 (955.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:37:53.755007: step 9800, loss = 0.87 (869.5 examples/sec; 0.147 sec/batch)
2017-05-07 19:37:54.971944: step 9810, loss = 0.64 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-07 19:37:56.312143: step 9820, loss = 0.83 (955.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:37:57.623779: step 9830, loss = 1.00 (975.9 examples/sec; 0.131 sec/batch)
2017-05-07 19:37:58.997095: step 9840, loss = 0.83 (932.1 examples/sec; 0.137 sec/batch)
2017-05-07 19:38:00.383656: step 9850, loss = 0.87 (923.2 examples/sec; 0.139 sec/batch)
2017-05-07 19:38:01.702710: step 9860, loss = 0.84 (970.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:38:03.051252: step 9870, loss = 0.96 (949.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:38:04.396910: step 9880, loss = 0.80 (951.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:38:05.731901: step 9890, loss = 0.91 (958.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:38:07.222933: step 9900, loss = 0.71 (858.5 examples/sec; 0.149 sec/batch)
2017-05-07 19:38:08.410680: step 9910, loss = 0.77 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-07 19:38:09.740625: step 9920, loss = 0.82 (962.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:38:11.082149: step 9930, loss = 1.00 (954.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:38:12.428087: step 9940, loss = 0.78 (951.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:38:13.829537: step 9950, loss = 1.08 (913.3 examples/sec; 0.140 sec/batch)
2017-05-07 19:38:15.138064: step 9960, loss = 0.88 (978.2 examples/sec; 0.131 sec/batch)
2017-05-07 19:38:16.490603: step 9970, loss = 1.06 (946.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:38:17.868151: step 9980, loss = 0.95 (929.2 examples/sec; 0.138 sec/batch)
2017-05-07 19:38:19.215821: step 9990, loss = 0.79 (949.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:38:20.675234: step 10000, loss = 0.86 (877.1 examples/sec; 0.146 sec/batch)
2017-05-07 19:38:21.872407: step 10010, loss = 0.79 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-07 19:38:23.213829: step 10020, loss = 0.64 (954.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:38:24.560762: step 10030, loss = 0.78 (950.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:38:25.929783: step 10040, loss = 0.92 (935.0 examples/sec; 0.137 sec/batch)
2017-05-07 19:38:27.334248: step 10050, loss = 0.89 (911.4 examples/sec; 0.140 sec/batch)
2017-05-07 19:38:28.595910: step 10060, loss = 0.86 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-07 19:38:29.941525: step 10070, loss = 0.89 (951.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:38:31.313701: step 10080, loss = 0.84 (932.8 examples/sec; 0.137 sec/batch)
2017-05-07 19:38:32.649398: step 10090, loss = 0.98 (958.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:38:34.161077: step 10100, loss = 0.83 (846.7 examples/sec; 0.151 sec/batch)
2017-05-07 19:38:35.340630: step 10110, loss = 1.09 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-07 19:38:36.703193: step 10120, loss = 0.94 (939.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:38:38.056956: step 10130, loss = 1.05 (945.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:38:39.433297: step 10140, loss = 0.94 (930.0 examples/sec; 0.138 sec/batch)
2017-05-07 19:38:40.808302: step 10150, loss = 0.90 (930.9 examples/sec; 0.138 sec/batch)
2017-05-07 19:38:42.104665: step 10160, loss = 0.84 (987.4 examples/sec; 0.130 sec/batch)
2017-05-07 19:38:43.437846: step 10170, loss = 0.76 (960.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:38:44.768722: step 10180, loss = 0.78 (961.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:38:46.133532: step 10190, loss = 1.00 (937.9 examples/sec; 0.136 sec/batch)
2017-05-07 19:38:47.619789: step 10200, loss = 0.92 (861.2 examples/sec; 0.149 sec/batch)
2017-05-07 19:38:48.828273: step 10210, loss = 1.14 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-07 19:38:50.162048: step 10220, loss = 0.75 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:38:51.503829: step 10230, loss = 0.75 (953.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:38:52.845699: step 10240, loss = 0.87 (953.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:38:54.241767: step 10250, loss = 0.91 (916.9 examples/sec; 0.140 sec/batch)
2017-05-07 19:38:55.540699: step 10260, loss = 0.93 (985.4 examples/sec; 0.130 sec/batch)
2017-05-07 19:38:56.878978: step 10270, loss = 0.82 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:38:58.231516: step 10280, loss = 0.96 (946.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:38:59.582096: step 10290, loss = 0.89 (947.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:39:01.047030: step 10300, loss = 1.00 (873.8 examples/sec; 0.146 sec/batch)
2017-05-07 19:39:02.234860: step 10310, loss = 0.86 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-07 19:39:03.580874: step 10320, loss = 0.97 (951.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:39:04.919910: step 10330, loss = 1.10 (955.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:39:06.270267: step 10340, loss = 0.75 (947.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:39:07.671775: step 10350, loss = 0.87 (913.3 examples/sec; 0.140 sec/batch)
2017-05-07 19:39:08.996309: step 10360, loss = 0.92 (966.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:39:10.363510: step 10370, loss = 0.85 (936.2 examples/sec; 0.137 sec/batch)
2017-05-07 19:39:11.698964: step 10380, loss = 0.70 (958.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:39:13.052213: step 10390, loss = 0.77 (945.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:39:14.514257: step 10400, loss = 0.94 (875.5 examples/sec; 0.146 sec/batch)
2017-05-07 19:39:15.684241: step 10410, loss = 0.78 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-07 19:39:17.061963: step 10420, loss = 0.92 (929.1 examples/sec; 0.138 sec/batch)
2017-05-07 19:39:18.371059: step 10430, loss = 0.94 (977.8 examples/sec; 0.131 sec/batch)
2017-05-07 19:39:19.710245: step 10440, loss = 0.75 (955.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:39:21.089939: step 10450, loss = 1.05 (927.7 examples/sec; 0.138 sec/batch)
2017-05-07 19:39:22.383014: step 10460, loss = 0.87 (989.9 examples/sec; 0.129 sec/batch)
2017-05-07 19:39:23.714927: step 10470, loss = 0.98 (961.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:39:25.066314: step 10480, loss = 0.76 (947.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:39:26.408765: step 10490, loss = 0.90 (953.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:39:27.878739: step 10500, loss = 0.88 (870.6 examples/sec; 0.147 sec/batch)
2017-05-07 19:39:29.105720: step 10510, loss = 0.97 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-07 19:39:30.467294: step 10520, loss = 0.78 (940.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:39:31.796572: step 10530, loss = 0.83 (962.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:39:33.135863: step 10540, loss = 0.79 (955.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:39:34.523215: step 10550, loss = 0.79 (922.6 examples/sec; 0.139 sec/batch)
2017-05-07 19:39:35.8E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 221 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
12939: step 10560, loss = 1.06 (992.5 examples/sec; 0.129 sec/batch)
2017-05-07 19:39:37.154754: step 10570, loss = 1.26 (953.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:39:38.492984: step 10580, loss = 0.93 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:39:39.819450: step 10590, loss = 0.76 (965.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:39:41.318149: step 10600, loss = 0.79 (854.1 examples/sec; 0.150 sec/batch)
2017-05-07 19:39:42.534633: step 10610, loss = 0.82 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-07 19:39:43.873578: step 10620, loss = 0.89 (956.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:39:45.206355: step 10630, loss = 0.92 (960.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:39:46.535055: step 10640, loss = 1.06 (963.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:39:47.932052: step 10650, loss = 1.04 (916.3 examples/sec; 0.140 sec/batch)
2017-05-07 19:39:49.218041: step 10660, loss = 0.94 (995.3 examples/sec; 0.129 sec/batch)
2017-05-07 19:39:50.569329: step 10670, loss = 0.77 (947.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:39:51.921128: step 10680, loss = 0.95 (946.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:39:53.270390: step 10690, loss = 0.86 (948.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:39:54.720141: step 10700, loss = 0.76 (882.9 examples/sec; 0.145 sec/batch)
2017-05-07 19:39:55.942204: step 10710, loss = 0.84 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-07 19:39:57.278651: step 10720, loss = 0.82 (957.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:39:58.619243: step 10730, loss = 0.80 (954.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:39:59.972097: step 10740, loss = 0.72 (946.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:40:01.358489: step 10750, loss = 0.91 (923.3 examples/sec; 0.139 sec/batch)
2017-05-07 19:40:02.660592: step 10760, loss = 0.88 (983.0 examples/sec; 0.130 sec/batch)
2017-05-07 19:40:04.020384: step 10770, loss = 0.97 (941.3 examples/sec; 0.136 sec/batch)
2017-05-07 19:40:05.353902: step 10780, loss = 0.77 (959.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:40:06.697580: step 10790, loss = 0.78 (952.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:40:08.174548: step 10800, loss = 0.79 (866.6 examples/sec; 0.148 sec/batch)
2017-05-07 19:40:09.352721: step 10810, loss = 0.89 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-07 19:40:10.699436: step 10820, loss = 0.88 (950.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:40:12.034371: step 10830, loss = 0.80 (958.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:40:13.385004: step 10840, loss = 0.73 (947.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:40:14.768985: step 10850, loss = 0.76 (924.9 examples/sec; 0.138 sec/batch)
2017-05-07 19:40:16.072807: step 10860, loss = 0.99 (981.7 examples/sec; 0.130 sec/batch)
2017-05-07 19:40:17.435184: step 10870, loss = 0.87 (939.5 examples/sec; 0.136 sec/batch)
2017-05-07 19:40:18.766195: step 10880, loss = 0.90 (961.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:40:20.138021: step 10890, loss = 0.96 (933.1 examples/sec; 0.137 sec/batch)
2017-05-07 19:40:21.604306: step 10900, loss = 0.73 (873.0 examples/sec; 0.147 sec/batch)
2017-05-07 19:40:22.792543: step 10910, loss = 0.88 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-07 19:40:24.153683: step 10920, loss = 0.78 (940.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:40:25.484261: step 10930, loss = 0.73 (962.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:40:26.816854: step 10940, loss = 0.84 (960.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:40:28.218466: step 10950, loss = 0.97 (913.2 examples/sec; 0.140 sec/batch)
2017-05-07 19:40:29.524012: step 10960, loss = 0.84 (980.4 examples/sec; 0.131 sec/batch)
2017-05-07 19:40:30.864946: step 10970, loss = 0.86 (954.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:40:32.221307: step 10980, loss = 0.84 (943.7 examples/sec; 0.136 sec/batch)
2017-05-07 19:40:33.550502: step 10990, loss = 0.83 (963.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:40:35.015619: step 11000, loss = 1.02 (873.6 examples/sec; 0.147 sec/batch)
2017-05-07 19:40:36.242809: step 11010, loss = 0.76 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-07 19:40:37.581594: step 11020, loss = 0.77 (956.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:40:38.923985: step 11030, loss = 0.96 (953.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:40:40.266201: step 11040, loss = 1.02 (953.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:40:41.623901: step 11050, loss = 0.87 (942.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:40:42.930731: step 11060, loss = 0.74 (979.5 examples/sec; 0.131 sec/batch)
2017-05-07 19:40:44.276400: step 11070, loss = 0.81 (951.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:40:45.595401: step 11080, loss = 0.81 (970.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:40:46.952649: step 11090, loss = 0.84 (943.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:40:48.429411: step 11100, loss = 0.70 (866.8 examples/sec; 0.148 sec/batch)
2017-05-07 19:40:49.630597: step 11110, loss = 0.97 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-07 19:40:50.967186: step 11120, loss = 0.80 (957.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:40:52.327064: step 11130, loss = 0.85 (941.3 examples/sec; 0.136 sec/batch)
2017-05-07 19:40:53.656087: step 11140, loss = 0.85 (963.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:40:55.040382: step 11150, loss = 0.95 (924.7 examples/sec; 0.138 sec/batch)
2017-05-07 19:40:56.374957: step 11160, loss = 0.93 (959.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:40:57.696887: step 11170, loss = 0.80 (968.3 examples/sec; 0.132 sec/batch)
2017-05-07 19:40:59.034250: step 11180, loss = 0.89 (957.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:41:00.372321: step 11190, loss = 0.88 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:41:01.886683: step 11200, loss = 0.79 (845.2 examples/sec; 0.151 sec/batch)
2017-05-07 19:41:03.056391: step 11210, loss = 1.05 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-07 19:41:04.363569: step 11220, loss = 0.79 (979.2 examples/sec; 0.131 sec/batch)
2017-05-07 19:41:05.705876: step 11230, loss = 0.97 (953.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:41:07.040145: step 11240, loss = 0.83 (959.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:41:08.438404: step 11250, loss = 0.93 (915.4 examples/sec; 0.140 sec/batch)
2017-05-07 19:41:09.729006: step 11260, loss = 0.82 (991.8 examples/sec; 0.129 sec/batch)
2017-05-07 19:41:11.073944: step 11270, loss = 0.77 (951.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:41:12.414229: step 11280, loss = 0.84 (955.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:41:13.751431: step 11290, loss = 0.75 (957.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:41:15.234150: step 11300, loss = 0.72 (863.3 examples/sec; 0.148 sec/batch)
2017-05-07 19:41:16.451101: step 11310, loss = 0.83 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-07 19:41:17.774544: step 11320, loss = 0.74 (967.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:41:19.107837: step 11330, loss = 0.89 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:41:20.448520: step 11340, loss = 0.83 (954.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:41:21.833992: step 11350, loss = 0.84 (923.9 examples/sec; 0.139 sec/batch)
2017-05-07 19:41:23.141858: step 11360, loss = 0.85 (978.7 examples/sec; 0.131 sec/batch)
2017-05-07 19:41:24.486187: step 11370, loss = 0.88 (952.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:41:25.822552: step 11380, loss = 0.88 (957.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:41:27.184825: step 11390, loss = 0.88 (939.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:41:28.649775: step 11400, loss = 0.81 (873.8 examples/sec; 0.146 sec/batch)
2017-05-07 19:41:29.823907: step 11410, loss = 0.85 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-07 19:41:31.158568: step 11420, loss = 0.81 (959.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:41:32.516976: step 11430, loss = 0.88 (942.3 examples/sec; 0.136 sec/batch)
2017-05-07 19:41:33.888152: step 11440, loss = 0.89 (933.5 examples/sec; 0.137 sec/batch)
2017-05-07 19:41:35.267383: step 11450, loss = 0.95 (928.1 examples/sec; 0.138 sec/batch)
2017-05-07 19:41:36.585829: step 11460, loss = 0.89 (970.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 239 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
41:37.911722: step 11470, loss = 0.97 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:41:39.266044: step 11480, loss = 0.84 (945.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:41:40.601986: step 11490, loss = 0.89 (958.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:41:42.076915: step 11500, loss = 0.89 (867.8 examples/sec; 0.147 sec/batch)
2017-05-07 19:41:43.263626: step 11510, loss = 0.91 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-07 19:41:44.613840: step 11520, loss = 0.82 (948.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:41:45.960969: step 11530, loss = 0.76 (950.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:41:47.292940: step 11540, loss = 1.03 (961.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:41:48.685558: step 11550, loss = 0.82 (919.1 examples/sec; 0.139 sec/batch)
2017-05-07 19:41:49.979477: step 11560, loss = 0.95 (989.3 examples/sec; 0.129 sec/batch)
2017-05-07 19:41:51.320423: step 11570, loss = 0.76 (954.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:41:52.659247: step 11580, loss = 0.87 (956.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:41:54.015573: step 11590, loss = 0.89 (943.7 examples/sec; 0.136 sec/batch)
2017-05-07 19:41:55.520520: step 11600, loss = 0.78 (850.5 examples/sec; 0.150 sec/batch)
2017-05-07 19:41:56.715840: step 11610, loss = 0.83 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-07 19:41:58.077719: step 11620, loss = 0.80 (939.9 examples/sec; 0.136 sec/batch)
2017-05-07 19:41:59.426232: step 11630, loss = 0.93 (949.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:42:00.753570: step 11640, loss = 0.93 (964.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:42:02.140258: step 11650, loss = 1.05 (923.1 examples/sec; 0.139 sec/batch)
2017-05-07 19:42:03.431854: step 11660, loss = 0.70 (991.0 examples/sec; 0.129 sec/batch)
2017-05-07 19:42:04.760334: step 11670, loss = 0.89 (963.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:42:06.104072: step 11680, loss = 0.92 (952.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:42:07.467084: step 11690, loss = 0.95 (939.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:42:08.957224: step 11700, loss = 0.92 (859.0 examples/sec; 0.149 sec/batch)
2017-05-07 19:42:10.161613: step 11710, loss = 0.96 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-07 19:42:11.492445: step 11720, loss = 0.78 (961.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:42:12.846994: step 11730, loss = 0.76 (945.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:42:14.198907: step 11740, loss = 0.81 (946.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:42:15.572273: step 11750, loss = 0.85 (932.0 examples/sec; 0.137 sec/batch)
2017-05-07 19:42:16.872100: step 11760, loss = 0.83 (984.7 examples/sec; 0.130 sec/batch)
2017-05-07 19:42:18.219437: step 11770, loss = 1.09 (950.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:42:19.569564: step 11780, loss = 0.82 (948.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:42:20.909796: step 11790, loss = 0.83 (955.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:42:22.385959: step 11800, loss = 0.97 (867.1 examples/sec; 0.148 sec/batch)
2017-05-07 19:42:23.601765: step 11810, loss = 0.82 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-07 19:42:24.935680: step 11820, loss = 0.78 (959.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:42:26.281119: step 11830, loss = 0.91 (951.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:42:27.624752: step 11840, loss = 0.79 (952.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:42:29.001965: step 11850, loss = 0.91 (929.4 examples/sec; 0.138 sec/batch)
2017-05-07 19:42:30.312617: step 11860, loss = 0.82 (976.6 examples/sec; 0.131 sec/batch)
2017-05-07 19:42:31.618717: step 11870, loss = 0.85 (980.0 examples/sec; 0.131 sec/batch)
2017-05-07 19:42:32.941209: step 11880, loss = 1.07 (967.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:42:34.273813: step 11890, loss = 0.79 (960.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:42:35.782934: step 11900, loss = 0.76 (848.2 examples/sec; 0.151 sec/batch)
2017-05-07 19:42:36.957658: step 11910, loss = 0.89 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-07 19:42:38.312710: step 11920, loss = 0.82 (944.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:42:39.632162: step 11930, loss = 0.90 (970.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:42:40.958084: step 11940, loss = 0.79 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:42:42.354581: step 11950, loss = 1.01 (916.6 examples/sec; 0.140 sec/batch)
2017-05-07 19:42:43.658066: step 11960, loss = 1.02 (982.0 examples/sec; 0.130 sec/batch)
2017-05-07 19:42:44.981812: step 11970, loss = 0.98 (967.0 examples/sec; 0.132 sec/batch)
2017-05-07 19:42:46.328566: step 11980, loss = 0.89 (950.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:42:47.689952: step 11990, loss = 0.74 (940.2 examples/sec; 0.136 sec/batch)
2017-05-07 19:42:49.176209: step 12000, loss = 1.05 (861.2 examples/sec; 0.149 sec/batch)
2017-05-07 19:42:50.382875: step 12010, loss = 0.86 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-07 19:42:51.701325: step 12020, loss = 0.99 (970.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:42:53.048116: step 12030, loss = 0.76 (950.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:42:54.384877: step 12040, loss = 0.74 (957.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:42:55.757910: step 12050, loss = 0.85 (932.2 examples/sec; 0.137 sec/batch)
2017-05-07 19:42:57.059128: step 12060, loss = 0.71 (983.7 examples/sec; 0.130 sec/batch)
2017-05-07 19:42:58.390115: step 12070, loss = 0.76 (961.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:42:59.735928: step 12080, loss = 0.81 (951.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:43:01.102099: step 12090, loss = 0.96 (936.9 examples/sec; 0.137 sec/batch)
2017-05-07 19:43:02.598946: step 12100, loss = 0.81 (855.1 examples/sec; 0.150 sec/batch)
2017-05-07 19:43:03.786403: step 12110, loss = 0.91 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-07 19:43:05.114622: step 12120, loss = 0.97 (963.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:43:06.467625: step 12130, loss = 0.69 (946.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:43:07.785214: step 12140, loss = 0.99 (971.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:43:09.186546: step 12150, loss = 0.85 (913.4 examples/sec; 0.140 sec/batch)
2017-05-07 19:43:10.500947: step 12160, loss = 0.84 (973.8 examples/sec; 0.131 sec/batch)
2017-05-07 19:43:11.811613: step 12170, loss = 0.75 (976.6 examples/sec; 0.131 sec/batch)
2017-05-07 19:43:13.144630: step 12180, loss = 0.80 (960.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:43:14.491430: step 12190, loss = 0.92 (950.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:43:16.011540: step 12200, loss = 0.77 (842.0 examples/sec; 0.152 sec/batch)
2017-05-07 19:43:17.181563: step 12210, loss = 0.94 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-07 19:43:18.535331: step 12220, loss = 0.88 (945.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:43:19.888320: step 12230, loss = 0.78 (946.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:43:21.211090: step 12240, loss = 0.96 (967.7 examples/sec; 0.132 sec/batch)
2017-05-07 19:43:22.592698: step 12250, loss = 0.99 (926.5 examples/sec; 0.138 sec/batch)
2017-05-07 19:43:23.895444: step 12260, loss = 0.82 (982.5 examples/sec; 0.130 sec/batch)
2017-05-07 19:43:25.225120: step 12270, loss = 0.81 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:43:26.547757: step 12280, loss = 0.95 (967.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:43:27.882962: step 12290, loss = 0.87 (958.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:43:29.360631: step 12300, loss = 0.74 (866.2 examples/sec; 0.148 sec/batch)
2017-05-07 19:43:30.601313: step 12310, loss = 0.72 (1031.7 examples/sec; 0.124 sec/batch)
2017-05-07 19:43:31.936662: step 12320, loss = 0.87 (958.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:43:33.253527: step 12330, loss = 0.74 (972.0 examples/sec; 0.132 sec/batch)
2017-05-07 19:43:34.578954: step 12340, loss = 0.79 (965.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:43:35.956220: step 12350, loss = 0.73 (929.4 examples/sec; 0.138 sec/batch)
2017-05-07 19:43:37.247504: step 12360, loss = 0.99 (991.3 examples/sec; 0.129 sec/batch)
2017-05-07 19:43:38.566120: step 12370, loss = 0.90 (970.7 examples/sec; 0.132 sec/batch)
2017-05E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 257 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
-07 19:43:39.919348: step 12380, loss = 0.80 (945.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:43:41.278587: step 12390, loss = 0.67 (941.7 examples/sec; 0.136 sec/batch)
2017-05-07 19:43:42.774595: step 12400, loss = 0.82 (855.6 examples/sec; 0.150 sec/batch)
2017-05-07 19:43:43.967829: step 12410, loss = 0.75 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-07 19:43:45.297688: step 12420, loss = 0.83 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:43:46.641367: step 12430, loss = 0.80 (952.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:43:47.983371: step 12440, loss = 0.81 (953.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:43:49.370299: step 12450, loss = 0.74 (922.9 examples/sec; 0.139 sec/batch)
2017-05-07 19:43:50.662864: step 12460, loss = 0.94 (990.3 examples/sec; 0.129 sec/batch)
2017-05-07 19:43:51.994572: step 12470, loss = 0.81 (961.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:43:53.306554: step 12480, loss = 0.82 (975.6 examples/sec; 0.131 sec/batch)
2017-05-07 19:43:54.651599: step 12490, loss = 0.82 (951.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:43:56.126821: step 12500, loss = 0.63 (867.7 examples/sec; 0.148 sec/batch)
2017-05-07 19:43:57.324723: step 12510, loss = 0.79 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-07 19:43:58.683846: step 12520, loss = 0.87 (941.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:44:00.040354: step 12530, loss = 0.98 (943.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:44:01.358955: step 12540, loss = 0.67 (970.7 examples/sec; 0.132 sec/batch)
2017-05-07 19:44:02.747524: step 12550, loss = 0.88 (921.8 examples/sec; 0.139 sec/batch)
2017-05-07 19:44:04.046030: step 12560, loss = 0.84 (985.7 examples/sec; 0.130 sec/batch)
2017-05-07 19:44:05.361327: step 12570, loss = 0.62 (973.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:44:06.711704: step 12580, loss = 0.83 (947.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:44:08.041386: step 12590, loss = 0.77 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:44:09.500626: step 12600, loss = 0.77 (877.2 examples/sec; 0.146 sec/batch)
2017-05-07 19:44:10.689179: step 12610, loss = 0.90 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-07 19:44:12.022590: step 12620, loss = 0.75 (959.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:44:13.355490: step 12630, loss = 0.68 (960.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:44:14.708970: step 12640, loss = 0.92 (945.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:44:16.082803: step 12650, loss = 0.90 (931.7 examples/sec; 0.137 sec/batch)
2017-05-07 19:44:17.399068: step 12660, loss = 0.73 (972.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:44:18.751918: step 12670, loss = 0.82 (946.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:44:20.070144: step 12680, loss = 0.79 (971.0 examples/sec; 0.132 sec/batch)
2017-05-07 19:44:21.411902: step 12690, loss = 0.81 (954.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:44:22.879887: step 12700, loss = 0.75 (871.9 examples/sec; 0.147 sec/batch)
2017-05-07 19:44:24.080335: step 12710, loss = 0.79 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-07 19:44:25.426708: step 12720, loss = 0.95 (950.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:44:26.760608: step 12730, loss = 0.96 (959.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:44:28.104599: step 12740, loss = 1.08 (952.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:44:29.484065: step 12750, loss = 1.11 (927.9 examples/sec; 0.138 sec/batch)
2017-05-07 19:44:30.793936: step 12760, loss = 0.79 (977.2 examples/sec; 0.131 sec/batch)
2017-05-07 19:44:32.120025: step 12770, loss = 0.85 (965.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:44:33.439206: step 12780, loss = 0.87 (970.3 examples/sec; 0.132 sec/batch)
2017-05-07 19:44:34.782350: step 12790, loss = 0.84 (953.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:44:36.251963: step 12800, loss = 0.68 (871.0 examples/sec; 0.147 sec/batch)
2017-05-07 19:44:37.436780: step 12810, loss = 0.82 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-07 19:44:38.795956: step 12820, loss = 0.91 (941.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:44:40.122639: step 12830, loss = 0.94 (964.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:44:41.446526: step 12840, loss = 1.05 (966.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:44:42.856715: step 12850, loss = 0.94 (907.7 examples/sec; 0.141 sec/batch)
2017-05-07 19:44:44.144156: step 12860, loss = 0.96 (994.2 examples/sec; 0.129 sec/batch)
2017-05-07 19:44:45.462136: step 12870, loss = 0.87 (971.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:44:46.788931: step 12880, loss = 0.78 (964.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:44:48.124534: step 12890, loss = 0.81 (958.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:44:49.594972: step 12900, loss = 1.04 (870.5 examples/sec; 0.147 sec/batch)
2017-05-07 19:44:50.807764: step 12910, loss = 0.82 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-07 19:44:52.136189: step 12920, loss = 1.14 (963.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:44:53.470155: step 12930, loss = 0.76 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:44:54.809856: step 12940, loss = 0.91 (955.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:44:56.185910: step 12950, loss = 0.96 (930.2 examples/sec; 0.138 sec/batch)
2017-05-07 19:44:57.486273: step 12960, loss = 0.85 (984.3 examples/sec; 0.130 sec/batch)
2017-05-07 19:44:58.806675: step 12970, loss = 0.96 (969.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:45:00.138805: step 12980, loss = 0.75 (960.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:45:01.482757: step 12990, loss = 0.95 (952.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:45:02.955700: step 13000, loss = 0.78 (869.0 examples/sec; 0.147 sec/batch)
2017-05-07 19:45:04.155579: step 13010, loss = 0.82 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-07 19:45:05.477691: step 13020, loss = 0.83 (968.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:45:06.803026: step 13030, loss = 0.84 (965.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:45:08.134847: step 13040, loss = 0.97 (961.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:45:09.520361: step 13050, loss = 0.93 (923.8 examples/sec; 0.139 sec/batch)
2017-05-07 19:45:10.832136: step 13060, loss = 0.83 (975.8 examples/sec; 0.131 sec/batch)
2017-05-07 19:45:12.194720: step 13070, loss = 0.80 (939.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:45:13.524671: step 13080, loss = 1.01 (962.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:45:14.875938: step 13090, loss = 0.82 (947.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:45:16.344315: step 13100, loss = 0.74 (871.7 examples/sec; 0.147 sec/batch)
2017-05-07 19:45:17.558502: step 13110, loss = 0.91 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-07 19:45:18.881073: step 13120, loss = 0.78 (967.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:45:20.243535: step 13130, loss = 0.82 (939.5 examples/sec; 0.136 sec/batch)
2017-05-07 19:45:21.572647: step 13140, loss = 0.98 (963.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:45:22.943105: step 13150, loss = 0.83 (934.0 examples/sec; 0.137 sec/batch)
2017-05-07 19:45:24.228514: step 13160, loss = 0.88 (995.8 examples/sec; 0.129 sec/batch)
2017-05-07 19:45:25.555187: step 13170, loss = 0.61 (964.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:45:26.885758: step 13180, loss = 0.94 (962.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:45:28.225704: step 13190, loss = 1.08 (955.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:45:29.718016: step 13200, loss = 0.74 (857.7 examples/sec; 0.149 sec/batch)
2017-05-07 19:45:30.916570: step 13210, loss = 0.82 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-07 19:45:32.270183: step 13220, loss = 0.69 (945.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:45:33.613060: step 13230, loss = 0.75 (953.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:45:34.979855: step 13240, loss = 0.66 (936.5 examples/sec; 0.137 sec/batch)
2017-05-07 19:45:36.359924: step 13250, loss = 0.96 (927.5 examples/sec; 0.138 sec/batch)
2017-05-07 19:45:37.649217: step 13260, loss = 0.77 (992.8 examples/sec; 0.129 sec/batch)
2017-05-07 19:45:38.978232: step 13270, loss = 0.95 (963.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:45:40.305608: step 13280, loss = 0.89 (964.3 examples/sec; 0.133 sec/batch)
E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 276 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
2017-05-07 19:45:41.624379: step 13290, loss = 0.72 (970.6 examples/sec; 0.132 sec/batch)
2017-05-07 19:45:43.117314: step 13300, loss = 0.95 (857.4 examples/sec; 0.149 sec/batch)
2017-05-07 19:45:44.305425: step 13310, loss = 0.75 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-07 19:45:45.645015: step 13320, loss = 1.04 (955.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:45:46.981818: step 13330, loss = 0.89 (957.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:45:48.311583: step 13340, loss = 0.75 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:45:49.668396: step 13350, loss = 0.87 (943.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:45:50.969348: step 13360, loss = 0.69 (983.9 examples/sec; 0.130 sec/batch)
2017-05-07 19:45:52.308763: step 13370, loss = 0.85 (955.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:45:53.642866: step 13380, loss = 0.87 (959.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:45:54.972233: step 13390, loss = 0.63 (962.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:45:56.487885: step 13400, loss = 0.93 (844.5 examples/sec; 0.152 sec/batch)
2017-05-07 19:45:57.714572: step 13410, loss = 0.78 (1043.5 examples/sec; 0.123 sec/batch)
2017-05-07 19:45:59.047610: step 13420, loss = 0.88 (960.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:46:00.376221: step 13430, loss = 0.81 (963.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:46:01.748742: step 13440, loss = 0.75 (932.6 examples/sec; 0.137 sec/batch)
2017-05-07 19:46:03.113662: step 13450, loss = 0.94 (937.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:46:04.382253: step 13460, loss = 0.72 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-07 19:46:05.739314: step 13470, loss = 0.80 (943.2 examples/sec; 0.136 sec/batch)
2017-05-07 19:46:07.067160: step 13480, loss = 0.86 (964.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:46:08.394946: step 13490, loss = 0.85 (964.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:46:09.847757: step 13500, loss = 0.76 (881.0 examples/sec; 0.145 sec/batch)
2017-05-07 19:46:11.057276: step 13510, loss = 0.91 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-07 19:46:12.405218: step 13520, loss = 0.83 (949.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:46:13.732323: step 13530, loss = 0.97 (964.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:46:15.083507: step 13540, loss = 0.98 (947.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:46:16.464220: step 13550, loss = 0.82 (927.0 examples/sec; 0.138 sec/batch)
2017-05-07 19:46:17.760223: step 13560, loss = 0.68 (987.7 examples/sec; 0.130 sec/batch)
2017-05-07 19:46:19.125642: step 13570, loss = 0.84 (937.4 examples/sec; 0.137 sec/batch)
2017-05-07 19:46:20.462012: step 13580, loss = 0.83 (957.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:46:21.816113: step 13590, loss = 0.88 (945.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:46:23.317528: step 13600, loss = 1.00 (852.5 examples/sec; 0.150 sec/batch)
2017-05-07 19:46:24.480629: step 13610, loss = 0.93 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-07 19:46:25.789644: step 13620, loss = 0.83 (977.8 examples/sec; 0.131 sec/batch)
2017-05-07 19:46:27.126313: step 13630, loss = 1.04 (957.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:46:28.477663: step 13640, loss = 0.75 (947.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:46:29.854599: step 13650, loss = 0.93 (929.6 examples/sec; 0.138 sec/batch)
2017-05-07 19:46:31.142372: step 13660, loss = 0.68 (994.0 examples/sec; 0.129 sec/batch)
2017-05-07 19:46:32.457112: step 13670, loss = 0.76 (973.6 examples/sec; 0.131 sec/batch)
2017-05-07 19:46:33.793516: step 13680, loss = 0.82 (957.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:46:35.125026: step 13690, loss = 0.74 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:46:36.579787: step 13700, loss = 0.59 (879.9 examples/sec; 0.145 sec/batch)
2017-05-07 19:46:37.777797: step 13710, loss = 0.80 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-07 19:46:39.104226: step 13720, loss = 0.82 (965.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:46:40.441353: step 13730, loss = 0.87 (957.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:46:41.765169: step 13740, loss = 0.94 (966.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:46:43.154263: step 13750, loss = 0.73 (921.5 examples/sec; 0.139 sec/batch)
2017-05-07 19:46:44.464121: step 13760, loss = 1.09 (977.2 examples/sec; 0.131 sec/batch)
2017-05-07 19:46:45.797953: step 13770, loss = 0.90 (959.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:46:47.155659: step 13780, loss = 0.79 (942.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:46:48.473257: step 13790, loss = 0.90 (971.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:46:49.955202: step 13800, loss = 0.85 (863.7 examples/sec; 0.148 sec/batch)
2017-05-07 19:46:51.182095: step 13810, loss = 0.82 (1043.3 examples/sec; 0.123 sec/batch)
2017-05-07 19:46:52.527533: step 13820, loss = 0.78 (951.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:46:53.868779: step 13830, loss = 0.97 (954.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:46:55.211510: step 13840, loss = 0.68 (953.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:46:56.599333: step 13850, loss = 1.01 (922.3 examples/sec; 0.139 sec/batch)
2017-05-07 19:46:57.912199: step 13860, loss = 0.70 (975.0 examples/sec; 0.131 sec/batch)
2017-05-07 19:46:59.250485: step 13870, loss = 0.90 (956.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:47:00.596869: step 13880, loss = 0.92 (950.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:47:01.923043: step 13890, loss = 0.86 (965.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:47:03.418332: step 13900, loss = 0.88 (856.0 examples/sec; 0.150 sec/batch)
2017-05-07 19:47:04.609635: step 13910, loss = 0.93 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-07 19:47:05.934964: step 13920, loss = 0.91 (965.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:47:07.290735: step 13930, loss = 0.89 (944.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:47:08.619858: step 13940, loss = 0.84 (963.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:47:09.989882: step 13950, loss = 0.85 (934.3 examples/sec; 0.137 sec/batch)
2017-05-07 19:47:11.300996: step 13960, loss = 0.99 (976.3 examples/sec; 0.131 sec/batch)
2017-05-07 19:47:12.647707: step 13970, loss = 0.89 (950.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:47:14.009269: step 13980, loss = 0.93 (940.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:47:15.313907: step 13990, loss = 0.88 (981.1 examples/sec; 0.130 sec/batch)
2017-05-07 19:47:16.767289: step 14000, loss = 0.71 (880.7 examples/sec; 0.145 sec/batch)
2017-05-07 19:47:17.952356: step 14010, loss = 0.90 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-07 19:47:19.301950: step 14020, loss = 0.86 (948.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:47:20.646989: step 14030, loss = 0.77 (951.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:47:21.955628: step 14040, loss = 0.87 (978.1 examples/sec; 0.131 sec/batch)
2017-05-07 19:47:23.339870: step 14050, loss = 0.82 (924.7 examples/sec; 0.138 sec/batch)
2017-05-07 19:47:24.614529: step 14060, loss = 0.82 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-07 19:47:25.948009: step 14070, loss = 0.80 (959.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:47:27.307466: step 14080, loss = 1.00 (941.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:47:28.634734: step 14090, loss = 0.89 (964.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:47:30.125090: step 14100, loss = 0.90 (858.9 examples/sec; 0.149 sec/batch)
2017-05-07 19:47:31.315210: step 14110, loss = 0.82 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-07 19:47:32.627588: step 14120, loss = 0.81 (975.3 examples/sec; 0.131 sec/batch)
2017-05-07 19:47:33.960396: step 14130, loss = 0.70 (960.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:47:35.307075: step 14140, loss = 0.97 (950.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:47:36.683213: step 14150, loss = 0.83 (930.1 examples/sec; 0.138 sec/batch)
2017-05-07 19:47:37.980770: step 14160, loss = 0.86 (986.5 examples/sec; 0.130 sec/batch)
2017-05-07 19:47:39.325082: step 14170, loss = 0.84 (952.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:47:40.658351: step 14180, loss = 0.82 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:47:41.988381: step 14190, loss = 0.86 (962.4 examples/sec; 0.133 seE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 294 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
c/batch)
2017-05-07 19:47:43.466107: step 14200, loss = 0.80 (866.2 examples/sec; 0.148 sec/batch)
2017-05-07 19:47:44.681302: step 14210, loss = 0.89 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-07 19:47:46.003098: step 14220, loss = 0.82 (968.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:47:47.333803: step 14230, loss = 0.73 (961.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:47:48.656794: step 14240, loss = 0.95 (967.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:47:50.044115: step 14250, loss = 0.94 (922.6 examples/sec; 0.139 sec/batch)
2017-05-07 19:47:51.343722: step 14260, loss = 0.68 (984.9 examples/sec; 0.130 sec/batch)
2017-05-07 19:47:52.664861: step 14270, loss = 0.68 (968.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:47:54.003916: step 14280, loss = 0.86 (955.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:47:55.324603: step 14290, loss = 0.93 (969.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:47:56.816251: step 14300, loss = 0.76 (858.1 examples/sec; 0.149 sec/batch)
2017-05-07 19:47:58.011090: step 14310, loss = 0.79 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-07 19:47:59.333419: step 14320, loss = 0.98 (968.0 examples/sec; 0.132 sec/batch)
2017-05-07 19:48:00.677491: step 14330, loss = 0.75 (952.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:48:01.997769: step 14340, loss = 0.69 (969.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:48:03.392104: step 14350, loss = 0.84 (918.0 examples/sec; 0.139 sec/batch)
2017-05-07 19:48:04.661730: step 14360, loss = 0.87 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-07 19:48:06.015181: step 14370, loss = 0.79 (945.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:48:07.341261: step 14380, loss = 0.71 (965.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:48:08.666675: step 14390, loss = 0.74 (965.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:48:10.132935: step 14400, loss = 0.92 (873.0 examples/sec; 0.147 sec/batch)
2017-05-07 19:48:11.357937: step 14410, loss = 0.73 (1044.9 examples/sec; 0.122 sec/batch)
2017-05-07 19:48:12.701428: step 14420, loss = 0.75 (952.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:48:14.033575: step 14430, loss = 0.69 (960.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:48:15.373589: step 14440, loss = 0.84 (955.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:48:16.733522: step 14450, loss = 0.88 (941.2 examples/sec; 0.136 sec/batch)
2017-05-07 19:48:18.012005: step 14460, loss = 0.62 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-07 19:48:19.361446: step 14470, loss = 0.93 (948.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:48:20.687311: step 14480, loss = 1.03 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:48:22.000137: step 14490, loss = 0.75 (975.0 examples/sec; 0.131 sec/batch)
2017-05-07 19:48:23.491830: step 14500, loss = 0.97 (858.1 examples/sec; 0.149 sec/batch)
2017-05-07 19:48:24.675999: step 14510, loss = 0.69 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-07 19:48:26.011927: step 14520, loss = 0.71 (958.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:48:27.342582: step 14530, loss = 0.82 (961.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:48:28.667191: step 14540, loss = 0.83 (966.3 examples/sec; 0.132 sec/batch)
2017-05-07 19:48:30.053809: step 14550, loss = 0.70 (923.1 examples/sec; 0.139 sec/batch)
2017-05-07 19:48:31.399563: step 14560, loss = 0.84 (951.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:48:32.741798: step 14570, loss = 0.98 (953.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:48:34.082423: step 14580, loss = 0.72 (954.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:48:35.443072: step 14590, loss = 0.83 (940.7 examples/sec; 0.136 sec/batch)
2017-05-07 19:48:36.897399: step 14600, loss = 0.87 (880.1 examples/sec; 0.145 sec/batch)
2017-05-07 19:48:38.112119: step 14610, loss = 0.99 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-07 19:48:39.439943: step 14620, loss = 0.99 (964.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:48:40.768743: step 14630, loss = 0.76 (963.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:48:42.103572: step 14640, loss = 0.96 (958.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:48:43.497901: step 14650, loss = 0.83 (918.0 examples/sec; 0.139 sec/batch)
2017-05-07 19:48:44.790062: step 14660, loss = 0.72 (990.6 examples/sec; 0.129 sec/batch)
2017-05-07 19:48:46.107842: step 14670, loss = 0.82 (971.3 examples/sec; 0.132 sec/batch)
2017-05-07 19:48:47.439369: step 14680, loss = 0.84 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:48:48.767471: step 14690, loss = 0.86 (963.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:48:50.242433: step 14700, loss = 0.75 (867.8 examples/sec; 0.147 sec/batch)
2017-05-07 19:48:51.437024: step 14710, loss = 0.75 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-07 19:48:52.744040: step 14720, loss = 0.96 (979.3 examples/sec; 0.131 sec/batch)
2017-05-07 19:48:54.099438: step 14730, loss = 0.69 (944.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:48:55.430558: step 14740, loss = 0.84 (961.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:48:56.796659: step 14750, loss = 0.78 (937.0 examples/sec; 0.137 sec/batch)
2017-05-07 19:48:58.107895: step 14760, loss = 0.78 (976.2 examples/sec; 0.131 sec/batch)
2017-05-07 19:48:59.435714: step 14770, loss = 0.85 (964.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:49:00.781397: step 14780, loss = 0.92 (951.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:49:02.112394: step 14790, loss = 0.80 (961.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:49:03.656694: step 14800, loss = 0.81 (828.9 examples/sec; 0.154 sec/batch)
2017-05-07 19:49:04.798863: step 14810, loss = 0.77 (1120.7 examples/sec; 0.114 sec/batch)
2017-05-07 19:49:06.156763: step 14820, loss = 0.73 (942.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:49:07.514536: step 14830, loss = 0.99 (942.7 examples/sec; 0.136 sec/batch)
2017-05-07 19:49:08.826289: step 14840, loss = 0.76 (975.8 examples/sec; 0.131 sec/batch)
2017-05-07 19:49:10.233693: step 14850, loss = 0.64 (909.5 examples/sec; 0.141 sec/batch)
2017-05-07 19:49:11.536116: step 14860, loss = 0.80 (982.8 examples/sec; 0.130 sec/batch)
2017-05-07 19:49:12.871289: step 14870, loss = 0.89 (958.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:49:14.190122: step 14880, loss = 0.82 (970.6 examples/sec; 0.132 sec/batch)
2017-05-07 19:49:15.526853: step 14890, loss = 0.85 (957.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:49:17.016676: step 14900, loss = 0.70 (859.2 examples/sec; 0.149 sec/batch)
2017-05-07 19:49:18.214307: step 14910, loss = 0.74 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-07 19:49:19.532801: step 14920, loss = 0.80 (970.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:49:20.842961: step 14930, loss = 0.78 (977.0 examples/sec; 0.131 sec/batch)
2017-05-07 19:49:22.182191: step 14940, loss = 0.71 (955.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:49:23.558829: step 14950, loss = 0.85 (929.8 examples/sec; 0.138 sec/batch)
2017-05-07 19:49:24.854706: step 14960, loss = 0.81 (987.7 examples/sec; 0.130 sec/batch)
2017-05-07 19:49:26.191121: step 14970, loss = 0.67 (957.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:49:27.524075: step 14980, loss = 0.86 (960.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:49:28.867257: step 14990, loss = 0.83 (953.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:49:30.363251: step 15000, loss = 0.77 (855.6 examples/sec; 0.150 sec/batch)
2017-05-07 19:49:31.557363: step 15010, loss = 0.67 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-07 19:49:32.872786: step 15020, loss = 0.88 (973.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:49:34.242225: step 15030, loss = 0.89 (934.7 examples/sec; 0.137 sec/batch)
2017-05-07 19:49:35.556822: step 15040, loss = 0.88 (973.7 examples/sec; 0.131 sec/batch)
2017-05-07 19:49:36.929985: step 15050, loss = 0.77 (932.2 examples/sec; 0.137 sec/batch)
2017-05-07 19:49:38.228647: step 15060, loss = 0.83 (985.6 examples/sec; 0.130 sec/batch)
2017-05-07 19:49:39.575404: step 15070, loss = 0.83 (950.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:49:40.897950: step 15080, loss = 0.88 (967.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:49:42.227734: step 15090, loss = 0.86 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:49:43.708333: step 15100, loss = 0.72 (864.5 examples/sec;E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 312 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
 0.148 sec/batch)
2017-05-07 19:49:44.907083: step 15110, loss = 0.75 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-07 19:49:46.228262: step 15120, loss = 0.73 (968.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:49:47.539354: step 15130, loss = 0.78 (976.3 examples/sec; 0.131 sec/batch)
2017-05-07 19:49:48.888993: step 15140, loss = 0.73 (948.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:49:50.266016: step 15150, loss = 0.75 (929.5 examples/sec; 0.138 sec/batch)
2017-05-07 19:49:51.558420: step 15160, loss = 0.67 (990.4 examples/sec; 0.129 sec/batch)
2017-05-07 19:49:52.889488: step 15170, loss = 0.67 (961.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:49:54.256241: step 15180, loss = 0.78 (936.5 examples/sec; 0.137 sec/batch)
2017-05-07 19:49:55.583930: step 15190, loss = 0.99 (964.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:49:57.053202: step 15200, loss = 0.86 (871.2 examples/sec; 0.147 sec/batch)
2017-05-07 19:49:58.300823: step 15210, loss = 0.93 (1025.9 examples/sec; 0.125 sec/batch)
2017-05-07 19:49:59.659703: step 15220, loss = 0.84 (942.0 examples/sec; 0.136 sec/batch)
2017-05-07 19:50:01.001115: step 15230, loss = 0.81 (954.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:50:02.348019: step 15240, loss = 0.90 (950.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:50:03.709531: step 15250, loss = 0.83 (940.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:50:05.021712: step 15260, loss = 0.86 (975.5 examples/sec; 0.131 sec/batch)
2017-05-07 19:50:06.355886: step 15270, loss = 0.86 (959.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:50:07.682244: step 15280, loss = 1.01 (965.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:50:09.030027: step 15290, loss = 0.81 (949.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:50:10.490780: step 15300, loss = 0.70 (876.3 examples/sec; 0.146 sec/batch)
2017-05-07 19:50:11.721364: step 15310, loss = 0.72 (1040.2 examples/sec; 0.123 sec/batch)
2017-05-07 19:50:13.029491: step 15320, loss = 0.84 (978.5 examples/sec; 0.131 sec/batch)
2017-05-07 19:50:14.373611: step 15330, loss = 0.78 (952.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:50:15.711705: step 15340, loss = 0.80 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:50:17.076302: step 15350, loss = 0.86 (938.0 examples/sec; 0.136 sec/batch)
2017-05-07 19:50:18.396979: step 15360, loss = 0.76 (969.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:50:19.730806: step 15370, loss = 1.04 (959.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:50:21.073524: step 15380, loss = 0.88 (953.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:50:22.415840: step 15390, loss = 0.99 (953.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:50:23.860961: step 15400, loss = 0.79 (885.7 examples/sec; 0.145 sec/batch)
2017-05-07 19:50:25.092726: step 15410, loss = 0.80 (1039.2 examples/sec; 0.123 sec/batch)
2017-05-07 19:50:26.427840: step 15420, loss = 0.92 (958.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:50:27.758183: step 15430, loss = 0.74 (962.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:50:29.082707: step 15440, loss = 0.81 (966.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:50:30.440970: step 15450, loss = 0.76 (942.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:50:31.764839: step 15460, loss = 0.88 (966.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:50:33.102071: step 15470, loss = 0.83 (957.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:50:34.445148: step 15480, loss = 0.79 (953.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:50:35.750812: step 15490, loss = 0.98 (980.3 examples/sec; 0.131 sec/batch)
2017-05-07 19:50:37.172585: step 15500, loss = 0.78 (900.3 examples/sec; 0.142 sec/batch)
2017-05-07 19:50:38.411994: step 15510, loss = 0.73 (1032.8 examples/sec; 0.124 sec/batch)
2017-05-07 19:50:39.739000: step 15520, loss = 0.87 (964.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:50:41.050780: step 15530, loss = 0.76 (975.8 examples/sec; 0.131 sec/batch)
2017-05-07 19:50:42.384114: step 15540, loss = 0.89 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:50:43.736243: step 15550, loss = 0.91 (946.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:50:45.060660: step 15560, loss = 0.77 (966.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:50:46.394129: step 15570, loss = 0.90 (959.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:50:47.745039: step 15580, loss = 0.80 (947.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:50:49.054389: step 15590, loss = 0.86 (977.6 examples/sec; 0.131 sec/batch)
2017-05-07 19:50:50.497349: step 15600, loss = 0.78 (887.1 examples/sec; 0.144 sec/batch)
2017-05-07 19:50:51.758655: step 15610, loss = 0.86 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-07 19:50:53.078818: step 15620, loss = 0.73 (969.6 examples/sec; 0.132 sec/batch)
2017-05-07 19:50:54.420677: step 15630, loss = 0.85 (953.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:50:55.760438: step 15640, loss = 0.79 (955.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:50:57.118367: step 15650, loss = 0.80 (942.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:50:58.444638: step 15660, loss = 0.78 (965.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:50:59.772860: step 15670, loss = 0.89 (963.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:51:01.092608: step 15680, loss = 0.98 (969.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:51:02.444917: step 15690, loss = 0.78 (946.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:51:03.889008: step 15700, loss = 0.79 (886.4 examples/sec; 0.144 sec/batch)
2017-05-07 19:51:05.098006: step 15710, loss = 0.76 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-07 19:51:06.413067: step 15720, loss = 0.88 (973.3 examples/sec; 0.132 sec/batch)
2017-05-07 19:51:07.762764: step 15730, loss = 0.84 (948.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:51:09.060085: step 15740, loss = 1.04 (986.6 examples/sec; 0.130 sec/batch)
2017-05-07 19:51:10.428206: step 15750, loss = 0.77 (935.6 examples/sec; 0.137 sec/batch)
2017-05-07 19:51:11.735309: step 15760, loss = 0.71 (979.3 examples/sec; 0.131 sec/batch)
2017-05-07 19:51:13.056436: step 15770, loss = 0.84 (968.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:51:14.387565: step 15780, loss = 0.82 (961.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:51:15.706795: step 15790, loss = 0.69 (970.3 examples/sec; 0.132 sec/batch)
2017-05-07 19:51:17.160948: step 15800, loss = 0.80 (880.2 examples/sec; 0.145 sec/batch)
2017-05-07 19:51:18.404984: step 15810, loss = 0.80 (1028.9 examples/sec; 0.124 sec/batch)
2017-05-07 19:51:19.724210: step 15820, loss = 0.73 (970.3 examples/sec; 0.132 sec/batch)
2017-05-07 19:51:21.061111: step 15830, loss = 0.68 (957.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:51:22.415107: step 15840, loss = 0.64 (945.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:51:23.779782: step 15850, loss = 0.80 (937.9 examples/sec; 0.136 sec/batch)
2017-05-07 19:51:25.099010: step 15860, loss = 0.81 (970.3 examples/sec; 0.132 sec/batch)
2017-05-07 19:51:26.441908: step 15870, loss = 1.00 (953.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:51:27.770510: step 15880, loss = 0.90 (963.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:51:29.115161: step 15890, loss = 0.81 (951.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:51:30.561383: step 15900, loss = 0.77 (885.1 examples/sec; 0.145 sec/batch)
2017-05-07 19:51:31.798524: step 15910, loss = 0.87 (1034.6 examples/sec; 0.124 sec/batch)
2017-05-07 19:51:33.113652: step 15920, loss = 0.84 (973.3 examples/sec; 0.132 sec/batch)
2017-05-07 19:51:34.435697: step 15930, loss = 0.66 (968.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:51:35.745511: step 15940, loss = 1.11 (977.2 examples/sec; 0.131 sec/batch)
2017-05-07 19:51:37.091269: step 15950, loss = 0.77 (951.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:51:38.421255: step 15960, loss = 0.77 (962.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:51:39.743981: step 15970, loss = 0.86 (967.7 examples/sec; 0.132 sec/batch)
2017-05-07 19:51:41.082443: step 15980, loss = 0.86 (956.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:51:42.424732: step 15990, loss = 0.73 (953.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:51:43.885044: step 16000, loss = 0.82 (876.5 examples/sec; 0.146 sec/batch)
2017-05-07 19:51:45.131883: step 16010, loss = 0.65 (1026.6 exampE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 330 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
les/sec; 0.125 sec/batch)
2017-05-07 19:51:46.484900: step 16020, loss = 0.73 (946.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:51:47.810798: step 16030, loss = 0.80 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:51:49.149003: step 16040, loss = 0.85 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:51:50.490399: step 16050, loss = 0.88 (954.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:51:51.823811: step 16060, loss = 0.85 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:51:53.165111: step 16070, loss = 0.83 (954.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:51:54.500420: step 16080, loss = 0.80 (958.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:51:55.833493: step 16090, loss = 0.84 (960.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:51:57.278029: step 16100, loss = 0.83 (886.1 examples/sec; 0.144 sec/batch)
2017-05-07 19:51:58.487215: step 16110, loss = 0.81 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-07 19:51:59.832153: step 16120, loss = 0.85 (951.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:52:01.169564: step 16130, loss = 1.04 (957.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:52:02.502861: step 16140, loss = 0.80 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:52:03.892034: step 16150, loss = 0.86 (921.4 examples/sec; 0.139 sec/batch)
2017-05-07 19:52:05.201218: step 16160, loss = 0.77 (977.7 examples/sec; 0.131 sec/batch)
2017-05-07 19:52:06.500574: step 16170, loss = 0.80 (985.1 examples/sec; 0.130 sec/batch)
2017-05-07 19:52:07.866925: step 16180, loss = 0.92 (936.8 examples/sec; 0.137 sec/batch)
2017-05-07 19:52:09.186952: step 16190, loss = 0.92 (969.7 examples/sec; 0.132 sec/batch)
2017-05-07 19:52:10.653235: step 16200, loss = 0.95 (873.0 examples/sec; 0.147 sec/batch)
2017-05-07 19:52:11.911521: step 16210, loss = 0.73 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-07 19:52:13.254407: step 16220, loss = 0.97 (953.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:52:14.607414: step 16230, loss = 0.62 (946.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:52:15.937891: step 16240, loss = 0.84 (962.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:52:17.274053: step 16250, loss = 0.82 (958.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:52:18.606665: step 16260, loss = 0.92 (960.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:52:19.916252: step 16270, loss = 0.85 (977.4 examples/sec; 0.131 sec/batch)
2017-05-07 19:52:21.240075: step 16280, loss = 0.75 (966.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:52:22.583674: step 16290, loss = 0.83 (952.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:52:24.032811: step 16300, loss = 0.86 (883.3 examples/sec; 0.145 sec/batch)
2017-05-07 19:52:25.264842: step 16310, loss = 0.76 (1038.9 examples/sec; 0.123 sec/batch)
2017-05-07 19:52:26.613201: step 16320, loss = 0.96 (949.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:52:27.953636: step 16330, loss = 0.85 (954.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:52:29.287656: step 16340, loss = 0.82 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:52:30.641241: step 16350, loss = 0.80 (945.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:52:31.959202: step 16360, loss = 0.75 (971.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:52:33.292178: step 16370, loss = 0.93 (960.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:52:34.631000: step 16380, loss = 0.80 (956.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:52:35.979326: step 16390, loss = 0.84 (949.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:52:37.431857: step 16400, loss = 0.73 (881.2 examples/sec; 0.145 sec/batch)
2017-05-07 19:52:38.667979: step 16410, loss = 0.88 (1035.5 examples/sec; 0.124 sec/batch)
2017-05-07 19:52:40.003840: step 16420, loss = 0.90 (958.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:52:41.353757: step 16430, loss = 0.72 (948.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:52:42.691424: step 16440, loss = 0.81 (956.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:52:44.047712: step 16450, loss = 1.03 (943.7 examples/sec; 0.136 sec/batch)
2017-05-07 19:52:45.387995: step 16460, loss = 0.90 (955.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:52:46.719541: step 16470, loss = 0.84 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:52:48.044521: step 16480, loss = 0.86 (966.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:52:49.362749: step 16490, loss = 0.83 (971.0 examples/sec; 0.132 sec/batch)
2017-05-07 19:52:50.815036: step 16500, loss = 0.85 (881.4 examples/sec; 0.145 sec/batch)
2017-05-07 19:52:52.032035: step 16510, loss = 0.87 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-07 19:52:53.355818: step 16520, loss = 0.80 (966.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:52:54.717902: step 16530, loss = 0.74 (939.7 examples/sec; 0.136 sec/batch)
2017-05-07 19:52:56.028100: step 16540, loss = 0.73 (976.9 examples/sec; 0.131 sec/batch)
2017-05-07 19:52:57.378596: step 16550, loss = 0.76 (947.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:52:58.702205: step 16560, loss = 0.73 (967.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:53:00.032617: step 16570, loss = 0.86 (962.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:53:01.343796: step 16580, loss = 0.87 (976.2 examples/sec; 0.131 sec/batch)
2017-05-07 19:53:02.683807: step 16590, loss = 0.64 (955.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:53:04.116982: step 16600, loss = 0.78 (893.1 examples/sec; 0.143 sec/batch)
2017-05-07 19:53:05.363112: step 16610, loss = 0.83 (1027.2 examples/sec; 0.125 sec/batch)
2017-05-07 19:53:06.686935: step 16620, loss = 0.88 (966.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:53:08.016776: step 16630, loss = 0.67 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:53:09.371062: step 16640, loss = 0.84 (945.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:53:10.728890: step 16650, loss = 0.79 (942.7 examples/sec; 0.136 sec/batch)
2017-05-07 19:53:12.058710: step 16660, loss = 0.90 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:53:13.386579: step 16670, loss = 0.71 (963.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:53:14.734576: step 16680, loss = 0.91 (949.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:53:16.065108: step 16690, loss = 0.81 (962.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:53:17.490989: step 16700, loss = 0.84 (897.7 examples/sec; 0.143 sec/batch)
2017-05-07 19:53:18.749639: step 16710, loss = 1.06 (1017.0 examples/sec; 0.126 sec/batch)
2017-05-07 19:53:20.090552: step 16720, loss = 0.78 (954.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:53:21.418317: step 16730, loss = 0.70 (964.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:53:22.743853: step 16740, loss = 0.81 (965.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:53:24.087728: step 16750, loss = 0.86 (952.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:53:25.400895: step 16760, loss = 0.77 (974.7 examples/sec; 0.131 sec/batch)
2017-05-07 19:53:26.757161: step 16770, loss = 0.77 (943.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:53:28.108021: step 16780, loss = 0.73 (947.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:53:29.447329: step 16790, loss = 0.87 (955.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:53:30.881975: step 16800, loss = 0.74 (892.2 examples/sec; 0.143 sec/batch)
2017-05-07 19:53:32.101330: step 16810, loss = 0.85 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-07 19:53:33.451190: step 16820, loss = 0.86 (948.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:53:34.770258: step 16830, loss = 0.85 (970.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:53:36.107323: step 16840, loss = 0.90 (957.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:53:37.454180: step 16850, loss = 0.75 (950.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:53:38.767407: step 16860, loss = 0.83 (974.7 examples/sec; 0.131 sec/batch)
2017-05-07 19:53:40.103907: step 16870, loss = 0.70 (957.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:53:41.441236: step 16880, loss = 0.92 (957.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:53:42.790760: step 16890, loss = 0.74 (948.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:53:44.254290: step 16900, loss = 0.76 (874.6 examples/sec; 0.146 sec/batch)
2017-05-07 19:53:45.505091: step 16910, loss = 0.90 (1023.3 examples/sec; 0.125 sec/batch)
2017-05-07 19:53:46.825078: step 16920, loss = 0.88 (969.E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 348 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
7 examples/sec; 0.132 sec/batch)
2017-05-07 19:53:48.186033: step 16930, loss = 0.89 (940.5 examples/sec; 0.136 sec/batch)
2017-05-07 19:53:49.502071: step 16940, loss = 0.72 (972.6 examples/sec; 0.132 sec/batch)
2017-05-07 19:53:50.838438: step 16950, loss = 0.72 (957.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:53:52.183108: step 16960, loss = 0.79 (951.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:53:53.504873: step 16970, loss = 0.82 (968.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:53:54.842735: step 16980, loss = 0.84 (956.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:53:56.189049: step 16990, loss = 0.99 (950.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:53:57.625264: step 17000, loss = 0.82 (891.2 examples/sec; 0.144 sec/batch)
2017-05-07 19:53:58.863990: step 17010, loss = 0.73 (1033.3 examples/sec; 0.124 sec/batch)
2017-05-07 19:54:00.197914: step 17020, loss = 0.89 (959.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:54:01.529832: step 17030, loss = 0.81 (961.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:54:02.854338: step 17040, loss = 0.74 (966.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:54:04.209920: step 17050, loss = 0.71 (944.2 examples/sec; 0.136 sec/batch)
2017-05-07 19:54:05.528948: step 17060, loss = 0.75 (970.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:54:06.848990: step 17070, loss = 0.91 (969.7 examples/sec; 0.132 sec/batch)
2017-05-07 19:54:08.184091: step 17080, loss = 0.68 (958.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:54:09.509380: step 17090, loss = 0.72 (965.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:54:10.958589: step 17100, loss = 0.86 (883.2 examples/sec; 0.145 sec/batch)
2017-05-07 19:54:12.181229: step 17110, loss = 0.86 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-07 19:54:13.515328: step 17120, loss = 0.84 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:54:14.852039: step 17130, loss = 0.84 (957.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:54:16.199851: step 17140, loss = 0.82 (949.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:54:17.565950: step 17150, loss = 0.86 (937.0 examples/sec; 0.137 sec/batch)
2017-05-07 19:54:18.888437: step 17160, loss = 0.88 (967.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:54:20.227803: step 17170, loss = 0.87 (955.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:54:21.565725: step 17180, loss = 0.75 (956.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:54:22.902907: step 17190, loss = 0.81 (957.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:54:24.347844: step 17200, loss = 0.82 (885.8 examples/sec; 0.144 sec/batch)
2017-05-07 19:54:25.568035: step 17210, loss = 1.00 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-07 19:54:26.912929: step 17220, loss = 0.68 (951.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:54:28.264136: step 17230, loss = 0.93 (947.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:54:29.594390: step 17240, loss = 0.81 (962.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:54:30.946876: step 17250, loss = 0.68 (946.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:54:32.265212: step 17260, loss = 0.94 (970.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:54:33.616947: step 17270, loss = 0.77 (946.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:54:34.937108: step 17280, loss = 0.95 (969.6 examples/sec; 0.132 sec/batch)
2017-05-07 19:54:36.271397: step 17290, loss = 0.78 (959.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:54:37.696816: step 17300, loss = 0.69 (898.0 examples/sec; 0.143 sec/batch)
2017-05-07 19:54:38.914936: step 17310, loss = 0.89 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-07 19:54:40.254335: step 17320, loss = 0.79 (955.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:54:41.558238: step 17330, loss = 0.72 (981.7 examples/sec; 0.130 sec/batch)
2017-05-07 19:54:42.889200: step 17340, loss = 0.78 (961.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:54:44.234479: step 17350, loss = 0.91 (951.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:54:45.542244: step 17360, loss = 0.75 (978.7 examples/sec; 0.131 sec/batch)
2017-05-07 19:54:46.858523: step 17370, loss = 0.79 (972.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:54:48.216267: step 17380, loss = 0.93 (942.7 examples/sec; 0.136 sec/batch)
2017-05-07 19:54:49.534595: step 17390, loss = 0.66 (970.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:54:50.985186: step 17400, loss = 0.84 (882.4 examples/sec; 0.145 sec/batch)
2017-05-07 19:54:52.236935: step 17410, loss = 0.67 (1022.6 examples/sec; 0.125 sec/batch)
2017-05-07 19:54:53.565959: step 17420, loss = 0.91 (963.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:54:54.900634: step 17430, loss = 0.90 (959.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:54:56.230980: step 17440, loss = 0.85 (962.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:54:57.591409: step 17450, loss = 0.70 (940.9 examples/sec; 0.136 sec/batch)
2017-05-07 19:54:58.918715: step 17460, loss = 0.88 (964.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:55:00.269847: step 17470, loss = 0.80 (947.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:55:01.608609: step 17480, loss = 0.83 (956.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:55:02.955766: step 17490, loss = 0.93 (950.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:55:04.397992: step 17500, loss = 0.85 (887.5 examples/sec; 0.144 sec/batch)
2017-05-07 19:55:05.619097: step 17510, loss = 0.94 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-07 19:55:06.959354: step 17520, loss = 0.89 (955.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:55:08.285763: step 17530, loss = 0.79 (965.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:55:09.582227: step 17540, loss = 0.85 (987.3 examples/sec; 0.130 sec/batch)
2017-05-07 19:55:10.936002: step 17550, loss = 0.72 (945.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:55:12.264936: step 17560, loss = 0.89 (963.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:55:13.585216: step 17570, loss = 0.84 (969.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:55:14.911755: step 17580, loss = 0.72 (964.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:55:16.245261: step 17590, loss = 0.86 (959.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:55:17.669475: step 17600, loss = 0.85 (898.7 examples/sec; 0.142 sec/batch)
2017-05-07 19:55:18.932985: step 17610, loss = 0.74 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-07 19:55:20.267435: step 17620, loss = 0.90 (959.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:55:21.612045: step 17630, loss = 0.78 (952.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:55:22.947591: step 17640, loss = 0.74 (958.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:55:24.295972: step 17650, loss = 0.77 (949.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:55:25.624759: step 17660, loss = 0.80 (963.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:55:26.972818: step 17670, loss = 0.70 (949.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:55:28.328340: step 17680, loss = 0.64 (944.3 examples/sec; 0.136 sec/batch)
2017-05-07 19:55:29.669129: step 17690, loss = 1.01 (954.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:55:31.107305: step 17700, loss = 0.65 (890.0 examples/sec; 0.144 sec/batch)
2017-05-07 19:55:32.347156: step 17710, loss = 0.85 (1032.4 examples/sec; 0.124 sec/batch)
2017-05-07 19:55:33.678379: step 17720, loss = 1.03 (961.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:55:35.007932: step 17730, loss = 0.89 (962.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:55:36.367955: step 17740, loss = 0.86 (941.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:55:37.722732: step 17750, loss = 0.95 (944.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:55:39.034183: step 17760, loss = 0.87 (976.0 examples/sec; 0.131 sec/batch)
2017-05-07 19:55:40.386009: step 17770, loss = 0.73 (946.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:55:41.715753: step 17780, loss = 0.79 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:55:43.034387: step 17790, loss = 0.69 (970.7 examples/sec; 0.132 sec/batch)
2017-05-07 19:55:44.477410: step 17800, loss = 0.84 (887.0 examples/sec; 0.144 sec/batch)
2017-05-07 19:55:45.707593: step 17810, loss = 0.80 (1040.5 examples/sec; 0.123 sec/batch)
2017-05-07 19:55:47.046999: step 17820, loss = 0.72 (955.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:55:48.381870: step 17830, loss = 0.7E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 367 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
6 (958.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:55:49.700122: step 17840, loss = 0.80 (971.0 examples/sec; 0.132 sec/batch)
2017-05-07 19:55:51.046398: step 17850, loss = 0.91 (950.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:55:52.384517: step 17860, loss = 0.93 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:55:53.722877: step 17870, loss = 0.80 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:55:55.084198: step 17880, loss = 1.01 (940.2 examples/sec; 0.136 sec/batch)
2017-05-07 19:55:56.439425: step 17890, loss = 0.64 (944.5 examples/sec; 0.136 sec/batch)
2017-05-07 19:55:57.866202: step 17900, loss = 0.76 (897.1 examples/sec; 0.143 sec/batch)
2017-05-07 19:55:59.118042: step 17910, loss = 0.71 (1022.5 examples/sec; 0.125 sec/batch)
2017-05-07 19:56:00.429070: step 17920, loss = 0.65 (976.3 examples/sec; 0.131 sec/batch)
2017-05-07 19:56:01.742203: step 17930, loss = 0.68 (974.8 examples/sec; 0.131 sec/batch)
2017-05-07 19:56:03.076111: step 17940, loss = 0.97 (959.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:56:04.447273: step 17950, loss = 0.79 (933.5 examples/sec; 0.137 sec/batch)
2017-05-07 19:56:05.765822: step 17960, loss = 1.00 (970.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:56:07.095626: step 17970, loss = 0.63 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:56:08.436507: step 17980, loss = 0.82 (954.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:56:09.758294: step 17990, loss = 0.90 (968.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:56:11.202143: step 18000, loss = 0.82 (886.5 examples/sec; 0.144 sec/batch)
2017-05-07 19:56:12.475805: step 18010, loss = 0.79 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-07 19:56:13.796517: step 18020, loss = 0.79 (969.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:56:15.115511: step 18030, loss = 0.87 (970.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:56:16.455170: step 18040, loss = 0.69 (955.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:56:17.802613: step 18050, loss = 0.81 (950.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:56:19.135064: step 18060, loss = 0.88 (960.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:56:20.490981: step 18070, loss = 0.76 (944.0 examples/sec; 0.136 sec/batch)
2017-05-07 19:56:21.817162: step 18080, loss = 0.62 (965.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:56:23.134121: step 18090, loss = 0.78 (971.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:56:24.581081: step 18100, loss = 0.90 (884.6 examples/sec; 0.145 sec/batch)
2017-05-07 19:56:25.827171: step 18110, loss = 0.75 (1027.2 examples/sec; 0.125 sec/batch)
2017-05-07 19:56:27.171862: step 18120, loss = 0.93 (951.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:56:28.480333: step 18130, loss = 0.80 (978.2 examples/sec; 0.131 sec/batch)
2017-05-07 19:56:29.801155: step 18140, loss = 0.82 (969.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:56:31.158215: step 18150, loss = 0.79 (943.2 examples/sec; 0.136 sec/batch)
2017-05-07 19:56:32.501712: step 18160, loss = 0.68 (952.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:56:33.837493: step 18170, loss = 0.83 (958.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:56:35.176486: step 18180, loss = 0.65 (956.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:56:36.513505: step 18190, loss = 0.76 (957.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:56:37.962608: step 18200, loss = 0.71 (883.3 examples/sec; 0.145 sec/batch)
2017-05-07 19:56:39.217080: step 18210, loss = 0.87 (1020.3 examples/sec; 0.125 sec/batch)
2017-05-07 19:56:40.548435: step 18220, loss = 0.84 (961.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:56:41.878892: step 18230, loss = 0.78 (962.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:56:43.210192: step 18240, loss = 0.91 (961.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:56:44.551076: step 18250, loss = 0.77 (954.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:56:45.888407: step 18260, loss = 0.84 (957.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:56:47.220907: step 18270, loss = 0.96 (960.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:56:48.549775: step 18280, loss = 0.78 (963.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:56:49.886903: step 18290, loss = 0.74 (957.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:56:51.325171: step 18300, loss = 0.92 (890.0 examples/sec; 0.144 sec/batch)
2017-05-07 19:56:52.578487: step 18310, loss = 0.73 (1021.3 examples/sec; 0.125 sec/batch)
2017-05-07 19:56:53.926774: step 18320, loss = 1.01 (949.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:56:55.285819: step 18330, loss = 0.90 (941.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:56:56.634103: step 18340, loss = 0.72 (949.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:56:57.985705: step 18350, loss = 0.72 (947.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:56:59.315987: step 18360, loss = 1.00 (962.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:57:00.643257: step 18370, loss = 0.86 (964.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:57:01.991798: step 18380, loss = 0.99 (949.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:57:03.328151: step 18390, loss = 0.85 (957.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:57:04.751485: step 18400, loss = 0.73 (899.3 examples/sec; 0.142 sec/batch)
2017-05-07 19:57:05.990853: step 18410, loss = 0.72 (1032.8 examples/sec; 0.124 sec/batch)
2017-05-07 19:57:07.340559: step 18420, loss = 0.85 (948.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:57:08.694932: step 18430, loss = 0.81 (945.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:57:10.012018: step 18440, loss = 0.78 (971.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:57:11.370428: step 18450, loss = 0.72 (942.3 examples/sec; 0.136 sec/batch)
2017-05-07 19:57:12.688325: step 18460, loss = 0.84 (971.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:57:14.020432: step 18470, loss = 0.89 (960.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:57:15.337552: step 18480, loss = 0.67 (971.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:57:16.660590: step 18490, loss = 0.67 (967.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:57:18.096789: step 18500, loss = 1.06 (891.2 examples/sec; 0.144 sec/batch)
2017-05-07 19:57:19.354619: step 18510, loss = 0.59 (1017.6 examples/sec; 0.126 sec/batch)
2017-05-07 19:57:20.676852: step 18520, loss = 0.81 (968.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:57:21.995908: step 18530, loss = 0.83 (970.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:57:23.325135: step 18540, loss = 0.81 (963.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:57:24.677789: step 18550, loss = 0.76 (946.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:57:25.997326: step 18560, loss = 0.65 (970.0 examples/sec; 0.132 sec/batch)
2017-05-07 19:57:27.334900: step 18570, loss = 0.62 (957.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:57:28.640115: step 18580, loss = 0.82 (980.7 examples/sec; 0.131 sec/batch)
2017-05-07 19:57:29.973500: step 18590, loss = 0.74 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:57:31.395932: step 18600, loss = 0.92 (899.9 examples/sec; 0.142 sec/batch)
2017-05-07 19:57:32.636365: step 18610, loss = 0.80 (1031.9 examples/sec; 0.124 sec/batch)
2017-05-07 19:57:33.978957: step 18620, loss = 0.79 (953.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:57:35.332112: step 18630, loss = 0.77 (945.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:57:36.659875: step 18640, loss = 0.59 (964.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:57:38.015181: step 18650, loss = 0.93 (944.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:57:39.357928: step 18660, loss = 0.98 (953.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:57:40.685484: step 18670, loss = 0.82 (964.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:57:42.001282: step 18680, loss = 0.73 (972.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:57:43.325103: step 18690, loss = 0.91 (966.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:57:44.773963: step 18700, loss = 0.73 (883.4 examples/sec; 0.145 sec/batch)
2017-05-07 19:57:46.014196: step 18710, loss = 0.91 (1032.1 examples/sec; 0.124 sec/batch)
2017-05-07 19:57:47.333854: step 18720, loss = 0.72 (970.0 examples/sec; 0.132 sec/batch)
2017-05-07 19:57:48.691622: step 18730, loss = 0.92 (942.7 examples/sec; 0.136 sec/batch)
2017-05-07 19:57:50.056913: step 18740, losE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 385 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
s = 0.71 (937.5 examples/sec; 0.137 sec/batch)
2017-05-07 19:57:51.430720: step 18750, loss = 0.76 (931.7 examples/sec; 0.137 sec/batch)
2017-05-07 19:57:52.753879: step 18760, loss = 0.62 (967.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:57:54.118909: step 18770, loss = 0.83 (937.7 examples/sec; 0.137 sec/batch)
2017-05-07 19:57:55.464967: step 18780, loss = 0.77 (950.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:57:56.781976: step 18790, loss = 0.89 (971.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:57:58.236518: step 18800, loss = 0.91 (880.0 examples/sec; 0.145 sec/batch)
2017-05-07 19:57:59.511278: step 18810, loss = 0.72 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-07 19:58:00.812423: step 18820, loss = 0.84 (983.8 examples/sec; 0.130 sec/batch)
2017-05-07 19:58:02.167028: step 18830, loss = 0.77 (944.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:58:03.532315: step 18840, loss = 0.61 (937.5 examples/sec; 0.137 sec/batch)
2017-05-07 19:58:04.893718: step 18850, loss = 0.70 (940.2 examples/sec; 0.136 sec/batch)
2017-05-07 19:58:06.221530: step 18860, loss = 0.77 (964.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:58:07.563959: step 18870, loss = 0.82 (953.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:58:08.893956: step 18880, loss = 0.97 (962.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:58:10.229270: step 18890, loss = 0.83 (958.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:58:11.681877: step 18900, loss = 0.84 (881.2 examples/sec; 0.145 sec/batch)
2017-05-07 19:58:12.947345: step 18910, loss = 0.93 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-07 19:58:14.294175: step 18920, loss = 0.85 (950.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:58:15.613326: step 18930, loss = 0.73 (970.3 examples/sec; 0.132 sec/batch)
2017-05-07 19:58:16.944241: step 18940, loss = 0.87 (961.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:58:18.295070: step 18950, loss = 0.88 (947.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:58:19.614059: step 18960, loss = 0.77 (970.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:58:20.935801: step 18970, loss = 0.89 (968.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:58:22.290919: step 18980, loss = 0.75 (944.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:58:23.612651: step 18990, loss = 0.89 (968.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:58:25.073903: step 19000, loss = 0.69 (876.0 examples/sec; 0.146 sec/batch)
2017-05-07 19:58:26.331609: step 19010, loss = 0.79 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-07 19:58:27.659846: step 19020, loss = 0.86 (963.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:58:29.005808: step 19030, loss = 0.86 (951.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:58:30.351040: step 19040, loss = 0.69 (951.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:58:31.738645: step 19050, loss = 0.95 (922.5 examples/sec; 0.139 sec/batch)
2017-05-07 19:58:33.061829: step 19060, loss = 0.87 (967.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:58:34.404041: step 19070, loss = 0.77 (953.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:58:35.723597: step 19080, loss = 0.85 (970.0 examples/sec; 0.132 sec/batch)
2017-05-07 19:58:37.054262: step 19090, loss = 0.98 (961.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:58:38.464765: step 19100, loss = 0.86 (907.5 examples/sec; 0.141 sec/batch)
2017-05-07 19:58:39.690283: step 19110, loss = 0.90 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-07 19:58:41.028673: step 19120, loss = 0.74 (956.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:58:42.347831: step 19130, loss = 0.87 (970.3 examples/sec; 0.132 sec/batch)
2017-05-07 19:58:43.703149: step 19140, loss = 0.68 (944.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:58:45.069004: step 19150, loss = 0.80 (937.1 examples/sec; 0.137 sec/batch)
2017-05-07 19:58:46.407810: step 19160, loss = 0.75 (956.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:58:47.762690: step 19170, loss = 0.89 (944.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:58:49.102282: step 19180, loss = 0.84 (955.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:58:50.470362: step 19190, loss = 0.75 (935.6 examples/sec; 0.137 sec/batch)
2017-05-07 19:58:51.916135: step 19200, loss = 0.87 (885.3 examples/sec; 0.145 sec/batch)
2017-05-07 19:58:53.137607: step 19210, loss = 0.80 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-07 19:58:54.502211: step 19220, loss = 0.82 (938.0 examples/sec; 0.136 sec/batch)
2017-05-07 19:58:55.859195: step 19230, loss = 0.88 (943.3 examples/sec; 0.136 sec/batch)
2017-05-07 19:58:57.204140: step 19240, loss = 0.76 (951.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:58:58.572728: step 19250, loss = 0.76 (935.3 examples/sec; 0.137 sec/batch)
2017-05-07 19:58:59.886139: step 19260, loss = 0.78 (974.6 examples/sec; 0.131 sec/batch)
2017-05-07 19:59:01.202448: step 19270, loss = 1.03 (972.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:59:02.539775: step 19280, loss = 0.84 (957.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:59:03.892389: step 19290, loss = 0.70 (946.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:59:05.312824: step 19300, loss = 0.71 (901.1 examples/sec; 0.142 sec/batch)
2017-05-07 19:59:06.547319: step 19310, loss = 0.86 (1036.9 examples/sec; 0.123 sec/batch)
2017-05-07 19:59:07.873637: step 19320, loss = 0.92 (965.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:59:09.195371: step 19330, loss = 0.72 (968.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:59:10.544661: step 19340, loss = 0.72 (948.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:59:11.911253: step 19350, loss = 0.88 (936.6 examples/sec; 0.137 sec/batch)
2017-05-07 19:59:13.211117: step 19360, loss = 0.71 (984.7 examples/sec; 0.130 sec/batch)
2017-05-07 19:59:14.529546: step 19370, loss = 0.79 (970.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:59:15.860684: step 19380, loss = 0.77 (961.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:59:17.179576: step 19390, loss = 0.81 (970.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:59:18.610270: step 19400, loss = 0.77 (894.7 examples/sec; 0.143 sec/batch)
2017-05-07 19:59:19.848355: step 19410, loss = 0.79 (1033.9 examples/sec; 0.124 sec/batch)
2017-05-07 19:59:21.182949: step 19420, loss = 0.78 (959.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:59:22.527928: step 19430, loss = 0.78 (951.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:59:23.877975: step 19440, loss = 0.84 (948.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:59:25.241587: step 19450, loss = 0.82 (938.7 examples/sec; 0.136 sec/batch)
2017-05-07 19:59:26.540900: step 19460, loss = 0.73 (985.1 examples/sec; 0.130 sec/batch)
2017-05-07 19:59:27.880647: step 19470, loss = 1.00 (955.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:59:29.218656: step 19480, loss = 0.87 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:59:30.579926: step 19490, loss = 0.95 (940.3 examples/sec; 0.136 sec/batch)
2017-05-07 19:59:32.013018: step 19500, loss = 0.69 (893.2 examples/sec; 0.143 sec/batch)
2017-05-07 19:59:33.256101: step 19510, loss = 0.69 (1029.7 examples/sec; 0.124 sec/batch)
2017-05-07 19:59:34.586085: step 19520, loss = 0.88 (962.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:59:35.919161: step 19530, loss = 0.83 (960.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:59:37.252826: step 19540, loss = 0.84 (959.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:59:38.635088: step 19550, loss = 0.74 (926.0 examples/sec; 0.138 sec/batch)
2017-05-07 19:59:39.909140: step 19560, loss = 0.92 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-07 19:59:41.262915: step 19570, loss = 0.78 (945.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:59:42.580913: step 19580, loss = 0.85 (971.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:59:43.915737: step 19590, loss = 0.77 (958.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:59:45.374685: step 19600, loss = 0.63 (877.3 examples/sec; 0.146 sec/batch)
2017-05-07 19:59:46.575980: step 19610, loss = 0.80 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-07 19:59:47.919939: step 19620, loss = 0.88 (952.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:59:49.236598: step 19630, loss = 0.80 (972.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:59:50.562586: step 19640, loss = 0.71 (965.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:59:51.950873: step 19E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 403 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
650, loss = 0.81 (922.0 examples/sec; 0.139 sec/batch)
2017-05-07 19:59:53.233610: step 19660, loss = 0.72 (997.9 examples/sec; 0.128 sec/batch)
2017-05-07 19:59:54.557522: step 19670, loss = 0.78 (966.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:59:55.912599: step 19680, loss = 0.81 (944.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:59:57.256656: step 19690, loss = 0.72 (952.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:59:58.751903: step 19700, loss = 0.79 (856.0 examples/sec; 0.150 sec/batch)
2017-05-07 19:59:59.947100: step 19710, loss = 0.69 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-07 20:00:01.263946: step 19720, loss = 0.87 (972.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:00:02.621865: step 19730, loss = 0.73 (942.6 examples/sec; 0.136 sec/batch)
2017-05-07 20:00:03.943217: step 19740, loss = 0.71 (968.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:00:05.323750: step 19750, loss = 0.84 (927.2 examples/sec; 0.138 sec/batch)
2017-05-07 20:00:06.606373: step 19760, loss = 0.96 (998.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:00:07.973038: step 19770, loss = 0.79 (936.6 examples/sec; 0.137 sec/batch)
2017-05-07 20:00:09.276786: step 19780, loss = 0.80 (981.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:00:10.606217: step 19790, loss = 0.84 (962.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:00:12.084942: step 19800, loss = 0.71 (865.6 examples/sec; 0.148 sec/batch)
2017-05-07 20:00:13.297413: step 19810, loss = 0.69 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-07 20:00:14.628260: step 19820, loss = 0.76 (961.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:00:15.937904: step 19830, loss = 0.82 (977.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:00:17.275319: step 19840, loss = 0.94 (957.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:00:18.650412: step 19850, loss = 0.62 (930.9 examples/sec; 0.138 sec/batch)
2017-05-07 20:00:19.937579: step 19860, loss = 0.86 (994.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:00:21.258387: step 19870, loss = 0.90 (969.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:00:22.597766: step 19880, loss = 0.82 (955.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:00:23.904625: step 19890, loss = 0.85 (979.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:00:25.398326: step 19900, loss = 0.86 (856.9 examples/sec; 0.149 sec/batch)
2017-05-07 20:00:26.573573: step 19910, loss = 0.81 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-07 20:00:27.915718: step 19920, loss = 0.80 (953.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:00:29.238106: step 19930, loss = 0.73 (968.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:00:30.588993: step 19940, loss = 0.81 (947.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:00:31.955295: step 19950, loss = 0.80 (936.8 examples/sec; 0.137 sec/batch)
2017-05-07 20:00:33.253681: step 19960, loss = 0.77 (985.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:00:34.597676: step 19970, loss = 0.79 (952.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:00:35.961048: step 19980, loss = 0.71 (938.9 examples/sec; 0.136 sec/batch)
2017-05-07 20:00:37.301138: step 19990, loss = 0.72 (955.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:00:38.743163: step 20000, loss = 0.78 (887.6 examples/sec; 0.144 sec/batch)
2017-05-07 20:00:39.978082: step 20010, loss = 0.66 (1036.5 examples/sec; 0.123 sec/batch)
2017-05-07 20:00:41.302282: step 20020, loss = 0.80 (966.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:00:42.660954: step 20030, loss = 0.72 (942.1 examples/sec; 0.136 sec/batch)
2017-05-07 20:00:43.990951: step 20040, loss = 0.84 (962.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:00:45.325733: step 20050, loss = 0.68 (959.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:00:46.644572: step 20060, loss = 0.85 (970.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:00:47.958078: step 20070, loss = 0.60 (974.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:00:49.291887: step 20080, loss = 0.81 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:00:50.617983: step 20090, loss = 0.93 (965.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:00:52.064528: step 20100, loss = 0.94 (884.9 examples/sec; 0.145 sec/batch)
2017-05-07 20:00:53.309184: step 20110, loss = 0.69 (1028.4 examples/sec; 0.124 sec/batch)
2017-05-07 20:00:54.618370: step 20120, loss = 0.79 (977.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:00:55.946327: step 20130, loss = 0.76 (963.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:00:57.278129: step 20140, loss = 0.80 (961.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:00:58.629962: step 20150, loss = 0.71 (946.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:00:59.960632: step 20160, loss = 0.81 (961.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:01:01.336913: step 20170, loss = 0.65 (930.1 examples/sec; 0.138 sec/batch)
2017-05-07 20:01:02.693550: step 20180, loss = 0.83 (943.5 examples/sec; 0.136 sec/batch)
2017-05-07 20:01:04.044445: step 20190, loss = 0.74 (947.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:01:05.533335: step 20200, loss = 0.69 (859.7 examples/sec; 0.149 sec/batch)
2017-05-07 20:01:06.700618: step 20210, loss = 0.71 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-07 20:01:08.029238: step 20220, loss = 0.84 (963.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:01:09.345641: step 20230, loss = 0.83 (972.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:01:10.684718: step 20240, loss = 0.73 (955.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:01:12.017653: step 20250, loss = 0.71 (960.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:01:13.344997: step 20260, loss = 0.85 (964.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:01:14.678915: step 20270, loss = 0.57 (959.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:01:16.033216: step 20280, loss = 0.72 (945.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:01:17.339209: step 20290, loss = 0.85 (980.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:01:18.824954: step 20300, loss = 0.68 (861.5 examples/sec; 0.149 sec/batch)
2017-05-07 20:01:20.029983: step 20310, loss = 0.74 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-07 20:01:21.348138: step 20320, loss = 0.88 (971.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:01:22.657341: step 20330, loss = 0.90 (977.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:01:24.015147: step 20340, loss = 0.73 (942.7 examples/sec; 0.136 sec/batch)
2017-05-07 20:01:25.371941: step 20350, loss = 0.80 (943.4 examples/sec; 0.136 sec/batch)
2017-05-07 20:01:26.706772: step 20360, loss = 0.85 (958.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:01:28.046131: step 20370, loss = 0.80 (955.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:01:29.397200: step 20380, loss = 0.97 (947.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:01:30.726270: step 20390, loss = 0.81 (963.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:01:32.178076: step 20400, loss = 0.90 (881.7 examples/sec; 0.145 sec/batch)
2017-05-07 20:01:33.414064: step 20410, loss = 0.91 (1035.6 examples/sec; 0.124 sec/batch)
2017-05-07 20:01:34.745109: step 20420, loss = 0.82 (961.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:01:36.101486: step 20430, loss = 0.95 (943.7 examples/sec; 0.136 sec/batch)
2017-05-07 20:01:37.421360: step 20440, loss = 0.78 (969.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:01:38.754973: step 20450, loss = 0.77 (959.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:01:40.075510: step 20460, loss = 0.88 (969.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:01:41.375586: step 20470, loss = 0.83 (984.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:01:42.721752: step 20480, loss = 0.86 (950.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:01:44.058642: step 20490, loss = 0.84 (957.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:01:45.486208: step 20500, loss = 0.91 (896.6 examples/sec; 0.143 sec/batch)
2017-05-07 20:01:46.713863: step 20510, loss = 0.79 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-07 20:01:48.065418: step 20520, loss = 0.92 (947.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:01:49.395256: step 20530, loss = 0.83 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:01:50.719122: step 20540, loss = 0.86 (966.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:01:52.091695: step 20550, loss = 0.75 (932.6 examples/sec; 0.137 sec/batch)
2017-05-07 20:01:53.414266: E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 421 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
step 20560, loss = 0.85 (967.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:01:54.747794: step 20570, loss = 0.94 (959.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:01:56.088740: step 20580, loss = 0.87 (954.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:01:57.426415: step 20590, loss = 0.68 (956.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:01:58.873956: step 20600, loss = 1.00 (884.3 examples/sec; 0.145 sec/batch)
2017-05-07 20:02:00.087785: step 20610, loss = 0.79 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-07 20:02:01.410695: step 20620, loss = 0.76 (967.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:02:02.742744: step 20630, loss = 0.81 (960.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:02:04.069356: step 20640, loss = 0.89 (964.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:02:05.407943: step 20650, loss = 0.84 (956.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:02:06.730322: step 20660, loss = 0.90 (968.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:02:08.075180: step 20670, loss = 0.93 (951.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:02:09.415057: step 20680, loss = 0.84 (955.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:02:10.774868: step 20690, loss = 0.62 (941.3 examples/sec; 0.136 sec/batch)
2017-05-07 20:02:12.198237: step 20700, loss = 0.82 (899.3 examples/sec; 0.142 sec/batch)
2017-05-07 20:02:13.433983: step 20710, loss = 0.83 (1035.8 examples/sec; 0.124 sec/batch)
2017-05-07 20:02:14.761336: step 20720, loss = 0.89 (964.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:02:16.095806: step 20730, loss = 0.92 (959.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:02:17.435521: step 20740, loss = 1.04 (955.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:02:18.787196: step 20750, loss = 0.94 (947.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:02:20.094678: step 20760, loss = 0.79 (979.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:02:21.411058: step 20770, loss = 0.82 (972.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:02:22.746561: step 20780, loss = 0.78 (958.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:02:24.085140: step 20790, loss = 0.95 (956.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:02:25.520866: step 20800, loss = 0.88 (891.5 examples/sec; 0.144 sec/batch)
2017-05-07 20:02:26.742999: step 20810, loss = 0.99 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-07 20:02:28.068499: step 20820, loss = 1.08 (965.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:02:29.401569: step 20830, loss = 0.75 (960.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:02:30.724525: step 20840, loss = 0.63 (967.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:02:32.101807: step 20850, loss = 0.75 (929.4 examples/sec; 0.138 sec/batch)
2017-05-07 20:02:33.423501: step 20860, loss = 0.78 (968.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:02:34.768398: step 20870, loss = 0.70 (951.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:02:36.097589: step 20880, loss = 0.77 (963.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:02:37.411715: step 20890, loss = 0.75 (974.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:02:38.865413: step 20900, loss = 0.83 (880.5 examples/sec; 0.145 sec/batch)
2017-05-07 20:02:40.100435: step 20910, loss = 0.79 (1036.4 examples/sec; 0.124 sec/batch)
2017-05-07 20:02:41.459272: step 20920, loss = 0.80 (942.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:02:42.799535: step 20930, loss = 0.71 (955.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:02:44.122920: step 20940, loss = 0.71 (967.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:02:45.473942: step 20950, loss = 0.71 (947.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:02:46.771147: step 20960, loss = 0.81 (986.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:02:48.092432: step 20970, loss = 0.71 (968.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:02:49.420699: step 20980, loss = 0.78 (963.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:02:50.765866: step 20990, loss = 0.99 (951.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:02:52.229537: step 21000, loss = 0.80 (874.5 examples/sec; 0.146 sec/batch)
2017-05-07 20:02:53.456853: step 21010, loss = 0.79 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-07 20:02:54.783275: step 21020, loss = 0.82 (965.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:02:56.134329: step 21030, loss = 0.82 (947.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:02:57.447710: step 21040, loss = 0.73 (974.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:02:58.788093: step 21050, loss = 0.88 (954.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:03:00.096057: step 21060, loss = 0.75 (978.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:03:01.419117: step 21070, loss = 1.19 (967.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:03:02.741123: step 21080, loss = 0.72 (968.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:03:04.070757: step 21090, loss = 0.76 (962.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:03:05.497409: step 21100, loss = 0.69 (897.2 examples/sec; 0.143 sec/batch)
2017-05-07 20:03:06.752227: step 21110, loss = 0.83 (1020.1 examples/sec; 0.125 sec/batch)
2017-05-07 20:03:08.082919: step 21120, loss = 0.84 (961.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:03:09.413853: step 21130, loss = 0.74 (961.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:03:10.765645: step 21140, loss = 0.75 (946.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:03:12.129103: step 21150, loss = 0.82 (938.8 examples/sec; 0.136 sec/batch)
2017-05-07 20:03:13.423386: step 21160, loss = 0.64 (989.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:03:14.742666: step 21170, loss = 0.78 (970.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:03:16.087503: step 21180, loss = 0.79 (951.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:03:17.420441: step 21190, loss = 0.67 (960.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:03:18.879081: step 21200, loss = 0.71 (877.5 examples/sec; 0.146 sec/batch)
2017-05-07 20:03:20.139538: step 21210, loss = 0.84 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:03:21.456636: step 21220, loss = 0.97 (971.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:03:22.808908: step 21230, loss = 0.83 (946.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:03:24.154852: step 21240, loss = 0.74 (951.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:03:25.497118: step 21250, loss = 0.90 (953.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:03:26.850587: step 21260, loss = 0.78 (945.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:03:28.189065: step 21270, loss = 0.89 (956.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:03:29.522254: step 21280, loss = 0.75 (960.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:03:30.856729: step 21290, loss = 0.85 (959.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:03:32.299865: step 21300, loss = 0.74 (886.9 examples/sec; 0.144 sec/batch)
2017-05-07 20:03:33.522025: step 21310, loss = 0.87 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-07 20:03:34.859832: step 21320, loss = 0.92 (956.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:03:36.231261: step 21330, loss = 0.76 (933.3 examples/sec; 0.137 sec/batch)
2017-05-07 20:03:37.572974: step 21340, loss = 0.84 (954.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:03:38.895948: step 21350, loss = 1.03 (967.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:03:40.231279: step 21360, loss = 0.72 (958.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:03:41.572373: step 21370, loss = 0.79 (954.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:03:42.895488: step 21380, loss = 0.71 (967.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:03:44.234286: step 21390, loss = 0.79 (956.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:03:45.657504: step 21400, loss = 0.66 (899.4 examples/sec; 0.142 sec/batch)
2017-05-07 20:03:46.896065: step 21410, loss = 0.88 (1033.5 examples/sec; 0.124 sec/batch)
2017-05-07 20:03:48.250555: step 21420, loss = 0.79 (945.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:03:49.603501: step 21430, loss = 0.79 (946.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:03:50.930821: step 21440, loss = 0.77 (964.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:03:52.283887: step 21450, loss = 0.83 (946.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:03:53.609760: step 21460, loss = 0.71 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:03:54.9E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 439 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
57360: step 21470, loss = 0.81 (949.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:03:56.290863: step 21480, loss = 0.84 (959.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:03:57.614509: step 21490, loss = 0.73 (967.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:03:59.067789: step 21500, loss = 0.65 (880.8 examples/sec; 0.145 sec/batch)
2017-05-07 20:04:00.284369: step 21510, loss = 0.84 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-07 20:04:01.612413: step 21520, loss = 0.81 (963.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:04:02.933669: step 21530, loss = 0.92 (968.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:04:04.269417: step 21540, loss = 0.89 (958.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:04:05.622636: step 21550, loss = 0.74 (945.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:04:06.946443: step 21560, loss = 0.95 (966.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:04:08.290300: step 21570, loss = 0.71 (952.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:04:09.627043: step 21580, loss = 0.90 (957.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:04:10.960797: step 21590, loss = 0.76 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:04:12.387538: step 21600, loss = 0.77 (897.1 examples/sec; 0.143 sec/batch)
2017-05-07 20:04:13.601560: step 21610, loss = 0.87 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-07 20:04:14.942278: step 21620, loss = 1.00 (954.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:04:16.299527: step 21630, loss = 0.70 (943.1 examples/sec; 0.136 sec/batch)
2017-05-07 20:04:17.626391: step 21640, loss = 1.06 (964.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:04:18.987555: step 21650, loss = 0.94 (940.4 examples/sec; 0.136 sec/batch)
2017-05-07 20:04:20.310661: step 21660, loss = 0.69 (967.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:04:21.630190: step 21670, loss = 0.85 (970.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:04:22.955133: step 21680, loss = 0.77 (966.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:04:24.275347: step 21690, loss = 0.73 (969.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:04:25.713643: step 21700, loss = 0.95 (889.9 examples/sec; 0.144 sec/batch)
2017-05-07 20:04:26.956254: step 21710, loss = 0.71 (1030.1 examples/sec; 0.124 sec/batch)
2017-05-07 20:04:28.280245: step 21720, loss = 0.77 (966.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:04:29.604137: step 21730, loss = 0.74 (966.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:04:30.931683: step 21740, loss = 0.72 (964.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:04:32.279346: step 21750, loss = 0.75 (949.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:04:33.613534: step 21760, loss = 0.75 (959.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:04:34.929233: step 21770, loss = 0.97 (972.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:04:36.285176: step 21780, loss = 0.82 (944.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:04:37.617592: step 21790, loss = 0.87 (960.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:04:39.077720: step 21800, loss = 0.80 (876.6 examples/sec; 0.146 sec/batch)
2017-05-07 20:04:40.286332: step 21810, loss = 0.75 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-07 20:04:41.603501: step 21820, loss = 0.73 (971.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:04:42.951898: step 21830, loss = 0.70 (949.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:04:44.300390: step 21840, loss = 0.78 (949.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:04:45.643098: step 21850, loss = 0.74 (953.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:04:46.973547: step 21860, loss = 0.77 (962.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:04:48.312476: step 21870, loss = 0.78 (956.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:04:49.623984: step 21880, loss = 0.88 (976.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:04:50.943655: step 21890, loss = 0.75 (969.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:04:52.383956: step 21900, loss = 0.83 (888.7 examples/sec; 0.144 sec/batch)
2017-05-07 20:04:53.604488: step 21910, loss = 0.77 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-07 20:04:54.923761: step 21920, loss = 0.69 (970.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:04:56.296857: step 21930, loss = 0.72 (932.2 examples/sec; 0.137 sec/batch)
2017-05-07 20:04:57.605632: step 21940, loss = 0.82 (978.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:04:58.967330: step 21950, loss = 1.05 (940.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:05:00.301671: step 21960, loss = 0.87 (959.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:05:01.660219: step 21970, loss = 0.95 (942.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:05:03.002399: step 21980, loss = 0.79 (953.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:05:04.356817: step 21990, loss = 0.80 (945.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:05:05.806150: step 22000, loss = 0.79 (883.2 examples/sec; 0.145 sec/batch)
2017-05-07 20:05:07.037007: step 22010, loss = 0.84 (1039.9 examples/sec; 0.123 sec/batch)
2017-05-07 20:05:08.376587: step 22020, loss = 0.88 (955.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:05:09.712367: step 22030, loss = 0.81 (958.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:05:11.050967: step 22040, loss = 0.74 (956.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:05:12.380642: step 22050, loss = 0.82 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:05:13.691551: step 22060, loss = 0.78 (976.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:05:15.029131: step 22070, loss = 0.86 (956.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:05:16.384509: step 22080, loss = 0.77 (944.4 examples/sec; 0.136 sec/batch)
2017-05-07 20:05:17.714596: step 22090, loss = 0.75 (962.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:05:19.158278: step 22100, loss = 0.75 (886.6 examples/sec; 0.144 sec/batch)
2017-05-07 20:05:20.388729: step 22110, loss = 0.86 (1040.3 examples/sec; 0.123 sec/batch)
2017-05-07 20:05:21.740902: step 22120, loss = 0.93 (946.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:05:23.074299: step 22130, loss = 0.84 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:05:24.434245: step 22140, loss = 0.86 (941.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:05:25.763521: step 22150, loss = 0.82 (962.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:05:27.092144: step 22160, loss = 0.68 (963.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:05:28.428016: step 22170, loss = 0.90 (958.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:05:29.740702: step 22180, loss = 0.86 (975.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:05:31.073261: step 22190, loss = 0.80 (960.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:05:32.506005: step 22200, loss = 0.76 (893.4 examples/sec; 0.143 sec/batch)
2017-05-07 20:05:33.764278: step 22210, loss = 0.74 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:05:35.095424: step 22220, loss = 0.69 (961.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:05:36.432259: step 22230, loss = 0.84 (957.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:05:37.762205: step 22240, loss = 0.83 (962.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:05:39.105188: step 22250, loss = 0.78 (953.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:05:40.425969: step 22260, loss = 0.70 (969.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:05:41.767676: step 22270, loss = 0.76 (954.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:05:43.098199: step 22280, loss = 0.62 (962.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:05:44.444056: step 22290, loss = 0.83 (951.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:05:45.871561: step 22300, loss = 0.88 (896.7 examples/sec; 0.143 sec/batch)
2017-05-07 20:05:47.141498: step 22310, loss = 0.86 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:05:48.482280: step 22320, loss = 0.89 (954.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:05:49.808546: step 22330, loss = 0.56 (965.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:05:51.131399: step 22340, loss = 0.72 (967.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:05:52.497608: step 22350, loss = 0.80 (936.9 examples/sec; 0.137 sec/batch)
2017-05-07 20:05:53.790324: step 22360, loss = 0.82 (990.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:05:55.167583: step 22370, loss = 0.77 (929.4 examples/sec; 0.138 sec/batch)
2017-05-07 20:E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 458 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
05:56.496227: step 22380, loss = 0.79 (963.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:05:57.840801: step 22390, loss = 0.80 (952.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:05:59.273618: step 22400, loss = 0.77 (893.3 examples/sec; 0.143 sec/batch)
2017-05-07 20:06:00.509858: step 22410, loss = 0.76 (1035.4 examples/sec; 0.124 sec/batch)
2017-05-07 20:06:01.852771: step 22420, loss = 0.69 (953.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:06:03.197915: step 22430, loss = 0.76 (951.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:06:04.527150: step 22440, loss = 0.81 (963.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:06:05.885006: step 22450, loss = 0.77 (942.7 examples/sec; 0.136 sec/batch)
2017-05-07 20:06:07.216361: step 22460, loss = 0.70 (961.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:06:08.523763: step 22470, loss = 0.77 (979.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:06:09.846328: step 22480, loss = 0.91 (967.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:06:11.197701: step 22490, loss = 0.78 (947.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:06:12.696841: step 22500, loss = 0.83 (853.8 examples/sec; 0.150 sec/batch)
2017-05-07 20:06:13.867485: step 22510, loss = 0.71 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-07 20:06:15.199013: step 22520, loss = 0.67 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:06:16.546384: step 22530, loss = 0.83 (950.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:06:17.876719: step 22540, loss = 0.90 (962.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:06:19.238394: step 22550, loss = 0.65 (940.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:06:20.554502: step 22560, loss = 0.75 (972.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:06:21.877846: step 22570, loss = 0.83 (967.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:06:23.204903: step 22580, loss = 0.74 (964.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:06:24.530140: step 22590, loss = 0.68 (965.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:06:25.972658: step 22600, loss = 0.84 (887.3 examples/sec; 0.144 sec/batch)
2017-05-07 20:06:27.208715: step 22610, loss = 0.95 (1035.6 examples/sec; 0.124 sec/batch)
2017-05-07 20:06:28.536104: step 22620, loss = 0.73 (964.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:06:29.852365: step 22630, loss = 0.74 (972.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:06:31.206193: step 22640, loss = 0.77 (945.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:06:32.567764: step 22650, loss = 0.93 (940.1 examples/sec; 0.136 sec/batch)
2017-05-07 20:06:33.904669: step 22660, loss = 0.69 (957.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:06:35.240792: step 22670, loss = 0.93 (958.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:06:36.557258: step 22680, loss = 0.80 (972.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:06:37.884677: step 22690, loss = 0.88 (964.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:06:39.327218: step 22700, loss = 0.81 (887.3 examples/sec; 0.144 sec/batch)
2017-05-07 20:06:40.581663: step 22710, loss = 0.79 (1020.4 examples/sec; 0.125 sec/batch)
2017-05-07 20:06:41.924799: step 22720, loss = 0.80 (953.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:06:43.249860: step 22730, loss = 0.77 (966.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:06:44.553754: step 22740, loss = 0.76 (981.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:06:45.900030: step 22750, loss = 0.81 (950.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:06:47.211547: step 22760, loss = 0.82 (976.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:06:48.534736: step 22770, loss = 0.82 (967.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:06:49.861068: step 22780, loss = 0.77 (965.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:06:51.185356: step 22790, loss = 0.87 (966.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:06:52.602792: step 22800, loss = 0.76 (903.0 examples/sec; 0.142 sec/batch)
2017-05-07 20:06:53.848156: step 22810, loss = 0.69 (1027.8 examples/sec; 0.125 sec/batch)
2017-05-07 20:06:55.178730: step 22820, loss = 0.90 (962.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:06:56.501433: step 22830, loss = 0.92 (967.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:06:57.836140: step 22840, loss = 0.74 (959.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:06:59.190396: step 22850, loss = 0.83 (945.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:07:00.530484: step 22860, loss = 0.87 (955.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:07:01.854480: step 22870, loss = 0.83 (966.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:07:03.188520: step 22880, loss = 0.81 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:07:04.527014: step 22890, loss = 0.77 (956.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:07:06.004355: step 22900, loss = 0.76 (866.4 examples/sec; 0.148 sec/batch)
2017-05-07 20:07:07.218390: step 22910, loss = 0.89 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-07 20:07:08.591862: step 22920, loss = 0.70 (931.9 examples/sec; 0.137 sec/batch)
2017-05-07 20:07:09.915978: step 22930, loss = 0.69 (966.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:07:11.253092: step 22940, loss = 0.71 (957.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:07:12.613849: step 22950, loss = 0.61 (940.7 examples/sec; 0.136 sec/batch)
2017-05-07 20:07:13.962667: step 22960, loss = 0.70 (949.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:07:15.282166: step 22970, loss = 0.86 (970.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:07:16.615857: step 22980, loss = 0.74 (959.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:07:17.962032: step 22990, loss = 0.71 (950.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:07:19.410300: step 23000, loss = 0.63 (883.8 examples/sec; 0.145 sec/batch)
2017-05-07 20:07:20.639389: step 23010, loss = 0.64 (1041.4 examples/sec; 0.123 sec/batch)
2017-05-07 20:07:21.973437: step 23020, loss = 0.67 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:07:23.295342: step 23030, loss = 0.70 (968.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:07:24.622957: step 23040, loss = 0.70 (964.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:07:25.986506: step 23050, loss = 0.63 (938.7 examples/sec; 0.136 sec/batch)
2017-05-07 20:07:27.308301: step 23060, loss = 0.66 (968.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:07:28.636064: step 23070, loss = 0.76 (964.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:07:29.985533: step 23080, loss = 0.90 (948.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:07:31.321804: step 23090, loss = 0.83 (957.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:07:32.811276: step 23100, loss = 0.93 (859.4 examples/sec; 0.149 sec/batch)
2017-05-07 20:07:34.030643: step 23110, loss = 0.88 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-07 20:07:35.374542: step 23120, loss = 0.70 (952.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:07:36.687806: step 23130, loss = 0.71 (974.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:07:38.036318: step 23140, loss = 0.85 (949.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:07:39.404836: step 23150, loss = 0.72 (935.3 examples/sec; 0.137 sec/batch)
2017-05-07 20:07:40.726062: step 23160, loss = 0.87 (968.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:07:42.062412: step 23170, loss = 0.95 (957.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:07:43.422749: step 23180, loss = 0.83 (940.9 examples/sec; 0.136 sec/batch)
2017-05-07 20:07:44.749251: step 23190, loss = 0.84 (964.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:07:46.166828: step 23200, loss = 0.69 (902.9 examples/sec; 0.142 sec/batch)
2017-05-07 20:07:47.383610: step 23210, loss = 0.93 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-07 20:07:48.710841: step 23220, loss = 0.70 (964.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:07:50.029713: step 23230, loss = 0.78 (970.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:07:51.374956: step 23240, loss = 0.73 (951.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:07:52.735704: step 23250, loss = 0.75 (940.7 examples/sec; 0.136 sec/batch)
2017-05-07 20:07:54.062163: step 23260, loss = 0.90 (965.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:07:55.400444: step 23270, loss = 0.62 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:07:56.722761: step 23280, loss = 0.69 (968.0 examples/sec; 0.132 sec/batch)
2017-05E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 476 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
-07 20:07:58.046150: step 23290, loss = 0.72 (967.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:07:59.473723: step 23300, loss = 0.87 (896.6 examples/sec; 0.143 sec/batch)
2017-05-07 20:08:00.714809: step 23310, loss = 0.84 (1031.4 examples/sec; 0.124 sec/batch)
2017-05-07 20:08:02.052218: step 23320, loss = 0.75 (957.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:08:03.383050: step 23330, loss = 0.81 (961.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:08:04.704586: step 23340, loss = 0.78 (968.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:08:06.040307: step 23350, loss = 0.88 (958.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:08:07.368388: step 23360, loss = 0.81 (963.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:08:08.703086: step 23370, loss = 0.75 (959.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:08:10.035811: step 23380, loss = 0.86 (960.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:08:11.372872: step 23390, loss = 0.75 (957.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:08:12.787142: step 23400, loss = 0.74 (905.1 examples/sec; 0.141 sec/batch)
2017-05-07 20:08:14.016581: step 23410, loss = 0.79 (1041.1 examples/sec; 0.123 sec/batch)
2017-05-07 20:08:15.355554: step 23420, loss = 0.68 (956.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:08:16.685921: step 23430, loss = 0.87 (962.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:08:18.024679: step 23440, loss = 0.86 (956.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:08:19.377381: step 23450, loss = 0.72 (946.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:08:20.730753: step 23460, loss = 0.75 (945.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:08:22.074199: step 23470, loss = 0.68 (952.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:08:23.379055: step 23480, loss = 0.76 (980.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:08:24.712609: step 23490, loss = 0.86 (959.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:08:26.167539: step 23500, loss = 0.79 (879.8 examples/sec; 0.145 sec/batch)
2017-05-07 20:08:27.379317: step 23510, loss = 0.74 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-07 20:08:28.703645: step 23520, loss = 0.77 (966.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:08:30.048152: step 23530, loss = 0.73 (952.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:08:31.372572: step 23540, loss = 0.88 (966.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:08:32.713286: step 23550, loss = 0.71 (954.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:08:34.052230: step 23560, loss = 0.71 (956.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:08:35.389564: step 23570, loss = 0.79 (957.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:08:36.692044: step 23580, loss = 0.78 (982.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:08:38.026050: step 23590, loss = 0.74 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:08:39.475593: step 23600, loss = 0.68 (883.0 examples/sec; 0.145 sec/batch)
2017-05-07 20:08:40.696881: step 23610, loss = 0.68 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-07 20:08:42.028361: step 23620, loss = 0.82 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:08:43.340767: step 23630, loss = 0.73 (975.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:08:44.678033: step 23640, loss = 0.67 (957.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:08:46.018597: step 23650, loss = 0.87 (954.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:08:47.332840: step 23660, loss = 0.83 (973.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:08:48.661086: step 23670, loss = 0.72 (963.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:08:49.992105: step 23680, loss = 0.67 (961.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:08:51.310401: step 23690, loss = 0.78 (970.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:08:52.758904: step 23700, loss = 0.90 (883.7 examples/sec; 0.145 sec/batch)
2017-05-07 20:08:53.988633: step 23710, loss = 0.76 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-07 20:08:55.343596: step 23720, loss = 0.69 (944.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:08:56.685346: step 23730, loss = 0.77 (954.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:08:57.998955: step 23740, loss = 0.80 (974.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:08:59.369040: step 23750, loss = 0.73 (934.2 examples/sec; 0.137 sec/batch)
2017-05-07 20:09:00.683420: step 23760, loss = 0.92 (973.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:09:02.012922: step 23770, loss = 0.96 (962.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:09:03.348365: step 23780, loss = 0.78 (958.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:09:04.675932: step 23790, loss = 0.75 (964.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:09:06.117005: step 23800, loss = 0.78 (888.2 examples/sec; 0.144 sec/batch)
2017-05-07 20:09:07.328172: step 23810, loss = 0.90 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-07 20:09:08.657191: step 23820, loss = 0.86 (963.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:09:10.014444: step 23830, loss = 0.84 (943.1 examples/sec; 0.136 sec/batch)
2017-05-07 20:09:11.335838: step 23840, loss = 0.70 (968.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:09:12.693789: step 23850, loss = 0.97 (942.6 examples/sec; 0.136 sec/batch)
2017-05-07 20:09:14.035412: step 23860, loss = 0.92 (954.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:09:15.358954: step 23870, loss = 0.95 (967.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:09:16.681676: step 23880, loss = 0.68 (967.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:09:18.019629: step 23890, loss = 0.70 (956.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:09:19.449857: step 23900, loss = 0.87 (895.0 examples/sec; 0.143 sec/batch)
2017-05-07 20:09:20.673599: step 23910, loss = 0.87 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-07 20:09:21.976679: step 23920, loss = 0.81 (982.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:09:23.299309: step 23930, loss = 0.84 (967.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:09:24.627723: step 23940, loss = 0.77 (963.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:09:25.996512: step 23950, loss = 0.77 (935.1 examples/sec; 0.137 sec/batch)
2017-05-07 20:09:27.316245: step 23960, loss = 0.95 (969.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:09:28.633205: step 23970, loss = 0.89 (971.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:09:29.967536: step 23980, loss = 0.78 (959.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:09:31.308341: step 23990, loss = 0.80 (954.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:09:32.770147: step 24000, loss = 0.93 (875.6 examples/sec; 0.146 sec/batch)
2017-05-07 20:09:33.977035: step 24010, loss = 0.98 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-07 20:09:35.311774: step 24020, loss = 0.83 (959.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:09:36.655604: step 24030, loss = 0.74 (952.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:09:37.981716: step 24040, loss = 0.64 (965.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:09:39.319482: step 24050, loss = 0.76 (956.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:09:40.630250: step 24060, loss = 0.74 (976.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:09:41.962273: step 24070, loss = 0.73 (960.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:09:43.291288: step 24080, loss = 0.84 (963.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:09:44.615732: step 24090, loss = 0.72 (966.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:09:46.029836: step 24100, loss = 0.76 (905.2 examples/sec; 0.141 sec/batch)
2017-05-07 20:09:47.292909: step 24110, loss = 0.79 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:09:48.619255: step 24120, loss = 0.75 (965.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:09:49.953415: step 24130, loss = 0.72 (959.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:09:51.288161: step 24140, loss = 0.83 (959.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:09:52.647921: step 24150, loss = 0.83 (941.3 examples/sec; 0.136 sec/batch)
2017-05-07 20:09:53.960359: step 24160, loss = 0.69 (975.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:09:55.297118: step 24170, loss = 0.81 (957.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:09:56.623113: step 24180, loss = 0.77 (965.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:09:57.938503: step 24190, loss = 0.66 (973.1 examples/sec; 0.132 sec/batch)
E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 496 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
2017-05-07 20:09:59.372404: step 24200, loss = 0.82 (892.7 examples/sec; 0.143 sec/batch)
2017-05-07 20:10:00.615593: step 24210, loss = 0.82 (1029.6 examples/sec; 0.124 sec/batch)
2017-05-07 20:10:01.958497: step 24220, loss = 0.95 (953.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:10:03.320970: step 24230, loss = 0.68 (939.5 examples/sec; 0.136 sec/batch)
2017-05-07 20:10:04.658747: step 24240, loss = 0.67 (956.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:10:05.997215: step 24250, loss = 0.86 (956.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:10:07.315879: step 24260, loss = 0.64 (970.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:10:08.639935: step 24270, loss = 0.74 (966.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:10:09.963792: step 24280, loss = 0.82 (966.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:10:11.302672: step 24290, loss = 0.76 (956.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:10:12.738694: step 24300, loss = 0.77 (891.3 examples/sec; 0.144 sec/batch)
2017-05-07 20:10:13.962939: step 24310, loss = 0.90 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-07 20:10:15.291260: step 24320, loss = 0.69 (963.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:10:16.604288: step 24330, loss = 0.80 (974.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:10:17.935699: step 24340, loss = 0.65 (961.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:10:19.263780: step 24350, loss = 0.99 (963.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:10:20.577651: step 24360, loss = 0.72 (974.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:10:21.912136: step 24370, loss = 0.92 (959.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:10:23.262078: step 24380, loss = 0.90 (948.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:10:24.568775: step 24390, loss = 0.82 (979.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:10:25.983355: step 24400, loss = 0.83 (904.9 examples/sec; 0.141 sec/batch)
2017-05-07 20:10:27.244593: step 24410, loss = 0.73 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:10:28.591728: step 24420, loss = 0.71 (950.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:10:29.924915: step 24430, loss = 0.92 (960.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:10:31.275299: step 24440, loss = 0.86 (947.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:10:32.616121: step 24450, loss = 0.75 (954.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:10:33.942748: step 24460, loss = 0.82 (964.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:10:35.290085: step 24470, loss = 0.92 (950.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:10:36.617666: step 24480, loss = 0.76 (964.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:10:37.975004: step 24490, loss = 0.72 (943.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:10:39.419321: step 24500, loss = 0.79 (886.2 examples/sec; 0.144 sec/batch)
2017-05-07 20:10:40.629641: step 24510, loss = 0.68 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-07 20:10:41.959298: step 24520, loss = 0.71 (962.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:10:43.292491: step 24530, loss = 0.66 (960.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:10:44.617626: step 24540, loss = 0.75 (965.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:10:45.985198: step 24550, loss = 0.71 (936.0 examples/sec; 0.137 sec/batch)
2017-05-07 20:10:47.299529: step 24560, loss = 0.73 (973.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:10:48.624219: step 24570, loss = 0.73 (966.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:10:49.941720: step 24580, loss = 0.66 (971.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:10:51.267742: step 24590, loss = 0.76 (965.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:10:52.715472: step 24600, loss = 0.77 (884.2 examples/sec; 0.145 sec/batch)
2017-05-07 20:10:53.974826: step 24610, loss = 0.75 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:10:55.305412: step 24620, loss = 0.92 (962.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:10:56.653840: step 24630, loss = 0.87 (949.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:10:57.973577: step 24640, loss = 0.85 (969.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:10:59.322510: step 24650, loss = 0.75 (948.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:11:00.632740: step 24660, loss = 0.79 (976.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:11:01.958813: step 24670, loss = 0.74 (965.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:11:03.272767: step 24680, loss = 0.82 (974.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:11:04.594969: step 24690, loss = 0.84 (968.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:11:06.017382: step 24700, loss = 0.85 (899.9 examples/sec; 0.142 sec/batch)
2017-05-07 20:11:07.236633: step 24710, loss = 0.88 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-07 20:11:08.557770: step 24720, loss = 0.87 (968.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:11:09.875547: step 24730, loss = 0.83 (971.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:11:11.214318: step 24740, loss = 0.75 (956.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:11:12.538100: step 24750, loss = 0.77 (966.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:11:13.882214: step 24760, loss = 0.99 (952.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:11:15.214739: step 24770, loss = 0.69 (960.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:11:16.529309: step 24780, loss = 0.71 (973.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:11:17.835661: step 24790, loss = 0.82 (979.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:11:19.282149: step 24800, loss = 0.72 (884.9 examples/sec; 0.145 sec/batch)
2017-05-07 20:11:20.510188: step 24810, loss = 0.56 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-07 20:11:21.831617: step 24820, loss = 0.75 (968.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:11:23.174075: step 24830, loss = 0.87 (953.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:11:24.492725: step 24840, loss = 0.84 (970.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:11:25.838776: step 24850, loss = 0.74 (950.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:11:27.177048: step 24860, loss = 0.78 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:11:28.491138: step 24870, loss = 0.69 (974.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:11:29.848177: step 24880, loss = 0.69 (943.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:11:31.200415: step 24890, loss = 0.73 (946.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:11:32.640656: step 24900, loss = 0.71 (888.7 examples/sec; 0.144 sec/batch)
2017-05-07 20:11:33.878795: step 24910, loss = 0.67 (1033.8 examples/sec; 0.124 sec/batch)
2017-05-07 20:11:35.207375: step 24920, loss = 0.63 (963.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:11:36.539411: step 24930, loss = 0.65 (960.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:11:37.881885: step 24940, loss = 0.78 (953.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:11:39.224904: step 24950, loss = 0.83 (953.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:11:40.546270: step 24960, loss = 0.68 (968.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:11:41.877983: step 24970, loss = 0.79 (961.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:11:43.216631: step 24980, loss = 0.69 (956.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:11:44.560999: step 24990, loss = 0.80 (952.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:11:46.005027: step 25000, loss = 0.76 (886.4 examples/sec; 0.144 sec/batch)
2017-05-07 20:11:47.232063: step 25010, loss = 0.86 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-07 20:11:48.574936: step 25020, loss = 1.01 (953.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:11:49.913565: step 25030, loss = 0.82 (956.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:11:51.233752: step 25040, loss = 0.77 (969.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:11:52.585286: step 25050, loss = 0.70 (947.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:11:53.905111: step 25060, loss = 0.67 (969.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:11:55.253841: step 25070, loss = 0.79 (949.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:11:56.568684: step 25080, loss = 0.67 (973.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:11:57.901099: step 25090, loss = 0.92 (960.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:11:59.335209: step 25100, loss = 0.89 (892.5 examples/sec; 0.143 sec/E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 516 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
batch)
2017-05-07 20:12:00.558577: step 25110, loss = 0.85 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-07 20:12:01.881240: step 25120, loss = 0.78 (967.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:12:03.214170: step 25130, loss = 0.80 (960.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:12:04.559562: step 25140, loss = 0.89 (951.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:12:05.877359: step 25150, loss = 0.60 (971.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:12:07.217679: step 25160, loss = 0.76 (955.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:12:08.541005: step 25170, loss = 0.76 (967.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:12:09.870389: step 25180, loss = 0.75 (962.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:12:11.191437: step 25190, loss = 0.72 (968.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:12:12.624820: step 25200, loss = 0.72 (893.0 examples/sec; 0.143 sec/batch)
2017-05-07 20:12:13.880853: step 25210, loss = 0.70 (1019.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:12:15.222389: step 25220, loss = 0.70 (954.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:12:16.555773: step 25230, loss = 0.75 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:12:17.858688: step 25240, loss = 0.79 (982.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:12:19.200087: step 25250, loss = 0.81 (954.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:12:20.553158: step 25260, loss = 0.65 (946.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:12:21.884391: step 25270, loss = 0.81 (961.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:12:23.215553: step 25280, loss = 0.95 (961.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:12:24.542054: step 25290, loss = 0.89 (964.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:12:25.978951: step 25300, loss = 0.78 (890.8 examples/sec; 0.144 sec/batch)
2017-05-07 20:12:27.236181: step 25310, loss = 0.85 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:12:28.559707: step 25320, loss = 0.93 (967.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:12:29.890831: step 25330, loss = 0.93 (961.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:12:31.232473: step 25340, loss = 0.72 (954.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:12:32.590739: step 25350, loss = 0.74 (942.4 examples/sec; 0.136 sec/batch)
2017-05-07 20:12:33.916902: step 25360, loss = 0.89 (965.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:12:35.243432: step 25370, loss = 0.59 (964.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:12:36.553520: step 25380, loss = 0.73 (977.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:12:37.879242: step 25390, loss = 1.00 (965.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:12:39.294828: step 25400, loss = 0.80 (904.2 examples/sec; 0.142 sec/batch)
2017-05-07 20:12:40.564621: step 25410, loss = 0.83 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:12:41.891227: step 25420, loss = 0.99 (964.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:12:43.233113: step 25430, loss = 0.68 (953.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:12:44.540725: step 25440, loss = 0.75 (978.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:12:45.895410: step 25450, loss = 0.78 (944.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:12:47.216326: step 25460, loss = 0.80 (969.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:12:48.562369: step 25470, loss = 0.70 (950.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:12:49.898975: step 25480, loss = 0.82 (957.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:12:51.202123: step 25490, loss = 0.80 (982.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:12:52.641907: step 25500, loss = 0.88 (889.0 examples/sec; 0.144 sec/batch)
2017-05-07 20:12:53.888229: step 25510, loss = 0.88 (1027.0 examples/sec; 0.125 sec/batch)
2017-05-07 20:12:55.218375: step 25520, loss = 0.79 (962.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:12:56.536849: step 25530, loss = 0.68 (970.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:12:57.904815: step 25540, loss = 0.69 (935.7 examples/sec; 0.137 sec/batch)
2017-05-07 20:12:59.239486: step 25550, loss = 0.71 (959.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:13:00.546687: step 25560, loss = 0.80 (979.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:13:01.911668: step 25570, loss = 0.72 (937.7 examples/sec; 0.136 sec/batch)
2017-05-07 20:13:03.230508: step 25580, loss = 0.73 (970.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:13:04.552915: step 25590, loss = 0.84 (967.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:13:05.984324: step 25600, loss = 0.70 (894.2 examples/sec; 0.143 sec/batch)
2017-05-07 20:13:07.223008: step 25610, loss = 0.77 (1033.4 examples/sec; 0.124 sec/batch)
2017-05-07 20:13:08.532049: step 25620, loss = 0.73 (977.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:13:09.865897: step 25630, loss = 0.83 (959.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:13:11.198317: step 25640, loss = 0.76 (960.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:13:12.550029: step 25650, loss = 1.08 (946.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:13:13.865929: step 25660, loss = 0.73 (972.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:13:15.216559: step 25670, loss = 0.67 (947.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:13:16.555207: step 25680, loss = 0.75 (956.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:13:17.884521: step 25690, loss = 0.90 (962.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:13:19.331487: step 25700, loss = 0.69 (884.6 examples/sec; 0.145 sec/batch)
2017-05-07 20:13:20.558870: step 25710, loss = 0.70 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-07 20:13:21.884157: step 25720, loss = 0.78 (965.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:13:23.220740: step 25730, loss = 0.95 (957.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:13:24.549551: step 25740, loss = 0.69 (963.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:13:25.891097: step 25750, loss = 0.89 (954.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:13:27.212083: step 25760, loss = 0.89 (969.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:13:28.553876: step 25770, loss = 0.84 (953.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:13:29.890246: step 25780, loss = 0.80 (957.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:13:31.218307: step 25790, loss = 0.84 (963.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:13:32.671404: step 25800, loss = 0.92 (880.9 examples/sec; 0.145 sec/batch)
2017-05-07 20:13:33.894105: step 25810, loss = 0.84 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-07 20:13:35.222402: step 25820, loss = 0.77 (963.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:13:36.555883: step 25830, loss = 0.61 (959.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:13:37.894208: step 25840, loss = 0.86 (956.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:13:39.252413: step 25850, loss = 0.83 (942.4 examples/sec; 0.136 sec/batch)
2017-05-07 20:13:40.573116: step 25860, loss = 0.73 (969.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:13:41.910143: step 25870, loss = 0.82 (957.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:13:43.231246: step 25880, loss = 0.90 (968.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:13:44.560441: step 25890, loss = 0.82 (963.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:13:45.998397: step 25900, loss = 0.93 (890.1 examples/sec; 0.144 sec/batch)
2017-05-07 20:13:47.228754: step 25910, loss = 0.91 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-07 20:13:48.572059: step 25920, loss = 0.92 (952.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:13:49.917203: step 25930, loss = 0.68 (951.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:13:51.256216: step 25940, loss = 0.73 (955.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:13:52.603602: step 25950, loss = 0.73 (950.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:13:53.933272: step 25960, loss = 0.80 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:13:55.263142: step 25970, loss = 0.75 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:13:56.592938: step 25980, loss = 0.97 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:13:57.926592: step 25990, loss = 0.83 (959.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:13:59.369897: step 26000, loss = 0.65 (886.9 examples/sec; 0.144 sec/batch)
2017-05-07 20:14:00.571868: step 26010, loss = 0.87 (1064.9 examples/sec; 0.E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 534 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
120 sec/batch)
2017-05-07 20:14:01.908030: step 26020, loss = 0.85 (958.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:14:03.226408: step 26030, loss = 0.75 (970.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:14:04.552982: step 26040, loss = 0.95 (964.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:14:05.919322: step 26050, loss = 0.71 (936.8 examples/sec; 0.137 sec/batch)
2017-05-07 20:14:07.239480: step 26060, loss = 0.83 (969.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:14:08.589820: step 26070, loss = 0.85 (947.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:14:09.905503: step 26080, loss = 0.82 (972.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:14:11.237396: step 26090, loss = 0.85 (961.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:14:12.716894: step 26100, loss = 0.67 (865.2 examples/sec; 0.148 sec/batch)
2017-05-07 20:14:13.918925: step 26110, loss = 0.61 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-07 20:14:15.278936: step 26120, loss = 0.75 (941.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:14:16.602349: step 26130, loss = 0.85 (967.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:14:17.930200: step 26140, loss = 0.79 (964.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:14:19.298304: step 26150, loss = 0.67 (935.6 examples/sec; 0.137 sec/batch)
2017-05-07 20:14:20.622383: step 26160, loss = 0.79 (966.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:14:21.925781: step 26170, loss = 0.80 (982.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:14:23.250069: step 26180, loss = 0.83 (966.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:14:24.555816: step 26190, loss = 0.99 (980.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:14:26.003898: step 26200, loss = 0.78 (883.9 examples/sec; 0.145 sec/batch)
2017-05-07 20:14:27.250184: step 26210, loss = 0.74 (1027.1 examples/sec; 0.125 sec/batch)
2017-05-07 20:14:28.570038: step 26220, loss = 0.61 (969.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:14:29.906677: step 26230, loss = 0.78 (957.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:14:31.238226: step 26240, loss = 0.89 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:14:32.572194: step 26250, loss = 0.66 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:14:33.905863: step 26260, loss = 0.88 (959.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:14:35.247521: step 26270, loss = 0.77 (954.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:14:36.574132: step 26280, loss = 0.58 (964.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:14:37.919253: step 26290, loss = 0.75 (951.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:14:39.356006: step 26300, loss = 1.06 (890.9 examples/sec; 0.144 sec/batch)
2017-05-07 20:14:40.560588: step 26310, loss = 0.70 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-07 20:14:41.895335: step 26320, loss = 0.70 (959.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:14:43.226701: step 26330, loss = 0.65 (961.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:14:44.549173: step 26340, loss = 0.68 (967.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:14:45.898335: step 26350, loss = 0.82 (948.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:14:47.227168: step 26360, loss = 0.80 (963.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:14:48.581648: step 26370, loss = 0.82 (945.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:14:49.922007: step 26380, loss = 0.78 (955.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:14:51.248211: step 26390, loss = 0.64 (965.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:14:52.671564: step 26400, loss = 0.64 (899.3 examples/sec; 0.142 sec/batch)
2017-05-07 20:14:53.914369: step 26410, loss = 0.85 (1029.9 examples/sec; 0.124 sec/batch)
2017-05-07 20:14:55.244918: step 26420, loss = 0.68 (962.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:14:56.567777: step 26430, loss = 0.65 (967.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:14:57.892232: step 26440, loss = 0.78 (966.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:14:59.261258: step 26450, loss = 0.60 (935.0 examples/sec; 0.137 sec/batch)
2017-05-07 20:15:00.579894: step 26460, loss = 0.68 (970.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:15:01.939890: step 26470, loss = 0.85 (941.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:15:03.256903: step 26480, loss = 0.69 (971.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:15:04.580060: step 26490, loss = 0.84 (967.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:15:06.022745: step 26500, loss = 0.70 (887.2 examples/sec; 0.144 sec/batch)
2017-05-07 20:15:07.256489: step 26510, loss = 0.69 (1037.5 examples/sec; 0.123 sec/batch)
2017-05-07 20:15:08.585976: step 26520, loss = 0.85 (962.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:15:09.899431: step 26530, loss = 0.72 (974.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:15:11.221941: step 26540, loss = 0.83 (967.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:15:12.558768: step 26550, loss = 0.89 (957.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:15:13.894169: step 26560, loss = 0.73 (958.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:15:15.212559: step 26570, loss = 0.83 (970.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:15:16.545413: step 26580, loss = 0.79 (960.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:15:17.877082: step 26590, loss = 0.83 (961.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:15:19.369873: step 26600, loss = 0.79 (857.5 examples/sec; 0.149 sec/batch)
2017-05-07 20:15:20.570505: step 26610, loss = 0.74 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-07 20:15:21.902429: step 26620, loss = 0.72 (961.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:15:23.232905: step 26630, loss = 0.96 (962.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:15:24.553842: step 26640, loss = 0.73 (969.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:15:25.887255: step 26650, loss = 0.73 (959.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:15:27.204798: step 26660, loss = 0.75 (971.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:15:28.539970: step 26670, loss = 0.73 (958.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:15:29.883114: step 26680, loss = 0.85 (953.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:15:31.198706: step 26690, loss = 0.64 (972.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:15:32.634884: step 26700, loss = 1.00 (891.3 examples/sec; 0.144 sec/batch)
2017-05-07 20:15:33.885482: step 26710, loss = 0.72 (1023.5 examples/sec; 0.125 sec/batch)
2017-05-07 20:15:35.221900: step 26720, loss = 0.84 (957.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:15:36.560633: step 26730, loss = 0.84 (956.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:15:37.875932: step 26740, loss = 0.91 (973.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:15:39.225396: step 26750, loss = 0.85 (948.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:15:40.532326: step 26760, loss = 0.71 (979.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:15:41.863312: step 26770, loss = 0.76 (961.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:15:43.191719: step 26780, loss = 0.76 (963.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:15:44.524705: step 26790, loss = 0.82 (960.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:15:45.967116: step 26800, loss = 0.79 (887.4 examples/sec; 0.144 sec/batch)
2017-05-07 20:15:47.181353: step 26810, loss = 0.68 (1054.2 examples/sec; 0.121 sec/batch)
2017-05-07 20:15:48.522858: step 26820, loss = 0.94 (954.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:15:49.842791: step 26830, loss = 0.78 (969.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:15:51.178886: step 26840, loss = 0.76 (958.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:15:52.541990: step 26850, loss = 0.75 (939.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:15:53.847743: step 26860, loss = 0.73 (980.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:15:55.226683: step 26870, loss = 0.79 (928.2 examples/sec; 0.138 sec/batch)
2017-05-07 20:15:56.550884: step 26880, loss = 0.67 (966.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:15:57.878635: step 26890, loss = 0.71 (964.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:15:59.307653: step 26900, loss = 0.86 (895.7 examples/sec; 0.143 sec/batch)
2017-05-07 20:16:00.540178: step 26910, loss = 0.74 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-07 20:16:01.868768: step 26920, loss = 0.71 (963.4 examples/E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 553 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
sec; 0.133 sec/batch)
2017-05-07 20:16:03.184593: step 26930, loss = 1.06 (972.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:16:04.506520: step 26940, loss = 0.64 (968.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:16:05.876179: step 26950, loss = 0.79 (934.5 examples/sec; 0.137 sec/batch)
2017-05-07 20:16:07.206319: step 26960, loss = 0.60 (962.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:16:08.553965: step 26970, loss = 0.82 (949.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:16:09.891608: step 26980, loss = 0.61 (956.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:16:11.215991: step 26990, loss = 0.70 (966.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:16:12.653748: step 27000, loss = 0.78 (890.3 examples/sec; 0.144 sec/batch)
2017-05-07 20:16:13.898491: step 27010, loss = 0.87 (1028.4 examples/sec; 0.124 sec/batch)
2017-05-07 20:16:15.240914: step 27020, loss = 0.77 (953.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:16:16.558803: step 27030, loss = 0.84 (971.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:16:17.902935: step 27040, loss = 0.98 (952.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:16:19.245391: step 27050, loss = 0.82 (953.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:16:20.547527: step 27060, loss = 0.72 (983.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:16:21.873301: step 27070, loss = 0.80 (965.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:16:23.207275: step 27080, loss = 0.76 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:16:24.543045: step 27090, loss = 0.75 (958.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:16:26.034638: step 27100, loss = 0.72 (858.1 examples/sec; 0.149 sec/batch)
2017-05-07 20:16:27.208976: step 27110, loss = 0.79 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-07 20:16:28.543218: step 27120, loss = 0.55 (959.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:16:29.875278: step 27130, loss = 0.76 (960.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:16:31.211319: step 27140, loss = 0.83 (958.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:16:32.563260: step 27150, loss = 0.63 (946.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:16:33.879354: step 27160, loss = 0.77 (972.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:16:35.199994: step 27170, loss = 0.67 (969.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:16:36.536984: step 27180, loss = 0.68 (957.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:16:37.852142: step 27190, loss = 0.73 (973.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:16:39.293528: step 27200, loss = 0.73 (888.0 examples/sec; 0.144 sec/batch)
2017-05-07 20:16:40.569982: step 27210, loss = 0.94 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:16:41.903108: step 27220, loss = 0.79 (960.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:16:43.230706: step 27230, loss = 0.80 (964.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:16:44.567352: step 27240, loss = 0.68 (957.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:16:45.900299: step 27250, loss = 0.78 (960.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:16:47.238320: step 27260, loss = 0.77 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:16:48.577865: step 27270, loss = 0.82 (955.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:16:49.907389: step 27280, loss = 0.85 (962.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:16:51.256233: step 27290, loss = 0.84 (949.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:16:52.716249: step 27300, loss = 0.83 (876.7 examples/sec; 0.146 sec/batch)
2017-05-07 20:16:53.971049: step 27310, loss = 0.87 (1020.1 examples/sec; 0.125 sec/batch)
2017-05-07 20:16:55.304115: step 27320, loss = 0.77 (960.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:16:56.619358: step 27330, loss = 0.67 (973.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:16:57.940386: step 27340, loss = 0.70 (968.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:16:59.305250: step 27350, loss = 0.75 (937.8 examples/sec; 0.136 sec/batch)
2017-05-07 20:17:00.623159: step 27360, loss = 0.85 (971.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:17:01.961896: step 27370, loss = 0.60 (956.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:17:03.295487: step 27380, loss = 0.80 (959.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:17:04.614122: step 27390, loss = 0.67 (970.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:17:06.048042: step 27400, loss = 0.76 (892.7 examples/sec; 0.143 sec/batch)
2017-05-07 20:17:07.260961: step 27410, loss = 0.68 (1055.3 examples/sec; 0.121 sec/batch)
2017-05-07 20:17:08.588130: step 27420, loss = 0.76 (964.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:17:09.895641: step 27430, loss = 0.79 (979.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:17:11.210245: step 27440, loss = 0.92 (973.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:17:12.548589: step 27450, loss = 0.77 (956.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:17:13.860829: step 27460, loss = 0.74 (975.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:17:15.188943: step 27470, loss = 0.65 (963.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:17:16.518912: step 27480, loss = 0.75 (962.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:17:17.844435: step 27490, loss = 0.72 (965.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:17:19.295618: step 27500, loss = 0.69 (882.0 examples/sec; 0.145 sec/batch)
2017-05-07 20:17:20.551757: step 27510, loss = 0.64 (1019.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:17:21.851722: step 27520, loss = 1.04 (984.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:17:23.189537: step 27530, loss = 0.71 (956.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:17:24.528668: step 27540, loss = 0.76 (955.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:17:25.850292: step 27550, loss = 0.73 (968.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:17:27.173793: step 27560, loss = 0.75 (967.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:17:28.494632: step 27570, loss = 0.73 (969.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:17:29.834761: step 27580, loss = 0.83 (955.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:17:31.174416: step 27590, loss = 0.75 (955.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:17:32.620115: step 27600, loss = 0.87 (885.4 examples/sec; 0.145 sec/batch)
2017-05-07 20:17:33.863401: step 27610, loss = 0.73 (1029.5 examples/sec; 0.124 sec/batch)
2017-05-07 20:17:35.204603: step 27620, loss = 0.73 (954.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:17:36.533807: step 27630, loss = 0.87 (963.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:17:37.869860: step 27640, loss = 0.92 (958.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:17:39.225389: step 27650, loss = 0.86 (944.3 examples/sec; 0.136 sec/batch)
2017-05-07 20:17:40.562098: step 27660, loss = 0.59 (957.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:17:41.903388: step 27670, loss = 0.75 (954.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:17:43.251267: step 27680, loss = 0.72 (949.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:17:44.567307: step 27690, loss = 0.79 (972.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:17:46.042193: step 27700, loss = 0.90 (867.9 examples/sec; 0.147 sec/batch)
2017-05-07 20:17:47.208415: step 27710, loss = 0.80 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-07 20:17:48.533963: step 27720, loss = 0.81 (965.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:17:49.858703: step 27730, loss = 0.86 (966.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:17:51.184052: step 27740, loss = 0.74 (965.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:17:52.536013: step 27750, loss = 0.90 (946.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:17:53.833616: step 27760, loss = 0.57 (986.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:17:55.177348: step 27770, loss = 0.86 (952.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:17:56.506065: step 27780, loss = 0.64 (963.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:17:57.850936: step 27790, loss = 0.87 (951.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:17:59.329503: step 27800, loss = 0.66 (865.7 examples/sec; 0.148 sec/batch)
2017-05-07 20:18:00.545646: step 27810, loss = 0.70 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-07 20:18:01.873596: step 27820, loss = 0.78 (963.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:18:03.222030: step 27830, loss = 0.68 (949.3 exE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 571 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
amples/sec; 0.135 sec/batch)
2017-05-07 20:18:04.561753: step 27840, loss = 0.71 (955.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:18:05.913553: step 27850, loss = 0.81 (946.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:18:07.264510: step 27860, loss = 0.59 (947.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:18:08.600667: step 27870, loss = 0.69 (958.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:18:09.955818: step 27880, loss = 0.74 (944.5 examples/sec; 0.136 sec/batch)
2017-05-07 20:18:11.302428: step 27890, loss = 0.70 (950.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:18:12.734587: step 27900, loss = 0.75 (893.8 examples/sec; 0.143 sec/batch)
2017-05-07 20:18:13.950728: step 27910, loss = 0.87 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-07 20:18:15.303367: step 27920, loss = 0.70 (946.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:18:16.629180: step 27930, loss = 0.78 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:18:17.964171: step 27940, loss = 1.03 (958.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:18:19.330758: step 27950, loss = 0.94 (936.6 examples/sec; 0.137 sec/batch)
2017-05-07 20:18:20.667569: step 27960, loss = 0.92 (957.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:18:22.022714: step 27970, loss = 0.77 (944.5 examples/sec; 0.136 sec/batch)
2017-05-07 20:18:23.344415: step 27980, loss = 0.71 (968.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:18:24.662004: step 27990, loss = 0.67 (971.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:18:26.101250: step 28000, loss = 0.87 (889.4 examples/sec; 0.144 sec/batch)
2017-05-07 20:18:27.320507: step 28010, loss = 0.62 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-07 20:18:28.641694: step 28020, loss = 0.79 (968.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:18:29.964087: step 28030, loss = 0.75 (967.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:18:31.302979: step 28040, loss = 0.81 (956.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:18:32.650979: step 28050, loss = 0.67 (949.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:18:33.964298: step 28060, loss = 0.74 (974.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:18:35.300414: step 28070, loss = 0.89 (958.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:18:36.613792: step 28080, loss = 0.76 (974.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:18:37.956508: step 28090, loss = 0.70 (953.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:18:39.420698: step 28100, loss = 0.59 (874.2 examples/sec; 0.146 sec/batch)
2017-05-07 20:18:40.674815: step 28110, loss = 0.80 (1020.6 examples/sec; 0.125 sec/batch)
2017-05-07 20:18:41.998134: step 28120, loss = 0.78 (967.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:18:43.327771: step 28130, loss = 0.82 (962.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:18:44.643979: step 28140, loss = 0.87 (972.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:18:46.010840: step 28150, loss = 0.82 (936.5 examples/sec; 0.137 sec/batch)
2017-05-07 20:18:47.343308: step 28160, loss = 0.81 (960.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:18:48.670218: step 28170, loss = 0.87 (964.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:18:49.983280: step 28180, loss = 0.73 (974.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:18:51.330275: step 28190, loss = 0.83 (950.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:18:52.749192: step 28200, loss = 0.51 (902.1 examples/sec; 0.142 sec/batch)
2017-05-07 20:18:53.999635: step 28210, loss = 0.93 (1023.6 examples/sec; 0.125 sec/batch)
2017-05-07 20:18:55.325254: step 28220, loss = 0.77 (965.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:18:56.641518: step 28230, loss = 0.78 (972.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:18:57.954429: step 28240, loss = 0.79 (974.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:18:59.303932: step 28250, loss = 0.88 (948.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:19:00.620144: step 28260, loss = 0.89 (972.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:19:01.959511: step 28270, loss = 0.81 (955.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:19:03.274623: step 28280, loss = 0.81 (973.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:19:04.607983: step 28290, loss = 1.07 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:19:06.043331: step 28300, loss = 0.67 (891.8 examples/sec; 0.144 sec/batch)
2017-05-07 20:19:07.267131: step 28310, loss = 0.80 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-07 20:19:08.577076: step 28320, loss = 0.79 (977.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:19:09.893642: step 28330, loss = 0.66 (972.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:19:11.238032: step 28340, loss = 0.96 (952.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:19:12.589214: step 28350, loss = 0.70 (947.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:19:13.898543: step 28360, loss = 0.88 (977.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:19:15.238188: step 28370, loss = 0.71 (955.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:19:16.584378: step 28380, loss = 0.73 (950.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:19:17.920772: step 28390, loss = 0.83 (957.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:19:19.369839: step 28400, loss = 0.57 (883.3 examples/sec; 0.145 sec/batch)
2017-05-07 20:19:20.604689: step 28410, loss = 0.84 (1036.6 examples/sec; 0.123 sec/batch)
2017-05-07 20:19:21.930988: step 28420, loss = 0.69 (965.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:19:23.288545: step 28430, loss = 0.70 (942.9 examples/sec; 0.136 sec/batch)
2017-05-07 20:19:24.583947: step 28440, loss = 0.74 (988.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:19:25.916616: step 28450, loss = 0.94 (960.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:19:27.236166: step 28460, loss = 0.80 (970.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:19:28.551542: step 28470, loss = 0.70 (973.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:19:29.905861: step 28480, loss = 0.76 (945.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:19:31.240882: step 28490, loss = 0.72 (958.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:19:32.677066: step 28500, loss = 0.76 (891.2 examples/sec; 0.144 sec/batch)
2017-05-07 20:19:33.933215: step 28510, loss = 0.81 (1019.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:19:35.264686: step 28520, loss = 0.92 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:19:36.592323: step 28530, loss = 0.74 (964.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:19:37.925178: step 28540, loss = 0.72 (960.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:19:39.284295: step 28550, loss = 0.81 (941.8 examples/sec; 0.136 sec/batch)
2017-05-07 20:19:40.601859: step 28560, loss = 0.74 (971.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:19:41.932008: step 28570, loss = 0.79 (962.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:19:43.263798: step 28580, loss = 0.75 (961.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:19:44.579317: step 28590, loss = 0.63 (973.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:19:46.063732: step 28600, loss = 0.75 (862.3 examples/sec; 0.148 sec/batch)
2017-05-07 20:19:47.288147: step 28610, loss = 0.68 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-07 20:19:48.622413: step 28620, loss = 0.60 (959.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:19:49.957947: step 28630, loss = 0.80 (958.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:19:51.295301: step 28640, loss = 0.86 (957.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:19:52.644687: step 28650, loss = 0.65 (948.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:19:53.972185: step 28660, loss = 0.80 (964.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:19:55.319236: step 28670, loss = 0.77 (950.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:19:56.653231: step 28680, loss = 0.86 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:19:57.986308: step 28690, loss = 0.67 (960.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:19:59.453186: step 28700, loss = 0.87 (872.6 examples/sec; 0.147 sec/batch)
2017-05-07 20:20:00.647973: step 28710, loss = 0.65 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-07 20:20:02.000761: step 28720, loss = 0.80 (946.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:20:03.345136: step 28730, loss = 0.66 (952.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:20:04.680815: step 28740, loss = 0.63 (9E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 589 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
58.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:20:06.028548: step 28750, loss = 0.75 (949.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:20:07.371529: step 28760, loss = 0.80 (953.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:20:08.690152: step 28770, loss = 0.81 (970.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:20:10.023756: step 28780, loss = 0.69 (959.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:20:11.381097: step 28790, loss = 0.77 (943.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:20:12.799303: step 28800, loss = 0.75 (902.5 examples/sec; 0.142 sec/batch)
2017-05-07 20:20:14.015341: step 28810, loss = 0.91 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-07 20:20:15.333133: step 28820, loss = 0.76 (971.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:20:16.672560: step 28830, loss = 0.77 (955.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:20:18.012566: step 28840, loss = 0.69 (955.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:20:19.388076: step 28850, loss = 0.80 (930.6 examples/sec; 0.138 sec/batch)
2017-05-07 20:20:20.696209: step 28860, loss = 0.89 (978.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:20:22.036085: step 28870, loss = 0.81 (955.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:20:23.379873: step 28880, loss = 0.67 (952.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:20:24.713687: step 28890, loss = 0.89 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:20:26.156888: step 28900, loss = 0.71 (886.9 examples/sec; 0.144 sec/batch)
2017-05-07 20:20:27.386441: step 28910, loss = 0.70 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-07 20:20:28.729368: step 28920, loss = 0.75 (953.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:20:30.060635: step 28930, loss = 0.77 (961.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:20:31.374779: step 28940, loss = 0.70 (974.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:20:32.739187: step 28950, loss = 0.74 (938.1 examples/sec; 0.136 sec/batch)
2017-05-07 20:20:34.075858: step 28960, loss = 0.77 (957.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:20:35.395702: step 28970, loss = 0.70 (969.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:20:36.736231: step 28980, loss = 0.77 (954.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:20:38.074724: step 28990, loss = 0.90 (956.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:20:39.511166: step 29000, loss = 0.72 (891.1 examples/sec; 0.144 sec/batch)
2017-05-07 20:20:40.738372: step 29010, loss = 0.62 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-07 20:20:42.073335: step 29020, loss = 0.68 (958.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:20:43.387645: step 29030, loss = 0.83 (973.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:20:44.713257: step 29040, loss = 0.85 (965.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:20:46.074160: step 29050, loss = 0.84 (940.5 examples/sec; 0.136 sec/batch)
2017-05-07 20:20:47.394288: step 29060, loss = 0.80 (969.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:20:48.709723: step 29070, loss = 0.84 (973.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:20:50.061034: step 29080, loss = 0.91 (947.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:20:51.381836: step 29090, loss = 0.76 (969.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:20:52.797978: step 29100, loss = 0.76 (903.9 examples/sec; 0.142 sec/batch)
2017-05-07 20:20:54.044646: step 29110, loss = 0.98 (1026.7 examples/sec; 0.125 sec/batch)
2017-05-07 20:20:55.375750: step 29120, loss = 0.62 (961.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:20:56.703077: step 29130, loss = 0.63 (964.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:20:58.032415: step 29140, loss = 0.74 (962.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:20:59.362940: step 29150, loss = 0.77 (962.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:21:00.692223: step 29160, loss = 0.77 (962.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:21:02.042751: step 29170, loss = 0.64 (947.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:21:03.396560: step 29180, loss = 0.80 (945.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:21:04.728650: step 29190, loss = 0.66 (960.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:21:06.184253: step 29200, loss = 0.72 (879.4 examples/sec; 0.146 sec/batch)
2017-05-07 20:21:07.434995: step 29210, loss = 0.79 (1023.4 examples/sec; 0.125 sec/batch)
2017-05-07 20:21:08.745261: step 29220, loss = 1.06 (976.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:21:10.074256: step 29230, loss = 0.77 (963.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:21:11.425731: step 29240, loss = 0.78 (947.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:21:12.775115: step 29250, loss = 0.63 (948.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:21:14.078057: step 29260, loss = 0.84 (982.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:21:15.433441: step 29270, loss = 0.90 (944.4 examples/sec; 0.136 sec/batch)
2017-05-07 20:21:16.769850: step 29280, loss = 0.81 (957.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:21:18.098550: step 29290, loss = 0.73 (963.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:21:19.542196: step 29300, loss = 0.77 (886.6 examples/sec; 0.144 sec/batch)
2017-05-07 20:21:20.745025: step 29310, loss = 0.66 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-07 20:21:22.087264: step 29320, loss = 0.82 (953.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:21:23.411133: step 29330, loss = 0.62 (966.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:21:24.728175: step 29340, loss = 0.91 (971.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:21:26.081323: step 29350, loss = 0.68 (945.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:21:27.411882: step 29360, loss = 0.69 (962.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:21:28.743823: step 29370, loss = 0.83 (961.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:21:30.070002: step 29380, loss = 0.71 (965.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:21:31.398204: step 29390, loss = 0.70 (963.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:21:32.837738: step 29400, loss = 0.75 (889.2 examples/sec; 0.144 sec/batch)
2017-05-07 20:21:34.083788: step 29410, loss = 0.73 (1027.2 examples/sec; 0.125 sec/batch)
2017-05-07 20:21:35.403569: step 29420, loss = 0.81 (969.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:21:36.735329: step 29430, loss = 0.75 (961.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:21:38.062830: step 29440, loss = 0.67 (964.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:21:39.415829: step 29450, loss = 0.77 (946.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:21:40.743203: step 29460, loss = 0.77 (964.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:21:42.080035: step 29470, loss = 0.90 (957.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:21:43.391145: step 29480, loss = 0.75 (976.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:21:44.713168: step 29490, loss = 0.78 (968.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:21:46.169340: step 29500, loss = 0.68 (879.0 examples/sec; 0.146 sec/batch)
2017-05-07 20:21:47.429624: step 29510, loss = 0.93 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:21:48.740070: step 29520, loss = 0.73 (976.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:21:50.101428: step 29530, loss = 0.85 (940.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:21:51.457872: step 29540, loss = 0.80 (943.7 examples/sec; 0.136 sec/batch)
2017-05-07 20:21:52.808670: step 29550, loss = 0.89 (947.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:21:54.122172: step 29560, loss = 0.78 (974.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:21:55.462303: step 29570, loss = 0.75 (955.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:21:56.779714: step 29580, loss = 0.81 (971.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:21:58.114734: step 29590, loss = 0.65 (958.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:21:59.594642: step 29600, loss = 0.71 (864.9 examples/sec; 0.148 sec/batch)
2017-05-07 20:22:00.804908: step 29610, loss = 0.70 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-07 20:22:02.159822: step 29620, loss = 0.76 (944.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:22:03.482262: step 29630, loss = 0.68 (967.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:22:04.816201: step 29640, loss = 0.74 (959.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:22:06.208038: step 29650, loss = E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 607 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
0.82 (919.6 examples/sec; 0.139 sec/batch)
2017-05-07 20:22:07.547218: step 29660, loss = 0.71 (955.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:22:08.894088: step 29670, loss = 0.67 (950.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:22:10.237168: step 29680, loss = 0.80 (953.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:22:11.572119: step 29690, loss = 0.76 (958.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:22:12.995622: step 29700, loss = 0.80 (899.2 examples/sec; 0.142 sec/batch)
2017-05-07 20:22:14.258146: step 29710, loss = 0.79 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:22:15.601929: step 29720, loss = 0.86 (952.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:22:16.906302: step 29730, loss = 0.59 (981.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:22:18.249284: step 29740, loss = 0.78 (953.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:22:19.617890: step 29750, loss = 0.72 (935.3 examples/sec; 0.137 sec/batch)
2017-05-07 20:22:20.930566: step 29760, loss = 0.67 (975.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:22:22.254169: step 29770, loss = 1.07 (967.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:22:23.608123: step 29780, loss = 0.69 (945.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:22:24.946155: step 29790, loss = 0.79 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:22:26.378406: step 29800, loss = 0.78 (893.7 examples/sec; 0.143 sec/batch)
2017-05-07 20:22:27.640705: step 29810, loss = 0.64 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:22:28.962569: step 29820, loss = 0.94 (968.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:22:30.299638: step 29830, loss = 0.72 (957.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:22:31.646276: step 29840, loss = 0.85 (950.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:22:32.975188: step 29850, loss = 0.80 (963.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:22:34.300135: step 29860, loss = 0.63 (966.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:22:35.647875: step 29870, loss = 0.78 (949.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:22:36.962809: step 29880, loss = 0.63 (973.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:22:38.288822: step 29890, loss = 0.77 (965.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:22:39.701942: step 29900, loss = 0.83 (905.8 examples/sec; 0.141 sec/batch)
2017-05-07 20:22:40.929324: step 29910, loss = 0.80 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-07 20:22:42.278081: step 29920, loss = 0.70 (949.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:22:43.602832: step 29930, loss = 0.79 (966.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:22:44.949560: step 29940, loss = 0.75 (950.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:22:46.286442: step 29950, loss = 0.61 (957.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:22:47.630041: step 29960, loss = 0.76 (952.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:22:48.968699: step 29970, loss = 0.74 (956.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:22:50.301045: step 29980, loss = 0.67 (960.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:22:51.611614: step 29990, loss = 0.69 (976.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:22:53.062634: step 30000, loss = 0.56 (882.1 examples/sec; 0.145 sec/batch)
2017-05-07 20:22:54.267411: step 30010, loss = 0.88 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-07 20:22:55.614305: step 30020, loss = 0.73 (950.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:22:56.948973: step 30030, loss = 0.83 (959.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:22:58.281624: step 30040, loss = 0.61 (960.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:22:59.621770: step 30050, loss = 0.78 (955.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:23:00.932908: step 30060, loss = 0.91 (976.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:23:02.251062: step 30070, loss = 0.95 (971.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:23:03.568250: step 30080, loss = 0.86 (971.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:23:04.903422: step 30090, loss = 0.77 (958.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:23:06.332876: step 30100, loss = 0.65 (895.4 examples/sec; 0.143 sec/batch)
2017-05-07 20:23:07.593208: step 30110, loss = 0.76 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:23:08.942132: step 30120, loss = 0.81 (948.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:23:10.294039: step 30130, loss = 0.77 (946.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:23:11.624919: step 30140, loss = 0.76 (961.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:23:12.977601: step 30150, loss = 0.98 (946.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:23:14.306377: step 30160, loss = 0.72 (963.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:23:15.643149: step 30170, loss = 0.83 (957.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:23:16.931547: step 30180, loss = 0.80 (993.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:23:18.259074: step 30190, loss = 0.73 (964.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:23:19.681889: step 30200, loss = 0.76 (899.6 examples/sec; 0.142 sec/batch)
2017-05-07 20:23:20.934434: step 30210, loss = 0.85 (1021.9 examples/sec; 0.125 sec/batch)
2017-05-07 20:23:22.256657: step 30220, loss = 0.96 (968.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:23:23.604421: step 30230, loss = 1.04 (949.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:23:24.939223: step 30240, loss = 0.88 (958.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:23:26.300845: step 30250, loss = 0.70 (940.1 examples/sec; 0.136 sec/batch)
2017-05-07 20:23:27.618275: step 30260, loss = 0.63 (971.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:23:28.948247: step 30270, loss = 0.59 (962.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:23:30.285032: step 30280, loss = 0.71 (957.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:23:31.624226: step 30290, loss = 0.78 (955.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:23:33.032943: step 30300, loss = 0.74 (908.6 examples/sec; 0.141 sec/batch)
2017-05-07 20:23:34.263550: step 30310, loss = 0.83 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-07 20:23:35.594809: step 30320, loss = 0.78 (961.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:23:36.921830: step 30330, loss = 0.74 (964.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:23:38.288264: step 30340, loss = 0.84 (936.7 examples/sec; 0.137 sec/batch)
2017-05-07 20:23:39.634062: step 30350, loss = 0.76 (951.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:23:40.966021: step 30360, loss = 0.86 (961.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:23:42.297850: step 30370, loss = 0.83 (961.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:23:43.665967: step 30380, loss = 0.66 (935.6 examples/sec; 0.137 sec/batch)
2017-05-07 20:23:45.000004: step 30390, loss = 0.83 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:23:46.438862: step 30400, loss = 0.68 (889.6 examples/sec; 0.144 sec/batch)
2017-05-07 20:23:47.671975: step 30410, loss = 0.81 (1038.0 examples/sec; 0.123 sec/batch)
2017-05-07 20:23:48.994036: step 30420, loss = 0.80 (968.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:23:50.328207: step 30430, loss = 0.87 (959.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:23:51.659120: step 30440, loss = 0.72 (961.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:23:53.004400: step 30450, loss = 0.81 (951.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:23:54.338405: step 30460, loss = 0.64 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:23:55.671674: step 30470, loss = 0.71 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:23:56.993541: step 30480, loss = 0.76 (968.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:23:58.330870: step 30490, loss = 0.78 (957.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:23:59.782576: step 30500, loss = 0.79 (881.7 examples/sec; 0.145 sec/batch)
2017-05-07 20:24:01.014841: step 30510, loss = 0.86 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-07 20:24:02.339089: step 30520, loss = 0.72 (966.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:24:03.684982: step 30530, loss = 0.88 (951.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:24:05.010340: step 30540, loss = 0.76 (965.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:24:06.362292: step 30550, loss = 0.75 (946.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:24:07.681345: step 30560, E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 625 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
loss = 0.64 (970.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:24:09.006327: step 30570, loss = 0.78 (966.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:24:10.333086: step 30580, loss = 0.83 (964.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:24:11.668062: step 30590, loss = 0.76 (958.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:24:13.090066: step 30600, loss = 0.65 (900.1 examples/sec; 0.142 sec/batch)
2017-05-07 20:24:14.331197: step 30610, loss = 0.65 (1031.3 examples/sec; 0.124 sec/batch)
2017-05-07 20:24:15.657084: step 30620, loss = 0.68 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:24:16.990833: step 30630, loss = 0.82 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:24:18.327089: step 30640, loss = 0.72 (957.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:24:19.685272: step 30650, loss = 0.84 (942.4 examples/sec; 0.136 sec/batch)
2017-05-07 20:24:21.004483: step 30660, loss = 0.83 (970.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:24:22.347105: step 30670, loss = 0.70 (953.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:24:23.666640: step 30680, loss = 0.79 (970.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:24:24.998639: step 30690, loss = 0.70 (961.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:24:26.436085: step 30700, loss = 0.70 (890.5 examples/sec; 0.144 sec/batch)
2017-05-07 20:24:27.665932: step 30710, loss = 0.65 (1040.8 examples/sec; 0.123 sec/batch)
2017-05-07 20:24:28.987763: step 30720, loss = 0.68 (968.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:24:30.325746: step 30730, loss = 0.85 (956.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:24:31.643032: step 30740, loss = 0.72 (971.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:24:32.996987: step 30750, loss = 0.71 (945.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:24:34.333601: step 30760, loss = 0.80 (957.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:24:35.668202: step 30770, loss = 0.73 (959.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:24:37.009067: step 30780, loss = 0.80 (954.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:24:38.335417: step 30790, loss = 0.77 (965.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:24:39.779105: step 30800, loss = 0.74 (886.6 examples/sec; 0.144 sec/batch)
2017-05-07 20:24:41.001184: step 30810, loss = 0.64 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-07 20:24:42.343855: step 30820, loss = 0.57 (953.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:24:43.682502: step 30830, loss = 0.80 (956.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:24:45.010758: step 30840, loss = 0.81 (963.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:24:46.362936: step 30850, loss = 0.76 (946.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:24:47.696957: step 30860, loss = 0.87 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:24:49.029078: step 30870, loss = 0.73 (960.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:24:50.386310: step 30880, loss = 0.79 (943.1 examples/sec; 0.136 sec/batch)
2017-05-07 20:24:51.706055: step 30890, loss = 0.81 (969.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:24:53.142411: step 30900, loss = 0.79 (891.2 examples/sec; 0.144 sec/batch)
2017-05-07 20:24:54.371356: step 30910, loss = 0.68 (1041.5 examples/sec; 0.123 sec/batch)
2017-05-07 20:24:55.706528: step 30920, loss = 1.00 (958.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:24:57.039024: step 30930, loss = 0.85 (960.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:24:58.382327: step 30940, loss = 0.77 (952.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:24:59.743743: step 30950, loss = 0.70 (940.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:25:01.037898: step 30960, loss = 0.61 (989.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:25:02.385931: step 30970, loss = 0.89 (949.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:25:03.731216: step 30980, loss = 0.70 (951.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:25:05.059409: step 30990, loss = 0.82 (963.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:25:06.494793: step 31000, loss = 0.73 (891.7 examples/sec; 0.144 sec/batch)
2017-05-07 20:25:07.740679: step 31010, loss = 0.93 (1027.4 examples/sec; 0.125 sec/batch)
2017-05-07 20:25:09.061756: step 31020, loss = 0.67 (968.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:25:10.403630: step 31030, loss = 0.70 (953.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:25:11.709807: step 31040, loss = 0.76 (980.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:25:13.040578: step 31050, loss = 0.77 (961.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:25:14.390442: step 31060, loss = 0.76 (948.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:25:15.704970: step 31070, loss = 0.71 (973.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:25:17.039934: step 31080, loss = 0.82 (958.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:25:18.364625: step 31090, loss = 0.75 (966.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:25:19.796860: step 31100, loss = 0.86 (893.7 examples/sec; 0.143 sec/batch)
2017-05-07 20:25:21.013999: step 31110, loss = 0.94 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-07 20:25:22.330570: step 31120, loss = 0.73 (972.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:25:23.648479: step 31130, loss = 0.88 (971.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:25:24.961810: step 31140, loss = 0.69 (974.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:25:26.319548: step 31150, loss = 0.73 (942.7 examples/sec; 0.136 sec/batch)
2017-05-07 20:25:27.648200: step 31160, loss = 1.12 (963.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:25:29.004283: step 31170, loss = 0.81 (943.9 examples/sec; 0.136 sec/batch)
2017-05-07 20:25:30.355342: step 31180, loss = 0.72 (947.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:25:31.685420: step 31190, loss = 0.74 (962.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:25:33.140678: step 31200, loss = 0.71 (879.6 examples/sec; 0.146 sec/batch)
2017-05-07 20:25:34.379587: step 31210, loss = 0.73 (1033.2 examples/sec; 0.124 sec/batch)
2017-05-07 20:25:35.730973: step 31220, loss = 0.75 (947.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:25:37.072758: step 31230, loss = 0.82 (953.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:25:38.406222: step 31240, loss = 0.75 (959.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:25:39.758871: step 31250, loss = 0.81 (946.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:25:41.062253: step 31260, loss = 0.86 (982.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:25:42.387612: step 31270, loss = 0.60 (965.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:25:43.714676: step 31280, loss = 0.84 (964.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:25:45.039334: step 31290, loss = 0.83 (966.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:25:46.513074: step 31300, loss = 0.69 (868.5 examples/sec; 0.147 sec/batch)
2017-05-07 20:25:47.752875: step 31310, loss = 0.78 (1032.4 examples/sec; 0.124 sec/batch)
2017-05-07 20:25:49.082782: step 31320, loss = 0.64 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:25:50.438323: step 31330, loss = 0.78 (944.3 examples/sec; 0.136 sec/batch)
2017-05-07 20:25:51.772040: step 31340, loss = 0.85 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:25:53.114851: step 31350, loss = 0.75 (953.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:25:54.468924: step 31360, loss = 0.87 (945.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:25:55.840315: step 31370, loss = 0.65 (933.4 examples/sec; 0.137 sec/batch)
2017-05-07 20:25:57.139770: step 31380, loss = 0.69 (985.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:25:58.488859: step 31390, loss = 0.86 (948.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:25:59.945947: step 31400, loss = 0.82 (878.5 examples/sec; 0.146 sec/batch)
2017-05-07 20:26:01.168243: step 31410, loss = 0.56 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-07 20:26:02.486474: step 31420, loss = 0.69 (971.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:26:03.820756: step 31430, loss = 0.75 (959.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:26:05.142719: step 31440, loss = 0.97 (968.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:26:06.519865: step 31450, loss = 0.74 (929.5 examples/sec; 0.138 sec/batch)
2017-05-07 20:26:07.840529: step 31460, loss = 0.77 (969.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:26:09.159264: step E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 644 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
31470, loss = 0.86 (970.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:26:10.507085: step 31480, loss = 0.63 (949.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:26:11.818331: step 31490, loss = 0.71 (976.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:26:13.235996: step 31500, loss = 0.76 (902.9 examples/sec; 0.142 sec/batch)
2017-05-07 20:26:14.484294: step 31510, loss = 0.61 (1025.4 examples/sec; 0.125 sec/batch)
2017-05-07 20:26:15.829196: step 31520, loss = 0.83 (951.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:26:17.153024: step 31530, loss = 0.81 (966.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:26:18.495344: step 31540, loss = 0.77 (953.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:26:19.865831: step 31550, loss = 0.80 (934.0 examples/sec; 0.137 sec/batch)
2017-05-07 20:26:21.183645: step 31560, loss = 0.78 (971.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:26:22.530549: step 31570, loss = 0.89 (950.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:26:23.860578: step 31580, loss = 0.86 (962.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:26:25.192770: step 31590, loss = 0.74 (960.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:26:26.619334: step 31600, loss = 0.83 (897.3 examples/sec; 0.143 sec/batch)
2017-05-07 20:26:27.835699: step 31610, loss = 0.64 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-07 20:26:29.137765: step 31620, loss = 0.74 (983.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:26:30.460897: step 31630, loss = 0.77 (967.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:26:31.824760: step 31640, loss = 0.59 (938.5 examples/sec; 0.136 sec/batch)
2017-05-07 20:26:33.154955: step 31650, loss = 0.67 (962.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:26:34.517775: step 31660, loss = 0.76 (939.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:26:35.823328: step 31670, loss = 0.75 (980.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:26:37.159861: step 31680, loss = 0.65 (957.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:26:38.502226: step 31690, loss = 0.64 (953.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:26:39.936023: step 31700, loss = 0.76 (892.7 examples/sec; 0.143 sec/batch)
2017-05-07 20:26:41.170852: step 31710, loss = 0.66 (1036.6 examples/sec; 0.123 sec/batch)
2017-05-07 20:26:42.501236: step 31720, loss = 0.78 (962.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:26:43.834860: step 31730, loss = 0.67 (959.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:26:45.150946: step 31740, loss = 0.73 (972.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:26:46.490281: step 31750, loss = 0.82 (955.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:26:47.827513: step 31760, loss = 0.73 (957.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:26:49.191661: step 31770, loss = 0.72 (938.3 examples/sec; 0.136 sec/batch)
2017-05-07 20:26:50.530199: step 31780, loss = 0.76 (956.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:26:51.879887: step 31790, loss = 0.64 (948.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:26:53.310744: step 31800, loss = 0.68 (894.6 examples/sec; 0.143 sec/batch)
2017-05-07 20:26:54.557940: step 31810, loss = 0.84 (1026.3 examples/sec; 0.125 sec/batch)
2017-05-07 20:26:55.892024: step 31820, loss = 0.82 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:26:57.228953: step 31830, loss = 0.79 (957.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:26:58.578687: step 31840, loss = 0.75 (948.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:26:59.943721: step 31850, loss = 0.87 (937.7 examples/sec; 0.137 sec/batch)
2017-05-07 20:27:01.258988: step 31860, loss = 0.80 (973.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:27:02.583912: step 31870, loss = 0.80 (966.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:27:03.951514: step 31880, loss = 0.87 (935.9 examples/sec; 0.137 sec/batch)
2017-05-07 20:27:05.256192: step 31890, loss = 0.70 (981.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:27:06.712204: step 31900, loss = 0.67 (879.1 examples/sec; 0.146 sec/batch)
2017-05-07 20:27:07.920889: step 31910, loss = 0.75 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-07 20:27:09.249730: step 31920, loss = 0.73 (963.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:27:10.606172: step 31930, loss = 0.76 (943.6 examples/sec; 0.136 sec/batch)
2017-05-07 20:27:11.947352: step 31940, loss = 0.69 (954.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:27:13.293707: step 31950, loss = 0.82 (950.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:27:14.603712: step 31960, loss = 0.72 (977.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:27:15.936495: step 31970, loss = 0.80 (960.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:27:17.249613: step 31980, loss = 0.78 (974.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:27:18.577229: step 31990, loss = 0.76 (964.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:27:20.077919: step 32000, loss = 0.80 (852.9 examples/sec; 0.150 sec/batch)
2017-05-07 20:27:21.275617: step 32010, loss = 0.65 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-07 20:27:22.605897: step 32020, loss = 0.78 (962.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:27:23.952235: step 32030, loss = 0.71 (950.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:27:25.263356: step 32040, loss = 0.91 (976.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:27:26.623220: step 32050, loss = 0.69 (941.3 examples/sec; 0.136 sec/batch)
2017-05-07 20:27:27.960137: step 32060, loss = 0.74 (957.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:27:29.270037: step 32070, loss = 0.75 (977.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:27:30.622095: step 32080, loss = 0.68 (946.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:27:31.973067: step 32090, loss = 0.91 (947.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:27:33.393579: step 32100, loss = 0.75 (901.1 examples/sec; 0.142 sec/batch)
2017-05-07 20:27:34.660793: step 32110, loss = 0.66 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:27:35.984244: step 32120, loss = 0.64 (967.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:27:37.333350: step 32130, loss = 0.92 (948.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:27:38.656769: step 32140, loss = 0.75 (967.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:27:40.008968: step 32150, loss = 0.82 (946.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:27:41.347007: step 32160, loss = 0.79 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:27:42.682972: step 32170, loss = 0.72 (958.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:27:44.024613: step 32180, loss = 0.79 (954.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:27:45.340716: step 32190, loss = 0.65 (972.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:27:46.769678: step 32200, loss = 0.78 (895.8 examples/sec; 0.143 sec/batch)
2017-05-07 20:27:47.997861: step 32210, loss = 0.75 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-07 20:27:49.306903: step 32220, loss = 0.67 (977.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:27:50.638115: step 32230, loss = 0.84 (961.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:27:51.982928: step 32240, loss = 0.71 (951.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:27:53.323122: step 32250, loss = 0.85 (955.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:27:54.666593: step 32260, loss = 0.89 (952.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:27:55.992423: step 32270, loss = 0.89 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:27:57.335398: step 32280, loss = 0.67 (953.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:27:58.693722: step 32290, loss = 0.75 (942.3 examples/sec; 0.136 sec/batch)
2017-05-07 20:28:00.129414: step 32300, loss = 0.74 (891.6 examples/sec; 0.144 sec/batch)
2017-05-07 20:28:01.368579: step 32310, loss = 0.83 (1033.0 examples/sec; 0.124 sec/batch)
2017-05-07 20:28:02.741725: step 32320, loss = 0.71 (932.2 examples/sec; 0.137 sec/batch)
2017-05-07 20:28:04.067387: step 32330, loss = 0.79 (965.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:28:05.390803: step 32340, loss = 0.71 (967.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:28:06.731890: step 32350, loss = 0.86 (954.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:28:08.079113: step 32360, loss = 0.71 (950.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:28:09.434657: step 32370, loss = 0.73 (944.3 examples/sec; 0.136 sec/batch)
2017-05-07 20:28:10.748909E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 662 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
: step 32380, loss = 0.82 (973.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:28:12.093033: step 32390, loss = 0.80 (952.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:28:13.519928: step 32400, loss = 0.77 (897.1 examples/sec; 0.143 sec/batch)
2017-05-07 20:28:14.748949: step 32410, loss = 0.87 (1041.5 examples/sec; 0.123 sec/batch)
2017-05-07 20:28:16.080836: step 32420, loss = 0.77 (961.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:28:17.390265: step 32430, loss = 0.78 (977.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:28:18.763152: step 32440, loss = 0.69 (932.3 examples/sec; 0.137 sec/batch)
2017-05-07 20:28:20.111604: step 32450, loss = 0.80 (949.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:28:21.433758: step 32460, loss = 0.90 (968.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:28:22.759765: step 32470, loss = 0.72 (965.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:28:24.103277: step 32480, loss = 0.78 (952.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:28:25.431552: step 32490, loss = 0.82 (963.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:28:26.871230: step 32500, loss = 0.75 (889.1 examples/sec; 0.144 sec/batch)
2017-05-07 20:28:28.105523: step 32510, loss = 0.81 (1037.0 examples/sec; 0.123 sec/batch)
2017-05-07 20:28:29.443830: step 32520, loss = 0.66 (956.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:28:30.752672: step 32530, loss = 0.90 (978.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:28:32.080698: step 32540, loss = 0.67 (963.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:28:33.426553: step 32550, loss = 0.81 (951.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:28:34.751818: step 32560, loss = 0.60 (965.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:28:36.073248: step 32570, loss = 0.77 (968.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:28:37.417632: step 32580, loss = 0.88 (952.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:28:38.741941: step 32590, loss = 0.80 (966.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:28:40.173356: step 32600, loss = 0.78 (894.2 examples/sec; 0.143 sec/batch)
2017-05-07 20:28:41.426751: step 32610, loss = 0.68 (1021.2 examples/sec; 0.125 sec/batch)
2017-05-07 20:28:42.755660: step 32620, loss = 0.72 (963.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:28:44.113261: step 32630, loss = 0.76 (942.8 examples/sec; 0.136 sec/batch)
2017-05-07 20:28:45.455765: step 32640, loss = 0.65 (953.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:28:46.811036: step 32650, loss = 0.74 (944.5 examples/sec; 0.136 sec/batch)
2017-05-07 20:28:48.137862: step 32660, loss = 0.64 (964.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:28:49.464679: step 32670, loss = 0.80 (964.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:28:50.782553: step 32680, loss = 0.76 (971.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:28:52.115688: step 32690, loss = 0.78 (960.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:28:53.525021: step 32700, loss = 0.75 (908.2 examples/sec; 0.141 sec/batch)
2017-05-07 20:28:54.777113: step 32710, loss = 0.86 (1022.3 examples/sec; 0.125 sec/batch)
2017-05-07 20:28:56.105775: step 32720, loss = 0.89 (963.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:28:57.434593: step 32730, loss = 0.73 (963.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:28:58.772509: step 32740, loss = 0.74 (956.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:29:00.098936: step 32750, loss = 0.68 (965.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:29:01.441123: step 32760, loss = 0.62 (953.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:29:02.771044: step 32770, loss = 0.83 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:29:04.119262: step 32780, loss = 0.84 (949.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:29:05.443721: step 32790, loss = 0.71 (966.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:29:06.858809: step 32800, loss = 0.58 (904.5 examples/sec; 0.142 sec/batch)
2017-05-07 20:29:08.107918: step 32810, loss = 0.82 (1024.7 examples/sec; 0.125 sec/batch)
2017-05-07 20:29:09.413136: step 32820, loss = 0.77 (980.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:29:10.741645: step 32830, loss = 0.75 (963.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:29:12.077251: step 32840, loss = 0.94 (958.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:29:13.400968: step 32850, loss = 0.81 (967.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:29:14.721377: step 32860, loss = 0.54 (969.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:29:16.049695: step 32870, loss = 0.74 (963.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:29:17.377344: step 32880, loss = 0.81 (964.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:29:18.709517: step 32890, loss = 0.81 (960.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:29:20.148744: step 32900, loss = 0.65 (889.4 examples/sec; 0.144 sec/batch)
2017-05-07 20:29:21.396432: step 32910, loss = 0.81 (1025.9 examples/sec; 0.125 sec/batch)
2017-05-07 20:29:22.731184: step 32920, loss = 0.76 (959.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:29:24.063527: step 32930, loss = 0.77 (960.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:29:25.416516: step 32940, loss = 0.98 (946.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:29:26.765729: step 32950, loss = 0.85 (948.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:29:28.095799: step 32960, loss = 0.79 (962.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:29:29.441751: step 32970, loss = 0.76 (951.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:29:30.783616: step 32980, loss = 0.77 (953.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:29:32.093675: step 32990, loss = 0.86 (977.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:29:33.531664: step 33000, loss = 0.68 (890.4 examples/sec; 0.144 sec/batch)
2017-05-07 20:29:34.755802: step 33010, loss = 0.85 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-07 20:29:36.084937: step 33020, loss = 0.75 (963.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:29:37.419064: step 33030, loss = 0.81 (959.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:29:38.761540: step 33040, loss = 0.79 (953.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:29:40.098049: step 33050, loss = 0.85 (957.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:29:41.413154: step 33060, loss = 0.65 (973.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:29:42.740021: step 33070, loss = 0.61 (964.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:29:44.080305: step 33080, loss = 0.83 (955.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:29:45.401164: step 33090, loss = 0.88 (969.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:29:46.821358: step 33100, loss = 0.67 (901.3 examples/sec; 0.142 sec/batch)
2017-05-07 20:29:48.087182: step 33110, loss = 0.75 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:29:49.418486: step 33120, loss = 0.80 (961.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:29:50.733728: step 33130, loss = 0.83 (973.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:29:52.064704: step 33140, loss = 0.65 (961.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:29:53.416776: step 33150, loss = 0.71 (946.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:29:54.745913: step 33160, loss = 0.78 (963.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:29:56.083407: step 33170, loss = 0.89 (957.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:29:57.412268: step 33180, loss = 0.72 (963.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:29:58.752548: step 33190, loss = 0.76 (955.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:30:00.197314: step 33200, loss = 0.78 (886.0 examples/sec; 0.144 sec/batch)
2017-05-07 20:30:01.423592: step 33210, loss = 0.83 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-07 20:30:02.765673: step 33220, loss = 0.66 (953.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:30:04.100800: step 33230, loss = 0.78 (958.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:30:05.421820: step 33240, loss = 0.66 (968.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:30:06.786682: step 33250, loss = 0.79 (937.8 examples/sec; 0.136 sec/batch)
2017-05-07 20:30:08.104557: step 33260, loss = 0.70 (971.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:30:09.436541: step 33270, loss = 0.73 (961.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:30:10.787485: step 33280, loss = 0.82 (947.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:30:12E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 680 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
.133638: step 33290, loss = 0.67 (950.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:30:13.546149: step 33300, loss = 0.80 (906.2 examples/sec; 0.141 sec/batch)
2017-05-07 20:30:14.767205: step 33310, loss = 0.65 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-07 20:30:16.098864: step 33320, loss = 0.77 (961.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:30:17.414807: step 33330, loss = 0.67 (972.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:30:18.762598: step 33340, loss = 0.71 (949.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:30:20.110230: step 33350, loss = 0.88 (949.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:30:21.436353: step 33360, loss = 0.65 (965.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:30:22.777677: step 33370, loss = 0.58 (954.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:30:24.094558: step 33380, loss = 0.82 (972.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:30:25.419174: step 33390, loss = 0.72 (966.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:30:26.900348: step 33400, loss = 0.66 (864.2 examples/sec; 0.148 sec/batch)
2017-05-07 20:30:28.081223: step 33410, loss = 0.86 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-07 20:30:29.394984: step 33420, loss = 0.61 (974.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:30:30.740232: step 33430, loss = 0.92 (951.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:30:32.064358: step 33440, loss = 0.69 (966.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:30:33.418053: step 33450, loss = 0.93 (945.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:30:34.753511: step 33460, loss = 0.58 (958.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:30:36.081550: step 33470, loss = 0.93 (963.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:30:37.421929: step 33480, loss = 0.70 (955.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:30:38.748277: step 33490, loss = 0.73 (965.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:30:40.178612: step 33500, loss = 0.85 (894.9 examples/sec; 0.143 sec/batch)
2017-05-07 20:30:41.443171: step 33510, loss = 0.68 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:30:42.778087: step 33520, loss = 0.68 (958.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:30:44.090176: step 33530, loss = 0.87 (975.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:30:45.416057: step 33540, loss = 0.95 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:30:46.769530: step 33550, loss = 1.02 (945.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:30:48.103235: step 33560, loss = 0.81 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:30:49.437082: step 33570, loss = 0.79 (959.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:30:50.773834: step 33580, loss = 0.78 (957.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:30:52.082767: step 33590, loss = 0.79 (977.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:30:53.505291: step 33600, loss = 0.78 (899.8 examples/sec; 0.142 sec/batch)
2017-05-07 20:30:54.752737: step 33610, loss = 0.76 (1026.1 examples/sec; 0.125 sec/batch)
2017-05-07 20:30:56.093179: step 33620, loss = 0.76 (954.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:30:57.409402: step 33630, loss = 0.70 (972.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:30:58.743132: step 33640, loss = 0.66 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:31:00.097412: step 33650, loss = 0.81 (945.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:31:01.443265: step 33660, loss = 0.82 (951.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:31:02.763785: step 33670, loss = 0.66 (969.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:31:04.098702: step 33680, loss = 0.78 (958.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:31:05.422001: step 33690, loss = 0.68 (967.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:31:06.844144: step 33700, loss = 0.79 (900.1 examples/sec; 0.142 sec/batch)
2017-05-07 20:31:08.060586: step 33710, loss = 0.82 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-07 20:31:09.405501: step 33720, loss = 0.69 (951.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:31:10.730240: step 33730, loss = 0.68 (966.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:31:12.046795: step 33740, loss = 0.88 (972.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:31:13.406095: step 33750, loss = 0.63 (941.7 examples/sec; 0.136 sec/batch)
2017-05-07 20:31:14.751707: step 33760, loss = 0.75 (951.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:31:16.089376: step 33770, loss = 0.76 (956.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:31:17.416974: step 33780, loss = 0.65 (964.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:31:18.758352: step 33790, loss = 0.70 (954.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:31:20.199085: step 33800, loss = 0.75 (888.4 examples/sec; 0.144 sec/batch)
2017-05-07 20:31:21.395111: step 33810, loss = 0.88 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-07 20:31:22.716289: step 33820, loss = 0.81 (968.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:31:24.053928: step 33830, loss = 0.76 (956.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:31:25.383218: step 33840, loss = 0.74 (962.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:31:26.740622: step 33850, loss = 0.71 (943.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:31:28.079906: step 33860, loss = 0.69 (955.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:31:29.427308: step 33870, loss = 0.86 (950.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:31:30.746829: step 33880, loss = 0.71 (970.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:31:32.098182: step 33890, loss = 0.85 (947.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:31:33.520685: step 33900, loss = 0.72 (899.8 examples/sec; 0.142 sec/batch)
2017-05-07 20:31:34.765531: step 33910, loss = 0.79 (1028.2 examples/sec; 0.124 sec/batch)
2017-05-07 20:31:36.100319: step 33920, loss = 0.66 (959.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:31:37.416157: step 33930, loss = 0.82 (972.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:31:38.736528: step 33940, loss = 0.73 (969.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:31:40.087664: step 33950, loss = 0.83 (947.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:31:41.403376: step 33960, loss = 0.80 (972.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:31:42.742275: step 33970, loss = 0.85 (956.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:31:44.074765: step 33980, loss = 0.69 (960.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:31:45.392776: step 33990, loss = 0.72 (971.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:31:46.844135: step 34000, loss = 0.68 (881.9 examples/sec; 0.145 sec/batch)
2017-05-07 20:31:48.088112: step 34010, loss = 0.81 (1029.0 examples/sec; 0.124 sec/batch)
2017-05-07 20:31:49.423208: step 34020, loss = 0.77 (958.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:31:50.768215: step 34030, loss = 0.81 (951.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:31:52.108385: step 34040, loss = 0.75 (955.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:31:53.481122: step 34050, loss = 0.65 (932.4 examples/sec; 0.137 sec/batch)
2017-05-07 20:31:54.797671: step 34060, loss = 0.86 (972.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:31:56.118723: step 34070, loss = 0.61 (968.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:31:57.433274: step 34080, loss = 0.54 (973.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:31:58.766340: step 34090, loss = 0.75 (960.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:32:00.251190: step 34100, loss = 0.72 (862.0 examples/sec; 0.148 sec/batch)
2017-05-07 20:32:01.426921: step 34110, loss = 0.81 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-07 20:32:02.756642: step 34120, loss = 0.82 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:32:04.082211: step 34130, loss = 0.70 (965.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:32:05.413067: step 34140, loss = 0.82 (961.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:32:06.757046: step 34150, loss = 0.78 (952.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:32:08.065351: step 34160, loss = 0.81 (978.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:32:09.427091: step 34170, loss = 0.68 (940.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:32:10.753401: step 34180, loss = 0.65 (965.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:32:12.096286: step 34190, loss = 0.82 (953.2 examples/sec; 0.134 sec/batch)
2017-05-07 2E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 700 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
0:32:13.528353: step 34200, loss = 0.86 (893.8 examples/sec; 0.143 sec/batch)
2017-05-07 20:32:14.766748: step 34210, loss = 0.92 (1033.6 examples/sec; 0.124 sec/batch)
2017-05-07 20:32:16.099953: step 34220, loss = 0.64 (960.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:32:17.411872: step 34230, loss = 0.70 (975.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:32:18.757816: step 34240, loss = 0.72 (951.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:32:20.103717: step 34250, loss = 0.78 (951.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:32:21.417578: step 34260, loss = 0.81 (974.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:32:22.751200: step 34270, loss = 0.86 (959.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:32:24.075756: step 34280, loss = 0.88 (966.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:32:25.408943: step 34290, loss = 0.82 (960.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:32:26.830573: step 34300, loss = 0.82 (900.4 examples/sec; 0.142 sec/batch)
2017-05-07 20:32:28.088829: step 34310, loss = 0.75 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:32:29.421744: step 34320, loss = 0.73 (960.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:32:30.777297: step 34330, loss = 0.65 (944.3 examples/sec; 0.136 sec/batch)
2017-05-07 20:32:32.084620: step 34340, loss = 0.82 (979.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:32:33.421146: step 34350, loss = 0.70 (957.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:32:34.744078: step 34360, loss = 0.92 (967.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:32:36.083384: step 34370, loss = 0.71 (955.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:32:37.404537: step 34380, loss = 0.81 (968.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:32:38.733902: step 34390, loss = 0.58 (962.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:32:40.161580: step 34400, loss = 0.67 (896.6 examples/sec; 0.143 sec/batch)
2017-05-07 20:32:41.389972: step 34410, loss = 0.68 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-07 20:32:42.746762: step 34420, loss = 0.70 (943.4 examples/sec; 0.136 sec/batch)
2017-05-07 20:32:44.064746: step 34430, loss = 0.69 (971.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:32:45.397813: step 34440, loss = 0.74 (960.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:32:46.757132: step 34450, loss = 0.80 (941.6 examples/sec; 0.136 sec/batch)
2017-05-07 20:32:48.086076: step 34460, loss = 0.70 (963.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:32:49.414949: step 34470, loss = 0.85 (963.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:32:50.739680: step 34480, loss = 0.75 (966.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:32:52.064626: step 34490, loss = 0.68 (966.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:32:53.499379: step 34500, loss = 0.73 (892.1 examples/sec; 0.143 sec/batch)
2017-05-07 20:32:54.761955: step 34510, loss = 0.82 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:32:56.099853: step 34520, loss = 0.81 (956.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:32:57.430149: step 34530, loss = 0.62 (962.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:32:58.754617: step 34540, loss = 0.68 (966.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:33:00.098184: step 34550, loss = 0.84 (952.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:33:01.434542: step 34560, loss = 0.85 (957.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:33:02.768051: step 34570, loss = 0.76 (959.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:33:04.078265: step 34580, loss = 0.66 (977.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:33:05.418249: step 34590, loss = 0.68 (955.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:33:06.837803: step 34600, loss = 0.77 (901.7 examples/sec; 0.142 sec/batch)
2017-05-07 20:33:08.088118: step 34610, loss = 0.89 (1023.7 examples/sec; 0.125 sec/batch)
2017-05-07 20:33:09.418924: step 34620, loss = 0.73 (961.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:33:10.768987: step 34630, loss = 0.86 (948.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:33:12.122087: step 34640, loss = 0.78 (946.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:33:13.463646: step 34650, loss = 0.85 (954.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:33:14.807696: step 34660, loss = 0.81 (952.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:33:16.150375: step 34670, loss = 0.70 (953.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:33:17.496182: step 34680, loss = 0.70 (951.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:33:18.831155: step 34690, loss = 0.80 (958.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:33:20.254128: step 34700, loss = 0.70 (899.5 examples/sec; 0.142 sec/batch)
2017-05-07 20:33:21.478443: step 34710, loss = 0.82 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-07 20:33:22.817573: step 34720, loss = 0.84 (955.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:33:24.150208: step 34730, loss = 0.79 (960.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:33:25.458080: step 34740, loss = 0.73 (978.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:33:26.828713: step 34750, loss = 0.81 (933.9 examples/sec; 0.137 sec/batch)
2017-05-07 20:33:28.161119: step 34760, loss = 0.72 (960.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:33:29.491907: step 34770, loss = 0.87 (961.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:33:30.825483: step 34780, loss = 0.70 (959.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:33:32.150336: step 34790, loss = 0.67 (966.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:33:33.650673: step 34800, loss = 0.71 (853.1 examples/sec; 0.150 sec/batch)
2017-05-07 20:33:34.811141: step 34810, loss = 0.70 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-07 20:33:36.140196: step 34820, loss = 0.61 (963.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:33:37.476644: step 34830, loss = 1.14 (957.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:33:38.812926: step 34840, loss = 0.78 (957.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:33:40.161617: step 34850, loss = 0.88 (949.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:33:41.473105: step 34860, loss = 0.85 (976.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:33:42.810502: step 34870, loss = 0.79 (957.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:33:44.148263: step 34880, loss = 0.69 (956.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:33:45.474528: step 34890, loss = 0.84 (965.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:33:46.921939: step 34900, loss = 0.74 (884.3 examples/sec; 0.145 sec/batch)
2017-05-07 20:33:48.189295: step 34910, loss = 0.67 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:33:49.507584: step 34920, loss = 0.78 (971.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:33:50.874149: step 34930, loss = 0.68 (936.7 examples/sec; 0.137 sec/batch)
2017-05-07 20:33:52.218991: step 34940, loss = 0.84 (951.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:33:53.568976: step 34950, loss = 0.82 (948.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:33:54.898667: step 34960, loss = 0.69 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:33:56.224688: step 34970, loss = 0.90 (965.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:33:57.565646: step 34980, loss = 0.65 (954.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:33:58.905093: step 34990, loss = 0.87 (955.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:34:00.327057: step 35000, loss = 0.98 (900.2 examples/sec; 0.142 sec/batch)
2017-05-07 20:34:01.576742: step 35010, loss = 0.72 (1024.3 examples/sec; 0.125 sec/batch)
2017-05-07 20:34:02.910722: step 35020, loss = 0.85 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:34:04.244525: step 35030, loss = 0.89 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:34:05.589168: step 35040, loss = 0.71 (951.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:34:06.948705: step 35050, loss = 0.70 (941.5 examples/sec; 0.136 sec/batch)
2017-05-07 20:34:08.269902: step 35060, loss = 0.70 (968.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:34:09.590618: step 35070, loss = 0.87 (969.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:34:10.941026: step 35080, loss = 0.72 (947.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:34:12.298765: step 35090, loss = 0.72 (942.7 examples/sec; 0.136 sec/batch)
2017-05-07 20:34:13.738568: step 35100, loss = 0.80 (889.0 examples/sec; 0.144 sec/batch)
2017-E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 718 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
05-07 20:34:14.947306: step 35110, loss = 0.75 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-07 20:34:16.281639: step 35120, loss = 0.78 (959.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:34:17.623006: step 35130, loss = 0.74 (954.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:34:18.947858: step 35140, loss = 0.74 (966.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:34:20.296204: step 35150, loss = 0.71 (949.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:34:21.630631: step 35160, loss = 0.76 (959.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:34:22.956037: step 35170, loss = 0.84 (965.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:34:24.276854: step 35180, loss = 0.91 (969.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:34:25.592058: step 35190, loss = 0.68 (973.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:34:27.058585: step 35200, loss = 0.64 (872.8 examples/sec; 0.147 sec/batch)
2017-05-07 20:34:28.279500: step 35210, loss = 0.79 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-07 20:34:29.614693: step 35220, loss = 0.69 (958.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:34:30.952622: step 35230, loss = 0.73 (956.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:34:32.281448: step 35240, loss = 0.77 (963.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:34:33.618104: step 35250, loss = 0.74 (957.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:34:34.960111: step 35260, loss = 0.90 (953.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:34:36.283684: step 35270, loss = 0.74 (967.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:34:37.618754: step 35280, loss = 0.83 (958.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:34:38.940883: step 35290, loss = 0.58 (968.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:34:40.375136: step 35300, loss = 0.81 (892.4 examples/sec; 0.143 sec/batch)
2017-05-07 20:34:41.616536: step 35310, loss = 0.70 (1031.1 examples/sec; 0.124 sec/batch)
2017-05-07 20:34:42.955488: step 35320, loss = 0.77 (956.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:34:44.276290: step 35330, loss = 0.71 (969.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:34:45.594694: step 35340, loss = 0.70 (970.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:34:46.948022: step 35350, loss = 0.79 (945.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:34:48.268031: step 35360, loss = 0.66 (969.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:34:49.578685: step 35370, loss = 0.68 (976.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:34:50.917946: step 35380, loss = 0.83 (955.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:34:52.253880: step 35390, loss = 0.78 (958.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:34:53.680040: step 35400, loss = 0.76 (897.5 examples/sec; 0.143 sec/batch)
2017-05-07 20:34:54.927786: step 35410, loss = 0.87 (1025.9 examples/sec; 0.125 sec/batch)
2017-05-07 20:34:56.276260: step 35420, loss = 0.73 (949.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:34:57.605575: step 35430, loss = 0.65 (962.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:34:58.922968: step 35440, loss = 0.89 (971.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:35:00.271385: step 35450, loss = 0.64 (949.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:35:01.564701: step 35460, loss = 0.68 (989.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:35:02.917351: step 35470, loss = 0.75 (946.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:35:04.246615: step 35480, loss = 0.77 (962.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:35:05.571072: step 35490, loss = 0.57 (966.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:35:06.997239: step 35500, loss = 0.86 (897.5 examples/sec; 0.143 sec/batch)
2017-05-07 20:35:08.238280: step 35510, loss = 0.72 (1031.4 examples/sec; 0.124 sec/batch)
2017-05-07 20:35:09.559083: step 35520, loss = 0.73 (969.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:35:10.899523: step 35530, loss = 0.78 (954.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:35:12.233134: step 35540, loss = 0.95 (959.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:35:13.576824: step 35550, loss = 0.82 (952.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:35:14.906157: step 35560, loss = 0.94 (962.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:35:16.229831: step 35570, loss = 0.96 (967.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:35:17.525495: step 35580, loss = 0.77 (987.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:35:18.845636: step 35590, loss = 0.69 (969.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:35:20.309956: step 35600, loss = 0.86 (874.1 examples/sec; 0.146 sec/batch)
2017-05-07 20:35:21.556045: step 35610, loss = 1.00 (1027.2 examples/sec; 0.125 sec/batch)
2017-05-07 20:35:22.904063: step 35620, loss = 0.72 (949.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:35:24.236732: step 35630, loss = 0.75 (960.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:35:25.556437: step 35640, loss = 0.82 (969.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:35:26.932960: step 35650, loss = 0.86 (929.9 examples/sec; 0.138 sec/batch)
2017-05-07 20:35:28.260955: step 35660, loss = 0.72 (963.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:35:29.590671: step 35670, loss = 0.80 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:35:30.930932: step 35680, loss = 0.74 (955.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:35:32.272221: step 35690, loss = 0.74 (954.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:35:33.711865: step 35700, loss = 0.77 (889.1 examples/sec; 0.144 sec/batch)
2017-05-07 20:35:34.922438: step 35710, loss = 0.79 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-07 20:35:36.246746: step 35720, loss = 0.65 (966.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:35:37.587898: step 35730, loss = 0.73 (954.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:35:38.932900: step 35740, loss = 0.69 (951.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:35:40.280354: step 35750, loss = 0.67 (949.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:35:41.610028: step 35760, loss = 0.83 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:35:42.955016: step 35770, loss = 0.71 (951.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:35:44.254850: step 35780, loss = 0.72 (984.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:35:45.590897: step 35790, loss = 0.94 (958.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:35:47.026238: step 35800, loss = 0.86 (891.8 examples/sec; 0.144 sec/batch)
2017-05-07 20:35:48.273391: step 35810, loss = 0.72 (1026.3 examples/sec; 0.125 sec/batch)
2017-05-07 20:35:49.599775: step 35820, loss = 0.83 (965.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:35:50.943712: step 35830, loss = 0.75 (952.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:35:52.278383: step 35840, loss = 0.82 (959.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:35:53.628108: step 35850, loss = 0.67 (948.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:35:54.967147: step 35860, loss = 0.81 (955.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:35:56.294662: step 35870, loss = 0.74 (964.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:35:57.637542: step 35880, loss = 0.72 (953.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:35:58.987403: step 35890, loss = 0.72 (948.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:36:00.440521: step 35900, loss = 0.88 (880.9 examples/sec; 0.145 sec/batch)
2017-05-07 20:36:01.662969: step 35910, loss = 0.56 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-07 20:36:03.017031: step 35920, loss = 0.76 (945.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:36:04.372258: step 35930, loss = 0.58 (944.5 examples/sec; 0.136 sec/batch)
2017-05-07 20:36:05.698317: step 35940, loss = 0.60 (965.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:36:07.054690: step 35950, loss = 0.63 (943.7 examples/sec; 0.136 sec/batch)
2017-05-07 20:36:08.389754: step 35960, loss = 0.73 (958.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:36:09.713845: step 35970, loss = 0.81 (966.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:36:11.054572: step 35980, loss = 0.60 (954.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:36:12.402119: step 35990, loss = 0.70 (949.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:36:13.850289: step 36000, loss = 0.88 (883.9 examples/sec; 0.145 sec/batch)
2017-05-07 20:36:15.078694: step 36010, loss = 0.85 (1042.0 examples/sec; 0.123 sec/batcE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 737 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
h)
2017-05-07 20:36:16.409178: step 36020, loss = 0.65 (962.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:36:17.747349: step 36030, loss = 0.79 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:36:19.102445: step 36040, loss = 0.72 (944.6 examples/sec; 0.136 sec/batch)
2017-05-07 20:36:20.490763: step 36050, loss = 0.79 (922.0 examples/sec; 0.139 sec/batch)
2017-05-07 20:36:21.795427: step 36060, loss = 0.78 (981.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:36:23.136223: step 36070, loss = 0.83 (954.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:36:24.463989: step 36080, loss = 0.73 (964.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:36:25.776990: step 36090, loss = 0.70 (974.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:36:27.234878: step 36100, loss = 0.69 (878.0 examples/sec; 0.146 sec/batch)
2017-05-07 20:36:28.455120: step 36110, loss = 0.68 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-07 20:36:29.781182: step 36120, loss = 0.68 (965.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:36:31.103539: step 36130, loss = 0.91 (968.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:36:32.420110: step 36140, loss = 0.81 (972.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:36:33.783267: step 36150, loss = 0.81 (939.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:36:35.119426: step 36160, loss = 0.65 (958.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:36:36.465829: step 36170, loss = 0.60 (950.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:36:37.801396: step 36180, loss = 0.79 (958.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:36:39.136750: step 36190, loss = 0.70 (958.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:36:40.576519: step 36200, loss = 0.62 (889.0 examples/sec; 0.144 sec/batch)
2017-05-07 20:36:41.818795: step 36210, loss = 0.86 (1030.4 examples/sec; 0.124 sec/batch)
2017-05-07 20:36:43.137555: step 36220, loss = 0.56 (970.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:36:44.479039: step 36230, loss = 0.74 (954.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:36:45.796906: step 36240, loss = 0.82 (971.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:36:47.155087: step 36250, loss = 0.79 (942.4 examples/sec; 0.136 sec/batch)
2017-05-07 20:36:48.502020: step 36260, loss = 0.83 (950.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:36:49.829148: step 36270, loss = 0.68 (964.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:36:51.178529: step 36280, loss = 0.62 (948.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:36:52.513415: step 36290, loss = 0.70 (958.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:36:53.956724: step 36300, loss = 0.74 (886.8 examples/sec; 0.144 sec/batch)
2017-05-07 20:36:55.168062: step 36310, loss = 0.80 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-07 20:36:56.487735: step 36320, loss = 0.74 (969.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:36:57.824507: step 36330, loss = 0.77 (957.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:36:59.175285: step 36340, loss = 0.72 (947.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:37:00.526722: step 36350, loss = 0.73 (947.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:37:01.838607: step 36360, loss = 1.14 (975.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:37:03.157105: step 36370, loss = 0.84 (970.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:37:04.479085: step 36380, loss = 0.83 (968.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:37:05.795269: step 36390, loss = 0.76 (972.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:37:07.220773: step 36400, loss = 0.75 (897.9 examples/sec; 0.143 sec/batch)
2017-05-07 20:37:08.473393: step 36410, loss = 0.81 (1021.9 examples/sec; 0.125 sec/batch)
2017-05-07 20:37:09.793481: step 36420, loss = 0.80 (969.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:37:11.114626: step 36430, loss = 0.80 (968.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:37:12.461698: step 36440, loss = 0.65 (950.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:37:13.805896: step 36450, loss = 0.69 (952.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:37:15.146243: step 36460, loss = 0.88 (955.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:37:16.488431: step 36470, loss = 0.78 (953.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:37:17.821854: step 36480, loss = 0.72 (959.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:37:19.163540: step 36490, loss = 0.70 (954.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:37:20.597931: step 36500, loss = 0.85 (892.4 examples/sec; 0.143 sec/batch)
2017-05-07 20:37:21.829933: step 36510, loss = 0.81 (1039.0 examples/sec; 0.123 sec/batch)
2017-05-07 20:37:23.155183: step 36520, loss = 0.61 (965.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:37:24.477645: step 36530, loss = 0.69 (967.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:37:25.785357: step 36540, loss = 0.87 (978.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:37:27.122986: step 36550, loss = 0.71 (956.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:37:28.467829: step 36560, loss = 0.75 (951.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:37:29.793011: step 36570, loss = 0.62 (965.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:37:31.109892: step 36580, loss = 0.75 (972.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:37:32.458074: step 36590, loss = 0.69 (949.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:37:33.888813: step 36600, loss = 0.81 (894.6 examples/sec; 0.143 sec/batch)
2017-05-07 20:37:35.150125: step 36610, loss = 0.72 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:37:36.483119: step 36620, loss = 0.73 (960.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:37:37.802869: step 36630, loss = 0.68 (969.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:37:39.141141: step 36640, loss = 0.73 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:37:40.471683: step 36650, loss = 0.86 (962.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:37:41.784219: step 36660, loss = 0.79 (975.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:37:43.101728: step 36670, loss = 0.81 (971.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:37:44.430590: step 36680, loss = 0.91 (963.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:37:45.770816: step 36690, loss = 0.60 (955.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:37:47.212005: step 36700, loss = 0.80 (888.2 examples/sec; 0.144 sec/batch)
2017-05-07 20:37:48.464153: step 36710, loss = 0.85 (1022.2 examples/sec; 0.125 sec/batch)
2017-05-07 20:37:49.798177: step 36720, loss = 0.83 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:37:51.111629: step 36730, loss = 0.78 (974.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:37:52.432782: step 36740, loss = 0.73 (968.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:37:53.785363: step 36750, loss = 0.83 (946.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:37:55.104598: step 36760, loss = 0.67 (970.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:37:56.421052: step 36770, loss = 0.78 (972.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:37:57.769206: step 36780, loss = 0.79 (949.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:37:59.102210: step 36790, loss = 0.73 (960.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:38:00.542799: step 36800, loss = 0.82 (888.5 examples/sec; 0.144 sec/batch)
2017-05-07 20:38:01.763391: step 36810, loss = 0.81 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-07 20:38:03.081672: step 36820, loss = 0.83 (971.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:38:04.409762: step 36830, loss = 0.70 (963.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:38:05.725951: step 36840, loss = 0.61 (972.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:38:07.100761: step 36850, loss = 0.95 (931.0 examples/sec; 0.137 sec/batch)
2017-05-07 20:38:08.423935: step 36860, loss = 0.67 (967.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:38:09.740811: step 36870, loss = 0.72 (972.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:38:11.086571: step 36880, loss = 0.71 (951.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:38:12.400839: step 36890, loss = 0.67 (973.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:38:13.821102: step 36900, loss = 0.79 (901.2 examples/sec; 0.142 sec/batch)
2017-05-07 20:38:15.067108: step 36910, loss = 0.73 (1027.3 examples/sec; 0.125 sec/batch)
2017-05-07 20:38:16.386397: step 36920, loss = 0.79 (970.2 examples/sec; 0.132 sE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 757 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
ec/batch)
2017-05-07 20:38:17.722131: step 36930, loss = 0.80 (958.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:38:19.064703: step 36940, loss = 0.90 (953.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:38:20.414734: step 36950, loss = 0.63 (948.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:38:21.734079: step 36960, loss = 0.60 (970.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:38:23.067441: step 36970, loss = 0.80 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:38:24.394112: step 36980, loss = 1.01 (964.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:38:25.720053: step 36990, loss = 0.72 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:38:27.157918: step 37000, loss = 0.66 (890.2 examples/sec; 0.144 sec/batch)
2017-05-07 20:38:28.394983: step 37010, loss = 0.92 (1034.7 examples/sec; 0.124 sec/batch)
2017-05-07 20:38:29.708144: step 37020, loss = 0.84 (974.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:38:31.042248: step 37030, loss = 0.80 (959.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:38:32.395350: step 37040, loss = 0.80 (946.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:38:33.733051: step 37050, loss = 0.74 (956.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:38:35.055838: step 37060, loss = 0.88 (967.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:38:36.385007: step 37070, loss = 0.75 (963.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:38:37.723254: step 37080, loss = 0.61 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:38:39.065108: step 37090, loss = 0.74 (953.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:38:40.521013: step 37100, loss = 0.63 (879.2 examples/sec; 0.146 sec/batch)
2017-05-07 20:38:41.761979: step 37110, loss = 0.58 (1031.4 examples/sec; 0.124 sec/batch)
2017-05-07 20:38:43.110622: step 37120, loss = 0.71 (949.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:38:44.438844: step 37130, loss = 0.82 (963.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:38:45.773067: step 37140, loss = 0.82 (959.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:38:47.106119: step 37150, loss = 0.69 (960.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:38:48.431753: step 37160, loss = 0.74 (965.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:38:49.753352: step 37170, loss = 0.64 (968.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:38:51.096317: step 37180, loss = 0.77 (953.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:38:52.434258: step 37190, loss = 0.65 (956.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:38:53.860787: step 37200, loss = 0.81 (897.3 examples/sec; 0.143 sec/batch)
2017-05-07 20:38:55.087502: step 37210, loss = 0.75 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-07 20:38:56.425920: step 37220, loss = 0.85 (956.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:38:57.759124: step 37230, loss = 1.36 (960.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:38:59.095752: step 37240, loss = 0.81 (957.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:39:00.440540: step 37250, loss = 0.66 (951.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:39:01.749385: step 37260, loss = 0.84 (978.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:39:03.096483: step 37270, loss = 0.64 (950.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:39:04.438588: step 37280, loss = 0.94 (953.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:39:05.775305: step 37290, loss = 0.69 (957.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:39:07.200175: step 37300, loss = 0.65 (898.3 examples/sec; 0.142 sec/batch)
2017-05-07 20:39:08.412036: step 37310, loss = 0.77 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-07 20:39:09.747894: step 37320, loss = 0.69 (958.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:39:11.060299: step 37330, loss = 0.83 (975.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:39:12.408604: step 37340, loss = 0.69 (949.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:39:13.743979: step 37350, loss = 0.86 (958.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:39:15.069003: step 37360, loss = 0.69 (966.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:39:16.407407: step 37370, loss = 0.71 (956.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:39:17.730390: step 37380, loss = 0.65 (967.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:39:19.073125: step 37390, loss = 0.75 (953.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:39:20.536688: step 37400, loss = 0.80 (874.6 examples/sec; 0.146 sec/batch)
2017-05-07 20:39:21.783947: step 37410, loss = 0.80 (1026.3 examples/sec; 0.125 sec/batch)
2017-05-07 20:39:23.120913: step 37420, loss = 0.84 (957.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:39:24.434444: step 37430, loss = 0.83 (974.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:39:25.775646: step 37440, loss = 1.01 (954.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:39:27.161007: step 37450, loss = 0.67 (923.9 examples/sec; 0.139 sec/batch)
2017-05-07 20:39:28.494749: step 37460, loss = 0.85 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:39:29.838875: step 37470, loss = 0.68 (952.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:39:31.178492: step 37480, loss = 0.75 (955.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:39:32.515797: step 37490, loss = 0.76 (957.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:39:33.945485: step 37500, loss = 0.86 (895.3 examples/sec; 0.143 sec/batch)
2017-05-07 20:39:35.155592: step 37510, loss = 0.73 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-07 20:39:36.505596: step 37520, loss = 0.89 (948.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:39:37.825130: step 37530, loss = 0.86 (970.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:39:39.160932: step 37540, loss = 0.74 (958.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:39:40.510578: step 37550, loss = 0.76 (948.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:39:41.838057: step 37560, loss = 0.76 (964.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:39:43.158864: step 37570, loss = 0.79 (969.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:39:44.456286: step 37580, loss = 0.77 (986.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:39:45.803622: step 37590, loss = 0.64 (950.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:39:47.259892: step 37600, loss = 0.71 (879.0 examples/sec; 0.146 sec/batch)
2017-05-07 20:39:48.496377: step 37610, loss = 0.72 (1035.2 examples/sec; 0.124 sec/batch)
2017-05-07 20:39:49.827983: step 37620, loss = 0.68 (961.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:39:51.175406: step 37630, loss = 0.90 (950.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:39:52.510172: step 37640, loss = 0.80 (959.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:39:53.855964: step 37650, loss = 0.92 (951.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:39:55.185856: step 37660, loss = 0.77 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:39:56.517956: step 37670, loss = 0.74 (960.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:39:57.863421: step 37680, loss = 0.85 (951.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:39:59.216023: step 37690, loss = 0.62 (946.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:40:00.630865: step 37700, loss = 0.67 (904.7 examples/sec; 0.141 sec/batch)
2017-05-07 20:40:01.833317: step 37710, loss = 0.59 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-07 20:40:03.186130: step 37720, loss = 0.80 (946.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:40:04.536878: step 37730, loss = 0.71 (947.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:40:05.846961: step 37740, loss = 0.74 (977.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:40:07.184496: step 37750, loss = 0.69 (957.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:40:08.524942: step 37760, loss = 0.83 (954.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:40:09.848029: step 37770, loss = 0.72 (967.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:40:11.169551: step 37780, loss = 0.76 (968.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:40:12.504185: step 37790, loss = 0.94 (959.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:40:13.932548: step 37800, loss = 0.72 (896.1 examples/sec; 0.143 sec/batch)
2017-05-07 20:40:15.176140: step 37810, loss = 0.56 (1029.3 examples/sec; 0.124 sec/batch)
2017-05-07 20:40:16.508768: step 37820, loss = 0.93 (960.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:40:17.842296: step 37830, loss = 0.66 (959.9 examples/sec; E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 775 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
0.133 sec/batch)
2017-05-07 20:40:19.166272: step 37840, loss = 0.70 (966.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:40:20.522104: step 37850, loss = 0.79 (944.1 examples/sec; 0.136 sec/batch)
2017-05-07 20:40:21.834730: step 37860, loss = 0.61 (975.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:40:23.158827: step 37870, loss = 0.76 (966.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:40:24.492372: step 37880, loss = 0.78 (959.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:40:25.833432: step 37890, loss = 0.74 (954.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:40:27.251484: step 37900, loss = 0.97 (902.6 examples/sec; 0.142 sec/batch)
2017-05-07 20:40:28.494118: step 37910, loss = 0.77 (1030.1 examples/sec; 0.124 sec/batch)
2017-05-07 20:40:29.833237: step 37920, loss = 0.69 (955.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:40:31.166527: step 37930, loss = 0.79 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:40:32.499705: step 37940, loss = 0.72 (960.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:40:33.860387: step 37950, loss = 0.83 (940.7 examples/sec; 0.136 sec/batch)
2017-05-07 20:40:35.182144: step 37960, loss = 0.84 (968.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:40:36.523103: step 37970, loss = 0.77 (954.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:40:37.861845: step 37980, loss = 0.90 (956.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:40:39.212558: step 37990, loss = 0.63 (947.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:40:40.670404: step 38000, loss = 0.98 (878.0 examples/sec; 0.146 sec/batch)
2017-05-07 20:40:41.882696: step 38010, loss = 0.69 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-07 20:40:43.231493: step 38020, loss = 0.59 (949.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:40:44.576976: step 38030, loss = 0.71 (951.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:40:45.899498: step 38040, loss = 0.86 (967.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:40:47.271115: step 38050, loss = 0.71 (933.2 examples/sec; 0.137 sec/batch)
2017-05-07 20:40:48.601433: step 38060, loss = 0.71 (962.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:40:49.937075: step 38070, loss = 0.87 (958.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:40:51.244716: step 38080, loss = 0.74 (978.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:40:52.570927: step 38090, loss = 0.83 (965.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:40:54.025012: step 38100, loss = 0.76 (880.3 examples/sec; 0.145 sec/batch)
2017-05-07 20:40:55.276737: step 38110, loss = 0.71 (1022.6 examples/sec; 0.125 sec/batch)
2017-05-07 20:40:56.624792: step 38120, loss = 0.78 (949.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:40:57.946492: step 38130, loss = 0.77 (968.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:40:59.297687: step 38140, loss = 0.83 (947.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:41:00.639414: step 38150, loss = 0.84 (954.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:41:01.939499: step 38160, loss = 0.68 (984.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:41:03.272105: step 38170, loss = 0.95 (960.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:41:04.597048: step 38180, loss = 0.85 (966.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:41:05.936763: step 38190, loss = 0.77 (955.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:41:07.365803: step 38200, loss = 0.72 (895.7 examples/sec; 0.143 sec/batch)
2017-05-07 20:41:08.595143: step 38210, loss = 0.70 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-07 20:41:09.946239: step 38220, loss = 0.70 (947.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:41:11.260737: step 38230, loss = 0.67 (973.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:41:12.576856: step 38240, loss = 0.67 (972.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:41:13.902568: step 38250, loss = 0.63 (965.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:41:15.240940: step 38260, loss = 0.73 (956.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:41:16.553113: step 38270, loss = 0.65 (975.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:41:17.874856: step 38280, loss = 0.72 (968.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:41:19.210740: step 38290, loss = 0.62 (958.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:41:20.641090: step 38300, loss = 0.74 (894.9 examples/sec; 0.143 sec/batch)
2017-05-07 20:41:21.884980: step 38310, loss = 0.66 (1029.0 examples/sec; 0.124 sec/batch)
2017-05-07 20:41:23.221650: step 38320, loss = 0.57 (957.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:41:24.536211: step 38330, loss = 0.66 (973.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:41:25.881792: step 38340, loss = 0.59 (951.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:41:27.223012: step 38350, loss = 0.67 (954.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:41:28.564766: step 38360, loss = 0.74 (954.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:41:29.883979: step 38370, loss = 0.72 (970.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:41:31.201951: step 38380, loss = 0.74 (971.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:41:32.545852: step 38390, loss = 0.76 (952.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:41:33.978077: step 38400, loss = 0.69 (893.7 examples/sec; 0.143 sec/batch)
2017-05-07 20:41:35.216190: step 38410, loss = 0.85 (1033.8 examples/sec; 0.124 sec/batch)
2017-05-07 20:41:36.551655: step 38420, loss = 0.76 (958.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:41:37.873992: step 38430, loss = 0.70 (968.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:41:39.212370: step 38440, loss = 0.84 (956.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:41:40.554786: step 38450, loss = 0.68 (953.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:41:41.871489: step 38460, loss = 0.83 (972.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:41:43.201412: step 38470, loss = 0.66 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:41:44.534128: step 38480, loss = 0.90 (960.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:41:45.865891: step 38490, loss = 0.64 (961.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:41:47.291667: step 38500, loss = 0.84 (897.8 examples/sec; 0.143 sec/batch)
2017-05-07 20:41:48.540973: step 38510, loss = 0.82 (1024.6 examples/sec; 0.125 sec/batch)
2017-05-07 20:41:49.859711: step 38520, loss = 0.56 (970.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:41:51.185797: step 38530, loss = 0.63 (965.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:41:52.525883: step 38540, loss = 0.90 (955.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:41:53.885089: step 38550, loss = 0.85 (941.7 examples/sec; 0.136 sec/batch)
2017-05-07 20:41:55.204771: step 38560, loss = 0.65 (969.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:41:56.544098: step 38570, loss = 0.75 (955.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:41:57.860759: step 38580, loss = 0.74 (972.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:41:59.231622: step 38590, loss = 0.73 (933.7 examples/sec; 0.137 sec/batch)
2017-05-07 20:42:00.672439: step 38600, loss = 0.67 (888.4 examples/sec; 0.144 sec/batch)
2017-05-07 20:42:01.912586: step 38610, loss = 0.71 (1032.1 examples/sec; 0.124 sec/batch)
2017-05-07 20:42:03.246785: step 38620, loss = 0.66 (959.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:42:04.575198: step 38630, loss = 0.68 (963.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:42:05.893813: step 38640, loss = 0.74 (970.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:42:07.243657: step 38650, loss = 0.83 (948.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:42:08.533000: step 38660, loss = 1.02 (992.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:42:09.855942: step 38670, loss = 0.74 (967.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:42:11.199177: step 38680, loss = 0.72 (952.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:42:12.510445: step 38690, loss = 0.78 (976.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:42:13.957927: step 38700, loss = 0.77 (884.3 examples/sec; 0.145 sec/batch)
2017-05-07 20:42:15.209875: step 38710, loss = 0.71 (1022.4 examples/sec; 0.125 sec/batch)
2017-05-07 20:42:16.529691: step 38720, loss = 0.86 (969.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:42:17.879585: step 38730, loss = 0.72 (948.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:42:19.217747: step 38740, loss = 0.70 (956.5 exampleE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 793 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
s/sec; 0.134 sec/batch)
2017-05-07 20:42:20.579346: step 38750, loss = 0.66 (940.1 examples/sec; 0.136 sec/batch)
2017-05-07 20:42:21.901887: step 38760, loss = 1.04 (967.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:42:23.235824: step 38770, loss = 0.60 (959.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:42:24.566366: step 38780, loss = 0.78 (962.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:42:25.900798: step 38790, loss = 0.78 (959.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:42:27.357772: step 38800, loss = 0.67 (878.5 examples/sec; 0.146 sec/batch)
2017-05-07 20:42:28.599440: step 38810, loss = 0.68 (1030.9 examples/sec; 0.124 sec/batch)
2017-05-07 20:42:29.943279: step 38820, loss = 0.62 (952.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:42:31.283541: step 38830, loss = 0.79 (955.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:42:32.639199: step 38840, loss = 0.69 (944.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:42:33.975772: step 38850, loss = 0.70 (957.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:42:35.302657: step 38860, loss = 0.86 (964.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:42:36.611592: step 38870, loss = 0.75 (977.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:42:37.943116: step 38880, loss = 0.78 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:42:39.276429: step 38890, loss = 0.64 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:42:40.700975: step 38900, loss = 0.73 (898.5 examples/sec; 0.142 sec/batch)
2017-05-07 20:42:41.929205: step 38910, loss = 0.58 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-07 20:42:43.266539: step 38920, loss = 0.71 (957.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:42:44.593102: step 38930, loss = 0.72 (964.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:42:45.918257: step 38940, loss = 0.88 (965.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:42:47.283440: step 38950, loss = 0.62 (937.6 examples/sec; 0.137 sec/batch)
2017-05-07 20:42:48.616095: step 38960, loss = 0.89 (960.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:42:49.947641: step 38970, loss = 0.81 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:42:51.283647: step 38980, loss = 0.73 (958.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:42:52.612323: step 38990, loss = 0.63 (963.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:42:54.053732: step 39000, loss = 0.71 (888.0 examples/sec; 0.144 sec/batch)
2017-05-07 20:42:55.280547: step 39010, loss = 0.92 (1043.3 examples/sec; 0.123 sec/batch)
2017-05-07 20:42:56.630839: step 39020, loss = 0.70 (947.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:42:57.977850: step 39030, loss = 0.71 (950.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:42:59.312931: step 39040, loss = 0.85 (958.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:43:00.648645: step 39050, loss = 0.72 (958.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:43:01.969648: step 39060, loss = 0.75 (969.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:43:03.325581: step 39070, loss = 0.75 (944.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:43:04.650615: step 39080, loss = 0.71 (966.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:43:05.994885: step 39090, loss = 0.75 (952.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:43:07.452734: step 39100, loss = 0.73 (878.0 examples/sec; 0.146 sec/batch)
2017-05-07 20:43:08.677743: step 39110, loss = 0.80 (1044.9 examples/sec; 0.123 sec/batch)
2017-05-07 20:43:10.019522: step 39120, loss = 0.72 (954.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:43:11.363004: step 39130, loss = 0.85 (952.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:43:12.715730: step 39140, loss = 0.67 (946.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:43:14.082886: step 39150, loss = 0.73 (936.3 examples/sec; 0.137 sec/batch)
2017-05-07 20:43:15.391124: step 39160, loss = 0.73 (978.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:43:16.728160: step 39170, loss = 0.69 (957.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:43:18.057197: step 39180, loss = 0.75 (963.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:43:19.388764: step 39190, loss = 0.85 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:43:20.843439: step 39200, loss = 0.66 (879.9 examples/sec; 0.145 sec/batch)
2017-05-07 20:43:22.066728: step 39210, loss = 0.60 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-07 20:43:23.404853: step 39220, loss = 0.79 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:43:24.729595: step 39230, loss = 0.65 (966.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:43:26.079389: step 39240, loss = 0.59 (948.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:43:27.452778: step 39250, loss = 0.85 (932.0 examples/sec; 0.137 sec/batch)
2017-05-07 20:43:28.774892: step 39260, loss = 0.81 (968.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:43:30.095423: step 39270, loss = 0.84 (969.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:43:31.399245: step 39280, loss = 0.83 (981.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:43:32.707616: step 39290, loss = 0.76 (978.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:43:34.144246: step 39300, loss = 0.74 (891.0 examples/sec; 0.144 sec/batch)
2017-05-07 20:43:35.382933: step 39310, loss = 0.80 (1033.4 examples/sec; 0.124 sec/batch)
2017-05-07 20:43:36.714400: step 39320, loss = 0.56 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:43:38.057300: step 39330, loss = 0.76 (953.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:43:39.386048: step 39340, loss = 0.79 (963.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:43:40.727666: step 39350, loss = 0.68 (954.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:43:42.072379: step 39360, loss = 0.63 (951.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:43:43.406860: step 39370, loss = 0.59 (959.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:43:44.760565: step 39380, loss = 0.61 (945.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:43:46.113687: step 39390, loss = 0.71 (946.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:43:47.569558: step 39400, loss = 0.63 (879.2 examples/sec; 0.146 sec/batch)
2017-05-07 20:43:48.772683: step 39410, loss = 0.74 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-07 20:43:50.131993: step 39420, loss = 0.95 (941.7 examples/sec; 0.136 sec/batch)
2017-05-07 20:43:51.468347: step 39430, loss = 0.74 (957.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:43:52.801505: step 39440, loss = 0.77 (960.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:43:54.155536: step 39450, loss = 0.59 (945.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:43:55.473494: step 39460, loss = 0.77 (971.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:43:56.806167: step 39470, loss = 0.90 (960.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:43:58.100148: step 39480, loss = 0.85 (989.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:43:59.458253: step 39490, loss = 0.82 (942.5 examples/sec; 0.136 sec/batch)
2017-05-07 20:44:00.876789: step 39500, loss = 0.79 (902.3 examples/sec; 0.142 sec/batch)
2017-05-07 20:44:02.108734: step 39510, loss = 0.66 (1039.0 examples/sec; 0.123 sec/batch)
2017-05-07 20:44:03.446369: step 39520, loss = 0.72 (956.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:44:04.776859: step 39530, loss = 0.96 (962.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:44:06.114797: step 39540, loss = 0.68 (956.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:44:07.481540: step 39550, loss = 0.72 (936.5 examples/sec; 0.137 sec/batch)
2017-05-07 20:44:08.798148: step 39560, loss = 0.82 (972.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:44:10.142909: step 39570, loss = 0.79 (951.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:44:11.469076: step 39580, loss = 0.76 (965.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:44:12.783805: step 39590, loss = 0.75 (973.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:44:14.233701: step 39600, loss = 0.93 (882.8 examples/sec; 0.145 sec/batch)
2017-05-07 20:44:15.459681: step 39610, loss = 0.80 (1044.1 examples/sec; 0.123 sec/batch)
2017-05-07 20:44:16.800113: step 39620, loss = 0.91 (954.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:44:18.144431: step 39630, loss = 0.81 (952.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:44:19.484668: step 39640, loss = 0.72 (955.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:44:20.817490: step 39650, loss = 0.67 (960.4 E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 811 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
examples/sec; 0.133 sec/batch)
2017-05-07 20:44:22.168832: step 39660, loss = 0.66 (947.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:44:23.500087: step 39670, loss = 0.84 (961.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:44:24.818233: step 39680, loss = 0.75 (971.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:44:26.177691: step 39690, loss = 0.92 (941.5 examples/sec; 0.136 sec/batch)
2017-05-07 20:44:27.600958: step 39700, loss = 0.70 (899.3 examples/sec; 0.142 sec/batch)
2017-05-07 20:44:28.800723: step 39710, loss = 0.83 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-07 20:44:30.133266: step 39720, loss = 0.86 (960.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:44:31.482191: step 39730, loss = 0.73 (948.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:44:32.822794: step 39740, loss = 0.66 (954.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:44:34.165831: step 39750, loss = 0.61 (953.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:44:35.506684: step 39760, loss = 0.72 (954.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:44:36.816007: step 39770, loss = 0.63 (977.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:44:38.148727: step 39780, loss = 0.63 (960.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:44:39.506300: step 39790, loss = 0.82 (942.9 examples/sec; 0.136 sec/batch)
2017-05-07 20:44:40.963857: step 39800, loss = 0.85 (878.2 examples/sec; 0.146 sec/batch)
2017-05-07 20:44:42.190276: step 39810, loss = 0.79 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-07 20:44:43.525561: step 39820, loss = 0.73 (958.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:44:44.876367: step 39830, loss = 0.69 (947.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:44:46.208532: step 39840, loss = 0.71 (960.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:44:47.568790: step 39850, loss = 0.94 (941.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:44:48.906339: step 39860, loss = 0.76 (957.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:44:50.234987: step 39870, loss = 0.71 (963.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:44:51.550979: step 39880, loss = 0.77 (972.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:44:52.900587: step 39890, loss = 0.67 (948.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:44:54.346549: step 39900, loss = 0.70 (885.2 examples/sec; 0.145 sec/batch)
2017-05-07 20:44:55.602045: step 39910, loss = 0.76 (1019.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:44:56.938965: step 39920, loss = 0.74 (957.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:44:58.277845: step 39930, loss = 0.65 (956.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:44:59.615063: step 39940, loss = 0.63 (957.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:45:00.958725: step 39950, loss = 0.85 (952.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:45:02.284111: step 39960, loss = 0.81 (965.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:45:03.626838: step 39970, loss = 0.71 (953.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:45:04.959820: step 39980, loss = 0.75 (960.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:45:06.319244: step 39990, loss = 0.82 (941.6 examples/sec; 0.136 sec/batch)
2017-05-07 20:45:07.751088: step 40000, loss = 0.87 (894.0 examples/sec; 0.143 sec/batch)
2017-05-07 20:45:08.967694: step 40010, loss = 0.77 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-07 20:45:10.278886: step 40020, loss = 0.95 (976.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:45:11.642736: step 40030, loss = 0.78 (938.5 examples/sec; 0.136 sec/batch)
2017-05-07 20:45:12.979672: step 40040, loss = 0.93 (957.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:45:14.358508: step 40050, loss = 0.99 (928.3 examples/sec; 0.138 sec/batch)
2017-05-07 20:45:15.695411: step 40060, loss = 0.71 (957.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:45:17.033365: step 40070, loss = 0.74 (956.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:45:18.368084: step 40080, loss = 0.63 (959.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:45:19.702317: step 40090, loss = 0.79 (959.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:45:21.150056: step 40100, loss = 0.71 (884.1 examples/sec; 0.145 sec/batch)
2017-05-07 20:45:22.400759: step 40110, loss = 0.77 (1023.4 examples/sec; 0.125 sec/batch)
2017-05-07 20:45:23.738473: step 40120, loss = 0.70 (956.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:45:25.072136: step 40130, loss = 0.58 (959.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:45:26.424375: step 40140, loss = 0.74 (946.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:45:27.769669: step 40150, loss = 1.02 (951.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:45:29.121622: step 40160, loss = 0.91 (946.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:45:30.454288: step 40170, loss = 0.79 (960.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:45:31.789636: step 40180, loss = 0.82 (958.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:45:33.114254: step 40190, loss = 0.61 (966.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:45:34.551244: step 40200, loss = 0.68 (890.7 examples/sec; 0.144 sec/batch)
2017-05-07 20:45:35.780186: step 40210, loss = 0.73 (1041.5 examples/sec; 0.123 sec/batch)
2017-05-07 20:45:37.094703: step 40220, loss = 0.75 (973.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:45:38.437186: step 40230, loss = 0.82 (953.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:45:39.779638: step 40240, loss = 0.88 (953.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:45:41.124349: step 40250, loss = 0.91 (951.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:45:42.471727: step 40260, loss = 0.83 (950.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:45:43.811211: step 40270, loss = 0.77 (955.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:45:45.120250: step 40280, loss = 0.71 (977.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:45:46.449895: step 40290, loss = 0.79 (962.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:45:47.893891: step 40300, loss = 0.75 (886.4 examples/sec; 0.144 sec/batch)
2017-05-07 20:45:49.128922: step 40310, loss = 0.61 (1036.4 examples/sec; 0.124 sec/batch)
2017-05-07 20:45:50.460652: step 40320, loss = 0.65 (961.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:45:51.803864: step 40330, loss = 0.71 (953.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:45:53.153899: step 40340, loss = 0.71 (948.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:45:54.659996: step 40350, loss = 0.74 (849.9 examples/sec; 0.151 sec/batch)
2017-05-07 20:45:55.896648: step 40360, loss = 0.82 (1035.0 examples/sec; 0.124 sec/batch)
2017-05-07 20:45:57.234679: step 40370, loss = 0.59 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:45:58.561867: step 40380, loss = 0.66 (964.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:45:59.898418: step 40390, loss = 0.73 (957.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:46:01.323902: step 40400, loss = 0.70 (897.9 examples/sec; 0.143 sec/batch)
2017-05-07 20:46:02.547578: step 40410, loss = 0.80 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-07 20:46:03.886427: step 40420, loss = 0.81 (956.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:46:05.222638: step 40430, loss = 0.64 (957.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:46:06.539919: step 40440, loss = 0.76 (971.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:46:07.915575: step 40450, loss = 0.74 (930.5 examples/sec; 0.138 sec/batch)
2017-05-07 20:46:09.247331: step 40460, loss = 0.74 (961.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:46:10.574893: step 40470, loss = 0.66 (964.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:46:11.920893: step 40480, loss = 0.67 (951.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:46:13.253129: step 40490, loss = 0.68 (960.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:46:14.742281: step 40500, loss = 0.71 (859.6 examples/sec; 0.149 sec/batch)
2017-05-07 20:46:15.964007: step 40510, loss = 0.67 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-07 20:46:17.310804: step 40520, loss = 0.61 (950.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:46:18.640638: step 40530, loss = 0.71 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:46:19.984749: step 40540, loss = 0.90 (952.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:46:21.331653: step 40550, loss = 0.68 (950.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:46:22.671950: step 40560, loss = 0.84E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 830 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
 (955.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:46:23.997200: step 40570, loss = 0.79 (965.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:46:25.321923: step 40580, loss = 0.78 (966.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:46:26.642969: step 40590, loss = 0.59 (968.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:46:28.083772: step 40600, loss = 0.74 (888.4 examples/sec; 0.144 sec/batch)
2017-05-07 20:46:29.314123: step 40610, loss = 0.70 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-07 20:46:30.654493: step 40620, loss = 0.69 (955.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:46:31.992536: step 40630, loss = 0.86 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:46:33.319986: step 40640, loss = 0.77 (964.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:46:34.677681: step 40650, loss = 0.62 (942.8 examples/sec; 0.136 sec/batch)
2017-05-07 20:46:36.001822: step 40660, loss = 0.83 (966.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:46:37.333576: step 40670, loss = 0.76 (961.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:46:38.660123: step 40680, loss = 0.70 (964.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:46:39.995498: step 40690, loss = 0.75 (958.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:46:41.436180: step 40700, loss = 0.64 (888.5 examples/sec; 0.144 sec/batch)
2017-05-07 20:46:42.658777: step 40710, loss = 0.74 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-07 20:46:43.994747: step 40720, loss = 0.66 (958.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:46:45.315617: step 40730, loss = 0.73 (969.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:46:46.658842: step 40740, loss = 0.75 (952.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:46:47.986731: step 40750, loss = 0.80 (963.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:46:49.308943: step 40760, loss = 0.78 (968.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:46:50.627873: step 40770, loss = 0.60 (970.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:46:51.964880: step 40780, loss = 0.67 (957.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:46:53.283397: step 40790, loss = 0.96 (970.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:46:54.725288: step 40800, loss = 0.71 (887.7 examples/sec; 0.144 sec/batch)
2017-05-07 20:46:55.980508: step 40810, loss = 0.89 (1019.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:46:57.322667: step 40820, loss = 0.97 (953.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:46:58.666096: step 40830, loss = 0.66 (952.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:46:59.985155: step 40840, loss = 0.77 (970.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:47:01.333357: step 40850, loss = 0.78 (949.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:47:02.677692: step 40860, loss = 0.70 (952.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:47:03.989500: step 40870, loss = 0.87 (975.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:47:05.329092: step 40880, loss = 0.79 (955.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:47:06.660896: step 40890, loss = 0.77 (961.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:47:08.113158: step 40900, loss = 0.70 (881.4 examples/sec; 0.145 sec/batch)
2017-05-07 20:47:09.350616: step 40910, loss = 0.62 (1034.4 examples/sec; 0.124 sec/batch)
2017-05-07 20:47:10.662697: step 40920, loss = 0.66 (975.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:47:11.994431: step 40930, loss = 0.70 (961.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:47:13.330946: step 40940, loss = 1.01 (957.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:47:14.678213: step 40950, loss = 0.73 (950.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:47:15.993637: step 40960, loss = 0.72 (973.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:47:17.339229: step 40970, loss = 0.68 (951.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:47:18.667319: step 40980, loss = 0.63 (963.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:47:20.001421: step 40990, loss = 0.80 (959.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:47:21.420923: step 41000, loss = 0.76 (901.7 examples/sec; 0.142 sec/batch)
2017-05-07 20:47:22.677257: step 41010, loss = 0.63 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:47:24.007642: step 41020, loss = 0.73 (962.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:47:25.335833: step 41030, loss = 0.73 (963.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:47:26.675328: step 41040, loss = 0.81 (955.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:47:28.054443: step 41050, loss = 0.70 (928.1 examples/sec; 0.138 sec/batch)
2017-05-07 20:47:29.377278: step 41060, loss = 0.68 (967.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:47:30.720351: step 41070, loss = 0.82 (953.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:47:32.047822: step 41080, loss = 0.90 (964.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:47:33.395184: step 41090, loss = 0.81 (950.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:47:34.837619: step 41100, loss = 0.77 (887.4 examples/sec; 0.144 sec/batch)
2017-05-07 20:47:36.034399: step 41110, loss = 0.67 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-07 20:47:37.375566: step 41120, loss = 0.71 (954.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:47:38.696498: step 41130, loss = 0.63 (969.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:47:40.041767: step 41140, loss = 0.85 (951.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:47:41.400941: step 41150, loss = 0.68 (941.7 examples/sec; 0.136 sec/batch)
2017-05-07 20:47:42.744563: step 41160, loss = 0.64 (952.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:47:44.083940: step 41170, loss = 0.78 (955.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:47:45.417382: step 41180, loss = 0.98 (959.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:47:46.739507: step 41190, loss = 0.85 (968.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:47:48.200208: step 41200, loss = 0.69 (876.3 examples/sec; 0.146 sec/batch)
2017-05-07 20:47:49.405510: step 41210, loss = 0.80 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-07 20:47:50.731283: step 41220, loss = 0.89 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:47:52.067180: step 41230, loss = 0.79 (958.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:47:53.398475: step 41240, loss = 0.80 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:47:54.746592: step 41250, loss = 0.85 (949.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:47:56.084698: step 41260, loss = 0.71 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:47:57.402571: step 41270, loss = 0.83 (971.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:47:58.740790: step 41280, loss = 0.73 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:48:00.090789: step 41290, loss = 0.74 (948.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:48:01.530382: step 41300, loss = 0.86 (889.1 examples/sec; 0.144 sec/batch)
2017-05-07 20:48:02.776126: step 41310, loss = 0.71 (1027.5 examples/sec; 0.125 sec/batch)
2017-05-07 20:48:04.105523: step 41320, loss = 0.72 (962.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:48:05.420091: step 41330, loss = 0.59 (973.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:48:06.767605: step 41340, loss = 0.71 (949.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:48:08.120329: step 41350, loss = 0.69 (946.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:48:09.431681: step 41360, loss = 0.72 (976.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:48:10.769834: step 41370, loss = 0.72 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:48:12.124947: step 41380, loss = 0.66 (944.6 examples/sec; 0.136 sec/batch)
2017-05-07 20:48:13.462154: step 41390, loss = 0.75 (957.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:48:14.892095: step 41400, loss = 0.62 (895.1 examples/sec; 0.143 sec/batch)
2017-05-07 20:48:16.126775: step 41410, loss = 0.70 (1036.7 examples/sec; 0.123 sec/batch)
2017-05-07 20:48:17.456221: step 41420, loss = 0.74 (962.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:48:18.791274: step 41430, loss = 0.67 (958.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:48:20.124804: step 41440, loss = 0.78 (959.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:48:21.470705: step 41450, loss = 0.86 (951.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:48:22.796967: step 41460, loss = 0.76 (965.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:48:24.110247: step 41470, lossE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 848 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
 = 0.75 (974.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:48:25.454743: step 41480, loss = 0.89 (952.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:48:26.794837: step 41490, loss = 0.65 (955.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:48:28.234411: step 41500, loss = 0.69 (889.2 examples/sec; 0.144 sec/batch)
2017-05-07 20:48:29.445969: step 41510, loss = 0.70 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-07 20:48:30.788166: step 41520, loss = 0.77 (953.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:48:32.148064: step 41530, loss = 0.88 (941.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:48:33.484793: step 41540, loss = 0.78 (957.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:48:34.843634: step 41550, loss = 0.86 (942.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:48:36.166830: step 41560, loss = 0.59 (967.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:48:37.504125: step 41570, loss = 0.90 (957.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:48:38.830522: step 41580, loss = 0.76 (965.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:48:40.155899: step 41590, loss = 0.66 (965.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:48:41.579290: step 41600, loss = 0.81 (899.2 examples/sec; 0.142 sec/batch)
2017-05-07 20:48:42.782530: step 41610, loss = 0.74 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-07 20:48:44.125820: step 41620, loss = 0.74 (952.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:48:45.438260: step 41630, loss = 0.79 (975.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:48:46.787620: step 41640, loss = 0.77 (948.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:48:48.125776: step 41650, loss = 0.85 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:48:49.425780: step 41660, loss = 0.66 (984.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:48:50.780597: step 41670, loss = 0.75 (944.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:48:52.120639: step 41680, loss = 0.68 (955.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:48:53.438751: step 41690, loss = 0.75 (971.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:48:54.897811: step 41700, loss = 0.66 (877.3 examples/sec; 0.146 sec/batch)
2017-05-07 20:48:56.153340: step 41710, loss = 0.49 (1019.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:48:57.502770: step 41720, loss = 0.71 (948.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:48:58.834755: step 41730, loss = 0.74 (961.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:49:00.173425: step 41740, loss = 0.70 (956.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:49:01.508617: step 41750, loss = 0.75 (958.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:49:02.826410: step 41760, loss = 0.71 (971.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:49:04.143629: step 41770, loss = 0.67 (971.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:49:05.443265: step 41780, loss = 0.74 (984.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:49:06.794649: step 41790, loss = 0.82 (947.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:49:08.236359: step 41800, loss = 0.74 (887.8 examples/sec; 0.144 sec/batch)
2017-05-07 20:49:09.466351: step 41810, loss = 0.90 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-07 20:49:10.811346: step 41820, loss = 0.76 (951.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:49:12.167086: step 41830, loss = 0.69 (944.1 examples/sec; 0.136 sec/batch)
2017-05-07 20:49:13.505635: step 41840, loss = 0.88 (956.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:49:14.841208: step 41850, loss = 0.91 (958.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:49:16.190020: step 41860, loss = 0.86 (949.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:49:17.518662: step 41870, loss = 0.76 (963.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:49:18.855750: step 41880, loss = 0.66 (957.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:49:20.198276: step 41890, loss = 0.81 (953.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:49:21.672833: step 41900, loss = 0.70 (868.1 examples/sec; 0.147 sec/batch)
2017-05-07 20:49:22.825833: step 41910, loss = 0.75 (1110.1 examples/sec; 0.115 sec/batch)
2017-05-07 20:49:24.167340: step 41920, loss = 0.63 (954.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:49:25.499675: step 41930, loss = 0.96 (960.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:49:26.848313: step 41940, loss = 0.66 (949.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:49:28.222028: step 41950, loss = 0.81 (931.8 examples/sec; 0.137 sec/batch)
2017-05-07 20:49:29.515850: step 41960, loss = 0.83 (989.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:49:30.852168: step 41970, loss = 0.83 (957.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:49:32.198358: step 41980, loss = 0.77 (950.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:49:33.514299: step 41990, loss = 0.71 (972.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:49:34.941339: step 42000, loss = 0.60 (897.0 examples/sec; 0.143 sec/batch)
2017-05-07 20:49:36.172705: step 42010, loss = 0.59 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-07 20:49:37.500444: step 42020, loss = 0.71 (964.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:49:38.842899: step 42030, loss = 0.77 (953.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:49:40.202076: step 42040, loss = 0.76 (941.7 examples/sec; 0.136 sec/batch)
2017-05-07 20:49:41.533343: step 42050, loss = 0.72 (961.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:49:42.837695: step 42060, loss = 0.73 (981.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:49:44.178683: step 42070, loss = 0.68 (954.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:49:45.510194: step 42080, loss = 0.76 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:49:46.856237: step 42090, loss = 0.74 (950.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:49:48.319026: step 42100, loss = 0.74 (875.0 examples/sec; 0.146 sec/batch)
2017-05-07 20:49:49.547427: step 42110, loss = 0.73 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-07 20:49:50.901262: step 42120, loss = 0.65 (945.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:49:52.265032: step 42130, loss = 0.65 (938.6 examples/sec; 0.136 sec/batch)
2017-05-07 20:49:53.600216: step 42140, loss = 0.70 (958.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:49:54.951430: step 42150, loss = 0.92 (947.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:49:56.294794: step 42160, loss = 0.81 (952.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:49:57.630859: step 42170, loss = 0.62 (958.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:49:58.959257: step 42180, loss = 0.74 (963.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:50:00.305202: step 42190, loss = 0.56 (951.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:50:01.748259: step 42200, loss = 0.86 (887.0 examples/sec; 0.144 sec/batch)
2017-05-07 20:50:03.017080: step 42210, loss = 0.64 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:50:04.349382: step 42220, loss = 0.75 (960.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:50:05.660842: step 42230, loss = 0.68 (976.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:50:06.986139: step 42240, loss = 0.62 (965.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:50:08.355224: step 42250, loss = 0.63 (934.9 examples/sec; 0.137 sec/batch)
2017-05-07 20:50:09.680432: step 42260, loss = 0.76 (965.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:50:11.015512: step 42270, loss = 0.59 (958.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:50:12.349493: step 42280, loss = 0.73 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:50:13.683026: step 42290, loss = 0.72 (959.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:50:15.135071: step 42300, loss = 0.65 (881.5 examples/sec; 0.145 sec/batch)
2017-05-07 20:50:16.379514: step 42310, loss = 0.77 (1028.6 examples/sec; 0.124 sec/batch)
2017-05-07 20:50:17.692213: step 42320, loss = 0.86 (975.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:50:19.028885: step 42330, loss = 0.86 (957.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:50:20.353012: step 42340, loss = 0.59 (966.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:50:21.718567: step 42350, loss = 0.62 (937.3 examples/sec; 0.137 sec/batch)
2017-05-07 20:50:23.025333: step 42360, loss = 0.88 (979.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:50:24.367188: step 42370, loss = 0.86 (953.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:50:25.679971: step 4238E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 866 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
0, loss = 0.83 (975.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:50:27.028514: step 42390, loss = 0.71 (949.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:50:28.455867: step 42400, loss = 0.69 (896.8 examples/sec; 0.143 sec/batch)
2017-05-07 20:50:29.713217: step 42410, loss = 0.85 (1018.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:50:31.054718: step 42420, loss = 0.66 (954.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:50:32.365405: step 42430, loss = 0.82 (976.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:50:33.683666: step 42440, loss = 0.70 (971.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:50:35.041107: step 42450, loss = 0.74 (943.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:50:36.394655: step 42460, loss = 0.85 (945.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:50:37.733954: step 42470, loss = 0.64 (955.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:50:39.083351: step 42480, loss = 0.66 (948.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:50:40.425121: step 42490, loss = 0.67 (954.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:50:41.851825: step 42500, loss = 0.76 (897.2 examples/sec; 0.143 sec/batch)
2017-05-07 20:50:43.093906: step 42510, loss = 0.74 (1030.5 examples/sec; 0.124 sec/batch)
2017-05-07 20:50:44.417756: step 42520, loss = 0.79 (966.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:50:45.764672: step 42530, loss = 0.71 (950.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:50:47.096965: step 42540, loss = 0.66 (960.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:50:48.448814: step 42550, loss = 0.64 (946.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:50:49.779569: step 42560, loss = 0.69 (961.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:50:51.100251: step 42570, loss = 0.83 (969.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:50:52.439022: step 42580, loss = 0.67 (956.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:50:53.778771: step 42590, loss = 0.82 (955.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:50:55.216341: step 42600, loss = 0.67 (890.4 examples/sec; 0.144 sec/batch)
2017-05-07 20:50:56.469244: step 42610, loss = 0.81 (1021.6 examples/sec; 0.125 sec/batch)
2017-05-07 20:50:57.797882: step 42620, loss = 0.79 (963.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:50:59.132749: step 42630, loss = 0.76 (958.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:51:00.471283: step 42640, loss = 0.71 (956.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:51:01.816232: step 42650, loss = 0.63 (951.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:51:03.142993: step 42660, loss = 0.62 (964.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:51:04.460956: step 42670, loss = 0.70 (971.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:51:05.812621: step 42680, loss = 0.83 (947.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:51:07.144210: step 42690, loss = 0.82 (961.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:51:08.579480: step 42700, loss = 0.96 (891.8 examples/sec; 0.144 sec/batch)
2017-05-07 20:51:09.814292: step 42710, loss = 0.71 (1036.6 examples/sec; 0.123 sec/batch)
2017-05-07 20:51:11.148005: step 42720, loss = 0.77 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:51:12.508810: step 42730, loss = 0.88 (940.6 examples/sec; 0.136 sec/batch)
2017-05-07 20:51:13.844495: step 42740, loss = 0.76 (958.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:51:15.196246: step 42750, loss = 0.94 (946.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:51:16.508742: step 42760, loss = 0.87 (975.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:51:17.835816: step 42770, loss = 0.75 (964.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:51:19.176821: step 42780, loss = 0.72 (954.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:51:20.508524: step 42790, loss = 0.62 (961.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:51:21.937685: step 42800, loss = 0.59 (895.6 examples/sec; 0.143 sec/batch)
2017-05-07 20:51:23.212144: step 42810, loss = 0.74 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:51:24.556216: step 42820, loss = 0.70 (952.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:51:25.880532: step 42830, loss = 0.68 (966.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:51:27.211356: step 42840, loss = 0.78 (961.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:51:28.567900: step 42850, loss = 0.69 (943.6 examples/sec; 0.136 sec/batch)
2017-05-07 20:51:29.859640: step 42860, loss = 0.80 (990.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:51:31.183576: step 42870, loss = 0.75 (966.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:51:32.493601: step 42880, loss = 0.66 (977.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:51:33.815827: step 42890, loss = 0.68 (968.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:51:35.267364: step 42900, loss = 0.81 (881.8 examples/sec; 0.145 sec/batch)
2017-05-07 20:51:36.506663: step 42910, loss = 0.68 (1032.8 examples/sec; 0.124 sec/batch)
2017-05-07 20:51:37.842739: step 42920, loss = 0.99 (958.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:51:39.190205: step 42930, loss = 0.65 (949.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:51:40.532282: step 42940, loss = 0.74 (953.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:51:41.890833: step 42950, loss = 0.83 (942.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:51:43.206515: step 42960, loss = 0.70 (972.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:51:44.515736: step 42970, loss = 0.79 (977.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:51:45.822012: step 42980, loss = 0.68 (979.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:51:47.181557: step 42990, loss = 0.66 (941.5 examples/sec; 0.136 sec/batch)
2017-05-07 20:51:48.589577: step 43000, loss = 0.81 (909.1 examples/sec; 0.141 sec/batch)
2017-05-07 20:51:49.834008: step 43010, loss = 0.68 (1028.6 examples/sec; 0.124 sec/batch)
2017-05-07 20:51:51.179844: step 43020, loss = 0.62 (951.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:51:52.520507: step 43030, loss = 0.94 (954.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:51:53.831003: step 43040, loss = 0.73 (976.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:51:55.187070: step 43050, loss = 0.58 (943.9 examples/sec; 0.136 sec/batch)
2017-05-07 20:51:56.511168: step 43060, loss = 0.79 (966.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:51:57.830623: step 43070, loss = 0.84 (970.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:51:59.167941: step 43080, loss = 0.78 (957.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:52:00.494026: step 43090, loss = 0.68 (965.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:52:01.924438: step 43100, loss = 0.68 (894.8 examples/sec; 0.143 sec/batch)
2017-05-07 20:52:03.173492: step 43110, loss = 0.73 (1024.8 examples/sec; 0.125 sec/batch)
2017-05-07 20:52:04.499374: step 43120, loss = 0.68 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:52:05.818582: step 43130, loss = 0.80 (970.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:52:07.170148: step 43140, loss = 0.84 (947.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:52:08.522601: step 43150, loss = 0.73 (946.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:52:09.842957: step 43160, loss = 0.88 (969.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:52:11.169782: step 43170, loss = 1.04 (964.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:52:12.489341: step 43180, loss = 0.54 (970.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:52:13.792747: step 43190, loss = 0.84 (982.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:52:15.218263: step 43200, loss = 0.77 (897.9 examples/sec; 0.143 sec/batch)
2017-05-07 20:52:16.481630: step 43210, loss = 0.69 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:52:17.810683: step 43220, loss = 0.83 (963.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:52:19.127142: step 43230, loss = 0.88 (972.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:52:20.437980: step 43240, loss = 0.77 (976.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:52:21.789149: step 43250, loss = 0.65 (947.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:52:23.110552: step 43260, loss = 0.74 (968.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:52:24.424232: step 43270, loss = 0.65 (974.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:52:25.752022: step 43280, loss = 0.67 (964.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:52:27.081145: stE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 884 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
ep 43290, loss = 0.60 (963.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:52:28.557396: step 43300, loss = 0.86 (867.1 examples/sec; 0.148 sec/batch)
2017-05-07 20:52:29.777326: step 43310, loss = 0.78 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-07 20:52:31.131420: step 43320, loss = 0.65 (945.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:52:32.489411: step 43330, loss = 0.69 (942.6 examples/sec; 0.136 sec/batch)
2017-05-07 20:52:33.802831: step 43340, loss = 0.80 (974.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:52:35.181545: step 43350, loss = 0.74 (928.4 examples/sec; 0.138 sec/batch)
2017-05-07 20:52:36.501737: step 43360, loss = 0.65 (969.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:52:37.846216: step 43370, loss = 0.84 (952.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:52:39.182368: step 43380, loss = 0.72 (958.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:52:40.511014: step 43390, loss = 0.81 (963.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:52:41.960368: step 43400, loss = 0.77 (883.2 examples/sec; 0.145 sec/batch)
2017-05-07 20:52:43.228378: step 43410, loss = 0.66 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:52:44.562659: step 43420, loss = 0.67 (959.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:52:45.913110: step 43430, loss = 0.58 (947.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:52:47.252737: step 43440, loss = 0.70 (955.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:52:48.628194: step 43450, loss = 0.71 (930.6 examples/sec; 0.138 sec/batch)
2017-05-07 20:52:49.933333: step 43460, loss = 0.74 (980.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:52:51.276751: step 43470, loss = 0.80 (952.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:52:52.619422: step 43480, loss = 0.77 (953.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:52:53.973893: step 43490, loss = 0.70 (945.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:52:55.441618: step 43500, loss = 0.75 (872.1 examples/sec; 0.147 sec/batch)
2017-05-07 20:52:56.679347: step 43510, loss = 0.62 (1034.2 examples/sec; 0.124 sec/batch)
2017-05-07 20:52:58.016614: step 43520, loss = 0.65 (957.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:52:59.340649: step 43530, loss = 0.74 (966.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:53:00.667997: step 43540, loss = 0.57 (964.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:53:02.010025: step 43550, loss = 0.77 (953.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:53:03.335874: step 43560, loss = 0.84 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:53:04.677646: step 43570, loss = 0.72 (954.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:53:06.007801: step 43580, loss = 0.74 (962.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:53:07.354107: step 43590, loss = 0.63 (950.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:53:08.783399: step 43600, loss = 0.60 (895.5 examples/sec; 0.143 sec/batch)
2017-05-07 20:53:10.003946: step 43610, loss = 0.65 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-07 20:53:11.348211: step 43620, loss = 0.89 (952.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:53:12.680442: step 43630, loss = 0.76 (960.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:53:14.021788: step 43640, loss = 0.78 (954.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:53:15.368611: step 43650, loss = 0.67 (950.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:53:16.686827: step 43660, loss = 0.61 (971.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:53:18.023125: step 43670, loss = 0.76 (957.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:53:19.348798: step 43680, loss = 0.81 (965.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:53:20.672995: step 43690, loss = 0.81 (966.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:53:22.168351: step 43700, loss = 0.56 (856.0 examples/sec; 0.150 sec/batch)
2017-05-07 20:53:23.355925: step 43710, loss = 0.70 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-07 20:53:24.695683: step 43720, loss = 0.81 (955.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:53:26.037664: step 43730, loss = 0.75 (953.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:53:27.371298: step 43740, loss = 0.74 (959.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:53:28.718671: step 43750, loss = 0.81 (950.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:53:30.038164: step 43760, loss = 0.75 (970.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:53:31.357939: step 43770, loss = 0.78 (969.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:53:32.683341: step 43780, loss = 0.72 (965.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:53:34.010719: step 43790, loss = 0.71 (964.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:53:35.449396: step 43800, loss = 0.74 (889.7 examples/sec; 0.144 sec/batch)
2017-05-07 20:53:36.681844: step 43810, loss = 0.67 (1038.6 examples/sec; 0.123 sec/batch)
2017-05-07 20:53:38.003291: step 43820, loss = 0.84 (968.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:53:39.338283: step 43830, loss = 0.66 (958.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:53:40.646656: step 43840, loss = 0.64 (978.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:53:42.022505: step 43850, loss = 1.07 (930.4 examples/sec; 0.138 sec/batch)
2017-05-07 20:53:43.358830: step 43860, loss = 0.74 (957.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:53:44.681624: step 43870, loss = 0.67 (967.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:53:46.033708: step 43880, loss = 0.68 (946.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:53:47.390326: step 43890, loss = 0.76 (943.5 examples/sec; 0.136 sec/batch)
2017-05-07 20:53:48.837409: step 43900, loss = 0.65 (884.5 examples/sec; 0.145 sec/batch)
2017-05-07 20:53:50.060306: step 43910, loss = 0.75 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-07 20:53:51.400105: step 43920, loss = 0.75 (955.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:53:52.731032: step 43930, loss = 0.76 (961.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:53:54.045983: step 43940, loss = 0.84 (973.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:53:55.389498: step 43950, loss = 0.70 (952.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:53:56.718171: step 43960, loss = 0.67 (963.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:53:58.038929: step 43970, loss = 0.96 (969.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:53:59.365873: step 43980, loss = 0.81 (964.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:54:00.705064: step 43990, loss = 0.59 (955.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:54:02.129943: step 44000, loss = 0.79 (898.3 examples/sec; 0.142 sec/batch)
2017-05-07 20:54:03.383628: step 44010, loss = 0.72 (1021.0 examples/sec; 0.125 sec/batch)
2017-05-07 20:54:04.716165: step 44020, loss = 0.77 (960.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:54:06.064017: step 44030, loss = 0.67 (949.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:54:07.381650: step 44040, loss = 0.89 (971.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:54:08.730096: step 44050, loss = 0.82 (949.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:54:10.056362: step 44060, loss = 0.83 (965.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:54:11.378146: step 44070, loss = 0.87 (968.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:54:12.703605: step 44080, loss = 0.74 (965.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:54:14.033837: step 44090, loss = 0.60 (962.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:54:15.469388: step 44100, loss = 0.69 (891.6 examples/sec; 0.144 sec/batch)
2017-05-07 20:54:16.712733: step 44110, loss = 0.71 (1029.5 examples/sec; 0.124 sec/batch)
2017-05-07 20:54:18.049276: step 44120, loss = 0.64 (957.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:54:19.403072: step 44130, loss = 0.67 (945.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:54:20.745225: step 44140, loss = 0.71 (953.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:54:22.097812: step 44150, loss = 0.68 (946.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:54:23.409513: step 44160, loss = 0.61 (975.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:54:24.736076: step 44170, loss = 0.79 (964.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:54:26.051355: step 44180, loss = 0.74 (973.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:54:27.378724: step 44190, loss = 0.91 (964.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:54:28.820E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 902 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
347: step 44200, loss = 0.71 (887.9 examples/sec; 0.144 sec/batch)
2017-05-07 20:54:30.056697: step 44210, loss = 0.72 (1035.3 examples/sec; 0.124 sec/batch)
2017-05-07 20:54:31.392808: step 44220, loss = 0.78 (958.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:54:32.709091: step 44230, loss = 0.72 (972.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:54:34.041982: step 44240, loss = 0.74 (960.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:54:35.400804: step 44250, loss = 0.62 (942.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:54:36.731536: step 44260, loss = 0.77 (961.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:54:38.074308: step 44270, loss = 0.69 (953.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:54:39.422799: step 44280, loss = 0.74 (949.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:54:40.767055: step 44290, loss = 0.85 (952.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:54:42.206055: step 44300, loss = 0.84 (889.5 examples/sec; 0.144 sec/batch)
2017-05-07 20:54:43.424438: step 44310, loss = 0.68 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-07 20:54:44.730542: step 44320, loss = 0.76 (980.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:54:46.065139: step 44330, loss = 0.73 (959.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:54:47.394054: step 44340, loss = 0.77 (963.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:54:48.747190: step 44350, loss = 0.83 (946.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:54:50.039036: step 44360, loss = 0.79 (990.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:54:51.393837: step 44370, loss = 0.77 (944.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:54:52.716541: step 44380, loss = 0.73 (967.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:54:54.046139: step 44390, loss = 0.67 (962.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:54:55.481598: step 44400, loss = 0.75 (891.7 examples/sec; 0.144 sec/batch)
2017-05-07 20:54:56.728692: step 44410, loss = 0.71 (1026.4 examples/sec; 0.125 sec/batch)
2017-05-07 20:54:58.046959: step 44420, loss = 0.64 (971.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:54:59.400670: step 44430, loss = 0.69 (945.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:55:00.729707: step 44440, loss = 0.71 (963.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:55:02.100933: step 44450, loss = 0.67 (933.5 examples/sec; 0.137 sec/batch)
2017-05-07 20:55:03.452884: step 44460, loss = 0.81 (946.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:55:04.786904: step 44470, loss = 0.83 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:55:06.113912: step 44480, loss = 0.68 (964.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:55:07.450403: step 44490, loss = 0.76 (957.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:55:08.880533: step 44500, loss = 0.81 (895.0 examples/sec; 0.143 sec/batch)
2017-05-07 20:55:10.096717: step 44510, loss = 0.77 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-07 20:55:11.451516: step 44520, loss = 0.74 (944.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:55:12.768104: step 44530, loss = 0.76 (972.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:55:14.096609: step 44540, loss = 0.91 (963.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:55:15.460988: step 44550, loss = 0.75 (938.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:55:16.787099: step 44560, loss = 0.64 (965.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:55:18.130879: step 44570, loss = 0.66 (952.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:55:19.472753: step 44580, loss = 0.63 (953.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:55:20.799676: step 44590, loss = 0.78 (964.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:55:22.229585: step 44600, loss = 0.82 (895.2 examples/sec; 0.143 sec/batch)
2017-05-07 20:55:23.456986: step 44610, loss = 0.77 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-07 20:55:24.785262: step 44620, loss = 0.58 (963.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:55:26.105917: step 44630, loss = 0.74 (969.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:55:27.452940: step 44640, loss = 0.72 (950.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:55:28.849063: step 44650, loss = 0.74 (916.8 examples/sec; 0.140 sec/batch)
2017-05-07 20:55:30.170226: step 44660, loss = 0.71 (968.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:55:31.517049: step 44670, loss = 0.69 (950.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:55:32.845742: step 44680, loss = 0.74 (963.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:55:34.201285: step 44690, loss = 0.69 (944.3 examples/sec; 0.136 sec/batch)
2017-05-07 20:55:35.647171: step 44700, loss = 0.90 (885.3 examples/sec; 0.145 sec/batch)
2017-05-07 20:55:36.851910: step 44710, loss = 0.65 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-07 20:55:38.201874: step 44720, loss = 0.72 (948.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:55:39.560705: step 44730, loss = 0.85 (942.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:55:40.876054: step 44740, loss = 0.67 (973.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:55:42.214159: step 44750, loss = 0.74 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:55:43.547991: step 44760, loss = 0.65 (959.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:55:44.867973: step 44770, loss = 0.54 (969.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:55:46.218121: step 44780, loss = 0.72 (948.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:55:47.540365: step 44790, loss = 0.82 (968.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:55:48.971336: step 44800, loss = 0.76 (894.5 examples/sec; 0.143 sec/batch)
2017-05-07 20:55:50.222561: step 44810, loss = 0.83 (1023.0 examples/sec; 0.125 sec/batch)
2017-05-07 20:55:51.543271: step 44820, loss = 0.83 (969.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:55:52.863783: step 44830, loss = 0.68 (969.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:55:54.211358: step 44840, loss = 0.75 (949.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:55:55.595065: step 44850, loss = 0.60 (925.1 examples/sec; 0.138 sec/batch)
2017-05-07 20:55:56.923606: step 44860, loss = 0.62 (963.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:55:58.278937: step 44870, loss = 0.68 (944.4 examples/sec; 0.136 sec/batch)
2017-05-07 20:55:59.609267: step 44880, loss = 0.89 (962.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:56:00.929894: step 44890, loss = 0.74 (969.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:56:02.359262: step 44900, loss = 0.67 (895.5 examples/sec; 0.143 sec/batch)
2017-05-07 20:56:03.621190: step 44910, loss = 0.66 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:56:04.966525: step 44920, loss = 0.62 (951.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:56:06.304604: step 44930, loss = 0.68 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:56:07.663088: step 44940, loss = 0.76 (942.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:56:09.007952: step 44950, loss = 0.94 (951.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:56:10.356595: step 44960, loss = 0.71 (949.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:56:11.689372: step 44970, loss = 0.83 (960.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:56:13.012438: step 44980, loss = 0.80 (967.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:56:14.331239: step 44990, loss = 0.84 (970.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:56:15.808262: step 45000, loss = 0.48 (866.6 examples/sec; 0.148 sec/batch)
2017-05-07 20:56:17.009349: step 45010, loss = 0.62 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-07 20:56:18.351557: step 45020, loss = 0.71 (953.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:56:19.671116: step 45030, loss = 0.65 (970.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:56:20.987069: step 45040, loss = 0.64 (972.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:56:22.335599: step 45050, loss = 0.79 (949.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:56:23.672649: step 45060, loss = 0.66 (957.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:56:25.003738: step 45070, loss = 0.75 (961.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:56:26.318571: step 45080, loss = 0.81 (973.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:56:27.667161: step 45090, loss = 0.66 (949.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:56:29.098306: step 45100, loss = 0.74 (894.4 examples/sec; 0.143 sec/batch)
2017-05-07 20:56E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 921 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
:30.312294: step 45110, loss = 0.73 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-07 20:56:31.630261: step 45120, loss = 0.74 (971.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:56:32.963555: step 45130, loss = 0.64 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:56:34.317433: step 45140, loss = 0.71 (945.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:56:35.693962: step 45150, loss = 0.69 (929.9 examples/sec; 0.138 sec/batch)
2017-05-07 20:56:36.995316: step 45160, loss = 0.70 (983.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:56:38.323244: step 45170, loss = 0.84 (963.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:56:39.660242: step 45180, loss = 0.66 (957.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:56:40.995967: step 45190, loss = 0.65 (958.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:56:42.422660: step 45200, loss = 0.78 (897.2 examples/sec; 0.143 sec/batch)
2017-05-07 20:56:43.688620: step 45210, loss = 0.76 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:56:45.043748: step 45220, loss = 0.77 (944.6 examples/sec; 0.136 sec/batch)
2017-05-07 20:56:46.366150: step 45230, loss = 0.99 (967.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:56:47.719330: step 45240, loss = 0.68 (945.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:56:49.042846: step 45250, loss = 0.75 (967.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:56:50.382920: step 45260, loss = 0.57 (955.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:56:51.747540: step 45270, loss = 0.60 (938.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:56:53.064795: step 45280, loss = 0.70 (971.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:56:54.405090: step 45290, loss = 0.70 (955.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:56:55.852801: step 45300, loss = 0.63 (884.2 examples/sec; 0.145 sec/batch)
2017-05-07 20:56:57.062369: step 45310, loss = 0.73 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-07 20:56:58.393996: step 45320, loss = 0.64 (961.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:56:59.716506: step 45330, loss = 0.68 (967.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:57:01.038232: step 45340, loss = 0.71 (968.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:57:02.379841: step 45350, loss = 0.55 (954.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:57:03.711289: step 45360, loss = 0.72 (961.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:57:05.042164: step 45370, loss = 0.70 (961.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:57:06.376281: step 45380, loss = 0.77 (959.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:57:07.726788: step 45390, loss = 0.67 (947.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:57:09.145870: step 45400, loss = 0.88 (902.0 examples/sec; 0.142 sec/batch)
2017-05-07 20:57:10.412365: step 45410, loss = 0.80 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:57:11.727606: step 45420, loss = 0.69 (973.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:57:13.026635: step 45430, loss = 0.86 (985.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:57:14.356759: step 45440, loss = 0.68 (962.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:57:15.713660: step 45450, loss = 0.73 (943.3 examples/sec; 0.136 sec/batch)
2017-05-07 20:57:17.026909: step 45460, loss = 0.68 (974.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:57:18.357650: step 45470, loss = 0.76 (961.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:57:19.699648: step 45480, loss = 0.75 (953.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:57:21.034266: step 45490, loss = 0.76 (959.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:57:22.468834: step 45500, loss = 0.77 (892.3 examples/sec; 0.143 sec/batch)
2017-05-07 20:57:23.706876: step 45510, loss = 0.66 (1033.9 examples/sec; 0.124 sec/batch)
2017-05-07 20:57:25.048044: step 45520, loss = 0.66 (954.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:57:26.370087: step 45530, loss = 0.64 (968.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:57:27.730512: step 45540, loss = 0.87 (940.9 examples/sec; 0.136 sec/batch)
2017-05-07 20:57:29.091844: step 45550, loss = 0.91 (940.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:57:30.394992: step 45560, loss = 0.67 (982.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:57:31.754152: step 45570, loss = 0.71 (941.8 examples/sec; 0.136 sec/batch)
2017-05-07 20:57:33.083835: step 45580, loss = 0.85 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:57:34.437560: step 45590, loss = 0.75 (945.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:57:35.884725: step 45600, loss = 0.75 (884.5 examples/sec; 0.145 sec/batch)
2017-05-07 20:57:37.098744: step 45610, loss = 0.66 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-07 20:57:38.425892: step 45620, loss = 0.78 (964.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:57:39.758616: step 45630, loss = 0.87 (960.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:57:41.092373: step 45640, loss = 0.64 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:57:42.452696: step 45650, loss = 0.66 (940.9 examples/sec; 0.136 sec/batch)
2017-05-07 20:57:43.777434: step 45660, loss = 0.63 (966.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:57:45.091176: step 45670, loss = 0.84 (974.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:57:46.422379: step 45680, loss = 0.90 (961.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:57:47.744109: step 45690, loss = 0.85 (968.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:57:49.158997: step 45700, loss = 0.63 (904.7 examples/sec; 0.141 sec/batch)
2017-05-07 20:57:50.408026: step 45710, loss = 0.81 (1024.8 examples/sec; 0.125 sec/batch)
2017-05-07 20:57:51.721849: step 45720, loss = 0.90 (974.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:57:53.064691: step 45730, loss = 0.71 (953.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:57:54.396175: step 45740, loss = 0.72 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:57:55.754630: step 45750, loss = 0.62 (942.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:57:57.071513: step 45760, loss = 0.89 (972.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:57:58.401806: step 45770, loss = 0.98 (962.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:57:59.740678: step 45780, loss = 0.82 (956.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:58:01.089894: step 45790, loss = 0.68 (948.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:58:02.567613: step 45800, loss = 0.68 (866.2 examples/sec; 0.148 sec/batch)
2017-05-07 20:58:03.809020: step 45810, loss = 0.66 (1031.1 examples/sec; 0.124 sec/batch)
2017-05-07 20:58:05.141147: step 45820, loss = 0.62 (960.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:58:06.464043: step 45830, loss = 0.80 (967.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:58:07.803046: step 45840, loss = 0.58 (955.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:58:09.147219: step 45850, loss = 0.79 (952.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:58:10.481678: step 45860, loss = 0.70 (959.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:58:11.815072: step 45870, loss = 0.69 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:58:13.156317: step 45880, loss = 0.68 (954.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:58:14.474986: step 45890, loss = 0.61 (970.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:58:15.911353: step 45900, loss = 0.70 (891.1 examples/sec; 0.144 sec/batch)
2017-05-07 20:58:17.156226: step 45910, loss = 0.67 (1028.2 examples/sec; 0.124 sec/batch)
2017-05-07 20:58:18.509440: step 45920, loss = 0.68 (945.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:58:19.841382: step 45930, loss = 0.69 (961.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:58:21.175727: step 45940, loss = 0.68 (959.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:58:22.512159: step 45950, loss = 0.70 (957.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:58:23.838583: step 45960, loss = 0.67 (965.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:58:25.167986: step 45970, loss = 0.63 (962.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:58:26.477336: step 45980, loss = 0.68 (977.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:58:27.844195: step 45990, loss = 0.76 (936.5 examples/sec; 0.137 sec/batch)
2017-05-07 20:58:29.284125: step 46000, loss = 0.84 (888.9 examples/sec; 0.144 sec/batch)
2017-05-07 20:58:30.512487: step 46010, loss = 0.76 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 939 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
07 20:58:31.860944: step 46020, loss = 0.79 (949.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:58:33.218619: step 46030, loss = 0.80 (942.8 examples/sec; 0.136 sec/batch)
2017-05-07 20:58:34.561014: step 46040, loss = 0.79 (953.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:58:35.924785: step 46050, loss = 0.88 (938.6 examples/sec; 0.136 sec/batch)
2017-05-07 20:58:37.236891: step 46060, loss = 0.69 (975.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:58:38.579785: step 46070, loss = 0.87 (953.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:58:39.908165: step 46080, loss = 0.70 (963.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:58:41.247124: step 46090, loss = 0.77 (956.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:58:42.666193: step 46100, loss = 0.92 (902.0 examples/sec; 0.142 sec/batch)
2017-05-07 20:58:43.930419: step 46110, loss = 0.69 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:58:45.263117: step 46120, loss = 0.62 (960.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:58:46.619817: step 46130, loss = 0.86 (943.5 examples/sec; 0.136 sec/batch)
2017-05-07 20:58:47.952848: step 46140, loss = 0.74 (960.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:58:49.298957: step 46150, loss = 0.61 (950.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:58:50.630863: step 46160, loss = 0.69 (961.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:58:51.964192: step 46170, loss = 0.75 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:58:53.282651: step 46180, loss = 0.82 (970.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:58:54.629240: step 46190, loss = 0.69 (950.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:58:56.047755: step 46200, loss = 0.70 (902.4 examples/sec; 0.142 sec/batch)
2017-05-07 20:58:57.267988: step 46210, loss = 0.63 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-07 20:58:58.584297: step 46220, loss = 0.82 (972.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:58:59.912799: step 46230, loss = 0.92 (963.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:59:01.231067: step 46240, loss = 0.65 (971.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:59:02.581277: step 46250, loss = 0.64 (948.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:59:03.893146: step 46260, loss = 0.61 (975.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:59:05.238406: step 46270, loss = 0.73 (951.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:59:06.585437: step 46280, loss = 0.73 (950.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:59:07.918392: step 46290, loss = 0.82 (960.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:59:09.386593: step 46300, loss = 0.68 (871.8 examples/sec; 0.147 sec/batch)
2017-05-07 20:59:10.600852: step 46310, loss = 0.75 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-07 20:59:11.932969: step 46320, loss = 0.73 (960.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:59:13.273976: step 46330, loss = 0.67 (954.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:59:14.634722: step 46340, loss = 0.82 (940.7 examples/sec; 0.136 sec/batch)
2017-05-07 20:59:15.989569: step 46350, loss = 0.65 (944.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:59:17.303417: step 46360, loss = 1.02 (974.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:59:18.632709: step 46370, loss = 0.71 (962.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:59:19.968686: step 46380, loss = 0.65 (958.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:59:21.306257: step 46390, loss = 0.82 (956.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:59:22.762634: step 46400, loss = 0.64 (878.9 examples/sec; 0.146 sec/batch)
2017-05-07 20:59:23.975750: step 46410, loss = 0.61 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-07 20:59:25.288244: step 46420, loss = 0.61 (975.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:59:26.638978: step 46430, loss = 0.80 (947.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:59:27.981152: step 46440, loss = 0.86 (953.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:59:29.318876: step 46450, loss = 0.83 (956.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:59:30.641819: step 46460, loss = 0.66 (967.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:59:31.978027: step 46470, loss = 0.71 (957.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:59:33.336426: step 46480, loss = 0.70 (942.3 examples/sec; 0.136 sec/batch)
2017-05-07 20:59:34.680974: step 46490, loss = 0.75 (952.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:59:36.116795: step 46500, loss = 0.66 (891.5 examples/sec; 0.144 sec/batch)
2017-05-07 20:59:37.339250: step 46510, loss = 0.63 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-07 20:59:38.653565: step 46520, loss = 0.75 (973.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:59:39.987810: step 46530, loss = 0.77 (959.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:59:41.321975: step 46540, loss = 0.67 (959.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:59:42.675277: step 46550, loss = 0.60 (945.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:59:43.986017: step 46560, loss = 0.66 (976.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:59:45.336626: step 46570, loss = 0.75 (947.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:59:46.652986: step 46580, loss = 0.67 (972.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:59:47.970733: step 46590, loss = 0.73 (971.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:59:49.408509: step 46600, loss = 0.65 (890.3 examples/sec; 0.144 sec/batch)
2017-05-07 20:59:50.675166: step 46610, loss = 0.78 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:59:51.986953: step 46620, loss = 0.69 (975.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:59:53.315790: step 46630, loss = 0.75 (963.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:59:54.620297: step 46640, loss = 0.75 (981.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:59:55.999125: step 46650, loss = 0.82 (928.3 examples/sec; 0.138 sec/batch)
2017-05-07 20:59:57.339648: step 46660, loss = 0.80 (954.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:59:58.664329: step 46670, loss = 0.60 (966.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:59:59.987517: step 46680, loss = 0.66 (967.4 examples/sec; 0.132 sec/batch)
2017-05-07 21:00:01.303082: step 46690, loss = 0.47 (973.0 examples/sec; 0.132 sec/batch)
2017-05-07 21:00:02.733169: step 46700, loss = 0.64 (895.1 examples/sec; 0.143 sec/batch)
2017-05-07 21:00:03.974252: step 46710, loss = 0.78 (1031.4 examples/sec; 0.124 sec/batch)
2017-05-07 21:00:05.302446: step 46720, loss = 0.68 (963.7 examples/sec; 0.133 sec/batch)
2017-05-07 21:00:06.641993: step 46730, loss = 0.73 (955.6 examples/sec; 0.134 sec/batch)
2017-05-07 21:00:07.991300: step 46740, loss = 0.91 (948.6 examples/sec; 0.135 sec/batch)
2017-05-07 21:00:09.329851: step 46750, loss = 0.80 (956.3 examples/sec; 0.134 sec/batch)
2017-05-07 21:00:10.645670: step 46760, loss = 0.66 (972.8 examples/sec; 0.132 sec/batch)
2017-05-07 21:00:11.996417: step 46770, loss = 0.75 (947.6 examples/sec; 0.135 sec/batch)
2017-05-07 21:00:13.318670: step 46780, loss = 0.76 (968.0 examples/sec; 0.132 sec/batch)
2017-05-07 21:00:14.641859: step 46790, loss = 0.70 (967.4 examples/sec; 0.132 sec/batch)
2017-05-07 21:00:16.090763: step 46800, loss = 0.72 (883.4 examples/sec; 0.145 sec/batch)
2017-05-07 21:00:17.316939: step 46810, loss = 0.71 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-07 21:00:18.657237: step 46820, loss = 0.93 (955.0 examples/sec; 0.134 sec/batch)
2017-05-07 21:00:19.988994: step 46830, loss = 0.71 (961.1 examples/sec; 0.133 sec/batch)
2017-05-07 21:00:21.358877: step 46840, loss = 0.78 (934.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:00:22.724312: step 46850, loss = 0.63 (937.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:00:24.044441: step 46860, loss = 0.53 (969.6 examples/sec; 0.132 sec/batch)
2017-05-07 21:00:25.364757: step 46870, loss = 0.74 (969.5 examples/sec; 0.132 sec/batch)
2017-05-07 21:00:26.694431: step 46880, loss = 0.86 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 21:00:28.035895: step 46890, loss = 0.59 (954.2 examples/sec; 0.134 sec/batch)
2017-05-07 21:00:29.481956: step 46900, loss = 0.55 (885.2 examples/sec; 0.145 sec/batch)
2017-05-07 21:00:30.725131: step 46910, loss = 0.61 (1029.6 examples/sec; 0.124 sec/batch)
2017-05-07 21:00:32.082746: step 46920, loss = 0.77 (942.8 examples/sec; 0.136 sec/batch)
2E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 957 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
017-05-07 21:00:33.402028: step 46930, loss = 0.74 (970.2 examples/sec; 0.132 sec/batch)
2017-05-07 21:00:34.726633: step 46940, loss = 0.77 (966.3 examples/sec; 0.132 sec/batch)
2017-05-07 21:00:36.073996: step 46950, loss = 0.72 (950.0 examples/sec; 0.135 sec/batch)
2017-05-07 21:00:37.393040: step 46960, loss = 0.83 (970.4 examples/sec; 0.132 sec/batch)
2017-05-07 21:00:38.725739: step 46970, loss = 0.84 (960.5 examples/sec; 0.133 sec/batch)
2017-05-07 21:00:40.062566: step 46980, loss = 0.78 (957.5 examples/sec; 0.134 sec/batch)
2017-05-07 21:00:41.385045: step 46990, loss = 0.66 (967.9 examples/sec; 0.132 sec/batch)
2017-05-07 21:00:42.835178: step 47000, loss = 0.77 (882.7 examples/sec; 0.145 sec/batch)
2017-05-07 21:00:44.049836: step 47010, loss = 0.73 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-07 21:00:45.356572: step 47020, loss = 0.72 (979.5 examples/sec; 0.131 sec/batch)
2017-05-07 21:00:46.677388: step 47030, loss = 0.89 (969.1 examples/sec; 0.132 sec/batch)
2017-05-07 21:00:47.987705: step 47040, loss = 0.82 (976.9 examples/sec; 0.131 sec/batch)
2017-05-07 21:00:49.360580: step 47050, loss = 0.72 (932.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:00:50.666828: step 47060, loss = 0.72 (979.9 examples/sec; 0.131 sec/batch)
2017-05-07 21:00:52.004793: step 47070, loss = 0.67 (956.7 examples/sec; 0.134 sec/batch)
2017-05-07 21:00:53.344321: step 47080, loss = 0.69 (955.6 examples/sec; 0.134 sec/batch)
2017-05-07 21:00:54.698535: step 47090, loss = 0.70 (945.2 examples/sec; 0.135 sec/batch)
2017-05-07 21:00:56.131523: step 47100, loss = 0.67 (893.2 examples/sec; 0.143 sec/batch)
2017-05-07 21:00:57.364875: step 47110, loss = 0.78 (1037.8 examples/sec; 0.123 sec/batch)
2017-05-07 21:00:58.702821: step 47120, loss = 0.66 (956.7 examples/sec; 0.134 sec/batch)
2017-05-07 21:01:00.041566: step 47130, loss = 0.94 (956.1 examples/sec; 0.134 sec/batch)
2017-05-07 21:01:01.350588: step 47140, loss = 0.82 (977.8 examples/sec; 0.131 sec/batch)
2017-05-07 21:01:02.714143: step 47150, loss = 0.81 (938.7 examples/sec; 0.136 sec/batch)
2017-05-07 21:01:04.047026: step 47160, loss = 0.71 (960.3 examples/sec; 0.133 sec/batch)
2017-05-07 21:01:05.388682: step 47170, loss = 0.77 (954.0 examples/sec; 0.134 sec/batch)
2017-05-07 21:01:06.746071: step 47180, loss = 0.70 (943.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:01:08.084548: step 47190, loss = 0.67 (956.3 examples/sec; 0.134 sec/batch)
2017-05-07 21:01:09.508691: step 47200, loss = 0.69 (898.8 examples/sec; 0.142 sec/batch)
2017-05-07 21:01:10.757972: step 47210, loss = 0.74 (1024.6 examples/sec; 0.125 sec/batch)
2017-05-07 21:01:12.085930: step 47220, loss = 0.75 (963.9 examples/sec; 0.133 sec/batch)
2017-05-07 21:01:13.403428: step 47230, loss = 0.55 (971.5 examples/sec; 0.132 sec/batch)
2017-05-07 21:01:14.735002: step 47240, loss = 0.64 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 21:01:16.080721: step 47250, loss = 0.75 (951.2 examples/sec; 0.135 sec/batch)
2017-05-07 21:01:17.396597: step 47260, loss = 0.93 (972.7 examples/sec; 0.132 sec/batch)
2017-05-07 21:01:18.750114: step 47270, loss = 0.74 (945.7 examples/sec; 0.135 sec/batch)
2017-05-07 21:01:20.104629: step 47280, loss = 0.73 (945.0 examples/sec; 0.135 sec/batch)
2017-05-07 21:01:21.431981: step 47290, loss = 0.72 (964.3 examples/sec; 0.133 sec/batch)
2017-05-07 21:01:22.881714: step 47300, loss = 0.75 (882.9 examples/sec; 0.145 sec/batch)
2017-05-07 21:01:24.110656: step 47310, loss = 0.81 (1041.6 examples/sec; 0.123 sec/batch)
2017-05-07 21:01:25.420747: step 47320, loss = 0.67 (977.0 examples/sec; 0.131 sec/batch)
2017-05-07 21:01:26.753037: step 47330, loss = 0.70 (960.8 examples/sec; 0.133 sec/batch)
2017-05-07 21:01:28.098625: step 47340, loss = 0.74 (951.3 examples/sec; 0.135 sec/batch)
2017-05-07 21:01:29.438903: step 47350, loss = 0.76 (955.0 examples/sec; 0.134 sec/batch)
2017-05-07 21:01:30.758790: step 47360, loss = 0.75 (969.8 examples/sec; 0.132 sec/batch)
2017-05-07 21:01:32.100344: step 47370, loss = 0.69 (954.1 examples/sec; 0.134 sec/batch)
2017-05-07 21:01:33.413888: step 47380, loss = 0.83 (974.5 examples/sec; 0.131 sec/batch)
2017-05-07 21:01:34.740520: step 47390, loss = 0.86 (964.9 examples/sec; 0.133 sec/batch)
2017-05-07 21:01:36.189342: step 47400, loss = 0.74 (883.5 examples/sec; 0.145 sec/batch)
2017-05-07 21:01:37.420264: step 47410, loss = 0.57 (1039.9 examples/sec; 0.123 sec/batch)
2017-05-07 21:01:38.757358: step 47420, loss = 0.61 (957.3 examples/sec; 0.134 sec/batch)
2017-05-07 21:01:40.092488: step 47430, loss = 0.70 (958.7 examples/sec; 0.134 sec/batch)
2017-05-07 21:01:41.408665: step 47440, loss = 0.82 (972.5 examples/sec; 0.132 sec/batch)
2017-05-07 21:01:42.773016: step 47450, loss = 0.65 (938.2 examples/sec; 0.136 sec/batch)
2017-05-07 21:01:44.095647: step 47460, loss = 0.85 (967.8 examples/sec; 0.132 sec/batch)
2017-05-07 21:01:45.415003: step 47470, loss = 0.63 (970.2 examples/sec; 0.132 sec/batch)
2017-05-07 21:01:46.742398: step 47480, loss = 0.78 (964.3 examples/sec; 0.133 sec/batch)
2017-05-07 21:01:48.098997: step 47490, loss = 0.73 (943.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:01:49.521373: step 47500, loss = 0.60 (899.9 examples/sec; 0.142 sec/batch)
2017-05-07 21:01:50.782994: step 47510, loss = 0.69 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:01:52.114707: step 47520, loss = 0.77 (961.2 examples/sec; 0.133 sec/batch)
2017-05-07 21:01:53.442161: step 47530, loss = 0.84 (964.3 examples/sec; 0.133 sec/batch)
2017-05-07 21:01:54.769943: step 47540, loss = 0.78 (964.0 examples/sec; 0.133 sec/batch)
2017-05-07 21:01:56.141706: step 47550, loss = 0.72 (933.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:01:57.462425: step 47560, loss = 0.67 (969.2 examples/sec; 0.132 sec/batch)
2017-05-07 21:01:58.812034: step 47570, loss = 0.68 (948.4 examples/sec; 0.135 sec/batch)
2017-05-07 21:02:00.142779: step 47580, loss = 0.86 (961.9 examples/sec; 0.133 sec/batch)
2017-05-07 21:02:01.480994: step 47590, loss = 0.77 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 21:02:02.916342: step 47600, loss = 0.81 (891.8 examples/sec; 0.144 sec/batch)
2017-05-07 21:02:04.139325: step 47610, loss = 0.72 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-07 21:02:05.466372: step 47620, loss = 0.78 (964.5 examples/sec; 0.133 sec/batch)
2017-05-07 21:02:06.793818: step 47630, loss = 0.68 (964.3 examples/sec; 0.133 sec/batch)
2017-05-07 21:02:08.107395: step 47640, loss = 0.65 (974.4 examples/sec; 0.131 sec/batch)
2017-05-07 21:02:09.451945: step 47650, loss = 0.84 (952.0 examples/sec; 0.134 sec/batch)
2017-05-07 21:02:10.777614: step 47660, loss = 0.74 (965.6 examples/sec; 0.133 sec/batch)
2017-05-07 21:02:12.100317: step 47670, loss = 0.75 (967.7 examples/sec; 0.132 sec/batch)
2017-05-07 21:02:13.404330: step 47680, loss = 0.99 (981.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:02:14.741170: step 47690, loss = 0.63 (957.5 examples/sec; 0.134 sec/batch)
2017-05-07 21:02:16.195727: step 47700, loss = 0.69 (880.0 examples/sec; 0.145 sec/batch)
2017-05-07 21:02:17.454716: step 47710, loss = 0.69 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:02:18.801103: step 47720, loss = 0.74 (950.7 examples/sec; 0.135 sec/batch)
2017-05-07 21:02:20.140718: step 47730, loss = 0.70 (955.5 examples/sec; 0.134 sec/batch)
2017-05-07 21:02:21.467315: step 47740, loss = 0.71 (964.9 examples/sec; 0.133 sec/batch)
2017-05-07 21:02:22.812076: step 47750, loss = 0.80 (951.8 examples/sec; 0.134 sec/batch)
2017-05-07 21:02:24.145325: step 47760, loss = 0.78 (960.1 examples/sec; 0.133 sec/batch)
2017-05-07 21:02:25.469603: step 47770, loss = 0.82 (966.6 examples/sec; 0.132 sec/batch)
2017-05-07 21:02:26.800983: step 47780, loss = 0.60 (961.4 examples/sec; 0.133 sec/batch)
2017-05-07 21:02:28.140802: step 47790, loss = 0.86 (955.4 examples/sec; 0.134 sec/batch)
2017-05-07 21:02:29.558496: step 47800, loss = 0.51 (902.9 examples/sec; 0.142 sec/batch)
2017-05-07 21:02:30.811826: step 47810, loss = 0.80 (1021.3 examples/sec; 0.125 sec/batch)
2017-05-07 21:02:32.133893: step 47820, loss = 0.91 (968.2 examples/sec; 0.132 sec/batch)
2017-05-07 21:02:33.466742: step 47830, loss = 0.69 (960.3 examples/sec; 0.133 sec/bE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 975 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
atch)
2017-05-07 21:02:34.798329: step 47840, loss = 0.91 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 21:02:36.159377: step 47850, loss = 0.79 (940.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:02:37.482667: step 47860, loss = 0.66 (967.3 examples/sec; 0.132 sec/batch)
2017-05-07 21:02:38.813917: step 47870, loss = 0.81 (961.5 examples/sec; 0.133 sec/batch)
2017-05-07 21:02:40.134485: step 47880, loss = 0.82 (969.3 examples/sec; 0.132 sec/batch)
2017-05-07 21:02:41.467954: step 47890, loss = 0.92 (959.9 examples/sec; 0.133 sec/batch)
2017-05-07 21:02:42.922586: step 47900, loss = 0.95 (879.9 examples/sec; 0.145 sec/batch)
2017-05-07 21:02:44.119701: step 47910, loss = 0.76 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-07 21:02:45.456244: step 47920, loss = 0.71 (957.7 examples/sec; 0.134 sec/batch)
2017-05-07 21:02:46.813090: step 47930, loss = 0.75 (943.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:02:48.146618: step 47940, loss = 0.80 (959.9 examples/sec; 0.133 sec/batch)
2017-05-07 21:02:49.481395: step 47950, loss = 0.81 (959.0 examples/sec; 0.133 sec/batch)
2017-05-07 21:02:50.828063: step 47960, loss = 0.80 (950.5 examples/sec; 0.135 sec/batch)
2017-05-07 21:02:52.183629: step 47970, loss = 0.68 (944.3 examples/sec; 0.136 sec/batch)
2017-05-07 21:02:53.498674: step 47980, loss = 0.62 (973.4 examples/sec; 0.132 sec/batch)
2017-05-07 21:02:54.799900: step 47990, loss = 0.76 (983.7 examples/sec; 0.130 sec/batch)
2017-05-07 21:02:56.271316: step 48000, loss = 0.67 (869.9 examples/sec; 0.147 sec/batch)
2017-05-07 21:02:57.487228: step 48010, loss = 0.62 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-07 21:02:58.809578: step 48020, loss = 0.69 (968.0 examples/sec; 0.132 sec/batch)
2017-05-07 21:03:00.161384: step 48030, loss = 0.77 (946.9 examples/sec; 0.135 sec/batch)
2017-05-07 21:03:01.498026: step 48040, loss = 0.65 (957.6 examples/sec; 0.134 sec/batch)
2017-05-07 21:03:02.860915: step 48050, loss = 0.81 (939.2 examples/sec; 0.136 sec/batch)
2017-05-07 21:03:04.169362: step 48060, loss = 0.66 (978.3 examples/sec; 0.131 sec/batch)
2017-05-07 21:03:05.493773: step 48070, loss = 0.67 (966.5 examples/sec; 0.132 sec/batch)
2017-05-07 21:03:06.823662: step 48080, loss = 0.73 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 21:03:08.162892: step 48090, loss = 0.67 (955.8 examples/sec; 0.134 sec/batch)
2017-05-07 21:03:09.609660: step 48100, loss = 0.67 (884.7 examples/sec; 0.145 sec/batch)
2017-05-07 21:03:10.838420: step 48110, loss = 0.87 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-07 21:03:12.164348: step 48120, loss = 0.77 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 21:03:13.480067: step 48130, loss = 0.75 (972.9 examples/sec; 0.132 sec/batch)
2017-05-07 21:03:14.812086: step 48140, loss = 0.65 (960.9 examples/sec; 0.133 sec/batch)
2017-05-07 21:03:16.140802: step 48150, loss = 0.81 (963.3 examples/sec; 0.133 sec/batch)
2017-05-07 21:03:17.448367: step 48160, loss = 0.78 (978.9 examples/sec; 0.131 sec/batch)
2017-05-07 21:03:18.783921: step 48170, loss = 0.79 (958.4 examples/sec; 0.134 sec/batch)
2017-05-07 21:03:20.108764: step 48180, loss = 0.69 (966.2 examples/sec; 0.132 sec/batch)
2017-05-07 21:03:21.456029: step 48190, loss = 0.60 (950.1 examples/sec; 0.135 sec/batch)
2017-05-07 21:03:22.888422: step 48200, loss = 0.74 (893.6 examples/sec; 0.143 sec/batch)
2017-05-07 21:03:24.134776: step 48210, loss = 0.72 (1027.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:03:25.458023: step 48220, loss = 0.65 (967.3 examples/sec; 0.132 sec/batch)
2017-05-07 21:03:26.782040: step 48230, loss = 0.85 (966.8 examples/sec; 0.132 sec/batch)
2017-05-07 21:03:28.124193: step 48240, loss = 1.05 (953.7 examples/sec; 0.134 sec/batch)
2017-05-07 21:03:29.487335: step 48250, loss = 0.67 (939.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:03:30.800799: step 48260, loss = 0.68 (974.5 examples/sec; 0.131 sec/batch)
2017-05-07 21:03:32.138178: step 48270, loss = 0.73 (957.1 examples/sec; 0.134 sec/batch)
2017-05-07 21:03:33.453566: step 48280, loss = 0.72 (973.1 examples/sec; 0.132 sec/batch)
2017-05-07 21:03:34.786968: step 48290, loss = 0.64 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 21:03:36.238661: step 48300, loss = 0.76 (881.7 examples/sec; 0.145 sec/batch)
2017-05-07 21:03:37.447410: step 48310, loss = 0.77 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-07 21:03:38.790138: step 48320, loss = 0.74 (953.3 examples/sec; 0.134 sec/batch)
2017-05-07 21:03:40.123966: step 48330, loss = 0.78 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 21:03:41.456026: step 48340, loss = 0.72 (960.9 examples/sec; 0.133 sec/batch)
2017-05-07 21:03:42.801535: step 48350, loss = 0.69 (951.3 examples/sec; 0.135 sec/batch)
2017-05-07 21:03:44.139935: step 48360, loss = 0.76 (956.4 examples/sec; 0.134 sec/batch)
2017-05-07 21:03:45.464104: step 48370, loss = 0.73 (966.6 examples/sec; 0.132 sec/batch)
2017-05-07 21:03:46.778286: step 48380, loss = 0.60 (974.0 examples/sec; 0.131 sec/batch)
2017-05-07 21:03:48.108145: step 48390, loss = 0.82 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 21:03:49.518951: step 48400, loss = 0.56 (907.3 examples/sec; 0.141 sec/batch)
2017-05-07 21:03:50.763386: step 48410, loss = 0.80 (1028.6 examples/sec; 0.124 sec/batch)
2017-05-07 21:03:52.092112: step 48420, loss = 0.76 (963.3 examples/sec; 0.133 sec/batch)
2017-05-07 21:03:53.404909: step 48430, loss = 0.73 (975.0 examples/sec; 0.131 sec/batch)
2017-05-07 21:03:54.743120: step 48440, loss = 0.70 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 21:03:56.079043: step 48450, loss = 0.84 (958.1 examples/sec; 0.134 sec/batch)
2017-05-07 21:03:57.377947: step 48460, loss = 0.68 (985.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:03:58.710447: step 48470, loss = 0.68 (960.6 examples/sec; 0.133 sec/batch)
2017-05-07 21:04:00.060682: step 48480, loss = 0.65 (948.0 examples/sec; 0.135 sec/batch)
2017-05-07 21:04:01.369411: step 48490, loss = 0.85 (978.0 examples/sec; 0.131 sec/batch)
2017-05-07 21:04:02.817842: step 48500, loss = 0.70 (883.7 examples/sec; 0.145 sec/batch)
2017-05-07 21:04:04.061115: step 48510, loss = 0.75 (1029.5 examples/sec; 0.124 sec/batch)
2017-05-07 21:04:05.401714: step 48520, loss = 0.63 (954.8 examples/sec; 0.134 sec/batch)
2017-05-07 21:04:06.737533: step 48530, loss = 0.71 (958.2 examples/sec; 0.134 sec/batch)
2017-05-07 21:04:08.075110: step 48540, loss = 0.72 (957.0 examples/sec; 0.134 sec/batch)
2017-05-07 21:04:09.442603: step 48550, loss = 0.77 (936.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:04:10.758509: step 48560, loss = 0.81 (972.7 examples/sec; 0.132 sec/batch)
2017-05-07 21:04:12.096648: step 48570, loss = 0.80 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 21:04:13.417642: step 48580, loss = 0.82 (969.0 examples/sec; 0.132 sec/batch)
2017-05-07 21:04:14.742690: step 48590, loss = 0.76 (966.0 examples/sec; 0.133 sec/batch)
2017-05-07 21:04:16.184897: step 48600, loss = 0.80 (887.5 examples/sec; 0.144 sec/batch)
2017-05-07 21:04:17.409247: step 48610, loss = 0.66 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-07 21:04:18.739745: step 48620, loss = 0.64 (962.1 examples/sec; 0.133 sec/batch)
2017-05-07 21:04:20.071252: step 48630, loss = 0.81 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 21:04:21.412887: step 48640, loss = 0.77 (954.1 examples/sec; 0.134 sec/batch)
2017-05-07 21:04:22.767876: step 48650, loss = 0.74 (944.7 examples/sec; 0.135 sec/batch)
2017-05-07 21:04:24.103231: step 48660, loss = 0.68 (958.5 examples/sec; 0.134 sec/batch)
2017-05-07 21:04:25.433352: step 48670, loss = 0.67 (962.3 examples/sec; 0.133 sec/batch)
2017-05-07 21:04:26.795066: step 48680, loss = 0.61 (940.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:04:28.140429: step 48690, loss = 0.75 (951.4 examples/sec; 0.135 sec/batch)
2017-05-07 21:04:29.572445: step 48700, loss = 0.79 (893.8 examples/sec; 0.143 sec/batch)
2017-05-07 21:04:30.796380: step 48710, loss = 0.81 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-07 21:04:32.143234: step 48720, loss = 0.94 (950.4 examples/sec; 0.135 sec/batch)
2017-05-07 21:04:33.473108: step 48730, loss = 0.82 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 21:04:34.825776: step 48740, loss = 0.69 (946.3 examples/sec; 0.13E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 993 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
5 sec/batch)
2017-05-07 21:04:36.212752: step 48750, loss = 0.63 (922.9 examples/sec; 0.139 sec/batch)
2017-05-07 21:04:37.520728: step 48760, loss = 0.85 (978.6 examples/sec; 0.131 sec/batch)
2017-05-07 21:04:38.855786: step 48770, loss = 0.82 (958.8 examples/sec; 0.134 sec/batch)
2017-05-07 21:04:40.186308: step 48780, loss = 0.74 (962.0 examples/sec; 0.133 sec/batch)
2017-05-07 21:04:41.517276: step 48790, loss = 0.77 (961.7 examples/sec; 0.133 sec/batch)
2017-05-07 21:04:42.955097: step 48800, loss = 0.81 (890.2 examples/sec; 0.144 sec/batch)
2017-05-07 21:04:44.175980: step 48810, loss = 0.67 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-07 21:04:45.507814: step 48820, loss = 0.74 (961.1 examples/sec; 0.133 sec/batch)
2017-05-07 21:04:46.835231: step 48830, loss = 0.77 (964.3 examples/sec; 0.133 sec/batch)
2017-05-07 21:04:48.158857: step 48840, loss = 0.74 (967.0 examples/sec; 0.132 sec/batch)
2017-05-07 21:04:49.508640: step 48850, loss = 0.58 (948.3 examples/sec; 0.135 sec/batch)
2017-05-07 21:04:50.825985: step 48860, loss = 0.74 (971.7 examples/sec; 0.132 sec/batch)
2017-05-07 21:04:52.161829: step 48870, loss = 0.72 (958.2 examples/sec; 0.134 sec/batch)
2017-05-07 21:04:53.502804: step 48880, loss = 0.68 (954.5 examples/sec; 0.134 sec/batch)
2017-05-07 21:04:54.840941: step 48890, loss = 0.83 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 21:04:56.275208: step 48900, loss = 0.72 (892.4 examples/sec; 0.143 sec/batch)
2017-05-07 21:04:57.509715: step 48910, loss = 0.65 (1036.9 examples/sec; 0.123 sec/batch)
2017-05-07 21:04:58.852714: step 48920, loss = 0.68 (953.1 examples/sec; 0.134 sec/batch)
2017-05-07 21:05:00.188558: step 48930, loss = 0.65 (958.2 examples/sec; 0.134 sec/batch)
2017-05-07 21:05:01.552032: step 48940, loss = 0.69 (938.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:05:02.917170: step 48950, loss = 0.61 (937.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:05:04.235210: step 48960, loss = 0.78 (971.1 examples/sec; 0.132 sec/batch)
2017-05-07 21:05:05.561154: step 48970, loss = 0.75 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 21:05:06.882791: step 48980, loss = 0.70 (968.5 examples/sec; 0.132 sec/batch)
2017-05-07 21:05:08.205001: step 48990, loss = 0.75 (968.1 examples/sec; 0.132 sec/batch)
2017-05-07 21:05:09.612498: step 49000, loss = 0.68 (909.4 examples/sec; 0.141 sec/batch)
2017-05-07 21:05:10.859395: step 49010, loss = 0.81 (1026.5 examples/sec; 0.125 sec/batch)
2017-05-07 21:05:12.194405: step 49020, loss = 0.83 (958.8 examples/sec; 0.134 sec/batch)
2017-05-07 21:05:13.506618: step 49030, loss = 0.72 (975.5 examples/sec; 0.131 sec/batch)
2017-05-07 21:05:14.834518: step 49040, loss = 0.76 (963.9 examples/sec; 0.133 sec/batch)
2017-05-07 21:05:16.187553: step 49050, loss = 0.73 (946.0 examples/sec; 0.135 sec/batch)
2017-05-07 21:05:17.495817: step 49060, loss = 0.87 (978.4 examples/sec; 0.131 sec/batch)
2017-05-07 21:05:18.836188: step 49070, loss = 0.76 (954.9 examples/sec; 0.134 sec/batch)
2017-05-07 21:05:20.178750: step 49080, loss = 0.62 (953.4 examples/sec; 0.134 sec/batch)
2017-05-07 21:05:21.506910: step 49090, loss = 0.76 (963.7 examples/sec; 0.133 sec/batch)
2017-05-07 21:05:22.949085: step 49100, loss = 0.82 (887.5 examples/sec; 0.144 sec/batch)
2017-05-07 21:05:24.176977: step 49110, loss = 0.74 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-07 21:05:25.489335: step 49120, loss = 0.86 (975.3 examples/sec; 0.131 sec/batch)
2017-05-07 21:05:26.818098: step 49130, loss = 0.82 (963.3 examples/sec; 0.133 sec/batch)
2017-05-07 21:05:28.161389: step 49140, loss = 0.66 (952.9 examples/sec; 0.134 sec/batch)
2017-05-07 21:05:29.529436: step 49150, loss = 0.78 (935.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:05:30.843994: step 49160, loss = 0.56 (973.7 examples/sec; 0.131 sec/batch)
2017-05-07 21:05:32.166692: step 49170, loss = 0.75 (967.7 examples/sec; 0.132 sec/batch)
2017-05-07 21:05:33.484447: step 49180, loss = 0.67 (971.3 examples/sec; 0.132 sec/batch)
2017-05-07 21:05:34.838583: step 49190, loss = 0.80 (945.2 examples/sec; 0.135 sec/batch)
2017-05-07 21:05:36.295535: step 49200, loss = 0.77 (878.5 examples/sec; 0.146 sec/batch)
2017-05-07 21:05:37.517371: step 49210, loss = 0.70 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-07 21:05:38.851011: step 49220, loss = 0.83 (959.8 examples/sec; 0.133 sec/batch)
2017-05-07 21:05:40.186509: step 49230, loss = 0.78 (958.4 examples/sec; 0.134 sec/batch)
2017-05-07 21:05:41.509374: step 49240, loss = 0.68 (967.6 examples/sec; 0.132 sec/batch)
2017-05-07 21:05:42.873712: step 49250, loss = 0.55 (938.2 examples/sec; 0.136 sec/batch)
2017-05-07 21:05:44.199939: step 49260, loss = 0.68 (965.2 examples/sec; 0.133 sec/batch)
2017-05-07 21:05:45.522173: step 49270, loss = 0.80 (968.1 examples/sec; 0.132 sec/batch)
2017-05-07 21:05:46.858059: step 49280, loss = 0.63 (958.2 examples/sec; 0.134 sec/batch)
2017-05-07 21:05:48.204646: step 49290, loss = 0.59 (950.5 examples/sec; 0.135 sec/batch)
2017-05-07 21:05:49.627089: step 49300, loss = 0.60 (899.9 examples/sec; 0.142 sec/batch)
2017-05-07 21:05:50.850297: step 49310, loss = 1.01 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-07 21:05:52.191092: step 49320, loss = 0.76 (954.7 examples/sec; 0.134 sec/batch)
2017-05-07 21:05:53.522961: step 49330, loss = 0.66 (961.1 examples/sec; 0.133 sec/batch)
2017-05-07 21:05:54.888905: step 49340, loss = 0.48 (937.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:05:56.252659: step 49350, loss = 0.74 (938.6 examples/sec; 0.136 sec/batch)
2017-05-07 21:05:57.566966: step 49360, loss = 0.66 (973.9 examples/sec; 0.131 sec/batch)
2017-05-07 21:05:58.879960: step 49370, loss = 0.65 (974.9 examples/sec; 0.131 sec/batch)
2017-05-07 21:06:00.227618: step 49380, loss = 0.74 (949.8 examples/sec; 0.135 sec/batch)
2017-05-07 21:06:01.545071: step 49390, loss = 0.62 (971.6 examples/sec; 0.132 sec/batch)
2017-05-07 21:06:02.982245: step 49400, loss = 0.77 (890.6 examples/sec; 0.144 sec/batch)
2017-05-07 21:06:04.238731: step 49410, loss = 0.76 (1018.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:06:05.566413: step 49420, loss = 0.70 (964.1 examples/sec; 0.133 sec/batch)
2017-05-07 21:06:06.899603: step 49430, loss = 0.64 (960.1 examples/sec; 0.133 sec/batch)
2017-05-07 21:06:08.217901: step 49440, loss = 0.81 (971.0 examples/sec; 0.132 sec/batch)
2017-05-07 21:06:09.563620: step 49450, loss = 0.65 (951.2 examples/sec; 0.135 sec/batch)
2017-05-07 21:06:10.885665: step 49460, loss = 0.76 (968.2 examples/sec; 0.132 sec/batch)
2017-05-07 21:06:12.216885: step 49470, loss = 0.79 (961.5 examples/sec; 0.133 sec/batch)
2017-05-07 21:06:13.537886: step 49480, loss = 0.65 (969.0 examples/sec; 0.132 sec/batch)
2017-05-07 21:06:14.882027: step 49490, loss = 0.82 (952.3 examples/sec; 0.134 sec/batch)
2017-05-07 21:06:16.311383: step 49500, loss = 0.76 (895.5 examples/sec; 0.143 sec/batch)
2017-05-07 21:06:17.538362: step 49510, loss = 0.60 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-07 21:06:18.885335: step 49520, loss = 0.87 (950.3 examples/sec; 0.135 sec/batch)
2017-05-07 21:06:20.249281: step 49530, loss = 0.64 (938.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:06:21.568827: step 49540, loss = 0.70 (970.0 examples/sec; 0.132 sec/batch)
2017-05-07 21:06:22.912633: step 49550, loss = 0.66 (952.5 examples/sec; 0.134 sec/batch)
2017-05-07 21:06:24.246976: step 49560, loss = 0.76 (959.3 examples/sec; 0.133 sec/batch)
2017-05-07 21:06:25.571229: step 49570, loss = 0.80 (966.6 examples/sec; 0.132 sec/batch)
2017-05-07 21:06:26.904290: step 49580, loss = 0.59 (960.2 examples/sec; 0.133 sec/batch)
2017-05-07 21:06:28.253695: step 49590, loss = 0.70 (948.6 examples/sec; 0.135 sec/batch)
2017-05-07 21:06:29.688656: step 49600, loss = 0.69 (892.0 examples/sec; 0.143 sec/batch)
2017-05-07 21:06:30.920376: step 49610, loss = 0.91 (1039.2 examples/sec; 0.123 sec/batch)
2017-05-07 21:06:32.250568: step 49620, loss = 0.65 (962.3 examples/sec; 0.133 sec/batch)
2017-05-07 21:06:33.570659: step 49630, loss = 0.71 (969.6 examples/sec; 0.132 sec/batch)
2017-05-07 21:06:34.908821: step 49640, loss = 0.81 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 21:06:36.276814: step 49650, loss = 0.69 (935.7 examples/seE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1012 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1017 events to /tmp/cifar10_train/events.out.tfevents.1494198937.GHC31.GHC.ANDREW.CMU.EDU
c; 0.137 sec/batch)
2017-05-07 21:06:37.565632: step 49660, loss = 0.70 (993.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:06:38.896518: step 49670, loss = 0.68 (961.8 examples/sec; 0.133 sec/batch)
2017-05-07 21:06:40.259111: step 49680, loss = 0.83 (939.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:06:41.586697: step 49690, loss = 0.68 (964.2 examples/sec; 0.133 sec/batch)
2017-05-07 21:06:43.037700: step 49700, loss = 0.61 (882.1 examples/sec; 0.145 sec/batch)
2017-05-07 21:06:44.272850: step 49710, loss = 0.64 (1036.3 examples/sec; 0.124 sec/batch)
2017-05-07 21:06:45.592326: step 49720, loss = 0.73 (970.1 examples/sec; 0.132 sec/batch)
2017-05-07 21:06:46.918238: step 49730, loss = 0.60 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 21:06:48.268487: step 49740, loss = 0.62 (948.0 examples/sec; 0.135 sec/batch)
2017-05-07 21:06:49.602216: step 49750, loss = 0.76 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 21:06:50.922306: step 49760, loss = 0.84 (969.6 examples/sec; 0.132 sec/batch)
2017-05-07 21:06:52.261029: step 49770, loss = 0.72 (956.1 examples/sec; 0.134 sec/batch)
2017-05-07 21:06:53.598995: step 49780, loss = 0.63 (956.7 examples/sec; 0.134 sec/batch)
2017-05-07 21:06:54.934309: step 49790, loss = 0.76 (958.6 examples/sec; 0.134 sec/batch)
2017-05-07 21:06:56.421445: step 49800, loss = 0.89 (860.7 examples/sec; 0.149 sec/batch)
2017-05-07 21:06:57.656592: step 49810, loss = 0.82 (1036.3 examples/sec; 0.124 sec/batch)
2017-05-07 21:06:58.997737: step 49820, loss = 0.66 (954.4 examples/sec; 0.134 sec/batch)
2017-05-07 21:07:00.331813: step 49830, loss = 0.71 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 21:07:01.642211: step 49840, loss = 0.66 (976.8 examples/sec; 0.131 sec/batch)
2017-05-07 21:07:02.997550: step 49850, loss = 0.71 (944.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:07:04.322297: step 49860, loss = 0.86 (966.2 examples/sec; 0.132 sec/batch)
2017-05-07 21:07:05.659192: step 49870, loss = 0.73 (957.4 examples/sec; 0.134 sec/batch)
2017-05-07 21:07:07.005848: step 49880, loss = 0.67 (950.5 examples/sec; 0.135 sec/batch)
2017-05-07 21:07:08.349744: step 49890, loss = 0.64 (952.5 examples/sec; 0.134 sec/batch)
2017-05-07 21:07:09.788826: step 49900, loss = 0.82 (889.5 examples/sec; 0.144 sec/batch)
2017-05-07 21:07:11.015287: step 49910, loss = 0.71 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-07 21:07:12.374111: step 49920, loss = 0.63 (942.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:07:13.718009: step 49930, loss = 0.56 (952.5 examples/sec; 0.134 sec/batch)
2017-05-07 21:07:15.061899: step 49940, loss = 0.80 (952.5 examples/sec; 0.134 sec/batch)
2017-05-07 21:07:16.423627: step 49950, loss = 0.58 (940.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:07:17.751927: step 49960, loss = 0.71 (963.6 examples/sec; 0.133 sec/batch)
2017-05-07 21:07:19.081783: step 49970, loss = 0.69 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 21:07:20.400138: step 49980, loss = 0.74 (970.9 examples/sec; 0.132 sec/batch)
2017-05-07 21:07:21.744120: step 49990, loss = 0.83 (952.4 examples/sec; 0.134 sec/batch)
2017-05-07 21:07:23.176293: step 50000, loss = 0.63 (893.7 examples/sec; 0.143 sec/batch)
--- 6706.4194541 seconds ---
