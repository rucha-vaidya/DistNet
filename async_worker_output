I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.7335
pciBusID 0000:04:00.0
Total memory: 7.92GiB
Free memory: 4.10GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x33368e0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: 
name: Quadro K620
major: 5 minor: 0 memoryClockRate (GHz) 1.124
pciBusID 0000:03:00.0
Total memory: 1.95GiB
Free memory: 1.85GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:04:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:962] Ignoring gpu device (device: 1, name: Quadro K620, pci bus id: 0000:03:00.0) with Cuda multiprocessor count: 3. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.
Connecting to port  16000
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Received variable values from ps
2017-05-06 20:58:12.530300: step 0, loss = 4.68 (102.8 examples/sec; 1.246 sec/batch)
2017-05-06 20:58:13.222946: step 10, loss = 4.58 (1848.0 examples/sec; 0.069 sec/batch)
2017-05-06 20:58:14.044083: step 20, loss = 4.49 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 20:58:14.872489: step 30, loss = 4.60 (1545.2 examples/sec; 0.083 sec/batch)
2017-05-06 20:58:15.685524: step 40, loss = 4.36 (1574.3 examples/sec; 0.081 sec/batch)
2017-05-06 20:58:16.507072: step 50, loss = 4.45 (1558.1 examples/sec; 0.082 sec/batch)
2017-05-06 20:58:17.327854: step 60, loss = 4.21 (1559.4 examples/sec; 0.082 sec/batch)
2017-05-06 20:58:18.155025: step 70, loss = 4.31 (1547.4 examples/sec; 0.083 sec/batch)
2017-05-06 20:58:18.983849: step 80, loss = 4.21 (1544.4 examples/sec; 0.083 sec/batch)
2017-05-06 20:58:19.806944: step 90, loss = 4.13 (1555.1 examples/sec; 0.082 sec/batch)
2017-05-06 20:58:20.763311: step 100, loss = 4.11 (1338.4 examples/sec; 0.096 sec/batch)
2017-05-06 20:58:21.466735: step 110, loss = 4.28 (1819.7 examples/sec; 0.070 sec/batch)
2017-05-06 20:58:22.284019: step 120, loss = 4.20 (1566.2 examples/sec; 0.082 sec/batch)
2017-05-06 20:58:23.108104: step 130, loss = 4.04 (1553.2 examples/sec; 0.082 sec/batch)
2017-05-06 20:58:23.928077: step 140, loss = 3.92 (1561.0 examples/sec; 0.082 sec/batch)
2017-05-06 20:58:24.755176: step 150, loss = 4.17 (1547.6 examples/sec; 0.083 sec/batch)
2017-05-06 20:58:25.571709: step 160, loss = 4.06 (1567.6 examples/sec; 0.082 sec/batch)
2017-05-06 20:58:26.394320: step 170, loss = 3.86 (1556.0 examples/sec; 0.082 sec/batch)
2017-05-06 20:58:27.218415: step 180, loss = 3.80 (1553.2 examples/sec; 0.082 sec/batch)
2017-05-06 20:58:28.050297: step 190, loss = 3.71 (1538.7 examples/sec; 0.083 sec/batch)
2017-05-06 20:58:28.994552: step 200, loss = 3.81 (1355.6 examples/sec; 0.094 sec/batch)
2017-05-06 20:58:29.701590: step 210, loss = 3.62 (1810.3 examples/sec; 0.071 sec/batch)
2017-05-06 20:58:30.529474: step 220, loss = 3.79 (1546.1 examples/sec; 0.083 sec/batch)
2017-05-06 20:58:31.358190: step 230, loss = 3.82 (1544.6 examples/sec; 0.083 sec/batch)
2017-05-06 20:58:32.171926: step 240, loss = 3.58 (1573.0 examples/sec; 0.081 sec/batch)
2017-05-06 20:58:33.001844: step 250, loss = 3.60 (1542.3 examples/sec; 0.083 sec/batch)
2017-05-06 20:58:33.821454: step 260, loss = 4.38 (1561.7 examples/sec; 0.082 sec/batch)
2017-05-06 20:58:34.649100: step 270, loss = 3.66 (1546.6 examples/sec; 0.083 sec/batch)
2017-05-06 20:58:35.464362: step 280, loss = 3.67 (1570.0 examples/sec; 0.082 sec/batch)
2017-05-06 20:58:36.293257: step 290, loss = 3.81 (1544.2 examples/sec; 0.083 sec/batch)
2017-05-06 20:58:37.246213: step 300, loss = 3.72 (1343.2 examples/sec; 0.095 sec/batch)
2017-05-06 20:58:37.948688: step 310, loss = 4.05 (1822.1 examples/sec; 0.070 sec/batch)
2017-05-06 20:58:38.768711: step 320, loss = 3.55 (1560.9 examples/sec; 0.082 sec/batch)
2017-05-06 20:58:39.580961: step 330, loss = 3.35 (1575.9 examples/sec; 0.081 sec/batch)
2017-05-06 20:58:40.410241: step 340, loss = 3.34 (1543.5 examples/sec; 0.083 sec/batch)
2017-05-06 20:58:41.241116: step 350, loss = 3.29 (1540.5 examples/sec; 0.083 sec/batch)
2017-05-06 20:58:42.057110: step 360, loss = 3.45 (1568.6 examples/sec; 0.082 sec/batch)
2017-05-06 20:58:42.880157: step 370, loss = 3.40 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 20:58:43.708121: step 380, loss = 3.38 (1546.0 examples/sec; 0.083 sec/batch)
2017-05-06 20:58:44.523176: step 390, loss = 3.22 (1570.5 examples/sec; 0.082 sec/batch)
2017-05-06 20:58:45.482735: step 400, loss = 3.43 (1333.9 examples/sec; 0.096 sec/batch)
2017-05-06 20:58:46.183742: step 410, loss = 3.46 (1826.0 examples/sec; 0.070 sec/batch)
2017-05-06 20:58:47.008066: step 420, loss = 3.19 (1552.8 examples/sec; 0.082 sec/batch)
2017-05-06 20:58:47.834030: step 430, loss = 3.29 (1549.7 examples/sec; 0.083 sec/batch)
2017-05-06 20:58:48.666019: step 440, loss = 3.12 (1538.5 examples/sec; 0.083 sec/batch)
2017-05-06 20:58:49.493193: step 450, loss = 3.25 (1547.4 examples/sec; 0.083 sec/batch)
2017-05-06 20:58:50.325470: step 460, loss = 3.28 (1538.0 examples/sec; 0.083 sec/batch)
2017-05-06 20:58:51.151132: step 470, loss = 2.93 (1550.3 examples/sec; 0.083 sec/batch)
2017-05-06 20:58:51.961400: step 480, loss = 3.17 (1579.7 examples/sec; 0.081 sec/batch)
2017-05-06 20:58:52.785450: step 490, loss = 3.01 (1553.3 examples/sec; 0.082 sec/batch)
2017-05-06 20:58:53.740252: step 500, loss = 3.43 (1340.6 examples/sec; 0.095 sec/batch)
2017-05-06 20:58:54.444673: step 510, loss = 3.21 (1817.1 examples/sec; 0.070 sec/batch)
2017-05-06 20:58:55.266121: step 520, loss = 3.05 (1558.2 examples/sec; 0.082 sec/batch)
2017-05-06 20:58:56.075798: step 530, loss = 3.08 (1580.9 examples/sec; 0.081 sec/batch)
2017-05-06 20:58:56.901176: step 540, loss = 3.24 (1550.8 examples/sec; 0.083 sec/batch)
2017-05-06 20:58:57.724389: step 550, loss = 3.09 (1554.9 examples/sec; 0.082 sec/batch)
2017-05-06 20:58:58.545126: step 560, loss = 2.91 (1559.6 examples/sec; 0.082 sec/batch)
2017-05-06 20:58:59.364505: step 570, loss = 3.17 (1562.1 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:00.182647: step 580, loss = 2.89 (1564.5 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:01.007579: step 590, loss = 3.04 (1551.6 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:01.939862: step 600, loss = 2.98 (1373.0 examples/sec; 0.093 sec/batch)
2017-05-06 20:59:02.651996: step 610, loss = 2.84 (1797.4 examples/sec; 0.071 sec/batch)
2017-05-06 20:59:03.467698: step 620, loss = 3.09 (1569.2 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:04.298334: step 630, loss = 3.00 (1541.0 examples/sec; 0.083 sec/batch)
2017-05-06 20:59:05.116737: step 640, loss = 3.21 (1564.0 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:05.939332: step 650, loss = 2.81 (1556.1 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:06.758356: step 660, loss = 3.03 (1562.8 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:07.570515: step 670, loss = 3.01 (1576.1 examples/sec; 0.081 sec/batch)
2017-05-06 20:59:08.402682: step 680, loss = 2.93 (1538.1 examples/sec; 0.083 sec/batch)
2017-05-06 20:59:09.227456: step 690, loss = 2.84 (1551.9 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:10.171455: step 700, loss = 2.71 (1355.9 examples/sec; 0.094 sec/batch)
2017-05-06 20:59:10.888084: step 710, loss = 2.79 (1786.1 examples/sec; 0.072 sec/batch)
2017-05-06 20:59:11.712309: step 720, loss = 2.72 (1553.0 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:12.531826: step 730, loss = 2.79 (1561.9 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:13.361927: step 740, loss = 2.87 (1542.0 examples/sec; 0.083 sec/batch)
2017-05-06 20:59:14.188931: step 750, loss = 2.69 (1547.8 examples/sec; 0.083 sec/batch)
2017-05-06 20:59:15.020001: step 760, loss = 2.60 (1540.2 examples/sec; 0.083 sec/batch)
2017-05-06 20:59:15.829527: step 770, loss = 2.66 (1581.2 examples/sec; 0.081 sec/batch)
2017-05-06 20:59:16.657257: step 780, loss = 2.94 (1546.4 examples/sec; 0.083 sec/batch)
2017-05-06 20:59:17.479302: step 790, loss = 2.96 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:18.422644: step 800, loss = 2.69 (1356.9 examples/sec; 0.094 sec/batch)
2017-05-06 20:59:19.141655: step 810, loss = 2.71 (1780.2 examples/sec; 0.072 sec/batch)
2017-05-06 20:59:19.959321: step 820, loss = 2.66 (1565.4 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:20.801366: step 830, loss = 2.78 (1520.1 examples/sec; 0.084 sec/batch)
2017-05-06 20:59:21.626064: step 840, loss = 3.15 (1552.1 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:22.457224: step 850, loss = 3.06 (1540.0 examples/sec; 0.083 sec/batch)
2017-05-06 20:59:23.273974: step 860, loss = 2.53 (1567.2 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:24.093684: step 870, loss = 2.39 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:24.921172: step 880, loss = 2.78 (1546.9 examples/sec; 0.083 sec/batch)
2017-05-06 20:59:25.739518: step 890, loss = 2.60 (1564.1 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:26.672397: step 900, loss = 2.55 (1372.1 examples/sec; 0.093 sec/batch)
2017-05-06 20:59:27.391421: step 910, loss = 2.52 (1780.2 examples/sec; 0.072 sec/batch)
2017-05-06 20:59:28.211806: step 920, loss = 2.42 (1560.2 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:29.040514: step 930, loss = 2.60 (1544.6 examples/sec; 0.083 sec/batch)
2017-05-06 20:59:29.864126: step 940, loss = 2.47 (1554.1 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:30.681260: step 950, loss = 2.50 (1566.5 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:31.505214: step 960, loss = 2.68 (1553.5 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:32.331728: step 970, loss = 2.70 (1548.7 examples/sec; 0.083 sec/batch)
2017-05-06 20:59:33.172166: step 980, loss = 2.29 (1523.0 examples/sec; 0.084 sec/batch)
2017-05-06 20:59:34.019391: step 990, loss = 2.44 (1510.8 examples/sec; 0.085 sec/batch)
2017-05-06 20:59:34.942284: step 1000, loss = 2.26 (1386.9 examples/sec; 0.092 sec/batch)
2017-05-06 20:59:35.646395: step 1010, loss = 2.49 (1817.9 examples/sec; 0.070 sec/batch)
2017-05-06 20:59:36.469467: step 1020, loss = 2.56 (1555.1 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:37.295114: step 1030, loss = 2.32 (1550.3 examples/sec; 0.083 sec/batch)
2017-05-06 20:59:38.126610: step 1040, loss = 2.46 (1539.5 examples/sec; 0.083 sec/batch)
2017-05-06 20:59:38.954410: step 1050, loss = 2.27 (1546.2 examples/sec; 0.083 sec/batch)
2017-05-06 20:59:39.774504: step 1060, loss = 2.29 (1560.8 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:40.598974: step 1070, loss = 2.36 (1552.5 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:41.425591: step 1080, loss = 2.54 (1548.5 examples/sec; 0.083 sec/batch)
2017-05-06 20:59:42.259846: step 1090, loss = 2.20 (1534.3 examples/sec; 0.083 sec/batch)
2017-05-06 20:59:43.198981: step 1100, loss = 2.25 (1363.0 examples/sec; 0.094 sec/batch)
2017-05-06 20:59:43.905212: step 1110, loss = 2.30 (1812.5 examples/sec; 0.071 sec/batch)
2017-05-06 20:59:44.724458: step 1120, loss = 2.23 (1562.4 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:45.545708: step 1130, loss = 2.36 (1558.6 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:46.367774: step 1140, loss = 2.25 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:47.193544: step 1150, loss = 2.37 (1550.1 examples/sec; 0.083 sec/batch)
2017-05-06 20:59:48.012811: step 1160, loss = 2.44 (1562.4 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:48.839339: step 1170, loss = 2.12 (1548.6 examples/sec; 0.083 sec/batch)
2017-05-06 20:59:49.669530: step 1180, loss = 2.39 (1541.8 examples/sec; 0.083 sec/batch)
2017-05-06 20:59:50.489099: step 1190, loss = 2.30 (1561.8 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:51.424620: step 1200, loss = 2.48 (1368.2 examples/sec; 0.094 sec/batch)
2017-05-06 20:59:52.136544: step 1210, loss = 2.15 (1798.0 examples/sec; 0.071 sec/batch)
2017-05-06 20:59:52.963243: step 1220, loss = 2.12 (1548.3 examples/sec; 0.083 sec/batch)
2017-05-06 20:59:53.785787: step 1230, loss = 2.10 (1556.1 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:54.612452: step 1240, loss = 2.34 (1548.4 examples/sec; 0.083 sec/batch)
2017-05-06 20:59:55.429700: step 1250, loss = 2.55 (1566.2 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:56.247907: step 1260, loss = 2.25 (1564.4 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:57.071203: step 1270, loss = 2.19 (1554.7 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:57.893446: step 1280, loss = 2.02 (1556.7 examples/sec; 0.082 sec/batch)
2017-05-06 20:59:58.726082: step 1290, loss = 2.19 (1537.3 examples/sec; 0.083 sec/batch)
2017-05-06 20:59:59.664074: step 1300, loss = 2.14 (1364.6 examples/sec; 0.094 sec/batch)
2017-05-06 21:00:00.369805: step 1310, loss = 2.11 (1813.7 examples/sec; 0.071 sec/batch)
2017-05-06 21:00:01.196924: step 1320, loss = 2.11 (1547.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:00:02.027095: step 1330, loss = 2.12 (1541.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:00:02.852067: step 1340, loss = 2.14 (1551.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:00:03.671173: step 1350, loss = 2.16 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:00:04.495538: step 1360, loss = 2.47 (1552.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:00:05.317432: step 1370, loss = 2.07 (1557.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:00:06.142325: step 1380, loss = 2.06 (1551.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:00:06.968531: step 1390, loss = 2.34 (1549.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:00:07.899139: step 1400, loss = 1.83 (1375.4 examples/sec; 0.093 sec/batch)
2017-05-06 21:00:08.618570: step 1410, loss = 1.92 (1779.2 examples/sec; 0.072 sec/batch)
2017-05-06 21:00:09.448320: step 1420, loss = 1.99 (1542.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:00:10.268343: step 1430, loss = 1.94 (1560.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:00:11.086678: step 1440, loss = 1.99 (1564.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:00:11.898209: step 1450, loss = 1.89 (1577.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:00:12.723526: step 1460, loss = 1.92 (1550.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:00:13.548490: step 1470, loss = 2.12 (1551.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:00:14.371379: step 1480, loss = 1.99 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:00:15.199715: step 1490, loss = 2.04 (1545.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:00:16.123855: step 1500, loss = 1.84 (1385.1 examples/sec; 0.092 sec/batch)
2017-05-06 21:00:16.850766: step 1510, loss = 1.94 (1760.9 examples/sec; 0.073 sec/batch)
2017-05-06 21:00:17.674569: step 1520, loss = 1.76 (1553.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:00:18.503621: step 1530, loss = 1.95 (1543.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:00:19.339315: step 1540, loss = 1.87 (1531.7 examples/sec; 0.084 sec/batch)
2017-05-06 21:00:20.168395: step 1550, loss = 2.18 (1543.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:00:20.996619: step 1560, loss = 1.87 (1545.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:00:21.817112: step 1570, loss = 1.78 (1560.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:00:22.648751: step 1580, loss = 1.84 (1539.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:00:23.475984: step 1590, loss = 1.91 (1547.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:00:24.407375: step 1600, loss = 1.80 (1374.3 examples/sec; 0.093 sec/batch)
2017-05-06 21:00:25.127855: step 1610, loss = 1.78 (1776.7 examples/sec; 0.072 sec/batch)
2017-05-06 21:00:25.955923: step 1620, loss = 1.76 (1545.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:00:26.783444: step 1630, loss = 1.80 (1546.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:00:27.607731: step 1640, loss = 1.98 (1552.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:00:28.431268: step 1650, loss = 1.91 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:00:29.250445: step 1660, loss = 2.15 (1562.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:00:30.082781: step 1670, loss = 1.72 (1537.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:00:30.909708: step 1680, loss = 1.96 (1547.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:00:31.732680: step 1690, loss = 1.82 (1555.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:00:32.663762: step 1700, loss = 1.72 (1374.7 examples/sec; 0.093 sec/batch)
2017-05-06 21:00:33.387327: step 1710, loss = 1.80 (1769.1 examples/sec; 0.072 sec/batch)
2017-05-06 21:00:34.224608: step 1720, loss = 1.65 (1528.7 examples/sec; 0.084 sec/batch)
2017-05-06 21:00:35.050154: step 1730, loss = 1.85 (1550.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:00:35.862545: step 1740, loss = 1.78 (1575.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:00:36.688917: step 1750, loss = 1.60 (1548.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:00:37.502719: step 1760, loss = 2.17 (1572.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:00:38.332104: step 1770, loss = 1.70 (1543.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:00:39.154782: step 1780, loss = 1.79 (1555.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:00:39.980352: step 1790, loss = 1.72 (1550.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:00:40.904993: step 1800, loss = 1.70 (1384.3 examples/sec; 0.092 sec/batch)
2017-05-06 21:00:41.625511: step 1810, loss = 1.80 (1776.5 examples/sec; 0.072 sec/batch)
2017-05-06 21:00:42.448884: step 1820, loss = 1.95 (1554.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:00:43.282969: step 1830, loss = 1.63 (1534.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:00:44.102999: step 1840, loss = 1.90 (1560.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:00:44.925320: step 1850, loss = 1.80 (1556.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:00:45.749189: step 1860, loss = 1.84 (1553.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:00:46.584404: step 1870, loss = 1.64 (1532.5 examples/sec; 0.084 sec/batch)
2017-05-06 21:00:47.405567: step 1880, loss = 1.69 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:00:48.234816: step 1890, loss = 1.62 (1543.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:00:49.173781: step 1900, loss = 1.83 (1363.2 examples/sec; 0.094 sec/batch)
2017-05-06 21:00:49.897894: step 1910, loss = 1.75 (1767.7 examples/sec; 0.072 sec/batch)
2017-05-06 21:00:50.726809: step 1920, loss = 1.62 (1544.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:00:51.546416: step 1930, loss = 1.58 (1561.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:00:52.372740: step 1940, loss = 2.15 (1549.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:00:53.205962: step 1950, loss = 1.68 (1536.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:00:54.029458: step 1960, loss = 1.62 (1554.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:00:54.854947: step 1970, loss = 1.58 (1550.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:00:55.696377: step 1980, loss = 1.70 (1521.2 examples/sec; 0.084 sec/batch)
2017-05-06 21:00:56.494513: step 1990, loss = 1.64 (1603.7 examples/sec; 0.080 sec/batch)
2017-05-06 21:00:57.430821: step 2000, loss = 1.74 (1367.1 examples/sec; 0.094 sec/batch)
2017-05-06 21:00:58.159318: step 2010, loss = 1.58 (1757.0 examples/sec; 0.073 sec/batch)
2017-05-06 21:00:58.989920: step 2020, loss = 1.62 (1541.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:00:59.807422: step 2030, loss = 1.57 (1565.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:01:00.629699: step 2040, loss = 1.74 (1556.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:01:01.476514: step 2050, loss = 1.65 (1511.6 examples/sec; 0.085 sec/batch)
2017-05-06 21:01:02.298235: step 2060, loss = 1.70 (1557.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:01:03.122033: step 2070, loss = 1.75 (1553.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:01:03.954799: step 2080, loss = 1.59 (1537.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:01:04.775843: step 2090, loss = 1.62 (1559.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:01:05.701794: step 2100, loss = 1.52 (1382.4 examples/sec; 0.093 sec/batch)
2017-05-06 21:01:06.420805: step 2110, loss = 1.45 (1780.2 examples/sec; 0.072 sec/batch)
2017-05-06 21:01:07.241363: step 2120, loss = 1.60 (1559.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:01:08.068281: step 2130, loss = 1.57 (1547.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:01:08.887569: step 2140, loss = 1.47 (1562.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:01:09.711577: step 2150, loss = 1.73 (1553.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:01:10.537674: step 2160, loss = 1.76 (1549.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:01:11.368591: step 2170, loss = 1.39 (1540.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:01:12.186426: step 2180, loss = 1.67 (1565.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:01:13.012954: step 2190, loss = 1.65 (1548.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:01:13.963848: step 2200, loss = 1.61 (1346.1 examples/sec; 0.095 sec/batch)
2017-05-06 21:01:14.659572: step 2210, loss = 1.59 (1839.8 examples/sec; 0.070 sec/batch)
2017-05-06 21:01:15.477978: step 2220, loss = 1.60 (1564.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:01:16.301295: step 2230, loss = 1.54 (1554.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:01:17.130331: step 2240, loss = 1.58 (1544.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:01:17.956845: step 2250, loss = 1.42 (1548.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:01:18.790225: step 2260, loss = 1.52 (1535.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:01:19.607093: step 2270, loss = 1.66 (1567.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:01:20.444715: step 2280, loss = 1.73 (1528.1 examples/sec; 0.084 sec/batch)
2017-05-06 21:01:21.270503: step 2290, loss = 1.64 (1550.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:01:22.223227: step 2300, loss = 1.53 (1343.5 examples/sec; 0.095 sec/batch)
2017-05-06 21:01:22.918451: step 2310, loss = 1.45 (1841.1 examples/sec; 0.070 sec/batch)
2017-05-06 21:01:23.734110: step 2320, loss = 1.51 (1569.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:01:24.561421: step 2330, loss = 1.21 (1547.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:01:25.384896: step 2340, loss = 1.77 (1554.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:01:26.211451: step 2350, loss = 1.48 (1548.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:01:27.035337: step 2360, loss = 1.59 (1553.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:01:27.856112: step 2370, loss = 1.47 (1559.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:01:28.689563: step 2380, loss = 1.63 (1535.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:01:29.505414: step 2390, loss = 1.62 (1568.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:01:30.435913: step 2400, loss = 1.62 (1375.6 examples/sec; 0.093 sec/batch)
2017-05-06 21:01:31.157935: step 2410, loss = 1.76 (1772.8 examples/sec; 0.072 sec/batch)
2017-05-06 21:01:31.967396: step 2420, loss = 1.53 (1581.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:01:32.802038: step 2430, loss = 1.49 (1533.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:01:33.629548: step 2440, loss = 1.51 (1546.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:01:34.453714: step 2450, loss = 1.65 (1553.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:01:35.269335: step 2460, loss = 1.63 (1569.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:01:36.081103: step 2470, loss = 1.48 (1576.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:01:36.908319: step 2480, loss = 1.45 (1547.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:01:37.738607: step 2490, loss = 1.55 (1541.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:01:38.670280: step 2500, loss = 1.37 (1373.9 examples/sec; 0.093 sec/batch)
2017-05-06 21:01:39.387718: step 2510, loss = 1.19 (1784.1 examples/sec; 0.072 sec/batch)
2017-05-06 21:01:40.220170: step 2520, loss = 1.51 (1537.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:01:41.041391: step 2530, loss = 1.29 (1558.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:01:41.874942: step 2540, loss = 1.38 (1535.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:01:42.699680: step 2550, loss = 1.37 (1552.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:01:43.534137: step 2560, loss = 1.42 (1533.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:01:44.355063: step 2570, loss = 1.55 (1559.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:01:45.184204: step 2580, loss = 1.50 (1543.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:01:46.014772: step 2590, loss = 1.17 (1541.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:01:46.948019: step 2600, loss = 1.49 (1371.6 examples/sec; 0.093 sec/batch)
2017-05-06 21:01:47.670672: step 2610, loss = 1.39 (1771.2 examples/sec; 0.072 sec/batch)
2017-05-06 21:01:48.498779: step 2620, loss = 1.37 (1545.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:01:49.324954: step 2630, loss = 1.36 (1549.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:01:50.154976: step 2640, loss = 1.47 (1542.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:01:50.979663: step 2650, loss = 1.29 (1552.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:01:51.802542: step 2660, loss = 1.61 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:01:52.625115: step 2670, loss = 1.33 (1556.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:01:53.451597: step 2680, loss = 1.26 (1548.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:01:54.286808: step 2690, loss = 1.47 (1532.5 examples/sec; 0.084 sec/batch)
2017-05-06 21:01:55.207931: step 2700, loss = 1.31 (1389.6 examples/sec; 0.092 sec/batch)
2017-05-06 21:01:55.930558: step 2710, loss = 1.10 (1771.3 examples/sec; 0.072 sec/batch)
2017-05-06 21:01:56.759359: step 2720, loss = 1.50 (1544.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:01:57.594969: step 2730, loss = 1.42 (1531.8 examples/sec; 0.084 sec/batch)
2017-05-06 21:01:58.423628: step 2740, loss = 1.32 (1544.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:01:59.253477: step 2750, loss = 1.63 (1542.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:02:00.082984: step 2760, loss = 1.21 (1543.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:02:00.907145: step 2770, loss = 1.32 (1553.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:02:01.731338: step 2780, loss = 1.27 (1553.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:02:02.559996: step 2790, loss = 1.31 (1544.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:02:03.503128: step 2800, loss = 1.39 (1357.2 examples/sec; 0.094 sec/batch)
2017-05-06 21:02:04.219580: step 2810, loss = 1.15 (1786.6 examples/sec; 0.072 sec/batch)
2017-05-06 21:02:05.041443: step 2820, loss = 1.29 (1557.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:02:05.876363: step 2830, loss = 1.37 (1533.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:02:06.699299: step 2840, loss = 1.38 (1555.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:02:07.520876: step 2850, loss = 1.26 (1558.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:02:08.332987: step 2860, loss = 1.44 (1576.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:02:09.161171: step 2870, loss = 1.24 (1545.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:02:09.997412: step 2880, loss = 1.32 (1530.7 examples/sec; 0.084 sec/batch)
2017-05-06 21:02:10.825038: step 2890, loss = 1.44 (1546.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:02:11.744094: step 2900, loss = 1.21 (1392.7 examples/sec; 0.092 sec/batch)
2017-05-06 21:02:12.468696: step 2910, loss = 1.32 (1766.5 examples/sec; 0.072 sec/batch)
2017-05-06 21:02:13.292335: step 2920, loss = 1.27 (1554.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:02:14.120658: step 2930, loss = 1.44 (1545.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:02:14.943208: step 2940, loss = 1.34 (1556.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:02:15.763790: step 2950, loss = 1.20 (1559.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:02:16.583586: step 2960, loss = 1.19 (1561.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:02:17.433317: step 2970, loss = 1.30 (1506.3 examples/sec; 0.085 sec/batch)
2017-05-06 21:02:18.243721: step 2980, loss = 1.19 (1579.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:02:19.067862: step 2990, loss = 1.15 (1553.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:02:19.986664: step 3000, loss = 1.19 (1393.1 examples/sec; 0.092 sec/batch)
2017-05-06 21:02:20.706818: step 3010, loss = 1.17 (1777.4 examples/sec; 0.072 sec/batch)
2017-05-06 21:02:21.532472: step 3020, loss = 1.28 (1550.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:02:22.358112: step 3030, loss = 1.32 (1550.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:02:23.184024: step 3040, loss = 1.14 (1549.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:02:23.999905: step 3050, loss = 1.21 (1568.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:02:24.824715: step 3060, loss = 1.16 (1551.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:02:25.665616: step 3070, loss = 1.29 (1522.2 examples/sec; 0.084 sec/batch)
2017-05-06 21:02:26.478340: step 3080, loss = 1.37 (1575.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:02:27.302245: step 3090, loss = 1.45 (1553.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:02:28.221990: step 3100, loss = 1.13 (1391.7 examples/sec; 0.092 sec/batch)
2017-05-06 21:02:28.946895: step 3110, loss = 1.28 (1765.8 examples/sec; 0.072 sec/batch)
2017-05-06 21:02:29.764984: step 3120, loss = 1.25 (1564.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:02:30.592447: step 3130, loss = 1.21 (1546.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:02:31.408943: step 3140, loss = 1.34 (1567.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:02:32.230395: step 3150, loss = 1.31 (1558.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:02:33.055585: step 3160, loss = 1.21 (1551.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:02:33.879637: step 3170, loss = 1.21 (1553.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:02:34.703845: step 3180, loss = 1.18 (1553.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:02:35.522928: step 3190, loss = 1.11 (1562.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:02:36.464719: step 3200, loss = 1.36 (1359.1 examples/sec; 0.094 sec/batch)
2017-05-06 21:02:37.176790: step 3210, loss = 1.17 (1797.5 examples/sec; 0.071 sec/batch)
2017-05-06 21:02:38.004523: step 3220, loss = 1.40 (1546.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:02:38.824955: step 3230, loss = 1.25 (1560.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:02:39.649380: step 3240, loss = 1.19 (1552.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:02:40.476239: step 3250, loss = 1.19 (1548.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:02:41.295156: step 3260, loss = 1.12 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:02:42.121321: step 3270, loss = 1.13 (1549.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:02:42.945349: step 3280, loss = 1.22 (1553.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:02:43.769964: step 3290, loss = 1.15 (1552.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:02:44.702844: step 3300, loss = 1.14 (1372.1 examples/sec; 0.093 sec/batch)
2017-05-06 21:02:45.419843: step 3310, loss = 1.24 (1785.2 examples/sec; 0.072 sec/batch)
2017-05-06 21:02:46.241647: step 3320, loss = 1.09 (1557.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:02:47.067580: step 3330, loss = 1.08 (1549.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:02:47.887990: step 3340, loss = 1.22 (1560.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:02:48.716016: step 3350, loss = 1.11 (1545.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:02:49.540609: step 3360, loss = 1.30 (1552.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:02:50.382406: step 3370, loss = 1.14 (1520.6 examples/sec; 0.084 sec/batch)
2017-05-06 21:02:51.201318: step 3380, loss = 1.24 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:02:52.023238: step 3390, loss = 1.31 (1557.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:02:52.953237: step 3400, loss = 1.20 (1376.3 examples/sec; 0.093 sec/batch)
2017-05-06 21:02:53.677158: step 3410, loss = 1.23 (1768.2 examples/sec; 0.072 sec/batch)
2017-05-06 21:02:54.502348: step 3420, loss = 1.15 (1551.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:02:55.327447: step 3430, loss = 1.22 (1551.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:02:56.153076: step 3440, loss = 1.12 (1550.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:02:56.976815: step 3450, loss = 1.15 (1553.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:02:57.803072: step 3460, loss = 1.28 (1549.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:02:58.637131: step 3470, loss = 1.27 (1534.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:02:59.457390: step 3480, loss = 1.30 (1560.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:03:00.274877: step 3490, loss = 1.16 (1565.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:03:01.203685: step 3500, loss = 1.32 (1378.1 examples/sec; 0.093 sec/batch)
2017-05-06 21:03:01.925367: step 3510, loss = 1.26 (1773.7 examples/sec; 0.072 sec/batch)
2017-05-06 21:03:02.753543: step 3520, loss = 1.20 (1545.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:03:03.578670: step 3530, loss = 1.31 (1551.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:03:04.400962: step 3540, loss = 1.10 (1556.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:03:05.219829: step 3550, loss = 1.15 (1563.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:03:06.048854: step 3560, loss = 1.30 (1544.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:03:06.873746: step 3570, loss = 1.10 (1551.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:03:07.693517: step 3580, loss = 0.99 (1561.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:03:08.518239: step 3590, loss = 1.30 (1552.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:03:09.450089: step 3600, loss = 1.20 (1373.6 examples/sec; 0.093 sec/batch)
2017-05-06 21:03:10.170954: step 3610, loss = 1.09 (1775.7 examples/sec; 0.072 sec/batch)
2017-05-06 21:03:11.003126: step 3620, loss = 1.11 (1538.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:03:11.819080: step 3630, loss = 1.19 (1568.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:03:12.647557: step 3640, loss = 1.19 (1545.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:03:13.477124: step 3650, loss = 1.06 (1543.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:03:14.305846: step 3660, loss = 1.38 (1544.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:03:15.147286: step 3670, loss = 1.20 (1521.2 examples/sec; 0.084 sec/batch)
2017-05-06 21:03:15.958520: step 3680, loss = 1.14 (1577.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:03:16.790186: step 3690, loss = 1.18 (1539.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:03:17.715687: step 3700, loss = 1.20 (1383.0 examples/sec; 0.093 sec/batch)
2017-05-06 21:03:18.437613: step 3710, loss = 1.40 (1773.0 examples/sec; 0.072 sec/batch)
2017-05-06 21:03:19.266382: step 3720, loss = 1.01 (1544.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:03:20.077226: step 3730, loss = 1.16 (1578.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:03:20.907157: step 3740, loss = 1.15 (1542.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:03:21.734853: step 3750, loss = 1.28 (1546.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:03:22.563458: step 3760, loss = 1.36 (1544.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:03:23.400010: step 3770, loss = 1.10 (1530.1 examples/sec; 0.084 sec/batch)
2017-05-06 21:03:24.219904: step 3780, loss = 1.24 (1561.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:03:25.039143: step 3790, loss = 1.16 (1562.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:03:25.974327: step 3800, loss = 1.17 (1368.7 examples/sec; 0.094 sec/batch)
2017-05-06 21:03:26.693135: step 3810, loss = 1.10 (1780.7 examples/sec; 0.072 sec/batch)
2017-05-06 21:03:27.519593: step 3820, loss = 1.20 (1548.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:03:28.342446: step 3830, loss = 1.20 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:03:29.178714: step 3840, loss = 1.01 (1530.6 examples/sec; 0.084 sec/batch)
2017-05-06 21:03:29.998750: step 3850, loss = 1.28 (1560.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:03:30.823792: step 3860, loss = 1.08 (1551.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:03:31.656074: step 3870, loss = 1.04 (1537.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:03:32.490286: step 3880, loss = 1.12 (1534.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:03:33.313080: step 3890, loss = 1.10 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:03:34.245285: step 3900, loss = 1.06 (1373.1 examples/sec; 0.093 sec/batch)
2017-05-06 21:03:34.962829: step 3910, loss = 1.02 (1783.9 examples/sec; 0.072 sec/batch)
2017-05-06 21:03:35.773026: step 3920, loss = 1.08 (1579.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:03:36.600381: step 3930, loss = 0.99 (1547.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:03:37.425775: step 3940, loss = 1.15 (1550.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:03:38.256978: step 3950, loss = 0.85 (1539.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:03:39.098781: step 3960, loss = 1.06 (1520.5 examples/sec; 0.084 sec/batch)
2017-05-06 21:03:39.908679: step 3970, loss = 1.28 (1580.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:03:40.732859: step 3980, loss = 1.03 (1553.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:03:41.548138: step 3990, loss = 0.98 (1570.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:03:42.478869: step 4000, loss = 1.05 (1375.3 examples/sec; 0.093 sec/batch)
2017-05-06 21:03:43.194734: step 4010, loss = 1.21 (1788.0 examples/sec; 0.072 sec/batch)
2017-05-06 21:03:44.005194: step 4020, loss = 1.08 (1579.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:03:44.836319: step 4030, loss = 1.14 (1540.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:03:45.653029: step 4040, loss = 1.04 (1567.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:03:46.480435: step 4050, loss = 1.05 (1547.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:03:47.306917: step 4060, loss = 1.04 (1548.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:03:48.128974: step 4070, loss = 1.17 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:03:48.947615: step 4080, loss = 0.98 (1563.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:03:49.769840: step 4090, loss = 1.05 (1556.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:03:50.723234: step 4100, loss = 1.03 (1342.6 examples/sec; 0.095 sec/batch)
2017-05-06 21:03:51.428741: step 4110, loss = 1.12 (1814.3 examples/sec; 0.071 sec/batch)
2017-05-06 21:03:52.252980: step 4120, loss = 0.98 (1553.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:03:53.076832: step 4130, loss = 0.99 (1553.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:03:53.899188: step 4140, loss = 0.92 (1556.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:03:54.725741: step 4150, loss = 1.09 (1548.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:03:55.560245: step 4160, loss = 1.48 (1533.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:03:56.383133: step 4170, loss = 1.20 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:03:57.218822: step 4180, loss = 0.88 (1531.7 examples/sec; 0.084 sec/batch)
2017-05-06 21:03:58.044139: step 4190, loss = 1.20 (1550.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:03:58.974445: step 4200, loss = 1.03 (1375.9 examples/sec; 0.093 sec/batch)
2017-05-06 21:03:59.682581: step 4210, loss = 1.25 (1807.6 examples/sec; 0.071 sec/batch)
2017-05-06 21:04:00.509304: step 4220, loss = 0.92 (1548.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:04:01.331343: step 4230, loss = 1.04 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:04:02.163599: step 4240, loss = 1.07 (1538.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:04:02.984512: step 4250, loss = 1.22 (1559.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:04:03.804425: step 4260, loss = 0.99 (1561.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:04:04.628808: step 4270, loss = 1.04 (1552.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:04:05.450961: step 4280, loss = 1.03 (1556.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:04:06.283293: step 4290, loss = 0.95 (1537.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:04:07.204742: step 4300, loss = 1.06 (1389.1 examples/sec; 0.092 sec/batch)
2017-05-06 21:04:07.919168: step 4310, loss = 0.96 (1791.7 examples/sec; 0.071 sec/batch)
2017-05-06 21:04:08.748859: step 4320, loss = 1.08 (1542.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:04:09.574977: step 4330, loss = 1.02 (1549.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:04:10.403347: step 4340, loss = 1.21 (1545.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:04:11.232916: step 4350, loss = 1.04 (1543.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:04:12.059633: step 4360, loss = 1.03 (1548.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:04:12.884896: step 4370, loss = 1.20 (1551.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:04:13.715840: step 4380, loss = 0.94 (1540.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:04:14.543344: step 4390, loss = 1.14 (1546.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:04:15.459890: step 4400, loss = 1.00 (1396.6 examples/sec; 0.092 sec/batch)
2017-05-06 21:04:16.184113: step 4410, loss = 1.03 (1767.4 examples/sec; 0.072 sec/batch)
2017-05-06 21:04:17.014974: step 4420, loss = 1.02 (1540.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:04:17.849136: step 4430, loss = 1.20 (1534.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:04:18.690059: step 4440, loss = 1.27 (1522.2 examples/sec; 0.084 sec/batch)
2017-05-06 21:04:19.509385: step 4450, loss = 1.12 (1562.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:04:20.336155: step 4460, loss = 1.07 (1548.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:04:21.160662: step 4470, loss = 1.07 (1552.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:04:21.982897: step 4480, loss = 1.12 (1556.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:04:22.805150: step 4490, loss = 1.21 (1556.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:04:23.729306: step 4500, loss = 0.93 (1385.0 examples/sec; 0.092 sec/batch)
2017-05-06 21:04:24.453859: step 4510, loss = 1.20 (1766.6 examples/sec; 0.072 sec/batch)
2017-05-06 21:04:25.284280: step 4520, loss = 1.28 (1541.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:04:26.109790: step 4530, loss = 0.96 (1550.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:04:26.929338: step 4540, loss = 1.23 (1561.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:04:27.744131: step 4550, loss = 0.97 (1571.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:04:28.581146: step 4560, loss = 0.96 (1529.2 examples/sec; 0.084 sec/batch)
2017-05-06 21:04:29.406360: step 4570, loss = 1.18 (1551.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:04:30.247377: step 4580, loss = 0.98 (1522.0 examples/sec; 0.084 sec/batch)
2017-05-06 21:04:31.071935: step 4590, loss = 1.17 (1552.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:04:32.020999: step 4600, loss = 1.05 (1348.7 examples/sec; 0.095 sec/batch)
2017-05-06 21:04:32.727236: step 4610, loss = 1.31 (1812.5 examples/sec; 0.071 sec/batch)
2017-05-06 21:04:33.560364: step 4620, loss = 1.08 (1536.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:04:34.388539: step 4630, loss = 0.98 (1545.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:04:35.211912: step 4640, loss = 1.01 (1554.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:04:36.035824: step 4650, loss = 1.05 (1553.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:04:36.859842: step 4660, loss = 1.10 (1553.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:04:37.683013: step 4670, loss = 0.96 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:04:38.509969: step 4680, loss = 1.27 (1547.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:04:39.331390: step 4690, loss = 1.03 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:04:40.262941: step 4700, loss = 0.85 (1374.1 examples/sec; 0.093 sec/batch)
2017-05-06 21:04:40.986700: step 4710, loss = 1.07 (1768.5 examples/sec; 0.072 sec/batch)
2017-05-06 21:04:41.804252: step 4720, loss = 1.17 (1565.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:04:42.628079: step 4730, loss = 0.96 (1553.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:04:43.457348: step 4740, loss = 1.09 (1543.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:04:44.281036: step 4750, loss = 1.09 (1554.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:04:45.103962: step 4760, loss = 1.10 (1555.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:04:45.930158: step 4770, loss = 1.05 (1549.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:04:46.755981: step 4780, loss = 1.12 (1550.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:04:47.583874: step 4790, loss = 1.00 (1546.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:04:48.516393: step 4800, loss = 0.99 (1372.6 examples/sec; 0.093 sec/batch)
2017-05-06 21:04:49.224033: step 4810, loss = 1.21 (1808.8 examples/sec; 0.071 sec/batch)
2017-05-06 21:04:50.047458: step 4820, loss = 0.85 (1554.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:04:50.873357: step 4830, loss = 1.06 (1549.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:04:51.696192: step 4840, loss = 0.86 (1555.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:04:52.527196: step 4850, loss = 1.01 (1540.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:04:53.363175: step 4860, loss = 1.02 (1531.1 examples/sec; 0.084 sec/batch)
2017-05-06 21:04:54.185410: step 4870, loss = 0.94 (1556.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:04:55.005446: step 4880, loss = 1.23 (1560.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:04:55.824864: step 4890, loss = 1.04 (1562.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:04:56.756861: step 4900, loss = 0.99 (1373.4 examples/sec; 0.093 sec/batch)
2017-05-06 21:04:57.483818: step 4910, loss = 1.07 (1760.8 examples/sec; 0.073 sec/batch)
2017-05-06 21:04:58.307595: step 4920, loss = 0.99 (1553.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:04:59.130732: step 4930, loss = 1.08 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:04:59.951103: step 4940, loss = 1.01 (1560.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:05:00.796128: step 4950, loss = 1.01 (1514.8 examples/sec; 0.085 sec/batch)
2017-05-06 21:05:01.615326: step 4960, loss = 1.08 (1562.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:05:02.441975: step 4970, loss = 0.96 (1548.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:05:03.263418: step 4980, loss = 1.03 (1558.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:05:04.080380: step 4990, loss = 0.96 (1566.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:05:05.011634: step 5000, loss = 1.03 (1374.5 examples/sec; 0.093 sec/batch)
2017-05-06 21:05:05.733449: step 5010, loss = 0.87 (1773.3 examples/sec; 0.072 sec/batch)
2017-05-06 21:05:06.558479: step 5020, loss = 1.08 (1551.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:05:07.380731: step 5030, loss = 1.29 (1556.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:05:08.206106: step 5040, loss = 1.19 (1550.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:05:09.044760: step 5050, loss = 1.11 (1526.2 examples/sec; 0.084 sec/batch)
2017-05-06 21:05:09.873224: step 5060, loss = 1.02 (1545.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:05:10.709789: step 5070, loss = 1.15 (1530.1 examples/sec; 0.084 sec/batch)
2017-05-06 21:05:11.535229: step 5080, loss = 0.96 (1550.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:05:12.357652: step 5090, loss = 0.93 (1556.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:05:13.282859: step 5100, loss = 1.01 (1383.5 examples/sec; 0.093 sec/batch)
2017-05-06 21:05:14.015308: step 5110, loss = 0.93 (1747.6 examples/sec; 0.073 sec/batch)
2017-05-06 21:05:14.842810: step 5120, loss = 1.00 (1546.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:05:15.667191: step 5130, loss = 0.92 (1552.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:05:16.490587: step 5140, loss = 1.13 (1554.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:05:17.317917: step 5150, loss = 1.18 (1547.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:05:18.148718: step 5160, loss = 0.97 (1540.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:05:18.975718: step 5170, loss = 0.90 (1547.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:05:19.787295: step 5180, loss = 1.13 (1577.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:05:20.607758: step 5190, loss = 1.16 (1560.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:05:21.536182: step 5200, loss = 0.99 (1378.7 examples/sec; 0.093 sec/batch)
2017-05-06 21:05:22.259252: step 5210, loss = 0.97 (1770.2 examples/sec; 0.072 sec/batch)
2017-05-06 21:05:23.080774: step 5220, loss = 0.81 (1558.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:05:23.896168: step 5230, loss = 1.02 (1569.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:05:24.718087: step 5240, loss = 1.11 (1557.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:05:25.560161: step 5250, loss = 0.91 (1520.1 examples/sec; 0.084 sec/batch)
2017-05-06 21:05:26.384283: step 5260, loss = 1.01 (1553.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:05:27.208349: step 5270, loss = 1.25 (1553.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:05:28.028783: step 5280, loss = 1.03 (1560.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:05:28.860320: step 5290, loss = 0.88 (1539.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:05:29.792083: step 5300, loss = 1.20 (1373.7 examples/sec; 0.093 sec/batch)
2017-05-06 21:05:30.517520: step 5310, loss = 0.95 (1764.4 examples/sec; 0.073 sec/batch)
2017-05-06 21:05:31.338780: step 5320, loss = 0.88 (1558.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:05:32.167749: step 5330, loss = 0.94 (1544.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:05:32.996187: step 5340, loss = 1.17 (1545.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:05:33.831235: step 5350, loss = 0.92 (1532.9 examples/sec; 0.084 sec/batch)
2017-05-06 21:05:34.663620: step 5360, loss = 1.07 (1537.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:05:35.495327: step 5370, loss = 0.96 (1539.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:05:36.314537: step 5380, loss = 1.02 (1562.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:05:37.140418: step 5390, loss = 1.02 (1549.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:05:38.071994: step 5400, loss = 1.21 (1374.0 examples/sec; 0.093 sec/batch)
2017-05-06 21:05:38.788774: step 5410, loss = 1.13 (1785.8 examples/sec; 0.072 sec/batch)
2017-05-06 21:05:39.605534: step 5420, loss = 0.80 (1567.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:05:40.433416: step 5430, loss = 0.88 (1546.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:05:41.258202: step 5440, loss = 1.04 (1551.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:05:42.084538: step 5450, loss = 0.77 (1549.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:05:42.902000: step 5460, loss = 1.20 (1565.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:05:43.728742: step 5470, loss = 1.14 (1548.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:05:44.547456: step 5480, loss = 0.85 (1563.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:05:45.378066: step 5490, loss = 0.96 (1541.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:05:46.314967: step 5500, loss = 0.88 (1366.2 examples/sec; 0.094 sec/batch)
2017-05-06 21:05:47.027099: step 5510, loss = 0.86 (1797.4 examples/sec; 0.071 sec/batch)
2017-05-06 21:05:47.850579: step 5520, loss = 1.04 (1554.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:05:48.682314: step 5530, loss = 1.03 (1538.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:05:49.511809: step 5540, loss = 1.09 (1543.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:05:50.345296: step 5550, loss = 0.86 (1535.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:05:51.168591: step 5560, loss = 0.94 (1554.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:05:51.992267: step 5570, loss = 0.93 (1554.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:05:52.824119: step 5580, loss = 0.95 (1538.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:05:53.647752: step 5590, loss = 1.16 (1554.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:05:54.586979: step 5600, loss = 0.80 (1362.8 examples/sec; 0.094 sec/batch)
2017-05-06 21:05:55.302251: step 5610, loss = 0.88 (1789.5 examples/sec; 0.072 sec/batch)
2017-05-06 21:05:56.128001: step 5620, loss = 0.87 (1550.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:05:56.958083: step 5630, loss = 1.00 (1542.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:05:57.787400: step 5640, loss = 0.92 (1543.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:05:58.621113: step 5650, loss = 0.88 (1535.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:05:59.439639: step 5660, loss = 0.91 (1563.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:06:00.267645: step 5670, loss = 0.97 (1545.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:06:01.096907: step 5680, loss = 0.92 (1543.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:06:01.931394: step 5690, loss = 0.88 (1533.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:06:02.856221: step 5700, loss = 1.28 (1384.0 examples/sec; 0.092 sec/batch)
2017-05-06 21:06:03.574833: step 5710, loss = 1.09 (1781.2 examples/sec; 0.072 sec/batch)
2017-05-06 21:06:04.395456: step 5720, loss = 0.96 (1559.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:06:05.218223: step 5730, loss = 1.13 (1555.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:06:06.044810: step 5740, loss = 1.13 (1548.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:06:06.866220: step 5750, loss = 1.14 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:06:07.679539: step 5760, loss = 0.93 (1573.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:06:08.503195: step 5770, loss = 1.07 (1554.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:06:09.330080: step 5780, loss = 1.00 (1548.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:06:10.152810: step 5790, loss = 0.97 (1555.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:06:11.090920: step 5800, loss = 0.91 (1364.4 examples/sec; 0.094 sec/batch)
2017-05-06 21:06:11.806752: step 5810, loss = 0.94 (1788.1 examples/sec; 0.072 sec/batch)
2017-05-06 21:06:12.630774: step 5820, loss = 0.95 (1553.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:06:13.456920: step 5830, loss = 0.86 (1549.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:06:14.286706: step 5840, loss = 0.80 (1542.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:06:15.115766: step 5850, loss = 0.72 (1543.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:06:15.940458: step 5860, loss = 1.11 (1552.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:06:16.768130: step 5870, loss = 1.03 (1546.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:06:17.592707: step 5880, loss = 1.06 (1552.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:06:18.420328: step 5890, loss = 0.90 (1546.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:06:19.352797: step 5900, loss = 0.85 (1372.7 examples/sec; 0.093 sec/batch)
2017-05-06 21:06:20.067024: step 5910, loss = 1.13 (1792.1 examples/sec; 0.071 sec/batch)
2017-05-06 21:06:20.894685: step 5920, loss = 0.99 (1546.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:06:21.715227: step 5930, loss = 1.01 (1559.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:06:22.578567: step 5940, loss = 0.99 (1482.6 examples/sec; 0.086 sec/batch)
2017-05-06 21:06:23.383126: step 5950, loss = 1.02 (1590.9 examples/sec; 0.080 sec/batch)
2017-05-06 21:06:24.200082: step 5960, loss = 1.00 (1566.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:06:25.033725: step 5970, loss = 1.08 (1535.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:06:25.860692: step 5980, loss = 0.99 (1547.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:06:26.691494: step 5990, loss = 0.93 (1540.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:06:27.614297: step 6000, loss = 1.10 (1387.1 examples/sec; 0.092 sec/batch)
2017-05-06 21:06:28.345361: step 6010, loss = 0.94 (1750.9 examples/sec; 0.073 sec/batch)
2017-05-06 21:06:29.173757: step 6020, loss = 1.07 (1545.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:06:29.997295: step 6030, loss = 0.93 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:06:30.839565: step 6040, loss = 0.91 (1519.7 examples/sec; 0.084 sec/batch)
2017-05-06 21:06:31.651070: step 6050, loss = 1.05 (1577.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:06:32.481746: step 6060, loss = 0.92 (1540.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:06:33.301846: step 6070, loss = 0.97 (1560.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:06:34.129826: step 6080, loss = 1.00 (1545.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:06:34.954613: step 6090, loss = 0.98 (1551.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:06:35.870580: step 6100, loss = 0.97 (1397.4 examples/sec; 0.092 sec/batch)
2017-05-06 21:06:36.605344: step 6110, loss = 0.84 (1742.1 examples/sec; 0.073 sec/batch)
2017-05-06 21:06:37.433670: step 6120, loss = 1.10 (1545.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:06:38.260002: step 6130, loss = 1.32 (1549.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:06:39.085452: step 6140, loss = 0.96 (1550.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:06:39.901831: step 6150, loss = 0.94 (1567.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:06:40.723220: step 6160, loss = 1.05 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:06:41.551881: step 6170, loss = 1.08 (1544.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:06:42.374076: step 6180, loss = 1.02 (1556.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:06:43.193859: step 6190, loss = 1.08 (1561.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:06:44.132167: step 6200, loss = 1.03 (1364.2 examples/sec; 0.094 sec/batch)
2017-05-06 21:06:44.836459: step 6210, loss = 0.86 (1817.4 examples/sec; 0.070 sec/batch)
2017-05-06 21:06:45.667418: step 6220, loss = 1.06 (1540.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:06:46.498302: step 6230, loss = 0.90 (1540.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:06:47.325416: step 6240, loss = 1.09 (1547.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:06:48.158970: step 6250, loss = 1.08 (1535.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:06:48.985865: step 6260, loss = 0.96 (1548.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:06:49.805425: step 6270, loss = 0.91 (1561.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:06:50.630186: step 6280, loss = 0.98 (1552.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:06:51.457880: step 6290, loss = 0.89 (1546.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:06:52.397619: step 6300, loss = 0.94 (1362.1 examples/sec; 0.094 sec/batch)
2017-05-06 21:06:53.093529: step 6310, loss = 0.99 (1839.3 examples/sec; 0.070 sec/batch)
2017-05-06 21:06:53.922242: step 6320, loss = 0.94 (1544.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:06:54.736956: step 6330, loss = 0.83 (1571.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:06:55.563338: step 6340, loss = 1.01 (1548.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:06:56.386410: step 6350, loss = 1.01 (1555.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:06:57.208566: step 6360, loss = 0.97 (1556.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:06:58.035091: step 6370, loss = 1.06 (1548.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:06:58.863031: step 6380, loss = 0.95 (1546.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:06:59.684923: step 6390, loss = 0.90 (1557.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:07:00.607756: step 6400, loss = 1.02 (1387.0 examples/sec; 0.092 sec/batch)
2017-05-06 21:07:01.329808: step 6410, loss = 0.97 (1772.7 examples/sec; 0.072 sec/batch)
2017-05-06 21:07:02.148142: step 6420, loss = 0.94 (1564.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:07:02.971190: step 6430, loss = 0.98 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:07:03.797549: step 6440, loss = 0.92 (1549.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:07:04.627176: step 6450, loss = 0.98 (1542.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:07:05.451065: step 6460, loss = 0.82 (1553.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:07:06.281092: step 6470, loss = 0.99 (1542.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:07:07.109788: step 6480, loss = 0.87 (1544.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:07:07.928370: step 6490, loss = 0.81 (1563.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:07:08.861244: step 6500, loss = 1.08 (1372.1 examples/sec; 0.093 sec/batch)
2017-05-06 21:07:09.583947: step 6510, loss = 1.04 (1771.1 examples/sec; 0.072 sec/batch)
2017-05-06 21:07:10.404839: step 6520, loss = 1.00 (1559.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:07:11.226908: step 6530, loss = 0.90 (1557.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:07:12.059406: step 6540, loss = 1.02 (1537.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:07:12.881959: step 6550, loss = 1.01 (1556.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:07:13.707472: step 6560, loss = 0.98 (1550.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:07:14.528005: step 6570, loss = 0.93 (1560.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:07:15.361019: step 6580, loss = 0.89 (1536.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:07:16.180746: step 6590, loss = 0.87 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:07:17.113530: step 6600, loss = 0.94 (1372.2 examples/sec; 0.093 sec/batch)
2017-05-06 21:07:17.836976: step 6610, loss = 0.77 (1769.3 examples/sec; 0.072 sec/batch)
2017-05-06 21:07:18.659883: step 6620, loss = 0.97 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:07:19.482162: step 6630, loss = 1.04 (1556.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:07:20.308478: step 6640, loss = 1.04 (1549.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:07:21.134727: step 6650, loss = 0.93 (1549.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:07:21.961279: step 6660, loss = 0.93 (1548.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:07:22.791748: step 6670, loss = 1.07 (1541.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:07:23.616532: step 6680, loss = 1.02 (1551.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:07:24.443788: step 6690, loss = 1.08 (1547.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:07:25.370729: step 6700, loss = 1.05 (1380.9 examples/sec; 0.093 sec/batch)
2017-05-06 21:07:26.103130: step 6710, loss = 1.00 (1747.7 examples/sec; 0.073 sec/batch)
2017-05-06 21:07:26.932187: step 6720, loss = 0.85 (1543.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:07:27.754728: step 6730, loss = 1.03 (1556.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:07:28.582400: step 6740, loss = 1.07 (1546.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:07:29.406586: step 6750, loss = 0.81 (1553.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:07:30.236143: step 6760, loss = 1.19 (1543.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:07:31.064994: step 6770, loss = 0.90 (1544.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:07:31.892133: step 6780, loss = 1.06 (1547.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:07:32.715652: step 6790, loss = 0.96 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:07:33.644383: step 6800, loss = 0.93 (1378.2 examples/sec; 0.093 sec/batch)
2017-05-06 21:07:34.376824: step 6810, loss = 1.14 (1747.6 examples/sec; 0.073 sec/batch)
2017-05-06 21:07:35.197440: step 6820, loss = 0.90 (1559.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:07:36.017654: step 6830, loss = 0.97 (1560.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:07:36.846188: step 6840, loss = 0.82 (1544.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:07:37.676287: step 6850, loss = 1.01 (1542.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:07:38.501706: step 6860, loss = 0.89 (1550.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:07:39.325861: step 6870, loss = 0.96 (1553.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:07:40.139517: step 6880, loss = 0.97 (1573.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:07:40.977675: step 6890, loss = 1.11 (1527.2 examples/sec; 0.084 sec/batch)
2017-05-06 21:07:41.908157: step 6900, loss = 1.06 (1375.6 examples/sec; 0.093 sec/batch)
2017-05-06 21:07:42.640613: step 6910, loss = 1.13 (1747.5 examples/sec; 0.073 sec/batch)
2017-05-06 21:07:43.459736: step 6920, loss = 1.04 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:07:44.297138: step 6930, loss = 0.82 (1528.5 examples/sec; 0.084 sec/batch)
2017-05-06 21:07:45.106974: step 6940, loss = 0.78 (1580.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:07:45.931807: step 6950, loss = 0.86 (1551.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:07:46.757463: step 6960, loss = 0.95 (1550.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:07:47.570065: step 6970, loss = 1.21 (1575.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:07:48.397232: step 6980, loss = 0.97 (1547.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:07:49.225945: step 6990, loss = 0.82 (1544.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:07:50.146927: step 7000, loss = 0.84 (1389.8 examples/sec; 0.092 sec/batch)
2017-05-06 21:07:50.872647: step 7010, loss = 0.98 (1763.8 examples/sec; 0.073 sec/batch)
2017-05-06 21:07:51.685609: step 7020, loss = 0.80 (1574.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:07:52.519491: step 7030, loss = 1.07 (1535.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:07:53.333024: step 7040, loss = 1.08 (1573.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:07:54.160344: step 7050, loss = 0.87 (1547.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:07:54.979745: step 7060, loss = 1.17 (1562.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:07:55.804473: step 7070, loss = 0.93 (1552.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:07:56.626853: step 7080, loss = 1.00 (1556.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:07:57.450828: step 7090, loss = 0.90 (1553.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:07:58.378693: step 7100, loss = 1.02 (1379.5 examples/sec; 0.093 sec/batch)
2017-05-06 21:07:59.103234: step 7110, loss = 0.97 (1766.7 examples/sec; 0.072 sec/batch)
2017-05-06 21:07:59.923747: step 7120, loss = 1.04 (1560.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:08:00.759106: step 7130, loss = 0.97 (1532.3 examples/sec; 0.084 sec/batch)
2017-05-06 21:08:01.591011: step 7140, loss = 0.94 (1538.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:02.422126: step 7150, loss = 0.94 (1540.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:03.243989: step 7160, loss = 0.81 (1557.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:08:04.066739: step 7170, loss = 1.26 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:08:04.891667: step 7180, loss = 1.00 (1551.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:08:05.719811: step 7190, loss = 0.97 (1545.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:06.642680: step 7200, loss = 0.99 (1387.0 examples/sec; 0.092 sec/batch)
2017-05-06 21:08:07.372592: step 7210, loss = 1.04 (1753.6 examples/sec; 0.073 sec/batch)
2017-05-06 21:08:08.199792: step 7220, loss = 1.04 (1547.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:09.039666: step 7230, loss = 0.86 (1524.1 examples/sec; 0.084 sec/batch)
2017-05-06 21:08:09.872925: step 7240, loss = 0.88 (1536.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:10.700803: step 7250, loss = 1.02 (1546.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:11.518580: step 7260, loss = 0.95 (1565.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:08:12.345293: step 7270, loss = 0.84 (1548.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:13.201976: step 7280, loss = 0.91 (1494.1 examples/sec; 0.086 sec/batch)
2017-05-06 21:08:14.028469: step 7290, loss = 0.74 (1548.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:14.962114: step 7300, loss = 0.92 (1371.0 examples/sec; 0.093 sec/batch)
2017-05-06 21:08:15.676506: step 7310, loss = 0.79 (1791.7 examples/sec; 0.071 sec/batch)
2017-05-06 21:08:16.509440: step 7320, loss = 1.19 (1536.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:17.337629: step 7330, loss = 0.82 (1545.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:18.165794: step 7340, loss = 0.79 (1545.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:18.982636: step 7350, loss = 0.98 (1567.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:08:19.811596: step 7360, loss = 1.05 (1544.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:20.634027: step 7370, loss = 0.84 (1556.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:08:21.466954: step 7380, loss = 0.91 (1536.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:22.298180: step 7390, loss = 1.01 (1539.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:23.223499: step 7400, loss = 1.08 (1383.3 examples/sec; 0.093 sec/batch)
2017-05-06 21:08:23.944526: step 7410, loss = 0.92 (1775.3 examples/sec; 0.072 sec/batch)
2017-05-06 21:08:24.768347: step 7420, loss = 0.88 (1553.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:08:25.594037: step 7430, loss = 0.75 (1550.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:26.419018: step 7440, loss = 0.93 (1551.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:08:27.255352: step 7450, loss = 1.12 (1530.5 examples/sec; 0.084 sec/batch)
2017-05-06 21:08:28.074520: step 7460, loss = 0.89 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:08:28.902207: step 7470, loss = 0.78 (1546.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:29.736211: step 7480, loss = 0.90 (1534.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:30.563317: step 7490, loss = 1.06 (1547.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:31.494997: step 7500, loss = 0.77 (1373.8 examples/sec; 0.093 sec/batch)
2017-05-06 21:08:32.215506: step 7510, loss = 0.87 (1776.5 examples/sec; 0.072 sec/batch)
2017-05-06 21:08:33.041937: step 7520, loss = 0.83 (1548.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:33.871203: step 7530, loss = 0.95 (1543.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:34.710606: step 7540, loss = 0.80 (1524.9 examples/sec; 0.084 sec/batch)
2017-05-06 21:08:35.533274: step 7550, loss = 0.88 (1555.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:08:36.359886: step 7560, loss = 0.95 (1548.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:37.191196: step 7570, loss = 0.82 (1539.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:38.022699: step 7580, loss = 0.93 (1539.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:38.852462: step 7590, loss = 0.89 (1542.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:39.781176: step 7600, loss = 0.91 (1378.2 examples/sec; 0.093 sec/batch)
2017-05-06 21:08:40.501976: step 7610, loss = 1.04 (1775.8 examples/sec; 0.072 sec/batch)
2017-05-06 21:08:41.337167: step 7620, loss = 0.92 (1532.6 examples/sec; 0.084 sec/batch)
2017-05-06 21:08:42.177765: step 7630, loss = 0.78 (1522.7 examples/sec; 0.084 sec/batch)
2017-05-06 21:08:43.004784: step 7640, loss = 1.08 (1547.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:43.823088: step 7650, loss = 1.13 (1564.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:08:44.647789: step 7660, loss = 0.83 (1552.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:08:45.485985: step 7670, loss = 0.93 (1527.1 examples/sec; 0.084 sec/batch)
2017-05-06 21:08:46.314302: step 7680, loss = 0.96 (1545.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:47.145071: step 7690, loss = 0.76 (1540.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:48.071283: step 7700, loss = 1.03 (1381.9 examples/sec; 0.093 sec/batch)
2017-05-06 21:08:48.803483: step 7710, loss = 0.87 (1748.1 examples/sec; 0.073 sec/batch)
2017-05-06 21:08:49.628550: step 7720, loss = 0.77 (1551.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:50.460313: step 7730, loss = 0.84 (1538.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:51.290081: step 7740, loss = 1.11 (1542.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:52.108759: step 7750, loss = 0.79 (1563.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:08:52.941226: step 7760, loss = 1.03 (1537.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:53.781766: step 7770, loss = 1.14 (1522.8 examples/sec; 0.084 sec/batch)
2017-05-06 21:08:54.611160: step 7780, loss = 1.06 (1543.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:55.441002: step 7790, loss = 0.93 (1542.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:56.375095: step 7800, loss = 0.86 (1370.3 examples/sec; 0.093 sec/batch)
2017-05-06 21:08:57.096650: step 7810, loss = 0.84 (1773.9 examples/sec; 0.072 sec/batch)
2017-05-06 21:08:57.930321: step 7820, loss = 0.95 (1535.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:58.761467: step 7830, loss = 0.78 (1540.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:08:59.582849: step 7840, loss = 0.91 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:09:00.411413: step 7850, loss = 1.06 (1544.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:09:01.245021: step 7860, loss = 0.83 (1535.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:09:02.078538: step 7870, loss = 0.89 (1535.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:09:02.901136: step 7880, loss = 0.81 (1556.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:09:03.729807: step 7890, loss = 0.91 (1544.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:09:04.663987: step 7900, loss = 0.90 (1370.2 examples/sec; 0.093 sec/batch)
2017-05-06 21:09:05.390138: step 7910, loss = 0.92 (1762.7 examples/sec; 0.073 sec/batch)
2017-05-06 21:09:06.239476: step 7920, loss = 0.97 (1507.1 examples/sec; 0.085 sec/batch)
2017-05-06 21:09:07.046835: step 7930, loss = 0.89 (1585.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:09:07.871137: step 7940, loss = 1.10 (1552.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:09:08.694051: step 7950, loss = 0.91 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:09:09.522514: step 7960, loss = 0.79 (1545.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:09:10.355393: step 7970, loss = 0.98 (1536.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:09:11.178963: step 7980, loss = 1.15 (1554.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:09:11.997850: step 7990, loss = 0.97 (1563.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:09:12.934285: step 8000, loss = 1.09 (1366.9 examples/sec; 0.094 sec/batch)
2017-05-06 21:09:13.658906: step 8010, loss = 0.95 (1766.4 examples/sec; 0.072 sec/batch)
2017-05-06 21:09:14.481864: step 8020, loss = 0.89 (1555.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:09:15.304365: step 8030, loss = 0.81 (1556.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:09:16.123268: step 8040, loss = 0.87 (1563.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:09:16.947282: step 8050, loss = 0.92 (1553.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:09:17.774311: step 8060, loss = 0.93 (1547.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:09:18.602985: step 8070, loss = 0.92 (1544.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:09:19.424897: step 8080, loss = 0.93 (1557.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:09:20.247809: step 8090, loss = 0.80 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:09:21.179803: step 8100, loss = 0.95 (1373.4 examples/sec; 0.093 sec/batch)
2017-05-06 21:09:21.901965: step 8110, loss = 0.86 (1772.5 examples/sec; 0.072 sec/batch)
2017-05-06 21:09:22.735173: step 8120, loss = 0.97 (1536.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:09:23.543988: step 8130, loss = 0.88 (1582.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:09:24.368203: step 8140, loss = 0.89 (1553.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:09:25.190039: step 8150, loss = 0.95 (1557.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:09:26.018582: step 8160, loss = 0.93 (1544.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:09:26.842594: step 8170, loss = 0.92 (1553.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:09:27.661318: step 8180, loss = 1.02 (1563.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:09:28.486477: step 8190, loss = 1.13 (1551.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:09:29.417279: step 8200, loss = 0.86 (1375.2 examples/sec; 0.093 sec/batch)
2017-05-06 21:09:30.140663: step 8210, loss = 0.81 (1769.5 examples/sec; 0.072 sec/batch)
2017-05-06 21:09:30.965742: step 8220, loss = 0.80 (1551.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:09:31.779906: step 8230, loss = 0.80 (1572.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:09:32.603561: step 8240, loss = 0.88 (1554.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:09:33.425381: step 8250, loss = 1.00 (1557.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:09:34.247208: step 8260, loss = 0.87 (1557.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:09:35.069328: step 8270, loss = 1.11 (1556.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:09:35.894333: step 8280, loss = 0.90 (1551.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:09:36.710731: step 8290, loss = 0.96 (1567.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:09:37.644602: step 8300, loss = 0.99 (1370.6 examples/sec; 0.093 sec/batch)
2017-05-06 21:09:38.373025: step 8310, loss = 1.09 (1757.2 examples/sec; 0.073 sec/batch)
2017-05-06 21:09:39.197027: step 8320, loss = 0.88 (1553.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:09:40.012495: step 8330, loss = 0.87 (1569.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:09:40.837609: step 8340, loss = 0.90 (1551.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:09:41.660379: step 8350, loss = 0.93 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:09:42.485442: step 8360, loss = 1.13 (1551.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:09:43.316068: step 8370, loss = 0.97 (1541.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:09:44.142858: step 8380, loss = 0.93 (1548.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:09:44.964302: step 8390, loss = 0.84 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:09:45.893177: step 8400, loss = 1.03 (1378.0 examples/sec; 0.093 sec/batch)
2017-05-06 21:09:46.620763: step 8410, loss = 1.00 (1759.2 examples/sec; 0.073 sec/batch)
2017-05-06 21:09:47.445819: step 8420, loss = 1.02 (1551.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:09:48.272977: step 8430, loss = 1.08 (1547.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:09:49.108862: step 8440, loss = 0.79 (1531.3 examples/sec; 0.084 sec/batch)
2017-05-06 21:09:49.935256: step 8450, loss = 1.02 (1548.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:09:50.765795: step 8460, loss = 0.75 (1541.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:09:51.582211: step 8470, loss = 1.05 (1567.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:09:52.399214: step 8480, loss = 1.01 (1566.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:09:53.217510: step 8490, loss = 0.98 (1564.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:09:54.140445: step 8500, loss = 0.93 (1386.9 examples/sec; 0.092 sec/batch)
2017-05-06 21:09:54.865812: step 8510, loss = 1.02 (1764.6 examples/sec; 0.073 sec/batch)
2017-05-06 21:09:55.693375: step 8520, loss = 0.93 (1546.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:09:56.509727: step 8530, loss = 0.82 (1568.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:09:57.331980: step 8540, loss = 0.98 (1556.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:09:58.155012: step 8550, loss = 1.02 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:09:58.977774: step 8560, loss = 0.89 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:09:59.793177: step 8570, loss = 0.86 (1569.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:10:00.614435: step 8580, loss = 1.04 (1558.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:10:01.444434: step 8590, loss = 0.87 (1542.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:10:02.385746: step 8600, loss = 1.01 (1359.8 examples/sec; 0.094 sec/batch)
2017-05-06 21:10:03.110030: step 8610, loss = 0.96 (1767.3 examples/sec; 0.072 sec/batch)
2017-05-06 21:10:03.928651: step 8620, loss = 0.72 (1563.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:10:04.756681: step 8630, loss = 0.90 (1545.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:10:05.575039: step 8640, loss = 0.91 (1564.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:10:06.395509: step 8650, loss = 1.17 (1560.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:10:07.222090: step 8660, loss = 0.99 (1548.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:10:08.036930: step 8670, loss = 0.94 (1570.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:10:08.860005: step 8680, loss = 0.78 (1555.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:10:09.681799: step 8690, loss = 0.80 (1557.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:10:10.614153: step 8700, loss = 0.88 (1372.9 examples/sec; 0.093 sec/batch)
2017-05-06 21:10:11.339772: step 8710, loss = 0.94 (1764.0 examples/sec; 0.073 sec/batch)
2017-05-06 21:10:12.160366: step 8720, loss = 1.09 (1559.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:10:12.986410: step 8730, loss = 0.94 (1549.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:10:13.811427: step 8740, loss = 0.95 (1551.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:10:14.635950: step 8750, loss = 0.78 (1552.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:10:15.451712: step 8760, loss = 1.04 (1569.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:10:16.277200: step 8770, loss = 0.93 (1550.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:10:17.107550: step 8780, loss = 0.73 (1541.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:10:17.924173: step 8790, loss = 0.93 (1567.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:10:18.854180: step 8800, loss = 0.78 (1376.3 examples/sec; 0.093 sec/batch)
2017-05-06 21:10:19.579875: step 8810, loss = 1.01 (1763.8 examples/sec; 0.073 sec/batch)
2017-05-06 21:10:20.405029: step 8820, loss = 0.80 (1551.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:10:21.229728: step 8830, loss = 1.08 (1552.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:10:22.054011: step 8840, loss = 0.91 (1552.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:10:22.879525: step 8850, loss = 0.93 (1550.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:10:23.699960: step 8860, loss = 1.04 (1560.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:10:24.528178: step 8870, loss = 0.99 (1545.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:10:25.353406: step 8880, loss = 0.81 (1551.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:10:26.185085: step 8890, loss = 0.98 (1539.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:10:27.110281: step 8900, loss = 0.92 (1383.5 examples/sec; 0.093 sec/batch)
2017-05-06 21:10:27.844917: step 8910, loss = 0.87 (1742.4 examples/sec; 0.073 sec/batch)
2017-05-06 21:10:28.647689: step 8920, loss = 0.99 (1594.5 examples/sec; 0.080 sec/batch)
2017-05-06 21:10:29.471949: step 8930, loss = 0.78 (1552.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:10:30.304087: step 8940, loss = 0.96 (1538.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:10:31.127113: step 8950, loss = 0.89 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:10:31.945860: step 8960, loss = 0.88 (1563.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:10:32.767184: step 8970, loss = 0.94 (1558.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:10:33.593581: step 8980, loss = 0.94 (1548.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:10:34.421809: step 8990, loss = 0.95 (1545.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:10:35.357950: step 9000, loss = 0.82 (1367.3 examples/sec; 0.094 sec/batch)
2017-05-06 21:10:36.073785: step 9010, loss = 1.00 (1788.1 examples/sec; 0.072 sec/batch)
2017-05-06 21:10:36.897813: step 9020, loss = 0.89 (1553.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:10:37.726593: step 9030, loss = 1.06 (1544.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:10:38.557670: step 9040, loss = 0.96 (1540.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:10:39.378701: step 9050, loss = 0.86 (1559.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:10:40.193945: step 9060, loss = 1.01 (1570.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:10:41.019386: step 9070, loss = 0.89 (1550.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:10:41.850634: step 9080, loss = 0.86 (1539.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:10:42.669610: step 9090, loss = 0.86 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:10:43.603005: step 9100, loss = 0.99 (1371.3 examples/sec; 0.093 sec/batch)
2017-05-06 21:10:44.329275: step 9110, loss = 0.79 (1762.4 examples/sec; 0.073 sec/batch)
2017-05-06 21:10:45.154079: step 9120, loss = 0.89 (1551.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:10:45.981155: step 9130, loss = 0.90 (1547.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:10:46.802330: step 9140, loss = 0.97 (1558.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:10:47.621474: step 9150, loss = 0.87 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:10:48.443776: step 9160, loss = 0.94 (1556.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:10:49.270815: step 9170, loss = 0.78 (1547.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:10:50.088648: step 9180, loss = 0.89 (1565.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:10:50.915312: step 9190, loss = 0.93 (1548.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:10:51.840736: step 9200, loss = 0.87 (1383.1 examples/sec; 0.093 sec/batch)
2017-05-06 21:10:52.558119: step 9210, loss = 0.79 (1784.3 examples/sec; 0.072 sec/batch)
2017-05-06 21:10:53.387728: step 9220, loss = 0.98 (1542.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:10:54.217283: step 9230, loss = 0.88 (1543.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:10:55.046924: step 9240, loss = 0.89 (1542.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:10:55.861533: step 9250, loss = 0.84 (1571.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:10:56.692295: step 9260, loss = 0.77 (1540.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:10:57.527840: step 9270, loss = 1.12 (1531.9 examples/sec; 0.084 sec/batch)
2017-05-06 21:10:58.353746: step 9280, loss = 0.86 (1549.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:10:59.178286: step 9290, loss = 0.92 (1552.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:11:00.105900: step 9300, loss = 0.82 (1379.9 examples/sec; 0.093 sec/batch)
2017-05-06 21:11:00.838084: step 9310, loss = 0.82 (1748.2 examples/sec; 0.073 sec/batch)
2017-05-06 21:11:01.670516: step 9320, loss = 0.75 (1537.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:11:02.497745: step 9330, loss = 0.86 (1547.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:11:03.324457: step 9340, loss = 0.82 (1548.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:11:04.142763: step 9350, loss = 0.89 (1564.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:11:04.972330: step 9360, loss = 0.89 (1543.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:11:05.804129: step 9370, loss = 0.92 (1538.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:11:06.628983: step 9380, loss = 0.77 (1551.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:11:07.448452: step 9390, loss = 0.77 (1562.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:11:08.374108: step 9400, loss = 0.96 (1382.8 examples/sec; 0.093 sec/batch)
2017-05-06 21:11:09.091823: step 9410, loss = 0.93 (1783.4 examples/sec; 0.072 sec/batch)
2017-05-06 21:11:09.922700: step 9420, loss = 0.97 (1540.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:11:10.746922: step 9430, loss = 1.12 (1553.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:11:11.565286: step 9440, loss = 0.97 (1564.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:11:12.388851: step 9450, loss = 0.82 (1554.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:11:13.207276: step 9460, loss = 1.06 (1564.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:11:14.033246: step 9470, loss = 0.85 (1549.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:11:14.864336: step 9480, loss = 0.87 (1540.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:11:15.683179: step 9490, loss = 0.86 (1563.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:11:16.610953: step 9500, loss = 0.79 (1379.6 examples/sec; 0.093 sec/batch)
2017-05-06 21:11:17.337086: step 9510, loss = 1.07 (1762.8 examples/sec; 0.073 sec/batch)
2017-05-06 21:11:18.167824: step 9520, loss = 0.79 (1540.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:11:18.991506: step 9530, loss = 0.87 (1554.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:11:19.805435: step 9540, loss = 0.76 (1572.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:11:20.625554: step 9550, loss = 0.74 (1560.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:11:21.453573: step 9560, loss = 0.92 (1545.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:11:22.281674: step 9570, loss = 0.78 (1545.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:11:23.109618: step 9580, loss = 0.87 (1546.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:11:23.919095: step 9590, loss = 0.88 (1581.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:11:24.847956: step 9600, loss = 0.96 (1378.0 examples/sec; 0.093 sec/batch)
2017-05-06 21:11:25.576643: step 9610, loss = 0.88 (1756.6 examples/sec; 0.073 sec/batch)
2017-05-06 21:11:26.391085: step 9620, loss = 0.88 (1571.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:11:27.216611: step 9630, loss = 0.81 (1550.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:11:28.037426: step 9640, loss = 0.96 (1559.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:11:28.864401: step 9650, loss = 0.90 (1547.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:11:29.684377: step 9660, loss = 0.92 (1561.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:11:30.514740: step 9670, loss = 0.88 (1541.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:11:31.341132: step 9680, loss = 0.93 (1548.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:11:32.166554: step 9690, loss = 0.90 (1550.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:11:33.089377: step 9700, loss = 0.79 (1387.1 examples/sec; 0.092 sec/batch)
2017-05-06 21:11:33.820778: step 9710, loss = 1.01 (1750.1 examples/sec; 0.073 sec/batch)
2017-05-06 21:11:34.650944: step 9720, loss = 1.03 (1541.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:11:35.475443: step 9730, loss = 0.84 (1552.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:11:36.293005: step 9740, loss = 0.81 (1565.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:11:37.120142: step 9750, loss = 0.93 (1547.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:11:37.951267: step 9760, loss = 1.17 (1540.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:11:38.772266: step 9770, loss = 0.77 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:11:39.590440: step 9780, loss = 0.96 (1564.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:11:40.404228: step 9790, loss = 0.97 (1572.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:11:41.333094: step 9800, loss = 0.83 (1378.0 examples/sec; 0.093 sec/batch)
2017-05-06 21:11:42.060012: step 9810, loss = 0.90 (1760.9 examples/sec; 0.073 sec/batch)
2017-05-06 21:11:42.891943: step 9820, loss = 0.98 (1538.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:11:43.703926: step 9830, loss = 0.84 (1576.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:11:44.531694: step 9840, loss = 0.89 (1546.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:11:45.354507: step 9850, loss = 1.07 (1555.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:11:46.182285: step 9860, loss = 0.78 (1546.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:11:47.009686: step 9870, loss = 0.99 (1547.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:11:47.817124: step 9880, loss = 0.91 (1585.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:11:48.659247: step 9890, loss = 0.98 (1520.0 examples/sec; 0.084 sec/batch)
2017-05-06 21:11:49.611442: step 9900, loss = 0.80 (1344.3 examples/sec; 0.095 sec/batch)
2017-05-06 21:11:50.321265: step 9910, loss = 1.00 (1803.3 examples/sec; 0.071 sec/batch)
2017-05-06 21:11:51.159592: step 9920, loss = 0.82 (1526.8 examples/sec; 0.084 sec/batch)
2017-05-06 21:11:51.988186: step 9930, loss = 0.92 (1544.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:11:52.821473: step 9940, loss = 0.87 (1536.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:11:53.646440: step 9950, loss = 0.92 (1551.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:11:54.480562: step 9960, loss = 0.97 (1534.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:11:55.304720: step 9970, loss = 0.79 (1553.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:11:56.133115: step 9980, loss = 0.75 (1545.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:11:56.947305: step 9990, loss = 0.70 (1572.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:11:57.874404: step 10000, loss = 0.76 (1380.7 examples/sec; 0.093 sec/batch)
2017-05-06 21:11:58.606852: step 10010, loss = 0.83 (1747.6 examples/sec; 0.073 sec/batch)
2017-05-06 21:11:59.426863: step 10020, loss = 0.89 (1561.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:12:00.244367: step 10030, loss = 0.82 (1565.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:12:01.068884: step 10040, loss = 0.99 (1552.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:12:01.896786: step 10050, loss = 0.89 (1546.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:12:02.723943: step 10060, loss = 0.86 (1547.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:12:03.546432: step 10070, loss = 0.95 (1556.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:12:04.371302: step 10080, loss = 0.86 (1551.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:12:05.197349: step 10090, loss = 0.75 (1549.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:12:06.132124: step 10100, loss = 0.70 (1369.3 examples/sec; 0.093 sec/batch)
2017-05-06 21:12:06.858014: step 10110, loss = 0.78 (1763.4 examples/sec; 0.073 sec/batch)
2017-05-06 21:12:07.681300: step 10120, loss = 0.83 (1554.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:12:08.506411: step 10130, loss = 0.78 (1551.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:12:09.334484: step 10140, loss = 0.81 (1545.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:12:10.155883: step 10150, loss = 0.96 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:12:10.986975: step 10160, loss = 0.90 (1540.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:12:11.795898: step 10170, loss = 0.81 (1582.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:12:12.621312: step 10180, loss = 0.87 (1550.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:12:13.451908: step 10190, loss = 1.02 (1541.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:12:14.380113: step 10200, loss = 0.77 (1379.0 examples/sec; 0.093 sec/batch)
2017-05-06 21:12:15.103286: step 10210, loss = 0.91 (1770.0 examples/sec; 0.072 sec/batch)
2017-05-06 21:12:15.924284: step 10220, loss = 0.69 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:12:16.752075: step 10230, loss = 0.76 (1546.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:12:17.575358: step 10240, loss = 0.82 (1554.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:12:18.405911: step 10250, loss = 1.06 (1541.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:12:19.234202: step 10260, loss = 0.77 (1545.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:12:20.056981: step 10270, loss = 0.74 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:12:20.881665: step 10280, loss = 0.94 (1552.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:12:21.708122: step 10290, loss = 0.95 (1548.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:12:22.652220: step 10300, loss = 0.93 (1355.8 examples/sec; 0.094 sec/batch)
2017-05-06 21:12:23.376435: step 10310, loss = 0.97 (1767.4 examples/sec; 0.072 sec/batch)
2017-05-06 21:12:24.189688: step 10320, loss = 0.79 (1573.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:12:25.017167: step 10330, loss = 0.93 (1546.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:12:25.840538: step 10340, loss = 0.94 (1554.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:12:26.662213: step 10350, loss = 0.95 (1557.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:12:27.485197: step 10360, loss = 0.96 (1555.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:12:28.317409: step 10370, loss = 0.81 (1538.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:12:29.143779: step 10380, loss = 0.79 (1548.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:12:29.968994: step 10390, loss = 0.85 (1551.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:12:30.896359: step 10400, loss = 0.74 (1380.3 examples/sec; 0.093 sec/batch)
2017-05-06 21:12:31.607429: step 10410, loss = 0.92 (1800.1 examples/sec; 0.071 sec/batch)
2017-05-06 21:12:32.432113: step 10420, loss = 0.93 (1552.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:12:33.263910: step 10430, loss = 0.82 (1538.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:12:34.091297: step 10440, loss = 0.69 (1547.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:12:34.914651: step 10450, loss = 1.03 (1554.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:12:35.722163: step 10460, loss = 1.02 (1585.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:12:36.543904: step 10470, loss = 0.72 (1557.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:12:37.366984: step 10480, loss = 0.94 (1555.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:12:38.178907: step 10490, loss = 0.94 (1576.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:12:39.104726: step 10500, loss = 0.89 (1382.6 examples/sec; 0.093 sec/batch)
2017-05-06 21:12:39.830039: step 10510, loss = 0.90 (1764.8 examples/sec; 0.073 sec/batch)
2017-05-06 21:12:40.657240: step 10520, loss = 0.95 (1547.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:12:41.481993: step 10530, loss = 0.89 (1552.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:12:42.306674: step 10540, loss = 0.74 (1552.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:12:43.138301: step 10550, loss = 0.98 (1539.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:12:43.952082: step 10560, loss = 0.96 (1572.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:12:44.784428: step 10570, loss = 1.03 (1537.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:12:45.603115: step 10580, loss = 0.83 (1563.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:12:46.429322: step 10590, loss = 0.94 (1549.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:12:47.356866: step 10600, loss = 0.86 (1380.0 examples/sec; 0.093 sec/batch)
2017-05-06 21:12:48.079143: step 10610, loss = 0.85 (1772.2 examples/sec; 0.072 sec/batch)
2017-05-06 21:12:48.904356: step 10620, loss = 1.08 (1551.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:12:49.728903: step 10630, loss = 0.77 (1552.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:12:50.553242: step 10640, loss = 1.00 (1552.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:12:51.377576: step 10650, loss = 0.90 (1552.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:12:52.189036: step 10660, loss = 0.98 (1577.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:12:53.010701: step 10670, loss = 0.80 (1557.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:12:53.844361: step 10680, loss = 0.96 (1535.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:12:54.669005: step 10690, loss = 0.96 (1552.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:12:55.598296: step 10700, loss = 1.00 (1377.4 examples/sec; 0.093 sec/batch)
2017-05-06 21:12:56.313085: step 10710, loss = 0.85 (1790.7 examples/sec; 0.071 sec/batch)
2017-05-06 21:12:57.132996: step 10720, loss = 1.04 (1561.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:12:57.954366: step 10730, loss = 0.88 (1558.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:12:58.789517: step 10740, loss = 0.78 (1532.7 examples/sec; 0.084 sec/batch)
2017-05-06 21:12:59.610499: step 10750, loss = 0.71 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:00.438257: step 10760, loss = 1.14 (1546.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:13:01.259320: step 10770, loss = 1.00 (1559.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:02.077444: step 10780, loss = 0.99 (1564.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:02.912238: step 10790, loss = 0.81 (1533.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:13:03.835016: step 10800, loss = 0.88 (1387.1 examples/sec; 0.092 sec/batch)
2017-05-06 21:13:04.547601: step 10810, loss = 0.74 (1796.3 examples/sec; 0.071 sec/batch)
2017-05-06 21:13:05.376647: step 10820, loss = 0.88 (1543.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:13:06.197661: step 10830, loss = 0.83 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:07.019210: step 10840, loss = 0.96 (1558.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:07.837631: step 10850, loss = 0.97 (1564.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:08.667779: step 10860, loss = 0.83 (1541.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:13:09.495887: step 10870, loss = 0.82 (1545.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:13:10.314148: step 10880, loss = 0.79 (1564.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:11.165177: step 10890, loss = 0.83 (1504.1 examples/sec; 0.085 sec/batch)
2017-05-06 21:13:12.060427: step 10900, loss = 0.77 (1429.8 examples/sec; 0.090 sec/batch)
2017-05-06 21:13:12.789824: step 10910, loss = 0.89 (1754.9 examples/sec; 0.073 sec/batch)
2017-05-06 21:13:13.615538: step 10920, loss = 0.90 (1550.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:13:14.442349: step 10930, loss = 0.80 (1548.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:13:15.261301: step 10940, loss = 0.93 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:16.077532: step 10950, loss = 0.90 (1568.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:16.906100: step 10960, loss = 0.83 (1544.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:13:17.730718: step 10970, loss = 0.85 (1552.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:18.553225: step 10980, loss = 0.93 (1556.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:19.377600: step 10990, loss = 0.86 (1552.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:20.309571: step 11000, loss = 0.97 (1373.4 examples/sec; 0.093 sec/batch)
2017-05-06 21:13:21.023017: step 11010, loss = 0.88 (1794.1 examples/sec; 0.071 sec/batch)
2017-05-06 21:13:21.849117: step 11020, loss = 0.97 (1549.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:13:22.670163: step 11030, loss = 0.88 (1559.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:23.493289: step 11040, loss = 0.84 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:24.318895: step 11050, loss = 0.77 (1550.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:13:25.139104: step 11060, loss = 0.91 (1560.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:25.977061: step 11070, loss = 0.90 (1527.5 examples/sec; 0.084 sec/batch)
2017-05-06 21:13:26.801237: step 11080, loss = 0.99 (1553.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:27.619798: step 11090, loss = 0.85 (1563.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:28.537780: step 11100, loss = 0.84 (1394.4 examples/sec; 0.092 sec/batch)
2017-05-06 21:13:29.261819: step 11110, loss = 0.83 (1767.9 examples/sec; 0.072 sec/batch)
2017-05-06 21:13:30.084861: step 11120, loss = 0.88 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:30.917930: step 11130, loss = 0.78 (1536.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:13:31.726912: step 11140, loss = 0.87 (1582.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:13:32.550031: step 11150, loss = 0.86 (1555.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:33.377947: step 11160, loss = 0.74 (1546.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:13:34.205024: step 11170, loss = 0.91 (1547.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:13:35.032395: step 11180, loss = 0.81 (1547.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:13:35.856447: step 11190, loss = 0.69 (1553.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:36.791688: step 11200, loss = 0.87 (1368.6 examples/sec; 0.094 sec/batch)
2017-05-06 21:13:37.517776: step 11210, loss = 0.84 (1762.9 examples/sec; 0.073 sec/batch)
2017-05-06 21:13:38.350004: step 11220, loss = 0.85 (1538.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:13:39.178804: step 11230, loss = 0.78 (1544.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:13:39.994153: step 11240, loss = 0.80 (1569.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:40.815717: step 11250, loss = 0.98 (1558.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:41.638084: step 11260, loss = 0.91 (1556.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:42.461738: step 11270, loss = 0.84 (1554.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:43.282923: step 11280, loss = 0.76 (1558.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:44.102659: step 11290, loss = 0.97 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:45.026146: step 11300, loss = 0.95 (1386.1 examples/sec; 0.092 sec/batch)
2017-05-06 21:13:45.760380: step 11310, loss = 0.81 (1743.3 examples/sec; 0.073 sec/batch)
2017-05-06 21:13:46.586726: step 11320, loss = 0.78 (1549.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:13:47.404613: step 11330, loss = 0.86 (1565.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:48.219268: step 11340, loss = 0.87 (1571.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:13:49.040616: step 11350, loss = 1.07 (1558.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:49.867486: step 11360, loss = 0.94 (1548.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:13:50.692194: step 11370, loss = 1.04 (1552.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:51.520547: step 11380, loss = 0.86 (1545.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:13:52.348310: step 11390, loss = 1.05 (1546.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:13:53.281649: step 11400, loss = 0.79 (1371.4 examples/sec; 0.093 sec/batch)
2017-05-06 21:13:54.007899: step 11410, loss = 0.77 (1762.5 examples/sec; 0.073 sec/batch)
2017-05-06 21:13:54.837026: step 11420, loss = 0.77 (1543.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:13:55.654012: step 11430, loss = 0.86 (1566.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:56.472089: step 11440, loss = 0.90 (1564.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:57.306962: step 11450, loss = 0.73 (1533.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:13:58.124101: step 11460, loss = 0.98 (1566.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:58.947317: step 11470, loss = 0.91 (1554.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:13:59.773930: step 11480, loss = 0.87 (1548.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:14:00.591284: step 11490, loss = 0.73 (1566.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:14:01.512665: step 11500, loss = 0.75 (1389.2 examples/sec; 0.092 sec/batch)
2017-05-06 21:14:02.244248: step 11510, loss = 0.91 (1749.6 examples/sec; 0.073 sec/batch)
2017-05-06 21:14:03.066381: step 11520, loss = 0.89 (1556.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:14:03.889173: step 11530, loss = 0.86 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:14:04.705017: step 11540, loss = 0.83 (1568.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:14:05.531429: step 11550, loss = 0.89 (1548.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:14:06.356883: step 11560, loss = 0.81 (1550.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:14:07.175131: step 11570, loss = 0.88 (1564.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:14:07.999725: step 11580, loss = 0.90 (1552.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:14:08.828376: step 11590, loss = 0.95 (1544.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:14:09.745003: step 11600, loss = 0.85 (1396.4 examples/sec; 0.092 sec/batch)
2017-05-06 21:14:10.470037: step 11610, loss = 1.04 (1765.4 examples/sec; 0.073 sec/batch)
2017-05-06 21:14:11.294543: step 11620, loss = 0.81 (1552.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:14:12.111593: step 11630, loss = 0.97 (1566.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:14:12.943799: step 11640, loss = 0.79 (1538.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:14:13.774453: step 11650, loss = 0.92 (1541.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:14:14.592582: step 11660, loss = 0.90 (1564.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:14:15.413727: step 11670, loss = 1.02 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:14:16.233736: step 11680, loss = 0.76 (1561.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:14:17.070172: step 11690, loss = 1.06 (1530.3 examples/sec; 0.084 sec/batch)
2017-05-06 21:14:17.993271: step 11700, loss = 0.82 (1386.6 examples/sec; 0.092 sec/batch)
2017-05-06 21:14:18.714175: step 11710, loss = 0.72 (1775.6 examples/sec; 0.072 sec/batch)
2017-05-06 21:14:19.531513: step 11720, loss = 1.13 (1566.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:14:20.351361: step 11730, loss = 0.84 (1561.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:14:21.178550: step 11740, loss = 0.82 (1547.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:14:22.001308: step 11750, loss = 0.81 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:14:22.838200: step 11760, loss = 0.80 (1529.5 examples/sec; 0.084 sec/batch)
2017-05-06 21:14:23.650298: step 11770, loss = 0.88 (1576.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:14:24.473214: step 11780, loss = 0.85 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:14:25.304211: step 11790, loss = 1.07 (1540.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:14:26.237593: step 11800, loss = 0.87 (1371.4 examples/sec; 0.093 sec/batch)
2017-05-06 21:14:26.961378: step 11810, loss = 0.75 (1768.5 examples/sec; 0.072 sec/batch)
2017-05-06 21:14:27.779544: step 11820, loss = 0.81 (1564.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:14:28.607058: step 11830, loss = 0.75 (1546.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:14:29.431403: step 11840, loss = 0.84 (1552.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:14:30.260626: step 11850, loss = 0.73 (1543.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:14:31.081097: step 11860, loss = 1.01 (1560.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:14:31.898147: step 11870, loss = 1.10 (1566.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:14:32.750499: step 11880, loss = 0.90 (1501.7 examples/sec; 0.085 sec/batch)
2017-05-06 21:14:33.560999: step 11890, loss = 0.91 (1579.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:14:34.490398: step 11900, loss = 0.87 (1377.2 examples/sec; 0.093 sec/batch)
2017-05-06 21:14:35.206159: step 11910, loss = 0.81 (1788.3 examples/sec; 0.072 sec/batch)
2017-05-06 21:14:36.029352: step 11920, loss = 0.79 (1554.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:14:36.854291: step 11930, loss = 0.84 (1551.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:14:37.689430: step 11940, loss = 0.81 (1532.7 examples/sec; 0.084 sec/batch)
2017-05-06 21:14:38.507472: step 11950, loss = 0.94 (1564.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:14:39.325139: step 11960, loss = 0.75 (1565.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:14:40.150332: step 11970, loss = 0.75 (1551.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:14:40.977501: step 11980, loss = 0.77 (1547.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:14:41.803490: step 11990, loss = 0.85 (1549.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:14:42.730354: step 12000, loss = 0.94 (1381.0 examples/sec; 0.093 sec/batch)
2017-05-06 21:14:43.446208: step 12010, loss = 1.03 (1788.0 examples/sec; 0.072 sec/batch)
2017-05-06 21:14:44.268924: step 12020, loss = 0.83 (1555.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:14:45.102408: step 12030, loss = 0.92 (1535.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:14:45.925207: step 12040, loss = 0.85 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:14:46.748338: step 12050, loss = 0.90 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:14:47.564305: step 12060, loss = 1.03 (1568.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:14:48.394682: step 12070, loss = 0.78 (1541.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:14:49.211122: step 12080, loss = 0.80 (1567.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:14:50.035055: step 12090, loss = 0.87 (1553.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:14:50.959679: step 12100, loss = 0.93 (1384.3 examples/sec; 0.092 sec/batch)
2017-05-06 21:14:51.679387: step 12110, loss = 0.80 (1778.5 examples/sec; 0.072 sec/batch)
2017-05-06 21:14:52.505188: step 12120, loss = 0.73 (1550.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:14:53.333073: step 12130, loss = 1.05 (1546.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:14:54.158204: step 12140, loss = 0.87 (1551.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:14:54.981678: step 12150, loss = 0.87 (1554.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:14:55.792555: step 12160, loss = 0.85 (1578.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:14:56.612247: step 12170, loss = 0.83 (1561.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:14:57.445501: step 12180, loss = 0.79 (1536.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:14:58.273352: step 12190, loss = 0.87 (1546.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:14:59.194107: step 12200, loss = 0.93 (1390.2 examples/sec; 0.092 sec/batch)
2017-05-06 21:14:59.909997: step 12210, loss = 0.99 (1788.0 examples/sec; 0.072 sec/batch)
2017-05-06 21:15:00.728844: step 12220, loss = 0.81 (1563.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:15:01.549948: step 12230, loss = 0.77 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:15:02.378862: step 12240, loss = 0.85 (1544.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:15:03.209261: step 12250, loss = 0.99 (1541.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:15:04.023263: step 12260, loss = 0.87 (1572.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:15:04.846922: step 12270, loss = 0.84 (1554.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:15:05.674492: step 12280, loss = 0.69 (1546.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:15:06.505636: step 12290, loss = 0.91 (1540.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:15:07.426421: step 12300, loss = 0.86 (1390.1 examples/sec; 0.092 sec/batch)
2017-05-06 21:15:08.140111: step 12310, loss = 0.78 (1793.5 examples/sec; 0.071 sec/batch)
2017-05-06 21:15:08.960861: step 12320, loss = 1.02 (1559.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:15:09.783075: step 12330, loss = 0.99 (1556.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:15:10.603544: step 12340, loss = 0.82 (1560.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:15:11.433153: step 12350, loss = 0.87 (1542.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:15:12.250083: step 12360, loss = 0.89 (1566.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:15:13.067981: step 12370, loss = 0.88 (1565.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:15:13.906357: step 12380, loss = 0.90 (1526.8 examples/sec; 0.084 sec/batch)
2017-05-06 21:15:14.735368: step 12390, loss = 0.88 (1544.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:15:15.656229: step 12400, loss = 0.78 (1390.0 examples/sec; 0.092 sec/batch)
2017-05-06 21:15:16.380891: step 12410, loss = 0.76 (1766.3 examples/sec; 0.072 sec/batch)
2017-05-06 21:15:17.209196: step 12420, loss = 0.82 (1545.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:15:18.040198: step 12430, loss = 0.99 (1540.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:15:18.856978: step 12440, loss = 0.96 (1567.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:15:19.673983: step 12450, loss = 1.08 (1566.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:15:20.497757: step 12460, loss = 0.81 (1553.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:15:21.331195: step 12470, loss = 0.82 (1535.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:15:22.166093: step 12480, loss = 1.00 (1533.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:15:22.993885: step 12490, loss = 0.88 (1546.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:15:23.907864: step 12500, loss = 0.95 (1400.5 examples/sec; 0.091 sec/batch)
2017-05-06 21:15:24.636484: step 12510, loss = 0.96 (1756.7 examples/sec; 0.073 sec/batch)
2017-05-06 21:15:25.457725: step 12520, loss = 0.99 (1558.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:15:26.284338: step 12530, loss = 0.71 (1548.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:15:27.109764: step 12540, loss = 0.85 (1550.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:15:27.931949: step 12550, loss = 1.06 (1556.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:15:28.751335: step 12560, loss = 0.85 (1562.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:15:29.580989: step 12570, loss = 0.82 (1542.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:15:30.404251: step 12580, loss = 0.78 (1554.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:15:31.226665: step 12590, loss = 0.81 (1556.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:15:32.147012: step 12600, loss = 0.88 (1390.8 examples/sec; 0.092 sec/batch)
2017-05-06 21:15:32.869958: step 12610, loss = 0.85 (1770.6 examples/sec; 0.072 sec/batch)
2017-05-06 21:15:33.697327: step 12620, loss = 0.78 (1547.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:15:34.517882: step 12630, loss = 0.86 (1559.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:15:35.330958: step 12640, loss = 1.02 (1574.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:15:36.147089: step 12650, loss = 0.82 (1568.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:15:36.971977: step 12660, loss = 0.86 (1551.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:15:37.794075: step 12670, loss = 0.87 (1557.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:15:38.618847: step 12680, loss = 0.81 (1551.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:15:39.429790: step 12690, loss = 0.91 (1578.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:15:40.356460: step 12700, loss = 0.98 (1381.3 examples/sec; 0.093 sec/batch)
2017-05-06 21:15:41.081275: step 12710, loss = 0.98 (1766.0 examples/sec; 0.072 sec/batch)
2017-05-06 21:15:41.905750: step 12720, loss = 1.00 (1552.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:15:42.730835: step 12730, loss = 0.93 (1551.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:15:43.552358: step 12740, loss = 1.08 (1558.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:15:44.377408: step 12750, loss = 0.93 (1551.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:15:45.199387: step 12760, loss = 0.85 (1557.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:15:46.024267: step 12770, loss = 0.87 (1551.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:15:46.851309: step 12780, loss = 0.87 (1547.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:15:47.670307: step 12790, loss = 0.78 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:15:48.591961: step 12800, loss = 1.02 (1388.8 examples/sec; 0.092 sec/batch)
2017-05-06 21:15:49.314779: step 12810, loss = 0.85 (1770.9 examples/sec; 0.072 sec/batch)
2017-05-06 21:15:50.138488: step 12820, loss = 0.87 (1554.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:15:50.968128: step 12830, loss = 0.88 (1542.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:15:51.784210: step 12840, loss = 0.83 (1568.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:15:52.617434: step 12850, loss = 0.87 (1536.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:15:53.449697: step 12860, loss = 0.99 (1538.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:15:54.297738: step 12870, loss = 1.01 (1509.4 examples/sec; 0.085 sec/batch)
2017-05-06 21:15:55.095083: step 12880, loss = 0.88 (1605.3 examples/sec; 0.080 sec/batch)
2017-05-06 21:15:55.914376: step 12890, loss = 0.82 (1562.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:15:56.836540: step 12900, loss = 1.03 (1388.0 examples/sec; 0.092 sec/batch)
2017-05-06 21:15:57.552815: step 12910, loss = 0.89 (1787.0 examples/sec; 0.072 sec/batch)
2017-05-06 21:15:58.366258: step 12920, loss = 0.82 (1573.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:15:59.183913: step 12930, loss = 0.86 (1565.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:15:59.992367: step 12940, loss = 0.76 (1583.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:16:00.809707: step 12950, loss = 0.80 (1566.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:01.638403: step 12960, loss = 1.09 (1544.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:16:02.463880: step 12970, loss = 0.67 (1550.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:16:03.284056: step 12980, loss = 0.95 (1560.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:04.099743: step 12990, loss = 0.92 (1569.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:05.018553: step 13000, loss = 0.88 (1393.1 examples/sec; 0.092 sec/batch)
2017-05-06 21:16:05.738091: step 13010, loss = 0.83 (1778.9 examples/sec; 0.072 sec/batch)
2017-05-06 21:16:06.560542: step 13020, loss = 0.80 (1556.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:07.373924: step 13030, loss = 0.81 (1573.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:16:08.197200: step 13040, loss = 1.06 (1554.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:09.015090: step 13050, loss = 1.01 (1565.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:09.841179: step 13060, loss = 0.81 (1549.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:16:10.664123: step 13070, loss = 0.69 (1555.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:11.474166: step 13080, loss = 0.82 (1580.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:16:12.299974: step 13090, loss = 0.90 (1550.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:16:13.231902: step 13100, loss = 0.93 (1373.5 examples/sec; 0.093 sec/batch)
2017-05-06 21:16:13.939607: step 13110, loss = 0.82 (1808.7 examples/sec; 0.071 sec/batch)
2017-05-06 21:16:14.753261: step 13120, loss = 0.98 (1573.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:16:15.572806: step 13130, loss = 0.78 (1561.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:16.397267: step 13140, loss = 0.77 (1552.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:17.220959: step 13150, loss = 0.70 (1554.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:18.046038: step 13160, loss = 0.87 (1551.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:16:18.866405: step 13170, loss = 0.76 (1560.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:19.676953: step 13180, loss = 0.77 (1579.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:16:20.492963: step 13190, loss = 0.86 (1568.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:21.428530: step 13200, loss = 0.96 (1368.1 examples/sec; 0.094 sec/batch)
2017-05-06 21:16:22.140539: step 13210, loss = 0.91 (1797.8 examples/sec; 0.071 sec/batch)
2017-05-06 21:16:22.963181: step 13220, loss = 0.83 (1555.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:23.777199: step 13230, loss = 0.95 (1572.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:16:24.602026: step 13240, loss = 0.74 (1551.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:25.408372: step 13250, loss = 0.80 (1587.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:16:26.228675: step 13260, loss = 0.83 (1560.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:27.054569: step 13270, loss = 0.92 (1549.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:16:27.865225: step 13280, loss = 0.88 (1579.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:16:28.685063: step 13290, loss = 0.96 (1561.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:29.614168: step 13300, loss = 0.86 (1377.7 examples/sec; 0.093 sec/batch)
2017-05-06 21:16:30.332239: step 13310, loss = 0.89 (1782.6 examples/sec; 0.072 sec/batch)
2017-05-06 21:16:31.152139: step 13320, loss = 1.01 (1561.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:31.976097: step 13330, loss = 0.90 (1553.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:32.791012: step 13340, loss = 0.88 (1570.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:16:33.615147: step 13350, loss = 0.93 (1553.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:34.437448: step 13360, loss = 0.87 (1556.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:35.255557: step 13370, loss = 0.98 (1564.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:36.076994: step 13380, loss = 0.91 (1558.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:36.891462: step 13390, loss = 0.90 (1571.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:16:37.824608: step 13400, loss = 0.91 (1371.7 examples/sec; 0.093 sec/batch)
2017-05-06 21:16:38.541183: step 13410, loss = 0.96 (1786.3 examples/sec; 0.072 sec/batch)
2017-05-06 21:16:39.359260: step 13420, loss = 1.21 (1564.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:40.176003: step 13430, loss = 0.96 (1567.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:40.990413: step 13440, loss = 0.80 (1571.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:16:41.814029: step 13450, loss = 0.97 (1554.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:42.642749: step 13460, loss = 0.80 (1544.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:16:43.459105: step 13470, loss = 0.72 (1568.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:44.283532: step 13480, loss = 0.81 (1552.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:45.103603: step 13490, loss = 0.87 (1560.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:46.032919: step 13500, loss = 0.86 (1377.4 examples/sec; 0.093 sec/batch)
2017-05-06 21:16:46.761813: step 13510, loss = 1.02 (1756.1 examples/sec; 0.073 sec/batch)
2017-05-06 21:16:47.589708: step 13520, loss = 0.88 (1546.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:16:48.410361: step 13530, loss = 0.90 (1559.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:49.239607: step 13540, loss = 0.78 (1543.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:16:50.062378: step 13550, loss = 1.13 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:50.884092: step 13560, loss = 1.02 (1557.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:51.706753: step 13570, loss = 0.73 (1555.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:52.527382: step 13580, loss = 0.85 (1559.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:53.350911: step 13590, loss = 0.75 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:54.264998: step 13600, loss = 0.77 (1400.3 examples/sec; 0.091 sec/batch)
2017-05-06 21:16:54.991773: step 13610, loss = 0.97 (1761.2 examples/sec; 0.073 sec/batch)
2017-05-06 21:16:55.804316: step 13620, loss = 0.85 (1575.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:16:56.634428: step 13630, loss = 0.86 (1542.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:16:57.457761: step 13640, loss = 1.06 (1554.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:16:58.285795: step 13650, loss = 1.04 (1545.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:16:59.113315: step 13660, loss = 0.94 (1546.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:16:59.925879: step 13670, loss = 0.95 (1575.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:17:00.743091: step 13680, loss = 0.98 (1566.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:01.561924: step 13690, loss = 0.80 (1563.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:02.487348: step 13700, loss = 0.81 (1383.2 examples/sec; 0.093 sec/batch)
2017-05-06 21:17:03.210079: step 13710, loss = 0.76 (1771.1 examples/sec; 0.072 sec/batch)
2017-05-06 21:17:04.019326: step 13720, loss = 1.00 (1581.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:17:04.835010: step 13730, loss = 0.87 (1569.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:05.651818: step 13740, loss = 0.79 (1567.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:06.478556: step 13750, loss = 1.07 (1548.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:17:07.306106: step 13760, loss = 0.90 (1546.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:17:08.135486: step 13770, loss = 0.93 (1543.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:17:08.951723: step 13780, loss = 0.85 (1568.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:09.781261: step 13790, loss = 0.85 (1543.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:17:10.704195: step 13800, loss = 0.91 (1386.9 examples/sec; 0.092 sec/batch)
2017-05-06 21:17:11.425954: step 13810, loss = 0.79 (1773.5 examples/sec; 0.072 sec/batch)
2017-05-06 21:17:12.249101: step 13820, loss = 0.76 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:13.065350: step 13830, loss = 0.77 (1568.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:13.887410: step 13840, loss = 0.91 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:14.709772: step 13850, loss = 0.88 (1556.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:15.546565: step 13860, loss = 0.95 (1529.6 examples/sec; 0.084 sec/batch)
2017-05-06 21:17:16.354301: step 13870, loss = 0.89 (1584.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:17:17.167475: step 13880, loss = 0.77 (1574.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:17:17.983554: step 13890, loss = 0.88 (1568.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:18.901901: step 13900, loss = 0.82 (1393.8 examples/sec; 0.092 sec/batch)
2017-05-06 21:17:19.620542: step 13910, loss = 0.85 (1781.1 examples/sec; 0.072 sec/batch)
2017-05-06 21:17:20.431798: step 13920, loss = 0.73 (1577.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:17:21.247808: step 13930, loss = 0.77 (1568.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:22.071350: step 13940, loss = 0.83 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:22.902692: step 13950, loss = 0.73 (1539.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:17:23.725698: step 13960, loss = 0.76 (1555.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:24.549647: step 13970, loss = 0.82 (1553.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:25.374035: step 13980, loss = 0.92 (1552.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:26.196214: step 13990, loss = 0.82 (1556.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:27.120611: step 14000, loss = 0.80 (1384.7 examples/sec; 0.092 sec/batch)
2017-05-06 21:17:27.839922: step 14010, loss = 0.97 (1779.5 examples/sec; 0.072 sec/batch)
2017-05-06 21:17:28.657481: step 14020, loss = 0.83 (1565.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:29.491953: step 14030, loss = 0.84 (1533.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:17:30.311187: step 14040, loss = 0.81 (1562.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:31.143850: step 14050, loss = 0.85 (1537.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:17:31.960093: step 14060, loss = 0.75 (1568.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:32.791734: step 14070, loss = 0.96 (1539.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:17:33.614679: step 14080, loss = 0.80 (1555.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:34.441528: step 14090, loss = 0.99 (1548.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:17:35.361605: step 14100, loss = 0.87 (1391.2 examples/sec; 0.092 sec/batch)
2017-05-06 21:17:36.073503: step 14110, loss = 0.90 (1798.0 examples/sec; 0.071 sec/batch)
2017-05-06 21:17:36.893257: step 14120, loss = 0.79 (1561.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:37.717585: step 14130, loss = 0.93 (1552.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:38.537436: step 14140, loss = 0.84 (1561.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:39.363015: step 14150, loss = 0.79 (1550.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:17:40.187628: step 14160, loss = 0.95 (1552.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:41.017833: step 14170, loss = 0.88 (1541.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:17:41.836864: step 14180, loss = 0.83 (1562.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:42.657838: step 14190, loss = 1.00 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:43.582121: step 14200, loss = 0.79 (1384.9 examples/sec; 0.092 sec/batch)
2017-05-06 21:17:44.298610: step 14210, loss = 0.73 (1786.5 examples/sec; 0.072 sec/batch)
2017-05-06 21:17:45.124025: step 14220, loss = 0.74 (1550.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:17:45.947757: step 14230, loss = 0.85 (1553.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:46.770751: step 14240, loss = 0.94 (1555.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:47.586442: step 14250, loss = 0.85 (1569.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:48.409167: step 14260, loss = 0.76 (1555.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:49.223364: step 14270, loss = 0.93 (1572.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:17:50.043674: step 14280, loss = 1.01 (1560.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:50.863129: step 14290, loss = 0.82 (1562.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:51.793875: step 14300, loss = 0.89 (1375.2 examples/sec; 0.093 sec/batch)
2017-05-06 21:17:52.513631: step 14310, loss = 0.87 (1778.4 examples/sec; 0.072 sec/batch)
2017-05-06 21:17:53.330333: step 14320, loss = 0.72 (1567.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:54.154089: step 14330, loss = 1.02 (1553.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:54.973450: step 14340, loss = 0.76 (1562.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:55.787486: step 14350, loss = 0.89 (1572.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:17:56.623311: step 14360, loss = 0.83 (1531.4 examples/sec; 0.084 sec/batch)
2017-05-06 21:17:57.450809: step 14370, loss = 0.76 (1546.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:17:58.266705: step 14380, loss = 0.85 (1568.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:17:59.099128: step 14390, loss = 0.97 (1537.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:18:00.018603: step 14400, loss = 0.87 (1392.1 examples/sec; 0.092 sec/batch)
2017-05-06 21:18:00.734844: step 14410, loss = 0.84 (1787.1 examples/sec; 0.072 sec/batch)
2017-05-06 21:18:01.556736: step 14420, loss = 0.85 (1557.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:18:02.373643: step 14430, loss = 0.78 (1566.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:18:03.194034: step 14440, loss = 0.79 (1560.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:18:04.009622: step 14450, loss = 0.94 (1569.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:18:04.840029: step 14460, loss = 0.75 (1541.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:18:05.665619: step 14470, loss = 0.86 (1550.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:18:06.489620: step 14480, loss = 1.02 (1553.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:18:07.309548: step 14490, loss = 0.79 (1561.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:18:08.222590: step 14500, loss = 0.85 (1401.9 examples/sec; 0.091 sec/batch)
2017-05-06 21:18:08.954556: step 14510, loss = 0.85 (1748.7 examples/sec; 0.073 sec/batch)
2017-05-06 21:18:09.776530: step 14520, loss = 0.90 (1557.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:18:10.604500: step 14530, loss = 0.85 (1545.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:18:11.430061: step 14540, loss = 0.86 (1550.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:18:12.263948: step 14550, loss = 0.73 (1535.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:18:13.118767: step 14560, loss = 1.04 (1497.4 examples/sec; 0.085 sec/batch)
2017-05-06 21:18:13.940322: step 14570, loss = 1.01 (1558.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:18:14.763259: step 14580, loss = 0.94 (1555.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:18:15.587785: step 14590, loss = 0.94 (1552.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:18:16.516490: step 14600, loss = 0.95 (1378.3 examples/sec; 0.093 sec/batch)
2017-05-06 21:18:17.236945: step 14610, loss = 0.88 (1776.7 examples/sec; 0.072 sec/batch)
2017-05-06 21:18:18.062057: step 14620, loss = 1.06 (1551.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:18:18.884696: step 14630, loss = 0.98 (1556.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:18:19.701281: step 14640, loss = 0.74 (1567.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:18:20.530862: step 14650, loss = 0.87 (1542.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:18:21.360748: step 14660, loss = 0.86 (1542.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:18:22.185352: step 14670, loss = 0.82 (1552.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:18:23.002965: step 14680, loss = 0.88 (1565.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:18:23.814240: step 14690, loss = 0.90 (1577.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:18:24.743962: step 14700, loss = 0.79 (1376.8 examples/sec; 0.093 sec/batch)
2017-05-06 21:18:25.450770: step 14710, loss = 0.85 (1811.0 examples/sec; 0.071 sec/batch)
2017-05-06 21:18:26.269614: step 14720, loss = 0.66 (1563.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:18:27.088502: step 14730, loss = 0.63 (1563.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:18:27.897529: step 14740, loss = 0.94 (1582.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:18:28.715497: step 14750, loss = 0.75 (1564.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:18:29.545406: step 14760, loss = 0.82 (1542.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:18:30.365229: step 14770, loss = 0.85 (1561.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:18:31.185001: step 14780, loss = 0.81 (1561.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:18:31.995022: step 14790, loss = 0.91 (1580.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:18:32.939819: step 14800, loss = 0.86 (1354.8 examples/sec; 0.094 sec/batch)
2017-05-06 21:18:33.638294: step 14810, loss = 0.74 (1832.6 examples/sec; 0.070 sec/batch)
2017-05-06 21:18:34.460348: step 14820, loss = 0.71 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:18:35.284894: step 14830, loss = 0.97 (1552.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:18:36.096170: step 14840, loss = 0.96 (1577.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:18:36.956104: step 14850, loss = 0.69 (1488.5 examples/sec; 0.086 sec/batch)
2017-05-06 21:18:37.744540: step 14860, loss = 0.95 (1623.5 examples/sec; 0.079 sec/batch)
2017-05-06 21:18:38.565880: step 14870, loss = 0.92 (1558.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:18:39.391635: step 14880, loss = 0.92 (1550.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:18:40.214650: step 14890, loss = 0.88 (1555.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:18:41.142998: step 14900, loss = 0.81 (1378.8 examples/sec; 0.093 sec/batch)
2017-05-06 21:18:41.872513: step 14910, loss = 0.78 (1754.6 examples/sec; 0.073 sec/batch)
2017-05-06 21:18:42.702611: step 14920, loss = 0.70 (1542.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:18:43.516368: step 14930, loss = 0.76 (1573.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:18:44.344486: step 14940, loss = 0.82 (1545.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:18:45.170849: step 14950, loss = 0.81 (1549.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:18:45.999619: step 14960, loss = 0.87 (1544.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:18:46.822436: step 14970, loss = 1.01 (1555.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:18:47.640347: step 14980, loss = 0.90 (1565.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:18:48.462953: step 14990, loss = 0.81 (1556.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:18:49.401977: step 15000, loss = 1.06 (1363.1 examples/sec; 0.094 sec/batch)
2017-05-06 21:18:50.114603: step 15010, loss = 0.88 (1796.2 examples/sec; 0.071 sec/batch)
2017-05-06 21:18:50.932078: step 15020, loss = 0.79 (1565.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:18:51.744916: step 15030, loss = 0.96 (1574.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:18:52.563433: step 15040, loss = 0.85 (1563.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:18:53.381028: step 15050, loss = 0.85 (1565.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:18:54.210159: step 15060, loss = 0.83 (1543.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:18:55.036381: step 15070, loss = 0.92 (1549.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:18:55.851035: step 15080, loss = 0.83 (1571.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:18:56.676702: step 15090, loss = 0.80 (1550.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:18:57.591962: step 15100, loss = 0.77 (1398.5 examples/sec; 0.092 sec/batch)
2017-05-06 21:18:58.323778: step 15110, loss = 0.99 (1749.1 examples/sec; 0.073 sec/batch)
2017-05-06 21:18:59.147784: step 15120, loss = 0.83 (1553.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:18:59.952186: step 15130, loss = 0.93 (1591.2 examples/sec; 0.080 sec/batch)
2017-05-06 21:19:00.770281: step 15140, loss = 0.77 (1564.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:01.594875: step 15150, loss = 0.88 (1552.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:02.415548: step 15160, loss = 0.80 (1559.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:03.232819: step 15170, loss = 0.63 (1566.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:04.046636: step 15180, loss = 0.92 (1572.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:19:04.873072: step 15190, loss = 0.87 (1548.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:19:05.799129: step 15200, loss = 0.73 (1382.2 examples/sec; 0.093 sec/batch)
2017-05-06 21:19:06.526625: step 15210, loss = 0.85 (1759.5 examples/sec; 0.073 sec/batch)
2017-05-06 21:19:07.349530: step 15220, loss = 0.80 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:08.162543: step 15230, loss = 1.04 (1574.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:19:08.976011: step 15240, loss = 0.90 (1573.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:19:09.800714: step 15250, loss = 0.87 (1552.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:10.625206: step 15260, loss = 0.87 (1552.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:11.437405: step 15270, loss = 0.77 (1576.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:19:12.257628: step 15280, loss = 0.80 (1560.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:13.072381: step 15290, loss = 0.83 (1571.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:19:14.009568: step 15300, loss = 0.91 (1365.8 examples/sec; 0.094 sec/batch)
2017-05-06 21:19:14.729867: step 15310, loss = 0.69 (1777.0 examples/sec; 0.072 sec/batch)
2017-05-06 21:19:15.539172: step 15320, loss = 0.91 (1581.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:19:16.361890: step 15330, loss = 0.75 (1555.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:17.185382: step 15340, loss = 0.95 (1554.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:17.997920: step 15350, loss = 0.90 (1575.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:19:18.815720: step 15360, loss = 0.89 (1565.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:19.629767: step 15370, loss = 1.07 (1572.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:19:20.450283: step 15380, loss = 0.94 (1560.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:21.266444: step 15390, loss = 0.83 (1568.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:22.195714: step 15400, loss = 1.05 (1377.4 examples/sec; 0.093 sec/batch)
2017-05-06 21:19:22.908188: step 15410, loss = 0.77 (1796.6 examples/sec; 0.071 sec/batch)
2017-05-06 21:19:23.724917: step 15420, loss = 0.98 (1567.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:24.544787: step 15430, loss = 0.87 (1561.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:25.360103: step 15440, loss = 0.90 (1569.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:26.186475: step 15450, loss = 0.93 (1548.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:19:27.006251: step 15460, loss = 0.77 (1561.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:27.822643: step 15470, loss = 0.74 (1567.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:28.644561: step 15480, loss = 0.65 (1557.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:29.469325: step 15490, loss = 0.98 (1552.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:30.395348: step 15500, loss = 0.81 (1382.3 examples/sec; 0.093 sec/batch)
2017-05-06 21:19:31.111366: step 15510, loss = 0.97 (1787.7 examples/sec; 0.072 sec/batch)
2017-05-06 21:19:31.917745: step 15520, loss = 0.83 (1587.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:19:32.733465: step 15530, loss = 0.76 (1569.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:33.552141: step 15540, loss = 0.90 (1563.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:34.381110: step 15550, loss = 0.98 (1544.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:19:35.205626: step 15560, loss = 0.84 (1552.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:36.020809: step 15570, loss = 0.91 (1570.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:36.845454: step 15580, loss = 0.82 (1552.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:37.669146: step 15590, loss = 0.79 (1554.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:38.617796: step 15600, loss = 0.72 (1349.3 examples/sec; 0.095 sec/batch)
2017-05-06 21:19:39.318717: step 15610, loss = 0.92 (1826.2 examples/sec; 0.070 sec/batch)
2017-05-06 21:19:40.143305: step 15620, loss = 0.78 (1552.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:40.962757: step 15630, loss = 0.85 (1562.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:41.782475: step 15640, loss = 0.76 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:42.604102: step 15650, loss = 0.89 (1557.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:43.426328: step 15660, loss = 0.83 (1556.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:44.254545: step 15670, loss = 0.85 (1545.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:19:45.072577: step 15680, loss = 0.84 (1564.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:45.901133: step 15690, loss = 0.79 (1544.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:19:46.818802: step 15700, loss = 1.04 (1394.8 examples/sec; 0.092 sec/batch)
2017-05-06 21:19:47.542197: step 15710, loss = 0.76 (1769.4 examples/sec; 0.072 sec/batch)
2017-05-06 21:19:48.358326: step 15720, loss = 0.87 (1568.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:49.179680: step 15730, loss = 0.79 (1558.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:50.010795: step 15740, loss = 0.82 (1540.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:19:50.833880: step 15750, loss = 0.77 (1555.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:51.654187: step 15760, loss = 0.87 (1560.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:52.474258: step 15770, loss = 0.80 (1560.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:53.295405: step 15780, loss = 0.87 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:54.116963: step 15790, loss = 0.70 (1558.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:55.036606: step 15800, loss = 0.94 (1391.8 examples/sec; 0.092 sec/batch)
2017-05-06 21:19:55.745114: step 15810, loss = 0.85 (1806.6 examples/sec; 0.071 sec/batch)
2017-05-06 21:19:56.562336: step 15820, loss = 0.79 (1566.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:19:57.387796: step 15830, loss = 0.91 (1550.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:19:58.228860: step 15840, loss = 0.91 (1521.9 examples/sec; 0.084 sec/batch)
2017-05-06 21:19:59.029375: step 15850, loss = 0.88 (1599.0 examples/sec; 0.080 sec/batch)
2017-05-06 21:19:59.838619: step 15860, loss = 0.98 (1581.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:20:00.657784: step 15870, loss = 0.82 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:01.465020: step 15880, loss = 0.74 (1585.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:20:02.300673: step 15890, loss = 0.75 (1531.7 examples/sec; 0.084 sec/batch)
2017-05-06 21:20:03.216113: step 15900, loss = 0.92 (1398.2 examples/sec; 0.092 sec/batch)
2017-05-06 21:20:03.929702: step 15910, loss = 0.93 (1793.7 examples/sec; 0.071 sec/batch)
2017-05-06 21:20:04.757168: step 15920, loss = 0.91 (1546.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:20:05.587938: step 15930, loss = 0.80 (1540.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:20:06.410472: step 15940, loss = 0.88 (1556.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:07.227292: step 15950, loss = 0.77 (1567.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:08.037322: step 15960, loss = 0.72 (1580.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:20:08.853659: step 15970, loss = 0.82 (1568.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:09.676622: step 15980, loss = 0.95 (1555.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:10.501213: step 15990, loss = 0.79 (1552.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:11.418959: step 16000, loss = 0.88 (1394.7 examples/sec; 0.092 sec/batch)
2017-05-06 21:20:12.133603: step 16010, loss = 0.78 (1791.1 examples/sec; 0.071 sec/batch)
2017-05-06 21:20:12.953126: step 16020, loss = 0.96 (1561.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:13.767717: step 16030, loss = 0.75 (1571.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:20:14.588585: step 16040, loss = 0.92 (1559.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:15.405723: step 16050, loss = 0.88 (1566.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:16.219151: step 16060, loss = 1.02 (1573.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:20:17.046229: step 16070, loss = 0.83 (1547.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:20:17.866886: step 16080, loss = 0.78 (1559.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:18.689153: step 16090, loss = 0.84 (1556.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:19.600153: step 16100, loss = 0.92 (1405.0 examples/sec; 0.091 sec/batch)
2017-05-06 21:20:20.315701: step 16110, loss = 0.71 (1788.8 examples/sec; 0.072 sec/batch)
2017-05-06 21:20:21.139224: step 16120, loss = 0.76 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:21.960483: step 16130, loss = 0.91 (1558.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:22.786263: step 16140, loss = 0.67 (1550.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:20:23.605451: step 16150, loss = 0.72 (1562.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:24.426454: step 16160, loss = 0.84 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:25.250315: step 16170, loss = 0.90 (1553.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:26.073066: step 16180, loss = 0.91 (1555.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:26.894358: step 16190, loss = 0.80 (1558.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:27.809695: step 16200, loss = 0.87 (1398.4 examples/sec; 0.092 sec/batch)
2017-05-06 21:20:28.522404: step 16210, loss = 1.01 (1796.0 examples/sec; 0.071 sec/batch)
2017-05-06 21:20:29.349671: step 16220, loss = 0.81 (1547.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:20:30.163590: step 16230, loss = 0.81 (1572.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:20:30.992716: step 16240, loss = 0.92 (1543.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:20:31.806336: step 16250, loss = 0.74 (1573.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:20:32.626131: step 16260, loss = 0.80 (1561.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:33.447316: step 16270, loss = 0.74 (1558.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:34.273415: step 16280, loss = 0.76 (1549.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:20:35.089762: step 16290, loss = 0.71 (1568.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:36.002919: step 16300, loss = 0.93 (1401.7 examples/sec; 0.091 sec/batch)
2017-05-06 21:20:36.714677: step 16310, loss = 0.90 (1798.4 examples/sec; 0.071 sec/batch)
2017-05-06 21:20:37.541946: step 16320, loss = 0.81 (1547.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:20:38.372524: step 16330, loss = 0.86 (1541.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:20:39.193571: step 16340, loss = 0.76 (1559.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:40.006063: step 16350, loss = 0.88 (1575.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:20:40.827733: step 16360, loss = 0.86 (1557.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:41.646459: step 16370, loss = 0.92 (1563.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:42.466387: step 16380, loss = 0.88 (1561.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:43.290944: step 16390, loss = 0.99 (1552.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:44.213844: step 16400, loss = 0.80 (1386.9 examples/sec; 0.092 sec/batch)
2017-05-06 21:20:44.928957: step 16410, loss = 0.82 (1789.9 examples/sec; 0.072 sec/batch)
2017-05-06 21:20:45.754972: step 16420, loss = 0.74 (1549.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:20:46.576556: step 16430, loss = 0.85 (1558.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:47.401073: step 16440, loss = 0.77 (1552.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:48.216556: step 16450, loss = 0.99 (1569.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:49.044454: step 16460, loss = 0.82 (1546.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:20:49.867499: step 16470, loss = 0.80 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:50.685036: step 16480, loss = 1.02 (1565.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:51.496221: step 16490, loss = 0.91 (1577.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:20:52.411258: step 16500, loss = 0.88 (1398.9 examples/sec; 0.092 sec/batch)
2017-05-06 21:20:53.138289: step 16510, loss = 0.99 (1760.6 examples/sec; 0.073 sec/batch)
2017-05-06 21:20:53.954878: step 16520, loss = 0.84 (1567.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:54.778238: step 16530, loss = 1.03 (1554.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:55.600114: step 16540, loss = 0.79 (1557.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:56.419854: step 16550, loss = 0.74 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:57.242335: step 16560, loss = 0.85 (1556.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:58.059588: step 16570, loss = 0.93 (1566.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:58.880562: step 16580, loss = 0.70 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:20:59.690335: step 16590, loss = 0.88 (1580.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:21:00.613700: step 16600, loss = 0.85 (1386.2 examples/sec; 0.092 sec/batch)
2017-05-06 21:21:01.335244: step 16610, loss = 0.86 (1774.0 examples/sec; 0.072 sec/batch)
2017-05-06 21:21:02.156686: step 16620, loss = 0.67 (1558.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:02.974309: step 16630, loss = 0.81 (1565.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:03.793141: step 16640, loss = 0.89 (1563.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:04.616847: step 16650, loss = 0.90 (1554.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:05.441630: step 16660, loss = 0.94 (1551.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:06.264704: step 16670, loss = 0.90 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:07.082957: step 16680, loss = 0.72 (1564.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:07.901548: step 16690, loss = 0.77 (1563.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:08.829750: step 16700, loss = 0.86 (1379.0 examples/sec; 0.093 sec/batch)
2017-05-06 21:21:09.545709: step 16710, loss = 0.69 (1787.8 examples/sec; 0.072 sec/batch)
2017-05-06 21:21:10.362509: step 16720, loss = 0.93 (1567.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:11.183201: step 16730, loss = 0.67 (1559.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:11.997748: step 16740, loss = 0.87 (1571.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:21:12.821878: step 16750, loss = 0.79 (1553.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:13.637373: step 16760, loss = 0.89 (1569.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:14.452579: step 16770, loss = 0.88 (1570.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:15.271063: step 16780, loss = 0.83 (1563.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:16.078020: step 16790, loss = 0.93 (1586.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:21:17.019272: step 16800, loss = 0.98 (1359.9 examples/sec; 0.094 sec/batch)
2017-05-06 21:21:17.716287: step 16810, loss = 0.78 (1836.4 examples/sec; 0.070 sec/batch)
2017-05-06 21:21:18.535401: step 16820, loss = 0.77 (1562.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:19.371017: step 16830, loss = 0.69 (1531.8 examples/sec; 0.084 sec/batch)
2017-05-06 21:21:20.169582: step 16840, loss = 0.82 (1602.9 examples/sec; 0.080 sec/batch)
2017-05-06 21:21:20.992463: step 16850, loss = 0.92 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:21.817147: step 16860, loss = 0.99 (1552.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:22.627643: step 16870, loss = 0.84 (1579.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:21:23.441318: step 16880, loss = 0.85 (1573.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:21:24.264360: step 16890, loss = 0.91 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:25.181681: step 16900, loss = 0.89 (1395.4 examples/sec; 0.092 sec/batch)
2017-05-06 21:21:25.896920: step 16910, loss = 0.86 (1789.6 examples/sec; 0.072 sec/batch)
2017-05-06 21:21:26.714925: step 16920, loss = 0.78 (1564.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:27.545252: step 16930, loss = 0.79 (1541.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:21:28.358185: step 16940, loss = 0.80 (1574.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:21:29.175606: step 16950, loss = 0.75 (1565.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:29.994635: step 16960, loss = 0.77 (1562.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:30.816659: step 16970, loss = 0.72 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:31.634284: step 16980, loss = 0.87 (1565.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:32.457229: step 16990, loss = 0.99 (1555.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:33.377879: step 17000, loss = 0.95 (1390.3 examples/sec; 0.092 sec/batch)
2017-05-06 21:21:34.096727: step 17010, loss = 0.77 (1780.6 examples/sec; 0.072 sec/batch)
2017-05-06 21:21:34.908224: step 17020, loss = 0.93 (1577.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:21:35.717626: step 17030, loss = 0.72 (1581.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:21:36.538760: step 17040, loss = 0.94 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:37.356764: step 17050, loss = 0.77 (1564.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:38.178498: step 17060, loss = 0.85 (1557.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:38.995906: step 17070, loss = 0.91 (1565.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:39.800643: step 17080, loss = 0.98 (1590.6 examples/sec; 0.080 sec/batch)
2017-05-06 21:21:40.623094: step 17090, loss = 0.80 (1556.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:41.547646: step 17100, loss = 0.96 (1384.5 examples/sec; 0.092 sec/batch)
2017-05-06 21:21:42.266175: step 17110, loss = 0.82 (1781.4 examples/sec; 0.072 sec/batch)
2017-05-06 21:21:43.091103: step 17120, loss = 0.79 (1551.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:43.901151: step 17130, loss = 0.74 (1580.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:21:44.714346: step 17140, loss = 0.82 (1574.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:21:45.530762: step 17150, loss = 0.80 (1567.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:46.349454: step 17160, loss = 0.89 (1563.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:47.159406: step 17170, loss = 0.89 (1580.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:21:47.972724: step 17180, loss = 0.79 (1573.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:21:48.795817: step 17190, loss = 0.93 (1555.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:49.707004: step 17200, loss = 0.95 (1404.8 examples/sec; 0.091 sec/batch)
2017-05-06 21:21:50.418789: step 17210, loss = 0.94 (1798.3 examples/sec; 0.071 sec/batch)
2017-05-06 21:21:51.242259: step 17220, loss = 0.78 (1554.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:52.063403: step 17230, loss = 0.85 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:52.875862: step 17240, loss = 1.00 (1575.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:21:53.696030: step 17250, loss = 0.86 (1560.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:54.513036: step 17260, loss = 0.87 (1566.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:55.326190: step 17270, loss = 0.70 (1574.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:21:56.143621: step 17280, loss = 0.87 (1565.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:21:56.956644: step 17290, loss = 0.74 (1574.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:21:57.891295: step 17300, loss = 0.85 (1369.5 examples/sec; 0.093 sec/batch)
2017-05-06 21:21:58.613563: step 17310, loss = 0.89 (1772.2 examples/sec; 0.072 sec/batch)
2017-05-06 21:21:59.434252: step 17320, loss = 0.68 (1559.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:00.252972: step 17330, loss = 0.84 (1563.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:01.078424: step 17340, loss = 0.79 (1550.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:22:01.893258: step 17350, loss = 0.81 (1570.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:22:02.719373: step 17360, loss = 0.72 (1549.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:22:03.533410: step 17370, loss = 0.87 (1572.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:22:04.351606: step 17380, loss = 1.09 (1564.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:05.168573: step 17390, loss = 0.88 (1566.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:06.087685: step 17400, loss = 0.73 (1392.7 examples/sec; 0.092 sec/batch)
2017-05-06 21:22:06.807863: step 17410, loss = 0.96 (1777.3 examples/sec; 0.072 sec/batch)
2017-05-06 21:22:07.624487: step 17420, loss = 0.87 (1567.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:08.454008: step 17430, loss = 0.91 (1543.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:22:09.269583: step 17440, loss = 0.90 (1569.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:10.092348: step 17450, loss = 0.79 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:10.920952: step 17460, loss = 0.91 (1544.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:22:11.729075: step 17470, loss = 0.81 (1583.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:22:12.549189: step 17480, loss = 0.85 (1560.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:13.374136: step 17490, loss = 0.72 (1551.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:14.294811: step 17500, loss = 0.88 (1390.3 examples/sec; 0.092 sec/batch)
2017-05-06 21:22:15.009245: step 17510, loss = 0.86 (1791.6 examples/sec; 0.071 sec/batch)
2017-05-06 21:22:15.822701: step 17520, loss = 0.78 (1573.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:22:16.646853: step 17530, loss = 0.86 (1553.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:17.455319: step 17540, loss = 0.78 (1583.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:22:18.267343: step 17550, loss = 0.85 (1576.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:22:19.084652: step 17560, loss = 0.78 (1566.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:19.895412: step 17570, loss = 0.89 (1578.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:22:20.712159: step 17580, loss = 0.88 (1567.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:21.531108: step 17590, loss = 0.85 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:22.447317: step 17600, loss = 0.88 (1397.1 examples/sec; 0.092 sec/batch)
2017-05-06 21:22:23.166502: step 17610, loss = 0.84 (1779.8 examples/sec; 0.072 sec/batch)
2017-05-06 21:22:23.975544: step 17620, loss = 0.81 (1582.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:22:24.801719: step 17630, loss = 0.92 (1549.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:22:25.618863: step 17640, loss = 0.80 (1566.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:26.435192: step 17650, loss = 0.80 (1568.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:27.253200: step 17660, loss = 0.84 (1564.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:28.064239: step 17670, loss = 0.79 (1578.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:22:28.881806: step 17680, loss = 0.83 (1565.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:29.705495: step 17690, loss = 0.96 (1554.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:30.634483: step 17700, loss = 0.78 (1377.8 examples/sec; 0.093 sec/batch)
2017-05-06 21:22:31.344693: step 17710, loss = 0.81 (1802.3 examples/sec; 0.071 sec/batch)
2017-05-06 21:22:32.151631: step 17720, loss = 0.88 (1586.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:22:32.978069: step 17730, loss = 0.77 (1548.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:22:33.791436: step 17740, loss = 0.73 (1573.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:22:34.615137: step 17750, loss = 0.72 (1554.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:35.430032: step 17760, loss = 0.97 (1570.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:22:36.237712: step 17770, loss = 0.82 (1584.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:22:37.056833: step 17780, loss = 0.91 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:37.875396: step 17790, loss = 0.83 (1563.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:38.811212: step 17800, loss = 0.74 (1367.8 examples/sec; 0.094 sec/batch)
2017-05-06 21:22:39.528858: step 17810, loss = 0.75 (1783.6 examples/sec; 0.072 sec/batch)
2017-05-06 21:22:40.369784: step 17820, loss = 0.89 (1522.1 examples/sec; 0.084 sec/batch)
2017-05-06 21:22:41.163984: step 17830, loss = 0.84 (1611.7 examples/sec; 0.079 sec/batch)
2017-05-06 21:22:41.986650: step 17840, loss = 0.99 (1555.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:42.800899: step 17850, loss = 0.80 (1572.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:22:43.614350: step 17860, loss = 0.66 (1573.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:22:44.436539: step 17870, loss = 0.84 (1556.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:45.254576: step 17880, loss = 0.97 (1564.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:46.069680: step 17890, loss = 0.75 (1570.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:46.993764: step 17900, loss = 0.82 (1385.2 examples/sec; 0.092 sec/batch)
2017-05-06 21:22:47.714850: step 17910, loss = 0.92 (1775.1 examples/sec; 0.072 sec/batch)
2017-05-06 21:22:48.535871: step 17920, loss = 0.81 (1559.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:49.351223: step 17930, loss = 0.83 (1569.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:50.178516: step 17940, loss = 0.81 (1547.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:22:50.994092: step 17950, loss = 0.75 (1569.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:51.815234: step 17960, loss = 0.80 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:52.642222: step 17970, loss = 1.02 (1547.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:22:53.468294: step 17980, loss = 0.85 (1549.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:22:54.286503: step 17990, loss = 0.92 (1564.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:55.217099: step 18000, loss = 0.81 (1375.5 examples/sec; 0.093 sec/batch)
2017-05-06 21:22:55.919201: step 18010, loss = 0.74 (1823.1 examples/sec; 0.070 sec/batch)
2017-05-06 21:22:56.743469: step 18020, loss = 0.76 (1552.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:57.560765: step 18030, loss = 0.77 (1566.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:58.376031: step 18040, loss = 0.72 (1570.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:22:59.198423: step 18050, loss = 0.86 (1556.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:00.005221: step 18060, loss = 1.03 (1586.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:23:00.825843: step 18070, loss = 0.73 (1559.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:01.639910: step 18080, loss = 0.68 (1572.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:23:02.462887: step 18090, loss = 0.70 (1555.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:03.385915: step 18100, loss = 0.71 (1386.7 examples/sec; 0.092 sec/batch)
2017-05-06 21:23:04.096594: step 18110, loss = 0.89 (1801.1 examples/sec; 0.071 sec/batch)
2017-05-06 21:23:04.913225: step 18120, loss = 0.86 (1567.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:05.724822: step 18130, loss = 0.88 (1577.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:23:06.550160: step 18140, loss = 0.77 (1550.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:23:07.358751: step 18150, loss = 0.73 (1583.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:23:08.181395: step 18160, loss = 0.93 (1556.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:09.000187: step 18170, loss = 0.85 (1563.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:09.819060: step 18180, loss = 0.77 (1563.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:10.634617: step 18190, loss = 0.77 (1569.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:11.551627: step 18200, loss = 0.84 (1395.8 examples/sec; 0.092 sec/batch)
2017-05-06 21:23:12.270420: step 18210, loss = 0.83 (1780.8 examples/sec; 0.072 sec/batch)
2017-05-06 21:23:13.101438: step 18220, loss = 1.02 (1540.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:23:13.923474: step 18230, loss = 0.81 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:14.736558: step 18240, loss = 0.74 (1574.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:23:15.547385: step 18250, loss = 0.95 (1578.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:23:16.353962: step 18260, loss = 1.05 (1587.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:23:17.178175: step 18270, loss = 0.86 (1553.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:17.997584: step 18280, loss = 0.79 (1562.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:18.817339: step 18290, loss = 0.94 (1561.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:19.741382: step 18300, loss = 0.71 (1385.2 examples/sec; 0.092 sec/batch)
2017-05-06 21:23:20.453141: step 18310, loss = 0.94 (1798.4 examples/sec; 0.071 sec/batch)
2017-05-06 21:23:21.276822: step 18320, loss = 0.73 (1554.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:22.096275: step 18330, loss = 0.92 (1562.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:22.908389: step 18340, loss = 0.75 (1576.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:23:23.723717: step 18350, loss = 0.93 (1569.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:24.546175: step 18360, loss = 0.71 (1556.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:25.362910: step 18370, loss = 0.70 (1567.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:26.178039: step 18380, loss = 0.82 (1570.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:26.996971: step 18390, loss = 0.84 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:27.906686: step 18400, loss = 0.98 (1407.0 examples/sec; 0.091 sec/batch)
2017-05-06 21:23:28.620793: step 18410, loss = 0.74 (1792.4 examples/sec; 0.071 sec/batch)
2017-05-06 21:23:29.450154: step 18420, loss = 0.88 (1543.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:23:30.264825: step 18430, loss = 0.97 (1571.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:23:31.085658: step 18440, loss = 0.72 (1559.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:31.897007: step 18450, loss = 0.79 (1577.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:23:32.725083: step 18460, loss = 1.01 (1545.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:23:33.551102: step 18470, loss = 0.94 (1549.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:23:34.375355: step 18480, loss = 0.82 (1552.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:35.200422: step 18490, loss = 0.68 (1551.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:23:36.114854: step 18500, loss = 0.89 (1399.8 examples/sec; 0.091 sec/batch)
2017-05-06 21:23:36.837144: step 18510, loss = 0.95 (1772.2 examples/sec; 0.072 sec/batch)
2017-05-06 21:23:37.657331: step 18520, loss = 0.82 (1560.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:38.479125: step 18530, loss = 0.86 (1557.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:39.297115: step 18540, loss = 0.93 (1564.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:40.110091: step 18550, loss = 0.86 (1574.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:23:40.925632: step 18560, loss = 0.85 (1569.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:41.748192: step 18570, loss = 0.83 (1556.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:42.569359: step 18580, loss = 0.82 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:43.390177: step 18590, loss = 1.00 (1559.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:44.312629: step 18600, loss = 0.82 (1387.6 examples/sec; 0.092 sec/batch)
2017-05-06 21:23:45.020350: step 18610, loss = 0.80 (1808.6 examples/sec; 0.071 sec/batch)
2017-05-06 21:23:45.842781: step 18620, loss = 0.95 (1556.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:46.661272: step 18630, loss = 0.75 (1563.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:47.473817: step 18640, loss = 0.93 (1575.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:23:48.293877: step 18650, loss = 0.81 (1560.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:49.114746: step 18660, loss = 0.78 (1559.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:49.937659: step 18670, loss = 0.91 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:50.754832: step 18680, loss = 0.92 (1566.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:51.570112: step 18690, loss = 0.78 (1570.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:52.499774: step 18700, loss = 0.95 (1376.9 examples/sec; 0.093 sec/batch)
2017-05-06 21:23:53.212667: step 18710, loss = 0.94 (1795.5 examples/sec; 0.071 sec/batch)
2017-05-06 21:23:54.027354: step 18720, loss = 0.86 (1571.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:23:54.851516: step 18730, loss = 0.83 (1553.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:55.661801: step 18740, loss = 0.79 (1579.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:23:56.482822: step 18750, loss = 0.80 (1559.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:57.303322: step 18760, loss = 0.80 (1560.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:58.123836: step 18770, loss = 0.95 (1560.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:23:58.950794: step 18780, loss = 0.91 (1547.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:23:59.762287: step 18790, loss = 0.93 (1577.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:24:00.692114: step 18800, loss = 0.79 (1376.6 examples/sec; 0.093 sec/batch)
2017-05-06 21:24:01.437902: step 18810, loss = 0.89 (1716.3 examples/sec; 0.075 sec/batch)
2017-05-06 21:24:02.234189: step 18820, loss = 0.95 (1607.4 examples/sec; 0.080 sec/batch)
2017-05-06 21:24:03.060536: step 18830, loss = 0.81 (1549.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:24:03.869573: step 18840, loss = 0.69 (1582.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:24:04.687621: step 18850, loss = 0.62 (1564.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:05.511639: step 18860, loss = 0.74 (1553.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:06.333391: step 18870, loss = 0.78 (1557.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:07.144953: step 18880, loss = 0.80 (1577.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:24:07.961197: step 18890, loss = 0.91 (1568.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:08.875870: step 18900, loss = 0.86 (1399.4 examples/sec; 0.091 sec/batch)
2017-05-06 21:24:09.606153: step 18910, loss = 0.81 (1752.7 examples/sec; 0.073 sec/batch)
2017-05-06 21:24:10.418895: step 18920, loss = 0.97 (1574.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:24:11.235075: step 18930, loss = 0.78 (1568.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:12.051233: step 18940, loss = 0.74 (1568.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:12.870937: step 18950, loss = 0.78 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:13.695250: step 18960, loss = 0.81 (1552.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:14.518115: step 18970, loss = 0.91 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:15.335567: step 18980, loss = 0.70 (1565.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:16.153235: step 18990, loss = 0.82 (1565.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:17.075592: step 19000, loss = 0.71 (1387.7 examples/sec; 0.092 sec/batch)
2017-05-06 21:24:17.793063: step 19010, loss = 0.76 (1784.1 examples/sec; 0.072 sec/batch)
2017-05-06 21:24:18.615655: step 19020, loss = 0.94 (1556.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:19.434957: step 19030, loss = 0.95 (1562.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:20.256420: step 19040, loss = 0.82 (1558.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:21.075685: step 19050, loss = 0.96 (1562.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:21.897274: step 19060, loss = 0.71 (1558.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:22.718604: step 19070, loss = 0.64 (1558.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:23.535072: step 19080, loss = 0.75 (1567.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:24.355227: step 19090, loss = 0.67 (1560.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:25.273386: step 19100, loss = 0.80 (1394.1 examples/sec; 0.092 sec/batch)
2017-05-06 21:24:25.993620: step 19110, loss = 0.85 (1777.2 examples/sec; 0.072 sec/batch)
2017-05-06 21:24:26.812440: step 19120, loss = 0.78 (1563.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:27.625208: step 19130, loss = 0.83 (1574.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:24:28.437255: step 19140, loss = 1.03 (1576.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:24:29.262766: step 19150, loss = 0.84 (1550.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:24:30.079109: step 19160, loss = 0.77 (1568.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:30.899666: step 19170, loss = 0.76 (1559.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:31.710581: step 19180, loss = 0.98 (1578.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:24:32.528463: step 19190, loss = 0.98 (1565.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:33.450792: step 19200, loss = 0.86 (1387.8 examples/sec; 0.092 sec/batch)
2017-05-06 21:24:34.180693: step 19210, loss = 0.90 (1753.7 examples/sec; 0.073 sec/batch)
2017-05-06 21:24:35.002213: step 19220, loss = 0.68 (1558.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:35.827699: step 19230, loss = 0.96 (1550.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:24:36.644768: step 19240, loss = 0.80 (1566.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:37.468682: step 19250, loss = 0.81 (1553.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:38.291919: step 19260, loss = 0.71 (1554.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:39.110659: step 19270, loss = 0.95 (1563.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:39.924074: step 19280, loss = 0.75 (1573.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:24:40.745798: step 19290, loss = 0.87 (1557.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:41.669623: step 19300, loss = 0.84 (1385.5 examples/sec; 0.092 sec/batch)
2017-05-06 21:24:42.387438: step 19310, loss = 0.83 (1783.2 examples/sec; 0.072 sec/batch)
2017-05-06 21:24:43.202752: step 19320, loss = 0.67 (1569.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:44.012599: step 19330, loss = 0.89 (1580.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:24:44.831891: step 19340, loss = 0.87 (1562.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:45.655176: step 19350, loss = 0.71 (1554.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:46.472299: step 19360, loss = 0.96 (1566.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:47.289480: step 19370, loss = 0.76 (1566.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:48.102074: step 19380, loss = 0.81 (1575.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:24:48.917747: step 19390, loss = 0.67 (1569.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:49.837017: step 19400, loss = 0.73 (1392.4 examples/sec; 0.092 sec/batch)
2017-05-06 21:24:50.565367: step 19410, loss = 0.66 (1757.4 examples/sec; 0.073 sec/batch)
2017-05-06 21:24:51.378728: step 19420, loss = 0.74 (1573.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:24:52.189363: step 19430, loss = 0.77 (1579.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:24:53.004979: step 19440, loss = 0.79 (1569.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:53.828581: step 19450, loss = 0.91 (1554.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:54.647921: step 19460, loss = 0.98 (1562.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:55.466388: step 19470, loss = 0.79 (1563.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:56.281148: step 19480, loss = 0.82 (1571.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:24:57.101353: step 19490, loss = 0.83 (1560.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:24:58.015574: step 19500, loss = 0.99 (1400.1 examples/sec; 0.091 sec/batch)
2017-05-06 21:24:58.734571: step 19510, loss = 0.75 (1780.3 examples/sec; 0.072 sec/batch)
2017-05-06 21:24:59.547873: step 19520, loss = 0.79 (1573.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:25:00.369741: step 19530, loss = 1.06 (1557.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:25:01.183115: step 19540, loss = 0.75 (1573.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:25:01.997727: step 19550, loss = 0.70 (1571.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:25:02.842283: step 19560, loss = 0.82 (1515.6 examples/sec; 0.084 sec/batch)
2017-05-06 21:25:03.652698: step 19570, loss = 0.81 (1579.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:25:04.476061: step 19580, loss = 0.75 (1554.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:25:05.287778: step 19590, loss = 0.97 (1576.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:25:06.204116: step 19600, loss = 0.86 (1396.9 examples/sec; 0.092 sec/batch)
2017-05-06 21:25:06.928448: step 19610, loss = 0.66 (1767.2 examples/sec; 0.072 sec/batch)
2017-05-06 21:25:07.741594: step 19620, loss = 0.78 (1574.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:25:08.559112: step 19630, loss = 0.75 (1565.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:25:09.384876: step 19640, loss = 0.74 (1550.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:25:10.197106: step 19650, loss = 0.84 (1575.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:25:11.017951: step 19660, loss = 0.79 (1559.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:25:11.832917: step 19670, loss = 0.88 (1570.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:25:12.657069: step 19680, loss = 0.84 (1553.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:25:13.480437: step 19690, loss = 0.78 (1554.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:25:14.402220: step 19700, loss = 0.95 (1388.6 examples/sec; 0.092 sec/batch)
2017-05-06 21:25:15.123338: step 19710, loss = 0.86 (1775.0 examples/sec; 0.072 sec/batch)
2017-05-06 21:25:15.936126: step 19720, loss = 0.81 (1574.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:25:16.758520: step 19730, loss = 0.87 (1556.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:25:17.585235: step 19740, loss = 0.71 (1548.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:25:18.394843: step 19750, loss = 0.93 (1581.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:25:19.226751: step 19760, loss = 0.71 (1538.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:25:20.030873: step 19770, loss = 0.86 (1591.8 examples/sec; 0.080 sec/batch)
2017-05-06 21:25:20.860871: step 19780, loss = 0.86 (1542.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:25:21.689516: step 19790, loss = 0.87 (1544.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:25:22.664021: step 19800, loss = 0.69 (1313.5 examples/sec; 0.097 sec/batch)
2017-05-06 21:25:23.344178: step 19810, loss = 0.85 (1881.9 examples/sec; 0.068 sec/batch)
2017-05-06 21:25:24.157025: step 19820, loss = 0.73 (1574.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:25:24.977339: step 19830, loss = 0.78 (1560.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:25:25.799904: step 19840, loss = 0.91 (1556.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:25:26.619087: step 19850, loss = 0.74 (1562.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:25:27.431273: step 19860, loss = 0.92 (1576.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:25:28.247578: step 19870, loss = 0.87 (1568.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:25:29.068622: step 19880, loss = 0.87 (1559.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:25:29.895456: step 19890, loss = 0.77 (1548.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:25:30.827323: step 19900, loss = 0.80 (1373.6 examples/sec; 0.093 sec/batch)
2017-05-06 21:25:31.542865: step 19910, loss = 0.85 (1788.9 examples/sec; 0.072 sec/batch)
2017-05-06 21:25:32.358639: step 19920, loss = 0.87 (1569.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:25:33.170129: step 19930, loss = 0.89 (1577.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:25:33.992309: step 19940, loss = 0.74 (1556.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:25:34.820512: step 19950, loss = 0.74 (1545.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:25:35.624496: step 19960, loss = 0.83 (1592.1 examples/sec; 0.080 sec/batch)
2017-05-06 21:25:36.440110: step 19970, loss = 0.81 (1569.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:25:37.258105: step 19980, loss = 0.76 (1564.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:25:38.081376: step 19990, loss = 0.82 (1554.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:25:39.001406: step 20000, loss = 0.67 (1391.3 examples/sec; 0.092 sec/batch)
2017-05-06 21:25:39.714510: step 20010, loss = 0.86 (1795.0 examples/sec; 0.071 sec/batch)
2017-05-06 21:25:40.534201: step 20020, loss = 0.73 (1561.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:25:41.355319: step 20030, loss = 0.86 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:25:42.184030: step 20040, loss = 0.77 (1544.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:25:42.995221: step 20050, loss = 0.75 (1577.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:25:43.810670: step 20060, loss = 0.96 (1569.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:25:44.625929: step 20070, loss = 0.83 (1570.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:25:45.445218: step 20080, loss = 0.66 (1562.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:25:46.265206: step 20090, loss = 0.83 (1561.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:25:47.188338: step 20100, loss = 0.89 (1386.6 examples/sec; 0.092 sec/batch)
2017-05-06 21:25:47.900934: step 20110, loss = 0.74 (1796.3 examples/sec; 0.071 sec/batch)
2017-05-06 21:25:48.720956: step 20120, loss = 0.92 (1560.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:25:49.539537: step 20130, loss = 0.97 (1563.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:25:50.358569: step 20140, loss = 0.86 (1562.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:25:51.176843: step 20150, loss = 0.97 (1564.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:25:51.990897: step 20160, loss = 1.01 (1572.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:25:52.808321: step 20170, loss = 0.71 (1565.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:25:53.629147: step 20180, loss = 0.73 (1559.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:25:54.447831: step 20190, loss = 0.91 (1563.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:25:55.374124: step 20200, loss = 0.89 (1381.9 examples/sec; 0.093 sec/batch)
2017-05-06 21:25:56.084786: step 20210, loss = 0.96 (1801.1 examples/sec; 0.071 sec/batch)
2017-05-06 21:25:56.911439: step 20220, loss = 0.85 (1548.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:25:57.730558: step 20230, loss = 0.93 (1562.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:25:58.549156: step 20240, loss = 0.69 (1563.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:25:59.369094: step 20250, loss = 0.86 (1561.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:00.176737: step 20260, loss = 0.81 (1584.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:26:00.986289: step 20270, loss = 0.98 (1581.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:26:01.808986: step 20280, loss = 0.86 (1555.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:02.627250: step 20290, loss = 0.89 (1564.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:03.556505: step 20300, loss = 0.84 (1377.5 examples/sec; 0.093 sec/batch)
2017-05-06 21:26:04.274498: step 20310, loss = 0.77 (1782.7 examples/sec; 0.072 sec/batch)
2017-05-06 21:26:05.094737: step 20320, loss = 0.88 (1560.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:05.914507: step 20330, loss = 0.70 (1561.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:06.736893: step 20340, loss = 0.85 (1556.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:07.555843: step 20350, loss = 0.82 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:08.379236: step 20360, loss = 0.79 (1554.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:09.197093: step 20370, loss = 0.82 (1565.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:10.016226: step 20380, loss = 0.83 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:10.839415: step 20390, loss = 0.77 (1554.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:11.770370: step 20400, loss = 1.00 (1374.9 examples/sec; 0.093 sec/batch)
2017-05-06 21:26:12.483659: step 20410, loss = 0.86 (1794.5 examples/sec; 0.071 sec/batch)
2017-05-06 21:26:13.311304: step 20420, loss = 0.86 (1546.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:26:14.125965: step 20430, loss = 0.71 (1571.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:26:14.950669: step 20440, loss = 0.61 (1552.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:15.757263: step 20450, loss = 0.85 (1586.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:26:16.577962: step 20460, loss = 0.80 (1559.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:17.397835: step 20470, loss = 0.75 (1561.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:18.217881: step 20480, loss = 0.72 (1560.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:19.040295: step 20490, loss = 0.77 (1556.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:19.977475: step 20500, loss = 0.77 (1365.8 examples/sec; 0.094 sec/batch)
2017-05-06 21:26:20.680568: step 20510, loss = 0.82 (1820.5 examples/sec; 0.070 sec/batch)
2017-05-06 21:26:21.500708: step 20520, loss = 0.74 (1560.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:22.329415: step 20530, loss = 0.81 (1544.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:26:23.145232: step 20540, loss = 0.89 (1569.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:23.958409: step 20550, loss = 0.86 (1574.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:26:24.775916: step 20560, loss = 1.04 (1565.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:25.595364: step 20570, loss = 0.78 (1562.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:26.413131: step 20580, loss = 0.88 (1565.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:27.232651: step 20590, loss = 0.88 (1561.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:28.152556: step 20600, loss = 1.07 (1391.4 examples/sec; 0.092 sec/batch)
2017-05-06 21:26:28.867418: step 20610, loss = 0.81 (1790.6 examples/sec; 0.071 sec/batch)
2017-05-06 21:26:29.686827: step 20620, loss = 0.75 (1562.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:30.507796: step 20630, loss = 0.84 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:31.327780: step 20640, loss = 0.78 (1561.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:32.139054: step 20650, loss = 0.84 (1577.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:26:32.967522: step 20660, loss = 0.91 (1545.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:26:33.779442: step 20670, loss = 0.80 (1576.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:26:34.603516: step 20680, loss = 0.76 (1553.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:35.419933: step 20690, loss = 0.73 (1567.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:36.335271: step 20700, loss = 0.89 (1398.4 examples/sec; 0.092 sec/batch)
2017-05-06 21:26:37.062239: step 20710, loss = 0.78 (1760.7 examples/sec; 0.073 sec/batch)
2017-05-06 21:26:37.880308: step 20720, loss = 1.01 (1564.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:38.696240: step 20730, loss = 0.94 (1568.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:39.508236: step 20740, loss = 0.80 (1576.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:26:40.328831: step 20750, loss = 0.81 (1559.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:41.148816: step 20760, loss = 0.92 (1561.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:41.970982: step 20770, loss = 0.79 (1556.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:42.799531: step 20780, loss = 0.86 (1544.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:26:43.638532: step 20790, loss = 0.74 (1525.6 examples/sec; 0.084 sec/batch)
2017-05-06 21:26:44.525384: step 20800, loss = 0.98 (1443.3 examples/sec; 0.089 sec/batch)
2017-05-06 21:26:45.241434: step 20810, loss = 0.88 (1787.6 examples/sec; 0.072 sec/batch)
2017-05-06 21:26:46.065281: step 20820, loss = 0.84 (1553.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:46.886265: step 20830, loss = 0.76 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:47.702889: step 20840, loss = 0.84 (1567.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:48.523179: step 20850, loss = 0.92 (1560.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:49.348577: step 20860, loss = 1.03 (1550.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:26:50.171978: step 20870, loss = 0.91 (1554.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:50.994178: step 20880, loss = 0.91 (1556.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:51.822961: step 20890, loss = 0.93 (1544.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:26:52.730510: step 20900, loss = 0.93 (1410.4 examples/sec; 0.091 sec/batch)
2017-05-06 21:26:53.449609: step 20910, loss = 0.85 (1780.0 examples/sec; 0.072 sec/batch)
2017-05-06 21:26:54.272294: step 20920, loss = 0.93 (1555.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:55.090096: step 20930, loss = 0.82 (1565.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:55.910982: step 20940, loss = 0.65 (1559.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:56.735342: step 20950, loss = 0.85 (1552.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:57.558276: step 20960, loss = 0.69 (1555.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:58.375776: step 20970, loss = 0.83 (1565.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:26:59.191953: step 20980, loss = 0.93 (1568.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:00.005107: step 20990, loss = 0.76 (1574.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:27:00.925064: step 21000, loss = 0.91 (1391.4 examples/sec; 0.092 sec/batch)
2017-05-06 21:27:01.646895: step 21010, loss = 0.90 (1773.2 examples/sec; 0.072 sec/batch)
2017-05-06 21:27:02.468069: step 21020, loss = 0.82 (1558.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:03.286601: step 21030, loss = 0.88 (1563.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:04.106666: step 21040, loss = 0.80 (1560.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:04.929981: step 21050, loss = 0.77 (1554.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:05.748213: step 21060, loss = 0.82 (1564.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:06.571505: step 21070, loss = 0.75 (1554.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:07.385592: step 21080, loss = 0.80 (1572.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:27:08.203217: step 21090, loss = 0.81 (1565.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:09.114200: step 21100, loss = 0.94 (1405.1 examples/sec; 0.091 sec/batch)
2017-05-06 21:27:09.839038: step 21110, loss = 0.64 (1765.9 examples/sec; 0.072 sec/batch)
2017-05-06 21:27:10.656811: step 21120, loss = 0.98 (1565.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:11.480654: step 21130, loss = 0.74 (1553.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:12.294024: step 21140, loss = 0.86 (1573.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:27:13.113876: step 21150, loss = 0.82 (1561.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:13.934548: step 21160, loss = 0.87 (1559.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:14.749397: step 21170, loss = 0.79 (1570.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:27:15.567291: step 21180, loss = 0.91 (1565.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:16.381494: step 21190, loss = 0.80 (1572.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:27:17.298380: step 21200, loss = 0.89 (1396.0 examples/sec; 0.092 sec/batch)
2017-05-06 21:27:18.026282: step 21210, loss = 0.69 (1758.5 examples/sec; 0.073 sec/batch)
2017-05-06 21:27:18.849421: step 21220, loss = 0.82 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:19.664861: step 21230, loss = 0.81 (1569.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:20.483250: step 21240, loss = 0.82 (1564.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:21.300910: step 21250, loss = 0.90 (1565.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:22.123081: step 21260, loss = 0.87 (1556.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:22.944253: step 21270, loss = 0.65 (1558.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:23.760522: step 21280, loss = 0.96 (1568.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:24.576364: step 21290, loss = 0.84 (1568.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:25.495413: step 21300, loss = 0.78 (1392.8 examples/sec; 0.092 sec/batch)
2017-05-06 21:27:26.210835: step 21310, loss = 0.69 (1789.1 examples/sec; 0.072 sec/batch)
2017-05-06 21:27:27.024426: step 21320, loss = 0.80 (1573.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:27:27.831960: step 21330, loss = 0.93 (1585.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:27:28.652565: step 21340, loss = 0.74 (1559.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:29.472491: step 21350, loss = 0.89 (1561.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:30.293070: step 21360, loss = 0.61 (1559.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:31.111160: step 21370, loss = 0.81 (1564.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:31.923818: step 21380, loss = 0.86 (1575.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:27:32.748464: step 21390, loss = 0.84 (1552.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:33.665715: step 21400, loss = 0.79 (1395.5 examples/sec; 0.092 sec/batch)
2017-05-06 21:27:34.386599: step 21410, loss = 0.90 (1775.6 examples/sec; 0.072 sec/batch)
2017-05-06 21:27:35.206802: step 21420, loss = 0.93 (1560.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:36.010497: step 21430, loss = 0.98 (1592.7 examples/sec; 0.080 sec/batch)
2017-05-06 21:27:36.824566: step 21440, loss = 0.75 (1572.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:27:37.639685: step 21450, loss = 0.88 (1570.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:38.454363: step 21460, loss = 0.72 (1571.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:27:39.274212: step 21470, loss = 0.88 (1561.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:40.088028: step 21480, loss = 1.01 (1572.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:27:40.911424: step 21490, loss = 0.80 (1554.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:41.830803: step 21500, loss = 0.77 (1392.2 examples/sec; 0.092 sec/batch)
2017-05-06 21:27:42.550739: step 21510, loss = 0.86 (1777.9 examples/sec; 0.072 sec/batch)
2017-05-06 21:27:43.366899: step 21520, loss = 0.77 (1568.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:44.181572: step 21530, loss = 0.85 (1571.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:27:44.999412: step 21540, loss = 0.80 (1565.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:45.818580: step 21550, loss = 0.84 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:46.642784: step 21560, loss = 0.92 (1553.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:47.454133: step 21570, loss = 0.80 (1577.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:27:48.273027: step 21580, loss = 0.62 (1563.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:49.104754: step 21590, loss = 0.92 (1539.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:27:50.017729: step 21600, loss = 0.71 (1402.0 examples/sec; 0.091 sec/batch)
2017-05-06 21:27:50.738822: step 21610, loss = 1.05 (1775.1 examples/sec; 0.072 sec/batch)
2017-05-06 21:27:51.547931: step 21620, loss = 0.73 (1582.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:27:52.363137: step 21630, loss = 0.84 (1570.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:53.187705: step 21640, loss = 0.91 (1552.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:54.013199: step 21650, loss = 0.92 (1550.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:27:54.825935: step 21660, loss = 0.91 (1574.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:27:55.639185: step 21670, loss = 0.98 (1573.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:27:56.459941: step 21680, loss = 1.07 (1559.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:57.283808: step 21690, loss = 0.84 (1553.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:27:58.198674: step 21700, loss = 0.77 (1399.1 examples/sec; 0.091 sec/batch)
2017-05-06 21:27:58.917626: step 21710, loss = 0.79 (1780.4 examples/sec; 0.072 sec/batch)
2017-05-06 21:27:59.724657: step 21720, loss = 0.91 (1586.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:28:00.544506: step 21730, loss = 0.93 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:28:01.364463: step 21740, loss = 0.92 (1560.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:28:02.204720: step 21750, loss = 0.88 (1523.3 examples/sec; 0.084 sec/batch)
2017-05-06 21:28:03.028253: step 21760, loss = 0.86 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:28:03.845813: step 21770, loss = 0.55 (1565.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:28:04.687948: step 21780, loss = 1.11 (1519.9 examples/sec; 0.084 sec/batch)
2017-05-06 21:28:05.485143: step 21790, loss = 0.87 (1605.6 examples/sec; 0.080 sec/batch)
2017-05-06 21:28:06.396477: step 21800, loss = 0.76 (1404.5 examples/sec; 0.091 sec/batch)
2017-05-06 21:28:07.122997: step 21810, loss = 0.70 (1761.8 examples/sec; 0.073 sec/batch)
2017-05-06 21:28:07.930875: step 21820, loss = 0.69 (1584.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:28:08.749394: step 21830, loss = 0.76 (1563.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:28:09.558946: step 21840, loss = 0.74 (1581.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:28:10.380756: step 21850, loss = 0.79 (1557.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:28:11.206793: step 21860, loss = 0.70 (1549.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:28:12.019211: step 21870, loss = 0.80 (1575.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:28:12.906146: step 21880, loss = 1.02 (1443.2 examples/sec; 0.089 sec/batch)
2017-05-06 21:28:13.695266: step 21890, loss = 0.79 (1622.0 examples/sec; 0.079 sec/batch)
2017-05-06 21:28:14.630258: step 21900, loss = 0.85 (1369.0 examples/sec; 0.093 sec/batch)
2017-05-06 21:28:15.340902: step 21910, loss = 0.89 (1801.2 examples/sec; 0.071 sec/batch)
2017-05-06 21:28:16.151444: step 21920, loss = 1.03 (1579.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:28:16.971393: step 21930, loss = 0.86 (1561.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:28:17.795198: step 21940, loss = 0.79 (1553.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:28:18.618442: step 21950, loss = 0.76 (1554.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:28:19.434845: step 21960, loss = 0.75 (1567.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:28:20.253676: step 21970, loss = 0.84 (1563.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:28:21.083836: step 21980, loss = 0.87 (1541.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:28:21.906875: step 21990, loss = 0.80 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:28:22.835823: step 22000, loss = 0.86 (1377.9 examples/sec; 0.093 sec/batch)
2017-05-06 21:28:23.535635: step 22010, loss = 0.74 (1829.1 examples/sec; 0.070 sec/batch)
2017-05-06 21:28:24.356119: step 22020, loss = 0.86 (1560.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:28:25.176517: step 22030, loss = 0.76 (1560.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:28:25.996520: step 22040, loss = 0.77 (1561.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:28:26.827323: step 22050, loss = 0.73 (1540.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:28:27.638218: step 22060, loss = 0.84 (1578.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:28:28.465284: step 22070, loss = 0.82 (1547.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:28:29.287793: step 22080, loss = 0.62 (1556.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:28:30.104215: step 22090, loss = 0.81 (1567.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:28:31.028002: step 22100, loss = 0.90 (1385.6 examples/sec; 0.092 sec/batch)
2017-05-06 21:28:31.740595: step 22110, loss = 0.69 (1796.3 examples/sec; 0.071 sec/batch)
2017-05-06 21:28:32.567039: step 22120, loss = 0.68 (1548.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:28:33.399384: step 22130, loss = 0.79 (1537.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:28:34.220314: step 22140, loss = 0.89 (1559.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:28:35.040354: step 22150, loss = 0.79 (1560.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:28:35.851757: step 22160, loss = 0.91 (1577.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:28:36.678392: step 22170, loss = 0.89 (1548.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:28:37.500642: step 22180, loss = 0.76 (1556.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:28:38.320284: step 22190, loss = 0.90 (1561.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:28:39.240565: step 22200, loss = 0.83 (1390.9 examples/sec; 0.092 sec/batch)
2017-05-06 21:28:39.953597: step 22210, loss = 0.84 (1795.1 examples/sec; 0.071 sec/batch)
2017-05-06 21:28:40.778867: step 22220, loss = 0.75 (1551.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:28:41.600382: step 22230, loss = 0.84 (1558.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:28:42.422148: step 22240, loss = 0.81 (1557.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:28:43.240098: step 22250, loss = 0.85 (1564.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:28:44.053709: step 22260, loss = 0.72 (1573.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:28:44.880472: step 22270, loss = 0.71 (1548.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:28:45.702383: step 22280, loss = 0.65 (1557.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:28:46.532878: step 22290, loss = 0.66 (1541.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:28:47.448902: step 22300, loss = 0.77 (1397.3 examples/sec; 0.092 sec/batch)
2017-05-06 21:28:48.169563: step 22310, loss = 0.78 (1776.1 examples/sec; 0.072 sec/batch)
2017-05-06 21:28:48.982417: step 22320, loss = 0.75 (1574.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:28:49.804569: step 22330, loss = 0.83 (1556.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:28:50.633575: step 22340, loss = 0.94 (1544.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:28:51.454130: step 22350, loss = 0.77 (1559.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:28:52.272955: step 22360, loss = 1.03 (1563.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:28:53.089155: step 22370, loss = 0.91 (1568.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:28:53.915611: step 22380, loss = 0.86 (1548.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:28:54.739525: step 22390, loss = 0.85 (1553.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:28:55.651734: step 22400, loss = 1.05 (1403.2 examples/sec; 0.091 sec/batch)
2017-05-06 21:28:56.369466: step 22410, loss = 0.76 (1783.4 examples/sec; 0.072 sec/batch)
2017-05-06 21:28:57.197074: step 22420, loss = 0.76 (1546.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:28:58.020178: step 22430, loss = 0.78 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:28:58.838816: step 22440, loss = 0.80 (1563.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:28:59.656868: step 22450, loss = 0.81 (1564.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:00.478788: step 22460, loss = 0.87 (1557.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:01.297950: step 22470, loss = 1.08 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:02.124981: step 22480, loss = 0.77 (1547.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:29:02.950645: step 22490, loss = 0.79 (1550.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:29:03.869880: step 22500, loss = 0.79 (1392.5 examples/sec; 0.092 sec/batch)
2017-05-06 21:29:04.588588: step 22510, loss = 0.83 (1781.0 examples/sec; 0.072 sec/batch)
2017-05-06 21:29:05.414597: step 22520, loss = 0.75 (1549.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:29:06.232823: step 22530, loss = 0.82 (1564.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:07.051674: step 22540, loss = 0.80 (1563.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:07.868955: step 22550, loss = 0.85 (1566.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:08.683768: step 22560, loss = 0.81 (1570.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:29:09.501199: step 22570, loss = 0.99 (1565.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:10.324430: step 22580, loss = 0.86 (1554.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:11.136156: step 22590, loss = 0.87 (1576.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:29:12.054959: step 22600, loss = 0.64 (1393.1 examples/sec; 0.092 sec/batch)
2017-05-06 21:29:12.770805: step 22610, loss = 0.81 (1788.1 examples/sec; 0.072 sec/batch)
2017-05-06 21:29:13.589683: step 22620, loss = 0.90 (1563.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:14.408099: step 22630, loss = 0.71 (1564.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:15.218683: step 22640, loss = 0.65 (1579.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:29:16.027799: step 22650, loss = 0.89 (1582.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:29:16.847745: step 22660, loss = 0.66 (1561.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:17.666931: step 22670, loss = 0.93 (1562.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:18.494766: step 22680, loss = 0.78 (1546.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:29:19.307108: step 22690, loss = 0.77 (1575.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:29:20.231495: step 22700, loss = 0.88 (1384.7 examples/sec; 0.092 sec/batch)
2017-05-06 21:29:20.942831: step 22710, loss = 0.80 (1799.4 examples/sec; 0.071 sec/batch)
2017-05-06 21:29:21.772089: step 22720, loss = 0.98 (1543.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:29:22.587809: step 22730, loss = 0.67 (1569.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:23.411007: step 22740, loss = 0.74 (1554.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:24.218422: step 22750, loss = 0.88 (1585.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:29:25.047654: step 22760, loss = 0.80 (1543.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:29:25.898069: step 22770, loss = 0.85 (1505.2 examples/sec; 0.085 sec/batch)
2017-05-06 21:29:26.696111: step 22780, loss = 0.80 (1603.9 examples/sec; 0.080 sec/batch)
2017-05-06 21:29:27.524231: step 22790, loss = 0.83 (1545.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:29:28.450672: step 22800, loss = 0.80 (1381.6 examples/sec; 0.093 sec/batch)
2017-05-06 21:29:29.157665: step 22810, loss = 0.80 (1810.5 examples/sec; 0.071 sec/batch)
2017-05-06 21:29:29.981277: step 22820, loss = 0.84 (1554.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:30.802096: step 22830, loss = 0.77 (1559.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:31.619433: step 22840, loss = 0.80 (1566.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:32.434952: step 22850, loss = 0.74 (1569.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:33.257153: step 22860, loss = 0.95 (1556.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:34.090027: step 22870, loss = 1.01 (1536.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:29:34.897711: step 22880, loss = 0.81 (1584.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:29:35.711762: step 22890, loss = 0.87 (1572.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:29:36.632069: step 22900, loss = 0.85 (1390.9 examples/sec; 0.092 sec/batch)
2017-05-06 21:29:37.353447: step 22910, loss = 0.82 (1774.3 examples/sec; 0.072 sec/batch)
2017-05-06 21:29:38.173883: step 22920, loss = 0.80 (1560.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:38.996666: step 22930, loss = 1.02 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:39.812270: step 22940, loss = 0.70 (1569.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:40.627345: step 22950, loss = 0.78 (1570.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:41.454663: step 22960, loss = 0.82 (1547.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:29:42.280437: step 22970, loss = 0.84 (1550.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:29:43.101917: step 22980, loss = 0.69 (1558.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:43.921532: step 22990, loss = 0.83 (1561.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:44.849114: step 23000, loss = 0.79 (1379.9 examples/sec; 0.093 sec/batch)
2017-05-06 21:29:45.566964: step 23010, loss = 0.78 (1783.1 examples/sec; 0.072 sec/batch)
2017-05-06 21:29:46.388641: step 23020, loss = 0.78 (1557.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:47.208943: step 23030, loss = 0.89 (1560.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:48.022645: step 23040, loss = 0.86 (1573.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:29:48.842002: step 23050, loss = 0.79 (1562.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:49.667539: step 23060, loss = 0.69 (1550.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:29:50.490879: step 23070, loss = 0.86 (1554.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:51.313490: step 23080, loss = 0.67 (1556.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:52.124978: step 23090, loss = 0.81 (1577.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:29:53.045801: step 23100, loss = 0.72 (1390.1 examples/sec; 0.092 sec/batch)
2017-05-06 21:29:53.773563: step 23110, loss = 0.73 (1758.8 examples/sec; 0.073 sec/batch)
2017-05-06 21:29:54.588244: step 23120, loss = 0.67 (1571.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:29:55.397878: step 23130, loss = 0.85 (1581.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:29:56.205094: step 23140, loss = 0.83 (1585.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:29:57.023993: step 23150, loss = 0.78 (1563.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:57.845464: step 23160, loss = 0.72 (1558.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:58.666874: step 23170, loss = 0.91 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:29:59.482350: step 23180, loss = 0.79 (1569.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:00.300803: step 23190, loss = 0.81 (1563.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:01.222753: step 23200, loss = 0.94 (1388.4 examples/sec; 0.092 sec/batch)
2017-05-06 21:30:01.958404: step 23210, loss = 0.91 (1740.0 examples/sec; 0.074 sec/batch)
2017-05-06 21:30:02.782513: step 23220, loss = 0.74 (1553.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:03.595872: step 23230, loss = 0.87 (1573.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:30:04.425189: step 23240, loss = 0.91 (1543.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:30:05.243440: step 23250, loss = 0.77 (1564.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:06.069804: step 23260, loss = 0.78 (1549.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:30:06.887408: step 23270, loss = 0.73 (1565.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:07.699165: step 23280, loss = 0.95 (1576.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:30:08.519192: step 23290, loss = 0.80 (1560.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:09.443236: step 23300, loss = 0.83 (1385.2 examples/sec; 0.092 sec/batch)
2017-05-06 21:30:10.165591: step 23310, loss = 0.79 (1772.0 examples/sec; 0.072 sec/batch)
2017-05-06 21:30:10.981783: step 23320, loss = 0.87 (1568.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:11.787404: step 23330, loss = 0.88 (1588.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:30:12.601321: step 23340, loss = 0.87 (1572.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:30:13.419160: step 23350, loss = 1.00 (1565.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:14.239501: step 23360, loss = 0.70 (1560.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:15.059444: step 23370, loss = 0.79 (1561.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:15.869534: step 23380, loss = 0.88 (1580.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:30:16.685265: step 23390, loss = 0.74 (1569.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:17.606562: step 23400, loss = 0.77 (1389.3 examples/sec; 0.092 sec/batch)
2017-05-06 21:30:18.333902: step 23410, loss = 0.81 (1759.8 examples/sec; 0.073 sec/batch)
2017-05-06 21:30:19.156175: step 23420, loss = 0.83 (1556.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:19.975215: step 23430, loss = 0.72 (1562.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:20.800610: step 23440, loss = 0.91 (1550.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:30:21.618927: step 23450, loss = 0.79 (1564.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:22.436133: step 23460, loss = 0.80 (1566.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:23.266069: step 23470, loss = 0.68 (1542.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:30:24.076015: step 23480, loss = 0.84 (1580.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:30:24.896637: step 23490, loss = 0.88 (1559.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:25.812309: step 23500, loss = 0.80 (1397.9 examples/sec; 0.092 sec/batch)
2017-05-06 21:30:26.528845: step 23510, loss = 0.84 (1786.4 examples/sec; 0.072 sec/batch)
2017-05-06 21:30:27.341906: step 23520, loss = 0.82 (1574.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:30:28.157999: step 23530, loss = 0.76 (1568.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:28.977644: step 23540, loss = 0.80 (1561.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:29.797401: step 23550, loss = 0.90 (1561.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:30.608704: step 23560, loss = 0.70 (1577.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:30:31.436942: step 23570, loss = 0.83 (1545.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:30:32.256512: step 23580, loss = 0.74 (1561.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:33.072996: step 23590, loss = 0.89 (1567.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:33.995455: step 23600, loss = 0.80 (1387.6 examples/sec; 0.092 sec/batch)
2017-05-06 21:30:34.719130: step 23610, loss = 0.72 (1768.7 examples/sec; 0.072 sec/batch)
2017-05-06 21:30:35.537644: step 23620, loss = 0.83 (1563.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:36.349950: step 23630, loss = 0.89 (1575.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:30:37.166574: step 23640, loss = 0.88 (1567.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:37.991873: step 23650, loss = 0.82 (1551.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:30:38.813892: step 23660, loss = 0.74 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:39.634896: step 23670, loss = 0.90 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:40.449319: step 23680, loss = 0.70 (1571.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:30:41.273549: step 23690, loss = 0.81 (1553.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:42.184522: step 23700, loss = 0.78 (1405.1 examples/sec; 0.091 sec/batch)
2017-05-06 21:30:42.908179: step 23710, loss = 0.79 (1768.8 examples/sec; 0.072 sec/batch)
2017-05-06 21:30:43.720519: step 23720, loss = 0.90 (1575.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:30:44.540779: step 23730, loss = 0.83 (1560.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:45.357331: step 23740, loss = 0.72 (1567.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:46.184849: step 23750, loss = 0.76 (1546.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:30:47.024165: step 23760, loss = 0.90 (1525.1 examples/sec; 0.084 sec/batch)
2017-05-06 21:30:47.811289: step 23770, loss = 0.87 (1626.2 examples/sec; 0.079 sec/batch)
2017-05-06 21:30:48.632364: step 23780, loss = 0.87 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:49.459744: step 23790, loss = 0.77 (1547.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:30:50.381961: step 23800, loss = 0.86 (1388.0 examples/sec; 0.092 sec/batch)
2017-05-06 21:30:51.095632: step 23810, loss = 0.76 (1793.6 examples/sec; 0.071 sec/batch)
2017-05-06 21:30:51.905167: step 23820, loss = 0.81 (1581.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:30:52.724154: step 23830, loss = 1.00 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:53.536554: step 23840, loss = 0.87 (1575.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:30:54.358686: step 23850, loss = 0.69 (1556.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:55.180458: step 23860, loss = 0.73 (1557.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:55.994771: step 23870, loss = 0.66 (1571.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:30:56.816925: step 23880, loss = 0.65 (1556.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:30:57.631088: step 23890, loss = 0.83 (1572.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:30:58.562535: step 23900, loss = 0.95 (1374.2 examples/sec; 0.093 sec/batch)
2017-05-06 21:30:59.277365: step 23910, loss = 0.89 (1790.6 examples/sec; 0.071 sec/batch)
2017-05-06 21:31:00.086559: step 23920, loss = 0.77 (1581.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:31:00.903969: step 23930, loss = 0.78 (1565.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:01.731860: step 23940, loss = 0.75 (1546.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:31:02.554860: step 23950, loss = 0.96 (1555.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:03.375365: step 23960, loss = 0.96 (1560.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:04.187828: step 23970, loss = 0.85 (1575.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:31:05.002731: step 23980, loss = 0.72 (1570.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:31:05.833800: step 23990, loss = 0.85 (1540.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:31:06.761749: step 24000, loss = 0.80 (1379.4 examples/sec; 0.093 sec/batch)
2017-05-06 21:31:07.473214: step 24010, loss = 0.78 (1799.1 examples/sec; 0.071 sec/batch)
2017-05-06 21:31:08.291312: step 24020, loss = 0.83 (1564.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:09.113617: step 24030, loss = 0.84 (1556.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:09.935821: step 24040, loss = 0.81 (1556.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:10.758487: step 24050, loss = 0.79 (1555.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:11.573331: step 24060, loss = 0.91 (1570.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:31:12.386082: step 24070, loss = 0.80 (1575.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:31:13.210812: step 24080, loss = 0.72 (1552.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:14.030247: step 24090, loss = 0.78 (1562.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:14.949929: step 24100, loss = 0.74 (1391.8 examples/sec; 0.092 sec/batch)
2017-05-06 21:31:15.653395: step 24110, loss = 0.81 (1819.6 examples/sec; 0.070 sec/batch)
2017-05-06 21:31:16.472458: step 24120, loss = 0.91 (1562.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:17.297924: step 24130, loss = 0.73 (1550.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:31:18.118014: step 24140, loss = 0.76 (1560.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:18.941548: step 24150, loss = 0.83 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:19.756831: step 24160, loss = 0.58 (1570.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:20.577236: step 24170, loss = 0.73 (1560.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:21.398036: step 24180, loss = 0.82 (1559.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:22.219571: step 24190, loss = 0.89 (1558.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:23.129780: step 24200, loss = 0.79 (1406.3 examples/sec; 0.091 sec/batch)
2017-05-06 21:31:23.838559: step 24210, loss = 0.90 (1805.9 examples/sec; 0.071 sec/batch)
2017-05-06 21:31:24.656910: step 24220, loss = 0.81 (1564.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:25.473352: step 24230, loss = 0.70 (1567.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:26.295793: step 24240, loss = 0.89 (1556.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:27.114798: step 24250, loss = 0.77 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:27.920983: step 24260, loss = 0.92 (1587.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:31:28.743088: step 24270, loss = 0.75 (1557.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:29.561794: step 24280, loss = 0.95 (1563.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:30.381363: step 24290, loss = 0.73 (1561.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:31.306419: step 24300, loss = 0.76 (1383.7 examples/sec; 0.093 sec/batch)
2017-05-06 21:31:32.011746: step 24310, loss = 0.80 (1814.8 examples/sec; 0.071 sec/batch)
2017-05-06 21:31:32.836803: step 24320, loss = 0.86 (1551.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:31:33.655446: step 24330, loss = 0.71 (1563.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:34.472221: step 24340, loss = 0.68 (1567.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:35.294117: step 24350, loss = 0.82 (1557.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:36.119649: step 24360, loss = 0.74 (1550.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:31:36.936251: step 24370, loss = 0.91 (1567.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:37.765078: step 24380, loss = 0.98 (1544.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:31:38.587123: step 24390, loss = 0.83 (1557.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:39.502891: step 24400, loss = 0.89 (1397.7 examples/sec; 0.092 sec/batch)
2017-05-06 21:31:40.233558: step 24410, loss = 0.92 (1751.8 examples/sec; 0.073 sec/batch)
2017-05-06 21:31:41.056496: step 24420, loss = 0.92 (1555.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:41.875840: step 24430, loss = 0.79 (1562.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:42.702797: step 24440, loss = 0.76 (1547.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:31:43.515280: step 24450, loss = 0.74 (1575.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:31:44.341984: step 24460, loss = 0.95 (1548.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:31:45.167431: step 24470, loss = 0.83 (1550.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:31:45.983475: step 24480, loss = 0.73 (1568.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:46.812337: step 24490, loss = 0.94 (1544.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:31:47.732190: step 24500, loss = 0.73 (1391.5 examples/sec; 0.092 sec/batch)
2017-05-06 21:31:48.456644: step 24510, loss = 0.85 (1766.8 examples/sec; 0.072 sec/batch)
2017-05-06 21:31:49.275758: step 24520, loss = 0.87 (1562.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:50.101546: step 24530, loss = 0.75 (1550.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:31:50.921332: step 24540, loss = 0.90 (1561.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:51.734002: step 24550, loss = 0.91 (1575.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:31:52.549969: step 24560, loss = 0.78 (1568.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:53.373698: step 24570, loss = 0.76 (1553.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:54.186435: step 24580, loss = 0.89 (1574.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:31:55.007173: step 24590, loss = 0.96 (1559.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:55.927360: step 24600, loss = 0.88 (1391.0 examples/sec; 0.092 sec/batch)
2017-05-06 21:31:56.644695: step 24610, loss = 1.06 (1784.4 examples/sec; 0.072 sec/batch)
2017-05-06 21:31:57.471551: step 24620, loss = 0.72 (1548.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:31:58.287185: step 24630, loss = 0.86 (1569.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:31:59.115761: step 24640, loss = 0.93 (1544.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:31:59.927244: step 24650, loss = 0.82 (1577.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:32:00.747424: step 24660, loss = 0.80 (1560.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:32:01.577570: step 24670, loss = 0.82 (1541.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:32:02.396588: step 24680, loss = 0.85 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:32:03.214520: step 24690, loss = 0.65 (1564.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:32:04.126438: step 24700, loss = 0.92 (1403.6 examples/sec; 0.091 sec/batch)
2017-05-06 21:32:04.854707: step 24710, loss = 0.84 (1757.6 examples/sec; 0.073 sec/batch)
2017-05-06 21:32:05.668194: step 24720, loss = 0.77 (1573.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:32:06.482822: step 24730, loss = 0.72 (1571.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:32:07.309659: step 24740, loss = 0.76 (1548.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:32:08.158792: step 24750, loss = 0.96 (1507.4 examples/sec; 0.085 sec/batch)
2017-05-06 21:32:08.947546: step 24760, loss = 0.73 (1622.8 examples/sec; 0.079 sec/batch)
2017-05-06 21:32:09.772372: step 24770, loss = 0.88 (1551.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:32:10.596117: step 24780, loss = 0.76 (1553.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:32:11.410200: step 24790, loss = 0.96 (1572.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:32:12.330799: step 24800, loss = 0.86 (1390.4 examples/sec; 0.092 sec/batch)
2017-05-06 21:32:13.051489: step 24810, loss = 0.86 (1776.0 examples/sec; 0.072 sec/batch)
2017-05-06 21:32:13.880603: step 24820, loss = 1.03 (1543.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:32:14.703902: step 24830, loss = 0.84 (1554.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:32:15.520690: step 24840, loss = 0.92 (1567.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:32:16.341999: step 24850, loss = 0.89 (1558.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:32:17.158022: step 24860, loss = 0.87 (1568.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:32:17.980783: step 24870, loss = 0.84 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:32:18.814844: step 24880, loss = 0.89 (1534.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:32:19.629898: step 24890, loss = 0.97 (1570.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:32:20.552659: step 24900, loss = 0.82 (1387.1 examples/sec; 0.092 sec/batch)
2017-05-06 21:32:21.270838: step 24910, loss = 0.73 (1782.3 examples/sec; 0.072 sec/batch)
2017-05-06 21:32:22.094288: step 24920, loss = 0.87 (1554.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:32:22.923962: step 24930, loss = 0.87 (1542.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:32:23.735610: step 24940, loss = 0.91 (1577.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:32:24.556947: step 24950, loss = 0.99 (1558.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:32:25.383449: step 24960, loss = 0.82 (1548.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:32:26.206990: step 24970, loss = 0.76 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:32:27.023785: step 24980, loss = 0.86 (1567.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:32:27.837411: step 24990, loss = 0.65 (1573.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:32:28.754587: step 25000, loss = 0.79 (1395.6 examples/sec; 0.092 sec/batch)
2017-05-06 21:32:29.477958: step 25010, loss = 0.76 (1769.5 examples/sec; 0.072 sec/batch)
2017-05-06 21:32:30.293156: step 25020, loss = 0.76 (1570.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:32:31.118102: step 25030, loss = 0.75 (1551.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:32:31.925331: step 25040, loss = 0.93 (1585.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:32:32.742642: step 25050, loss = 0.89 (1566.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:32:33.568934: step 25060, loss = 0.74 (1549.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:32:34.391101: step 25070, loss = 0.99 (1556.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:32:35.211827: step 25080, loss = 0.82 (1559.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:32:36.020483: step 25090, loss = 0.81 (1582.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:32:36.945699: step 25100, loss = 0.96 (1383.4 examples/sec; 0.093 sec/batch)
2017-05-06 21:32:37.672018: step 25110, loss = 0.91 (1762.3 examples/sec; 0.073 sec/batch)
2017-05-06 21:32:38.487690: step 25120, loss = 0.69 (1569.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:32:39.306363: step 25130, loss = 0.79 (1563.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:32:40.120289: step 25140, loss = 0.87 (1572.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:32:40.946559: step 25150, loss = 0.83 (1549.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:32:41.775268: step 25160, loss = 0.76 (1544.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:32:42.595968: step 25170, loss = 0.85 (1559.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:32:43.411819: step 25180, loss = 0.69 (1568.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:32:44.230674: step 25190, loss = 0.76 (1563.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:32:45.140812: step 25200, loss = 0.95 (1406.4 examples/sec; 0.091 sec/batch)
2017-05-06 21:32:45.875735: step 25210, loss = 0.80 (1741.7 examples/sec; 0.073 sec/batch)
2017-05-06 21:32:46.702214: step 25220, loss = 0.63 (1548.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:32:47.514418: step 25230, loss = 0.77 (1576.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:32:48.338955: step 25240, loss = 0.67 (1552.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:32:49.169005: step 25250, loss = 0.97 (1542.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:32:49.989411: step 25260, loss = 0.76 (1560.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:32:50.806877: step 25270, loss = 1.00 (1565.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:32:51.620866: step 25280, loss = 0.91 (1572.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:32:52.444770: step 25290, loss = 0.75 (1553.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:32:53.365333: step 25300, loss = 0.86 (1390.5 examples/sec; 0.092 sec/batch)
2017-05-06 21:32:54.087079: step 25310, loss = 0.91 (1773.5 examples/sec; 0.072 sec/batch)
2017-05-06 21:32:54.914645: step 25320, loss = 0.94 (1546.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:32:55.725133: step 25330, loss = 0.76 (1579.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:32:56.544695: step 25340, loss = 0.87 (1561.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:32:57.361588: step 25350, loss = 0.97 (1566.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:32:58.188126: step 25360, loss = 0.71 (1548.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:32:58.999584: step 25370, loss = 0.76 (1577.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:32:59.819986: step 25380, loss = 0.75 (1560.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:00.644015: step 25390, loss = 0.90 (1553.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:01.557521: step 25400, loss = 0.76 (1401.2 examples/sec; 0.091 sec/batch)
2017-05-06 21:33:02.288478: step 25410, loss = 0.67 (1751.1 examples/sec; 0.073 sec/batch)
2017-05-06 21:33:03.103822: step 25420, loss = 0.83 (1569.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:03.922019: step 25430, loss = 0.77 (1564.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:04.734946: step 25440, loss = 0.85 (1574.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:33:05.553476: step 25450, loss = 0.94 (1563.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:06.375505: step 25460, loss = 0.78 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:07.190239: step 25470, loss = 0.83 (1571.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:33:08.008919: step 25480, loss = 0.77 (1563.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:08.827827: step 25490, loss = 0.82 (1563.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:09.745866: step 25500, loss = 0.80 (1394.3 examples/sec; 0.092 sec/batch)
2017-05-06 21:33:10.466945: step 25510, loss = 0.76 (1775.1 examples/sec; 0.072 sec/batch)
2017-05-06 21:33:11.290276: step 25520, loss = 0.74 (1554.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:12.102724: step 25530, loss = 0.70 (1575.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:33:12.920428: step 25540, loss = 0.91 (1565.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:13.743304: step 25550, loss = 0.76 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:14.563369: step 25560, loss = 0.72 (1560.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:15.382602: step 25570, loss = 0.96 (1562.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:16.197744: step 25580, loss = 1.11 (1570.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:17.021600: step 25590, loss = 0.75 (1553.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:17.939782: step 25600, loss = 0.83 (1394.1 examples/sec; 0.092 sec/batch)
2017-05-06 21:33:18.659681: step 25610, loss = 0.83 (1778.0 examples/sec; 0.072 sec/batch)
2017-05-06 21:33:19.474013: step 25620, loss = 0.89 (1571.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:33:20.285000: step 25630, loss = 0.82 (1578.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:33:21.102806: step 25640, loss = 0.87 (1565.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:21.927317: step 25650, loss = 0.75 (1552.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:22.747390: step 25660, loss = 0.96 (1560.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:23.560941: step 25670, loss = 0.84 (1573.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:33:24.379675: step 25680, loss = 0.78 (1563.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:25.188212: step 25690, loss = 0.79 (1583.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:33:26.106275: step 25700, loss = 0.78 (1394.2 examples/sec; 0.092 sec/batch)
2017-05-06 21:33:26.824265: step 25710, loss = 0.62 (1782.7 examples/sec; 0.072 sec/batch)
2017-05-06 21:33:27.637241: step 25720, loss = 0.77 (1574.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:33:28.459441: step 25730, loss = 0.79 (1556.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:29.317525: step 25740, loss = 0.79 (1491.7 examples/sec; 0.086 sec/batch)
2017-05-06 21:33:30.095238: step 25750, loss = 0.75 (1645.9 examples/sec; 0.078 sec/batch)
2017-05-06 21:33:30.916350: step 25760, loss = 0.76 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:31.726658: step 25770, loss = 0.70 (1579.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:33:32.545222: step 25780, loss = 0.68 (1563.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:33.364371: step 25790, loss = 0.79 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:34.285266: step 25800, loss = 0.79 (1390.0 examples/sec; 0.092 sec/batch)
2017-05-06 21:33:35.007236: step 25810, loss = 0.87 (1772.9 examples/sec; 0.072 sec/batch)
2017-05-06 21:33:35.824120: step 25820, loss = 0.72 (1566.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:36.638912: step 25830, loss = 0.88 (1571.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:33:37.464898: step 25840, loss = 0.77 (1549.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:33:38.278906: step 25850, loss = 0.70 (1572.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:33:39.099866: step 25860, loss = 0.89 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:39.910521: step 25870, loss = 0.89 (1579.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:33:40.731949: step 25880, loss = 0.73 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:41.552065: step 25890, loss = 0.72 (1560.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:42.476896: step 25900, loss = 0.80 (1384.0 examples/sec; 0.092 sec/batch)
2017-05-06 21:33:43.190062: step 25910, loss = 0.83 (1794.9 examples/sec; 0.071 sec/batch)
2017-05-06 21:33:43.995124: step 25920, loss = 0.81 (1589.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:33:44.813111: step 25930, loss = 0.84 (1564.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:45.634487: step 25940, loss = 0.80 (1558.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:46.460086: step 25950, loss = 0.84 (1550.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:33:47.276150: step 25960, loss = 0.87 (1568.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:48.087574: step 25970, loss = 0.73 (1577.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:33:48.914574: step 25980, loss = 0.74 (1547.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:33:49.728420: step 25990, loss = 0.87 (1572.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:33:50.652764: step 26000, loss = 0.82 (1384.8 examples/sec; 0.092 sec/batch)
2017-05-06 21:33:51.371071: step 26010, loss = 0.78 (1782.0 examples/sec; 0.072 sec/batch)
2017-05-06 21:33:52.192436: step 26020, loss = 0.86 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:53.016641: step 26030, loss = 0.73 (1553.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:53.836893: step 26040, loss = 0.81 (1560.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:54.659562: step 26050, loss = 0.79 (1555.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:55.474591: step 26060, loss = 0.80 (1570.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:56.285724: step 26070, loss = 0.67 (1578.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:33:57.107472: step 26080, loss = 0.78 (1557.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:57.929228: step 26090, loss = 0.76 (1557.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:33:58.845673: step 26100, loss = 0.83 (1396.7 examples/sec; 0.092 sec/batch)
2017-05-06 21:33:59.556232: step 26110, loss = 0.76 (1801.4 examples/sec; 0.071 sec/batch)
2017-05-06 21:34:00.374710: step 26120, loss = 0.71 (1563.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:01.189904: step 26130, loss = 0.72 (1570.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:02.014834: step 26140, loss = 0.78 (1551.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:02.829890: step 26150, loss = 0.88 (1570.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:03.640487: step 26160, loss = 0.68 (1579.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:34:04.457832: step 26170, loss = 0.85 (1566.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:05.274608: step 26180, loss = 0.84 (1567.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:06.092895: step 26190, loss = 0.75 (1564.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:07.014996: step 26200, loss = 0.71 (1388.1 examples/sec; 0.092 sec/batch)
2017-05-06 21:34:07.731142: step 26210, loss = 0.81 (1787.3 examples/sec; 0.072 sec/batch)
2017-05-06 21:34:08.549078: step 26220, loss = 0.63 (1564.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:09.369303: step 26230, loss = 0.73 (1560.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:10.184303: step 26240, loss = 0.84 (1570.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:11.003693: step 26250, loss = 0.75 (1562.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:11.808360: step 26260, loss = 0.68 (1590.7 examples/sec; 0.080 sec/batch)
2017-05-06 21:34:12.633137: step 26270, loss = 0.84 (1551.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:13.462535: step 26280, loss = 0.79 (1543.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:34:14.279618: step 26290, loss = 0.70 (1566.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:15.197497: step 26300, loss = 0.73 (1394.5 examples/sec; 0.092 sec/batch)
2017-05-06 21:34:15.911380: step 26310, loss = 0.79 (1793.0 examples/sec; 0.071 sec/batch)
2017-05-06 21:34:16.728677: step 26320, loss = 0.79 (1566.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:17.546583: step 26330, loss = 0.63 (1565.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:18.370136: step 26340, loss = 0.63 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:19.191695: step 26350, loss = 0.74 (1558.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:20.001836: step 26360, loss = 0.87 (1580.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:34:20.819601: step 26370, loss = 0.87 (1565.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:21.640573: step 26380, loss = 0.82 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:22.459479: step 26390, loss = 0.92 (1563.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:23.386367: step 26400, loss = 0.80 (1380.6 examples/sec; 0.093 sec/batch)
2017-05-06 21:34:24.098467: step 26410, loss = 0.74 (1797.5 examples/sec; 0.071 sec/batch)
2017-05-06 21:34:24.925434: step 26420, loss = 0.65 (1547.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:34:25.750036: step 26430, loss = 0.90 (1552.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:26.576897: step 26440, loss = 0.79 (1548.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:34:27.390665: step 26450, loss = 1.05 (1572.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:34:28.195162: step 26460, loss = 0.89 (1591.0 examples/sec; 0.080 sec/batch)
2017-05-06 21:34:29.020430: step 26470, loss = 0.91 (1551.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:34:29.842506: step 26480, loss = 0.85 (1557.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:30.654713: step 26490, loss = 0.73 (1575.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:34:31.573669: step 26500, loss = 0.83 (1392.9 examples/sec; 0.092 sec/batch)
2017-05-06 21:34:32.287446: step 26510, loss = 0.88 (1793.3 examples/sec; 0.071 sec/batch)
2017-05-06 21:34:33.103788: step 26520, loss = 0.79 (1568.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:33.925028: step 26530, loss = 0.79 (1558.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:34.744255: step 26540, loss = 0.72 (1562.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:35.555667: step 26550, loss = 0.80 (1577.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:34:36.373488: step 26560, loss = 0.67 (1565.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:37.194872: step 26570, loss = 0.72 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:38.017788: step 26580, loss = 0.70 (1555.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:38.832657: step 26590, loss = 0.71 (1570.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:34:39.751490: step 26600, loss = 0.78 (1393.1 examples/sec; 0.092 sec/batch)
2017-05-06 21:34:40.469717: step 26610, loss = 0.95 (1782.2 examples/sec; 0.072 sec/batch)
2017-05-06 21:34:41.297550: step 26620, loss = 0.71 (1546.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:34:42.127429: step 26630, loss = 0.69 (1542.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:34:42.950080: step 26640, loss = 0.80 (1555.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:43.763007: step 26650, loss = 0.83 (1574.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:34:44.578567: step 26660, loss = 0.79 (1569.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:45.394869: step 26670, loss = 0.74 (1568.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:46.213677: step 26680, loss = 0.90 (1563.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:47.028205: step 26690, loss = 0.80 (1571.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:34:47.948016: step 26700, loss = 0.66 (1391.6 examples/sec; 0.092 sec/batch)
2017-05-06 21:34:48.665444: step 26710, loss = 0.81 (1784.2 examples/sec; 0.072 sec/batch)
2017-05-06 21:34:49.488135: step 26720, loss = 0.73 (1555.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:50.334570: step 26730, loss = 0.99 (1512.2 examples/sec; 0.085 sec/batch)
2017-05-06 21:34:51.141541: step 26740, loss = 0.75 (1586.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:34:51.953874: step 26750, loss = 0.80 (1575.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:34:52.770813: step 26760, loss = 0.81 (1566.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:53.588536: step 26770, loss = 0.79 (1565.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:54.409290: step 26780, loss = 0.91 (1559.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:55.226138: step 26790, loss = 0.74 (1567.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:56.141789: step 26800, loss = 0.92 (1397.9 examples/sec; 0.092 sec/batch)
2017-05-06 21:34:56.859791: step 26810, loss = 0.74 (1782.7 examples/sec; 0.072 sec/batch)
2017-05-06 21:34:57.676861: step 26820, loss = 0.82 (1566.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:58.493172: step 26830, loss = 0.80 (1568.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:34:59.306436: step 26840, loss = 0.79 (1573.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:35:00.124287: step 26850, loss = 0.93 (1565.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:35:00.940503: step 26860, loss = 0.71 (1568.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:35:01.750711: step 26870, loss = 0.80 (1579.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:35:02.593086: step 26880, loss = 0.77 (1519.5 examples/sec; 0.084 sec/batch)
2017-05-06 21:35:03.407751: step 26890, loss = 0.63 (1571.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:35:04.323674: step 26900, loss = 0.75 (1397.5 examples/sec; 0.092 sec/batch)
2017-05-06 21:35:05.047975: step 26910, loss = 0.96 (1767.2 examples/sec; 0.072 sec/batch)
2017-05-06 21:35:05.869140: step 26920, loss = 0.91 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:35:06.690400: step 26930, loss = 0.93 (1558.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:35:07.504468: step 26940, loss = 0.76 (1572.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:35:08.325911: step 26950, loss = 0.83 (1558.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:35:09.147944: step 26960, loss = 0.92 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:35:09.961830: step 26970, loss = 0.91 (1572.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:35:10.778293: step 26980, loss = 0.79 (1567.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:35:11.586290: step 26990, loss = 0.74 (1584.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:35:12.516081: step 27000, loss = 0.73 (1376.7 examples/sec; 0.093 sec/batch)
2017-05-06 21:35:13.240465: step 27010, loss = 0.91 (1767.0 examples/sec; 0.072 sec/batch)
2017-05-06 21:35:14.063479: step 27020, loss = 0.75 (1555.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:35:14.898942: step 27030, loss = 0.81 (1532.1 examples/sec; 0.084 sec/batch)
2017-05-06 21:35:15.716172: step 27040, loss = 0.91 (1566.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:35:16.546169: step 27050, loss = 0.86 (1542.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:35:17.367523: step 27060, loss = 0.81 (1558.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:35:18.185431: step 27070, loss = 0.68 (1565.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:35:19.004960: step 27080, loss = 0.69 (1561.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:35:19.820197: step 27090, loss = 0.97 (1570.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:35:20.746831: step 27100, loss = 0.81 (1381.3 examples/sec; 0.093 sec/batch)
2017-05-06 21:35:21.460040: step 27110, loss = 0.80 (1794.7 examples/sec; 0.071 sec/batch)
2017-05-06 21:35:22.290407: step 27120, loss = 0.76 (1541.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:35:23.108175: step 27130, loss = 0.62 (1565.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:35:23.922708: step 27140, loss = 0.83 (1571.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:35:24.741620: step 27150, loss = 0.62 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:35:25.564540: step 27160, loss = 0.72 (1555.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:35:26.392622: step 27170, loss = 0.98 (1545.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:35:27.218090: step 27180, loss = 0.65 (1550.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:35:28.033753: step 27190, loss = 0.66 (1569.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:35:28.953782: step 27200, loss = 0.92 (1391.3 examples/sec; 0.092 sec/batch)
2017-05-06 21:35:29.689917: step 27210, loss = 0.76 (1738.8 examples/sec; 0.074 sec/batch)
2017-05-06 21:35:30.516999: step 27220, loss = 0.81 (1547.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:35:31.346894: step 27230, loss = 0.78 (1542.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:35:32.168136: step 27240, loss = 0.89 (1558.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:35:32.988930: step 27250, loss = 0.71 (1559.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:35:33.805004: step 27260, loss = 0.76 (1568.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:35:34.626544: step 27270, loss = 0.79 (1558.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:35:35.443097: step 27280, loss = 0.87 (1567.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:35:36.255167: step 27290, loss = 0.75 (1576.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:35:37.179205: step 27300, loss = 0.82 (1385.2 examples/sec; 0.092 sec/batch)
2017-05-06 21:35:37.901862: step 27310, loss = 0.78 (1771.2 examples/sec; 0.072 sec/batch)
2017-05-06 21:35:38.728118: step 27320, loss = 0.71 (1549.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:35:39.549249: step 27330, loss = 0.80 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:35:40.363970: step 27340, loss = 0.75 (1571.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:35:41.194019: step 27350, loss = 0.88 (1542.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:35:42.019622: step 27360, loss = 0.85 (1550.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:35:42.843633: step 27370, loss = 0.93 (1553.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:35:43.652963: step 27380, loss = 0.86 (1581.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:35:44.481814: step 27390, loss = 0.72 (1544.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:35:45.407163: step 27400, loss = 0.87 (1383.3 examples/sec; 0.093 sec/batch)
2017-05-06 21:35:46.132960: step 27410, loss = 0.77 (1763.6 examples/sec; 0.073 sec/batch)
2017-05-06 21:35:46.961952: step 27420, loss = 0.76 (1544.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:35:47.777959: step 27430, loss = 0.75 (1568.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:35:48.605007: step 27440, loss = 0.80 (1547.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:35:49.417221: step 27450, loss = 0.93 (1575.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:35:50.241588: step 27460, loss = 0.92 (1552.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:35:51.060314: step 27470, loss = 0.70 (1563.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:35:51.871959: step 27480, loss = 0.99 (1577.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:35:52.693468: step 27490, loss = 0.83 (1558.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:35:53.633460: step 27500, loss = 0.96 (1361.7 examples/sec; 0.094 sec/batch)
2017-05-06 21:35:54.353566: step 27510, loss = 0.76 (1777.5 examples/sec; 0.072 sec/batch)
2017-05-06 21:35:55.170460: step 27520, loss = 0.80 (1566.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:35:55.990451: step 27530, loss = 0.97 (1561.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:35:56.813367: step 27540, loss = 0.75 (1555.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:35:57.641618: step 27550, loss = 0.96 (1545.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:35:58.468384: step 27560, loss = 0.72 (1548.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:35:59.293267: step 27570, loss = 0.85 (1551.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:36:00.106376: step 27580, loss = 0.78 (1574.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:36:00.934663: step 27590, loss = 1.00 (1545.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:36:01.859240: step 27600, loss = 0.96 (1384.4 examples/sec; 0.092 sec/batch)
2017-05-06 21:36:02.571445: step 27610, loss = 0.91 (1797.2 examples/sec; 0.071 sec/batch)
2017-05-06 21:36:03.385491: step 27620, loss = 0.76 (1572.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:36:04.204223: step 27630, loss = 0.77 (1563.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:36:05.026607: step 27640, loss = 0.70 (1556.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:36:05.849346: step 27650, loss = 0.80 (1555.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:36:06.671268: step 27660, loss = 0.84 (1557.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:36:07.490970: step 27670, loss = 0.79 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:36:08.316958: step 27680, loss = 0.69 (1549.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:36:09.145362: step 27690, loss = 0.83 (1545.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:36:10.074443: step 27700, loss = 0.82 (1377.7 examples/sec; 0.093 sec/batch)
2017-05-06 21:36:10.794323: step 27710, loss = 0.79 (1778.0 examples/sec; 0.072 sec/batch)
2017-05-06 21:36:11.636199: step 27720, loss = 0.83 (1520.4 examples/sec; 0.084 sec/batch)
2017-05-06 21:36:12.435860: step 27730, loss = 0.80 (1600.7 examples/sec; 0.080 sec/batch)
2017-05-06 21:36:13.266081: step 27740, loss = 0.97 (1541.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:36:14.089855: step 27750, loss = 0.90 (1553.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:36:14.907092: step 27760, loss = 1.04 (1566.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:36:15.719858: step 27770, loss = 0.80 (1574.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:36:16.540925: step 27780, loss = 0.75 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:36:17.362658: step 27790, loss = 0.71 (1557.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:36:18.288828: step 27800, loss = 0.75 (1382.0 examples/sec; 0.093 sec/batch)
2017-05-06 21:36:19.011109: step 27810, loss = 0.72 (1772.2 examples/sec; 0.072 sec/batch)
2017-05-06 21:36:19.835326: step 27820, loss = 0.73 (1553.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:36:20.655176: step 27830, loss = 0.78 (1561.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:36:21.467539: step 27840, loss = 0.75 (1575.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:36:22.290667: step 27850, loss = 0.76 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:36:23.103276: step 27860, loss = 0.77 (1575.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:36:23.919539: step 27870, loss = 0.84 (1568.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:36:24.746238: step 27880, loss = 0.77 (1548.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:36:25.565638: step 27890, loss = 0.65 (1562.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:36:26.494914: step 27900, loss = 0.72 (1377.4 examples/sec; 0.093 sec/batch)
2017-05-06 21:36:27.206798: step 27910, loss = 0.79 (1798.0 examples/sec; 0.071 sec/batch)
2017-05-06 21:36:28.020399: step 27920, loss = 0.83 (1573.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:36:28.833679: step 27930, loss = 0.80 (1573.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:36:29.659109: step 27940, loss = 0.91 (1550.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:36:30.475495: step 27950, loss = 0.88 (1567.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:36:31.295193: step 27960, loss = 0.83 (1561.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:36:32.107776: step 27970, loss = 0.81 (1575.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:36:32.926071: step 27980, loss = 0.69 (1564.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:36:33.742233: step 27990, loss = 0.74 (1568.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:36:34.678944: step 28000, loss = 0.74 (1366.5 examples/sec; 0.094 sec/batch)
2017-05-06 21:36:35.402064: step 28010, loss = 0.70 (1770.2 examples/sec; 0.072 sec/batch)
2017-05-06 21:36:36.228683: step 28020, loss = 0.84 (1548.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:36:37.049257: step 28030, loss = 0.93 (1559.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:36:37.878660: step 28040, loss = 0.72 (1543.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:36:38.696242: step 28050, loss = 0.92 (1565.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:36:39.506736: step 28060, loss = 0.81 (1579.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:36:40.321280: step 28070, loss = 0.71 (1571.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:36:41.137600: step 28080, loss = 0.80 (1568.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:36:41.954947: step 28090, loss = 0.83 (1566.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:36:42.872606: step 28100, loss = 0.89 (1394.9 examples/sec; 0.092 sec/batch)
2017-05-06 21:36:43.593439: step 28110, loss = 0.82 (1775.7 examples/sec; 0.072 sec/batch)
2017-05-06 21:36:44.426074: step 28120, loss = 0.69 (1537.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:36:45.254788: step 28130, loss = 0.76 (1544.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:36:46.069080: step 28140, loss = 0.92 (1571.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:36:46.890008: step 28150, loss = 0.87 (1559.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:36:47.706952: step 28160, loss = 0.81 (1566.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:36:48.535015: step 28170, loss = 0.90 (1545.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:36:49.359807: step 28180, loss = 0.83 (1551.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:36:50.177951: step 28190, loss = 0.78 (1564.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:36:51.104377: step 28200, loss = 0.94 (1381.7 examples/sec; 0.093 sec/batch)
2017-05-06 21:36:51.816281: step 28210, loss = 0.64 (1798.0 examples/sec; 0.071 sec/batch)
2017-05-06 21:36:52.641902: step 28220, loss = 0.81 (1550.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:36:53.461115: step 28230, loss = 0.74 (1562.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:36:54.278272: step 28240, loss = 0.74 (1566.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:36:55.102711: step 28250, loss = 0.83 (1552.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:36:55.907453: step 28260, loss = 0.77 (1590.6 examples/sec; 0.080 sec/batch)
2017-05-06 21:36:56.739958: step 28270, loss = 0.87 (1537.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:36:57.559246: step 28280, loss = 0.84 (1562.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:36:58.380067: step 28290, loss = 0.75 (1559.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:36:59.314182: step 28300, loss = 1.00 (1370.3 examples/sec; 0.093 sec/batch)
2017-05-06 21:37:00.023091: step 28310, loss = 0.80 (1805.6 examples/sec; 0.071 sec/batch)
2017-05-06 21:37:00.848991: step 28320, loss = 0.92 (1549.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:37:01.660671: step 28330, loss = 0.66 (1577.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:37:02.482396: step 28340, loss = 0.84 (1557.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:03.300169: step 28350, loss = 0.62 (1565.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:04.106194: step 28360, loss = 0.86 (1588.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:37:04.930066: step 28370, loss = 0.67 (1553.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:05.754695: step 28380, loss = 0.82 (1552.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:06.569769: step 28390, loss = 0.67 (1570.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:07.492894: step 28400, loss = 0.77 (1386.6 examples/sec; 0.092 sec/batch)
2017-05-06 21:37:08.206866: step 28410, loss = 0.71 (1792.8 examples/sec; 0.071 sec/batch)
2017-05-06 21:37:09.022275: step 28420, loss = 0.73 (1569.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:09.848571: step 28430, loss = 0.71 (1549.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:37:10.673816: step 28440, loss = 0.79 (1551.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:37:11.490441: step 28450, loss = 0.76 (1567.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:12.305874: step 28460, loss = 0.82 (1569.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:13.119613: step 28470, loss = 0.74 (1573.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:37:13.942704: step 28480, loss = 0.77 (1555.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:14.754815: step 28490, loss = 0.65 (1576.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:37:15.669720: step 28500, loss = 0.74 (1399.1 examples/sec; 0.091 sec/batch)
2017-05-06 21:37:16.400182: step 28510, loss = 0.54 (1752.3 examples/sec; 0.073 sec/batch)
2017-05-06 21:37:17.223809: step 28520, loss = 0.91 (1554.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:18.046319: step 28530, loss = 0.83 (1556.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:18.866732: step 28540, loss = 0.71 (1560.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:19.679591: step 28550, loss = 0.93 (1574.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:37:20.503095: step 28560, loss = 0.78 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:21.326814: step 28570, loss = 0.87 (1553.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:22.153148: step 28580, loss = 0.92 (1549.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:37:22.979363: step 28590, loss = 0.89 (1549.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:37:23.918604: step 28600, loss = 0.97 (1362.8 examples/sec; 0.094 sec/batch)
2017-05-06 21:37:24.616326: step 28610, loss = 0.67 (1834.5 examples/sec; 0.070 sec/batch)
2017-05-06 21:37:25.448223: step 28620, loss = 0.90 (1538.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:37:26.266301: step 28630, loss = 0.69 (1564.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:27.089652: step 28640, loss = 0.77 (1554.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:27.907848: step 28650, loss = 1.02 (1564.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:28.736580: step 28660, loss = 0.75 (1544.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:37:29.558080: step 28670, loss = 0.73 (1558.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:30.370422: step 28680, loss = 0.80 (1575.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:37:31.186072: step 28690, loss = 0.76 (1569.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:32.107418: step 28700, loss = 0.76 (1389.3 examples/sec; 0.092 sec/batch)
2017-05-06 21:37:32.852802: step 28710, loss = 0.76 (1717.2 examples/sec; 0.075 sec/batch)
2017-05-06 21:37:33.657874: step 28720, loss = 0.94 (1589.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:37:34.488628: step 28730, loss = 0.89 (1540.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:37:35.308281: step 28740, loss = 0.92 (1561.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:36.127179: step 28750, loss = 0.76 (1563.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:36.945634: step 28760, loss = 0.76 (1563.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:37.765916: step 28770, loss = 0.95 (1560.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:38.588317: step 28780, loss = 0.76 (1556.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:39.408119: step 28790, loss = 0.73 (1561.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:40.322374: step 28800, loss = 0.76 (1400.1 examples/sec; 0.091 sec/batch)
2017-05-06 21:37:41.058410: step 28810, loss = 0.96 (1739.0 examples/sec; 0.074 sec/batch)
2017-05-06 21:37:41.870634: step 28820, loss = 0.82 (1575.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:37:42.683108: step 28830, loss = 0.71 (1575.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:37:43.491954: step 28840, loss = 0.77 (1582.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:37:44.315171: step 28850, loss = 0.80 (1554.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:45.138508: step 28860, loss = 0.62 (1554.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:45.963509: step 28870, loss = 0.76 (1551.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:37:46.784414: step 28880, loss = 0.85 (1559.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:47.607261: step 28890, loss = 0.76 (1555.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:48.526708: step 28900, loss = 0.76 (1392.1 examples/sec; 0.092 sec/batch)
2017-05-06 21:37:49.249865: step 28910, loss = 0.77 (1770.0 examples/sec; 0.072 sec/batch)
2017-05-06 21:37:50.069784: step 28920, loss = 0.88 (1561.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:50.890202: step 28930, loss = 0.90 (1560.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:51.705832: step 28940, loss = 0.86 (1569.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:52.525534: step 28950, loss = 0.91 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:53.351813: step 28960, loss = 0.79 (1549.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:37:54.176414: step 28970, loss = 0.82 (1552.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:54.995608: step 28980, loss = 0.87 (1562.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:55.801455: step 28990, loss = 0.83 (1588.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:37:56.730910: step 29000, loss = 0.68 (1377.2 examples/sec; 0.093 sec/batch)
2017-05-06 21:37:57.447199: step 29010, loss = 0.70 (1787.0 examples/sec; 0.072 sec/batch)
2017-05-06 21:37:58.263643: step 29020, loss = 0.80 (1567.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:59.088080: step 29030, loss = 0.71 (1552.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:37:59.900556: step 29040, loss = 0.91 (1575.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:38:00.727999: step 29050, loss = 0.71 (1546.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:38:01.547992: step 29060, loss = 0.77 (1561.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:38:02.359439: step 29070, loss = 0.85 (1577.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:38:03.177915: step 29080, loss = 0.79 (1563.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:38:03.990743: step 29090, loss = 0.80 (1574.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:38:04.923417: step 29100, loss = 0.80 (1372.4 examples/sec; 0.093 sec/batch)
2017-05-06 21:38:05.665097: step 29110, loss = 0.81 (1725.8 examples/sec; 0.074 sec/batch)
2017-05-06 21:38:06.491827: step 29120, loss = 0.82 (1548.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:38:07.317921: step 29130, loss = 0.90 (1549.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:38:08.148594: step 29140, loss = 0.81 (1540.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:38:08.960541: step 29150, loss = 0.88 (1576.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:38:09.780025: step 29160, loss = 0.68 (1562.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:38:10.605839: step 29170, loss = 0.72 (1550.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:38:11.412971: step 29180, loss = 0.70 (1585.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:38:12.231140: step 29190, loss = 0.67 (1564.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:38:13.195787: step 29200, loss = 0.93 (1326.9 examples/sec; 0.096 sec/batch)
2017-05-06 21:38:13.911924: step 29210, loss = 0.70 (1787.4 examples/sec; 0.072 sec/batch)
2017-05-06 21:38:14.743518: step 29220, loss = 0.88 (1539.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:38:15.558591: step 29230, loss = 0.84 (1570.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:38:16.388191: step 29240, loss = 0.73 (1542.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:38:17.218565: step 29250, loss = 0.78 (1541.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:38:18.043714: step 29260, loss = 0.97 (1551.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:38:18.867228: step 29270, loss = 0.87 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:38:19.679123: step 29280, loss = 0.87 (1576.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:38:20.497887: step 29290, loss = 0.75 (1563.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:38:21.440477: step 29300, loss = 0.71 (1358.0 examples/sec; 0.094 sec/batch)
2017-05-06 21:38:22.157101: step 29310, loss = 0.75 (1786.1 examples/sec; 0.072 sec/batch)
2017-05-06 21:38:22.976904: step 29320, loss = 0.85 (1561.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:38:23.784145: step 29330, loss = 0.79 (1585.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:38:24.605151: step 29340, loss = 0.71 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:38:25.423905: step 29350, loss = 0.85 (1563.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:38:26.245647: step 29360, loss = 0.81 (1557.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:38:27.065761: step 29370, loss = 0.76 (1560.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:38:27.876432: step 29380, loss = 0.77 (1578.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:38:28.699929: step 29390, loss = 0.70 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:38:29.618210: step 29400, loss = 0.71 (1393.9 examples/sec; 0.092 sec/batch)
2017-05-06 21:38:30.341994: step 29410, loss = 0.69 (1768.5 examples/sec; 0.072 sec/batch)
2017-05-06 21:38:31.162614: step 29420, loss = 0.92 (1559.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:38:31.970199: step 29430, loss = 0.75 (1585.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:38:32.787030: step 29440, loss = 0.88 (1567.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:38:33.603618: step 29450, loss = 0.60 (1567.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:38:34.424896: step 29460, loss = 0.77 (1558.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:38:35.252489: step 29470, loss = 0.95 (1546.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:38:36.068081: step 29480, loss = 0.69 (1569.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:38:36.880127: step 29490, loss = 0.74 (1576.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:38:37.799197: step 29500, loss = 0.89 (1392.7 examples/sec; 0.092 sec/batch)
2017-05-06 21:38:38.533410: step 29510, loss = 0.89 (1743.4 examples/sec; 0.073 sec/batch)
2017-05-06 21:38:39.361073: step 29520, loss = 0.77 (1546.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:38:40.178894: step 29530, loss = 0.84 (1565.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:38:40.987891: step 29540, loss = 0.79 (1582.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:38:41.806447: step 29550, loss = 0.67 (1563.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:38:42.637760: step 29560, loss = 0.70 (1539.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:38:43.455431: step 29570, loss = 0.82 (1565.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:38:44.269853: step 29580, loss = 0.82 (1571.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:38:45.090202: step 29590, loss = 0.87 (1560.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:38:46.010054: step 29600, loss = 0.77 (1391.5 examples/sec; 0.092 sec/batch)
2017-05-06 21:38:46.736500: step 29610, loss = 0.79 (1762.0 examples/sec; 0.073 sec/batch)
2017-05-06 21:38:47.553363: step 29620, loss = 0.85 (1567.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:38:48.374946: step 29630, loss = 0.91 (1558.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:38:49.197222: step 29640, loss = 0.90 (1556.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:38:50.011380: step 29650, loss = 0.79 (1572.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:38:50.837000: step 29660, loss = 0.74 (1550.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:38:51.661559: step 29670, loss = 0.72 (1552.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:38:52.479895: step 29680, loss = 0.78 (1564.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:38:53.306825: step 29690, loss = 0.75 (1547.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:38:54.261251: step 29700, loss = 0.78 (1341.1 examples/sec; 0.095 sec/batch)
2017-05-06 21:38:54.966589: step 29710, loss = 0.66 (1814.8 examples/sec; 0.071 sec/batch)
2017-05-06 21:38:55.780151: step 29720, loss = 0.79 (1573.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:38:56.599786: step 29730, loss = 0.95 (1561.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:38:57.426739: step 29740, loss = 0.68 (1547.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:38:58.250396: step 29750, loss = 0.75 (1554.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:38:59.073952: step 29760, loss = 0.73 (1554.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:38:59.893589: step 29770, loss = 0.75 (1561.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:00.719787: step 29780, loss = 0.75 (1549.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:39:01.547885: step 29790, loss = 0.84 (1545.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:39:02.480729: step 29800, loss = 0.90 (1372.2 examples/sec; 0.093 sec/batch)
2017-05-06 21:39:03.191160: step 29810, loss = 0.69 (1801.7 examples/sec; 0.071 sec/batch)
2017-05-06 21:39:04.007638: step 29820, loss = 0.80 (1567.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:04.829413: step 29830, loss = 0.91 (1557.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:05.648339: step 29840, loss = 0.74 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:06.473290: step 29850, loss = 0.74 (1551.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:07.293521: step 29860, loss = 0.88 (1560.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:08.105621: step 29870, loss = 0.73 (1576.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:39:08.925659: step 29880, loss = 0.91 (1560.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:09.746032: step 29890, loss = 0.77 (1560.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:10.675131: step 29900, loss = 0.92 (1377.7 examples/sec; 0.093 sec/batch)
2017-05-06 21:39:11.390338: step 29910, loss = 0.59 (1789.7 examples/sec; 0.072 sec/batch)
2017-05-06 21:39:12.205888: step 29920, loss = 0.83 (1569.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:13.029056: step 29930, loss = 0.72 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:13.850449: step 29940, loss = 0.77 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:14.673234: step 29950, loss = 0.71 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:15.494531: step 29960, loss = 0.69 (1558.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:16.319655: step 29970, loss = 0.86 (1551.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:39:17.147269: step 29980, loss = 0.77 (1546.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:39:17.964739: step 29990, loss = 0.85 (1565.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:18.894477: step 30000, loss = 0.80 (1376.7 examples/sec; 0.093 sec/batch)
2017-05-06 21:39:19.608395: step 30010, loss = 0.78 (1792.9 examples/sec; 0.071 sec/batch)
2017-05-06 21:39:20.430724: step 30020, loss = 0.94 (1556.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:21.251979: step 30030, loss = 0.76 (1558.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:22.074729: step 30040, loss = 0.89 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:22.895953: step 30050, loss = 0.76 (1558.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:23.703964: step 30060, loss = 0.77 (1584.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:39:24.529099: step 30070, loss = 0.87 (1551.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:39:25.342627: step 30080, loss = 0.67 (1573.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:39:26.180993: step 30090, loss = 0.77 (1526.8 examples/sec; 0.084 sec/batch)
2017-05-06 21:39:27.112155: step 30100, loss = 0.81 (1374.6 examples/sec; 0.093 sec/batch)
2017-05-06 21:39:27.825126: step 30110, loss = 0.63 (1795.3 examples/sec; 0.071 sec/batch)
2017-05-06 21:39:28.655608: step 30120, loss = 0.87 (1541.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:39:29.477330: step 30130, loss = 0.77 (1557.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:30.304812: step 30140, loss = 0.96 (1546.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:39:31.123807: step 30150, loss = 0.88 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:31.938292: step 30160, loss = 0.86 (1571.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:39:32.766223: step 30170, loss = 0.83 (1546.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:39:33.595852: step 30180, loss = 0.84 (1542.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:39:34.415010: step 30190, loss = 0.73 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:35.341719: step 30200, loss = 0.79 (1381.2 examples/sec; 0.093 sec/batch)
2017-05-06 21:39:36.066448: step 30210, loss = 0.79 (1766.2 examples/sec; 0.072 sec/batch)
2017-05-06 21:39:36.892155: step 30220, loss = 0.74 (1550.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:39:37.718796: step 30230, loss = 0.86 (1548.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:39:38.546605: step 30240, loss = 1.01 (1546.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:39:39.363117: step 30250, loss = 0.75 (1567.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:40.179295: step 30260, loss = 0.86 (1568.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:40.996012: step 30270, loss = 0.75 (1567.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:41.814800: step 30280, loss = 0.89 (1563.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:42.636282: step 30290, loss = 0.72 (1558.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:43.545311: step 30300, loss = 0.86 (1408.1 examples/sec; 0.091 sec/batch)
2017-05-06 21:39:44.269567: step 30310, loss = 0.88 (1767.3 examples/sec; 0.072 sec/batch)
2017-05-06 21:39:45.094976: step 30320, loss = 0.80 (1550.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:39:45.908960: step 30330, loss = 0.79 (1572.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:39:46.735599: step 30340, loss = 0.71 (1548.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:39:47.555613: step 30350, loss = 0.92 (1561.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:48.371763: step 30360, loss = 0.77 (1568.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:49.190562: step 30370, loss = 0.83 (1563.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:50.015260: step 30380, loss = 0.98 (1552.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:50.838429: step 30390, loss = 0.81 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:51.755628: step 30400, loss = 0.73 (1395.5 examples/sec; 0.092 sec/batch)
2017-05-06 21:39:52.483515: step 30410, loss = 0.95 (1758.5 examples/sec; 0.073 sec/batch)
2017-05-06 21:39:53.300490: step 30420, loss = 0.80 (1566.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:54.128881: step 30430, loss = 0.75 (1545.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:39:54.950270: step 30440, loss = 0.77 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:55.763768: step 30450, loss = 0.98 (1573.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:39:56.586316: step 30460, loss = 1.00 (1556.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:57.404765: step 30470, loss = 0.93 (1563.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:58.220952: step 30480, loss = 0.81 (1568.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:59.043355: step 30490, loss = 0.82 (1556.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:39:59.966209: step 30500, loss = 0.84 (1387.0 examples/sec; 0.092 sec/batch)
2017-05-06 21:40:00.693101: step 30510, loss = 0.87 (1760.9 examples/sec; 0.073 sec/batch)
2017-05-06 21:40:01.510116: step 30520, loss = 0.73 (1566.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:02.341896: step 30530, loss = 0.93 (1538.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:40:03.172893: step 30540, loss = 0.82 (1540.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:40:03.990836: step 30550, loss = 0.70 (1564.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:04.811008: step 30560, loss = 0.85 (1560.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:05.628674: step 30570, loss = 0.89 (1565.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:06.460471: step 30580, loss = 0.76 (1538.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:40:07.282935: step 30590, loss = 0.85 (1556.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:08.210774: step 30600, loss = 0.89 (1379.5 examples/sec; 0.093 sec/batch)
2017-05-06 21:40:08.917496: step 30610, loss = 0.62 (1811.2 examples/sec; 0.071 sec/batch)
2017-05-06 21:40:09.735894: step 30620, loss = 0.72 (1564.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:10.563153: step 30630, loss = 0.75 (1547.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:40:11.382582: step 30640, loss = 0.78 (1562.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:12.196124: step 30650, loss = 0.81 (1573.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:40:13.015290: step 30660, loss = 0.75 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:13.829337: step 30670, loss = 0.88 (1572.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:40:14.648400: step 30680, loss = 0.88 (1562.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:15.504748: step 30690, loss = 0.63 (1494.7 examples/sec; 0.086 sec/batch)
2017-05-06 21:40:16.392690: step 30700, loss = 0.74 (1441.5 examples/sec; 0.089 sec/batch)
2017-05-06 21:40:17.116521: step 30710, loss = 0.79 (1768.4 examples/sec; 0.072 sec/batch)
2017-05-06 21:40:17.940012: step 30720, loss = 0.77 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:18.766093: step 30730, loss = 0.75 (1549.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:40:19.585632: step 30740, loss = 0.97 (1561.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:20.407836: step 30750, loss = 0.78 (1556.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:21.225279: step 30760, loss = 0.81 (1565.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:22.048523: step 30770, loss = 0.76 (1554.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:22.870697: step 30780, loss = 0.67 (1556.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:23.695014: step 30790, loss = 0.73 (1552.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:24.614795: step 30800, loss = 0.80 (1391.6 examples/sec; 0.092 sec/batch)
2017-05-06 21:40:25.340987: step 30810, loss = 0.96 (1762.6 examples/sec; 0.073 sec/batch)
2017-05-06 21:40:26.175555: step 30820, loss = 0.66 (1533.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:40:26.991694: step 30830, loss = 0.78 (1568.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:27.803598: step 30840, loss = 0.75 (1576.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:40:28.633722: step 30850, loss = 0.83 (1541.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:40:29.453011: step 30860, loss = 0.78 (1562.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:30.270778: step 30870, loss = 0.81 (1565.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:31.091859: step 30880, loss = 0.70 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:31.913272: step 30890, loss = 0.95 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:32.836431: step 30900, loss = 0.86 (1386.6 examples/sec; 0.092 sec/batch)
2017-05-06 21:40:33.571792: step 30910, loss = 0.75 (1740.6 examples/sec; 0.074 sec/batch)
2017-05-06 21:40:34.385456: step 30920, loss = 0.79 (1573.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:40:35.207754: step 30930, loss = 0.72 (1556.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:36.020318: step 30940, loss = 0.97 (1575.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:40:36.839735: step 30950, loss = 0.85 (1562.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:37.657137: step 30960, loss = 0.84 (1566.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:38.476622: step 30970, loss = 0.84 (1561.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:39.303703: step 30980, loss = 0.76 (1547.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:40:40.135036: step 30990, loss = 0.85 (1539.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:40:41.052122: step 31000, loss = 0.82 (1395.7 examples/sec; 0.092 sec/batch)
2017-05-06 21:40:41.774290: step 31010, loss = 0.80 (1772.5 examples/sec; 0.072 sec/batch)
2017-05-06 21:40:42.599774: step 31020, loss = 0.70 (1550.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:40:43.417563: step 31030, loss = 0.83 (1565.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:44.239506: step 31040, loss = 0.99 (1557.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:45.057799: step 31050, loss = 0.84 (1564.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:45.880602: step 31060, loss = 0.87 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:46.708583: step 31070, loss = 0.81 (1545.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:40:47.520656: step 31080, loss = 0.91 (1576.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:40:48.339422: step 31090, loss = 0.70 (1563.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:49.261000: step 31100, loss = 0.80 (1388.9 examples/sec; 0.092 sec/batch)
2017-05-06 21:40:49.979223: step 31110, loss = 0.86 (1782.2 examples/sec; 0.072 sec/batch)
2017-05-06 21:40:50.801107: step 31120, loss = 0.80 (1557.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:51.613795: step 31130, loss = 0.92 (1575.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:40:52.435949: step 31140, loss = 0.65 (1556.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:53.258817: step 31150, loss = 0.80 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:54.084367: step 31160, loss = 0.82 (1550.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:40:54.901359: step 31170, loss = 0.88 (1566.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:55.719584: step 31180, loss = 1.01 (1564.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:56.533542: step 31190, loss = 0.66 (1572.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:40:57.465109: step 31200, loss = 0.74 (1374.0 examples/sec; 0.093 sec/batch)
2017-05-06 21:40:58.172762: step 31210, loss = 0.66 (1808.8 examples/sec; 0.071 sec/batch)
2017-05-06 21:40:58.996838: step 31220, loss = 0.89 (1553.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:40:59.808494: step 31230, loss = 0.84 (1577.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:41:00.634268: step 31240, loss = 0.91 (1550.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:41:01.454590: step 31250, loss = 0.78 (1560.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:02.279834: step 31260, loss = 0.89 (1551.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:41:03.098496: step 31270, loss = 0.90 (1563.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:03.903415: step 31280, loss = 0.64 (1590.2 examples/sec; 0.080 sec/batch)
2017-05-06 21:41:04.722426: step 31290, loss = 0.78 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:05.640153: step 31300, loss = 1.03 (1394.8 examples/sec; 0.092 sec/batch)
2017-05-06 21:41:06.360620: step 31310, loss = 0.85 (1776.6 examples/sec; 0.072 sec/batch)
2017-05-06 21:41:07.181563: step 31320, loss = 0.81 (1559.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:07.999271: step 31330, loss = 0.98 (1565.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:08.820833: step 31340, loss = 0.78 (1558.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:09.651231: step 31350, loss = 0.75 (1541.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:41:10.480742: step 31360, loss = 0.70 (1543.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:41:11.298732: step 31370, loss = 0.98 (1564.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:12.114285: step 31380, loss = 0.69 (1569.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:12.936348: step 31390, loss = 0.75 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:13.850090: step 31400, loss = 0.95 (1400.8 examples/sec; 0.091 sec/batch)
2017-05-06 21:41:14.566981: step 31410, loss = 0.85 (1785.5 examples/sec; 0.072 sec/batch)
2017-05-06 21:41:15.386002: step 31420, loss = 0.84 (1562.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:16.204319: step 31430, loss = 0.89 (1564.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:17.018918: step 31440, loss = 0.86 (1571.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:41:17.842466: step 31450, loss = 0.75 (1554.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:18.666621: step 31460, loss = 0.83 (1553.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:19.488704: step 31470, loss = 0.81 (1557.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:20.309097: step 31480, loss = 0.66 (1560.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:21.135470: step 31490, loss = 0.85 (1548.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:41:22.050831: step 31500, loss = 0.81 (1398.4 examples/sec; 0.092 sec/batch)
2017-05-06 21:41:22.770399: step 31510, loss = 0.75 (1778.8 examples/sec; 0.072 sec/batch)
2017-05-06 21:41:23.587618: step 31520, loss = 0.85 (1566.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:24.404797: step 31530, loss = 0.87 (1566.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:25.223642: step 31540, loss = 0.79 (1563.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:26.040286: step 31550, loss = 0.85 (1567.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:26.858875: step 31560, loss = 0.85 (1563.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:27.669679: step 31570, loss = 0.77 (1578.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:41:28.487574: step 31580, loss = 0.82 (1565.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:29.302027: step 31590, loss = 0.92 (1571.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:41:30.226736: step 31600, loss = 0.91 (1384.2 examples/sec; 0.092 sec/batch)
2017-05-06 21:41:30.935739: step 31610, loss = 0.61 (1805.4 examples/sec; 0.071 sec/batch)
2017-05-06 21:41:31.741577: step 31620, loss = 0.81 (1588.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:41:32.559170: step 31630, loss = 0.80 (1565.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:33.373365: step 31640, loss = 0.78 (1572.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:41:34.189900: step 31650, loss = 0.66 (1567.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:35.006569: step 31660, loss = 0.73 (1567.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:35.821557: step 31670, loss = 0.85 (1570.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:41:36.659694: step 31680, loss = 0.89 (1527.2 examples/sec; 0.084 sec/batch)
2017-05-06 21:41:37.460692: step 31690, loss = 0.73 (1598.0 examples/sec; 0.080 sec/batch)
2017-05-06 21:41:38.377477: step 31700, loss = 0.76 (1396.2 examples/sec; 0.092 sec/batch)
2017-05-06 21:41:39.093980: step 31710, loss = 0.88 (1786.4 examples/sec; 0.072 sec/batch)
2017-05-06 21:41:39.905720: step 31720, loss = 0.62 (1576.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:41:40.728896: step 31730, loss = 0.78 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:41.547035: step 31740, loss = 0.72 (1564.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:42.363652: step 31750, loss = 0.86 (1567.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:43.187862: step 31760, loss = 0.78 (1553.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:44.002910: step 31770, loss = 0.81 (1570.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:44.830248: step 31780, loss = 0.86 (1547.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:41:45.631930: step 31790, loss = 0.77 (1596.6 examples/sec; 0.080 sec/batch)
2017-05-06 21:41:46.555791: step 31800, loss = 0.93 (1385.5 examples/sec; 0.092 sec/batch)
2017-05-06 21:41:47.278875: step 31810, loss = 0.67 (1770.2 examples/sec; 0.072 sec/batch)
2017-05-06 21:41:48.085329: step 31820, loss = 0.87 (1587.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:41:48.905127: step 31830, loss = 0.95 (1561.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:49.716667: step 31840, loss = 0.74 (1577.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:41:50.536816: step 31850, loss = 0.84 (1560.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:51.352885: step 31860, loss = 0.66 (1568.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:52.168328: step 31870, loss = 0.95 (1569.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:52.991910: step 31880, loss = 0.75 (1554.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:53.817083: step 31890, loss = 0.86 (1551.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:41:54.741408: step 31900, loss = 0.77 (1384.8 examples/sec; 0.092 sec/batch)
2017-05-06 21:41:55.452635: step 31910, loss = 0.79 (1799.7 examples/sec; 0.071 sec/batch)
2017-05-06 21:41:56.276908: step 31920, loss = 0.82 (1552.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:57.094376: step 31930, loss = 0.85 (1565.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:57.916565: step 31940, loss = 0.76 (1556.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:58.740166: step 31950, loss = 0.78 (1554.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:41:59.550510: step 31960, loss = 0.86 (1579.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:42:00.369606: step 31970, loss = 0.82 (1562.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:01.195425: step 31980, loss = 0.88 (1550.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:42:02.013610: step 31990, loss = 0.74 (1564.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:02.939597: step 32000, loss = 0.87 (1382.3 examples/sec; 0.093 sec/batch)
2017-05-06 21:42:03.655680: step 32010, loss = 0.77 (1787.5 examples/sec; 0.072 sec/batch)
2017-05-06 21:42:04.477431: step 32020, loss = 0.86 (1557.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:05.307276: step 32030, loss = 0.73 (1542.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:42:06.134703: step 32040, loss = 0.68 (1547.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:42:06.952179: step 32050, loss = 0.85 (1565.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:07.768720: step 32060, loss = 0.80 (1567.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:08.595199: step 32070, loss = 0.68 (1548.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:42:09.419875: step 32080, loss = 0.83 (1552.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:10.238498: step 32090, loss = 0.75 (1563.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:11.191031: step 32100, loss = 0.72 (1343.8 examples/sec; 0.095 sec/batch)
2017-05-06 21:42:11.877588: step 32110, loss = 0.86 (1864.4 examples/sec; 0.069 sec/batch)
2017-05-06 21:42:12.701888: step 32120, loss = 0.71 (1552.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:13.523530: step 32130, loss = 0.68 (1557.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:14.352346: step 32140, loss = 0.76 (1544.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:42:15.179927: step 32150, loss = 0.70 (1546.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:42:15.991981: step 32160, loss = 0.69 (1576.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:42:16.805523: step 32170, loss = 0.68 (1573.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:42:17.629721: step 32180, loss = 0.79 (1553.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:18.449345: step 32190, loss = 0.81 (1561.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:19.367148: step 32200, loss = 0.71 (1394.6 examples/sec; 0.092 sec/batch)
2017-05-06 21:42:20.077118: step 32210, loss = 0.59 (1802.9 examples/sec; 0.071 sec/batch)
2017-05-06 21:42:20.896086: step 32220, loss = 0.92 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:21.717641: step 32230, loss = 0.73 (1558.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:22.544945: step 32240, loss = 0.74 (1547.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:42:23.364902: step 32250, loss = 0.83 (1561.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:24.177954: step 32260, loss = 0.88 (1574.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:42:24.991487: step 32270, loss = 0.71 (1573.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:42:25.817853: step 32280, loss = 0.91 (1548.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:42:26.650087: step 32290, loss = 0.67 (1538.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:42:27.563406: step 32300, loss = 0.70 (1401.5 examples/sec; 0.091 sec/batch)
2017-05-06 21:42:28.284716: step 32310, loss = 0.66 (1774.6 examples/sec; 0.072 sec/batch)
2017-05-06 21:42:29.107975: step 32320, loss = 0.91 (1554.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:29.930313: step 32330, loss = 0.83 (1556.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:30.753447: step 32340, loss = 0.78 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:31.565943: step 32350, loss = 0.72 (1575.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:42:32.387161: step 32360, loss = 0.71 (1558.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:33.206407: step 32370, loss = 0.78 (1562.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:34.027454: step 32380, loss = 0.73 (1559.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:34.854178: step 32390, loss = 0.85 (1548.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:42:35.768221: step 32400, loss = 0.82 (1400.4 examples/sec; 0.091 sec/batch)
2017-05-06 21:42:36.499521: step 32410, loss = 0.84 (1750.3 examples/sec; 0.073 sec/batch)
2017-05-06 21:42:37.321350: step 32420, loss = 0.74 (1557.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:38.144824: step 32430, loss = 0.82 (1554.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:38.972843: step 32440, loss = 0.73 (1545.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:42:39.785973: step 32450, loss = 0.74 (1574.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:42:40.604493: step 32460, loss = 0.81 (1563.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:41.424527: step 32470, loss = 0.80 (1560.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:42.245040: step 32480, loss = 0.80 (1560.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:43.068774: step 32490, loss = 0.65 (1553.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:43.984610: step 32500, loss = 0.78 (1397.6 examples/sec; 0.092 sec/batch)
2017-05-06 21:42:44.697489: step 32510, loss = 0.84 (1795.5 examples/sec; 0.071 sec/batch)
2017-05-06 21:42:45.523458: step 32520, loss = 0.83 (1549.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:42:46.351430: step 32530, loss = 0.72 (1545.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:42:47.190035: step 32540, loss = 0.81 (1526.4 examples/sec; 0.084 sec/batch)
2017-05-06 21:42:48.003725: step 32550, loss = 0.90 (1573.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:42:48.825393: step 32560, loss = 0.63 (1557.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:49.648773: step 32570, loss = 0.71 (1554.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:50.469412: step 32580, loss = 0.69 (1559.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:51.290674: step 32590, loss = 0.70 (1558.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:52.196668: step 32600, loss = 0.75 (1412.8 examples/sec; 0.091 sec/batch)
2017-05-06 21:42:52.926009: step 32610, loss = 0.74 (1755.0 examples/sec; 0.073 sec/batch)
2017-05-06 21:42:53.745544: step 32620, loss = 0.84 (1561.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:54.571428: step 32630, loss = 0.80 (1549.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:42:55.393273: step 32640, loss = 0.75 (1557.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:56.206705: step 32650, loss = 0.83 (1573.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:42:57.028518: step 32660, loss = 0.74 (1557.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:42:57.875855: step 32670, loss = 0.92 (1510.6 examples/sec; 0.085 sec/batch)
2017-05-06 21:42:58.678342: step 32680, loss = 0.74 (1595.0 examples/sec; 0.080 sec/batch)
2017-05-06 21:42:59.500387: step 32690, loss = 0.73 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:00.420399: step 32700, loss = 0.89 (1391.3 examples/sec; 0.092 sec/batch)
2017-05-06 21:43:01.145695: step 32710, loss = 0.65 (1764.8 examples/sec; 0.073 sec/batch)
2017-05-06 21:43:01.966326: step 32720, loss = 0.78 (1559.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:02.795164: step 32730, loss = 0.77 (1544.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:43:03.611349: step 32740, loss = 0.79 (1568.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:04.432630: step 32750, loss = 0.77 (1558.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:05.260671: step 32760, loss = 0.75 (1545.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:43:06.086468: step 32770, loss = 0.89 (1550.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:43:06.909817: step 32780, loss = 0.75 (1554.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:07.731089: step 32790, loss = 0.65 (1558.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:08.650994: step 32800, loss = 0.69 (1391.4 examples/sec; 0.092 sec/batch)
2017-05-06 21:43:09.375855: step 32810, loss = 0.78 (1765.9 examples/sec; 0.072 sec/batch)
2017-05-06 21:43:10.188680: step 32820, loss = 1.00 (1574.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:43:11.010325: step 32830, loss = 0.76 (1557.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:11.822825: step 32840, loss = 0.69 (1575.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:43:12.640288: step 32850, loss = 0.82 (1565.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:13.461055: step 32860, loss = 0.76 (1559.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:14.281915: step 32870, loss = 0.78 (1559.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:15.105668: step 32880, loss = 0.81 (1553.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:15.924770: step 32890, loss = 0.80 (1562.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:16.856875: step 32900, loss = 0.70 (1373.2 examples/sec; 0.093 sec/batch)
2017-05-06 21:43:17.577980: step 32910, loss = 0.69 (1775.1 examples/sec; 0.072 sec/batch)
2017-05-06 21:43:18.405388: step 32920, loss = 0.99 (1547.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:43:19.229227: step 32930, loss = 0.84 (1553.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:20.050001: step 32940, loss = 0.79 (1559.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:20.876870: step 32950, loss = 0.88 (1548.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:43:21.695285: step 32960, loss = 0.84 (1564.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:22.516720: step 32970, loss = 0.79 (1558.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:23.338337: step 32980, loss = 0.79 (1557.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:24.161839: step 32990, loss = 0.67 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:25.098860: step 33000, loss = 0.80 (1366.0 examples/sec; 0.094 sec/batch)
2017-05-06 21:43:25.809811: step 33010, loss = 0.65 (1800.4 examples/sec; 0.071 sec/batch)
2017-05-06 21:43:26.630837: step 33020, loss = 0.73 (1559.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:27.449803: step 33030, loss = 0.77 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:28.267421: step 33040, loss = 0.81 (1565.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:29.088362: step 33050, loss = 0.66 (1559.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:29.917125: step 33060, loss = 0.85 (1544.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:43:30.746254: step 33070, loss = 0.81 (1543.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:43:31.572891: step 33080, loss = 0.80 (1548.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:43:32.392637: step 33090, loss = 0.85 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:33.328790: step 33100, loss = 0.89 (1367.3 examples/sec; 0.094 sec/batch)
2017-05-06 21:43:34.053320: step 33110, loss = 0.81 (1766.6 examples/sec; 0.072 sec/batch)
2017-05-06 21:43:34.871167: step 33120, loss = 0.69 (1565.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:35.688919: step 33130, loss = 0.71 (1565.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:36.509446: step 33140, loss = 0.76 (1560.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:37.327732: step 33150, loss = 0.72 (1564.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:38.150449: step 33160, loss = 0.71 (1555.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:38.980655: step 33170, loss = 0.83 (1541.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:43:39.806115: step 33180, loss = 0.78 (1550.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:43:40.621914: step 33190, loss = 0.76 (1569.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:41.547670: step 33200, loss = 0.82 (1382.6 examples/sec; 0.093 sec/batch)
2017-05-06 21:43:42.268819: step 33210, loss = 0.74 (1774.9 examples/sec; 0.072 sec/batch)
2017-05-06 21:43:43.088025: step 33220, loss = 0.80 (1562.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:43.901448: step 33230, loss = 0.90 (1573.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:43:44.715506: step 33240, loss = 0.82 (1572.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:43:45.534815: step 33250, loss = 0.73 (1562.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:46.354347: step 33260, loss = 0.67 (1561.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:47.176976: step 33270, loss = 0.80 (1556.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:47.988991: step 33280, loss = 0.82 (1576.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:43:48.811435: step 33290, loss = 0.72 (1556.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:49.732669: step 33300, loss = 0.80 (1389.4 examples/sec; 0.092 sec/batch)
2017-05-06 21:43:50.453371: step 33310, loss = 0.92 (1776.0 examples/sec; 0.072 sec/batch)
2017-05-06 21:43:51.283935: step 33320, loss = 0.87 (1541.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:43:52.100661: step 33330, loss = 0.67 (1567.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:52.917381: step 33340, loss = 0.72 (1567.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:53.743273: step 33350, loss = 0.72 (1549.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:43:54.564745: step 33360, loss = 0.72 (1558.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:55.392076: step 33370, loss = 0.82 (1547.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:43:56.213273: step 33380, loss = 0.75 (1558.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:57.036611: step 33390, loss = 0.77 (1554.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:43:57.966944: step 33400, loss = 0.76 (1375.8 examples/sec; 0.093 sec/batch)
2017-05-06 21:43:58.685223: step 33410, loss = 0.65 (1782.0 examples/sec; 0.072 sec/batch)
2017-05-06 21:43:59.498580: step 33420, loss = 0.83 (1573.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:44:00.312296: step 33430, loss = 0.86 (1573.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:44:01.131299: step 33440, loss = 0.83 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:44:01.953172: step 33450, loss = 0.65 (1557.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:44:02.777479: step 33460, loss = 0.70 (1552.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:44:03.594286: step 33470, loss = 0.69 (1567.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:44:04.414412: step 33480, loss = 0.82 (1560.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:44:05.244135: step 33490, loss = 0.83 (1542.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:44:06.161177: step 33500, loss = 0.96 (1395.8 examples/sec; 0.092 sec/batch)
2017-05-06 21:44:06.883754: step 33510, loss = 0.70 (1771.4 examples/sec; 0.072 sec/batch)
2017-05-06 21:44:07.704327: step 33520, loss = 0.87 (1559.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:44:08.529530: step 33530, loss = 0.73 (1551.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:44:09.350329: step 33540, loss = 0.75 (1559.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:44:10.174362: step 33550, loss = 0.79 (1553.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:44:10.996849: step 33560, loss = 0.64 (1556.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:44:11.817947: step 33570, loss = 0.84 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:44:12.638470: step 33580, loss = 0.86 (1560.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:44:13.459687: step 33590, loss = 0.75 (1558.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:44:14.388458: step 33600, loss = 0.84 (1378.2 examples/sec; 0.093 sec/batch)
2017-05-06 21:44:15.102657: step 33610, loss = 0.82 (1792.2 examples/sec; 0.071 sec/batch)
2017-05-06 21:44:15.920633: step 33620, loss = 0.63 (1564.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:44:16.750264: step 33630, loss = 0.88 (1542.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:44:17.568413: step 33640, loss = 0.72 (1564.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:44:18.398105: step 33650, loss = 0.88 (1542.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:44:19.238125: step 33660, loss = 0.85 (1523.8 examples/sec; 0.084 sec/batch)
2017-05-06 21:44:20.028293: step 33670, loss = 0.73 (1619.9 examples/sec; 0.079 sec/batch)
2017-05-06 21:44:20.847437: step 33680, loss = 0.88 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:44:21.671337: step 33690, loss = 0.83 (1553.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:44:22.591738: step 33700, loss = 0.86 (1390.7 examples/sec; 0.092 sec/batch)
2017-05-06 21:44:23.315498: step 33710, loss = 0.84 (1768.6 examples/sec; 0.072 sec/batch)
2017-05-06 21:44:24.128521: step 33720, loss = 0.68 (1574.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:44:24.956817: step 33730, loss = 0.76 (1545.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:44:25.784147: step 33740, loss = 0.83 (1547.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:44:26.615414: step 33750, loss = 1.11 (1539.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:44:27.450093: step 33760, loss = 0.69 (1533.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:44:28.269693: step 33770, loss = 0.65 (1561.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:44:29.101982: step 33780, loss = 0.67 (1537.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:44:29.918211: step 33790, loss = 0.72 (1568.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:44:30.841007: step 33800, loss = 0.75 (1387.1 examples/sec; 0.092 sec/batch)
2017-05-06 21:44:31.557225: step 33810, loss = 0.79 (1787.2 examples/sec; 0.072 sec/batch)
2017-05-06 21:44:32.367984: step 33820, loss = 0.82 (1578.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:44:33.194375: step 33830, loss = 0.76 (1548.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:44:34.018016: step 33840, loss = 0.69 (1554.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:44:34.844814: step 33850, loss = 0.77 (1548.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:44:35.665896: step 33860, loss = 0.79 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:44:36.496259: step 33870, loss = 0.92 (1541.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:44:37.318934: step 33880, loss = 0.83 (1555.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:44:38.144386: step 33890, loss = 0.75 (1550.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:44:39.070675: step 33900, loss = 0.75 (1381.9 examples/sec; 0.093 sec/batch)
2017-05-06 21:44:39.780243: step 33910, loss = 0.85 (1803.9 examples/sec; 0.071 sec/batch)
2017-05-06 21:44:40.599348: step 33920, loss = 0.86 (1562.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:44:41.421323: step 33930, loss = 0.76 (1557.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:44:42.252019: step 33940, loss = 0.79 (1540.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:44:43.077331: step 33950, loss = 0.69 (1550.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:44:43.891296: step 33960, loss = 0.86 (1572.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:44:44.721027: step 33970, loss = 0.73 (1542.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:44:45.546767: step 33980, loss = 0.73 (1550.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:44:46.372243: step 33990, loss = 0.88 (1550.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:44:47.295452: step 34000, loss = 0.74 (1386.5 examples/sec; 0.092 sec/batch)
2017-05-06 21:44:48.024716: step 34010, loss = 0.71 (1755.2 examples/sec; 0.073 sec/batch)
2017-05-06 21:44:48.849338: step 34020, loss = 0.80 (1552.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:44:49.675914: step 34030, loss = 0.83 (1548.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:44:50.499086: step 34040, loss = 0.82 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:44:51.324203: step 34050, loss = 0.83 (1551.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:44:52.150127: step 34060, loss = 0.77 (1549.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:44:52.984949: step 34070, loss = 0.72 (1533.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:44:53.813929: step 34080, loss = 0.71 (1544.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:44:54.649943: step 34090, loss = 0.70 (1531.1 examples/sec; 0.084 sec/batch)
2017-05-06 21:44:55.573995: step 34100, loss = 0.71 (1385.2 examples/sec; 0.092 sec/batch)
2017-05-06 21:44:56.293544: step 34110, loss = 0.87 (1778.9 examples/sec; 0.072 sec/batch)
2017-05-06 21:44:57.113283: step 34120, loss = 0.66 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:44:57.938519: step 34130, loss = 0.73 (1551.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:44:58.761364: step 34140, loss = 0.78 (1555.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:44:59.580110: step 34150, loss = 0.87 (1563.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:45:00.400492: step 34160, loss = 0.72 (1560.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:45:01.227150: step 34170, loss = 0.61 (1548.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:45:02.060247: step 34180, loss = 0.89 (1536.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:45:02.880004: step 34190, loss = 0.74 (1561.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:45:03.789357: step 34200, loss = 0.86 (1407.6 examples/sec; 0.091 sec/batch)
2017-05-06 21:45:04.521445: step 34210, loss = 0.70 (1748.4 examples/sec; 0.073 sec/batch)
2017-05-06 21:45:05.340887: step 34220, loss = 0.74 (1562.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:45:06.163783: step 34230, loss = 0.81 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:45:06.988850: step 34240, loss = 0.79 (1551.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:45:07.803678: step 34250, loss = 0.83 (1570.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:45:08.629830: step 34260, loss = 0.92 (1549.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:45:09.456245: step 34270, loss = 0.69 (1548.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:45:10.277671: step 34280, loss = 0.72 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:45:11.091725: step 34290, loss = 0.77 (1572.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:45:12.026211: step 34300, loss = 0.67 (1369.7 examples/sec; 0.093 sec/batch)
2017-05-06 21:45:12.737365: step 34310, loss = 0.80 (1799.9 examples/sec; 0.071 sec/batch)
2017-05-06 21:45:13.559976: step 34320, loss = 0.70 (1556.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:45:14.376887: step 34330, loss = 0.78 (1566.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:45:15.201299: step 34340, loss = 0.80 (1552.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:45:16.010749: step 34350, loss = 0.90 (1581.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:45:16.828957: step 34360, loss = 0.69 (1564.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:45:17.660437: step 34370, loss = 0.69 (1539.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:45:18.491405: step 34380, loss = 0.68 (1540.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:45:19.317989: step 34390, loss = 0.94 (1548.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:45:20.235986: step 34400, loss = 0.77 (1394.3 examples/sec; 0.092 sec/batch)
2017-05-06 21:45:20.979785: step 34410, loss = 0.69 (1720.9 examples/sec; 0.074 sec/batch)
2017-05-06 21:45:21.801086: step 34420, loss = 0.77 (1558.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:45:22.627977: step 34430, loss = 0.92 (1548.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:45:23.439365: step 34440, loss = 0.92 (1577.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:45:24.263662: step 34450, loss = 0.81 (1552.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:45:25.096012: step 34460, loss = 0.72 (1537.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:45:25.917921: step 34470, loss = 0.61 (1557.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:45:26.739952: step 34480, loss = 0.95 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:45:27.556205: step 34490, loss = 0.80 (1568.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:45:28.478393: step 34500, loss = 0.79 (1388.0 examples/sec; 0.092 sec/batch)
2017-05-06 21:45:29.206899: step 34510, loss = 1.00 (1757.0 examples/sec; 0.073 sec/batch)
2017-05-06 21:45:30.035769: step 34520, loss = 0.70 (1544.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:45:30.852360: step 34530, loss = 0.83 (1567.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:45:31.665113: step 34540, loss = 0.80 (1574.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:45:32.494860: step 34550, loss = 0.75 (1542.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:45:33.319366: step 34560, loss = 0.76 (1552.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:45:34.145678: step 34570, loss = 0.88 (1549.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:45:34.977546: step 34580, loss = 0.60 (1538.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:45:35.792622: step 34590, loss = 0.74 (1570.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:45:36.712828: step 34600, loss = 0.74 (1391.0 examples/sec; 0.092 sec/batch)
2017-05-06 21:45:37.446909: step 34610, loss = 0.68 (1743.7 examples/sec; 0.073 sec/batch)
2017-05-06 21:45:38.272470: step 34620, loss = 0.76 (1550.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:45:39.098683: step 34630, loss = 0.74 (1549.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:45:39.915306: step 34640, loss = 0.64 (1567.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:45:40.764154: step 34650, loss = 0.86 (1507.9 examples/sec; 0.085 sec/batch)
2017-05-06 21:45:41.562440: step 34660, loss = 0.80 (1603.4 examples/sec; 0.080 sec/batch)
2017-05-06 21:45:42.388037: step 34670, loss = 0.83 (1550.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:45:43.213009: step 34680, loss = 0.69 (1551.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:45:44.030143: step 34690, loss = 0.80 (1566.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:45:44.964751: step 34700, loss = 0.80 (1369.6 examples/sec; 0.093 sec/batch)
2017-05-06 21:45:45.689995: step 34710, loss = 0.85 (1764.9 examples/sec; 0.073 sec/batch)
2017-05-06 21:45:46.515784: step 34720, loss = 0.90 (1550.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:45:47.340966: step 34730, loss = 0.77 (1551.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:45:48.163007: step 34740, loss = 0.83 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:45:48.990710: step 34750, loss = 0.81 (1546.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:45:49.823568: step 34760, loss = 0.66 (1536.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:45:50.648701: step 34770, loss = 0.73 (1551.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:45:51.477679: step 34780, loss = 0.56 (1544.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:45:52.292230: step 34790, loss = 0.66 (1571.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:45:53.210290: step 34800, loss = 0.71 (1394.2 examples/sec; 0.092 sec/batch)
2017-05-06 21:45:53.933377: step 34810, loss = 0.96 (1770.2 examples/sec; 0.072 sec/batch)
2017-05-06 21:45:54.763193: step 34820, loss = 0.74 (1542.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:45:55.586416: step 34830, loss = 0.74 (1554.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:45:56.409237: step 34840, loss = 0.78 (1555.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:45:57.233384: step 34850, loss = 0.86 (1553.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:45:58.064034: step 34860, loss = 0.62 (1541.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:45:58.891481: step 34870, loss = 0.85 (1546.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:45:59.705740: step 34880, loss = 1.03 (1572.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:46:00.535988: step 34890, loss = 0.83 (1541.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:46:01.461779: step 34900, loss = 0.77 (1382.6 examples/sec; 0.093 sec/batch)
2017-05-06 21:46:02.188386: step 34910, loss = 0.80 (1761.6 examples/sec; 0.073 sec/batch)
2017-05-06 21:46:03.013938: step 34920, loss = 0.83 (1550.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:46:03.832008: step 34930, loss = 1.06 (1564.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:46:04.658414: step 34940, loss = 0.94 (1548.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:46:05.490584: step 34950, loss = 0.94 (1538.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:46:06.318854: step 34960, loss = 0.73 (1545.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:46:07.145937: step 34970, loss = 0.70 (1547.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:46:07.963268: step 34980, loss = 0.98 (1566.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:46:08.796447: step 34990, loss = 0.88 (1536.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:46:09.725739: step 35000, loss = 0.76 (1377.4 examples/sec; 0.093 sec/batch)
2017-05-06 21:46:10.450584: step 35010, loss = 0.67 (1765.9 examples/sec; 0.072 sec/batch)
2017-05-06 21:46:11.273553: step 35020, loss = 0.67 (1555.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:46:12.092730: step 35030, loss = 0.74 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:46:12.910681: step 35040, loss = 0.75 (1564.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:46:13.734865: step 35050, loss = 0.65 (1553.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:46:14.546824: step 35060, loss = 0.82 (1576.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:46:15.369426: step 35070, loss = 0.97 (1556.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:46:16.177963: step 35080, loss = 0.62 (1583.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:46:17.008287: step 35090, loss = 0.73 (1541.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:46:17.933159: step 35100, loss = 0.84 (1384.0 examples/sec; 0.092 sec/batch)
2017-05-06 21:46:18.659173: step 35110, loss = 0.86 (1763.1 examples/sec; 0.073 sec/batch)
2017-05-06 21:46:19.474538: step 35120, loss = 0.69 (1569.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:46:20.293286: step 35130, loss = 0.68 (1563.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:46:21.101311: step 35140, loss = 0.87 (1584.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:46:21.928747: step 35150, loss = 0.78 (1547.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:46:22.754907: step 35160, loss = 0.75 (1549.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:46:23.575027: step 35170, loss = 0.67 (1560.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:46:24.400950: step 35180, loss = 0.75 (1549.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:46:25.222289: step 35190, loss = 0.78 (1558.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:46:26.149620: step 35200, loss = 0.72 (1380.3 examples/sec; 0.093 sec/batch)
2017-05-06 21:46:26.871296: step 35210, loss = 0.83 (1773.7 examples/sec; 0.072 sec/batch)
2017-05-06 21:46:27.687351: step 35220, loss = 0.84 (1568.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:46:28.514602: step 35230, loss = 0.75 (1547.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:46:29.341488: step 35240, loss = 0.81 (1548.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:46:30.170665: step 35250, loss = 0.85 (1543.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:46:30.983798: step 35260, loss = 0.85 (1574.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:46:31.799686: step 35270, loss = 0.70 (1568.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:46:32.622875: step 35280, loss = 0.87 (1554.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:46:33.450331: step 35290, loss = 0.75 (1546.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:46:34.368859: step 35300, loss = 0.74 (1393.5 examples/sec; 0.092 sec/batch)
2017-05-06 21:46:35.092549: step 35310, loss = 0.64 (1768.7 examples/sec; 0.072 sec/batch)
2017-05-06 21:46:35.905630: step 35320, loss = 0.70 (1574.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:46:36.732017: step 35330, loss = 0.69 (1548.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:46:37.556749: step 35340, loss = 0.70 (1552.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:46:38.390768: step 35350, loss = 0.92 (1534.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:46:39.206636: step 35360, loss = 0.74 (1569.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:46:40.022505: step 35370, loss = 0.74 (1568.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:46:40.841514: step 35380, loss = 0.70 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:46:41.653531: step 35390, loss = 0.78 (1576.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:46:42.576472: step 35400, loss = 0.93 (1386.9 examples/sec; 0.092 sec/batch)
2017-05-06 21:46:43.289862: step 35410, loss = 0.80 (1794.2 examples/sec; 0.071 sec/batch)
2017-05-06 21:46:44.103967: step 35420, loss = 0.71 (1572.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:46:44.926316: step 35430, loss = 0.80 (1556.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:46:45.751597: step 35440, loss = 0.89 (1551.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:46:46.585945: step 35450, loss = 0.67 (1534.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:46:47.403955: step 35460, loss = 0.75 (1564.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:46:48.214935: step 35470, loss = 0.67 (1578.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:46:49.033093: step 35480, loss = 0.90 (1564.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:46:49.858382: step 35490, loss = 0.64 (1551.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:46:50.775723: step 35500, loss = 0.81 (1395.3 examples/sec; 0.092 sec/batch)
2017-05-06 21:46:51.498833: step 35510, loss = 0.69 (1770.1 examples/sec; 0.072 sec/batch)
2017-05-06 21:46:52.320340: step 35520, loss = 0.70 (1558.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:46:53.143237: step 35530, loss = 0.72 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:46:53.962221: step 35540, loss = 0.77 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:46:54.788137: step 35550, loss = 0.72 (1549.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:46:55.601356: step 35560, loss = 0.79 (1574.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:46:56.424749: step 35570, loss = 0.78 (1554.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:46:57.255477: step 35580, loss = 0.74 (1540.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:46:58.071323: step 35590, loss = 0.72 (1568.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:46:59.011647: step 35600, loss = 0.81 (1361.2 examples/sec; 0.094 sec/batch)
2017-05-06 21:46:59.719077: step 35610, loss = 0.61 (1809.4 examples/sec; 0.071 sec/batch)
2017-05-06 21:47:00.540301: step 35620, loss = 0.85 (1558.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:01.359993: step 35630, loss = 0.75 (1561.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:02.218701: step 35640, loss = 0.82 (1490.6 examples/sec; 0.086 sec/batch)
2017-05-06 21:47:03.007967: step 35650, loss = 0.83 (1621.7 examples/sec; 0.079 sec/batch)
2017-05-06 21:47:03.820147: step 35660, loss = 0.61 (1576.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:47:04.642230: step 35670, loss = 0.87 (1557.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:05.463121: step 35680, loss = 0.89 (1559.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:06.285356: step 35690, loss = 0.89 (1556.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:07.212135: step 35700, loss = 0.70 (1381.1 examples/sec; 0.093 sec/batch)
2017-05-06 21:47:07.937878: step 35710, loss = 0.79 (1763.7 examples/sec; 0.073 sec/batch)
2017-05-06 21:47:08.756841: step 35720, loss = 0.86 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:09.580614: step 35730, loss = 0.70 (1553.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:10.417559: step 35740, loss = 0.80 (1529.4 examples/sec; 0.084 sec/batch)
2017-05-06 21:47:11.230941: step 35750, loss = 0.81 (1573.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:47:12.043416: step 35760, loss = 0.78 (1575.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:47:12.867160: step 35770, loss = 0.85 (1553.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:13.693509: step 35780, loss = 0.85 (1549.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:47:14.515772: step 35790, loss = 1.00 (1556.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:15.438212: step 35800, loss = 0.71 (1387.6 examples/sec; 0.092 sec/batch)
2017-05-06 21:47:16.158396: step 35810, loss = 0.75 (1777.3 examples/sec; 0.072 sec/batch)
2017-05-06 21:47:16.977919: step 35820, loss = 0.71 (1561.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:17.802661: step 35830, loss = 0.87 (1552.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:18.630607: step 35840, loss = 0.71 (1546.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:47:19.437737: step 35850, loss = 0.81 (1585.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:47:20.258704: step 35860, loss = 0.95 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:21.077895: step 35870, loss = 0.73 (1562.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:21.904152: step 35880, loss = 0.71 (1549.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:47:22.726116: step 35890, loss = 0.90 (1557.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:23.651857: step 35900, loss = 0.68 (1382.7 examples/sec; 0.093 sec/batch)
2017-05-06 21:47:24.359862: step 35910, loss = 0.81 (1807.9 examples/sec; 0.071 sec/batch)
2017-05-06 21:47:25.176891: step 35920, loss = 0.87 (1566.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:25.999986: step 35930, loss = 0.68 (1555.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:26.830059: step 35940, loss = 0.79 (1542.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:47:27.642797: step 35950, loss = 0.79 (1574.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:47:28.462233: step 35960, loss = 0.76 (1562.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:29.276794: step 35970, loss = 0.67 (1571.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:47:30.107694: step 35980, loss = 0.85 (1540.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:47:30.923627: step 35990, loss = 0.79 (1568.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:31.840401: step 36000, loss = 0.76 (1396.2 examples/sec; 0.092 sec/batch)
2017-05-06 21:47:32.558811: step 36010, loss = 0.71 (1781.7 examples/sec; 0.072 sec/batch)
2017-05-06 21:47:33.379551: step 36020, loss = 0.87 (1559.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:34.229231: step 36030, loss = 0.77 (1506.5 examples/sec; 0.085 sec/batch)
2017-05-06 21:47:35.027023: step 36040, loss = 0.80 (1604.4 examples/sec; 0.080 sec/batch)
2017-05-06 21:47:35.838455: step 36050, loss = 0.72 (1577.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:47:36.657767: step 36060, loss = 0.93 (1562.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:37.496523: step 36070, loss = 0.75 (1526.1 examples/sec; 0.084 sec/batch)
2017-05-06 21:47:38.305028: step 36080, loss = 0.73 (1583.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:47:39.127706: step 36090, loss = 0.82 (1555.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:40.044679: step 36100, loss = 0.72 (1395.9 examples/sec; 0.092 sec/batch)
2017-05-06 21:47:40.765627: step 36110, loss = 0.70 (1775.5 examples/sec; 0.072 sec/batch)
2017-05-06 21:47:41.587061: step 36120, loss = 0.89 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:42.409494: step 36130, loss = 0.63 (1556.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:43.234668: step 36140, loss = 0.86 (1551.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:47:44.050159: step 36150, loss = 0.71 (1569.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:44.868505: step 36160, loss = 1.04 (1564.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:45.687789: step 36170, loss = 0.64 (1562.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:46.512777: step 36180, loss = 0.75 (1551.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:47.338236: step 36190, loss = 0.91 (1550.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:47:48.267661: step 36200, loss = 1.05 (1377.2 examples/sec; 0.093 sec/batch)
2017-05-06 21:47:48.982337: step 36210, loss = 0.78 (1791.0 examples/sec; 0.071 sec/batch)
2017-05-06 21:47:49.801325: step 36220, loss = 0.80 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:50.624251: step 36230, loss = 0.83 (1555.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:51.448497: step 36240, loss = 0.83 (1552.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:52.260202: step 36250, loss = 0.71 (1576.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:47:53.073219: step 36260, loss = 1.11 (1574.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:47:53.899081: step 36270, loss = 0.80 (1549.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:47:54.716506: step 36280, loss = 0.69 (1565.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:55.532066: step 36290, loss = 0.75 (1569.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:56.459160: step 36300, loss = 0.71 (1380.6 examples/sec; 0.093 sec/batch)
2017-05-06 21:47:57.171376: step 36310, loss = 0.83 (1797.2 examples/sec; 0.071 sec/batch)
2017-05-06 21:47:57.994886: step 36320, loss = 0.78 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:58.818807: step 36330, loss = 0.77 (1553.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:47:59.634335: step 36340, loss = 0.81 (1569.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:00.451331: step 36350, loss = 0.71 (1566.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:01.275369: step 36360, loss = 0.81 (1553.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:02.088640: step 36370, loss = 0.67 (1573.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:48:02.906640: step 36380, loss = 0.72 (1564.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:03.720780: step 36390, loss = 0.75 (1572.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:48:04.645770: step 36400, loss = 0.93 (1383.8 examples/sec; 0.092 sec/batch)
2017-05-06 21:48:05.356410: step 36410, loss = 0.76 (1801.2 examples/sec; 0.071 sec/batch)
2017-05-06 21:48:06.185056: step 36420, loss = 0.77 (1544.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:48:07.004331: step 36430, loss = 0.77 (1562.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:07.820694: step 36440, loss = 0.83 (1567.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:08.638738: step 36450, loss = 0.84 (1564.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:09.459410: step 36460, loss = 0.87 (1559.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:10.281765: step 36470, loss = 0.88 (1556.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:11.104490: step 36480, loss = 0.84 (1555.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:11.912544: step 36490, loss = 0.69 (1584.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:48:12.909322: step 36500, loss = 0.79 (1284.1 examples/sec; 0.100 sec/batch)
2017-05-06 21:48:13.570752: step 36510, loss = 0.93 (1935.2 examples/sec; 0.066 sec/batch)
2017-05-06 21:48:14.389498: step 36520, loss = 0.93 (1563.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:15.218620: step 36530, loss = 0.73 (1543.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:48:16.031663: step 36540, loss = 0.73 (1574.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:48:16.856200: step 36550, loss = 0.75 (1552.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:17.680255: step 36560, loss = 0.87 (1553.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:18.498176: step 36570, loss = 0.90 (1564.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:19.314910: step 36580, loss = 0.92 (1567.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:20.131182: step 36590, loss = 0.70 (1568.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:21.054719: step 36600, loss = 0.75 (1386.0 examples/sec; 0.092 sec/batch)
2017-05-06 21:48:21.773289: step 36610, loss = 0.68 (1781.3 examples/sec; 0.072 sec/batch)
2017-05-06 21:48:22.596895: step 36620, loss = 0.69 (1554.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:23.440203: step 36630, loss = 0.72 (1517.8 examples/sec; 0.084 sec/batch)
2017-05-06 21:48:24.219938: step 36640, loss = 0.81 (1641.6 examples/sec; 0.078 sec/batch)
2017-05-06 21:48:25.046544: step 36650, loss = 0.71 (1548.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:48:25.868964: step 36660, loss = 0.80 (1556.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:26.692668: step 36670, loss = 0.73 (1554.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:27.504281: step 36680, loss = 0.80 (1577.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:48:28.323141: step 36690, loss = 0.79 (1563.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:29.259031: step 36700, loss = 0.89 (1367.7 examples/sec; 0.094 sec/batch)
2017-05-06 21:48:29.992632: step 36710, loss = 0.84 (1744.8 examples/sec; 0.073 sec/batch)
2017-05-06 21:48:30.813781: step 36720, loss = 0.70 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:31.641948: step 36730, loss = 0.77 (1545.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:48:32.447224: step 36740, loss = 0.68 (1589.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:48:33.266006: step 36750, loss = 0.76 (1563.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:34.085881: step 36760, loss = 0.91 (1561.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:34.903912: step 36770, loss = 0.71 (1564.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:35.715087: step 36780, loss = 0.77 (1578.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:48:36.533068: step 36790, loss = 0.76 (1564.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:37.459289: step 36800, loss = 0.80 (1382.0 examples/sec; 0.093 sec/batch)
2017-05-06 21:48:38.187625: step 36810, loss = 0.71 (1757.4 examples/sec; 0.073 sec/batch)
2017-05-06 21:48:39.008485: step 36820, loss = 0.76 (1559.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:39.822996: step 36830, loss = 0.72 (1571.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:48:40.633774: step 36840, loss = 0.69 (1578.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:48:41.449939: step 36850, loss = 0.73 (1568.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:42.270877: step 36860, loss = 0.65 (1559.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:43.085434: step 36870, loss = 0.74 (1571.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:48:43.895842: step 36880, loss = 0.79 (1579.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:48:44.716061: step 36890, loss = 0.66 (1560.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:45.642135: step 36900, loss = 0.81 (1382.2 examples/sec; 0.093 sec/batch)
2017-05-06 21:48:46.358905: step 36910, loss = 0.73 (1785.8 examples/sec; 0.072 sec/batch)
2017-05-06 21:48:47.179335: step 36920, loss = 0.65 (1560.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:48.002466: step 36930, loss = 0.72 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:48.824605: step 36940, loss = 0.74 (1556.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:49.644625: step 36950, loss = 0.85 (1561.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:50.462366: step 36960, loss = 0.74 (1565.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:51.289726: step 36970, loss = 0.84 (1547.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:48:52.099130: step 36980, loss = 0.76 (1581.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:48:52.928509: step 36990, loss = 0.75 (1543.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:48:53.836255: step 37000, loss = 0.78 (1410.1 examples/sec; 0.091 sec/batch)
2017-05-06 21:48:54.555422: step 37010, loss = 0.73 (1779.8 examples/sec; 0.072 sec/batch)
2017-05-06 21:48:55.372708: step 37020, loss = 0.79 (1566.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:56.188796: step 37030, loss = 0.73 (1568.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:57.005221: step 37040, loss = 0.93 (1567.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:48:57.833518: step 37050, loss = 0.86 (1545.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:48:58.645674: step 37060, loss = 0.69 (1576.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:48:59.459198: step 37070, loss = 0.69 (1573.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:49:00.279474: step 37080, loss = 0.82 (1560.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:01.098789: step 37090, loss = 0.76 (1562.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:02.020623: step 37100, loss = 0.91 (1388.5 examples/sec; 0.092 sec/batch)
2017-05-06 21:49:02.746402: step 37110, loss = 0.78 (1763.6 examples/sec; 0.073 sec/batch)
2017-05-06 21:49:03.566390: step 37120, loss = 0.85 (1561.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:04.388652: step 37130, loss = 0.70 (1556.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:05.206358: step 37140, loss = 0.65 (1565.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:06.026631: step 37150, loss = 0.81 (1560.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:06.851996: step 37160, loss = 0.86 (1550.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:49:07.668249: step 37170, loss = 0.78 (1568.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:08.486962: step 37180, loss = 0.62 (1563.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:09.300695: step 37190, loss = 0.80 (1573.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:49:10.227806: step 37200, loss = 0.75 (1380.6 examples/sec; 0.093 sec/batch)
2017-05-06 21:49:10.953792: step 37210, loss = 0.72 (1763.1 examples/sec; 0.073 sec/batch)
2017-05-06 21:49:11.770190: step 37220, loss = 0.67 (1567.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:12.597214: step 37230, loss = 0.72 (1547.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:49:13.410709: step 37240, loss = 0.85 (1573.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:49:14.235278: step 37250, loss = 0.73 (1552.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:15.047573: step 37260, loss = 0.59 (1575.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:49:15.865068: step 37270, loss = 0.77 (1565.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:16.686345: step 37280, loss = 0.57 (1558.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:17.510880: step 37290, loss = 0.92 (1552.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:18.434398: step 37300, loss = 0.79 (1386.0 examples/sec; 0.092 sec/batch)
2017-05-06 21:49:19.156385: step 37310, loss = 0.88 (1772.9 examples/sec; 0.072 sec/batch)
2017-05-06 21:49:19.969224: step 37320, loss = 0.87 (1574.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:49:20.792189: step 37330, loss = 0.63 (1555.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:21.612801: step 37340, loss = 0.71 (1559.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:22.429914: step 37350, loss = 0.79 (1566.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:23.250222: step 37360, loss = 0.77 (1560.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:24.066825: step 37370, loss = 0.76 (1567.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:24.894464: step 37380, loss = 0.77 (1546.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:49:25.715554: step 37390, loss = 0.87 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:26.642161: step 37400, loss = 0.67 (1381.4 examples/sec; 0.093 sec/batch)
2017-05-06 21:49:27.370727: step 37410, loss = 0.77 (1756.9 examples/sec; 0.073 sec/batch)
2017-05-06 21:49:28.186253: step 37420, loss = 0.75 (1569.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:29.014576: step 37430, loss = 0.69 (1545.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:49:29.839057: step 37440, loss = 0.70 (1552.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:30.658697: step 37450, loss = 0.89 (1561.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:31.474832: step 37460, loss = 0.76 (1568.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:32.305094: step 37470, loss = 0.74 (1541.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:49:33.120629: step 37480, loss = 0.53 (1569.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:33.940922: step 37490, loss = 0.73 (1560.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:34.857913: step 37500, loss = 0.82 (1395.9 examples/sec; 0.092 sec/batch)
2017-05-06 21:49:35.578812: step 37510, loss = 0.72 (1775.5 examples/sec; 0.072 sec/batch)
2017-05-06 21:49:36.394743: step 37520, loss = 0.92 (1568.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:37.218758: step 37530, loss = 0.86 (1553.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:38.034344: step 37540, loss = 0.78 (1569.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:38.856722: step 37550, loss = 0.67 (1556.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:39.665871: step 37560, loss = 0.83 (1581.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:49:40.489601: step 37570, loss = 0.78 (1553.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:41.314011: step 37580, loss = 0.92 (1552.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:42.132773: step 37590, loss = 0.99 (1563.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:43.050856: step 37600, loss = 0.95 (1394.2 examples/sec; 0.092 sec/batch)
2017-05-06 21:49:43.773770: step 37610, loss = 0.67 (1770.6 examples/sec; 0.072 sec/batch)
2017-05-06 21:49:44.619684: step 37620, loss = 0.95 (1513.2 examples/sec; 0.085 sec/batch)
2017-05-06 21:49:45.410713: step 37630, loss = 0.76 (1618.2 examples/sec; 0.079 sec/batch)
2017-05-06 21:49:46.234935: step 37640, loss = 0.80 (1553.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:47.058230: step 37650, loss = 0.83 (1554.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:47.862945: step 37660, loss = 0.91 (1590.6 examples/sec; 0.080 sec/batch)
2017-05-06 21:49:48.695238: step 37670, loss = 0.67 (1537.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:49:49.518340: step 37680, loss = 0.97 (1555.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:50.339880: step 37690, loss = 1.00 (1558.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:51.261567: step 37700, loss = 0.78 (1388.8 examples/sec; 0.092 sec/batch)
2017-05-06 21:49:51.984462: step 37710, loss = 0.71 (1770.6 examples/sec; 0.072 sec/batch)
2017-05-06 21:49:52.814788: step 37720, loss = 0.86 (1541.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:49:53.633556: step 37730, loss = 0.67 (1563.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:54.460780: step 37740, loss = 0.80 (1547.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:49:55.275633: step 37750, loss = 0.80 (1570.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:49:56.090970: step 37760, loss = 0.74 (1569.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:56.912064: step 37770, loss = 0.72 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:57.734629: step 37780, loss = 0.70 (1556.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:49:58.560566: step 37790, loss = 0.80 (1549.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:49:59.502692: step 37800, loss = 0.76 (1358.6 examples/sec; 0.094 sec/batch)
2017-05-06 21:50:00.197669: step 37810, loss = 0.81 (1841.8 examples/sec; 0.069 sec/batch)
2017-05-06 21:50:01.026234: step 37820, loss = 0.85 (1544.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:50:01.856825: step 37830, loss = 0.71 (1541.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:50:02.677601: step 37840, loss = 0.83 (1559.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:03.507162: step 37850, loss = 0.84 (1543.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:50:04.313701: step 37860, loss = 0.91 (1587.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:50:05.136370: step 37870, loss = 0.84 (1555.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:05.958788: step 37880, loss = 0.72 (1556.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:06.777886: step 37890, loss = 0.89 (1562.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:07.708219: step 37900, loss = 0.70 (1375.9 examples/sec; 0.093 sec/batch)
2017-05-06 21:50:08.417026: step 37910, loss = 0.82 (1805.9 examples/sec; 0.071 sec/batch)
2017-05-06 21:50:09.239007: step 37920, loss = 0.77 (1557.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:10.066486: step 37930, loss = 0.71 (1546.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:50:10.883876: step 37940, loss = 0.72 (1566.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:11.699531: step 37950, loss = 0.84 (1569.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:12.519182: step 37960, loss = 0.71 (1561.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:13.342058: step 37970, loss = 0.75 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:14.172911: step 37980, loss = 0.71 (1540.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:50:14.988239: step 37990, loss = 0.81 (1569.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:15.905233: step 38000, loss = 0.72 (1395.9 examples/sec; 0.092 sec/batch)
2017-05-06 21:50:16.625767: step 38010, loss = 0.71 (1776.5 examples/sec; 0.072 sec/batch)
2017-05-06 21:50:17.459748: step 38020, loss = 0.84 (1534.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:50:18.284513: step 38030, loss = 0.79 (1552.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:19.108175: step 38040, loss = 0.70 (1554.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:19.924260: step 38050, loss = 0.79 (1568.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:20.739774: step 38060, loss = 0.62 (1569.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:21.568746: step 38070, loss = 0.76 (1544.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:50:22.395230: step 38080, loss = 0.89 (1548.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:50:23.216476: step 38090, loss = 0.92 (1558.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:24.132415: step 38100, loss = 0.80 (1397.5 examples/sec; 0.092 sec/batch)
2017-05-06 21:50:24.863136: step 38110, loss = 0.84 (1751.7 examples/sec; 0.073 sec/batch)
2017-05-06 21:50:25.692705: step 38120, loss = 0.76 (1543.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:50:26.506366: step 38130, loss = 0.76 (1573.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:50:27.324243: step 38140, loss = 0.78 (1565.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:28.152115: step 38150, loss = 0.81 (1546.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:50:28.975243: step 38160, loss = 0.79 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:29.803464: step 38170, loss = 0.88 (1545.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:50:30.631543: step 38180, loss = 0.85 (1545.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:50:31.457755: step 38190, loss = 0.73 (1549.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:50:32.373492: step 38200, loss = 0.88 (1397.8 examples/sec; 0.092 sec/batch)
2017-05-06 21:50:33.098378: step 38210, loss = 0.77 (1765.8 examples/sec; 0.072 sec/batch)
2017-05-06 21:50:33.920917: step 38220, loss = 0.72 (1556.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:34.748871: step 38230, loss = 0.90 (1546.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:50:35.566529: step 38240, loss = 0.76 (1565.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:36.390093: step 38250, loss = 0.75 (1554.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:37.208216: step 38260, loss = 0.83 (1564.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:38.031961: step 38270, loss = 0.65 (1553.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:38.855546: step 38280, loss = 0.74 (1554.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:39.669564: step 38290, loss = 0.69 (1572.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:50:40.580646: step 38300, loss = 0.71 (1404.9 examples/sec; 0.091 sec/batch)
2017-05-06 21:50:41.314128: step 38310, loss = 0.71 (1745.1 examples/sec; 0.073 sec/batch)
2017-05-06 21:50:42.136491: step 38320, loss = 0.79 (1556.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:42.965867: step 38330, loss = 0.89 (1543.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:50:43.774863: step 38340, loss = 0.76 (1582.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:50:44.595098: step 38350, loss = 0.74 (1560.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:45.421314: step 38360, loss = 0.65 (1549.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:50:46.236383: step 38370, loss = 0.66 (1570.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:47.054860: step 38380, loss = 0.87 (1563.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:47.873422: step 38390, loss = 0.78 (1563.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:48.793183: step 38400, loss = 0.73 (1391.7 examples/sec; 0.092 sec/batch)
2017-05-06 21:50:49.521022: step 38410, loss = 0.80 (1758.6 examples/sec; 0.073 sec/batch)
2017-05-06 21:50:50.344150: step 38420, loss = 0.73 (1555.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:51.167304: step 38430, loss = 0.73 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:51.980918: step 38440, loss = 0.83 (1573.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:50:52.800777: step 38450, loss = 0.63 (1561.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:53.623991: step 38460, loss = 0.72 (1554.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:54.444489: step 38470, loss = 0.79 (1560.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:55.254796: step 38480, loss = 0.72 (1579.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:50:56.073967: step 38490, loss = 0.78 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:50:56.986800: step 38500, loss = 0.76 (1402.2 examples/sec; 0.091 sec/batch)
2017-05-06 21:50:57.713371: step 38510, loss = 0.78 (1761.7 examples/sec; 0.073 sec/batch)
2017-05-06 21:50:58.546192: step 38520, loss = 0.89 (1537.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:50:59.365830: step 38530, loss = 0.81 (1561.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:51:00.177177: step 38540, loss = 0.83 (1577.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:51:01.002032: step 38550, loss = 0.73 (1551.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:51:01.822225: step 38560, loss = 0.83 (1560.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:51:02.646456: step 38570, loss = 0.83 (1552.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:51:03.457173: step 38580, loss = 0.62 (1578.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:51:04.283660: step 38590, loss = 0.82 (1548.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:51:05.210236: step 38600, loss = 0.72 (1381.4 examples/sec; 0.093 sec/batch)
2017-05-06 21:51:05.959285: step 38610, loss = 0.70 (1708.9 examples/sec; 0.075 sec/batch)
2017-05-06 21:51:06.754960: step 38620, loss = 0.82 (1608.7 examples/sec; 0.080 sec/batch)
2017-05-06 21:51:07.568837: step 38630, loss = 0.79 (1572.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:51:08.386054: step 38640, loss = 0.83 (1566.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:51:09.206596: step 38650, loss = 0.70 (1559.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:51:10.024933: step 38660, loss = 0.77 (1564.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:51:10.853985: step 38670, loss = 0.78 (1543.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:51:11.663597: step 38680, loss = 0.68 (1581.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:51:12.492569: step 38690, loss = 0.56 (1544.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:51:13.406903: step 38700, loss = 0.96 (1399.9 examples/sec; 0.091 sec/batch)
2017-05-06 21:51:14.129932: step 38710, loss = 0.77 (1770.3 examples/sec; 0.072 sec/batch)
2017-05-06 21:51:14.949809: step 38720, loss = 0.67 (1561.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:51:15.757351: step 38730, loss = 0.80 (1585.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:51:16.580776: step 38740, loss = 0.99 (1554.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:51:17.397126: step 38750, loss = 0.85 (1568.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:51:18.210678: step 38760, loss = 0.77 (1573.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:51:19.037490: step 38770, loss = 0.87 (1548.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:51:19.854207: step 38780, loss = 0.90 (1567.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:51:20.674037: step 38790, loss = 0.71 (1561.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:51:21.588497: step 38800, loss = 0.72 (1399.7 examples/sec; 0.091 sec/batch)
2017-05-06 21:51:22.331877: step 38810, loss = 0.74 (1721.9 examples/sec; 0.074 sec/batch)
2017-05-06 21:51:23.158934: step 38820, loss = 0.90 (1547.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:51:23.969254: step 38830, loss = 0.70 (1579.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:51:24.782452: step 38840, loss = 0.81 (1574.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:51:25.599621: step 38850, loss = 0.61 (1566.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:51:26.420999: step 38860, loss = 0.70 (1558.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:51:27.249585: step 38870, loss = 0.70 (1544.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:51:28.074803: step 38880, loss = 0.71 (1551.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:51:28.898203: step 38890, loss = 0.83 (1554.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:51:29.823119: step 38900, loss = 0.66 (1383.9 examples/sec; 0.092 sec/batch)
2017-05-06 21:51:30.557516: step 38910, loss = 0.76 (1742.9 examples/sec; 0.073 sec/batch)
2017-05-06 21:51:31.379134: step 38920, loss = 1.00 (1557.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:51:32.218899: step 38930, loss = 0.82 (1524.2 examples/sec; 0.084 sec/batch)
2017-05-06 21:51:33.031090: step 38940, loss = 0.84 (1576.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:51:33.856219: step 38950, loss = 0.68 (1551.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:51:34.677632: step 38960, loss = 0.76 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:51:35.489706: step 38970, loss = 0.75 (1576.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:51:36.319704: step 38980, loss = 0.83 (1542.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:51:37.141671: step 38990, loss = 0.71 (1557.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:51:38.078376: step 39000, loss = 0.78 (1366.5 examples/sec; 0.094 sec/batch)
2017-05-06 21:51:38.786450: step 39010, loss = 0.75 (1807.7 examples/sec; 0.071 sec/batch)
2017-05-06 21:51:39.603678: step 39020, loss = 0.62 (1566.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:51:40.422185: step 39030, loss = 0.70 (1563.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:51:41.245479: step 39040, loss = 0.66 (1554.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:51:42.065632: step 39050, loss = 0.84 (1560.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:51:42.882657: step 39060, loss = 0.73 (1566.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:51:43.699722: step 39070, loss = 0.62 (1566.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:51:44.525920: step 39080, loss = 0.88 (1549.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:51:45.351737: step 39090, loss = 0.86 (1550.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:51:46.273848: step 39100, loss = 0.87 (1388.1 examples/sec; 0.092 sec/batch)
2017-05-06 21:51:46.998797: step 39110, loss = 0.80 (1765.6 examples/sec; 0.072 sec/batch)
2017-05-06 21:51:47.811715: step 39120, loss = 0.63 (1574.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:51:48.634933: step 39130, loss = 0.85 (1554.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:51:49.460263: step 39140, loss = 0.81 (1550.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:51:50.276934: step 39150, loss = 0.70 (1567.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:51:51.094878: step 39160, loss = 0.94 (1564.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:51:51.909505: step 39170, loss = 0.72 (1571.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:51:52.736918: step 39180, loss = 0.75 (1547.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:51:53.558467: step 39190, loss = 0.84 (1558.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:51:54.487673: step 39200, loss = 0.82 (1377.5 examples/sec; 0.093 sec/batch)
2017-05-06 21:51:55.211791: step 39210, loss = 0.65 (1767.7 examples/sec; 0.072 sec/batch)
2017-05-06 21:51:56.034651: step 39220, loss = 0.78 (1555.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:51:56.865218: step 39230, loss = 0.65 (1541.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:51:57.689846: step 39240, loss = 0.72 (1552.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:51:58.519946: step 39250, loss = 1.00 (1542.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:51:59.340848: step 39260, loss = 0.67 (1559.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:00.169867: step 39270, loss = 0.86 (1544.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:52:00.995766: step 39280, loss = 0.82 (1549.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:52:01.815512: step 39290, loss = 0.73 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:02.742894: step 39300, loss = 0.68 (1380.2 examples/sec; 0.093 sec/batch)
2017-05-06 21:52:03.465317: step 39310, loss = 0.73 (1771.8 examples/sec; 0.072 sec/batch)
2017-05-06 21:52:04.290382: step 39320, loss = 0.81 (1551.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:52:05.113279: step 39330, loss = 0.70 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:05.937018: step 39340, loss = 0.82 (1553.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:06.768303: step 39350, loss = 0.76 (1539.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:52:07.585039: step 39360, loss = 0.82 (1567.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:08.403982: step 39370, loss = 0.66 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:09.225105: step 39380, loss = 0.75 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:10.044024: step 39390, loss = 0.88 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:10.966985: step 39400, loss = 0.83 (1386.8 examples/sec; 0.092 sec/batch)
2017-05-06 21:52:11.693684: step 39410, loss = 0.63 (1761.4 examples/sec; 0.073 sec/batch)
2017-05-06 21:52:12.517055: step 39420, loss = 0.78 (1554.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:13.342574: step 39430, loss = 0.84 (1550.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:52:14.173208: step 39440, loss = 0.72 (1541.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:52:15.001329: step 39450, loss = 0.69 (1545.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:52:15.814760: step 39460, loss = 0.80 (1573.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:52:16.640194: step 39470, loss = 0.68 (1550.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:52:17.456521: step 39480, loss = 0.82 (1568.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:18.279665: step 39490, loss = 0.79 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:19.196974: step 39500, loss = 0.61 (1395.4 examples/sec; 0.092 sec/batch)
2017-05-06 21:52:19.917016: step 39510, loss = 0.65 (1777.7 examples/sec; 0.072 sec/batch)
2017-05-06 21:52:20.735230: step 39520, loss = 0.77 (1564.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:21.557209: step 39530, loss = 0.90 (1557.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:22.376878: step 39540, loss = 0.72 (1561.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:23.194379: step 39550, loss = 0.74 (1565.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:24.011628: step 39560, loss = 0.73 (1566.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:24.826258: step 39570, loss = 0.67 (1571.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:52:25.642884: step 39580, loss = 0.87 (1567.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:26.457681: step 39590, loss = 0.74 (1570.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:52:27.401459: step 39600, loss = 0.88 (1356.3 examples/sec; 0.094 sec/batch)
2017-05-06 21:52:28.087304: step 39610, loss = 0.80 (1866.3 examples/sec; 0.069 sec/batch)
2017-05-06 21:52:28.903099: step 39620, loss = 0.71 (1569.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:29.727527: step 39630, loss = 0.72 (1552.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:30.557307: step 39640, loss = 0.90 (1542.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:52:31.368608: step 39650, loss = 0.73 (1577.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:52:32.178701: step 39660, loss = 0.89 (1580.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:52:32.994408: step 39670, loss = 0.89 (1569.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:33.814539: step 39680, loss = 0.85 (1560.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:34.635499: step 39690, loss = 0.74 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:35.555673: step 39700, loss = 0.77 (1391.0 examples/sec; 0.092 sec/batch)
2017-05-06 21:52:36.283157: step 39710, loss = 0.75 (1759.5 examples/sec; 0.073 sec/batch)
2017-05-06 21:52:37.104839: step 39720, loss = 0.75 (1557.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:37.920456: step 39730, loss = 0.69 (1569.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:38.743185: step 39740, loss = 0.71 (1555.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:39.560631: step 39750, loss = 0.77 (1565.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:40.377451: step 39760, loss = 0.72 (1567.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:41.205295: step 39770, loss = 0.81 (1546.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:52:42.028741: step 39780, loss = 0.66 (1554.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:42.850922: step 39790, loss = 0.75 (1556.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:43.768827: step 39800, loss = 0.74 (1394.5 examples/sec; 0.092 sec/batch)
2017-05-06 21:52:44.497574: step 39810, loss = 0.68 (1756.4 examples/sec; 0.073 sec/batch)
2017-05-06 21:52:45.315442: step 39820, loss = 0.74 (1565.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:46.145795: step 39830, loss = 0.74 (1541.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:52:46.972177: step 39840, loss = 0.79 (1548.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:52:47.788952: step 39850, loss = 0.82 (1567.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:48.612542: step 39860, loss = 0.66 (1554.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:49.438497: step 39870, loss = 0.84 (1549.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:52:50.261133: step 39880, loss = 0.82 (1556.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:51.081528: step 39890, loss = 0.72 (1560.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:51.994519: step 39900, loss = 0.63 (1402.0 examples/sec; 0.091 sec/batch)
2017-05-06 21:52:52.729034: step 39910, loss = 0.64 (1742.7 examples/sec; 0.073 sec/batch)
2017-05-06 21:52:53.548696: step 39920, loss = 0.85 (1561.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:54.368425: step 39930, loss = 0.71 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:55.199544: step 39940, loss = 0.86 (1540.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:52:56.014492: step 39950, loss = 0.70 (1570.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:52:56.836339: step 39960, loss = 0.68 (1557.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:57.650070: step 39970, loss = 0.73 (1573.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:52:58.469414: step 39980, loss = 0.71 (1562.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:52:59.296079: step 39990, loss = 0.74 (1548.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:53:00.206464: step 40000, loss = 0.68 (1406.0 examples/sec; 0.091 sec/batch)
2017-05-06 21:53:00.939248: step 40010, loss = 0.65 (1746.8 examples/sec; 0.073 sec/batch)
2017-05-06 21:53:01.756010: step 40020, loss = 0.73 (1567.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:53:02.575338: step 40030, loss = 0.78 (1562.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:53:03.394119: step 40040, loss = 0.67 (1563.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:53:04.214209: step 40050, loss = 0.85 (1560.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:53:05.035165: step 40060, loss = 0.66 (1559.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:53:05.859056: step 40070, loss = 0.74 (1553.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:53:06.680255: step 40080, loss = 0.96 (1558.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:53:07.494367: step 40090, loss = 0.73 (1572.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:53:08.406459: step 40100, loss = 0.93 (1403.4 examples/sec; 0.091 sec/batch)
2017-05-06 21:53:09.124465: step 40110, loss = 0.83 (1782.7 examples/sec; 0.072 sec/batch)
2017-05-06 21:53:09.949203: step 40120, loss = 0.75 (1552.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:53:10.771827: step 40130, loss = 0.82 (1556.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:53:11.585198: step 40140, loss = 0.72 (1573.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:53:12.414582: step 40150, loss = 0.67 (1543.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:53:13.234818: step 40160, loss = 0.84 (1560.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:53:14.074458: step 40170, loss = 0.85 (1524.5 examples/sec; 0.084 sec/batch)
2017-05-06 21:53:14.904601: step 40180, loss = 0.82 (1541.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:53:15.714492: step 40190, loss = 0.72 (1580.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:53:16.657388: step 40200, loss = 0.72 (1357.5 examples/sec; 0.094 sec/batch)
2017-05-06 21:53:17.369847: step 40210, loss = 0.66 (1796.6 examples/sec; 0.071 sec/batch)
2017-05-06 21:53:18.191644: step 40220, loss = 0.65 (1557.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:53:19.022773: step 40230, loss = 0.86 (1540.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:53:19.833000: step 40240, loss = 0.76 (1579.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:53:20.657228: step 40250, loss = 0.83 (1553.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:53:21.483399: step 40260, loss = 0.73 (1549.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:53:22.310079: step 40270, loss = 0.78 (1548.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:53:23.129330: step 40280, loss = 0.75 (1562.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:53:23.939810: step 40290, loss = 0.75 (1579.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:53:24.864924: step 40300, loss = 0.69 (1383.6 examples/sec; 0.093 sec/batch)
2017-05-06 21:53:25.590691: step 40310, loss = 0.84 (1763.6 examples/sec; 0.073 sec/batch)
2017-05-06 21:53:26.420749: step 40320, loss = 0.67 (1542.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:53:27.241037: step 40330, loss = 0.67 (1560.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:53:28.059403: step 40340, loss = 0.73 (1564.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:53:28.881804: step 40350, loss = 0.77 (1556.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:53:29.703666: step 40360, loss = 0.73 (1557.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:53:30.529358: step 40370, loss = 0.98 (1550.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:53:31.347306: step 40380, loss = 0.69 (1564.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:53:32.162440: step 40390, loss = 0.75 (1570.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:53:33.087788: step 40400, loss = 0.80 (1383.3 examples/sec; 0.093 sec/batch)
2017-05-06 21:53:33.813345: step 40410, loss = 0.68 (1764.2 examples/sec; 0.073 sec/batch)
2017-05-06 21:53:34.629410: step 40420, loss = 0.80 (1568.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:53:35.452567: step 40430, loss = 0.65 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:53:36.272407: step 40440, loss = 0.85 (1561.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:53:37.083340: step 40450, loss = 0.72 (1578.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:53:37.904097: step 40460, loss = 0.61 (1559.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:53:38.732207: step 40470, loss = 0.84 (1545.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:53:39.545452: step 40480, loss = 0.69 (1573.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:53:40.356447: step 40490, loss = 0.63 (1578.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:53:41.272406: step 40500, loss = 0.81 (1397.4 examples/sec; 0.092 sec/batch)
2017-05-06 21:53:41.990200: step 40510, loss = 0.80 (1783.2 examples/sec; 0.072 sec/batch)
2017-05-06 21:53:42.816990: step 40520, loss = 0.78 (1548.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:53:43.628541: step 40530, loss = 0.72 (1577.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:53:44.447505: step 40540, loss = 0.97 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:53:45.280954: step 40550, loss = 0.73 (1535.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:53:46.109906: step 40560, loss = 0.83 (1544.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:53:46.925855: step 40570, loss = 0.75 (1568.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:53:47.737957: step 40580, loss = 0.83 (1576.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:53:48.586482: step 40590, loss = 0.84 (1508.5 examples/sec; 0.085 sec/batch)
2017-05-06 21:53:49.483341: step 40600, loss = 0.84 (1427.2 examples/sec; 0.090 sec/batch)
2017-05-06 21:53:50.198356: step 40610, loss = 0.69 (1790.2 examples/sec; 0.072 sec/batch)
2017-05-06 21:53:51.016375: step 40620, loss = 0.79 (1564.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:53:51.830204: step 40630, loss = 0.76 (1572.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:53:52.646059: step 40640, loss = 0.85 (1568.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:53:53.465179: step 40650, loss = 0.68 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:53:54.285852: step 40660, loss = 0.81 (1559.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:53:55.112029: step 40670, loss = 0.89 (1549.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:53:55.929565: step 40680, loss = 0.68 (1565.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:53:56.760386: step 40690, loss = 0.78 (1540.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:53:57.686675: step 40700, loss = 0.75 (1381.9 examples/sec; 0.093 sec/batch)
2017-05-06 21:53:58.404518: step 40710, loss = 0.71 (1783.1 examples/sec; 0.072 sec/batch)
2017-05-06 21:53:59.236180: step 40720, loss = 0.80 (1539.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:54:00.043001: step 40730, loss = 0.87 (1586.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:54:00.865897: step 40740, loss = 0.90 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:01.689369: step 40750, loss = 0.91 (1554.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:02.505113: step 40760, loss = 0.76 (1569.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:03.326189: step 40770, loss = 0.83 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:04.139554: step 40780, loss = 0.79 (1573.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:54:04.967784: step 40790, loss = 0.66 (1545.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:54:05.895114: step 40800, loss = 0.84 (1380.3 examples/sec; 0.093 sec/batch)
2017-05-06 21:54:06.607685: step 40810, loss = 0.76 (1796.3 examples/sec; 0.071 sec/batch)
2017-05-06 21:54:07.433921: step 40820, loss = 0.72 (1549.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:54:08.251992: step 40830, loss = 0.69 (1564.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:09.076415: step 40840, loss = 0.74 (1552.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:09.897237: step 40850, loss = 0.70 (1559.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:10.720179: step 40860, loss = 0.72 (1555.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:11.533520: step 40870, loss = 0.82 (1573.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:54:12.349372: step 40880, loss = 0.79 (1568.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:13.174612: step 40890, loss = 0.64 (1551.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:54:14.103597: step 40900, loss = 0.85 (1377.8 examples/sec; 0.093 sec/batch)
2017-05-06 21:54:14.819939: step 40910, loss = 0.78 (1786.9 examples/sec; 0.072 sec/batch)
2017-05-06 21:54:15.637515: step 40920, loss = 0.78 (1565.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:16.455850: step 40930, loss = 0.74 (1564.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:17.282855: step 40940, loss = 0.75 (1547.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:54:18.102775: step 40950, loss = 0.71 (1561.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:18.929251: step 40960, loss = 0.81 (1548.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:54:19.740703: step 40970, loss = 0.82 (1577.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:54:20.560561: step 40980, loss = 0.70 (1561.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:21.380359: step 40990, loss = 0.78 (1561.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:22.303241: step 41000, loss = 0.73 (1387.0 examples/sec; 0.092 sec/batch)
2017-05-06 21:54:23.031745: step 41010, loss = 0.91 (1757.0 examples/sec; 0.073 sec/batch)
2017-05-06 21:54:23.854727: step 41020, loss = 0.77 (1555.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:24.675564: step 41030, loss = 0.69 (1559.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:25.500588: step 41040, loss = 0.76 (1551.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:54:26.328498: step 41050, loss = 0.75 (1546.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:54:27.147511: step 41060, loss = 0.87 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:27.957131: step 41070, loss = 0.79 (1581.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:54:28.778730: step 41080, loss = 0.83 (1558.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:29.598995: step 41090, loss = 0.72 (1560.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:30.533706: step 41100, loss = 0.71 (1369.4 examples/sec; 0.093 sec/batch)
2017-05-06 21:54:31.246429: step 41110, loss = 0.85 (1795.9 examples/sec; 0.071 sec/batch)
2017-05-06 21:54:32.061853: step 41120, loss = 0.61 (1569.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:32.881471: step 41130, loss = 0.69 (1561.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:33.702983: step 41140, loss = 0.81 (1558.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:34.513242: step 41150, loss = 0.67 (1579.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:54:35.337304: step 41160, loss = 0.81 (1553.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:36.158697: step 41170, loss = 0.78 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:36.979176: step 41180, loss = 0.72 (1560.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:37.808776: step 41190, loss = 0.74 (1542.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:54:38.727927: step 41200, loss = 0.80 (1392.6 examples/sec; 0.092 sec/batch)
2017-05-06 21:54:39.439305: step 41210, loss = 0.80 (1799.3 examples/sec; 0.071 sec/batch)
2017-05-06 21:54:40.253816: step 41220, loss = 0.65 (1571.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:54:41.071942: step 41230, loss = 0.66 (1564.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:41.896213: step 41240, loss = 0.82 (1552.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:42.719557: step 41250, loss = 0.77 (1554.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:43.528841: step 41260, loss = 0.93 (1581.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:54:44.351865: step 41270, loss = 0.90 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:45.169283: step 41280, loss = 0.72 (1565.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:45.994306: step 41290, loss = 0.73 (1551.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:54:46.914951: step 41300, loss = 0.99 (1390.3 examples/sec; 0.092 sec/batch)
2017-05-06 21:54:47.627830: step 41310, loss = 0.62 (1795.5 examples/sec; 0.071 sec/batch)
2017-05-06 21:54:48.451696: step 41320, loss = 0.72 (1553.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:49.270266: step 41330, loss = 0.83 (1563.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:50.096599: step 41340, loss = 0.74 (1549.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:54:50.920776: step 41350, loss = 0.93 (1553.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:51.725445: step 41360, loss = 0.73 (1590.7 examples/sec; 0.080 sec/batch)
2017-05-06 21:54:52.553855: step 41370, loss = 0.78 (1545.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:54:53.377243: step 41380, loss = 0.71 (1554.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:54.201911: step 41390, loss = 0.68 (1552.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:55.130812: step 41400, loss = 0.70 (1378.0 examples/sec; 0.093 sec/batch)
2017-05-06 21:54:55.840363: step 41410, loss = 0.64 (1804.0 examples/sec; 0.071 sec/batch)
2017-05-06 21:54:56.659810: step 41420, loss = 0.71 (1562.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:57.486393: step 41430, loss = 0.61 (1548.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:54:58.301772: step 41440, loss = 0.64 (1569.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:59.121748: step 41450, loss = 0.71 (1561.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:54:59.937514: step 41460, loss = 0.75 (1569.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:00.757870: step 41470, loss = 0.76 (1560.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:01.567851: step 41480, loss = 0.78 (1580.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:55:02.413553: step 41490, loss = 0.68 (1513.5 examples/sec; 0.085 sec/batch)
2017-05-06 21:55:03.338551: step 41500, loss = 0.82 (1383.8 examples/sec; 0.092 sec/batch)
2017-05-06 21:55:04.061486: step 41510, loss = 0.59 (1770.6 examples/sec; 0.072 sec/batch)
2017-05-06 21:55:04.882286: step 41520, loss = 0.72 (1559.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:05.703365: step 41530, loss = 0.74 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:06.527898: step 41540, loss = 0.89 (1552.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:07.350434: step 41550, loss = 0.67 (1556.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:08.164312: step 41560, loss = 0.71 (1572.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:55:08.977785: step 41570, loss = 0.62 (1573.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:55:09.828072: step 41580, loss = 0.64 (1505.4 examples/sec; 0.085 sec/batch)
2017-05-06 21:55:10.619953: step 41590, loss = 0.68 (1616.3 examples/sec; 0.079 sec/batch)
2017-05-06 21:55:11.541148: step 41600, loss = 0.96 (1389.5 examples/sec; 0.092 sec/batch)
2017-05-06 21:55:12.250527: step 41610, loss = 0.70 (1804.4 examples/sec; 0.071 sec/batch)
2017-05-06 21:55:13.071499: step 41620, loss = 0.89 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:13.888011: step 41630, loss = 0.73 (1567.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:14.712514: step 41640, loss = 0.82 (1552.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:15.530940: step 41650, loss = 0.64 (1564.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:16.349281: step 41660, loss = 0.76 (1564.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:17.159095: step 41670, loss = 0.81 (1580.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:55:17.993318: step 41680, loss = 0.78 (1534.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:55:18.814399: step 41690, loss = 0.73 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:19.735015: step 41700, loss = 0.66 (1390.4 examples/sec; 0.092 sec/batch)
2017-05-06 21:55:20.458154: step 41710, loss = 0.73 (1770.1 examples/sec; 0.072 sec/batch)
2017-05-06 21:55:21.285628: step 41720, loss = 0.77 (1546.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:55:22.116688: step 41730, loss = 0.78 (1540.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:55:22.943513: step 41740, loss = 0.73 (1548.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:55:23.758488: step 41750, loss = 0.72 (1570.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:55:24.582709: step 41760, loss = 0.71 (1553.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:25.402414: step 41770, loss = 0.87 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:26.228088: step 41780, loss = 0.69 (1550.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:55:27.047082: step 41790, loss = 0.74 (1562.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:27.961039: step 41800, loss = 0.80 (1400.5 examples/sec; 0.091 sec/batch)
2017-05-06 21:55:28.682494: step 41810, loss = 0.79 (1774.2 examples/sec; 0.072 sec/batch)
2017-05-06 21:55:29.499248: step 41820, loss = 0.83 (1567.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:30.320647: step 41830, loss = 0.80 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:31.152004: step 41840, loss = 0.74 (1539.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:55:31.963034: step 41850, loss = 0.92 (1578.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:55:32.785107: step 41860, loss = 0.73 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:33.607372: step 41870, loss = 0.72 (1556.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:34.435134: step 41880, loss = 0.85 (1546.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:55:35.256455: step 41890, loss = 0.84 (1558.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:36.171534: step 41900, loss = 0.86 (1398.8 examples/sec; 0.092 sec/batch)
2017-05-06 21:55:36.905517: step 41910, loss = 0.84 (1743.9 examples/sec; 0.073 sec/batch)
2017-05-06 21:55:37.733065: step 41920, loss = 0.82 (1546.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:55:38.555343: step 41930, loss = 0.82 (1556.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:39.371981: step 41940, loss = 0.79 (1567.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:40.189091: step 41950, loss = 0.85 (1566.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:41.000821: step 41960, loss = 0.89 (1576.9 examples/sec; 0.081 sec/batch)
2017-05-06 21:55:41.818463: step 41970, loss = 0.81 (1565.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:42.645252: step 41980, loss = 0.66 (1548.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:55:43.462625: step 41990, loss = 0.69 (1566.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:44.378918: step 42000, loss = 0.77 (1396.9 examples/sec; 0.092 sec/batch)
2017-05-06 21:55:45.107360: step 42010, loss = 0.75 (1757.2 examples/sec; 0.073 sec/batch)
2017-05-06 21:55:45.924293: step 42020, loss = 0.73 (1566.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:46.743388: step 42030, loss = 0.73 (1562.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:47.563475: step 42040, loss = 0.79 (1560.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:48.382986: step 42050, loss = 0.84 (1561.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:49.214344: step 42060, loss = 0.87 (1539.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:55:50.034676: step 42070, loss = 0.71 (1560.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:50.869967: step 42080, loss = 0.94 (1532.4 examples/sec; 0.084 sec/batch)
2017-05-06 21:55:51.682983: step 42090, loss = 0.63 (1574.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:55:52.626015: step 42100, loss = 0.83 (1357.3 examples/sec; 0.094 sec/batch)
2017-05-06 21:55:53.323435: step 42110, loss = 0.66 (1835.3 examples/sec; 0.070 sec/batch)
2017-05-06 21:55:54.140385: step 42120, loss = 0.73 (1566.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:54.964520: step 42130, loss = 0.82 (1553.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:55.783223: step 42140, loss = 0.83 (1563.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:56.604233: step 42150, loss = 0.75 (1559.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:57.420709: step 42160, loss = 0.84 (1567.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:58.239840: step 42170, loss = 0.80 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:59.058732: step 42180, loss = 0.72 (1563.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:55:59.870144: step 42190, loss = 0.81 (1577.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:56:00.785590: step 42200, loss = 0.79 (1398.2 examples/sec; 0.092 sec/batch)
2017-05-06 21:56:01.504259: step 42210, loss = 0.80 (1781.1 examples/sec; 0.072 sec/batch)
2017-05-06 21:56:02.315212: step 42220, loss = 0.72 (1578.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:56:03.138459: step 42230, loss = 0.76 (1554.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:03.953684: step 42240, loss = 0.83 (1570.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:04.776636: step 42250, loss = 0.64 (1555.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:05.594242: step 42260, loss = 0.72 (1565.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:06.417639: step 42270, loss = 0.70 (1554.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:07.238809: step 42280, loss = 0.83 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:08.055059: step 42290, loss = 0.80 (1568.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:08.982328: step 42300, loss = 0.80 (1380.4 examples/sec; 0.093 sec/batch)
2017-05-06 21:56:09.708687: step 42310, loss = 0.65 (1762.2 examples/sec; 0.073 sec/batch)
2017-05-06 21:56:10.530102: step 42320, loss = 0.62 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:11.347632: step 42330, loss = 0.72 (1565.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:12.163909: step 42340, loss = 0.75 (1568.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:12.978985: step 42350, loss = 1.01 (1570.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:13.803491: step 42360, loss = 0.95 (1552.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:14.632932: step 42370, loss = 0.76 (1543.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:56:15.455405: step 42380, loss = 0.75 (1556.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:16.274652: step 42390, loss = 0.74 (1562.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:17.195521: step 42400, loss = 0.75 (1390.0 examples/sec; 0.092 sec/batch)
2017-05-06 21:56:17.915837: step 42410, loss = 0.83 (1777.0 examples/sec; 0.072 sec/batch)
2017-05-06 21:56:18.732262: step 42420, loss = 0.90 (1567.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:19.538610: step 42430, loss = 0.77 (1587.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:56:20.361813: step 42440, loss = 0.72 (1554.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:21.188418: step 42450, loss = 0.89 (1548.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:56:22.003939: step 42460, loss = 0.87 (1569.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:22.830394: step 42470, loss = 0.73 (1548.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:56:23.644362: step 42480, loss = 0.69 (1572.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:56:24.461193: step 42490, loss = 0.76 (1567.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:25.379986: step 42500, loss = 0.70 (1393.1 examples/sec; 0.092 sec/batch)
2017-05-06 21:56:26.101236: step 42510, loss = 0.72 (1774.7 examples/sec; 0.072 sec/batch)
2017-05-06 21:56:26.918011: step 42520, loss = 0.76 (1567.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:27.734400: step 42530, loss = 0.73 (1567.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:28.556164: step 42540, loss = 0.87 (1557.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:29.374957: step 42550, loss = 0.76 (1563.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:30.215720: step 42560, loss = 0.83 (1522.4 examples/sec; 0.084 sec/batch)
2017-05-06 21:56:31.061295: step 42570, loss = 0.74 (1513.8 examples/sec; 0.085 sec/batch)
2017-05-06 21:56:31.859926: step 42580, loss = 0.84 (1602.7 examples/sec; 0.080 sec/batch)
2017-05-06 21:56:32.689567: step 42590, loss = 0.82 (1542.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:56:33.604455: step 42600, loss = 0.74 (1399.1 examples/sec; 0.091 sec/batch)
2017-05-06 21:56:34.328934: step 42610, loss = 0.69 (1766.8 examples/sec; 0.072 sec/batch)
2017-05-06 21:56:35.149493: step 42620, loss = 0.76 (1559.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:35.965770: step 42630, loss = 0.78 (1568.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:36.786686: step 42640, loss = 0.81 (1559.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:37.610731: step 42650, loss = 0.68 (1553.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:38.441321: step 42660, loss = 0.73 (1541.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:56:39.262111: step 42670, loss = 0.81 (1559.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:40.082776: step 42680, loss = 0.99 (1559.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:40.901403: step 42690, loss = 0.87 (1563.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:41.820977: step 42700, loss = 0.82 (1392.0 examples/sec; 0.092 sec/batch)
2017-05-06 21:56:42.547118: step 42710, loss = 0.77 (1762.7 examples/sec; 0.073 sec/batch)
2017-05-06 21:56:43.369172: step 42720, loss = 0.80 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:44.184068: step 42730, loss = 0.80 (1570.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:56:44.998445: step 42740, loss = 0.91 (1571.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:56:45.813415: step 42750, loss = 0.78 (1570.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:56:46.632342: step 42760, loss = 0.72 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:47.455094: step 42770, loss = 0.79 (1555.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:48.274512: step 42780, loss = 0.79 (1562.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:49.086897: step 42790, loss = 0.80 (1575.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:56:50.016561: step 42800, loss = 0.78 (1376.8 examples/sec; 0.093 sec/batch)
2017-05-06 21:56:50.738490: step 42810, loss = 0.79 (1773.0 examples/sec; 0.072 sec/batch)
2017-05-06 21:56:51.566657: step 42820, loss = 0.80 (1545.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:56:52.378802: step 42830, loss = 0.83 (1576.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:56:53.198495: step 42840, loss = 0.64 (1561.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:54.012358: step 42850, loss = 0.71 (1572.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:56:54.836194: step 42860, loss = 0.93 (1553.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:55.657666: step 42870, loss = 0.87 (1558.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:56.478036: step 42880, loss = 0.95 (1560.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:56:57.306703: step 42890, loss = 0.76 (1544.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:56:58.222034: step 42900, loss = 0.72 (1398.4 examples/sec; 0.092 sec/batch)
2017-05-06 21:56:58.945794: step 42910, loss = 0.67 (1768.6 examples/sec; 0.072 sec/batch)
2017-05-06 21:56:59.770933: step 42920, loss = 0.81 (1551.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:57:00.583798: step 42930, loss = 0.79 (1574.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:57:01.416123: step 42940, loss = 0.82 (1537.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:57:02.232984: step 42950, loss = 0.82 (1567.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:03.057583: step 42960, loss = 0.87 (1552.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:03.878987: step 42970, loss = 0.80 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:04.704082: step 42980, loss = 0.72 (1551.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:57:05.522237: step 42990, loss = 0.82 (1564.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:06.443192: step 43000, loss = 0.65 (1389.9 examples/sec; 0.092 sec/batch)
2017-05-06 21:57:07.174912: step 43010, loss = 0.85 (1749.3 examples/sec; 0.073 sec/batch)
2017-05-06 21:57:07.980895: step 43020, loss = 0.78 (1588.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:57:08.807066: step 43030, loss = 0.67 (1549.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:57:09.628349: step 43040, loss = 0.79 (1558.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:10.459243: step 43050, loss = 0.75 (1540.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:57:11.275392: step 43060, loss = 0.85 (1568.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:12.087225: step 43070, loss = 0.81 (1576.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:57:12.895974: step 43080, loss = 0.79 (1582.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:57:13.712209: step 43090, loss = 0.76 (1568.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:14.630081: step 43100, loss = 0.78 (1394.5 examples/sec; 0.092 sec/batch)
2017-05-06 21:57:15.355958: step 43110, loss = 0.95 (1763.4 examples/sec; 0.073 sec/batch)
2017-05-06 21:57:16.173648: step 43120, loss = 0.79 (1565.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:16.989377: step 43130, loss = 0.67 (1569.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:17.815662: step 43140, loss = 0.73 (1549.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:57:18.644522: step 43150, loss = 0.70 (1544.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:57:19.468329: step 43160, loss = 0.79 (1553.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:20.287603: step 43170, loss = 0.82 (1562.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:21.109310: step 43180, loss = 0.79 (1557.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:21.940928: step 43190, loss = 0.81 (1539.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:57:22.868100: step 43200, loss = 0.83 (1380.5 examples/sec; 0.093 sec/batch)
2017-05-06 21:57:23.576395: step 43210, loss = 0.70 (1807.2 examples/sec; 0.071 sec/batch)
2017-05-06 21:57:24.396252: step 43220, loss = 0.97 (1561.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:25.216338: step 43230, loss = 0.89 (1560.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:26.036392: step 43240, loss = 0.80 (1560.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:26.866632: step 43250, loss = 0.68 (1541.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:57:27.674301: step 43260, loss = 0.81 (1584.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:57:28.501271: step 43270, loss = 0.68 (1547.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:57:29.318988: step 43280, loss = 0.71 (1565.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:30.135270: step 43290, loss = 0.75 (1568.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:31.054677: step 43300, loss = 0.68 (1392.2 examples/sec; 0.092 sec/batch)
2017-05-06 21:57:31.763818: step 43310, loss = 0.65 (1805.0 examples/sec; 0.071 sec/batch)
2017-05-06 21:57:32.574341: step 43320, loss = 0.68 (1579.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:57:33.390674: step 43330, loss = 0.78 (1568.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:34.239358: step 43340, loss = 0.95 (1508.2 examples/sec; 0.085 sec/batch)
2017-05-06 21:57:35.030659: step 43350, loss = 0.66 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-06 21:57:35.837611: step 43360, loss = 0.75 (1586.2 examples/sec; 0.081 sec/batch)
2017-05-06 21:57:36.663897: step 43370, loss = 0.79 (1549.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:57:37.487093: step 43380, loss = 0.73 (1554.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:38.313102: step 43390, loss = 0.82 (1549.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:57:39.234397: step 43400, loss = 0.82 (1389.3 examples/sec; 0.092 sec/batch)
2017-05-06 21:57:39.953889: step 43410, loss = 0.61 (1779.0 examples/sec; 0.072 sec/batch)
2017-05-06 21:57:40.770680: step 43420, loss = 0.84 (1567.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:41.589102: step 43430, loss = 0.80 (1564.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:42.407064: step 43440, loss = 0.74 (1564.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:43.225185: step 43450, loss = 0.71 (1564.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:44.033909: step 43460, loss = 0.73 (1582.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:57:44.854323: step 43470, loss = 0.67 (1560.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:45.676354: step 43480, loss = 0.67 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:46.500628: step 43490, loss = 0.76 (1552.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:47.416906: step 43500, loss = 0.73 (1397.0 examples/sec; 0.092 sec/batch)
2017-05-06 21:57:48.133708: step 43510, loss = 0.71 (1785.7 examples/sec; 0.072 sec/batch)
2017-05-06 21:57:48.949159: step 43520, loss = 0.79 (1569.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:49.767515: step 43530, loss = 0.76 (1564.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:50.582795: step 43540, loss = 0.75 (1570.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:51.401442: step 43550, loss = 0.75 (1563.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:52.245289: step 43560, loss = 0.59 (1516.9 examples/sec; 0.084 sec/batch)
2017-05-06 21:57:53.038214: step 43570, loss = 0.65 (1614.2 examples/sec; 0.079 sec/batch)
2017-05-06 21:57:53.860990: step 43580, loss = 0.64 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:54.681572: step 43590, loss = 0.75 (1559.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:55.602164: step 43600, loss = 0.84 (1390.4 examples/sec; 0.092 sec/batch)
2017-05-06 21:57:56.320758: step 43610, loss = 0.74 (1781.3 examples/sec; 0.072 sec/batch)
2017-05-06 21:57:57.143321: step 43620, loss = 0.76 (1556.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:57.963224: step 43630, loss = 0.83 (1561.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:57:58.778018: step 43640, loss = 0.79 (1571.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:57:59.592381: step 43650, loss = 0.73 (1571.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:58:00.414504: step 43660, loss = 0.73 (1556.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:01.231954: step 43670, loss = 0.67 (1565.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:02.075999: step 43680, loss = 0.83 (1516.5 examples/sec; 0.084 sec/batch)
2017-05-06 21:58:02.901021: step 43690, loss = 0.71 (1551.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:58:03.813169: step 43700, loss = 1.05 (1403.3 examples/sec; 0.091 sec/batch)
2017-05-06 21:58:04.531866: step 43710, loss = 1.04 (1781.0 examples/sec; 0.072 sec/batch)
2017-05-06 21:58:05.351659: step 43720, loss = 0.75 (1561.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:06.171599: step 43730, loss = 0.91 (1561.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:06.994169: step 43740, loss = 0.74 (1556.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:07.804816: step 43750, loss = 0.60 (1579.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:58:08.635882: step 43760, loss = 0.73 (1540.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:58:09.446649: step 43770, loss = 0.82 (1578.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:58:10.264816: step 43780, loss = 0.83 (1564.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:11.076998: step 43790, loss = 0.70 (1576.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:58:11.992863: step 43800, loss = 0.76 (1397.6 examples/sec; 0.092 sec/batch)
2017-05-06 21:58:12.719122: step 43810, loss = 0.74 (1762.5 examples/sec; 0.073 sec/batch)
2017-05-06 21:58:13.564762: step 43820, loss = 0.67 (1513.6 examples/sec; 0.085 sec/batch)
2017-05-06 21:58:14.380933: step 43830, loss = 0.65 (1568.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:15.206290: step 43840, loss = 0.62 (1550.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:58:16.027061: step 43850, loss = 0.73 (1559.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:16.851484: step 43860, loss = 0.90 (1552.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:17.668623: step 43870, loss = 0.61 (1566.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:18.493827: step 43880, loss = 0.73 (1551.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:58:19.325595: step 43890, loss = 0.96 (1538.9 examples/sec; 0.083 sec/batch)
2017-05-06 21:58:20.249419: step 43900, loss = 0.87 (1385.5 examples/sec; 0.092 sec/batch)
2017-05-06 21:58:20.981638: step 43910, loss = 0.74 (1748.1 examples/sec; 0.073 sec/batch)
2017-05-06 21:58:21.801643: step 43920, loss = 0.84 (1560.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:22.628791: step 43930, loss = 0.71 (1547.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:58:23.441981: step 43940, loss = 0.79 (1574.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:58:24.263204: step 43950, loss = 0.89 (1558.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:25.086831: step 43960, loss = 0.61 (1554.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:25.904754: step 43970, loss = 0.84 (1564.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:26.726101: step 43980, loss = 0.80 (1558.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:27.544338: step 43990, loss = 0.92 (1564.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:28.457791: step 44000, loss = 0.86 (1401.3 examples/sec; 0.091 sec/batch)
2017-05-06 21:58:29.188156: step 44010, loss = 0.94 (1752.5 examples/sec; 0.073 sec/batch)
2017-05-06 21:58:30.006889: step 44020, loss = 0.75 (1563.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:30.834834: step 44030, loss = 0.74 (1546.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:58:31.649760: step 44040, loss = 0.88 (1570.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:58:32.468551: step 44050, loss = 0.70 (1563.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:33.287685: step 44060, loss = 0.74 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:34.107325: step 44070, loss = 0.77 (1561.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:34.929243: step 44080, loss = 0.87 (1557.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:35.743844: step 44090, loss = 0.74 (1571.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:58:36.678247: step 44100, loss = 0.69 (1369.9 examples/sec; 0.093 sec/batch)
2017-05-06 21:58:37.398088: step 44110, loss = 0.68 (1778.2 examples/sec; 0.072 sec/batch)
2017-05-06 21:58:38.216822: step 44120, loss = 0.82 (1563.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:39.038850: step 44130, loss = 0.86 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:39.850200: step 44140, loss = 0.65 (1577.6 examples/sec; 0.081 sec/batch)
2017-05-06 21:58:40.669403: step 44150, loss = 0.82 (1562.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:41.500766: step 44160, loss = 0.60 (1539.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:58:42.320111: step 44170, loss = 0.69 (1562.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:43.136423: step 44180, loss = 0.99 (1568.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:43.961662: step 44190, loss = 0.90 (1551.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:58:44.879794: step 44200, loss = 0.71 (1394.1 examples/sec; 0.092 sec/batch)
2017-05-06 21:58:45.609374: step 44210, loss = 0.71 (1754.4 examples/sec; 0.073 sec/batch)
2017-05-06 21:58:46.433651: step 44220, loss = 0.80 (1552.9 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:47.256006: step 44230, loss = 0.84 (1556.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:48.075032: step 44240, loss = 0.81 (1562.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:48.894472: step 44250, loss = 0.78 (1562.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:49.730362: step 44260, loss = 0.74 (1531.3 examples/sec; 0.084 sec/batch)
2017-05-06 21:58:50.546843: step 44270, loss = 0.87 (1567.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:51.369983: step 44280, loss = 0.79 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:52.187488: step 44290, loss = 0.80 (1565.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:53.114696: step 44300, loss = 0.70 (1380.5 examples/sec; 0.093 sec/batch)
2017-05-06 21:58:53.836565: step 44310, loss = 0.91 (1773.2 examples/sec; 0.072 sec/batch)
2017-05-06 21:58:54.656381: step 44320, loss = 0.71 (1561.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:55.479637: step 44330, loss = 0.81 (1554.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:56.297806: step 44340, loss = 0.76 (1564.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:57.121576: step 44350, loss = 0.63 (1553.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:58:57.946810: step 44360, loss = 0.85 (1551.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:58:58.775276: step 44370, loss = 0.68 (1545.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:58:59.599120: step 44380, loss = 0.78 (1553.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:59:00.426500: step 44390, loss = 0.66 (1547.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:59:01.346380: step 44400, loss = 0.72 (1391.5 examples/sec; 0.092 sec/batch)
2017-05-06 21:59:02.069348: step 44410, loss = 0.84 (1770.5 examples/sec; 0.072 sec/batch)
2017-05-06 21:59:02.889656: step 44420, loss = 0.72 (1560.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:59:03.710877: step 44430, loss = 0.76 (1558.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:59:04.533987: step 44440, loss = 0.72 (1555.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:59:05.354338: step 44450, loss = 0.79 (1560.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:59:06.191330: step 44460, loss = 0.74 (1529.3 examples/sec; 0.084 sec/batch)
2017-05-06 21:59:07.011773: step 44470, loss = 0.66 (1560.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:59:07.826657: step 44480, loss = 0.56 (1570.8 examples/sec; 0.081 sec/batch)
2017-05-06 21:59:08.651847: step 44490, loss = 0.82 (1551.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:59:09.571111: step 44500, loss = 0.85 (1392.4 examples/sec; 0.092 sec/batch)
2017-05-06 21:59:10.303434: step 44510, loss = 0.88 (1747.9 examples/sec; 0.073 sec/batch)
2017-05-06 21:59:11.130160: step 44520, loss = 0.73 (1548.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:59:11.940522: step 44530, loss = 0.70 (1579.5 examples/sec; 0.081 sec/batch)
2017-05-06 21:59:12.761328: step 44540, loss = 0.73 (1559.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:59:13.618211: step 44550, loss = 0.66 (1493.8 examples/sec; 0.086 sec/batch)
2017-05-06 21:59:14.409141: step 44560, loss = 0.71 (1618.3 examples/sec; 0.079 sec/batch)
2017-05-06 21:59:15.235494: step 44570, loss = 0.95 (1549.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:59:16.050900: step 44580, loss = 0.62 (1569.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:59:16.880625: step 44590, loss = 0.64 (1542.7 examples/sec; 0.083 sec/batch)
2017-05-06 21:59:17.802777: step 44600, loss = 0.67 (1388.0 examples/sec; 0.092 sec/batch)
2017-05-06 21:59:18.532799: step 44610, loss = 0.83 (1753.4 examples/sec; 0.073 sec/batch)
2017-05-06 21:59:19.364281: step 44620, loss = 0.64 (1539.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:59:20.181591: step 44630, loss = 0.82 (1566.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:59:21.007674: step 44640, loss = 0.86 (1549.5 examples/sec; 0.083 sec/batch)
2017-05-06 21:59:21.831209: step 44650, loss = 0.79 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:59:22.650974: step 44660, loss = 0.79 (1561.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:59:23.463980: step 44670, loss = 0.81 (1574.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:59:24.279239: step 44680, loss = 0.77 (1570.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:59:25.099471: step 44690, loss = 0.79 (1560.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:59:26.012003: step 44700, loss = 0.59 (1402.7 examples/sec; 0.091 sec/batch)
2017-05-06 21:59:26.734741: step 44710, loss = 0.80 (1771.1 examples/sec; 0.072 sec/batch)
2017-05-06 21:59:27.561053: step 44720, loss = 0.78 (1549.1 examples/sec; 0.083 sec/batch)
2017-05-06 21:59:28.380397: step 44730, loss = 0.77 (1562.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:59:29.208191: step 44740, loss = 0.71 (1546.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:59:30.040978: step 44750, loss = 0.86 (1537.0 examples/sec; 0.083 sec/batch)
2017-05-06 21:59:30.862159: step 44760, loss = 0.57 (1558.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:59:31.687262: step 44770, loss = 0.73 (1551.3 examples/sec; 0.083 sec/batch)
2017-05-06 21:59:32.504204: step 44780, loss = 0.82 (1566.8 examples/sec; 0.082 sec/batch)
2017-05-06 21:59:33.316526: step 44790, loss = 0.73 (1575.7 examples/sec; 0.081 sec/batch)
2017-05-06 21:59:34.235187: step 44800, loss = 0.95 (1393.3 examples/sec; 0.092 sec/batch)
2017-05-06 21:59:34.950628: step 44810, loss = 0.82 (1789.1 examples/sec; 0.072 sec/batch)
2017-05-06 21:59:35.767290: step 44820, loss = 0.71 (1567.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:59:36.592177: step 44830, loss = 0.69 (1551.7 examples/sec; 0.082 sec/batch)
2017-05-06 21:59:37.409204: step 44840, loss = 0.74 (1566.6 examples/sec; 0.082 sec/batch)
2017-05-06 21:59:38.230536: step 44850, loss = 0.84 (1558.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:59:39.043233: step 44860, loss = 0.77 (1575.0 examples/sec; 0.081 sec/batch)
2017-05-06 21:59:39.864735: step 44870, loss = 0.79 (1558.1 examples/sec; 0.082 sec/batch)
2017-05-06 21:59:40.685246: step 44880, loss = 0.64 (1560.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:59:41.511386: step 44890, loss = 0.78 (1549.4 examples/sec; 0.083 sec/batch)
2017-05-06 21:59:42.434134: step 44900, loss = 0.80 (1387.2 examples/sec; 0.092 sec/batch)
2017-05-06 21:59:43.156380: step 44910, loss = 0.80 (1772.2 examples/sec; 0.072 sec/batch)
2017-05-06 21:59:43.968854: step 44920, loss = 0.76 (1575.4 examples/sec; 0.081 sec/batch)
2017-05-06 21:59:44.793401: step 44930, loss = 0.75 (1552.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:59:45.610529: step 44940, loss = 0.85 (1566.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:59:46.434983: step 44950, loss = 0.71 (1552.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:59:47.258641: step 44960, loss = 0.87 (1554.0 examples/sec; 0.082 sec/batch)
2017-05-06 21:59:48.072193: step 44970, loss = 0.88 (1573.3 examples/sec; 0.081 sec/batch)
2017-05-06 21:59:48.899816: step 44980, loss = 0.63 (1546.6 examples/sec; 0.083 sec/batch)
2017-05-06 21:59:49.716409: step 44990, loss = 0.78 (1567.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:59:50.644652: step 45000, loss = 0.65 (1379.0 examples/sec; 0.093 sec/batch)
2017-05-06 21:59:51.359257: step 45010, loss = 0.71 (1791.2 examples/sec; 0.071 sec/batch)
2017-05-06 21:59:52.173982: step 45020, loss = 0.66 (1571.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:59:53.000189: step 45030, loss = 0.70 (1549.2 examples/sec; 0.083 sec/batch)
2017-05-06 21:59:53.833088: step 45040, loss = 0.79 (1536.8 examples/sec; 0.083 sec/batch)
2017-05-06 21:59:54.645756: step 45050, loss = 0.72 (1575.1 examples/sec; 0.081 sec/batch)
2017-05-06 21:59:55.465531: step 45060, loss = 0.81 (1561.4 examples/sec; 0.082 sec/batch)
2017-05-06 21:59:56.284842: step 45070, loss = 0.72 (1562.3 examples/sec; 0.082 sec/batch)
2017-05-06 21:59:57.108931: step 45080, loss = 0.75 (1553.2 examples/sec; 0.082 sec/batch)
2017-05-06 21:59:57.927106: step 45090, loss = 0.79 (1564.5 examples/sec; 0.082 sec/batch)
2017-05-06 21:59:58.859287: step 45100, loss = 0.77 (1373.1 examples/sec; 0.093 sec/batch)
2017-05-06 21:59:59.565063: step 45110, loss = 0.69 (1813.6 examples/sec; 0.071 sec/batch)
2017-05-06 22:00:00.382597: step 45120, loss = 0.87 (1565.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:01.207941: step 45130, loss = 0.82 (1550.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:00:02.042469: step 45140, loss = 0.79 (1533.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:00:02.859761: step 45150, loss = 0.90 (1566.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:03.676086: step 45160, loss = 0.84 (1568.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:04.490134: step 45170, loss = 0.72 (1572.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:00:05.317340: step 45180, loss = 0.84 (1547.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:00:06.133265: step 45190, loss = 0.81 (1568.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:07.065141: step 45200, loss = 0.78 (1373.6 examples/sec; 0.093 sec/batch)
2017-05-06 22:00:07.781555: step 45210, loss = 0.73 (1786.7 examples/sec; 0.072 sec/batch)
2017-05-06 22:00:08.603411: step 45220, loss = 0.82 (1557.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:09.421090: step 45230, loss = 0.57 (1565.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:10.246949: step 45240, loss = 0.62 (1549.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:00:11.071142: step 45250, loss = 0.70 (1553.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:11.906800: step 45260, loss = 0.80 (1531.7 examples/sec; 0.084 sec/batch)
2017-05-06 22:00:12.729821: step 45270, loss = 0.73 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:13.550826: step 45280, loss = 0.74 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:14.368726: step 45290, loss = 0.77 (1565.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:15.279639: step 45300, loss = 0.76 (1405.2 examples/sec; 0.091 sec/batch)
2017-05-06 22:00:15.997407: step 45310, loss = 0.84 (1783.3 examples/sec; 0.072 sec/batch)
2017-05-06 22:00:16.828977: step 45320, loss = 0.71 (1539.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:00:17.650641: step 45330, loss = 0.72 (1557.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:18.470008: step 45340, loss = 0.80 (1562.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:19.289573: step 45350, loss = 0.87 (1561.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:20.110893: step 45360, loss = 0.86 (1558.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:20.926762: step 45370, loss = 0.82 (1568.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:21.742828: step 45380, loss = 0.74 (1568.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:22.563513: step 45390, loss = 0.74 (1559.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:23.476489: step 45400, loss = 0.68 (1402.0 examples/sec; 0.091 sec/batch)
2017-05-06 22:00:24.188561: step 45410, loss = 0.82 (1797.6 examples/sec; 0.071 sec/batch)
2017-05-06 22:00:25.009147: step 45420, loss = 0.72 (1559.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:25.837332: step 45430, loss = 0.71 (1545.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:00:26.658337: step 45440, loss = 0.94 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:27.484863: step 45450, loss = 0.74 (1548.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:00:28.303003: step 45460, loss = 0.80 (1564.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:29.120343: step 45470, loss = 0.83 (1566.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:29.943439: step 45480, loss = 0.80 (1555.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:30.777634: step 45490, loss = 0.87 (1534.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:00:31.706909: step 45500, loss = 0.63 (1377.4 examples/sec; 0.093 sec/batch)
2017-05-06 22:00:32.440544: step 45510, loss = 0.95 (1744.7 examples/sec; 0.073 sec/batch)
2017-05-06 22:00:33.268103: step 45520, loss = 0.77 (1546.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:00:34.094477: step 45530, loss = 0.87 (1548.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:00:34.938251: step 45540, loss = 0.76 (1517.0 examples/sec; 0.084 sec/batch)
2017-05-06 22:00:35.736718: step 45550, loss = 0.74 (1603.1 examples/sec; 0.080 sec/batch)
2017-05-06 22:00:36.566187: step 45560, loss = 0.72 (1543.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:00:37.386081: step 45570, loss = 0.63 (1561.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:38.214213: step 45580, loss = 0.98 (1545.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:00:39.035222: step 45590, loss = 0.76 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:39.951546: step 45600, loss = 0.80 (1396.9 examples/sec; 0.092 sec/batch)
2017-05-06 22:00:40.676426: step 45610, loss = 0.71 (1765.8 examples/sec; 0.072 sec/batch)
2017-05-06 22:00:41.503759: step 45620, loss = 0.71 (1547.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:00:42.327736: step 45630, loss = 0.73 (1553.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:43.153890: step 45640, loss = 0.74 (1549.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:00:43.973664: step 45650, loss = 0.71 (1561.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:44.794550: step 45660, loss = 1.07 (1559.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:45.616584: step 45670, loss = 0.78 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:46.441744: step 45680, loss = 0.78 (1551.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:00:47.267015: step 45690, loss = 0.81 (1551.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:00:48.182305: step 45700, loss = 0.66 (1398.5 examples/sec; 0.092 sec/batch)
2017-05-06 22:00:48.914166: step 45710, loss = 0.68 (1749.0 examples/sec; 0.073 sec/batch)
2017-05-06 22:00:49.733271: step 45720, loss = 0.83 (1562.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:50.551628: step 45730, loss = 0.76 (1564.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:51.373473: step 45740, loss = 0.85 (1557.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:52.186238: step 45750, loss = 0.81 (1574.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:00:53.010903: step 45760, loss = 0.79 (1552.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:53.830478: step 45770, loss = 0.68 (1561.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:54.652112: step 45780, loss = 0.82 (1557.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:55.469242: step 45790, loss = 0.78 (1566.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:56.382124: step 45800, loss = 0.61 (1402.1 examples/sec; 0.091 sec/batch)
2017-05-06 22:00:57.112782: step 45810, loss = 0.77 (1751.9 examples/sec; 0.073 sec/batch)
2017-05-06 22:00:57.933859: step 45820, loss = 0.71 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:00:58.747944: step 45830, loss = 0.68 (1572.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:00:59.565402: step 45840, loss = 0.68 (1565.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:00.384410: step 45850, loss = 0.60 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:01.193881: step 45860, loss = 0.74 (1581.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:01:02.027539: step 45870, loss = 0.78 (1535.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:01:02.848916: step 45880, loss = 0.70 (1558.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:03.670515: step 45890, loss = 0.78 (1557.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:04.583523: step 45900, loss = 0.93 (1402.0 examples/sec; 0.091 sec/batch)
2017-05-06 22:01:05.313023: step 45910, loss = 0.73 (1754.6 examples/sec; 0.073 sec/batch)
2017-05-06 22:01:06.138585: step 45920, loss = 0.71 (1550.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:01:06.963371: step 45930, loss = 0.79 (1551.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:07.777602: step 45940, loss = 0.82 (1572.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:01:08.609428: step 45950, loss = 0.86 (1538.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:01:09.433131: step 45960, loss = 0.75 (1554.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:10.258601: step 45970, loss = 0.73 (1550.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:01:11.082304: step 45980, loss = 0.85 (1554.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:11.900690: step 45990, loss = 0.74 (1564.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:12.830367: step 46000, loss = 0.79 (1376.8 examples/sec; 0.093 sec/batch)
2017-05-06 22:01:13.540099: step 46010, loss = 0.83 (1803.5 examples/sec; 0.071 sec/batch)
2017-05-06 22:01:14.365753: step 46020, loss = 0.86 (1550.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:01:15.185441: step 46030, loss = 0.82 (1561.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:16.006560: step 46040, loss = 0.73 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:16.834403: step 46050, loss = 0.85 (1546.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:01:17.652298: step 46060, loss = 0.72 (1565.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:18.472763: step 46070, loss = 0.67 (1560.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:19.292271: step 46080, loss = 0.74 (1561.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:20.100166: step 46090, loss = 0.79 (1584.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:01:21.015109: step 46100, loss = 0.73 (1399.0 examples/sec; 0.091 sec/batch)
2017-05-06 22:01:21.738612: step 46110, loss = 0.89 (1769.2 examples/sec; 0.072 sec/batch)
2017-05-06 22:01:22.564879: step 46120, loss = 0.72 (1549.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:01:23.386984: step 46130, loss = 0.74 (1557.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:24.209402: step 46140, loss = 0.77 (1556.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:25.033756: step 46150, loss = 1.00 (1552.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:25.852504: step 46160, loss = 0.73 (1563.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:26.670087: step 46170, loss = 0.72 (1565.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:27.482507: step 46180, loss = 0.81 (1575.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:01:28.309954: step 46190, loss = 0.77 (1546.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:01:29.248509: step 46200, loss = 0.86 (1363.8 examples/sec; 0.094 sec/batch)
2017-05-06 22:01:29.949203: step 46210, loss = 0.76 (1826.8 examples/sec; 0.070 sec/batch)
2017-05-06 22:01:30.768750: step 46220, loss = 0.71 (1561.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:31.591731: step 46230, loss = 0.72 (1555.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:32.413492: step 46240, loss = 0.75 (1557.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:33.231778: step 46250, loss = 0.70 (1564.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:34.049058: step 46260, loss = 0.71 (1566.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:34.866270: step 46270, loss = 0.78 (1566.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:35.672334: step 46280, loss = 0.76 (1588.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:01:36.502098: step 46290, loss = 0.73 (1542.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:01:37.419497: step 46300, loss = 0.79 (1395.3 examples/sec; 0.092 sec/batch)
2017-05-06 22:01:38.147252: step 46310, loss = 0.78 (1758.8 examples/sec; 0.073 sec/batch)
2017-05-06 22:01:38.962211: step 46320, loss = 0.69 (1570.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:01:39.775154: step 46330, loss = 0.65 (1574.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:01:40.600868: step 46340, loss = 0.72 (1550.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:01:41.433405: step 46350, loss = 0.79 (1537.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:01:42.260168: step 46360, loss = 0.92 (1548.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:01:43.089391: step 46370, loss = 0.78 (1543.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:01:43.905498: step 46380, loss = 0.70 (1568.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:44.725468: step 46390, loss = 0.79 (1561.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:45.644876: step 46400, loss = 0.72 (1392.2 examples/sec; 0.092 sec/batch)
2017-05-06 22:01:46.373144: step 46410, loss = 0.72 (1757.6 examples/sec; 0.073 sec/batch)
2017-05-06 22:01:47.190209: step 46420, loss = 0.83 (1566.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:48.000088: step 46430, loss = 0.97 (1580.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:01:48.823750: step 46440, loss = 0.80 (1554.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:49.642704: step 46450, loss = 0.74 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:50.461880: step 46460, loss = 0.69 (1562.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:51.285304: step 46470, loss = 0.81 (1554.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:52.103577: step 46480, loss = 0.71 (1564.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:52.925770: step 46490, loss = 0.90 (1556.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:53.853391: step 46500, loss = 0.78 (1379.9 examples/sec; 0.093 sec/batch)
2017-05-06 22:01:54.566901: step 46510, loss = 0.77 (1793.9 examples/sec; 0.071 sec/batch)
2017-05-06 22:01:55.388136: step 46520, loss = 0.82 (1558.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:01:56.225671: step 46530, loss = 0.78 (1528.3 examples/sec; 0.084 sec/batch)
2017-05-06 22:01:57.038033: step 46540, loss = 0.69 (1575.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:01:57.865913: step 46550, loss = 0.68 (1546.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:01:58.702356: step 46560, loss = 0.80 (1530.3 examples/sec; 0.084 sec/batch)
2017-05-06 22:01:59.517331: step 46570, loss = 0.79 (1570.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:02:00.338111: step 46580, loss = 0.76 (1559.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:01.154547: step 46590, loss = 0.80 (1567.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:02.070359: step 46600, loss = 0.80 (1397.7 examples/sec; 0.092 sec/batch)
2017-05-06 22:02:02.804837: step 46610, loss = 0.87 (1742.7 examples/sec; 0.073 sec/batch)
2017-05-06 22:02:03.621422: step 46620, loss = 0.76 (1567.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:04.445610: step 46630, loss = 0.79 (1553.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:05.264098: step 46640, loss = 0.76 (1563.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:06.078806: step 46650, loss = 0.75 (1571.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:02:06.909181: step 46660, loss = 1.02 (1541.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:02:07.725109: step 46670, loss = 0.76 (1568.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:08.547563: step 46680, loss = 0.84 (1556.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:09.367121: step 46690, loss = 0.93 (1561.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:10.291014: step 46700, loss = 0.74 (1385.4 examples/sec; 0.092 sec/batch)
2017-05-06 22:02:11.019744: step 46710, loss = 0.89 (1756.5 examples/sec; 0.073 sec/batch)
2017-05-06 22:02:11.831984: step 46720, loss = 0.66 (1575.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:02:12.653495: step 46730, loss = 0.62 (1558.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:13.471106: step 46740, loss = 0.82 (1565.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:14.295160: step 46750, loss = 0.76 (1553.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:15.117037: step 46760, loss = 0.75 (1557.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:15.934141: step 46770, loss = 0.85 (1566.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:16.754621: step 46780, loss = 0.75 (1560.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:17.578328: step 46790, loss = 0.74 (1554.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:18.504806: step 46800, loss = 0.77 (1381.6 examples/sec; 0.093 sec/batch)
2017-05-06 22:02:19.227622: step 46810, loss = 0.69 (1770.8 examples/sec; 0.072 sec/batch)
2017-05-06 22:02:20.040173: step 46820, loss = 0.74 (1575.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:02:20.863761: step 46830, loss = 0.77 (1554.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:21.685995: step 46840, loss = 0.67 (1556.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:22.515469: step 46850, loss = 0.82 (1543.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:02:23.335330: step 46860, loss = 0.68 (1561.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:24.152735: step 46870, loss = 0.90 (1565.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:24.981401: step 46880, loss = 0.70 (1544.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:02:25.798774: step 46890, loss = 0.80 (1566.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:26.722809: step 46900, loss = 0.61 (1385.2 examples/sec; 0.092 sec/batch)
2017-05-06 22:02:27.440598: step 46910, loss = 0.73 (1783.2 examples/sec; 0.072 sec/batch)
2017-05-06 22:02:28.258758: step 46920, loss = 0.82 (1564.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:29.081847: step 46930, loss = 0.80 (1555.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:29.906356: step 46940, loss = 0.80 (1552.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:30.723327: step 46950, loss = 0.68 (1566.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:31.545542: step 46960, loss = 0.85 (1556.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:32.363899: step 46970, loss = 0.75 (1564.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:33.185753: step 46980, loss = 0.99 (1557.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:34.010274: step 46990, loss = 0.70 (1552.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:34.941356: step 47000, loss = 0.71 (1374.7 examples/sec; 0.093 sec/batch)
2017-05-06 22:02:35.646781: step 47010, loss = 0.76 (1814.5 examples/sec; 0.071 sec/batch)
2017-05-06 22:02:36.459450: step 47020, loss = 0.76 (1575.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:02:37.290177: step 47030, loss = 0.76 (1540.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:02:38.107671: step 47040, loss = 0.71 (1565.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:38.931750: step 47050, loss = 0.81 (1553.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:39.743679: step 47060, loss = 0.73 (1576.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:02:40.570313: step 47070, loss = 0.76 (1548.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:02:41.389402: step 47080, loss = 0.61 (1562.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:42.209341: step 47090, loss = 0.74 (1561.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:43.142038: step 47100, loss = 0.58 (1372.4 examples/sec; 0.093 sec/batch)
2017-05-06 22:02:43.841716: step 47110, loss = 0.85 (1829.4 examples/sec; 0.070 sec/batch)
2017-05-06 22:02:44.663063: step 47120, loss = 0.66 (1558.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:45.483767: step 47130, loss = 0.72 (1559.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:46.306371: step 47140, loss = 0.85 (1556.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:47.126817: step 47150, loss = 0.70 (1560.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:47.938720: step 47160, loss = 0.78 (1576.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:02:48.749062: step 47170, loss = 0.69 (1579.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:02:49.568693: step 47180, loss = 0.88 (1561.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:50.393140: step 47190, loss = 0.61 (1552.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:51.330600: step 47200, loss = 0.73 (1365.4 examples/sec; 0.094 sec/batch)
2017-05-06 22:02:52.043281: step 47210, loss = 0.76 (1796.0 examples/sec; 0.071 sec/batch)
2017-05-06 22:02:52.858404: step 47220, loss = 0.71 (1570.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:53.691819: step 47230, loss = 0.81 (1535.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:02:54.515074: step 47240, loss = 0.75 (1554.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:55.340505: step 47250, loss = 0.76 (1550.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:02:56.147985: step 47260, loss = 0.74 (1585.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:02:56.966957: step 47270, loss = 0.73 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:57.784963: step 47280, loss = 0.97 (1564.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:02:58.610961: step 47290, loss = 0.81 (1549.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:02:59.530570: step 47300, loss = 0.76 (1391.9 examples/sec; 0.092 sec/batch)
2017-05-06 22:03:00.252305: step 47310, loss = 0.68 (1773.5 examples/sec; 0.072 sec/batch)
2017-05-06 22:03:01.083907: step 47320, loss = 0.72 (1539.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:03:01.903881: step 47330, loss = 0.72 (1561.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:02.723594: step 47340, loss = 0.90 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:03.540147: step 47350, loss = 0.67 (1567.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:04.359370: step 47360, loss = 0.62 (1562.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:05.174742: step 47370, loss = 0.81 (1569.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:05.988750: step 47380, loss = 0.70 (1572.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:03:06.811161: step 47390, loss = 0.73 (1556.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:07.728470: step 47400, loss = 0.82 (1395.4 examples/sec; 0.092 sec/batch)
2017-05-06 22:03:08.461216: step 47410, loss = 0.68 (1746.9 examples/sec; 0.073 sec/batch)
2017-05-06 22:03:09.282360: step 47420, loss = 0.73 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:10.107879: step 47430, loss = 0.96 (1550.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:03:10.923215: step 47440, loss = 0.82 (1569.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:11.731034: step 47450, loss = 0.79 (1584.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:03:12.550065: step 47460, loss = 0.86 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:13.373512: step 47470, loss = 0.75 (1554.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:14.195037: step 47480, loss = 0.73 (1558.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:15.012593: step 47490, loss = 0.75 (1565.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:15.945836: step 47500, loss = 0.62 (1371.6 examples/sec; 0.093 sec/batch)
2017-05-06 22:03:16.654420: step 47510, loss = 0.84 (1806.5 examples/sec; 0.071 sec/batch)
2017-05-06 22:03:17.494736: step 47520, loss = 0.79 (1523.2 examples/sec; 0.084 sec/batch)
2017-05-06 22:03:18.298457: step 47530, loss = 0.58 (1592.6 examples/sec; 0.080 sec/batch)
2017-05-06 22:03:19.123362: step 47540, loss = 0.77 (1551.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:19.936325: step 47550, loss = 0.93 (1574.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:03:20.756235: step 47560, loss = 0.78 (1561.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:21.581849: step 47570, loss = 0.79 (1550.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:03:22.403534: step 47580, loss = 0.64 (1557.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:23.226151: step 47590, loss = 0.58 (1556.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:24.143064: step 47600, loss = 0.65 (1396.0 examples/sec; 0.092 sec/batch)
2017-05-06 22:03:24.875353: step 47610, loss = 0.91 (1747.9 examples/sec; 0.073 sec/batch)
2017-05-06 22:03:25.722075: step 47620, loss = 0.93 (1511.8 examples/sec; 0.085 sec/batch)
2017-05-06 22:03:26.534761: step 47630, loss = 0.84 (1575.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:03:27.354922: step 47640, loss = 0.89 (1560.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:28.165346: step 47650, loss = 0.78 (1579.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:03:28.993939: step 47660, loss = 0.79 (1544.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:03:29.813473: step 47670, loss = 0.89 (1561.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:30.635413: step 47680, loss = 0.81 (1557.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:31.452383: step 47690, loss = 0.76 (1566.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:32.395398: step 47700, loss = 0.68 (1357.3 examples/sec; 0.094 sec/batch)
2017-05-06 22:03:33.094981: step 47710, loss = 0.68 (1829.7 examples/sec; 0.070 sec/batch)
2017-05-06 22:03:33.916713: step 47720, loss = 0.77 (1557.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:34.731920: step 47730, loss = 0.90 (1570.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:35.550096: step 47740, loss = 0.75 (1564.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:36.366345: step 47750, loss = 0.78 (1568.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:37.182651: step 47760, loss = 0.72 (1568.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:38.010328: step 47770, loss = 0.74 (1546.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:03:38.831712: step 47780, loss = 0.75 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:39.649744: step 47790, loss = 0.78 (1564.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:40.581372: step 47800, loss = 0.86 (1373.9 examples/sec; 0.093 sec/batch)
2017-05-06 22:03:41.290861: step 47810, loss = 0.82 (1804.1 examples/sec; 0.071 sec/batch)
2017-05-06 22:03:42.111307: step 47820, loss = 0.79 (1560.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:42.930977: step 47830, loss = 0.88 (1561.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:43.745736: step 47840, loss = 0.73 (1571.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:03:44.566406: step 47850, loss = 0.64 (1559.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:45.390818: step 47860, loss = 0.75 (1552.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:46.209248: step 47870, loss = 0.83 (1564.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:47.036445: step 47880, loss = 0.77 (1547.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:03:47.845194: step 47890, loss = 0.73 (1582.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:03:48.756271: step 47900, loss = 0.89 (1404.9 examples/sec; 0.091 sec/batch)
2017-05-06 22:03:49.480096: step 47910, loss = 0.80 (1768.4 examples/sec; 0.072 sec/batch)
2017-05-06 22:03:50.310710: step 47920, loss = 0.89 (1541.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:03:51.130659: step 47930, loss = 0.71 (1561.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:51.935107: step 47940, loss = 0.83 (1591.2 examples/sec; 0.080 sec/batch)
2017-05-06 22:03:52.752175: step 47950, loss = 0.69 (1566.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:53.579215: step 47960, loss = 0.93 (1547.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:03:54.406818: step 47970, loss = 0.76 (1546.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:03:55.225599: step 47980, loss = 0.72 (1563.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:56.044817: step 47990, loss = 0.83 (1562.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:56.960901: step 48000, loss = 0.70 (1397.3 examples/sec; 0.092 sec/batch)
2017-05-06 22:03:57.681288: step 48010, loss = 0.65 (1776.8 examples/sec; 0.072 sec/batch)
2017-05-06 22:03:58.503481: step 48020, loss = 0.70 (1556.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:03:59.320273: step 48030, loss = 0.81 (1567.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:00.134794: step 48040, loss = 0.60 (1571.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:04:00.960602: step 48050, loss = 0.74 (1550.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:04:01.778525: step 48060, loss = 0.72 (1564.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:02.603636: step 48070, loss = 0.66 (1551.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:04:03.417930: step 48080, loss = 0.88 (1571.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:04:04.235235: step 48090, loss = 0.89 (1566.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:05.142288: step 48100, loss = 0.66 (1411.2 examples/sec; 0.091 sec/batch)
2017-05-06 22:04:05.878658: step 48110, loss = 0.70 (1738.3 examples/sec; 0.074 sec/batch)
2017-05-06 22:04:06.710397: step 48120, loss = 0.78 (1538.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:04:07.522996: step 48130, loss = 0.75 (1575.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:04:08.342021: step 48140, loss = 0.74 (1562.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:09.156757: step 48150, loss = 0.72 (1571.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:04:09.978763: step 48160, loss = 0.66 (1557.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:10.803670: step 48170, loss = 0.79 (1551.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:11.618250: step 48180, loss = 0.71 (1571.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:04:12.436431: step 48190, loss = 0.75 (1564.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:13.356739: step 48200, loss = 0.70 (1390.8 examples/sec; 0.092 sec/batch)
2017-05-06 22:04:14.085621: step 48210, loss = 0.63 (1756.1 examples/sec; 0.073 sec/batch)
2017-05-06 22:04:14.909519: step 48220, loss = 0.75 (1553.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:15.730154: step 48230, loss = 0.93 (1559.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:16.548232: step 48240, loss = 0.66 (1564.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:17.370768: step 48250, loss = 0.80 (1556.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:18.192666: step 48260, loss = 0.78 (1557.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:19.011250: step 48270, loss = 0.79 (1563.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:19.828103: step 48280, loss = 0.80 (1567.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:20.652328: step 48290, loss = 1.00 (1553.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:21.573184: step 48300, loss = 0.76 (1390.0 examples/sec; 0.092 sec/batch)
2017-05-06 22:04:22.286809: step 48310, loss = 0.83 (1793.7 examples/sec; 0.071 sec/batch)
2017-05-06 22:04:23.107020: step 48320, loss = 0.72 (1560.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:23.916258: step 48330, loss = 0.65 (1581.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:04:24.740201: step 48340, loss = 0.89 (1553.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:25.558035: step 48350, loss = 0.81 (1565.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:26.380812: step 48360, loss = 0.83 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:27.200502: step 48370, loss = 0.91 (1561.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:28.018432: step 48380, loss = 0.63 (1564.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:28.840246: step 48390, loss = 0.83 (1557.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:29.764624: step 48400, loss = 0.77 (1384.7 examples/sec; 0.092 sec/batch)
2017-05-06 22:04:30.482418: step 48410, loss = 0.71 (1783.3 examples/sec; 0.072 sec/batch)
2017-05-06 22:04:31.305087: step 48420, loss = 0.70 (1555.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:32.129946: step 48430, loss = 0.77 (1551.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:32.957139: step 48440, loss = 0.72 (1547.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:04:33.768996: step 48450, loss = 0.70 (1576.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:04:34.590090: step 48460, loss = 0.63 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:35.405722: step 48470, loss = 0.64 (1569.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:36.219371: step 48480, loss = 0.76 (1573.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:04:37.041224: step 48490, loss = 0.62 (1557.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:37.956004: step 48500, loss = 0.58 (1399.2 examples/sec; 0.091 sec/batch)
2017-05-06 22:04:38.708696: step 48510, loss = 0.69 (1700.6 examples/sec; 0.075 sec/batch)
2017-05-06 22:04:39.514924: step 48520, loss = 0.70 (1587.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:04:40.333512: step 48530, loss = 0.75 (1563.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:41.151476: step 48540, loss = 0.71 (1564.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:41.978760: step 48550, loss = 0.86 (1547.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:04:42.810275: step 48560, loss = 0.79 (1539.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:04:43.624723: step 48570, loss = 0.76 (1571.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:04:44.447279: step 48580, loss = 0.67 (1556.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:45.272104: step 48590, loss = 0.88 (1551.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:46.198913: step 48600, loss = 0.72 (1381.1 examples/sec; 0.093 sec/batch)
2017-05-06 22:04:46.926798: step 48610, loss = 0.70 (1758.5 examples/sec; 0.073 sec/batch)
2017-05-06 22:04:47.741896: step 48620, loss = 0.80 (1570.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:48.560187: step 48630, loss = 0.63 (1564.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:49.391473: step 48640, loss = 0.69 (1539.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:04:50.216843: step 48650, loss = 0.68 (1550.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:04:51.041118: step 48660, loss = 0.68 (1552.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:51.851537: step 48670, loss = 0.70 (1579.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:04:52.673295: step 48680, loss = 0.81 (1557.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:53.499649: step 48690, loss = 0.69 (1549.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:04:54.424191: step 48700, loss = 0.74 (1384.5 examples/sec; 0.092 sec/batch)
2017-05-06 22:04:55.155376: step 48710, loss = 0.70 (1750.6 examples/sec; 0.073 sec/batch)
2017-05-06 22:04:55.968093: step 48720, loss = 0.71 (1575.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:04:56.787397: step 48730, loss = 0.79 (1562.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:57.609238: step 48740, loss = 0.66 (1557.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:58.424529: step 48750, loss = 0.73 (1570.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:04:59.244658: step 48760, loss = 0.82 (1560.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:00.069563: step 48770, loss = 0.77 (1551.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:00.889322: step 48780, loss = 0.61 (1561.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:01.710329: step 48790, loss = 0.74 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:02.652859: step 48800, loss = 0.78 (1358.1 examples/sec; 0.094 sec/batch)
2017-05-06 22:05:03.364553: step 48810, loss = 0.82 (1798.5 examples/sec; 0.071 sec/batch)
2017-05-06 22:05:04.187281: step 48820, loss = 0.78 (1555.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:05.002321: step 48830, loss = 0.83 (1570.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:05.826982: step 48840, loss = 0.69 (1552.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:06.648086: step 48850, loss = 0.72 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:07.457128: step 48860, loss = 0.91 (1582.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:05:08.277227: step 48870, loss = 0.73 (1560.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:09.105573: step 48880, loss = 0.91 (1545.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:05:09.928882: step 48890, loss = 0.85 (1554.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:10.859482: step 48900, loss = 0.76 (1375.5 examples/sec; 0.093 sec/batch)
2017-05-06 22:05:11.590258: step 48910, loss = 0.73 (1751.6 examples/sec; 0.073 sec/batch)
2017-05-06 22:05:12.414925: step 48920, loss = 0.74 (1552.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:13.234859: step 48930, loss = 0.61 (1561.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:14.060962: step 48940, loss = 0.80 (1549.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:05:14.882465: step 48950, loss = 0.88 (1558.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:15.692513: step 48960, loss = 0.59 (1580.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:05:16.508790: step 48970, loss = 0.71 (1568.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:17.335505: step 48980, loss = 0.76 (1548.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:05:18.161861: step 48990, loss = 0.88 (1549.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:05:19.082163: step 49000, loss = 0.81 (1390.9 examples/sec; 0.092 sec/batch)
2017-05-06 22:05:19.796732: step 49010, loss = 0.67 (1791.3 examples/sec; 0.071 sec/batch)
2017-05-06 22:05:20.617881: step 49020, loss = 0.69 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:21.437184: step 49030, loss = 0.73 (1562.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:22.252519: step 49040, loss = 0.73 (1569.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:23.076514: step 49050, loss = 0.74 (1553.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:23.895000: step 49060, loss = 0.78 (1563.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:24.715180: step 49070, loss = 0.86 (1560.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:25.548883: step 49080, loss = 0.71 (1535.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:05:26.370225: step 49090, loss = 0.64 (1558.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:27.297767: step 49100, loss = 0.75 (1380.0 examples/sec; 0.093 sec/batch)
2017-05-06 22:05:28.022653: step 49110, loss = 0.78 (1765.8 examples/sec; 0.072 sec/batch)
2017-05-06 22:05:28.846028: step 49120, loss = 0.81 (1554.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:29.667922: step 49130, loss = 0.80 (1557.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:30.493077: step 49140, loss = 0.66 (1551.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:05:31.311082: step 49150, loss = 0.75 (1564.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:32.123461: step 49160, loss = 0.60 (1575.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:05:32.957682: step 49170, loss = 0.61 (1534.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:05:33.777713: step 49180, loss = 0.78 (1560.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:34.605057: step 49190, loss = 0.73 (1547.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:05:35.513737: step 49200, loss = 0.72 (1408.6 examples/sec; 0.091 sec/batch)
2017-05-06 22:05:36.242341: step 49210, loss = 0.72 (1756.8 examples/sec; 0.073 sec/batch)
2017-05-06 22:05:37.064419: step 49220, loss = 0.88 (1557.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:37.879467: step 49230, loss = 0.73 (1570.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:38.702484: step 49240, loss = 0.76 (1555.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:39.523623: step 49250, loss = 0.71 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:40.337832: step 49260, loss = 0.69 (1572.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:05:41.163651: step 49270, loss = 0.79 (1550.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:05:41.982439: step 49280, loss = 0.71 (1563.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:42.798408: step 49290, loss = 0.87 (1568.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:43.710494: step 49300, loss = 0.63 (1403.4 examples/sec; 0.091 sec/batch)
2017-05-06 22:05:44.439842: step 49310, loss = 0.66 (1755.0 examples/sec; 0.073 sec/batch)
2017-05-06 22:05:45.265608: step 49320, loss = 0.65 (1550.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:05:46.082242: step 49330, loss = 0.59 (1567.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:46.903087: step 49340, loss = 0.70 (1559.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:47.718461: step 49350, loss = 0.69 (1569.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:48.539687: step 49360, loss = 0.85 (1558.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:49.358859: step 49370, loss = 0.88 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:50.178569: step 49380, loss = 0.68 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:51.008075: step 49390, loss = 0.82 (1543.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:05:51.918557: step 49400, loss = 0.78 (1405.9 examples/sec; 0.091 sec/batch)
2017-05-06 22:05:52.641996: step 49410, loss = 0.71 (1769.3 examples/sec; 0.072 sec/batch)
2017-05-06 22:05:53.462237: step 49420, loss = 0.83 (1560.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:54.284163: step 49430, loss = 0.92 (1557.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:55.101239: step 49440, loss = 0.88 (1566.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:55.910981: step 49450, loss = 0.78 (1580.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:05:56.724455: step 49460, loss = 0.72 (1573.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:05:57.546760: step 49470, loss = 0.67 (1556.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:58.365256: step 49480, loss = 0.80 (1563.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:05:59.187894: step 49490, loss = 0.87 (1556.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:00.128069: step 49500, loss = 0.80 (1361.4 examples/sec; 0.094 sec/batch)
2017-05-06 22:06:00.830857: step 49510, loss = 0.68 (1821.3 examples/sec; 0.070 sec/batch)
2017-05-06 22:06:01.653455: step 49520, loss = 0.76 (1556.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:02.466998: step 49530, loss = 0.80 (1573.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:06:03.285190: step 49540, loss = 0.82 (1564.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:04.094196: step 49550, loss = 0.60 (1582.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:06:04.911886: step 49560, loss = 0.79 (1565.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:05.732490: step 49570, loss = 0.77 (1559.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:06.550335: step 49580, loss = 0.66 (1565.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:07.368281: step 49590, loss = 0.75 (1564.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:08.304774: step 49600, loss = 0.62 (1366.8 examples/sec; 0.094 sec/batch)
2017-05-06 22:06:09.018760: step 49610, loss = 0.70 (1792.7 examples/sec; 0.071 sec/batch)
2017-05-06 22:06:09.844553: step 49620, loss = 0.68 (1550.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:06:10.662938: step 49630, loss = 0.92 (1564.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:11.486673: step 49640, loss = 0.83 (1553.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:12.303838: step 49650, loss = 0.85 (1566.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:13.124368: step 49660, loss = 0.66 (1560.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:13.946007: step 49670, loss = 0.72 (1557.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:14.778971: step 49680, loss = 0.83 (1536.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:06:15.592535: step 49690, loss = 0.74 (1573.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:06:16.513626: step 49700, loss = 0.78 (1389.7 examples/sec; 0.092 sec/batch)
2017-05-06 22:06:17.236575: step 49710, loss = 0.78 (1770.5 examples/sec; 0.072 sec/batch)
2017-05-06 22:06:18.059975: step 49720, loss = 0.61 (1554.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:18.879846: step 49730, loss = 0.82 (1561.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:19.694301: step 49740, loss = 0.69 (1571.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:06:20.522136: step 49750, loss = 0.89 (1546.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:06:21.344428: step 49760, loss = 0.78 (1556.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:22.170919: step 49770, loss = 0.83 (1548.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:06:22.991082: step 49780, loss = 0.70 (1560.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:23.808684: step 49790, loss = 0.75 (1565.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:24.737670: step 49800, loss = 0.76 (1377.8 examples/sec; 0.093 sec/batch)
2017-05-06 22:06:25.460919: step 49810, loss = 0.64 (1769.8 examples/sec; 0.072 sec/batch)
2017-05-06 22:06:26.289318: step 49820, loss = 0.73 (1545.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:06:27.116942: step 49830, loss = 0.75 (1546.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:06:27.929501: step 49840, loss = 0.75 (1575.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:06:28.753533: step 49850, loss = 0.73 (1553.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:29.567291: step 49860, loss = 0.85 (1572.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:06:30.393198: step 49870, loss = 0.62 (1549.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:06:31.218514: step 49880, loss = 0.75 (1550.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:06:32.032322: step 49890, loss = 0.72 (1572.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:06:32.947591: step 49900, loss = 0.74 (1398.5 examples/sec; 0.092 sec/batch)
2017-05-06 22:06:33.671622: step 49910, loss = 0.74 (1767.9 examples/sec; 0.072 sec/batch)
2017-05-06 22:06:34.489472: step 49920, loss = 0.81 (1565.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:35.308632: step 49930, loss = 0.68 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:36.121005: step 49940, loss = 0.73 (1575.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:06:36.943691: step 49950, loss = 0.75 (1556.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:37.762420: step 49960, loss = 0.77 (1563.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:38.590609: step 49970, loss = 0.70 (1545.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:06:39.406003: step 49980, loss = 0.73 (1569.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:40.220954: step 49990, loss = 0.89 (1570.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:06:41.142020: step 50000, loss = 0.80 (1389.7 examples/sec; 0.092 sec/batch)
2017-05-06 22:06:41.873771: step 50010, loss = 0.69 (1749.2 examples/sec; 0.073 sec/batch)
2017-05-06 22:06:42.697296: step 50020, loss = 0.78 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:43.513351: step 50030, loss = 0.54 (1568.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:44.343743: step 50040, loss = 0.70 (1541.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:06:45.166025: step 50050, loss = 0.65 (1556.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:45.987348: step 50060, loss = 0.80 (1558.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:46.815330: step 50070, loss = 0.77 (1545.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:06:47.624124: step 50080, loss = 0.71 (1582.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:06:48.446641: step 50090, loss = 0.71 (1556.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:49.365392: step 50100, loss = 0.63 (1393.2 examples/sec; 0.092 sec/batch)
2017-05-06 22:06:50.084410: step 50110, loss = 0.76 (1780.2 examples/sec; 0.072 sec/batch)
2017-05-06 22:06:50.906978: step 50120, loss = 0.79 (1556.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:51.728001: step 50130, loss = 0.85 (1559.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:52.558615: step 50140, loss = 0.60 (1541.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:06:53.375312: step 50150, loss = 0.85 (1567.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:54.197770: step 50160, loss = 0.83 (1556.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:55.016875: step 50170, loss = 0.71 (1562.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:55.832320: step 50180, loss = 0.69 (1569.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:56.651732: step 50190, loss = 0.73 (1562.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:57.574900: step 50200, loss = 0.73 (1386.5 examples/sec; 0.092 sec/batch)
2017-05-06 22:06:58.302233: step 50210, loss = 0.81 (1759.9 examples/sec; 0.073 sec/batch)
2017-05-06 22:06:59.125870: step 50220, loss = 0.81 (1554.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:06:59.934283: step 50230, loss = 0.67 (1583.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:07:00.754502: step 50240, loss = 0.90 (1560.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:07:01.576316: step 50250, loss = 0.75 (1557.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:07:02.394478: step 50260, loss = 0.76 (1564.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:07:03.208923: step 50270, loss = 0.69 (1571.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:07:04.025466: step 50280, loss = 0.79 (1567.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:07:04.843969: step 50290, loss = 0.72 (1563.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:07:05.762197: step 50300, loss = 0.77 (1394.0 examples/sec; 0.092 sec/batch)
2017-05-06 22:07:06.483601: step 50310, loss = 0.68 (1774.3 examples/sec; 0.072 sec/batch)
2017-05-06 22:07:07.302546: step 50320, loss = 0.71 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:07:08.117528: step 50330, loss = 0.95 (1570.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:07:08.946280: step 50340, loss = 0.83 (1544.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:07:09.778436: step 50350, loss = 0.75 (1538.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:07:10.606234: step 50360, loss = 0.84 (1546.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:07:11.420094: step 50370, loss = 0.84 (1572.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:07:12.246115: step 50380, loss = 0.80 (1549.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:07:13.072408: step 50390, loss = 0.80 (1549.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:07:13.995365: step 50400, loss = 0.69 (1386.9 examples/sec; 0.092 sec/batch)
2017-05-06 22:07:14.710202: step 50410, loss = 0.80 (1790.6 examples/sec; 0.071 sec/batch)
2017-05-06 22:07:15.533203: step 50420, loss = 0.65 (1555.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:07:16.346936: step 50430, loss = 0.71 (1573.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:07:17.169073: step 50440, loss = 0.67 (1557.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:07:17.991941: step 50450, loss = 0.82 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:07:18.812643: step 50460, loss = 0.65 (1559.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:07:19.627020: step 50470, loss = 0.82 (1571.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:07:20.450577: step 50480, loss = 0.81 (1554.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:07:21.295163: step 50490, loss = 0.77 (1515.5 examples/sec; 0.084 sec/batch)
2017-05-06 22:07:22.181080: step 50500, loss = 0.82 (1444.8 examples/sec; 0.089 sec/batch)
2017-05-06 22:07:22.894506: step 50510, loss = 0.74 (1794.1 examples/sec; 0.071 sec/batch)
2017-05-06 22:07:23.710490: step 50520, loss = 0.70 (1568.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:07:24.534105: step 50530, loss = 0.77 (1554.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:07:25.349445: step 50540, loss = 0.72 (1569.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:07:26.179556: step 50550, loss = 0.85 (1542.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:07:27.001295: step 50560, loss = 0.69 (1557.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:07:27.816655: step 50570, loss = 0.66 (1569.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:07:28.634955: step 50580, loss = 0.70 (1564.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:07:29.467378: step 50590, loss = 0.73 (1537.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:07:30.386814: step 50600, loss = 0.86 (1392.2 examples/sec; 0.092 sec/batch)
2017-05-06 22:07:31.118601: step 50610, loss = 0.66 (1749.1 examples/sec; 0.073 sec/batch)
2017-05-06 22:07:31.933340: step 50620, loss = 0.68 (1571.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:07:32.754749: step 50630, loss = 0.68 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:07:33.581385: step 50640, loss = 0.79 (1548.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:07:34.425805: step 50650, loss = 0.87 (1515.8 examples/sec; 0.084 sec/batch)
2017-05-06 22:07:35.237932: step 50660, loss = 0.70 (1576.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:07:36.051095: step 50670, loss = 0.86 (1574.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:07:36.866222: step 50680, loss = 0.67 (1570.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:07:37.690387: step 50690, loss = 0.70 (1553.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:07:38.612295: step 50700, loss = 0.76 (1388.4 examples/sec; 0.092 sec/batch)
2017-05-06 22:07:39.334448: step 50710, loss = 0.67 (1772.5 examples/sec; 0.072 sec/batch)
2017-05-06 22:07:40.158806: step 50720, loss = 0.79 (1552.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:07:40.989747: step 50730, loss = 0.74 (1540.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:07:41.818345: step 50740, loss = 0.75 (1544.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:07:42.644886: step 50750, loss = 0.70 (1548.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:07:43.465787: step 50760, loss = 0.85 (1559.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:07:44.289007: step 50770, loss = 0.74 (1554.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:07:45.103199: step 50780, loss = 0.69 (1572.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:07:45.928879: step 50790, loss = 0.63 (1550.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:07:46.861949: step 50800, loss = 0.75 (1371.8 examples/sec; 0.093 sec/batch)
2017-05-06 22:07:47.576064: step 50810, loss = 0.73 (1792.4 examples/sec; 0.071 sec/batch)
2017-05-06 22:07:48.400942: step 50820, loss = 0.78 (1551.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:07:49.224943: step 50830, loss = 0.72 (1553.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:07:50.050652: step 50840, loss = 0.63 (1550.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:07:50.871874: step 50850, loss = 0.77 (1558.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:07:51.688003: step 50860, loss = 0.74 (1568.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:07:52.506132: step 50870, loss = 0.86 (1564.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:07:53.327768: step 50880, loss = 0.79 (1557.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:07:54.162029: step 50890, loss = 0.65 (1534.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:07:55.083826: step 50900, loss = 0.80 (1388.6 examples/sec; 0.092 sec/batch)
2017-05-06 22:07:55.807367: step 50910, loss = 0.75 (1769.1 examples/sec; 0.072 sec/batch)
2017-05-06 22:07:56.636323: step 50920, loss = 0.69 (1544.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:07:57.464957: step 50930, loss = 0.70 (1544.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:07:58.298821: step 50940, loss = 0.82 (1535.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:07:59.125943: step 50950, loss = 0.75 (1547.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:07:59.942374: step 50960, loss = 0.84 (1567.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:08:00.770460: step 50970, loss = 0.76 (1545.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:08:01.591354: step 50980, loss = 0.65 (1559.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:08:02.415227: step 50990, loss = 0.74 (1553.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:08:03.336975: step 51000, loss = 0.77 (1388.7 examples/sec; 0.092 sec/batch)
2017-05-06 22:08:04.057751: step 51010, loss = 0.65 (1775.9 examples/sec; 0.072 sec/batch)
2017-05-06 22:08:04.886920: step 51020, loss = 0.85 (1543.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:08:05.701590: step 51030, loss = 0.83 (1571.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:08:06.532377: step 51040, loss = 0.80 (1540.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:08:07.358565: step 51050, loss = 0.82 (1549.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:08:08.173874: step 51060, loss = 0.67 (1570.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:08:08.996989: step 51070, loss = 0.79 (1555.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:08:09.824539: step 51080, loss = 0.71 (1546.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:08:10.652002: step 51090, loss = 0.63 (1546.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:08:11.568001: step 51100, loss = 0.88 (1397.4 examples/sec; 0.092 sec/batch)
2017-05-06 22:08:12.280310: step 51110, loss = 0.70 (1797.0 examples/sec; 0.071 sec/batch)
2017-05-06 22:08:13.154906: step 51120, loss = 0.69 (1463.5 examples/sec; 0.087 sec/batch)
2017-05-06 22:08:13.958809: step 51130, loss = 0.73 (1592.2 examples/sec; 0.080 sec/batch)
2017-05-06 22:08:14.785719: step 51140, loss = 0.69 (1547.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:08:15.613028: step 51150, loss = 0.72 (1547.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:08:16.427742: step 51160, loss = 0.85 (1571.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:08:17.243640: step 51170, loss = 0.83 (1568.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:08:18.066291: step 51180, loss = 0.63 (1556.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:08:18.892411: step 51190, loss = 0.99 (1549.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:08:19.810551: step 51200, loss = 0.58 (1394.1 examples/sec; 0.092 sec/batch)
2017-05-06 22:08:20.531350: step 51210, loss = 0.77 (1775.8 examples/sec; 0.072 sec/batch)
2017-05-06 22:08:21.354002: step 51220, loss = 0.80 (1555.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:08:22.172558: step 51230, loss = 0.85 (1563.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:08:22.987164: step 51240, loss = 0.88 (1571.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:08:23.801829: step 51250, loss = 0.73 (1571.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:08:24.628824: step 51260, loss = 0.78 (1547.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:08:25.452865: step 51270, loss = 0.68 (1553.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:08:26.284992: step 51280, loss = 0.80 (1538.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:08:27.098715: step 51290, loss = 0.66 (1573.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:08:28.015372: step 51300, loss = 0.64 (1396.4 examples/sec; 0.092 sec/batch)
2017-05-06 22:08:28.732362: step 51310, loss = 0.80 (1785.2 examples/sec; 0.072 sec/batch)
2017-05-06 22:08:29.547647: step 51320, loss = 0.67 (1570.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:08:30.370419: step 51330, loss = 0.74 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:08:31.197798: step 51340, loss = 0.76 (1547.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:08:32.010443: step 51350, loss = 0.79 (1575.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:08:32.828901: step 51360, loss = 0.60 (1563.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:08:33.646246: step 51370, loss = 0.81 (1566.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:08:34.465788: step 51380, loss = 0.81 (1561.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:08:35.291476: step 51390, loss = 0.80 (1550.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:08:36.205479: step 51400, loss = 0.67 (1400.4 examples/sec; 0.091 sec/batch)
2017-05-06 22:08:36.933598: step 51410, loss = 0.67 (1758.0 examples/sec; 0.073 sec/batch)
2017-05-06 22:08:37.750364: step 51420, loss = 0.79 (1567.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:08:38.577412: step 51430, loss = 0.84 (1547.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:08:39.393308: step 51440, loss = 0.74 (1568.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:08:40.212056: step 51450, loss = 0.93 (1563.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:08:41.035551: step 51460, loss = 0.77 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:08:41.850015: step 51470, loss = 0.84 (1571.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:08:42.701077: step 51480, loss = 0.90 (1504.0 examples/sec; 0.085 sec/batch)
2017-05-06 22:08:43.490893: step 51490, loss = 0.80 (1620.6 examples/sec; 0.079 sec/batch)
2017-05-06 22:08:44.411837: step 51500, loss = 0.81 (1389.9 examples/sec; 0.092 sec/batch)
2017-05-06 22:08:45.135082: step 51510, loss = 0.73 (1769.8 examples/sec; 0.072 sec/batch)
2017-05-06 22:08:45.956766: step 51520, loss = 0.72 (1557.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:08:46.772217: step 51530, loss = 0.76 (1569.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:08:47.585014: step 51540, loss = 0.82 (1574.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:08:48.408285: step 51550, loss = 0.63 (1554.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:08:49.240400: step 51560, loss = 0.84 (1538.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:08:50.055350: step 51570, loss = 0.82 (1570.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:08:50.887845: step 51580, loss = 0.81 (1537.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:08:51.696605: step 51590, loss = 0.78 (1582.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:08:52.619639: step 51600, loss = 0.72 (1386.7 examples/sec; 0.092 sec/batch)
2017-05-06 22:08:53.345615: step 51610, loss = 0.74 (1763.1 examples/sec; 0.073 sec/batch)
2017-05-06 22:08:54.172962: step 51620, loss = 0.71 (1547.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:08:54.990737: step 51630, loss = 0.62 (1565.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:08:55.812842: step 51640, loss = 0.76 (1557.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:08:56.642098: step 51650, loss = 0.99 (1543.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:08:57.461763: step 51660, loss = 0.77 (1561.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:08:58.283500: step 51670, loss = 0.76 (1557.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:08:59.099538: step 51680, loss = 0.73 (1568.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:08:59.913199: step 51690, loss = 0.66 (1573.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:09:00.850670: step 51700, loss = 0.59 (1365.4 examples/sec; 0.094 sec/batch)
2017-05-06 22:09:01.559205: step 51710, loss = 0.70 (1806.5 examples/sec; 0.071 sec/batch)
2017-05-06 22:09:02.384524: step 51720, loss = 0.72 (1550.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:09:03.206208: step 51730, loss = 0.76 (1557.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:04.023021: step 51740, loss = 0.71 (1567.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:04.836831: step 51750, loss = 0.75 (1572.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:09:05.667618: step 51760, loss = 0.86 (1540.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:09:06.477924: step 51770, loss = 0.66 (1579.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:09:07.294889: step 51780, loss = 0.72 (1566.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:08.114418: step 51790, loss = 0.74 (1561.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:09.043478: step 51800, loss = 0.84 (1377.7 examples/sec; 0.093 sec/batch)
2017-05-06 22:09:09.773975: step 51810, loss = 0.83 (1752.3 examples/sec; 0.073 sec/batch)
2017-05-06 22:09:10.597974: step 51820, loss = 0.77 (1553.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:11.414705: step 51830, loss = 0.83 (1567.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:12.235834: step 51840, loss = 0.66 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:13.054422: step 51850, loss = 0.78 (1563.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:13.882598: step 51860, loss = 0.64 (1545.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:09:14.702225: step 51870, loss = 0.78 (1561.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:15.523256: step 51880, loss = 0.78 (1559.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:16.342927: step 51890, loss = 0.66 (1561.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:17.272963: step 51900, loss = 0.74 (1376.3 examples/sec; 0.093 sec/batch)
2017-05-06 22:09:17.990073: step 51910, loss = 0.69 (1785.0 examples/sec; 0.072 sec/batch)
2017-05-06 22:09:18.813210: step 51920, loss = 0.77 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:19.625205: step 51930, loss = 0.83 (1576.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:09:20.446767: step 51940, loss = 0.70 (1558.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:21.272340: step 51950, loss = 0.66 (1550.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:09:22.093099: step 51960, loss = 0.74 (1559.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:22.913743: step 51970, loss = 0.68 (1559.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:23.732662: step 51980, loss = 0.64 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:24.566935: step 51990, loss = 0.84 (1534.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:09:25.480206: step 52000, loss = 0.87 (1401.6 examples/sec; 0.091 sec/batch)
2017-05-06 22:09:26.207338: step 52010, loss = 0.77 (1760.4 examples/sec; 0.073 sec/batch)
2017-05-06 22:09:27.031199: step 52020, loss = 0.89 (1553.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:27.838504: step 52030, loss = 0.71 (1585.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:09:28.657589: step 52040, loss = 0.75 (1562.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:29.481317: step 52050, loss = 0.69 (1553.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:30.315864: step 52060, loss = 0.91 (1533.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:09:31.132120: step 52070, loss = 0.63 (1568.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:31.948894: step 52080, loss = 0.66 (1567.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:32.764087: step 52090, loss = 0.68 (1570.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:33.687401: step 52100, loss = 0.68 (1386.3 examples/sec; 0.092 sec/batch)
2017-05-06 22:09:34.406868: step 52110, loss = 0.96 (1779.1 examples/sec; 0.072 sec/batch)
2017-05-06 22:09:35.222006: step 52120, loss = 0.71 (1570.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:36.037294: step 52130, loss = 0.66 (1570.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:36.863200: step 52140, loss = 0.68 (1549.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:09:37.681091: step 52150, loss = 0.76 (1565.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:38.502024: step 52160, loss = 0.83 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:39.326986: step 52170, loss = 0.60 (1551.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:40.146112: step 52180, loss = 0.77 (1562.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:40.967998: step 52190, loss = 0.87 (1557.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:41.884684: step 52200, loss = 0.82 (1396.3 examples/sec; 0.092 sec/batch)
2017-05-06 22:09:42.612436: step 52210, loss = 0.73 (1758.8 examples/sec; 0.073 sec/batch)
2017-05-06 22:09:43.422324: step 52220, loss = 0.75 (1580.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:09:44.248109: step 52230, loss = 0.74 (1550.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:09:45.073281: step 52240, loss = 0.81 (1551.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:09:45.903127: step 52250, loss = 0.85 (1542.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:09:46.721841: step 52260, loss = 0.71 (1563.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:47.540770: step 52270, loss = 0.73 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:48.366875: step 52280, loss = 0.64 (1549.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:09:49.179861: step 52290, loss = 0.80 (1574.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:09:50.106541: step 52300, loss = 0.84 (1381.3 examples/sec; 0.093 sec/batch)
2017-05-06 22:09:50.834350: step 52310, loss = 0.73 (1758.7 examples/sec; 0.073 sec/batch)
2017-05-06 22:09:51.649028: step 52320, loss = 0.88 (1571.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:09:52.477964: step 52330, loss = 0.64 (1544.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:09:53.305589: step 52340, loss = 0.92 (1546.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:09:54.128847: step 52350, loss = 0.82 (1554.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:54.949905: step 52360, loss = 0.71 (1559.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:55.766991: step 52370, loss = 0.88 (1566.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:09:56.596112: step 52380, loss = 0.58 (1543.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:09:57.427549: step 52390, loss = 0.66 (1539.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:09:58.346984: step 52400, loss = 0.76 (1392.2 examples/sec; 0.092 sec/batch)
2017-05-06 22:09:59.072205: step 52410, loss = 0.72 (1765.0 examples/sec; 0.073 sec/batch)
2017-05-06 22:09:59.884998: step 52420, loss = 0.92 (1574.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:10:00.702670: step 52430, loss = 0.76 (1565.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:01.527666: step 52440, loss = 0.76 (1551.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:10:02.368577: step 52450, loss = 0.68 (1522.2 examples/sec; 0.084 sec/batch)
2017-05-06 22:10:03.190855: step 52460, loss = 0.74 (1556.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:04.025920: step 52470, loss = 0.69 (1532.8 examples/sec; 0.084 sec/batch)
2017-05-06 22:10:04.819614: step 52480, loss = 0.64 (1612.7 examples/sec; 0.079 sec/batch)
2017-05-06 22:10:05.632748: step 52490, loss = 0.63 (1574.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:10:06.572536: step 52500, loss = 0.66 (1362.0 examples/sec; 0.094 sec/batch)
2017-05-06 22:10:07.289124: step 52510, loss = 0.73 (1786.3 examples/sec; 0.072 sec/batch)
2017-05-06 22:10:08.100878: step 52520, loss = 0.79 (1576.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:10:08.919708: step 52530, loss = 0.68 (1563.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:09.732499: step 52540, loss = 0.67 (1574.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:10:10.553272: step 52550, loss = 0.70 (1559.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:11.370457: step 52560, loss = 0.76 (1566.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:12.193915: step 52570, loss = 0.77 (1554.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:13.014231: step 52580, loss = 0.69 (1560.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:13.832764: step 52590, loss = 0.68 (1563.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:14.759900: step 52600, loss = 0.70 (1380.6 examples/sec; 0.093 sec/batch)
2017-05-06 22:10:15.475541: step 52610, loss = 0.82 (1788.6 examples/sec; 0.072 sec/batch)
2017-05-06 22:10:16.288448: step 52620, loss = 0.87 (1574.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:10:17.111607: step 52630, loss = 0.73 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:17.930478: step 52640, loss = 0.91 (1563.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:18.743835: step 52650, loss = 0.87 (1573.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:10:19.561802: step 52660, loss = 0.68 (1564.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:20.382390: step 52670, loss = 0.83 (1559.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:21.204932: step 52680, loss = 0.63 (1556.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:22.022088: step 52690, loss = 0.76 (1566.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:22.933918: step 52700, loss = 0.73 (1403.8 examples/sec; 0.091 sec/batch)
2017-05-06 22:10:23.650107: step 52710, loss = 0.72 (1787.2 examples/sec; 0.072 sec/batch)
2017-05-06 22:10:24.474819: step 52720, loss = 0.65 (1552.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:25.291737: step 52730, loss = 0.63 (1566.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:26.116391: step 52740, loss = 0.73 (1552.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:26.942670: step 52750, loss = 0.80 (1549.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:10:27.748699: step 52760, loss = 0.78 (1588.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:10:28.581621: step 52770, loss = 0.68 (1536.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:10:29.400515: step 52780, loss = 0.78 (1563.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:30.219455: step 52790, loss = 0.71 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:31.145258: step 52800, loss = 0.78 (1382.6 examples/sec; 0.093 sec/batch)
2017-05-06 22:10:31.851200: step 52810, loss = 0.80 (1813.2 examples/sec; 0.071 sec/batch)
2017-05-06 22:10:32.670035: step 52820, loss = 0.79 (1563.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:33.501021: step 52830, loss = 0.78 (1540.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:10:34.325577: step 52840, loss = 0.66 (1552.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:35.147077: step 52850, loss = 0.81 (1558.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:35.964176: step 52860, loss = 0.79 (1566.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:36.788065: step 52870, loss = 0.67 (1553.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:37.612000: step 52880, loss = 0.74 (1553.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:38.426101: step 52890, loss = 0.85 (1572.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:10:39.350896: step 52900, loss = 0.83 (1384.1 examples/sec; 0.092 sec/batch)
2017-05-06 22:10:40.063551: step 52910, loss = 0.75 (1796.1 examples/sec; 0.071 sec/batch)
2017-05-06 22:10:40.884228: step 52920, loss = 0.70 (1559.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:41.701728: step 52930, loss = 0.74 (1565.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:42.527305: step 52940, loss = 0.70 (1550.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:10:43.354289: step 52950, loss = 0.63 (1547.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:10:44.176415: step 52960, loss = 0.76 (1556.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:45.005698: step 52970, loss = 0.77 (1543.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:10:45.822660: step 52980, loss = 0.80 (1566.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:46.644208: step 52990, loss = 0.92 (1558.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:47.566434: step 53000, loss = 0.65 (1388.0 examples/sec; 0.092 sec/batch)
2017-05-06 22:10:48.291474: step 53010, loss = 0.78 (1765.4 examples/sec; 0.073 sec/batch)
2017-05-06 22:10:49.114598: step 53020, loss = 0.62 (1555.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:49.943163: step 53030, loss = 0.74 (1544.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:10:50.759880: step 53040, loss = 0.71 (1567.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:51.581254: step 53050, loss = 0.61 (1558.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:52.409362: step 53060, loss = 0.70 (1545.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:10:53.232910: step 53070, loss = 0.86 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:54.056511: step 53080, loss = 0.64 (1554.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:54.871454: step 53090, loss = 0.69 (1570.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:10:55.785966: step 53100, loss = 0.68 (1399.6 examples/sec; 0.091 sec/batch)
2017-05-06 22:10:56.516037: step 53110, loss = 0.61 (1753.3 examples/sec; 0.073 sec/batch)
2017-05-06 22:10:57.344719: step 53120, loss = 0.70 (1544.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:10:58.169198: step 53130, loss = 0.75 (1552.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:10:58.997870: step 53140, loss = 0.91 (1544.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:10:59.812294: step 53150, loss = 0.68 (1571.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:11:00.640271: step 53160, loss = 0.71 (1545.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:11:01.470932: step 53170, loss = 0.60 (1540.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:11:02.293463: step 53180, loss = 0.73 (1556.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:11:03.122197: step 53190, loss = 0.78 (1544.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:11:04.039590: step 53200, loss = 0.78 (1395.3 examples/sec; 0.092 sec/batch)
2017-05-06 22:11:04.763124: step 53210, loss = 0.59 (1769.1 examples/sec; 0.072 sec/batch)
2017-05-06 22:11:05.584679: step 53220, loss = 0.59 (1558.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:11:06.407123: step 53230, loss = 0.87 (1556.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:11:07.230646: step 53240, loss = 0.79 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:11:08.049842: step 53250, loss = 0.88 (1562.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:11:08.869939: step 53260, loss = 0.71 (1560.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:11:09.702696: step 53270, loss = 0.67 (1537.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:11:10.529757: step 53280, loss = 0.75 (1547.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:11:11.348429: step 53290, loss = 0.72 (1563.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:11:12.269662: step 53300, loss = 0.51 (1389.4 examples/sec; 0.092 sec/batch)
2017-05-06 22:11:12.988154: step 53310, loss = 0.69 (1781.5 examples/sec; 0.072 sec/batch)
2017-05-06 22:11:13.830793: step 53320, loss = 0.80 (1519.1 examples/sec; 0.084 sec/batch)
2017-05-06 22:11:14.655590: step 53330, loss = 0.79 (1551.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:11:15.478344: step 53340, loss = 0.71 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:11:16.307889: step 53350, loss = 0.72 (1543.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:11:17.135022: step 53360, loss = 0.81 (1547.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:11:17.954177: step 53370, loss = 0.77 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:11:18.792630: step 53380, loss = 0.78 (1526.6 examples/sec; 0.084 sec/batch)
2017-05-06 22:11:19.612543: step 53390, loss = 0.66 (1561.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:11:20.537020: step 53400, loss = 0.64 (1384.6 examples/sec; 0.092 sec/batch)
2017-05-06 22:11:21.265892: step 53410, loss = 0.81 (1756.2 examples/sec; 0.073 sec/batch)
2017-05-06 22:11:22.093904: step 53420, loss = 0.66 (1545.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:11:22.924275: step 53430, loss = 0.85 (1541.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:11:23.737342: step 53440, loss = 0.62 (1574.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:11:24.556920: step 53450, loss = 0.95 (1561.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:11:25.409509: step 53460, loss = 0.73 (1501.3 examples/sec; 0.085 sec/batch)
2017-05-06 22:11:26.215385: step 53470, loss = 0.78 (1588.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:11:27.038622: step 53480, loss = 0.65 (1554.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:11:27.857366: step 53490, loss = 0.80 (1563.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:11:28.781003: step 53500, loss = 0.75 (1385.8 examples/sec; 0.092 sec/batch)
2017-05-06 22:11:29.502743: step 53510, loss = 0.75 (1773.5 examples/sec; 0.072 sec/batch)
2017-05-06 22:11:30.335609: step 53520, loss = 0.80 (1536.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:11:31.167105: step 53530, loss = 0.82 (1539.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:11:31.980177: step 53540, loss = 0.74 (1574.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:11:32.798691: step 53550, loss = 0.74 (1563.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:11:33.630644: step 53560, loss = 0.87 (1538.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:11:34.456835: step 53570, loss = 0.79 (1549.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:11:35.282925: step 53580, loss = 0.79 (1549.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:11:36.101021: step 53590, loss = 0.85 (1564.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:11:37.026651: step 53600, loss = 0.65 (1382.8 examples/sec; 0.093 sec/batch)
2017-05-06 22:11:37.760901: step 53610, loss = 0.64 (1743.3 examples/sec; 0.073 sec/batch)
2017-05-06 22:11:38.587612: step 53620, loss = 0.79 (1548.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:11:39.413752: step 53630, loss = 0.72 (1549.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:11:40.232967: step 53640, loss = 0.74 (1562.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:11:41.064959: step 53650, loss = 0.86 (1538.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:11:41.892409: step 53660, loss = 0.86 (1547.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:11:42.709347: step 53670, loss = 0.67 (1566.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:11:43.534098: step 53680, loss = 0.71 (1552.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:11:44.361035: step 53690, loss = 0.68 (1547.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:11:45.299256: step 53700, loss = 0.85 (1364.3 examples/sec; 0.094 sec/batch)
2017-05-06 22:11:46.016661: step 53710, loss = 0.73 (1784.2 examples/sec; 0.072 sec/batch)
2017-05-06 22:11:46.840413: step 53720, loss = 0.76 (1553.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:11:47.656095: step 53730, loss = 0.73 (1569.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:11:48.482838: step 53740, loss = 0.69 (1548.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:11:49.303625: step 53750, loss = 0.84 (1559.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:11:50.137447: step 53760, loss = 0.70 (1535.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:11:50.968741: step 53770, loss = 0.75 (1539.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:11:51.784706: step 53780, loss = 0.58 (1568.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:11:52.604246: step 53790, loss = 0.82 (1561.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:11:53.542360: step 53800, loss = 0.76 (1364.4 examples/sec; 0.094 sec/batch)
2017-05-06 22:11:54.258397: step 53810, loss = 0.87 (1787.6 examples/sec; 0.072 sec/batch)
2017-05-06 22:11:55.077995: step 53820, loss = 0.77 (1561.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:11:55.905015: step 53830, loss = 0.72 (1547.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:11:56.735157: step 53840, loss = 0.78 (1541.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:11:57.566563: step 53850, loss = 0.75 (1539.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:11:58.391447: step 53860, loss = 0.97 (1551.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:11:59.213429: step 53870, loss = 0.74 (1557.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:12:00.032522: step 53880, loss = 0.70 (1562.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:12:00.853922: step 53890, loss = 0.84 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:12:01.795392: step 53900, loss = 0.74 (1359.6 examples/sec; 0.094 sec/batch)
2017-05-06 22:12:02.507900: step 53910, loss = 0.91 (1796.5 examples/sec; 0.071 sec/batch)
2017-05-06 22:12:03.332697: step 53920, loss = 0.85 (1551.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:12:04.152628: step 53930, loss = 0.77 (1561.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:12:04.979685: step 53940, loss = 0.64 (1547.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:12:05.804264: step 53950, loss = 0.69 (1552.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:12:06.631757: step 53960, loss = 0.89 (1546.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:12:07.453640: step 53970, loss = 0.79 (1557.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:12:08.268336: step 53980, loss = 0.69 (1571.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:12:09.096112: step 53990, loss = 0.70 (1546.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:12:10.036182: step 54000, loss = 0.72 (1361.6 examples/sec; 0.094 sec/batch)
2017-05-06 22:12:10.742889: step 54010, loss = 0.76 (1811.2 examples/sec; 0.071 sec/batch)
2017-05-06 22:12:11.559796: step 54020, loss = 0.62 (1566.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:12:12.378574: step 54030, loss = 0.71 (1563.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:12:13.201859: step 54040, loss = 0.62 (1554.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:12:14.019342: step 54050, loss = 0.71 (1565.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:12:14.852158: step 54060, loss = 0.84 (1537.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:12:15.672014: step 54070, loss = 0.84 (1561.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:12:16.502525: step 54080, loss = 0.72 (1541.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:12:17.336466: step 54090, loss = 0.85 (1534.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:12:18.258068: step 54100, loss = 0.70 (1388.9 examples/sec; 0.092 sec/batch)
2017-05-06 22:12:18.985063: step 54110, loss = 0.65 (1760.6 examples/sec; 0.073 sec/batch)
2017-05-06 22:12:19.791838: step 54120, loss = 0.62 (1586.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:12:20.621447: step 54130, loss = 0.68 (1542.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:12:21.444268: step 54140, loss = 0.79 (1555.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:12:22.270546: step 54150, loss = 0.72 (1549.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:12:23.102349: step 54160, loss = 1.00 (1538.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:12:23.916817: step 54170, loss = 0.96 (1571.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:12:24.740551: step 54180, loss = 0.73 (1553.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:12:25.563874: step 54190, loss = 0.77 (1554.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:12:26.498436: step 54200, loss = 0.66 (1369.6 examples/sec; 0.093 sec/batch)
2017-05-06 22:12:27.227791: step 54210, loss = 0.82 (1755.0 examples/sec; 0.073 sec/batch)
2017-05-06 22:12:28.039701: step 54220, loss = 0.76 (1576.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:12:28.862403: step 54230, loss = 0.68 (1555.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:12:29.697599: step 54240, loss = 0.79 (1532.6 examples/sec; 0.084 sec/batch)
2017-05-06 22:12:30.522743: step 54250, loss = 0.96 (1551.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:12:31.348115: step 54260, loss = 0.68 (1550.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:12:32.171665: step 54270, loss = 0.68 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:12:32.997242: step 54280, loss = 0.68 (1550.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:12:33.823136: step 54290, loss = 0.65 (1549.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:12:34.751926: step 54300, loss = 0.77 (1378.1 examples/sec; 0.093 sec/batch)
2017-05-06 22:12:35.467580: step 54310, loss = 0.76 (1788.6 examples/sec; 0.072 sec/batch)
2017-05-06 22:12:36.286183: step 54320, loss = 0.69 (1563.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:12:37.104735: step 54330, loss = 0.63 (1563.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:12:37.925032: step 54340, loss = 0.71 (1560.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:12:38.749483: step 54350, loss = 0.65 (1552.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:12:39.566098: step 54360, loss = 0.73 (1567.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:12:40.389128: step 54370, loss = 0.69 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:12:41.211246: step 54380, loss = 0.87 (1556.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:12:42.035017: step 54390, loss = 0.70 (1553.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:12:42.958385: step 54400, loss = 0.78 (1386.2 examples/sec; 0.092 sec/batch)
2017-05-06 22:12:43.685226: step 54410, loss = 0.71 (1761.1 examples/sec; 0.073 sec/batch)
2017-05-06 22:12:44.510877: step 54420, loss = 0.73 (1550.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:12:45.339431: step 54430, loss = 0.73 (1544.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:12:46.159689: step 54440, loss = 0.77 (1560.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:12:47.006834: step 54450, loss = 0.71 (1511.0 examples/sec; 0.085 sec/batch)
2017-05-06 22:12:47.804401: step 54460, loss = 0.78 (1604.9 examples/sec; 0.080 sec/batch)
2017-05-06 22:12:48.630530: step 54470, loss = 0.80 (1549.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:12:49.453095: step 54480, loss = 0.81 (1556.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:12:50.277696: step 54490, loss = 0.75 (1552.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:12:51.206330: step 54500, loss = 0.81 (1378.4 examples/sec; 0.093 sec/batch)
2017-05-06 22:12:51.924712: step 54510, loss = 0.70 (1781.8 examples/sec; 0.072 sec/batch)
2017-05-06 22:12:52.755109: step 54520, loss = 0.75 (1541.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:12:53.585689: step 54530, loss = 0.93 (1541.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:12:54.407474: step 54540, loss = 0.77 (1557.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:12:55.239408: step 54550, loss = 0.85 (1538.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:12:56.050238: step 54560, loss = 0.62 (1578.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:12:56.871918: step 54570, loss = 0.82 (1557.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:12:57.688499: step 54580, loss = 0.92 (1567.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:12:58.507417: step 54590, loss = 0.59 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:12:59.426687: step 54600, loss = 0.90 (1392.4 examples/sec; 0.092 sec/batch)
2017-05-06 22:13:00.151366: step 54610, loss = 0.79 (1766.3 examples/sec; 0.072 sec/batch)
2017-05-06 22:13:00.986539: step 54620, loss = 0.71 (1532.6 examples/sec; 0.084 sec/batch)
2017-05-06 22:13:01.808009: step 54630, loss = 1.00 (1558.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:13:02.635919: step 54640, loss = 0.67 (1546.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:13:03.467762: step 54650, loss = 0.85 (1538.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:13:04.288926: step 54660, loss = 0.76 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:13:05.114056: step 54670, loss = 0.72 (1551.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:13:05.932653: step 54680, loss = 0.69 (1563.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:13:06.755204: step 54690, loss = 0.60 (1556.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:13:07.673436: step 54700, loss = 0.76 (1394.0 examples/sec; 0.092 sec/batch)
2017-05-06 22:13:08.391220: step 54710, loss = 0.64 (1783.3 examples/sec; 0.072 sec/batch)
2017-05-06 22:13:09.218111: step 54720, loss = 0.73 (1548.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:13:10.043464: step 54730, loss = 0.67 (1550.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:13:10.874112: step 54740, loss = 0.69 (1541.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:13:11.682319: step 54750, loss = 0.69 (1583.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:13:12.512680: step 54760, loss = 0.76 (1541.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:13:13.335195: step 54770, loss = 0.68 (1556.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:13:14.163838: step 54780, loss = 0.80 (1544.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:13:14.978912: step 54790, loss = 0.77 (1570.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:13:15.901750: step 54800, loss = 0.70 (1387.0 examples/sec; 0.092 sec/batch)
2017-05-06 22:13:16.629652: step 54810, loss = 0.81 (1758.5 examples/sec; 0.073 sec/batch)
2017-05-06 22:13:17.449377: step 54820, loss = 0.78 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:13:18.281408: step 54830, loss = 0.60 (1538.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:13:19.108967: step 54840, loss = 0.73 (1546.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:13:19.933845: step 54850, loss = 0.67 (1551.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:13:20.754388: step 54860, loss = 0.69 (1559.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:13:21.578630: step 54870, loss = 0.90 (1552.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:13:22.399157: step 54880, loss = 0.83 (1560.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:13:23.226105: step 54890, loss = 0.76 (1547.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:13:24.148939: step 54900, loss = 0.67 (1387.0 examples/sec; 0.092 sec/batch)
2017-05-06 22:13:24.872141: step 54910, loss = 0.98 (1769.9 examples/sec; 0.072 sec/batch)
2017-05-06 22:13:25.699999: step 54920, loss = 0.84 (1546.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:13:26.523928: step 54930, loss = 0.93 (1553.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:13:27.350227: step 54940, loss = 0.67 (1549.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:13:28.168117: step 54950, loss = 0.86 (1565.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:13:28.999803: step 54960, loss = 0.65 (1539.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:13:29.822367: step 54970, loss = 0.72 (1556.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:13:30.646291: step 54980, loss = 0.83 (1553.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:13:31.466306: step 54990, loss = 0.58 (1560.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:13:32.384916: step 55000, loss = 0.79 (1393.4 examples/sec; 0.092 sec/batch)
2017-05-06 22:13:33.112422: step 55010, loss = 0.81 (1759.4 examples/sec; 0.073 sec/batch)
2017-05-06 22:13:33.929744: step 55020, loss = 0.64 (1566.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:13:34.756809: step 55030, loss = 0.62 (1547.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:13:35.569645: step 55040, loss = 0.82 (1574.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:13:36.388916: step 55050, loss = 0.73 (1562.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:13:37.225218: step 55060, loss = 0.84 (1530.5 examples/sec; 0.084 sec/batch)
2017-05-06 22:13:38.052749: step 55070, loss = 0.89 (1546.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:13:38.879465: step 55080, loss = 0.65 (1548.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:13:39.697195: step 55090, loss = 0.74 (1565.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:13:40.627832: step 55100, loss = 0.84 (1375.4 examples/sec; 0.093 sec/batch)
2017-05-06 22:13:41.341299: step 55110, loss = 0.80 (1794.1 examples/sec; 0.071 sec/batch)
2017-05-06 22:13:42.164511: step 55120, loss = 0.72 (1554.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:13:42.989016: step 55130, loss = 0.74 (1552.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:13:43.812832: step 55140, loss = 0.64 (1553.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:13:44.631949: step 55150, loss = 0.74 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:13:45.460055: step 55160, loss = 0.69 (1545.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:13:46.293780: step 55170, loss = 0.67 (1535.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:13:47.122782: step 55180, loss = 0.72 (1544.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:13:47.939126: step 55190, loss = 0.69 (1568.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:13:48.870619: step 55200, loss = 0.64 (1374.1 examples/sec; 0.093 sec/batch)
2017-05-06 22:13:49.587473: step 55210, loss = 0.67 (1785.6 examples/sec; 0.072 sec/batch)
2017-05-06 22:13:50.416102: step 55220, loss = 0.67 (1544.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:13:51.235633: step 55230, loss = 0.59 (1561.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:13:52.048906: step 55240, loss = 0.95 (1573.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:13:52.884641: step 55250, loss = 0.67 (1531.6 examples/sec; 0.084 sec/batch)
2017-05-06 22:13:53.704525: step 55260, loss = 0.73 (1561.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:13:54.531904: step 55270, loss = 0.74 (1547.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:13:55.354374: step 55280, loss = 0.69 (1556.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:13:56.170613: step 55290, loss = 0.68 (1568.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:13:57.092978: step 55300, loss = 0.84 (1387.7 examples/sec; 0.092 sec/batch)
2017-05-06 22:13:57.821878: step 55310, loss = 0.77 (1756.1 examples/sec; 0.073 sec/batch)
2017-05-06 22:13:58.645702: step 55320, loss = 0.78 (1553.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:13:59.470691: step 55330, loss = 0.77 (1551.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:00.291324: step 55340, loss = 0.71 (1559.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:01.113103: step 55350, loss = 0.66 (1557.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:01.935993: step 55360, loss = 0.83 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:02.755983: step 55370, loss = 0.72 (1561.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:03.571374: step 55380, loss = 0.84 (1569.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:04.391666: step 55390, loss = 0.65 (1560.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:05.311489: step 55400, loss = 0.81 (1391.6 examples/sec; 0.092 sec/batch)
2017-05-06 22:14:06.032636: step 55410, loss = 0.75 (1775.0 examples/sec; 0.072 sec/batch)
2017-05-06 22:14:06.854494: step 55420, loss = 0.80 (1557.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:07.664743: step 55430, loss = 0.68 (1579.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:14:08.513548: step 55440, loss = 0.84 (1508.0 examples/sec; 0.085 sec/batch)
2017-05-06 22:14:09.310429: step 55450, loss = 0.81 (1606.3 examples/sec; 0.080 sec/batch)
2017-05-06 22:14:10.130133: step 55460, loss = 0.67 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:10.950370: step 55470, loss = 0.71 (1560.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:11.770364: step 55480, loss = 0.72 (1561.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:12.588626: step 55490, loss = 0.68 (1564.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:13.528511: step 55500, loss = 0.85 (1361.9 examples/sec; 0.094 sec/batch)
2017-05-06 22:14:14.215455: step 55510, loss = 0.74 (1863.3 examples/sec; 0.069 sec/batch)
2017-05-06 22:14:15.032977: step 55520, loss = 0.72 (1565.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:15.843547: step 55530, loss = 0.73 (1579.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:14:16.677327: step 55540, loss = 0.68 (1535.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:14:17.495504: step 55550, loss = 0.74 (1564.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:18.322063: step 55560, loss = 0.65 (1548.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:14:19.148135: step 55570, loss = 0.61 (1549.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:14:19.957661: step 55580, loss = 0.81 (1581.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:14:20.772528: step 55590, loss = 0.70 (1570.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:14:21.695156: step 55600, loss = 0.88 (1387.3 examples/sec; 0.092 sec/batch)
2017-05-06 22:14:22.418370: step 55610, loss = 0.69 (1769.9 examples/sec; 0.072 sec/batch)
2017-05-06 22:14:23.240871: step 55620, loss = 0.61 (1556.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:24.058771: step 55630, loss = 0.61 (1565.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:24.877467: step 55640, loss = 0.73 (1563.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:25.691602: step 55650, loss = 0.74 (1572.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:14:26.511492: step 55660, loss = 0.65 (1561.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:27.326899: step 55670, loss = 0.67 (1569.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:28.140837: step 55680, loss = 0.69 (1572.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:14:28.959904: step 55690, loss = 0.77 (1562.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:29.876510: step 55700, loss = 0.87 (1396.5 examples/sec; 0.092 sec/batch)
2017-05-06 22:14:30.603749: step 55710, loss = 0.67 (1760.1 examples/sec; 0.073 sec/batch)
2017-05-06 22:14:31.427986: step 55720, loss = 0.65 (1552.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:32.244221: step 55730, loss = 0.75 (1568.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:33.060881: step 55740, loss = 0.79 (1567.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:33.877916: step 55750, loss = 0.80 (1566.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:34.695191: step 55760, loss = 0.60 (1566.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:35.516006: step 55770, loss = 0.80 (1559.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:36.330313: step 55780, loss = 0.76 (1571.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:14:37.145608: step 55790, loss = 0.67 (1570.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:38.059219: step 55800, loss = 0.71 (1401.0 examples/sec; 0.091 sec/batch)
2017-05-06 22:14:38.780687: step 55810, loss = 0.64 (1774.2 examples/sec; 0.072 sec/batch)
2017-05-06 22:14:39.597208: step 55820, loss = 0.79 (1567.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:40.421875: step 55830, loss = 0.78 (1552.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:41.248500: step 55840, loss = 0.77 (1548.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:14:42.065652: step 55850, loss = 0.74 (1566.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:42.884651: step 55860, loss = 0.70 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:43.706928: step 55870, loss = 0.67 (1556.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:44.528948: step 55880, loss = 0.69 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:45.352290: step 55890, loss = 0.83 (1554.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:46.285391: step 55900, loss = 0.70 (1371.8 examples/sec; 0.093 sec/batch)
2017-05-06 22:14:46.994990: step 55910, loss = 0.79 (1803.8 examples/sec; 0.071 sec/batch)
2017-05-06 22:14:47.807927: step 55920, loss = 0.71 (1574.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:14:48.638577: step 55930, loss = 0.83 (1541.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:14:49.474828: step 55940, loss = 0.80 (1530.7 examples/sec; 0.084 sec/batch)
2017-05-06 22:14:50.299522: step 55950, loss = 0.74 (1552.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:51.115062: step 55960, loss = 0.73 (1569.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:51.937566: step 55970, loss = 0.70 (1556.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:52.767109: step 55980, loss = 0.70 (1543.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:14:53.589308: step 55990, loss = 0.73 (1556.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:54.520560: step 56000, loss = 0.77 (1374.5 examples/sec; 0.093 sec/batch)
2017-05-06 22:14:55.242542: step 56010, loss = 0.76 (1772.9 examples/sec; 0.072 sec/batch)
2017-05-06 22:14:56.049867: step 56020, loss = 0.69 (1585.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:14:56.873950: step 56030, loss = 0.73 (1553.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:57.692918: step 56040, loss = 0.67 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:14:58.519804: step 56050, loss = 0.61 (1548.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:14:59.343153: step 56060, loss = 0.93 (1554.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:00.153874: step 56070, loss = 0.77 (1578.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:15:00.967192: step 56080, loss = 0.60 (1573.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:15:01.795651: step 56090, loss = 0.74 (1545.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:15:02.732982: step 56100, loss = 0.84 (1365.6 examples/sec; 0.094 sec/batch)
2017-05-06 22:15:03.437409: step 56110, loss = 0.73 (1817.1 examples/sec; 0.070 sec/batch)
2017-05-06 22:15:04.253824: step 56120, loss = 0.99 (1567.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:05.071826: step 56130, loss = 0.86 (1564.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:05.907804: step 56140, loss = 0.76 (1531.1 examples/sec; 0.084 sec/batch)
2017-05-06 22:15:06.738664: step 56150, loss = 0.84 (1540.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:15:07.562511: step 56160, loss = 0.76 (1553.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:08.377956: step 56170, loss = 0.96 (1569.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:09.201207: step 56180, loss = 0.67 (1554.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:10.021261: step 56190, loss = 0.79 (1560.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:10.945171: step 56200, loss = 0.81 (1385.4 examples/sec; 0.092 sec/batch)
2017-05-06 22:15:11.652772: step 56210, loss = 0.69 (1808.9 examples/sec; 0.071 sec/batch)
2017-05-06 22:15:12.474706: step 56220, loss = 0.76 (1557.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:13.302122: step 56230, loss = 0.73 (1547.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:15:14.125148: step 56240, loss = 0.66 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:14.946879: step 56250, loss = 0.80 (1557.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:15.775997: step 56260, loss = 0.73 (1543.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:15:16.600907: step 56270, loss = 0.79 (1551.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:17.416551: step 56280, loss = 0.86 (1569.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:18.240665: step 56290, loss = 0.94 (1553.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:19.163294: step 56300, loss = 0.73 (1387.3 examples/sec; 0.092 sec/batch)
2017-05-06 22:15:19.871431: step 56310, loss = 0.82 (1807.6 examples/sec; 0.071 sec/batch)
2017-05-06 22:15:20.688486: step 56320, loss = 0.63 (1566.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:21.508836: step 56330, loss = 0.69 (1560.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:22.318961: step 56340, loss = 0.77 (1580.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:15:23.139955: step 56350, loss = 0.62 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:23.957196: step 56360, loss = 0.83 (1566.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:24.781541: step 56370, loss = 0.80 (1552.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:25.601670: step 56380, loss = 0.68 (1560.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:26.423405: step 56390, loss = 0.71 (1557.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:27.337617: step 56400, loss = 0.82 (1400.1 examples/sec; 0.091 sec/batch)
2017-05-06 22:15:28.056072: step 56410, loss = 0.67 (1781.7 examples/sec; 0.072 sec/batch)
2017-05-06 22:15:28.885677: step 56420, loss = 0.76 (1542.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:15:29.732714: step 56430, loss = 0.62 (1511.1 examples/sec; 0.085 sec/batch)
2017-05-06 22:15:30.523992: step 56440, loss = 0.69 (1617.6 examples/sec; 0.079 sec/batch)
2017-05-06 22:15:31.333862: step 56450, loss = 0.70 (1580.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:15:32.154278: step 56460, loss = 0.71 (1560.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:32.973871: step 56470, loss = 0.80 (1561.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:33.795874: step 56480, loss = 0.73 (1557.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:34.624931: step 56490, loss = 0.74 (1543.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:15:35.555275: step 56500, loss = 0.78 (1375.8 examples/sec; 0.093 sec/batch)
2017-05-06 22:15:36.266474: step 56510, loss = 0.67 (1799.8 examples/sec; 0.071 sec/batch)
2017-05-06 22:15:37.090866: step 56520, loss = 0.77 (1552.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:37.928022: step 56530, loss = 0.87 (1529.0 examples/sec; 0.084 sec/batch)
2017-05-06 22:15:38.750695: step 56540, loss = 0.73 (1555.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:39.565223: step 56550, loss = 0.81 (1571.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:15:40.391134: step 56560, loss = 0.60 (1549.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:15:41.225725: step 56570, loss = 0.70 (1533.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:15:42.053545: step 56580, loss = 0.85 (1546.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:15:42.884876: step 56590, loss = 0.84 (1539.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:15:43.806735: step 56600, loss = 0.58 (1388.5 examples/sec; 0.092 sec/batch)
2017-05-06 22:15:44.525674: step 56610, loss = 0.54 (1780.4 examples/sec; 0.072 sec/batch)
2017-05-06 22:15:45.356446: step 56620, loss = 0.73 (1540.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:15:46.176453: step 56630, loss = 0.73 (1561.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:47.006942: step 56640, loss = 0.81 (1541.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:15:47.820208: step 56650, loss = 0.67 (1573.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:15:48.640439: step 56660, loss = 0.84 (1560.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:49.466699: step 56670, loss = 0.94 (1549.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:15:50.292984: step 56680, loss = 0.75 (1549.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:15:51.114077: step 56690, loss = 0.81 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:52.017647: step 56700, loss = 0.79 (1416.6 examples/sec; 0.090 sec/batch)
2017-05-06 22:15:52.740273: step 56710, loss = 0.78 (1771.3 examples/sec; 0.072 sec/batch)
2017-05-06 22:15:53.565893: step 56720, loss = 0.63 (1550.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:15:54.383095: step 56730, loss = 0.76 (1566.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:55.203628: step 56740, loss = 0.76 (1560.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:56.019299: step 56750, loss = 0.67 (1569.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:56.838069: step 56760, loss = 0.67 (1563.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:57.664924: step 56770, loss = 0.66 (1548.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:15:58.488112: step 56780, loss = 0.85 (1554.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:15:59.304441: step 56790, loss = 0.80 (1568.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:16:00.218747: step 56800, loss = 0.76 (1400.0 examples/sec; 0.091 sec/batch)
2017-05-06 22:16:00.934968: step 56810, loss = 0.65 (1787.1 examples/sec; 0.072 sec/batch)
2017-05-06 22:16:01.756934: step 56820, loss = 0.81 (1557.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:16:02.589110: step 56830, loss = 0.73 (1538.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:16:03.408036: step 56840, loss = 0.73 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:16:04.230094: step 56850, loss = 0.64 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:16:05.056361: step 56860, loss = 0.64 (1549.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:16:05.883533: step 56870, loss = 0.81 (1547.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:16:06.698689: step 56880, loss = 0.69 (1570.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:16:07.520274: step 56890, loss = 0.68 (1558.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:16:08.439430: step 56900, loss = 0.64 (1392.6 examples/sec; 0.092 sec/batch)
2017-05-06 22:16:09.157551: step 56910, loss = 0.96 (1782.4 examples/sec; 0.072 sec/batch)
2017-05-06 22:16:09.983707: step 56920, loss = 0.68 (1549.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:16:10.805980: step 56930, loss = 0.67 (1556.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:16:11.625534: step 56940, loss = 0.60 (1561.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:16:12.447715: step 56950, loss = 0.80 (1556.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:16:13.265309: step 56960, loss = 0.80 (1565.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:16:14.086524: step 56970, loss = 0.77 (1558.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:16:14.905466: step 56980, loss = 0.88 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:16:15.715571: step 56990, loss = 0.68 (1580.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:16:16.643733: step 57000, loss = 0.74 (1379.1 examples/sec; 0.093 sec/batch)
2017-05-06 22:16:17.357764: step 57010, loss = 0.76 (1792.6 examples/sec; 0.071 sec/batch)
2017-05-06 22:16:18.182698: step 57020, loss = 0.74 (1551.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:16:19.009469: step 57030, loss = 0.74 (1548.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:16:19.820602: step 57040, loss = 0.66 (1578.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:16:20.636213: step 57050, loss = 0.73 (1569.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:16:21.468547: step 57060, loss = 0.60 (1537.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:16:22.292673: step 57070, loss = 0.82 (1553.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:16:23.120613: step 57080, loss = 0.78 (1546.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:16:23.937993: step 57090, loss = 0.72 (1566.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:16:24.867613: step 57100, loss = 0.83 (1376.9 examples/sec; 0.093 sec/batch)
2017-05-06 22:16:25.580478: step 57110, loss = 0.75 (1795.6 examples/sec; 0.071 sec/batch)
2017-05-06 22:16:26.400884: step 57120, loss = 0.71 (1560.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:16:27.235828: step 57130, loss = 0.63 (1533.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:16:28.059072: step 57140, loss = 0.72 (1554.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:16:28.880368: step 57150, loss = 0.72 (1558.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:16:29.702154: step 57160, loss = 0.92 (1557.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:16:30.527316: step 57170, loss = 0.86 (1551.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:16:31.332818: step 57180, loss = 0.72 (1589.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:16:32.147948: step 57190, loss = 0.75 (1570.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:16:33.066903: step 57200, loss = 0.64 (1392.9 examples/sec; 0.092 sec/batch)
2017-05-06 22:16:33.792828: step 57210, loss = 0.71 (1763.3 examples/sec; 0.073 sec/batch)
2017-05-06 22:16:34.619304: step 57220, loss = 0.80 (1548.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:16:35.440612: step 57230, loss = 0.97 (1558.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:16:36.255409: step 57240, loss = 0.65 (1570.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:16:37.082187: step 57250, loss = 0.77 (1548.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:16:37.902661: step 57260, loss = 0.59 (1560.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:16:38.724024: step 57270, loss = 0.69 (1558.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:16:39.541186: step 57280, loss = 0.79 (1566.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:16:40.367646: step 57290, loss = 0.66 (1548.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:16:41.287903: step 57300, loss = 0.92 (1390.9 examples/sec; 0.092 sec/batch)
2017-05-06 22:16:42.018650: step 57310, loss = 0.74 (1751.6 examples/sec; 0.073 sec/batch)
2017-05-06 22:16:42.841251: step 57320, loss = 0.65 (1556.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:16:43.668259: step 57330, loss = 0.70 (1547.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:16:44.486867: step 57340, loss = 0.87 (1563.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:16:45.308162: step 57350, loss = 0.62 (1558.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:16:46.130987: step 57360, loss = 0.87 (1555.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:16:46.956911: step 57370, loss = 0.80 (1549.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:16:47.776756: step 57380, loss = 0.72 (1561.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:16:48.602499: step 57390, loss = 0.83 (1550.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:16:49.534681: step 57400, loss = 0.68 (1373.1 examples/sec; 0.093 sec/batch)
2017-05-06 22:16:50.253506: step 57410, loss = 0.71 (1780.7 examples/sec; 0.072 sec/batch)
2017-05-06 22:16:51.100436: step 57420, loss = 0.80 (1511.3 examples/sec; 0.085 sec/batch)
2017-05-06 22:16:51.904756: step 57430, loss = 0.67 (1591.4 examples/sec; 0.080 sec/batch)
2017-05-06 22:16:52.714119: step 57440, loss = 0.72 (1581.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:16:53.531911: step 57450, loss = 0.77 (1565.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:16:54.351619: step 57460, loss = 0.58 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:16:55.182481: step 57470, loss = 0.67 (1540.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:16:55.995841: step 57480, loss = 0.77 (1573.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:16:56.821491: step 57490, loss = 0.73 (1550.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:16:57.745385: step 57500, loss = 0.71 (1385.4 examples/sec; 0.092 sec/batch)
2017-05-06 22:16:58.459914: step 57510, loss = 0.86 (1791.4 examples/sec; 0.071 sec/batch)
2017-05-06 22:16:59.290998: step 57520, loss = 0.80 (1540.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:17:00.117650: step 57530, loss = 0.75 (1548.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:17:00.946317: step 57540, loss = 0.84 (1544.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:17:01.761312: step 57550, loss = 0.74 (1570.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:17:02.582702: step 57560, loss = 0.71 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:17:03.405648: step 57570, loss = 0.84 (1555.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:17:04.223809: step 57580, loss = 0.73 (1564.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:17:05.043325: step 57590, loss = 0.73 (1561.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:17:05.963444: step 57600, loss = 0.66 (1391.1 examples/sec; 0.092 sec/batch)
2017-05-06 22:17:06.685488: step 57610, loss = 0.75 (1772.8 examples/sec; 0.072 sec/batch)
2017-05-06 22:17:07.506456: step 57620, loss = 0.70 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:17:08.327907: step 57630, loss = 0.65 (1558.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:17:09.147843: step 57640, loss = 0.83 (1561.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:17:09.965111: step 57650, loss = 0.78 (1566.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:17:10.779386: step 57660, loss = 0.76 (1571.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:17:11.599565: step 57670, loss = 0.86 (1560.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:17:12.418641: step 57680, loss = 0.81 (1562.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:17:13.245704: step 57690, loss = 0.77 (1547.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:17:14.168546: step 57700, loss = 0.67 (1387.0 examples/sec; 0.092 sec/batch)
2017-05-06 22:17:14.889488: step 57710, loss = 0.77 (1775.5 examples/sec; 0.072 sec/batch)
2017-05-06 22:17:15.712201: step 57720, loss = 0.77 (1555.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:17:16.538534: step 57730, loss = 0.75 (1549.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:17:17.357665: step 57740, loss = 0.70 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:17:18.182934: step 57750, loss = 0.74 (1551.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:17:18.997208: step 57760, loss = 0.74 (1571.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:17:19.800152: step 57770, loss = 0.69 (1594.1 examples/sec; 0.080 sec/batch)
2017-05-06 22:17:20.627647: step 57780, loss = 0.76 (1546.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:17:21.450626: step 57790, loss = 0.74 (1555.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:17:22.385140: step 57800, loss = 0.70 (1369.7 examples/sec; 0.093 sec/batch)
2017-05-06 22:17:23.107602: step 57810, loss = 0.70 (1771.7 examples/sec; 0.072 sec/batch)
2017-05-06 22:17:23.928613: step 57820, loss = 0.70 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:17:24.747393: step 57830, loss = 0.69 (1563.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:17:25.565452: step 57840, loss = 0.78 (1564.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:17:26.384449: step 57850, loss = 0.86 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:17:27.207888: step 57860, loss = 0.67 (1554.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:17:28.027530: step 57870, loss = 0.65 (1561.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:17:28.853849: step 57880, loss = 0.70 (1549.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:17:29.683768: step 57890, loss = 0.71 (1542.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:17:30.617438: step 57900, loss = 0.73 (1370.9 examples/sec; 0.093 sec/batch)
2017-05-06 22:17:31.342248: step 57910, loss = 0.75 (1766.0 examples/sec; 0.072 sec/batch)
2017-05-06 22:17:32.159599: step 57920, loss = 0.72 (1566.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:17:32.979353: step 57930, loss = 0.73 (1561.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:17:33.802451: step 57940, loss = 0.83 (1555.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:17:34.628952: step 57950, loss = 0.75 (1548.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:17:35.442368: step 57960, loss = 0.67 (1573.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:17:36.262600: step 57970, loss = 0.83 (1560.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:17:37.079104: step 57980, loss = 0.71 (1567.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:17:37.906766: step 57990, loss = 0.81 (1546.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:17:38.821513: step 58000, loss = 0.73 (1399.3 examples/sec; 0.091 sec/batch)
2017-05-06 22:17:39.545224: step 58010, loss = 0.74 (1768.7 examples/sec; 0.072 sec/batch)
2017-05-06 22:17:40.375109: step 58020, loss = 0.67 (1542.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:17:41.203171: step 58030, loss = 0.87 (1545.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:17:42.011323: step 58040, loss = 0.64 (1583.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:17:42.838714: step 58050, loss = 0.72 (1547.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:17:43.651460: step 58060, loss = 0.84 (1574.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:17:44.484462: step 58070, loss = 0.91 (1536.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:17:45.309500: step 58080, loss = 0.78 (1551.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:17:46.133039: step 58090, loss = 0.73 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:17:47.058418: step 58100, loss = 0.57 (1383.2 examples/sec; 0.093 sec/batch)
2017-05-06 22:17:47.770666: step 58110, loss = 0.69 (1797.2 examples/sec; 0.071 sec/batch)
2017-05-06 22:17:48.597860: step 58120, loss = 0.70 (1547.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:17:49.418091: step 58130, loss = 0.94 (1560.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:17:50.245317: step 58140, loss = 0.73 (1547.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:17:51.061737: step 58150, loss = 0.63 (1567.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:17:51.877771: step 58160, loss = 0.77 (1568.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:17:52.698731: step 58170, loss = 0.73 (1559.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:17:53.521952: step 58180, loss = 0.85 (1554.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:17:54.336858: step 58190, loss = 0.74 (1570.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:17:55.264711: step 58200, loss = 0.69 (1379.5 examples/sec; 0.093 sec/batch)
2017-05-06 22:17:55.968877: step 58210, loss = 0.75 (1817.7 examples/sec; 0.070 sec/batch)
2017-05-06 22:17:56.787751: step 58220, loss = 0.77 (1563.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:17:57.612031: step 58230, loss = 0.95 (1552.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:17:58.444672: step 58240, loss = 0.77 (1537.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:17:59.268996: step 58250, loss = 0.69 (1552.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:00.089776: step 58260, loss = 0.68 (1559.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:00.906101: step 58270, loss = 0.84 (1568.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:01.727401: step 58280, loss = 0.68 (1558.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:02.546529: step 58290, loss = 0.70 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:03.465414: step 58300, loss = 0.65 (1393.0 examples/sec; 0.092 sec/batch)
2017-05-06 22:18:04.190495: step 58310, loss = 0.76 (1765.3 examples/sec; 0.073 sec/batch)
2017-05-06 22:18:05.018402: step 58320, loss = 0.90 (1546.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:18:05.830359: step 58330, loss = 0.69 (1576.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:18:06.656032: step 58340, loss = 0.63 (1550.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:18:07.479755: step 58350, loss = 0.77 (1553.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:08.307983: step 58360, loss = 0.65 (1545.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:18:09.132687: step 58370, loss = 0.70 (1552.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:09.960117: step 58380, loss = 0.81 (1547.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:18:10.788224: step 58390, loss = 0.90 (1545.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:18:11.732028: step 58400, loss = 0.74 (1356.2 examples/sec; 0.094 sec/batch)
2017-05-06 22:18:12.464999: step 58410, loss = 0.74 (1746.3 examples/sec; 0.073 sec/batch)
2017-05-06 22:18:13.292721: step 58420, loss = 0.76 (1546.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:18:14.115885: step 58430, loss = 0.67 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:14.935110: step 58440, loss = 0.68 (1562.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:15.752101: step 58450, loss = 0.76 (1566.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:16.568397: step 58460, loss = 0.76 (1568.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:17.390436: step 58470, loss = 0.78 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:18.216689: step 58480, loss = 0.73 (1549.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:18:19.038946: step 58490, loss = 0.74 (1556.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:19.951767: step 58500, loss = 0.73 (1402.2 examples/sec; 0.091 sec/batch)
2017-05-06 22:18:20.682690: step 58510, loss = 0.75 (1751.2 examples/sec; 0.073 sec/batch)
2017-05-06 22:18:21.504834: step 58520, loss = 0.67 (1556.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:22.326191: step 58530, loss = 0.73 (1558.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:23.150858: step 58540, loss = 0.79 (1552.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:23.963845: step 58550, loss = 0.94 (1574.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:18:24.786354: step 58560, loss = 0.78 (1556.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:25.610130: step 58570, loss = 0.81 (1553.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:26.438365: step 58580, loss = 0.72 (1545.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:18:27.268328: step 58590, loss = 0.67 (1542.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:18:28.194219: step 58600, loss = 0.73 (1382.5 examples/sec; 0.093 sec/batch)
2017-05-06 22:18:28.920984: step 58610, loss = 0.71 (1761.2 examples/sec; 0.073 sec/batch)
2017-05-06 22:18:29.747383: step 58620, loss = 0.80 (1548.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:18:30.578519: step 58630, loss = 0.74 (1540.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:18:31.387222: step 58640, loss = 0.66 (1582.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:18:32.206686: step 58650, loss = 0.69 (1562.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:33.024968: step 58660, loss = 0.85 (1564.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:33.857153: step 58670, loss = 0.70 (1538.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:18:34.677976: step 58680, loss = 0.66 (1559.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:35.497967: step 58690, loss = 0.61 (1561.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:36.411733: step 58700, loss = 0.81 (1400.8 examples/sec; 0.091 sec/batch)
2017-05-06 22:18:37.145926: step 58710, loss = 0.79 (1743.4 examples/sec; 0.073 sec/batch)
2017-05-06 22:18:37.979423: step 58720, loss = 0.72 (1535.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:18:38.797416: step 58730, loss = 0.75 (1564.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:39.616780: step 58740, loss = 0.74 (1562.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:40.443747: step 58750, loss = 0.65 (1547.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:18:41.272114: step 58760, loss = 0.69 (1545.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:18:42.089934: step 58770, loss = 0.74 (1565.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:42.911408: step 58780, loss = 1.01 (1558.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:43.731250: step 58790, loss = 0.75 (1561.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:44.653511: step 58800, loss = 0.79 (1387.9 examples/sec; 0.092 sec/batch)
2017-05-06 22:18:45.382520: step 58810, loss = 0.82 (1755.8 examples/sec; 0.073 sec/batch)
2017-05-06 22:18:46.207906: step 58820, loss = 0.73 (1550.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:18:47.044541: step 58830, loss = 0.84 (1529.9 examples/sec; 0.084 sec/batch)
2017-05-06 22:18:47.871252: step 58840, loss = 0.66 (1548.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:18:48.687484: step 58850, loss = 0.82 (1568.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:49.510270: step 58860, loss = 0.73 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:50.334675: step 58870, loss = 0.58 (1552.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:51.150545: step 58880, loss = 0.73 (1568.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:51.977224: step 58890, loss = 0.86 (1548.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:18:52.892463: step 58900, loss = 0.72 (1398.5 examples/sec; 0.092 sec/batch)
2017-05-06 22:18:53.619194: step 58910, loss = 0.69 (1761.3 examples/sec; 0.073 sec/batch)
2017-05-06 22:18:54.437821: step 58920, loss = 0.71 (1563.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:55.262424: step 58930, loss = 0.69 (1552.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:18:56.071685: step 58940, loss = 0.65 (1581.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:18:56.907843: step 58950, loss = 0.78 (1530.8 examples/sec; 0.084 sec/batch)
2017-05-06 22:18:57.738957: step 58960, loss = 0.65 (1540.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:18:58.568880: step 58970, loss = 0.65 (1542.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:18:59.395506: step 58980, loss = 0.88 (1548.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:19:00.214742: step 58990, loss = 0.74 (1562.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:01.136102: step 59000, loss = 0.68 (1389.3 examples/sec; 0.092 sec/batch)
2017-05-06 22:19:01.864054: step 59010, loss = 0.76 (1758.4 examples/sec; 0.073 sec/batch)
2017-05-06 22:19:02.689568: step 59020, loss = 0.73 (1550.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:19:03.516158: step 59030, loss = 0.62 (1548.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:19:04.339001: step 59040, loss = 0.75 (1555.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:05.153494: step 59050, loss = 0.77 (1571.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:19:05.981688: step 59060, loss = 0.76 (1545.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:19:06.815856: step 59070, loss = 0.79 (1534.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:19:07.622455: step 59080, loss = 0.73 (1586.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:19:08.453214: step 59090, loss = 0.72 (1540.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:19:09.380988: step 59100, loss = 0.85 (1379.6 examples/sec; 0.093 sec/batch)
2017-05-06 22:19:10.109346: step 59110, loss = 0.74 (1757.4 examples/sec; 0.073 sec/batch)
2017-05-06 22:19:10.932728: step 59120, loss = 0.80 (1554.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:11.746099: step 59130, loss = 0.87 (1573.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:19:12.565990: step 59140, loss = 0.75 (1561.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:13.377764: step 59150, loss = 0.76 (1576.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:19:14.197656: step 59160, loss = 0.70 (1561.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:15.009525: step 59170, loss = 0.76 (1576.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:19:15.828607: step 59180, loss = 0.89 (1562.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:16.661774: step 59190, loss = 0.76 (1536.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:19:17.602664: step 59200, loss = 0.70 (1360.4 examples/sec; 0.094 sec/batch)
2017-05-06 22:19:18.317966: step 59210, loss = 0.70 (1789.5 examples/sec; 0.072 sec/batch)
2017-05-06 22:19:19.139848: step 59220, loss = 0.80 (1557.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:19.957255: step 59230, loss = 0.87 (1565.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:20.776343: step 59240, loss = 0.77 (1562.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:21.591806: step 59250, loss = 0.79 (1569.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:22.414516: step 59260, loss = 0.61 (1555.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:23.239416: step 59270, loss = 0.80 (1551.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:24.051358: step 59280, loss = 0.75 (1576.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:19:24.872863: step 59290, loss = 0.69 (1558.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:25.794741: step 59300, loss = 0.78 (1388.5 examples/sec; 0.092 sec/batch)
2017-05-06 22:19:26.517438: step 59310, loss = 0.90 (1771.1 examples/sec; 0.072 sec/batch)
2017-05-06 22:19:27.335786: step 59320, loss = 0.66 (1564.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:28.155356: step 59330, loss = 0.78 (1561.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:28.977632: step 59340, loss = 0.75 (1556.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:29.794250: step 59350, loss = 0.72 (1567.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:30.619992: step 59360, loss = 0.74 (1550.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:19:31.435337: step 59370, loss = 0.58 (1569.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:32.253323: step 59380, loss = 0.61 (1564.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:33.070830: step 59390, loss = 0.70 (1565.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:34.025309: step 59400, loss = 0.79 (1341.0 examples/sec; 0.095 sec/batch)
2017-05-06 22:19:34.721824: step 59410, loss = 0.72 (1837.7 examples/sec; 0.070 sec/batch)
2017-05-06 22:19:35.545738: step 59420, loss = 0.78 (1553.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:36.361258: step 59430, loss = 0.72 (1569.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:37.192755: step 59440, loss = 0.75 (1539.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:19:38.007342: step 59450, loss = 0.79 (1571.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:19:38.831645: step 59460, loss = 0.90 (1552.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:39.647016: step 59470, loss = 0.87 (1569.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:40.473431: step 59480, loss = 0.66 (1548.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:19:41.291295: step 59490, loss = 0.70 (1565.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:42.222393: step 59500, loss = 0.82 (1374.7 examples/sec; 0.093 sec/batch)
2017-05-06 22:19:42.950076: step 59510, loss = 0.75 (1759.0 examples/sec; 0.073 sec/batch)
2017-05-06 22:19:43.754714: step 59520, loss = 0.61 (1590.8 examples/sec; 0.080 sec/batch)
2017-05-06 22:19:44.578489: step 59530, loss = 0.69 (1553.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:45.400836: step 59540, loss = 0.70 (1556.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:46.228074: step 59550, loss = 0.84 (1547.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:19:47.052106: step 59560, loss = 0.69 (1553.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:47.858659: step 59570, loss = 0.87 (1587.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:19:48.687253: step 59580, loss = 0.81 (1544.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:19:49.528193: step 59590, loss = 0.75 (1522.1 examples/sec; 0.084 sec/batch)
2017-05-06 22:19:50.441997: step 59600, loss = 0.69 (1400.7 examples/sec; 0.091 sec/batch)
2017-05-06 22:19:51.176910: step 59610, loss = 0.70 (1741.7 examples/sec; 0.073 sec/batch)
2017-05-06 22:19:51.994472: step 59620, loss = 0.66 (1565.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:52.817355: step 59630, loss = 0.59 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:53.644291: step 59640, loss = 0.93 (1547.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:19:54.475279: step 59650, loss = 0.76 (1540.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:19:55.297294: step 59660, loss = 0.66 (1557.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:56.120290: step 59670, loss = 0.78 (1555.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:56.944133: step 59680, loss = 0.88 (1553.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:57.763141: step 59690, loss = 0.81 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:19:58.680258: step 59700, loss = 0.74 (1395.7 examples/sec; 0.092 sec/batch)
2017-05-06 22:19:59.398599: step 59710, loss = 0.65 (1781.9 examples/sec; 0.072 sec/batch)
2017-05-06 22:20:00.220862: step 59720, loss = 0.76 (1556.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:20:01.044497: step 59730, loss = 0.75 (1554.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:20:01.883024: step 59740, loss = 0.72 (1526.5 examples/sec; 0.084 sec/batch)
2017-05-06 22:20:02.716207: step 59750, loss = 0.71 (1536.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:20:03.539075: step 59760, loss = 0.72 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:20:04.364699: step 59770, loss = 0.85 (1550.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:20:05.194019: step 59780, loss = 0.83 (1543.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:20:06.025709: step 59790, loss = 0.81 (1539.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:20:06.961838: step 59800, loss = 0.82 (1367.3 examples/sec; 0.094 sec/batch)
2017-05-06 22:20:07.683378: step 59810, loss = 0.78 (1774.0 examples/sec; 0.072 sec/batch)
2017-05-06 22:20:08.505290: step 59820, loss = 0.66 (1557.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:20:09.336038: step 59830, loss = 0.58 (1540.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:20:10.164235: step 59840, loss = 0.91 (1545.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:20:10.983783: step 59850, loss = 0.58 (1561.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:20:11.790535: step 59860, loss = 0.73 (1586.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:20:12.611736: step 59870, loss = 0.71 (1558.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:20:13.423568: step 59880, loss = 0.78 (1576.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:20:14.250383: step 59890, loss = 0.81 (1548.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:20:15.179973: step 59900, loss = 0.84 (1376.9 examples/sec; 0.093 sec/batch)
2017-05-06 22:20:15.903520: step 59910, loss = 0.74 (1769.1 examples/sec; 0.072 sec/batch)
2017-05-06 22:20:16.730694: step 59920, loss = 0.80 (1547.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:20:17.551504: step 59930, loss = 0.74 (1559.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:20:18.370316: step 59940, loss = 0.55 (1563.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:20:19.195256: step 59950, loss = 0.61 (1551.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:20:20.011330: step 59960, loss = 0.86 (1568.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:20:20.838132: step 59970, loss = 0.71 (1548.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:20:21.657437: step 59980, loss = 0.63 (1562.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:20:22.480663: step 59990, loss = 0.78 (1554.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:20:23.395927: step 60000, loss = 0.69 (1398.5 examples/sec; 0.092 sec/batch)
2017-05-06 22:20:24.121517: step 60010, loss = 0.74 (1764.1 examples/sec; 0.073 sec/batch)
2017-05-06 22:20:24.950974: step 60020, loss = 0.74 (1543.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:20:25.778544: step 60030, loss = 0.76 (1546.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:20:26.605508: step 60040, loss = 0.92 (1547.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:20:27.417810: step 60050, loss = 0.78 (1575.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:20:28.227132: step 60060, loss = 0.80 (1581.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:20:29.054003: step 60070, loss = 0.63 (1548.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:20:29.873092: step 60080, loss = 0.62 (1562.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:20:30.706647: step 60090, loss = 0.75 (1535.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:20:31.620357: step 60100, loss = 0.82 (1400.9 examples/sec; 0.091 sec/batch)
2017-05-06 22:20:32.344833: step 60110, loss = 0.68 (1766.8 examples/sec; 0.072 sec/batch)
2017-05-06 22:20:33.158551: step 60120, loss = 0.83 (1573.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:20:33.972457: step 60130, loss = 0.68 (1572.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:20:34.789538: step 60140, loss = 0.76 (1566.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:20:35.612421: step 60150, loss = 0.79 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:20:36.433950: step 60160, loss = 0.59 (1558.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:20:37.254655: step 60170, loss = 0.76 (1559.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:20:38.077566: step 60180, loss = 0.81 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:20:38.895877: step 60190, loss = 0.77 (1564.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:20:39.808200: step 60200, loss = 0.76 (1403.0 examples/sec; 0.091 sec/batch)
2017-05-06 22:20:40.523726: step 60210, loss = 0.83 (1788.9 examples/sec; 0.072 sec/batch)
2017-05-06 22:20:41.348896: step 60220, loss = 0.73 (1551.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:20:42.169899: step 60230, loss = 0.80 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:20:42.990929: step 60240, loss = 0.76 (1559.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:20:43.805904: step 60250, loss = 0.69 (1570.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:20:44.627354: step 60260, loss = 0.85 (1558.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:20:45.450468: step 60270, loss = 0.76 (1555.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:20:46.274889: step 60280, loss = 0.62 (1552.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:20:47.095829: step 60290, loss = 0.68 (1559.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:20:48.011303: step 60300, loss = 0.71 (1398.2 examples/sec; 0.092 sec/batch)
2017-05-06 22:20:48.736288: step 60310, loss = 0.80 (1765.6 examples/sec; 0.072 sec/batch)
2017-05-06 22:20:49.562180: step 60320, loss = 0.85 (1549.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:20:50.378458: step 60330, loss = 0.96 (1568.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:20:51.198345: step 60340, loss = 0.76 (1561.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:20:52.011228: step 60350, loss = 0.63 (1574.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:20:52.827847: step 60360, loss = 0.74 (1567.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:20:53.645332: step 60370, loss = 0.69 (1565.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:20:54.464361: step 60380, loss = 0.87 (1562.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:20:55.304377: step 60390, loss = 0.81 (1523.8 examples/sec; 0.084 sec/batch)
2017-05-06 22:20:56.202935: step 60400, loss = 0.87 (1424.5 examples/sec; 0.090 sec/batch)
2017-05-06 22:20:56.939057: step 60410, loss = 0.66 (1738.8 examples/sec; 0.074 sec/batch)
2017-05-06 22:20:57.753150: step 60420, loss = 0.73 (1572.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:20:58.571960: step 60430, loss = 0.69 (1563.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:20:59.401034: step 60440, loss = 0.84 (1543.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:21:00.216114: step 60450, loss = 0.74 (1570.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:01.034255: step 60460, loss = 0.60 (1564.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:01.863444: step 60470, loss = 0.73 (1543.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:21:02.697406: step 60480, loss = 0.72 (1534.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:21:03.512823: step 60490, loss = 0.77 (1569.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:04.425225: step 60500, loss = 0.67 (1402.9 examples/sec; 0.091 sec/batch)
2017-05-06 22:21:05.149011: step 60510, loss = 0.77 (1768.5 examples/sec; 0.072 sec/batch)
2017-05-06 22:21:05.967522: step 60520, loss = 0.81 (1563.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:06.793120: step 60530, loss = 0.67 (1550.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:21:07.614414: step 60540, loss = 0.60 (1558.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:08.438664: step 60550, loss = 0.74 (1552.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:09.263388: step 60560, loss = 0.59 (1552.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:10.076573: step 60570, loss = 0.67 (1574.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:21:10.893802: step 60580, loss = 0.75 (1566.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:11.715131: step 60590, loss = 0.69 (1558.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:12.631164: step 60600, loss = 0.80 (1397.3 examples/sec; 0.092 sec/batch)
2017-05-06 22:21:13.348938: step 60610, loss = 0.79 (1783.3 examples/sec; 0.072 sec/batch)
2017-05-06 22:21:14.178537: step 60620, loss = 0.72 (1542.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:21:14.992529: step 60630, loss = 0.88 (1572.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:21:15.813177: step 60640, loss = 0.68 (1559.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:16.631928: step 60650, loss = 0.68 (1563.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:17.455352: step 60660, loss = 0.62 (1554.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:18.275260: step 60670, loss = 0.82 (1561.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:19.095919: step 60680, loss = 0.81 (1559.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:19.904647: step 60690, loss = 0.75 (1582.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:21:20.831452: step 60700, loss = 0.70 (1381.1 examples/sec; 0.093 sec/batch)
2017-05-06 22:21:21.547892: step 60710, loss = 0.79 (1786.6 examples/sec; 0.072 sec/batch)
2017-05-06 22:21:22.363557: step 60720, loss = 0.75 (1569.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:23.184076: step 60730, loss = 0.80 (1560.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:24.000547: step 60740, loss = 0.72 (1567.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:24.821914: step 60750, loss = 0.77 (1558.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:25.647417: step 60760, loss = 0.76 (1550.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:21:26.471501: step 60770, loss = 0.78 (1553.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:27.289986: step 60780, loss = 0.83 (1563.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:28.105616: step 60790, loss = 0.83 (1569.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:29.031652: step 60800, loss = 0.65 (1382.2 examples/sec; 0.093 sec/batch)
2017-05-06 22:21:29.751370: step 60810, loss = 0.68 (1778.5 examples/sec; 0.072 sec/batch)
2017-05-06 22:21:30.570966: step 60820, loss = 0.80 (1561.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:31.391493: step 60830, loss = 0.75 (1560.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:32.206782: step 60840, loss = 0.63 (1570.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:33.028997: step 60850, loss = 0.84 (1556.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:33.861017: step 60860, loss = 0.73 (1538.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:21:34.670390: step 60870, loss = 0.57 (1581.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:21:35.481157: step 60880, loss = 0.68 (1578.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:21:36.300758: step 60890, loss = 0.79 (1561.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:37.226562: step 60900, loss = 0.72 (1382.6 examples/sec; 0.093 sec/batch)
2017-05-06 22:21:37.944028: step 60910, loss = 0.85 (1784.1 examples/sec; 0.072 sec/batch)
2017-05-06 22:21:38.758590: step 60920, loss = 0.73 (1571.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:21:39.572410: step 60930, loss = 0.83 (1572.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:21:40.398478: step 60940, loss = 0.79 (1549.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:21:41.216612: step 60950, loss = 0.84 (1564.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:42.034335: step 60960, loss = 0.75 (1565.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:42.854975: step 60970, loss = 0.77 (1559.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:43.667998: step 60980, loss = 0.87 (1574.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:21:44.490960: step 60990, loss = 0.95 (1555.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:45.432559: step 61000, loss = 0.77 (1359.4 examples/sec; 0.094 sec/batch)
2017-05-06 22:21:46.129708: step 61010, loss = 0.82 (1836.0 examples/sec; 0.070 sec/batch)
2017-05-06 22:21:46.951825: step 61020, loss = 0.72 (1557.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:47.761839: step 61030, loss = 0.83 (1580.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:21:48.584210: step 61040, loss = 0.58 (1556.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:49.397150: step 61050, loss = 0.64 (1574.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:21:50.218124: step 61060, loss = 0.83 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:51.047854: step 61070, loss = 0.70 (1542.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:21:51.862349: step 61080, loss = 0.86 (1571.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:21:52.687434: step 61090, loss = 0.87 (1551.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:21:53.610256: step 61100, loss = 0.74 (1387.1 examples/sec; 0.092 sec/batch)
2017-05-06 22:21:54.332212: step 61110, loss = 0.64 (1772.9 examples/sec; 0.072 sec/batch)
2017-05-06 22:21:55.150118: step 61120, loss = 0.81 (1565.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:55.961383: step 61130, loss = 0.80 (1577.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:21:56.783923: step 61140, loss = 0.70 (1556.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:57.609242: step 61150, loss = 0.68 (1550.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:21:58.430135: step 61160, loss = 0.76 (1559.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:21:59.242242: step 61170, loss = 0.77 (1576.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:22:00.058966: step 61180, loss = 0.83 (1567.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:00.883162: step 61190, loss = 0.65 (1553.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:01.807100: step 61200, loss = 0.69 (1385.4 examples/sec; 0.092 sec/batch)
2017-05-06 22:22:02.523671: step 61210, loss = 0.70 (1786.3 examples/sec; 0.072 sec/batch)
2017-05-06 22:22:03.340299: step 61220, loss = 0.71 (1567.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:04.156130: step 61230, loss = 0.75 (1569.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:04.980977: step 61240, loss = 0.73 (1551.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:05.803449: step 61250, loss = 0.70 (1556.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:06.624228: step 61260, loss = 0.63 (1559.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:07.442412: step 61270, loss = 0.69 (1564.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:08.258876: step 61280, loss = 0.73 (1567.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:09.086265: step 61290, loss = 0.71 (1547.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:22:10.000924: step 61300, loss = 0.66 (1399.4 examples/sec; 0.091 sec/batch)
2017-05-06 22:22:10.737207: step 61310, loss = 0.66 (1738.5 examples/sec; 0.074 sec/batch)
2017-05-06 22:22:11.562500: step 61320, loss = 0.83 (1551.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:22:12.377999: step 61330, loss = 0.80 (1569.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:13.201869: step 61340, loss = 0.70 (1553.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:14.021433: step 61350, loss = 0.70 (1561.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:14.846370: step 61360, loss = 0.75 (1551.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:15.658865: step 61370, loss = 0.76 (1575.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:22:16.508153: step 61380, loss = 0.72 (1507.1 examples/sec; 0.085 sec/batch)
2017-05-06 22:22:17.314935: step 61390, loss = 0.58 (1586.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:22:18.221774: step 61400, loss = 0.58 (1411.5 examples/sec; 0.091 sec/batch)
2017-05-06 22:22:18.938728: step 61410, loss = 0.71 (1785.3 examples/sec; 0.072 sec/batch)
2017-05-06 22:22:19.757202: step 61420, loss = 0.81 (1563.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:20.574797: step 61430, loss = 0.77 (1565.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:21.390788: step 61440, loss = 0.86 (1568.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:22.201279: step 61450, loss = 0.69 (1579.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:22:23.021904: step 61460, loss = 0.90 (1559.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:23.834889: step 61470, loss = 0.68 (1574.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:22:24.662427: step 61480, loss = 0.61 (1546.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:22:25.489128: step 61490, loss = 0.60 (1548.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:22:26.402442: step 61500, loss = 0.73 (1401.5 examples/sec; 0.091 sec/batch)
2017-05-06 22:22:27.130734: step 61510, loss = 0.57 (1757.6 examples/sec; 0.073 sec/batch)
2017-05-06 22:22:27.946823: step 61520, loss = 0.81 (1568.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:28.759304: step 61530, loss = 0.66 (1575.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:22:29.582788: step 61540, loss = 0.96 (1554.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:30.398542: step 61550, loss = 0.95 (1569.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:31.223150: step 61560, loss = 0.64 (1552.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:32.033104: step 61570, loss = 0.61 (1580.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:22:32.863305: step 61580, loss = 0.67 (1541.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:22:33.683971: step 61590, loss = 0.84 (1559.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:34.603161: step 61600, loss = 0.78 (1392.5 examples/sec; 0.092 sec/batch)
2017-05-06 22:22:35.316062: step 61610, loss = 0.76 (1795.5 examples/sec; 0.071 sec/batch)
2017-05-06 22:22:36.129242: step 61620, loss = 0.86 (1574.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:22:36.944368: step 61630, loss = 0.71 (1570.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:37.766771: step 61640, loss = 0.79 (1556.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:38.582926: step 61650, loss = 0.72 (1568.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:39.397916: step 61660, loss = 0.75 (1570.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:22:40.220409: step 61670, loss = 0.91 (1556.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:41.041673: step 61680, loss = 0.73 (1558.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:41.861293: step 61690, loss = 0.82 (1561.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:42.778979: step 61700, loss = 0.85 (1394.8 examples/sec; 0.092 sec/batch)
2017-05-06 22:22:43.492365: step 61710, loss = 0.66 (1794.3 examples/sec; 0.071 sec/batch)
2017-05-06 22:22:44.308309: step 61720, loss = 0.78 (1568.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:45.133563: step 61730, loss = 0.77 (1551.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:22:45.958402: step 61740, loss = 0.69 (1551.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:46.787723: step 61750, loss = 0.67 (1543.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:22:47.604365: step 61760, loss = 0.74 (1567.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:48.421736: step 61770, loss = 0.82 (1566.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:49.251449: step 61780, loss = 0.82 (1542.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:22:50.071696: step 61790, loss = 0.68 (1560.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:50.988660: step 61800, loss = 0.83 (1395.9 examples/sec; 0.092 sec/batch)
2017-05-06 22:22:51.710513: step 61810, loss = 0.87 (1773.2 examples/sec; 0.072 sec/batch)
2017-05-06 22:22:52.531809: step 61820, loss = 0.69 (1558.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:53.353901: step 61830, loss = 0.69 (1557.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:54.172025: step 61840, loss = 0.57 (1564.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:54.991690: step 61850, loss = 0.90 (1561.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:55.808362: step 61860, loss = 0.76 (1567.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:56.629710: step 61870, loss = 0.72 (1558.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:22:57.454955: step 61880, loss = 0.75 (1551.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:22:58.281961: step 61890, loss = 0.65 (1547.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:22:59.197016: step 61900, loss = 0.78 (1398.8 examples/sec; 0.092 sec/batch)
2017-05-06 22:22:59.906318: step 61910, loss = 0.68 (1804.6 examples/sec; 0.071 sec/batch)
2017-05-06 22:23:00.740760: step 61920, loss = 0.89 (1534.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:23:01.552937: step 61930, loss = 0.67 (1576.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:23:02.374729: step 61940, loss = 0.56 (1557.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:03.192858: step 61950, loss = 0.71 (1564.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:04.006484: step 61960, loss = 0.77 (1573.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:23:04.824716: step 61970, loss = 0.76 (1564.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:05.647726: step 61980, loss = 0.71 (1555.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:06.470992: step 61990, loss = 0.86 (1554.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:07.394651: step 62000, loss = 0.83 (1385.8 examples/sec; 0.092 sec/batch)
2017-05-06 22:23:08.111868: step 62010, loss = 0.85 (1784.7 examples/sec; 0.072 sec/batch)
2017-05-06 22:23:08.937736: step 62020, loss = 0.72 (1549.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:23:09.763895: step 62030, loss = 0.62 (1549.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:23:10.589542: step 62040, loss = 0.91 (1550.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:23:11.408201: step 62050, loss = 0.61 (1563.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:12.217551: step 62060, loss = 0.72 (1581.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:23:13.038283: step 62070, loss = 0.93 (1559.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:13.864810: step 62080, loss = 0.72 (1548.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:23:14.688921: step 62090, loss = 0.73 (1553.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:15.602809: step 62100, loss = 0.92 (1400.6 examples/sec; 0.091 sec/batch)
2017-05-06 22:23:16.319233: step 62110, loss = 0.74 (1786.7 examples/sec; 0.072 sec/batch)
2017-05-06 22:23:17.132793: step 62120, loss = 0.77 (1573.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:23:17.953097: step 62130, loss = 0.82 (1560.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:18.773817: step 62140, loss = 0.79 (1559.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:19.593514: step 62150, loss = 0.68 (1561.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:20.421556: step 62160, loss = 0.90 (1545.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:23:21.241482: step 62170, loss = 0.83 (1561.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:22.077702: step 62180, loss = 0.72 (1530.7 examples/sec; 0.084 sec/batch)
2017-05-06 22:23:22.894470: step 62190, loss = 0.80 (1567.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:23.814946: step 62200, loss = 0.74 (1390.6 examples/sec; 0.092 sec/batch)
2017-05-06 22:23:24.549261: step 62210, loss = 0.75 (1743.1 examples/sec; 0.073 sec/batch)
2017-05-06 22:23:25.367185: step 62220, loss = 0.73 (1564.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:26.190006: step 62230, loss = 0.80 (1555.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:27.007010: step 62240, loss = 0.65 (1566.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:27.819517: step 62250, loss = 0.74 (1575.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:23:28.641639: step 62260, loss = 0.75 (1556.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:29.466128: step 62270, loss = 0.81 (1552.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:30.302074: step 62280, loss = 0.75 (1531.2 examples/sec; 0.084 sec/batch)
2017-05-06 22:23:31.126081: step 62290, loss = 0.70 (1553.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:32.047826: step 62300, loss = 0.64 (1388.7 examples/sec; 0.092 sec/batch)
2017-05-06 22:23:32.771015: step 62310, loss = 0.67 (1769.9 examples/sec; 0.072 sec/batch)
2017-05-06 22:23:33.593592: step 62320, loss = 0.73 (1556.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:34.408813: step 62330, loss = 0.86 (1570.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:35.223420: step 62340, loss = 0.72 (1571.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:23:36.035448: step 62350, loss = 0.85 (1576.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:23:36.858021: step 62360, loss = 0.67 (1556.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:37.698415: step 62370, loss = 0.64 (1523.1 examples/sec; 0.084 sec/batch)
2017-05-06 22:23:38.501464: step 62380, loss = 0.77 (1593.9 examples/sec; 0.080 sec/batch)
2017-05-06 22:23:39.315997: step 62390, loss = 0.79 (1571.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:23:40.234275: step 62400, loss = 0.63 (1393.9 examples/sec; 0.092 sec/batch)
2017-05-06 22:23:40.951659: step 62410, loss = 0.82 (1784.3 examples/sec; 0.072 sec/batch)
2017-05-06 22:23:41.774584: step 62420, loss = 0.78 (1555.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:42.595482: step 62430, loss = 0.79 (1559.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:43.422452: step 62440, loss = 0.75 (1547.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:23:44.238031: step 62450, loss = 0.62 (1569.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:45.056425: step 62460, loss = 0.86 (1564.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:45.883477: step 62470, loss = 0.61 (1547.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:23:46.699925: step 62480, loss = 0.77 (1567.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:47.519949: step 62490, loss = 0.73 (1560.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:48.441071: step 62500, loss = 0.77 (1389.6 examples/sec; 0.092 sec/batch)
2017-05-06 22:23:49.171229: step 62510, loss = 0.69 (1753.0 examples/sec; 0.073 sec/batch)
2017-05-06 22:23:49.975619: step 62520, loss = 0.73 (1591.3 examples/sec; 0.080 sec/batch)
2017-05-06 22:23:50.796702: step 62530, loss = 0.63 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:51.613752: step 62540, loss = 0.67 (1566.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:52.431662: step 62550, loss = 0.66 (1565.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:53.245849: step 62560, loss = 0.60 (1572.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:23:54.068216: step 62570, loss = 0.75 (1556.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:54.886532: step 62580, loss = 0.83 (1564.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:55.701920: step 62590, loss = 0.72 (1569.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:56.617201: step 62600, loss = 0.63 (1398.5 examples/sec; 0.092 sec/batch)
2017-05-06 22:23:57.349832: step 62610, loss = 0.84 (1747.1 examples/sec; 0.073 sec/batch)
2017-05-06 22:23:58.171880: step 62620, loss = 0.76 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:23:58.998123: step 62630, loss = 0.65 (1549.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:23:59.811682: step 62640, loss = 0.83 (1573.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:24:00.637464: step 62650, loss = 0.74 (1550.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:24:01.462603: step 62660, loss = 0.72 (1551.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:24:02.287725: step 62670, loss = 0.70 (1551.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:24:03.099592: step 62680, loss = 0.71 (1576.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:24:03.910860: step 62690, loss = 0.72 (1577.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:24:04.840568: step 62700, loss = 0.81 (1376.8 examples/sec; 0.093 sec/batch)
2017-05-06 22:24:05.558418: step 62710, loss = 0.70 (1783.1 examples/sec; 0.072 sec/batch)
2017-05-06 22:24:06.374887: step 62720, loss = 0.83 (1567.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:24:07.200550: step 62730, loss = 0.65 (1550.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:24:08.005966: step 62740, loss = 0.79 (1589.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:24:08.828373: step 62750, loss = 0.80 (1556.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:24:09.646196: step 62760, loss = 0.80 (1565.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:24:10.462841: step 62770, loss = 0.65 (1567.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:24:11.285264: step 62780, loss = 0.89 (1556.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:24:12.099575: step 62790, loss = 0.81 (1571.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:24:13.028071: step 62800, loss = 0.70 (1378.6 examples/sec; 0.093 sec/batch)
2017-05-06 22:24:13.760027: step 62810, loss = 0.79 (1748.7 examples/sec; 0.073 sec/batch)
2017-05-06 22:24:14.586111: step 62820, loss = 0.76 (1549.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:24:15.404436: step 62830, loss = 0.84 (1564.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:24:16.222537: step 62840, loss = 0.66 (1564.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:24:17.040439: step 62850, loss = 0.78 (1565.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:24:17.863968: step 62860, loss = 0.87 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:24:18.684693: step 62870, loss = 0.68 (1559.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:24:19.503777: step 62880, loss = 0.86 (1562.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:24:20.330656: step 62890, loss = 0.88 (1548.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:24:21.249957: step 62900, loss = 0.90 (1392.4 examples/sec; 0.092 sec/batch)
2017-05-06 22:24:21.978317: step 62910, loss = 0.75 (1757.4 examples/sec; 0.073 sec/batch)
2017-05-06 22:24:22.795909: step 62920, loss = 0.77 (1565.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:24:23.609056: step 62930, loss = 0.79 (1574.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:24:24.431488: step 62940, loss = 0.94 (1556.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:24:25.256188: step 62950, loss = 0.60 (1552.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:24:26.082444: step 62960, loss = 0.63 (1549.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:24:26.913450: step 62970, loss = 0.64 (1540.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:24:27.735740: step 62980, loss = 0.68 (1556.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:24:28.560747: step 62990, loss = 0.75 (1551.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:24:29.488064: step 63000, loss = 0.69 (1380.3 examples/sec; 0.093 sec/batch)
2017-05-06 22:24:30.211703: step 63010, loss = 0.75 (1768.8 examples/sec; 0.072 sec/batch)
2017-05-06 22:24:31.035063: step 63020, loss = 0.78 (1554.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:24:31.856531: step 63030, loss = 0.72 (1558.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:24:32.676723: step 63040, loss = 0.70 (1560.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:24:33.498550: step 63050, loss = 0.74 (1557.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:24:34.315268: step 63060, loss = 0.75 (1567.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:24:35.149915: step 63070, loss = 0.86 (1533.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:24:35.967657: step 63080, loss = 0.88 (1565.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:24:36.795727: step 63090, loss = 0.70 (1545.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:24:37.732263: step 63100, loss = 0.83 (1366.7 examples/sec; 0.094 sec/batch)
2017-05-06 22:24:38.448806: step 63110, loss = 0.74 (1786.3 examples/sec; 0.072 sec/batch)
2017-05-06 22:24:39.259452: step 63120, loss = 0.76 (1579.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:24:40.070279: step 63130, loss = 0.78 (1578.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:24:40.894895: step 63140, loss = 0.66 (1552.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:24:41.721061: step 63150, loss = 0.78 (1549.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:24:42.539555: step 63160, loss = 0.76 (1563.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:24:43.366327: step 63170, loss = 0.65 (1548.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:24:44.177781: step 63180, loss = 0.76 (1577.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:24:44.992647: step 63190, loss = 0.71 (1570.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:24:45.905893: step 63200, loss = 0.72 (1401.6 examples/sec; 0.091 sec/batch)
2017-05-06 22:24:46.627207: step 63210, loss = 0.94 (1774.5 examples/sec; 0.072 sec/batch)
2017-05-06 22:24:47.443162: step 63220, loss = 0.67 (1568.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:24:48.270567: step 63230, loss = 0.76 (1547.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:24:49.089440: step 63240, loss = 0.69 (1563.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:24:49.918577: step 63250, loss = 0.75 (1543.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:24:50.750940: step 63260, loss = 0.76 (1537.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:24:51.576865: step 63270, loss = 0.65 (1549.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:24:52.392710: step 63280, loss = 0.70 (1568.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:24:53.212210: step 63290, loss = 0.86 (1561.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:24:54.147694: step 63300, loss = 0.83 (1368.3 examples/sec; 0.094 sec/batch)
2017-05-06 22:24:54.870747: step 63310, loss = 0.71 (1770.3 examples/sec; 0.072 sec/batch)
2017-05-06 22:24:55.680668: step 63320, loss = 0.90 (1580.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:24:56.507108: step 63330, loss = 0.65 (1548.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:24:57.329894: step 63340, loss = 0.67 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:24:58.152139: step 63350, loss = 0.72 (1556.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:24:58.998147: step 63360, loss = 0.71 (1513.0 examples/sec; 0.085 sec/batch)
2017-05-06 22:24:59.790059: step 63370, loss = 0.67 (1616.4 examples/sec; 0.079 sec/batch)
2017-05-06 22:25:00.610336: step 63380, loss = 0.84 (1560.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:01.438331: step 63390, loss = 0.70 (1545.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:25:02.390070: step 63400, loss = 0.73 (1344.9 examples/sec; 0.095 sec/batch)
2017-05-06 22:25:03.089583: step 63410, loss = 0.71 (1829.8 examples/sec; 0.070 sec/batch)
2017-05-06 22:25:03.910904: step 63420, loss = 0.77 (1558.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:04.737285: step 63430, loss = 0.91 (1548.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:25:05.560031: step 63440, loss = 0.80 (1555.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:06.380195: step 63450, loss = 0.75 (1560.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:07.217899: step 63460, loss = 0.73 (1528.0 examples/sec; 0.084 sec/batch)
2017-05-06 22:25:08.026128: step 63470, loss = 0.67 (1583.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:25:08.853126: step 63480, loss = 0.76 (1547.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:25:09.678716: step 63490, loss = 0.78 (1550.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:25:10.604543: step 63500, loss = 0.69 (1382.5 examples/sec; 0.093 sec/batch)
2017-05-06 22:25:11.330667: step 63510, loss = 0.73 (1762.8 examples/sec; 0.073 sec/batch)
2017-05-06 22:25:12.148406: step 63520, loss = 0.78 (1565.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:12.963537: step 63530, loss = 0.62 (1570.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:13.781513: step 63540, loss = 0.71 (1564.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:14.602372: step 63550, loss = 0.86 (1559.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:15.434191: step 63560, loss = 0.78 (1538.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:25:16.257965: step 63570, loss = 0.73 (1553.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:17.074237: step 63580, loss = 0.65 (1568.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:17.896635: step 63590, loss = 0.76 (1556.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:18.813183: step 63600, loss = 0.61 (1396.5 examples/sec; 0.092 sec/batch)
2017-05-06 22:25:19.535504: step 63610, loss = 0.84 (1772.1 examples/sec; 0.072 sec/batch)
2017-05-06 22:25:20.353235: step 63620, loss = 0.67 (1565.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:21.174184: step 63630, loss = 0.74 (1559.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:21.989663: step 63640, loss = 0.78 (1569.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:22.820376: step 63650, loss = 0.82 (1540.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:25:23.637153: step 63660, loss = 0.73 (1567.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:24.450293: step 63670, loss = 0.61 (1574.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:25:25.273254: step 63680, loss = 0.68 (1555.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:26.105272: step 63690, loss = 0.72 (1538.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:25:27.015259: step 63700, loss = 0.76 (1406.6 examples/sec; 0.091 sec/batch)
2017-05-06 22:25:27.731139: step 63710, loss = 0.71 (1788.0 examples/sec; 0.072 sec/batch)
2017-05-06 22:25:28.558372: step 63720, loss = 0.73 (1547.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:25:29.379976: step 63730, loss = 0.66 (1557.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:30.201764: step 63740, loss = 0.94 (1557.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:31.015292: step 63750, loss = 0.71 (1573.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:25:31.834969: step 63760, loss = 0.72 (1561.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:32.658652: step 63770, loss = 0.89 (1554.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:33.487286: step 63780, loss = 0.86 (1544.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:25:34.326676: step 63790, loss = 0.76 (1524.9 examples/sec; 0.084 sec/batch)
2017-05-06 22:25:35.256840: step 63800, loss = 0.69 (1376.1 examples/sec; 0.093 sec/batch)
2017-05-06 22:25:35.975653: step 63810, loss = 0.79 (1780.7 examples/sec; 0.072 sec/batch)
2017-05-06 22:25:36.802961: step 63820, loss = 0.74 (1547.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:25:37.631026: step 63830, loss = 0.76 (1545.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:25:38.452221: step 63840, loss = 0.63 (1558.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:39.275446: step 63850, loss = 0.61 (1554.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:40.088008: step 63860, loss = 0.75 (1575.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:25:40.907274: step 63870, loss = 0.88 (1562.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:41.721023: step 63880, loss = 0.77 (1573.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:25:42.539606: step 63890, loss = 0.80 (1563.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:43.457310: step 63900, loss = 0.81 (1394.8 examples/sec; 0.092 sec/batch)
2017-05-06 22:25:44.168468: step 63910, loss = 0.74 (1799.9 examples/sec; 0.071 sec/batch)
2017-05-06 22:25:44.981281: step 63920, loss = 0.78 (1574.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:25:45.796460: step 63930, loss = 0.75 (1570.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:46.621815: step 63940, loss = 0.75 (1550.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:25:47.437348: step 63950, loss = 0.81 (1569.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:48.252891: step 63960, loss = 0.75 (1569.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:49.078216: step 63970, loss = 0.73 (1550.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:25:49.897687: step 63980, loss = 0.64 (1562.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:50.726635: step 63990, loss = 0.95 (1544.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:25:51.645701: step 64000, loss = 0.71 (1392.7 examples/sec; 0.092 sec/batch)
2017-05-06 22:25:52.370682: step 64010, loss = 0.84 (1765.6 examples/sec; 0.072 sec/batch)
2017-05-06 22:25:53.194276: step 64020, loss = 0.73 (1554.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:54.016213: step 64030, loss = 0.88 (1557.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:54.836972: step 64040, loss = 0.63 (1559.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:55.662752: step 64050, loss = 0.58 (1550.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:25:56.486925: step 64060, loss = 0.78 (1553.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:57.308294: step 64070, loss = 0.78 (1558.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:58.142018: step 64080, loss = 0.72 (1535.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:25:58.957861: step 64090, loss = 0.72 (1568.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:25:59.878757: step 64100, loss = 0.83 (1389.9 examples/sec; 0.092 sec/batch)
2017-05-06 22:26:00.598119: step 64110, loss = 0.75 (1779.4 examples/sec; 0.072 sec/batch)
2017-05-06 22:26:01.412345: step 64120, loss = 0.71 (1572.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:26:02.238016: step 64130, loss = 0.77 (1550.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:26:03.059695: step 64140, loss = 0.53 (1557.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:26:03.867425: step 64150, loss = 0.73 (1584.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:26:04.686634: step 64160, loss = 0.77 (1562.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:26:05.511896: step 64170, loss = 0.59 (1551.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:26:06.333318: step 64180, loss = 0.63 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:26:07.147149: step 64190, loss = 0.67 (1572.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:26:08.062241: step 64200, loss = 0.76 (1398.8 examples/sec; 0.092 sec/batch)
2017-05-06 22:26:08.789913: step 64210, loss = 0.75 (1759.0 examples/sec; 0.073 sec/batch)
2017-05-06 22:26:09.614164: step 64220, loss = 0.82 (1552.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:26:10.433682: step 64230, loss = 0.55 (1561.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:26:11.259812: step 64240, loss = 0.68 (1549.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:26:12.073029: step 64250, loss = 0.60 (1574.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:26:12.900269: step 64260, loss = 0.68 (1547.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:26:13.725288: step 64270, loss = 0.66 (1551.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:26:14.540585: step 64280, loss = 0.73 (1570.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:26:15.363096: step 64290, loss = 0.81 (1556.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:26:16.279477: step 64300, loss = 0.89 (1396.8 examples/sec; 0.092 sec/batch)
2017-05-06 22:26:17.004461: step 64310, loss = 0.83 (1765.5 examples/sec; 0.072 sec/batch)
2017-05-06 22:26:17.829197: step 64320, loss = 1.00 (1552.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:26:18.654031: step 64330, loss = 0.82 (1551.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:26:19.475276: step 64340, loss = 0.76 (1558.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:26:20.312522: step 64350, loss = 0.78 (1528.8 examples/sec; 0.084 sec/batch)
2017-05-06 22:26:21.117034: step 64360, loss = 0.71 (1591.0 examples/sec; 0.080 sec/batch)
2017-05-06 22:26:21.943792: step 64370, loss = 0.64 (1548.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:26:22.771156: step 64380, loss = 0.87 (1547.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:26:23.594688: step 64390, loss = 0.81 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:26:24.522971: step 64400, loss = 0.75 (1378.9 examples/sec; 0.093 sec/batch)
2017-05-06 22:26:25.234095: step 64410, loss = 0.64 (1800.0 examples/sec; 0.071 sec/batch)
2017-05-06 22:26:26.049858: step 64420, loss = 0.67 (1569.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:26:26.876183: step 64430, loss = 0.75 (1549.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:26:27.690720: step 64440, loss = 0.86 (1571.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:26:28.518145: step 64450, loss = 0.71 (1546.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:26:29.336687: step 64460, loss = 0.79 (1563.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:26:30.169327: step 64470, loss = 0.76 (1537.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:26:30.982803: step 64480, loss = 0.81 (1573.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:26:31.799243: step 64490, loss = 0.64 (1567.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:26:32.732229: step 64500, loss = 0.68 (1371.9 examples/sec; 0.093 sec/batch)
2017-05-06 22:26:33.456872: step 64510, loss = 0.71 (1766.4 examples/sec; 0.072 sec/batch)
2017-05-06 22:26:34.278976: step 64520, loss = 0.66 (1557.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:26:35.092225: step 64530, loss = 0.75 (1573.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:26:35.917398: step 64540, loss = 0.60 (1551.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:26:36.731734: step 64550, loss = 0.78 (1571.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:26:37.557631: step 64560, loss = 0.59 (1549.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:26:38.386016: step 64570, loss = 0.67 (1545.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:26:39.209137: step 64580, loss = 0.62 (1555.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:26:40.022208: step 64590, loss = 0.77 (1574.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:26:40.941877: step 64600, loss = 0.76 (1391.8 examples/sec; 0.092 sec/batch)
2017-05-06 22:26:41.674417: step 64610, loss = 0.69 (1747.4 examples/sec; 0.073 sec/batch)
2017-05-06 22:26:42.496770: step 64620, loss = 0.67 (1556.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:26:43.314590: step 64630, loss = 0.68 (1565.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:26:44.122777: step 64640, loss = 0.76 (1583.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:26:44.955604: step 64650, loss = 0.60 (1536.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:26:45.782914: step 64660, loss = 0.78 (1547.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:26:46.609278: step 64670, loss = 0.61 (1549.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:26:47.417247: step 64680, loss = 0.80 (1584.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:26:48.236260: step 64690, loss = 0.61 (1562.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:26:49.148040: step 64700, loss = 0.73 (1403.9 examples/sec; 0.091 sec/batch)
2017-05-06 22:26:49.879312: step 64710, loss = 0.69 (1750.4 examples/sec; 0.073 sec/batch)
2017-05-06 22:26:50.698356: step 64720, loss = 0.56 (1562.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:26:51.509816: step 64730, loss = 0.73 (1577.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:26:52.330319: step 64740, loss = 0.71 (1560.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:26:53.150300: step 64750, loss = 0.69 (1561.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:26:53.974323: step 64760, loss = 0.76 (1552.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:26:54.797960: step 64770, loss = 0.65 (1554.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:26:55.611258: step 64780, loss = 0.64 (1573.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:26:56.432809: step 64790, loss = 0.75 (1558.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:26:57.349764: step 64800, loss = 0.81 (1395.9 examples/sec; 0.092 sec/batch)
2017-05-06 22:26:58.080070: step 64810, loss = 0.74 (1752.7 examples/sec; 0.073 sec/batch)
2017-05-06 22:26:58.897894: step 64820, loss = 0.69 (1565.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:26:59.718153: step 64830, loss = 0.69 (1560.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:00.540484: step 64840, loss = 0.81 (1556.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:01.364904: step 64850, loss = 0.74 (1552.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:02.193538: step 64860, loss = 0.74 (1544.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:27:03.015072: step 64870, loss = 0.85 (1558.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:03.842247: step 64880, loss = 0.81 (1547.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:27:04.664487: step 64890, loss = 0.65 (1556.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:05.579366: step 64900, loss = 0.75 (1399.1 examples/sec; 0.091 sec/batch)
2017-05-06 22:27:06.311529: step 64910, loss = 0.58 (1748.2 examples/sec; 0.073 sec/batch)
2017-05-06 22:27:07.134319: step 64920, loss = 0.81 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:07.951335: step 64930, loss = 0.66 (1566.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:08.774143: step 64940, loss = 0.81 (1555.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:09.592867: step 64950, loss = 0.86 (1563.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:10.424436: step 64960, loss = 0.78 (1539.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:27:11.238190: step 64970, loss = 0.63 (1572.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:27:12.055913: step 64980, loss = 0.86 (1565.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:12.870208: step 64990, loss = 0.72 (1571.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:27:13.794165: step 65000, loss = 0.75 (1385.3 examples/sec; 0.092 sec/batch)
2017-05-06 22:27:14.515696: step 65010, loss = 0.77 (1774.0 examples/sec; 0.072 sec/batch)
2017-05-06 22:27:15.332624: step 65020, loss = 0.67 (1566.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:16.151102: step 65030, loss = 0.80 (1563.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:16.979385: step 65040, loss = 0.62 (1545.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:27:17.802348: step 65050, loss = 0.79 (1555.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:18.628428: step 65060, loss = 0.70 (1549.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:27:19.444786: step 65070, loss = 0.57 (1567.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:20.257977: step 65080, loss = 0.82 (1574.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:27:21.082710: step 65090, loss = 0.70 (1552.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:21.994263: step 65100, loss = 0.65 (1404.2 examples/sec; 0.091 sec/batch)
2017-05-06 22:27:22.719387: step 65110, loss = 0.69 (1765.2 examples/sec; 0.073 sec/batch)
2017-05-06 22:27:23.545839: step 65120, loss = 0.85 (1548.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:27:24.366175: step 65130, loss = 0.64 (1560.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:25.189844: step 65140, loss = 0.86 (1554.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:26.022501: step 65150, loss = 0.65 (1537.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:27:26.845871: step 65160, loss = 0.55 (1554.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:27.661198: step 65170, loss = 0.82 (1569.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:28.478085: step 65180, loss = 0.82 (1566.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:29.307579: step 65190, loss = 0.77 (1543.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:27:30.222650: step 65200, loss = 0.72 (1398.8 examples/sec; 0.092 sec/batch)
2017-05-06 22:27:30.946184: step 65210, loss = 0.75 (1769.1 examples/sec; 0.072 sec/batch)
2017-05-06 22:27:31.756602: step 65220, loss = 0.63 (1579.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:27:32.576973: step 65230, loss = 0.73 (1560.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:33.401512: step 65240, loss = 0.70 (1552.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:34.232445: step 65250, loss = 0.72 (1540.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:27:35.056594: step 65260, loss = 0.77 (1553.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:35.876073: step 65270, loss = 0.75 (1562.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:36.693536: step 65280, loss = 0.82 (1565.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:37.518665: step 65290, loss = 0.57 (1551.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:27:38.432670: step 65300, loss = 0.92 (1400.4 examples/sec; 0.091 sec/batch)
2017-05-06 22:27:39.151728: step 65310, loss = 0.74 (1780.1 examples/sec; 0.072 sec/batch)
2017-05-06 22:27:39.968922: step 65320, loss = 0.79 (1566.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:40.797712: step 65330, loss = 0.70 (1544.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:27:41.646560: step 65340, loss = 0.73 (1507.9 examples/sec; 0.085 sec/batch)
2017-05-06 22:27:42.441002: step 65350, loss = 0.75 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-06 22:27:43.258309: step 65360, loss = 0.73 (1566.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:44.078632: step 65370, loss = 0.68 (1560.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:44.904120: step 65380, loss = 0.58 (1550.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:27:45.716498: step 65390, loss = 0.77 (1575.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:27:46.644468: step 65400, loss = 0.81 (1379.4 examples/sec; 0.093 sec/batch)
2017-05-06 22:27:47.363338: step 65410, loss = 0.75 (1780.6 examples/sec; 0.072 sec/batch)
2017-05-06 22:27:48.183608: step 65420, loss = 0.77 (1560.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:49.009642: step 65430, loss = 0.67 (1549.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:27:49.837753: step 65440, loss = 0.72 (1545.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:27:50.652787: step 65450, loss = 0.79 (1570.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:51.471111: step 65460, loss = 0.78 (1564.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:52.282509: step 65470, loss = 0.60 (1577.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:27:53.116176: step 65480, loss = 0.80 (1535.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:27:53.942869: step 65490, loss = 0.69 (1548.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:27:54.863385: step 65500, loss = 0.75 (1390.5 examples/sec; 0.092 sec/batch)
2017-05-06 22:27:55.576800: step 65510, loss = 0.65 (1794.2 examples/sec; 0.071 sec/batch)
2017-05-06 22:27:56.397345: step 65520, loss = 0.67 (1559.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:57.215409: step 65530, loss = 0.75 (1564.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:58.046600: step 65540, loss = 0.69 (1540.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:27:58.869813: step 65550, loss = 0.85 (1554.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:27:59.693028: step 65560, loss = 0.73 (1554.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:00.519569: step 65570, loss = 0.93 (1548.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:28:01.347780: step 65580, loss = 0.78 (1545.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:28:02.188399: step 65590, loss = 0.76 (1522.7 examples/sec; 0.084 sec/batch)
2017-05-06 22:28:03.107578: step 65600, loss = 0.75 (1392.5 examples/sec; 0.092 sec/batch)
2017-05-06 22:28:03.835145: step 65610, loss = 0.66 (1759.3 examples/sec; 0.073 sec/batch)
2017-05-06 22:28:04.657790: step 65620, loss = 0.79 (1556.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:05.485161: step 65630, loss = 0.89 (1547.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:28:06.307943: step 65640, loss = 0.82 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:07.137217: step 65650, loss = 0.82 (1543.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:28:07.954101: step 65660, loss = 0.70 (1566.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:08.778529: step 65670, loss = 0.59 (1552.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:09.603586: step 65680, loss = 0.71 (1551.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:28:10.430592: step 65690, loss = 0.63 (1547.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:28:11.352469: step 65700, loss = 0.76 (1388.5 examples/sec; 0.092 sec/batch)
2017-05-06 22:28:12.075346: step 65710, loss = 0.66 (1770.7 examples/sec; 0.072 sec/batch)
2017-05-06 22:28:12.899786: step 65720, loss = 0.78 (1552.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:13.736616: step 65730, loss = 0.78 (1529.6 examples/sec; 0.084 sec/batch)
2017-05-06 22:28:14.556497: step 65740, loss = 0.60 (1561.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:15.382113: step 65750, loss = 0.85 (1550.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:28:16.193731: step 65760, loss = 0.78 (1577.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:28:17.022204: step 65770, loss = 0.61 (1545.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:28:17.850058: step 65780, loss = 0.69 (1546.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:28:18.667036: step 65790, loss = 0.76 (1566.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:19.585568: step 65800, loss = 0.71 (1393.5 examples/sec; 0.092 sec/batch)
2017-05-06 22:28:20.305448: step 65810, loss = 0.85 (1778.1 examples/sec; 0.072 sec/batch)
2017-05-06 22:28:21.127804: step 65820, loss = 0.81 (1556.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:21.952953: step 65830, loss = 0.93 (1551.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:28:22.782737: step 65840, loss = 0.86 (1542.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:28:23.606556: step 65850, loss = 0.75 (1553.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:24.421011: step 65860, loss = 0.77 (1571.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:28:25.241286: step 65870, loss = 0.79 (1560.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:26.058935: step 65880, loss = 0.80 (1565.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:26.884163: step 65890, loss = 0.71 (1551.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:28:27.811230: step 65900, loss = 0.69 (1380.7 examples/sec; 0.093 sec/batch)
2017-05-06 22:28:28.531036: step 65910, loss = 0.69 (1778.2 examples/sec; 0.072 sec/batch)
2017-05-06 22:28:29.355178: step 65920, loss = 0.70 (1553.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:30.168153: step 65930, loss = 0.65 (1574.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:28:30.992818: step 65940, loss = 0.74 (1552.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:31.806376: step 65950, loss = 0.69 (1573.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:28:32.616498: step 65960, loss = 0.82 (1580.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:28:33.439005: step 65970, loss = 0.79 (1556.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:34.259022: step 65980, loss = 0.76 (1560.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:35.080900: step 65990, loss = 0.59 (1557.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:35.993706: step 66000, loss = 0.64 (1402.3 examples/sec; 0.091 sec/batch)
2017-05-06 22:28:36.715111: step 66010, loss = 0.80 (1774.3 examples/sec; 0.072 sec/batch)
2017-05-06 22:28:37.533699: step 66020, loss = 0.77 (1563.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:38.352654: step 66030, loss = 0.61 (1563.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:39.179821: step 66040, loss = 0.67 (1547.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:28:39.988438: step 66050, loss = 0.61 (1582.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:28:40.809886: step 66060, loss = 0.71 (1558.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:41.636254: step 66070, loss = 0.78 (1548.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:28:42.456678: step 66080, loss = 0.69 (1560.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:43.281198: step 66090, loss = 0.82 (1552.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:44.187193: step 66100, loss = 0.73 (1412.8 examples/sec; 0.091 sec/batch)
2017-05-06 22:28:44.914198: step 66110, loss = 0.79 (1760.6 examples/sec; 0.073 sec/batch)
2017-05-06 22:28:45.733786: step 66120, loss = 0.79 (1561.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:46.562825: step 66130, loss = 0.70 (1544.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:28:47.380757: step 66140, loss = 0.66 (1564.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:48.189289: step 66150, loss = 0.71 (1583.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:28:49.006846: step 66160, loss = 0.76 (1565.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:49.823737: step 66170, loss = 0.73 (1566.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:50.645205: step 66180, loss = 0.89 (1558.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:51.462545: step 66190, loss = 0.90 (1566.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:52.398925: step 66200, loss = 0.65 (1367.0 examples/sec; 0.094 sec/batch)
2017-05-06 22:28:53.099798: step 66210, loss = 0.60 (1826.3 examples/sec; 0.070 sec/batch)
2017-05-06 22:28:53.918974: step 66220, loss = 0.76 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:54.744754: step 66230, loss = 0.67 (1550.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:28:55.567492: step 66240, loss = 0.93 (1555.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:56.391379: step 66250, loss = 0.77 (1553.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:57.214978: step 66260, loss = 0.74 (1554.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:58.031032: step 66270, loss = 0.76 (1568.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:28:58.858020: step 66280, loss = 0.82 (1547.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:28:59.662308: step 66290, loss = 0.80 (1591.5 examples/sec; 0.080 sec/batch)
2017-05-06 22:29:00.576387: step 66300, loss = 0.73 (1400.3 examples/sec; 0.091 sec/batch)
2017-05-06 22:29:01.302436: step 66310, loss = 0.64 (1762.9 examples/sec; 0.073 sec/batch)
2017-05-06 22:29:02.124546: step 66320, loss = 0.63 (1557.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:02.974669: step 66330, loss = 0.74 (1505.7 examples/sec; 0.085 sec/batch)
2017-05-06 22:29:03.768851: step 66340, loss = 0.70 (1611.7 examples/sec; 0.079 sec/batch)
2017-05-06 22:29:04.586802: step 66350, loss = 0.79 (1564.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:05.409940: step 66360, loss = 0.79 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:06.234455: step 66370, loss = 0.61 (1552.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:07.058363: step 66380, loss = 0.71 (1553.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:07.874863: step 66390, loss = 0.73 (1567.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:08.803131: step 66400, loss = 0.78 (1378.9 examples/sec; 0.093 sec/batch)
2017-05-06 22:29:09.516655: step 66410, loss = 0.65 (1793.9 examples/sec; 0.071 sec/batch)
2017-05-06 22:29:10.337982: step 66420, loss = 0.80 (1558.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:11.158006: step 66430, loss = 0.78 (1560.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:11.975117: step 66440, loss = 0.72 (1566.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:12.792928: step 66450, loss = 0.74 (1565.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:13.618990: step 66460, loss = 0.61 (1549.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:29:14.440982: step 66470, loss = 0.74 (1557.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:15.257594: step 66480, loss = 0.59 (1567.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:16.081136: step 66490, loss = 0.64 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:17.001408: step 66500, loss = 0.82 (1390.9 examples/sec; 0.092 sec/batch)
2017-05-06 22:29:17.718367: step 66510, loss = 0.69 (1785.3 examples/sec; 0.072 sec/batch)
2017-05-06 22:29:18.547921: step 66520, loss = 0.87 (1543.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:29:19.371944: step 66530, loss = 0.57 (1553.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:20.188472: step 66540, loss = 0.84 (1567.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:21.012955: step 66550, loss = 0.72 (1552.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:21.834901: step 66560, loss = 0.65 (1557.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:22.651644: step 66570, loss = 0.78 (1567.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:23.475191: step 66580, loss = 0.76 (1554.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:24.291868: step 66590, loss = 0.73 (1567.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:25.212400: step 66600, loss = 0.70 (1390.5 examples/sec; 0.092 sec/batch)
2017-05-06 22:29:25.937890: step 66610, loss = 0.76 (1764.3 examples/sec; 0.073 sec/batch)
2017-05-06 22:29:26.757363: step 66620, loss = 0.62 (1562.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:27.572956: step 66630, loss = 0.66 (1569.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:28.402465: step 66640, loss = 0.69 (1543.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:29:29.218378: step 66650, loss = 0.77 (1568.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:30.036914: step 66660, loss = 0.85 (1563.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:30.852000: step 66670, loss = 0.64 (1570.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:31.667667: step 66680, loss = 0.78 (1569.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:32.493502: step 66690, loss = 0.71 (1549.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:29:33.440356: step 66700, loss = 0.67 (1351.8 examples/sec; 0.095 sec/batch)
2017-05-06 22:29:34.147951: step 66710, loss = 0.73 (1808.9 examples/sec; 0.071 sec/batch)
2017-05-06 22:29:34.972616: step 66720, loss = 0.73 (1552.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:35.798783: step 66730, loss = 0.62 (1549.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:29:36.626400: step 66740, loss = 0.65 (1546.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:29:37.451283: step 66750, loss = 0.73 (1551.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:38.283079: step 66760, loss = 0.76 (1538.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:29:39.099751: step 66770, loss = 0.58 (1567.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:39.918032: step 66780, loss = 0.76 (1564.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:40.740739: step 66790, loss = 0.67 (1555.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:41.657324: step 66800, loss = 0.68 (1396.5 examples/sec; 0.092 sec/batch)
2017-05-06 22:29:42.385924: step 66810, loss = 0.85 (1756.8 examples/sec; 0.073 sec/batch)
2017-05-06 22:29:43.210122: step 66820, loss = 0.72 (1553.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:44.029801: step 66830, loss = 0.63 (1561.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:44.856103: step 66840, loss = 0.82 (1549.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:29:45.675483: step 66850, loss = 0.71 (1562.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:46.497038: step 66860, loss = 0.78 (1558.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:47.316800: step 66870, loss = 0.86 (1561.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:48.137663: step 66880, loss = 0.72 (1559.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:48.966659: step 66890, loss = 0.69 (1544.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:29:49.897032: step 66900, loss = 0.88 (1375.8 examples/sec; 0.093 sec/batch)
2017-05-06 22:29:50.616150: step 66910, loss = 0.86 (1780.0 examples/sec; 0.072 sec/batch)
2017-05-06 22:29:51.430610: step 66920, loss = 0.64 (1571.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:29:52.255005: step 66930, loss = 0.79 (1552.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:53.083093: step 66940, loss = 0.69 (1545.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:29:53.904289: step 66950, loss = 0.72 (1558.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:54.729866: step 66960, loss = 0.96 (1550.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:29:55.541625: step 66970, loss = 0.71 (1576.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:29:56.363013: step 66980, loss = 0.66 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:29:57.192601: step 66990, loss = 0.70 (1542.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:29:58.114694: step 67000, loss = 0.76 (1388.2 examples/sec; 0.092 sec/batch)
2017-05-06 22:29:58.840946: step 67010, loss = 0.77 (1762.5 examples/sec; 0.073 sec/batch)
2017-05-06 22:29:59.656186: step 67020, loss = 0.66 (1570.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:00.479785: step 67030, loss = 0.72 (1554.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:01.312622: step 67040, loss = 0.72 (1536.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:30:02.144682: step 67050, loss = 0.78 (1538.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:30:02.974027: step 67060, loss = 0.73 (1543.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:30:03.787834: step 67070, loss = 0.72 (1572.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:30:04.603844: step 67080, loss = 0.69 (1568.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:05.422433: step 67090, loss = 0.79 (1563.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:06.347758: step 67100, loss = 0.71 (1383.3 examples/sec; 0.093 sec/batch)
2017-05-06 22:30:07.071859: step 67110, loss = 0.57 (1767.7 examples/sec; 0.072 sec/batch)
2017-05-06 22:30:07.879354: step 67120, loss = 0.74 (1585.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:30:08.706299: step 67130, loss = 0.75 (1547.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:30:09.519254: step 67140, loss = 0.84 (1574.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:30:10.343114: step 67150, loss = 0.85 (1553.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:11.156807: step 67160, loss = 0.63 (1573.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:30:11.968391: step 67170, loss = 0.64 (1577.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:30:12.787375: step 67180, loss = 0.79 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:13.611347: step 67190, loss = 0.87 (1553.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:14.521901: step 67200, loss = 0.76 (1405.7 examples/sec; 0.091 sec/batch)
2017-05-06 22:30:15.248233: step 67210, loss = 0.70 (1762.3 examples/sec; 0.073 sec/batch)
2017-05-06 22:30:16.068512: step 67220, loss = 0.82 (1560.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:16.885563: step 67230, loss = 0.81 (1566.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:17.701301: step 67240, loss = 0.83 (1569.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:18.532276: step 67250, loss = 0.57 (1540.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:30:19.349651: step 67260, loss = 0.72 (1566.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:20.171130: step 67270, loss = 0.72 (1558.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:20.992028: step 67280, loss = 0.94 (1559.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:21.813271: step 67290, loss = 0.77 (1558.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:22.740561: step 67300, loss = 0.83 (1380.4 examples/sec; 0.093 sec/batch)
2017-05-06 22:30:23.462119: step 67310, loss = 0.65 (1773.9 examples/sec; 0.072 sec/batch)
2017-05-06 22:30:24.304281: step 67320, loss = 0.76 (1519.9 examples/sec; 0.084 sec/batch)
2017-05-06 22:30:25.102376: step 67330, loss = 0.74 (1603.8 examples/sec; 0.080 sec/batch)
2017-05-06 22:30:25.920651: step 67340, loss = 0.71 (1564.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:26.741002: step 67350, loss = 0.71 (1560.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:27.559967: step 67360, loss = 0.62 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:28.372637: step 67370, loss = 0.80 (1575.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:30:29.193718: step 67380, loss = 0.95 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:30.020877: step 67390, loss = 0.70 (1547.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:30:30.946167: step 67400, loss = 0.63 (1383.3 examples/sec; 0.093 sec/batch)
2017-05-06 22:30:31.660243: step 67410, loss = 0.77 (1792.5 examples/sec; 0.071 sec/batch)
2017-05-06 22:30:32.477811: step 67420, loss = 0.81 (1565.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:33.301279: step 67430, loss = 0.75 (1554.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:34.112277: step 67440, loss = 0.80 (1578.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:30:34.928515: step 67450, loss = 0.71 (1568.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:35.738127: step 67460, loss = 0.74 (1581.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:30:36.556153: step 67470, loss = 0.68 (1564.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:37.378647: step 67480, loss = 0.65 (1556.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:38.199836: step 67490, loss = 0.64 (1558.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:39.115644: step 67500, loss = 0.81 (1397.7 examples/sec; 0.092 sec/batch)
2017-05-06 22:30:39.828652: step 67510, loss = 0.65 (1795.2 examples/sec; 0.071 sec/batch)
2017-05-06 22:30:40.664329: step 67520, loss = 0.61 (1531.7 examples/sec; 0.084 sec/batch)
2017-05-06 22:30:41.483166: step 67530, loss = 0.79 (1563.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:42.291584: step 67540, loss = 0.68 (1583.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:30:43.110589: step 67550, loss = 0.75 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:43.927491: step 67560, loss = 0.82 (1566.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:44.750861: step 67570, loss = 0.82 (1554.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:45.575910: step 67580, loss = 0.80 (1551.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:30:46.403750: step 67590, loss = 0.84 (1546.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:30:47.326792: step 67600, loss = 0.83 (1386.7 examples/sec; 0.092 sec/batch)
2017-05-06 22:30:48.045835: step 67610, loss = 0.74 (1780.1 examples/sec; 0.072 sec/batch)
2017-05-06 22:30:48.872409: step 67620, loss = 0.80 (1548.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:30:49.701553: step 67630, loss = 0.66 (1543.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:30:50.518404: step 67640, loss = 0.86 (1567.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:51.340775: step 67650, loss = 0.77 (1556.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:52.157484: step 67660, loss = 0.58 (1567.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:52.977374: step 67670, loss = 0.65 (1561.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:53.794147: step 67680, loss = 0.71 (1567.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:54.617359: step 67690, loss = 0.73 (1554.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:55.529721: step 67700, loss = 0.80 (1403.0 examples/sec; 0.091 sec/batch)
2017-05-06 22:30:56.255879: step 67710, loss = 0.77 (1762.7 examples/sec; 0.073 sec/batch)
2017-05-06 22:30:57.079123: step 67720, loss = 0.75 (1554.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:57.902739: step 67730, loss = 0.64 (1554.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:58.726486: step 67740, loss = 0.87 (1553.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:30:59.549018: step 67750, loss = 0.68 (1556.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:00.363184: step 67760, loss = 0.64 (1572.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:31:01.178074: step 67770, loss = 0.75 (1570.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:31:02.004137: step 67780, loss = 0.77 (1549.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:31:02.829877: step 67790, loss = 0.78 (1550.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:31:03.747779: step 67800, loss = 0.68 (1394.5 examples/sec; 0.092 sec/batch)
2017-05-06 22:31:04.476375: step 67810, loss = 0.69 (1756.8 examples/sec; 0.073 sec/batch)
2017-05-06 22:31:05.299069: step 67820, loss = 0.62 (1555.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:06.110656: step 67830, loss = 0.74 (1577.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:31:06.937502: step 67840, loss = 0.69 (1548.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:31:07.752175: step 67850, loss = 0.72 (1571.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:31:08.581292: step 67860, loss = 0.72 (1543.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:31:09.401796: step 67870, loss = 0.71 (1560.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:10.219610: step 67880, loss = 0.75 (1565.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:11.037859: step 67890, loss = 0.85 (1564.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:11.955726: step 67900, loss = 0.64 (1394.5 examples/sec; 0.092 sec/batch)
2017-05-06 22:31:12.678909: step 67910, loss = 0.71 (1769.9 examples/sec; 0.072 sec/batch)
2017-05-06 22:31:13.505034: step 67920, loss = 0.71 (1549.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:31:14.331849: step 67930, loss = 0.82 (1548.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:31:15.150279: step 67940, loss = 0.72 (1564.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:15.963322: step 67950, loss = 0.75 (1574.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:31:16.784610: step 67960, loss = 0.84 (1558.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:17.601951: step 67970, loss = 0.72 (1566.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:18.423784: step 67980, loss = 0.84 (1557.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:19.238866: step 67990, loss = 0.70 (1570.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:20.151626: step 68000, loss = 0.67 (1402.3 examples/sec; 0.091 sec/batch)
2017-05-06 22:31:20.873952: step 68010, loss = 0.67 (1772.0 examples/sec; 0.072 sec/batch)
2017-05-06 22:31:21.697922: step 68020, loss = 0.74 (1553.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:22.520827: step 68030, loss = 0.63 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:23.337492: step 68040, loss = 0.71 (1567.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:24.151668: step 68050, loss = 0.63 (1572.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:31:24.972761: step 68060, loss = 0.72 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:25.799247: step 68070, loss = 0.69 (1548.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:31:26.619183: step 68080, loss = 0.75 (1561.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:27.432584: step 68090, loss = 0.66 (1573.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:31:28.340190: step 68100, loss = 0.68 (1410.3 examples/sec; 0.091 sec/batch)
2017-05-06 22:31:29.057746: step 68110, loss = 0.79 (1783.8 examples/sec; 0.072 sec/batch)
2017-05-06 22:31:29.881923: step 68120, loss = 0.82 (1553.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:30.699929: step 68130, loss = 0.87 (1564.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:31.516670: step 68140, loss = 0.75 (1567.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:32.339304: step 68150, loss = 0.96 (1556.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:33.163983: step 68160, loss = 0.79 (1552.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:33.990071: step 68170, loss = 0.64 (1549.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:31:34.814020: step 68180, loss = 0.72 (1553.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:35.631622: step 68190, loss = 0.90 (1565.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:36.545005: step 68200, loss = 0.76 (1401.4 examples/sec; 0.091 sec/batch)
2017-05-06 22:31:37.273606: step 68210, loss = 0.71 (1756.8 examples/sec; 0.073 sec/batch)
2017-05-06 22:31:38.094901: step 68220, loss = 0.71 (1558.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:38.911802: step 68230, loss = 0.74 (1566.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:39.732799: step 68240, loss = 0.72 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:40.554197: step 68250, loss = 0.70 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:41.374687: step 68260, loss = 0.78 (1560.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:42.196843: step 68270, loss = 0.69 (1556.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:43.021380: step 68280, loss = 0.85 (1552.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:43.835347: step 68290, loss = 0.85 (1572.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:31:44.746669: step 68300, loss = 0.66 (1404.6 examples/sec; 0.091 sec/batch)
2017-05-06 22:31:45.510132: step 68310, loss = 0.85 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-06 22:31:46.302814: step 68320, loss = 0.82 (1614.8 examples/sec; 0.079 sec/batch)
2017-05-06 22:31:47.120855: step 68330, loss = 0.63 (1564.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:47.937924: step 68340, loss = 0.79 (1566.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:48.752082: step 68350, loss = 0.65 (1572.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:31:49.573195: step 68360, loss = 0.74 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:50.392597: step 68370, loss = 0.64 (1562.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:51.211747: step 68380, loss = 0.77 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:52.025963: step 68390, loss = 0.67 (1572.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:31:52.940282: step 68400, loss = 0.58 (1399.9 examples/sec; 0.091 sec/batch)
2017-05-06 22:31:53.666026: step 68410, loss = 0.58 (1763.7 examples/sec; 0.073 sec/batch)
2017-05-06 22:31:54.495631: step 68420, loss = 0.73 (1542.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:31:55.322831: step 68430, loss = 0.70 (1547.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:31:56.138215: step 68440, loss = 0.74 (1569.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:56.960040: step 68450, loss = 0.72 (1557.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:57.781000: step 68460, loss = 0.67 (1559.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:31:58.595263: step 68470, loss = 0.76 (1572.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:31:59.413135: step 68480, loss = 0.67 (1565.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:00.228630: step 68490, loss = 0.66 (1569.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:01.154366: step 68500, loss = 0.89 (1382.7 examples/sec; 0.093 sec/batch)
2017-05-06 22:32:01.878135: step 68510, loss = 0.63 (1768.5 examples/sec; 0.072 sec/batch)
2017-05-06 22:32:02.708215: step 68520, loss = 0.67 (1542.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:32:03.527385: step 68530, loss = 0.71 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:04.340153: step 68540, loss = 0.73 (1574.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:32:05.165982: step 68550, loss = 0.70 (1550.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:32:05.985455: step 68560, loss = 0.75 (1562.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:06.808315: step 68570, loss = 0.72 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:07.626029: step 68580, loss = 0.83 (1565.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:08.446076: step 68590, loss = 0.82 (1560.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:09.361922: step 68600, loss = 0.64 (1397.6 examples/sec; 0.092 sec/batch)
2017-05-06 22:32:10.090149: step 68610, loss = 0.80 (1757.7 examples/sec; 0.073 sec/batch)
2017-05-06 22:32:10.908122: step 68620, loss = 0.74 (1564.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:11.722013: step 68630, loss = 0.68 (1572.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:32:12.561670: step 68640, loss = 0.71 (1524.4 examples/sec; 0.084 sec/batch)
2017-05-06 22:32:13.376366: step 68650, loss = 0.76 (1571.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:32:14.205363: step 68660, loss = 0.69 (1544.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:32:15.023165: step 68670, loss = 0.84 (1565.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:15.848414: step 68680, loss = 0.69 (1551.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:32:16.663249: step 68690, loss = 0.56 (1570.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:32:17.599462: step 68700, loss = 0.74 (1367.2 examples/sec; 0.094 sec/batch)
2017-05-06 22:32:18.315335: step 68710, loss = 0.72 (1788.0 examples/sec; 0.072 sec/batch)
2017-05-06 22:32:19.135956: step 68720, loss = 0.85 (1559.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:19.956072: step 68730, loss = 0.80 (1560.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:20.778486: step 68740, loss = 0.73 (1556.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:21.607675: step 68750, loss = 0.76 (1543.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:32:22.418341: step 68760, loss = 0.72 (1578.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:32:23.241675: step 68770, loss = 0.50 (1554.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:24.063446: step 68780, loss = 0.81 (1557.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:24.884968: step 68790, loss = 0.67 (1558.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:25.805736: step 68800, loss = 0.71 (1390.2 examples/sec; 0.092 sec/batch)
2017-05-06 22:32:26.529734: step 68810, loss = 0.81 (1767.9 examples/sec; 0.072 sec/batch)
2017-05-06 22:32:27.357321: step 68820, loss = 0.71 (1546.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:32:28.182370: step 68830, loss = 0.63 (1551.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:32:29.007946: step 68840, loss = 0.75 (1550.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:32:29.834799: step 68850, loss = 0.74 (1548.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:32:30.670321: step 68860, loss = 0.68 (1532.0 examples/sec; 0.084 sec/batch)
2017-05-06 22:32:31.507344: step 68870, loss = 0.66 (1529.2 examples/sec; 0.084 sec/batch)
2017-05-06 22:32:32.334084: step 68880, loss = 0.80 (1548.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:32:33.155766: step 68890, loss = 0.82 (1557.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:34.079129: step 68900, loss = 0.74 (1386.2 examples/sec; 0.092 sec/batch)
2017-05-06 22:32:34.803528: step 68910, loss = 0.74 (1767.0 examples/sec; 0.072 sec/batch)
2017-05-06 22:32:35.616542: step 68920, loss = 0.67 (1574.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:32:36.433489: step 68930, loss = 0.73 (1566.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:37.254652: step 68940, loss = 0.73 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:38.071498: step 68950, loss = 0.71 (1567.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:38.900102: step 68960, loss = 0.90 (1544.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:32:39.713182: step 68970, loss = 0.84 (1574.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:32:40.531818: step 68980, loss = 0.77 (1563.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:41.352845: step 68990, loss = 0.69 (1559.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:42.287190: step 69000, loss = 0.61 (1369.9 examples/sec; 0.093 sec/batch)
2017-05-06 22:32:43.005511: step 69010, loss = 0.78 (1781.9 examples/sec; 0.072 sec/batch)
2017-05-06 22:32:43.828675: step 69020, loss = 0.74 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:44.654620: step 69030, loss = 0.64 (1549.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:32:45.482437: step 69040, loss = 0.88 (1546.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:32:46.306289: step 69050, loss = 0.73 (1553.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:47.130905: step 69060, loss = 0.76 (1552.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:47.951958: step 69070, loss = 0.68 (1559.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:48.775192: step 69080, loss = 0.89 (1554.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:49.599845: step 69090, loss = 0.86 (1552.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:50.514951: step 69100, loss = 0.70 (1398.7 examples/sec; 0.092 sec/batch)
2017-05-06 22:32:51.232391: step 69110, loss = 0.80 (1784.1 examples/sec; 0.072 sec/batch)
2017-05-06 22:32:52.055792: step 69120, loss = 0.77 (1554.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:52.876971: step 69130, loss = 0.70 (1558.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:53.695290: step 69140, loss = 0.70 (1564.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:54.511316: step 69150, loss = 0.69 (1568.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:55.343710: step 69160, loss = 0.74 (1537.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:32:56.156151: step 69170, loss = 0.66 (1575.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:32:56.979157: step 69180, loss = 0.71 (1555.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:57.801359: step 69190, loss = 0.69 (1556.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:32:58.726890: step 69200, loss = 0.78 (1383.0 examples/sec; 0.093 sec/batch)
2017-05-06 22:32:59.446710: step 69210, loss = 0.63 (1778.2 examples/sec; 0.072 sec/batch)
2017-05-06 22:33:00.271024: step 69220, loss = 0.67 (1552.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:01.087874: step 69230, loss = 0.66 (1567.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:01.905243: step 69240, loss = 0.84 (1566.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:02.730259: step 69250, loss = 0.76 (1551.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:33:03.543686: step 69260, loss = 0.80 (1573.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:33:04.366221: step 69270, loss = 0.73 (1556.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:05.185789: step 69280, loss = 0.65 (1561.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:05.999724: step 69290, loss = 0.78 (1572.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:33:06.946495: step 69300, loss = 0.86 (1352.0 examples/sec; 0.095 sec/batch)
2017-05-06 22:33:07.641181: step 69310, loss = 0.88 (1842.6 examples/sec; 0.069 sec/batch)
2017-05-06 22:33:08.460906: step 69320, loss = 0.70 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:09.281067: step 69330, loss = 0.85 (1560.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:10.107055: step 69340, loss = 0.70 (1549.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:33:10.922355: step 69350, loss = 0.73 (1569.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:11.731648: step 69360, loss = 0.87 (1581.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:33:12.551253: step 69370, loss = 0.73 (1561.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:13.378637: step 69380, loss = 0.76 (1547.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:33:14.197135: step 69390, loss = 0.73 (1563.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:15.136735: step 69400, loss = 0.64 (1362.3 examples/sec; 0.094 sec/batch)
2017-05-06 22:33:15.840138: step 69410, loss = 0.68 (1819.7 examples/sec; 0.070 sec/batch)
2017-05-06 22:33:16.661182: step 69420, loss = 0.71 (1559.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:17.482590: step 69430, loss = 0.67 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:18.304174: step 69440, loss = 0.67 (1558.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:19.126296: step 69450, loss = 0.97 (1556.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:19.937190: step 69460, loss = 0.74 (1578.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:33:20.754733: step 69470, loss = 0.68 (1565.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:21.578959: step 69480, loss = 0.81 (1553.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:22.400897: step 69490, loss = 0.78 (1557.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:23.323088: step 69500, loss = 0.73 (1388.0 examples/sec; 0.092 sec/batch)
2017-05-06 22:33:24.041806: step 69510, loss = 0.65 (1780.9 examples/sec; 0.072 sec/batch)
2017-05-06 22:33:24.855103: step 69520, loss = 0.63 (1573.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:33:25.673901: step 69530, loss = 0.58 (1563.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:26.494972: step 69540, loss = 0.77 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:27.308228: step 69550, loss = 0.64 (1573.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:33:28.125226: step 69560, loss = 0.73 (1566.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:28.946037: step 69570, loss = 0.71 (1559.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:29.769073: step 69580, loss = 0.81 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:30.591777: step 69590, loss = 0.69 (1555.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:31.530409: step 69600, loss = 0.87 (1363.7 examples/sec; 0.094 sec/batch)
2017-05-06 22:33:32.251090: step 69610, loss = 0.72 (1776.1 examples/sec; 0.072 sec/batch)
2017-05-06 22:33:33.071738: step 69620, loss = 0.84 (1559.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:33.899642: step 69630, loss = 0.69 (1546.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:33:34.719206: step 69640, loss = 0.78 (1561.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:35.541448: step 69650, loss = 0.91 (1556.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:36.352522: step 69660, loss = 0.81 (1578.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:33:37.182462: step 69670, loss = 0.76 (1542.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:33:38.000790: step 69680, loss = 0.73 (1564.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:38.824963: step 69690, loss = 0.71 (1553.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:39.748263: step 69700, loss = 0.70 (1386.3 examples/sec; 0.092 sec/batch)
2017-05-06 22:33:40.467822: step 69710, loss = 0.79 (1778.9 examples/sec; 0.072 sec/batch)
2017-05-06 22:33:41.295428: step 69720, loss = 0.93 (1546.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:33:42.111242: step 69730, loss = 0.64 (1569.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:42.928372: step 69740, loss = 0.68 (1566.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:43.754350: step 69750, loss = 0.66 (1549.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:33:44.569522: step 69760, loss = 0.68 (1570.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:45.392203: step 69770, loss = 0.71 (1555.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:46.219275: step 69780, loss = 0.63 (1547.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:33:47.035564: step 69790, loss = 0.67 (1568.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:47.950611: step 69800, loss = 0.58 (1398.8 examples/sec; 0.092 sec/batch)
2017-05-06 22:33:48.672646: step 69810, loss = 0.77 (1772.8 examples/sec; 0.072 sec/batch)
2017-05-06 22:33:49.506239: step 69820, loss = 0.69 (1535.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:33:50.322700: step 69830, loss = 0.71 (1567.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:51.138947: step 69840, loss = 0.84 (1568.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:51.951869: step 69850, loss = 0.62 (1574.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:33:52.773603: step 69860, loss = 0.75 (1557.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:53.601633: step 69870, loss = 0.65 (1545.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:33:54.419214: step 69880, loss = 0.60 (1565.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:55.238880: step 69890, loss = 0.82 (1561.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:56.167074: step 69900, loss = 0.75 (1379.0 examples/sec; 0.093 sec/batch)
2017-05-06 22:33:56.872764: step 69910, loss = 0.73 (1813.8 examples/sec; 0.071 sec/batch)
2017-05-06 22:33:57.705603: step 69920, loss = 0.60 (1536.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:33:58.527000: step 69930, loss = 0.68 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:33:59.344138: step 69940, loss = 0.59 (1566.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:00.164271: step 69950, loss = 0.86 (1560.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:00.985009: step 69960, loss = 0.59 (1559.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:01.813128: step 69970, loss = 0.67 (1545.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:34:02.628815: step 69980, loss = 0.90 (1569.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:03.443736: step 69990, loss = 0.83 (1570.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:34:04.369319: step 70000, loss = 0.65 (1382.9 examples/sec; 0.093 sec/batch)
2017-05-06 22:34:05.090720: step 70010, loss = 0.69 (1774.3 examples/sec; 0.072 sec/batch)
2017-05-06 22:34:05.918641: step 70020, loss = 0.72 (1546.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:34:06.743541: step 70030, loss = 0.88 (1551.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:07.569449: step 70040, loss = 0.66 (1549.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:34:08.398444: step 70050, loss = 0.62 (1544.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:34:09.208569: step 70060, loss = 0.67 (1580.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:34:10.027381: step 70070, loss = 0.70 (1563.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:10.846567: step 70080, loss = 0.84 (1562.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:11.659667: step 70090, loss = 0.67 (1574.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:34:12.579470: step 70100, loss = 0.95 (1391.6 examples/sec; 0.092 sec/batch)
2017-05-06 22:34:13.303989: step 70110, loss = 0.74 (1766.7 examples/sec; 0.072 sec/batch)
2017-05-06 22:34:14.130699: step 70120, loss = 0.80 (1548.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:34:14.948516: step 70130, loss = 0.66 (1565.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:15.761899: step 70140, loss = 0.63 (1573.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:34:16.580342: step 70150, loss = 0.73 (1563.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:17.391576: step 70160, loss = 0.76 (1577.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:34:18.220811: step 70170, loss = 0.69 (1543.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:34:19.036352: step 70180, loss = 0.58 (1569.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:19.849921: step 70190, loss = 0.65 (1573.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:34:20.778880: step 70200, loss = 0.87 (1377.9 examples/sec; 0.093 sec/batch)
2017-05-06 22:34:21.501741: step 70210, loss = 0.67 (1770.7 examples/sec; 0.072 sec/batch)
2017-05-06 22:34:22.321495: step 70220, loss = 0.86 (1561.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:23.138500: step 70230, loss = 0.62 (1566.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:23.958260: step 70240, loss = 0.77 (1561.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:24.777721: step 70250, loss = 0.67 (1562.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:25.610910: step 70260, loss = 0.68 (1536.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:34:26.433145: step 70270, loss = 0.80 (1556.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:27.249024: step 70280, loss = 0.71 (1568.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:28.089125: step 70290, loss = 0.87 (1523.6 examples/sec; 0.084 sec/batch)
2017-05-06 22:34:28.990161: step 70300, loss = 0.66 (1420.6 examples/sec; 0.090 sec/batch)
2017-05-06 22:34:29.715856: step 70310, loss = 0.73 (1763.8 examples/sec; 0.073 sec/batch)
2017-05-06 22:34:30.537948: step 70320, loss = 0.74 (1557.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:31.364232: step 70330, loss = 0.75 (1549.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:34:32.181970: step 70340, loss = 0.82 (1565.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:33.000689: step 70350, loss = 0.85 (1563.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:33.822827: step 70360, loss = 0.78 (1556.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:34.642347: step 70370, loss = 0.81 (1561.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:35.465617: step 70380, loss = 0.71 (1554.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:36.294782: step 70390, loss = 0.71 (1543.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:34:37.213898: step 70400, loss = 0.77 (1392.6 examples/sec; 0.092 sec/batch)
2017-05-06 22:34:37.930126: step 70410, loss = 0.64 (1787.2 examples/sec; 0.072 sec/batch)
2017-05-06 22:34:38.752644: step 70420, loss = 0.68 (1556.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:39.564160: step 70430, loss = 0.83 (1577.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:34:40.385609: step 70440, loss = 0.67 (1558.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:41.209727: step 70450, loss = 0.77 (1553.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:42.036346: step 70460, loss = 0.72 (1548.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:34:42.859773: step 70470, loss = 0.73 (1554.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:43.675849: step 70480, loss = 0.76 (1568.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:44.500782: step 70490, loss = 0.52 (1551.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:45.415272: step 70500, loss = 0.78 (1399.7 examples/sec; 0.091 sec/batch)
2017-05-06 22:34:46.144327: step 70510, loss = 0.73 (1755.7 examples/sec; 0.073 sec/batch)
2017-05-06 22:34:46.970282: step 70520, loss = 0.79 (1549.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:34:47.779941: step 70530, loss = 0.97 (1580.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:34:48.607255: step 70540, loss = 0.66 (1547.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:34:49.426955: step 70550, loss = 0.73 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:50.250780: step 70560, loss = 0.68 (1553.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:51.063687: step 70570, loss = 0.79 (1574.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:34:51.884592: step 70580, loss = 0.89 (1559.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:52.704715: step 70590, loss = 0.63 (1560.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:53.614305: step 70600, loss = 0.76 (1407.2 examples/sec; 0.091 sec/batch)
2017-05-06 22:34:54.355219: step 70610, loss = 0.82 (1727.6 examples/sec; 0.074 sec/batch)
2017-05-06 22:34:55.179465: step 70620, loss = 0.83 (1552.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:55.997992: step 70630, loss = 0.71 (1563.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:56.821015: step 70640, loss = 0.92 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:57.642220: step 70650, loss = 0.62 (1558.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:58.462705: step 70660, loss = 0.71 (1560.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:34:59.285560: step 70670, loss = 0.62 (1555.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:00.099570: step 70680, loss = 0.71 (1572.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:35:00.920085: step 70690, loss = 0.65 (1560.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:01.848629: step 70700, loss = 0.80 (1378.5 examples/sec; 0.093 sec/batch)
2017-05-06 22:35:02.570909: step 70710, loss = 0.75 (1772.2 examples/sec; 0.072 sec/batch)
2017-05-06 22:35:03.399298: step 70720, loss = 0.59 (1545.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:35:04.226116: step 70730, loss = 0.70 (1548.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:35:05.041979: step 70740, loss = 0.73 (1568.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:05.857879: step 70750, loss = 0.66 (1568.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:06.682773: step 70760, loss = 0.74 (1551.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:07.499604: step 70770, loss = 0.86 (1567.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:08.324616: step 70780, loss = 0.73 (1551.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:35:09.142972: step 70790, loss = 0.73 (1564.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:10.065751: step 70800, loss = 0.76 (1387.1 examples/sec; 0.092 sec/batch)
2017-05-06 22:35:10.787692: step 70810, loss = 0.72 (1773.0 examples/sec; 0.072 sec/batch)
2017-05-06 22:35:11.599465: step 70820, loss = 0.75 (1576.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:35:12.418927: step 70830, loss = 0.65 (1562.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:13.236199: step 70840, loss = 0.80 (1566.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:14.061573: step 70850, loss = 0.63 (1550.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:35:14.883164: step 70860, loss = 0.76 (1557.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:15.699127: step 70870, loss = 0.76 (1568.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:16.516391: step 70880, loss = 0.66 (1566.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:17.343353: step 70890, loss = 0.76 (1547.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:35:18.261104: step 70900, loss = 0.77 (1394.7 examples/sec; 0.092 sec/batch)
2017-05-06 22:35:18.974870: step 70910, loss = 0.68 (1793.3 examples/sec; 0.071 sec/batch)
2017-05-06 22:35:19.788999: step 70920, loss = 0.76 (1572.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:35:20.604531: step 70930, loss = 0.82 (1569.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:21.433210: step 70940, loss = 0.72 (1544.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:35:22.251238: step 70950, loss = 0.88 (1564.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:23.074004: step 70960, loss = 0.69 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:23.889084: step 70970, loss = 0.72 (1570.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:24.715493: step 70980, loss = 0.75 (1548.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:35:25.536731: step 70990, loss = 0.80 (1558.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:26.468734: step 71000, loss = 0.62 (1373.4 examples/sec; 0.093 sec/batch)
2017-05-06 22:35:27.192566: step 71010, loss = 0.62 (1768.4 examples/sec; 0.072 sec/batch)
2017-05-06 22:35:27.999983: step 71020, loss = 0.76 (1585.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:35:28.821795: step 71030, loss = 0.85 (1557.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:29.641733: step 71040, loss = 0.72 (1561.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:30.471195: step 71050, loss = 0.81 (1543.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:35:31.298101: step 71060, loss = 0.71 (1547.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:35:32.115844: step 71070, loss = 0.77 (1565.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:32.932026: step 71080, loss = 0.80 (1568.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:33.769196: step 71090, loss = 0.84 (1529.0 examples/sec; 0.084 sec/batch)
2017-05-06 22:35:34.690404: step 71100, loss = 0.57 (1389.5 examples/sec; 0.092 sec/batch)
2017-05-06 22:35:35.408026: step 71110, loss = 0.76 (1783.7 examples/sec; 0.072 sec/batch)
2017-05-06 22:35:36.225756: step 71120, loss = 0.66 (1565.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:37.048030: step 71130, loss = 0.79 (1556.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:37.874229: step 71140, loss = 0.76 (1549.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:35:38.696331: step 71150, loss = 0.78 (1557.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:39.515063: step 71160, loss = 0.77 (1563.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:40.342309: step 71170, loss = 0.66 (1547.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:35:41.178479: step 71180, loss = 0.56 (1530.8 examples/sec; 0.084 sec/batch)
2017-05-06 22:35:42.007837: step 71190, loss = 0.75 (1543.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:35:42.929472: step 71200, loss = 0.92 (1388.8 examples/sec; 0.092 sec/batch)
2017-05-06 22:35:43.649250: step 71210, loss = 0.63 (1778.3 examples/sec; 0.072 sec/batch)
2017-05-06 22:35:44.470415: step 71220, loss = 0.70 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:45.291021: step 71230, loss = 0.78 (1559.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:46.102447: step 71240, loss = 0.79 (1577.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:35:46.924408: step 71250, loss = 0.68 (1557.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:47.739719: step 71260, loss = 0.78 (1569.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:48.553576: step 71270, loss = 0.69 (1572.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:35:49.397735: step 71280, loss = 0.63 (1516.3 examples/sec; 0.084 sec/batch)
2017-05-06 22:35:50.192191: step 71290, loss = 0.77 (1611.2 examples/sec; 0.079 sec/batch)
2017-05-06 22:35:51.103810: step 71300, loss = 0.75 (1404.1 examples/sec; 0.091 sec/batch)
2017-05-06 22:35:51.826408: step 71310, loss = 0.87 (1771.4 examples/sec; 0.072 sec/batch)
2017-05-06 22:35:52.646222: step 71320, loss = 0.69 (1561.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:53.463298: step 71330, loss = 0.73 (1566.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:54.289752: step 71340, loss = 0.77 (1548.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:35:55.114581: step 71350, loss = 0.63 (1551.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:55.932067: step 71360, loss = 0.81 (1565.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:56.753621: step 71370, loss = 0.66 (1558.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:57.582703: step 71380, loss = 0.93 (1543.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:35:58.403566: step 71390, loss = 0.86 (1559.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:35:59.328199: step 71400, loss = 0.68 (1384.3 examples/sec; 0.092 sec/batch)
2017-05-06 22:36:00.039572: step 71410, loss = 0.64 (1799.3 examples/sec; 0.071 sec/batch)
2017-05-06 22:36:00.863172: step 71420, loss = 0.74 (1554.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:36:01.677445: step 71430, loss = 0.61 (1572.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:36:02.503193: step 71440, loss = 0.68 (1550.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:36:03.324294: step 71450, loss = 0.67 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:36:04.136761: step 71460, loss = 0.87 (1575.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:36:04.966417: step 71470, loss = 0.68 (1542.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:36:05.788306: step 71480, loss = 0.79 (1557.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:36:06.617443: step 71490, loss = 0.73 (1543.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:36:07.547755: step 71500, loss = 0.59 (1375.9 examples/sec; 0.093 sec/batch)
2017-05-06 22:36:08.262144: step 71510, loss = 0.69 (1791.7 examples/sec; 0.071 sec/batch)
2017-05-06 22:36:09.079643: step 71520, loss = 0.74 (1565.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:36:09.900769: step 71530, loss = 0.64 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:36:10.719449: step 71540, loss = 0.88 (1563.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:36:11.543165: step 71550, loss = 0.88 (1553.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:36:12.369939: step 71560, loss = 0.72 (1548.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:36:13.186313: step 71570, loss = 0.66 (1567.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:36:14.016321: step 71580, loss = 0.73 (1542.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:36:14.834870: step 71590, loss = 0.70 (1563.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:36:15.736331: step 71600, loss = 0.76 (1419.9 examples/sec; 0.090 sec/batch)
2017-05-06 22:36:16.467796: step 71610, loss = 0.67 (1749.9 examples/sec; 0.073 sec/batch)
2017-05-06 22:36:17.291509: step 71620, loss = 0.86 (1553.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:36:18.117274: step 71630, loss = 0.79 (1550.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:36:18.935917: step 71640, loss = 0.63 (1563.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:36:19.758747: step 71650, loss = 0.76 (1555.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:36:20.575232: step 71660, loss = 0.71 (1567.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:36:21.398641: step 71670, loss = 0.74 (1554.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:36:22.235761: step 71680, loss = 0.69 (1529.1 examples/sec; 0.084 sec/batch)
2017-05-06 22:36:23.058199: step 71690, loss = 0.71 (1556.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:36:23.968249: step 71700, loss = 0.75 (1406.5 examples/sec; 0.091 sec/batch)
2017-05-06 22:36:24.688112: step 71710, loss = 0.63 (1778.1 examples/sec; 0.072 sec/batch)
2017-05-06 22:36:25.513564: step 71720, loss = 0.76 (1550.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:36:26.339464: step 71730, loss = 0.64 (1549.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:36:27.168624: step 71740, loss = 0.78 (1543.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:36:27.981899: step 71750, loss = 0.71 (1573.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:36:28.804447: step 71760, loss = 0.72 (1556.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:36:29.634186: step 71770, loss = 0.55 (1542.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:36:30.458002: step 71780, loss = 0.71 (1553.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:36:31.282608: step 71790, loss = 0.67 (1552.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:36:32.196859: step 71800, loss = 0.52 (1400.1 examples/sec; 0.091 sec/batch)
2017-05-06 22:36:32.923205: step 71810, loss = 0.76 (1762.2 examples/sec; 0.073 sec/batch)
2017-05-06 22:36:33.746705: step 71820, loss = 0.70 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:36:34.571902: step 71830, loss = 0.63 (1551.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:36:35.394853: step 71840, loss = 0.68 (1555.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:36:36.211939: step 71850, loss = 0.68 (1566.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:36:37.032722: step 71860, loss = 0.76 (1559.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:36:37.859356: step 71870, loss = 0.63 (1548.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:36:38.684820: step 71880, loss = 0.73 (1550.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:36:39.494599: step 71890, loss = 0.66 (1580.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:36:40.413950: step 71900, loss = 0.65 (1392.3 examples/sec; 0.092 sec/batch)
2017-05-06 22:36:41.130872: step 71910, loss = 0.67 (1785.4 examples/sec; 0.072 sec/batch)
2017-05-06 22:36:41.955404: step 71920, loss = 0.73 (1552.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:36:42.770421: step 71930, loss = 0.84 (1570.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:36:43.582653: step 71940, loss = 0.63 (1575.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:36:44.403440: step 71950, loss = 0.73 (1559.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:36:45.217927: step 71960, loss = 0.67 (1571.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:36:46.047885: step 71970, loss = 0.56 (1542.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:36:46.879533: step 71980, loss = 0.76 (1539.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:36:47.700685: step 71990, loss = 0.78 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:36:48.616155: step 72000, loss = 0.75 (1398.2 examples/sec; 0.092 sec/batch)
2017-05-06 22:36:49.347839: step 72010, loss = 0.65 (1749.4 examples/sec; 0.073 sec/batch)
2017-05-06 22:36:50.176481: step 72020, loss = 0.74 (1544.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:36:50.997457: step 72030, loss = 0.69 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:36:51.812216: step 72040, loss = 0.65 (1571.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:36:52.635675: step 72050, loss = 0.69 (1554.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:36:53.451851: step 72060, loss = 0.57 (1568.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:36:54.263901: step 72070, loss = 0.74 (1576.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:36:55.076764: step 72080, loss = 0.78 (1574.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:36:55.891109: step 72090, loss = 0.79 (1571.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:36:56.826180: step 72100, loss = 0.84 (1368.9 examples/sec; 0.094 sec/batch)
2017-05-06 22:36:57.533681: step 72110, loss = 0.80 (1809.2 examples/sec; 0.071 sec/batch)
2017-05-06 22:36:58.356748: step 72120, loss = 0.75 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:36:59.186948: step 72130, loss = 0.75 (1541.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:36:59.997492: step 72140, loss = 0.82 (1579.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:37:00.816689: step 72150, loss = 0.74 (1562.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:01.632836: step 72160, loss = 0.75 (1568.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:02.455884: step 72170, loss = 0.76 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:03.277452: step 72180, loss = 0.81 (1558.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:04.088090: step 72190, loss = 0.65 (1579.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:37:05.012786: step 72200, loss = 0.73 (1384.2 examples/sec; 0.092 sec/batch)
2017-05-06 22:37:05.737182: step 72210, loss = 0.75 (1767.0 examples/sec; 0.072 sec/batch)
2017-05-06 22:37:06.563610: step 72220, loss = 0.74 (1548.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:37:07.393061: step 72230, loss = 0.53 (1543.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:37:08.211172: step 72240, loss = 0.72 (1564.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:09.030308: step 72250, loss = 0.77 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:09.847800: step 72260, loss = 0.69 (1565.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:10.689023: step 72270, loss = 0.75 (1521.6 examples/sec; 0.084 sec/batch)
2017-05-06 22:37:11.487376: step 72280, loss = 0.67 (1603.3 examples/sec; 0.080 sec/batch)
2017-05-06 22:37:12.301843: step 72290, loss = 0.76 (1571.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:37:13.211515: step 72300, loss = 0.85 (1407.1 examples/sec; 0.091 sec/batch)
2017-05-06 22:37:13.942312: step 72310, loss = 0.63 (1751.5 examples/sec; 0.073 sec/batch)
2017-05-06 22:37:14.763884: step 72320, loss = 0.71 (1558.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:15.575210: step 72330, loss = 0.75 (1577.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:37:16.391621: step 72340, loss = 0.65 (1567.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:17.207284: step 72350, loss = 0.59 (1569.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:18.029432: step 72360, loss = 0.75 (1556.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:18.865395: step 72370, loss = 0.74 (1531.2 examples/sec; 0.084 sec/batch)
2017-05-06 22:37:19.664211: step 72380, loss = 0.75 (1602.4 examples/sec; 0.080 sec/batch)
2017-05-06 22:37:20.488571: step 72390, loss = 0.56 (1552.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:21.404657: step 72400, loss = 0.89 (1397.2 examples/sec; 0.092 sec/batch)
2017-05-06 22:37:22.132764: step 72410, loss = 0.76 (1758.0 examples/sec; 0.073 sec/batch)
2017-05-06 22:37:22.949918: step 72420, loss = 0.82 (1566.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:23.761252: step 72430, loss = 0.77 (1577.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:37:24.586056: step 72440, loss = 0.71 (1551.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:25.407210: step 72450, loss = 0.79 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:26.227063: step 72460, loss = 0.85 (1561.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:27.043885: step 72470, loss = 0.65 (1567.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:27.859881: step 72480, loss = 0.74 (1568.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:28.687636: step 72490, loss = 0.76 (1546.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:37:29.603781: step 72500, loss = 0.80 (1397.2 examples/sec; 0.092 sec/batch)
2017-05-06 22:37:30.322750: step 72510, loss = 0.69 (1780.3 examples/sec; 0.072 sec/batch)
2017-05-06 22:37:31.144748: step 72520, loss = 0.78 (1557.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:31.951755: step 72530, loss = 0.68 (1586.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:37:32.778391: step 72540, loss = 0.71 (1548.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:37:33.602056: step 72550, loss = 0.80 (1554.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:34.432490: step 72560, loss = 0.83 (1541.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:37:35.240427: step 72570, loss = 0.92 (1584.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:37:36.060000: step 72580, loss = 0.64 (1561.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:36.886958: step 72590, loss = 0.69 (1547.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:37:37.812264: step 72600, loss = 0.73 (1383.3 examples/sec; 0.093 sec/batch)
2017-05-06 22:37:38.543520: step 72610, loss = 0.66 (1750.4 examples/sec; 0.073 sec/batch)
2017-05-06 22:37:39.350189: step 72620, loss = 0.77 (1586.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:37:40.168596: step 72630, loss = 0.72 (1564.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:40.994951: step 72640, loss = 0.65 (1549.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:37:41.813765: step 72650, loss = 0.71 (1563.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:42.633762: step 72660, loss = 0.62 (1561.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:43.449921: step 72670, loss = 0.59 (1568.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:44.264565: step 72680, loss = 0.69 (1571.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:37:45.087449: step 72690, loss = 0.77 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:45.999929: step 72700, loss = 0.56 (1402.8 examples/sec; 0.091 sec/batch)
2017-05-06 22:37:46.721091: step 72710, loss = 0.75 (1774.9 examples/sec; 0.072 sec/batch)
2017-05-06 22:37:47.533970: step 72720, loss = 0.75 (1574.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:37:48.356642: step 72730, loss = 0.91 (1555.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:49.182132: step 72740, loss = 0.95 (1550.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:37:50.005337: step 72750, loss = 0.73 (1554.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:50.826948: step 72760, loss = 0.71 (1557.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:51.654259: step 72770, loss = 0.79 (1547.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:37:52.469847: step 72780, loss = 0.70 (1569.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:53.286109: step 72790, loss = 0.77 (1568.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:54.194936: step 72800, loss = 0.72 (1408.4 examples/sec; 0.091 sec/batch)
2017-05-06 22:37:54.911123: step 72810, loss = 0.77 (1787.2 examples/sec; 0.072 sec/batch)
2017-05-06 22:37:55.729312: step 72820, loss = 0.71 (1564.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:56.544646: step 72830, loss = 0.86 (1569.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:57.367446: step 72840, loss = 0.79 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:58.186428: step 72850, loss = 0.70 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:37:59.014139: step 72860, loss = 0.67 (1546.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:37:59.836963: step 72870, loss = 0.61 (1555.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:00.661195: step 72880, loss = 0.72 (1553.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:01.487895: step 72890, loss = 0.75 (1548.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:38:02.402424: step 72900, loss = 0.85 (1399.6 examples/sec; 0.091 sec/batch)
2017-05-06 22:38:03.127176: step 72910, loss = 0.75 (1766.1 examples/sec; 0.072 sec/batch)
2017-05-06 22:38:03.942085: step 72920, loss = 0.71 (1570.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:38:04.760506: step 72930, loss = 0.72 (1564.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:05.580550: step 72940, loss = 0.83 (1560.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:06.400320: step 72950, loss = 0.81 (1561.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:07.218481: step 72960, loss = 0.69 (1564.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:08.031941: step 72970, loss = 0.67 (1573.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:38:08.851496: step 72980, loss = 0.78 (1561.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:09.665384: step 72990, loss = 0.76 (1572.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:38:10.592707: step 73000, loss = 0.76 (1380.3 examples/sec; 0.093 sec/batch)
2017-05-06 22:38:11.308721: step 73010, loss = 0.70 (1787.7 examples/sec; 0.072 sec/batch)
2017-05-06 22:38:12.120716: step 73020, loss = 0.70 (1576.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:38:12.948203: step 73030, loss = 0.66 (1546.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:38:13.797634: step 73040, loss = 0.68 (1506.9 examples/sec; 0.085 sec/batch)
2017-05-06 22:38:14.618576: step 73050, loss = 0.72 (1559.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:15.432800: step 73060, loss = 0.84 (1572.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:38:16.252374: step 73070, loss = 0.65 (1561.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:17.072344: step 73080, loss = 0.70 (1561.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:17.895725: step 73090, loss = 0.64 (1554.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:18.818446: step 73100, loss = 0.74 (1387.2 examples/sec; 0.092 sec/batch)
2017-05-06 22:38:19.531035: step 73110, loss = 0.69 (1796.3 examples/sec; 0.071 sec/batch)
2017-05-06 22:38:20.349004: step 73120, loss = 0.82 (1564.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:21.168810: step 73130, loss = 0.57 (1561.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:21.990522: step 73140, loss = 0.74 (1557.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:22.807378: step 73150, loss = 0.67 (1567.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:23.624473: step 73160, loss = 0.75 (1566.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:24.451641: step 73170, loss = 0.74 (1547.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:38:25.280432: step 73180, loss = 0.83 (1544.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:38:26.098436: step 73190, loss = 0.83 (1564.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:27.014238: step 73200, loss = 0.82 (1397.7 examples/sec; 0.092 sec/batch)
2017-05-06 22:38:27.731330: step 73210, loss = 0.66 (1785.0 examples/sec; 0.072 sec/batch)
2017-05-06 22:38:28.557101: step 73220, loss = 0.65 (1550.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:38:29.380423: step 73230, loss = 0.62 (1554.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:30.207800: step 73240, loss = 0.67 (1547.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:38:31.026024: step 73250, loss = 0.66 (1564.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:31.861979: step 73260, loss = 0.86 (1531.2 examples/sec; 0.084 sec/batch)
2017-05-06 22:38:32.672250: step 73270, loss = 0.69 (1579.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:38:33.496644: step 73280, loss = 0.77 (1552.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:34.327372: step 73290, loss = 0.83 (1540.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:38:35.249509: step 73300, loss = 0.85 (1388.1 examples/sec; 0.092 sec/batch)
2017-05-06 22:38:35.960532: step 73310, loss = 0.75 (1800.2 examples/sec; 0.071 sec/batch)
2017-05-06 22:38:36.790301: step 73320, loss = 0.67 (1542.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:38:37.608153: step 73330, loss = 0.51 (1565.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:38.433881: step 73340, loss = 0.56 (1550.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:38:39.246630: step 73350, loss = 0.67 (1574.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:38:40.074376: step 73360, loss = 0.63 (1546.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:38:40.889529: step 73370, loss = 0.69 (1569.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:41.717969: step 73380, loss = 0.64 (1545.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:38:42.538967: step 73390, loss = 0.78 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:43.457602: step 73400, loss = 0.54 (1393.4 examples/sec; 0.092 sec/batch)
2017-05-06 22:38:44.175066: step 73410, loss = 0.81 (1784.0 examples/sec; 0.072 sec/batch)
2017-05-06 22:38:44.994780: step 73420, loss = 0.66 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:45.815633: step 73430, loss = 0.72 (1559.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:46.634713: step 73440, loss = 0.76 (1562.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:47.446092: step 73450, loss = 0.75 (1577.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:38:48.270356: step 73460, loss = 0.76 (1552.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:49.090172: step 73470, loss = 0.65 (1561.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:49.919276: step 73480, loss = 0.70 (1543.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:38:50.739722: step 73490, loss = 0.77 (1560.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:51.644958: step 73500, loss = 0.68 (1414.0 examples/sec; 0.091 sec/batch)
2017-05-06 22:38:52.368544: step 73510, loss = 0.69 (1769.0 examples/sec; 0.072 sec/batch)
2017-05-06 22:38:53.192903: step 73520, loss = 0.68 (1552.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:54.007428: step 73530, loss = 0.78 (1571.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:38:54.820111: step 73540, loss = 0.82 (1575.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:38:55.636393: step 73550, loss = 0.76 (1568.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:56.460611: step 73560, loss = 0.75 (1553.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:57.284821: step 73570, loss = 0.87 (1553.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:58.100864: step 73580, loss = 1.16 (1568.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:58.924097: step 73590, loss = 0.64 (1554.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:38:59.827883: step 73600, loss = 0.63 (1416.3 examples/sec; 0.090 sec/batch)
2017-05-06 22:39:00.552274: step 73610, loss = 0.59 (1767.0 examples/sec; 0.072 sec/batch)
2017-05-06 22:39:01.365539: step 73620, loss = 0.75 (1573.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:39:02.182358: step 73630, loss = 0.93 (1567.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:39:03.002195: step 73640, loss = 0.75 (1561.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:39:03.814952: step 73650, loss = 0.68 (1574.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:39:04.636652: step 73660, loss = 0.72 (1557.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:39:05.462244: step 73670, loss = 0.85 (1550.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:39:06.279113: step 73680, loss = 0.67 (1567.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:39:07.104222: step 73690, loss = 0.71 (1551.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:39:08.012236: step 73700, loss = 0.67 (1409.7 examples/sec; 0.091 sec/batch)
2017-05-06 22:39:08.738595: step 73710, loss = 0.77 (1762.2 examples/sec; 0.073 sec/batch)
2017-05-06 22:39:09.567398: step 73720, loss = 0.59 (1544.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:39:10.381036: step 73730, loss = 0.62 (1573.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:39:11.202152: step 73740, loss = 0.65 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:39:12.012623: step 73750, loss = 0.62 (1579.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:39:12.824758: step 73760, loss = 0.61 (1576.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:39:13.652315: step 73770, loss = 0.66 (1546.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:39:14.471595: step 73780, loss = 0.75 (1562.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:39:15.294537: step 73790, loss = 0.74 (1555.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:39:16.209571: step 73800, loss = 0.72 (1398.9 examples/sec; 0.092 sec/batch)
2017-05-06 22:39:16.932686: step 73810, loss = 0.58 (1770.1 examples/sec; 0.072 sec/batch)
2017-05-06 22:39:17.759108: step 73820, loss = 0.67 (1548.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:39:18.580086: step 73830, loss = 0.78 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:39:19.391431: step 73840, loss = 0.54 (1577.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:39:20.197999: step 73850, loss = 0.57 (1587.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:39:21.024385: step 73860, loss = 0.62 (1548.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:39:21.851447: step 73870, loss = 0.64 (1547.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:39:22.674103: step 73880, loss = 0.89 (1555.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:39:23.490666: step 73890, loss = 0.79 (1567.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:39:24.411770: step 73900, loss = 0.77 (1389.6 examples/sec; 0.092 sec/batch)
2017-05-06 22:39:25.120689: step 73910, loss = 0.86 (1805.6 examples/sec; 0.071 sec/batch)
2017-05-06 22:39:25.943548: step 73920, loss = 0.88 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:39:26.762203: step 73930, loss = 0.68 (1563.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:39:27.575201: step 73940, loss = 0.61 (1574.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:39:28.394134: step 73950, loss = 0.75 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:39:29.219140: step 73960, loss = 0.67 (1551.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:39:30.037264: step 73970, loss = 0.74 (1564.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:39:30.858792: step 73980, loss = 0.86 (1558.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:39:31.669353: step 73990, loss = 0.67 (1579.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:39:32.596014: step 74000, loss = 0.70 (1381.3 examples/sec; 0.093 sec/batch)
2017-05-06 22:39:33.313972: step 74010, loss = 0.85 (1782.8 examples/sec; 0.072 sec/batch)
2017-05-06 22:39:34.136972: step 74020, loss = 0.77 (1555.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:39:34.954770: step 74030, loss = 0.73 (1565.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:39:35.770953: step 74040, loss = 0.66 (1568.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:39:36.588678: step 74050, loss = 0.76 (1565.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:39:37.415751: step 74060, loss = 0.71 (1547.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:39:38.235775: step 74070, loss = 0.75 (1560.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:39:39.057620: step 74080, loss = 0.62 (1557.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:39:39.872939: step 74090, loss = 0.78 (1569.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:39:40.785857: step 74100, loss = 0.65 (1402.1 examples/sec; 0.091 sec/batch)
2017-05-06 22:39:41.506784: step 74110, loss = 0.75 (1775.5 examples/sec; 0.072 sec/batch)
2017-05-06 22:39:42.339101: step 74120, loss = 0.69 (1537.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:39:43.167580: step 74130, loss = 0.65 (1545.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:39:43.980540: step 74140, loss = 0.80 (1574.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:39:44.796984: step 74150, loss = 0.71 (1567.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:39:45.627933: step 74160, loss = 0.72 (1540.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:39:46.454687: step 74170, loss = 0.80 (1548.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:39:47.271766: step 74180, loss = 0.62 (1566.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:39:48.084033: step 74190, loss = 0.78 (1575.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:39:48.997889: step 74200, loss = 0.88 (1400.7 examples/sec; 0.091 sec/batch)
2017-05-06 22:39:49.724278: step 74210, loss = 0.68 (1762.2 examples/sec; 0.073 sec/batch)
2017-05-06 22:39:50.544344: step 74220, loss = 0.85 (1560.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:39:51.362398: step 74230, loss = 0.87 (1564.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:39:52.178656: step 74240, loss = 0.65 (1568.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:39:53.023551: step 74250, loss = 0.80 (1515.0 examples/sec; 0.084 sec/batch)
2017-05-06 22:39:53.817718: step 74260, loss = 0.61 (1611.7 examples/sec; 0.079 sec/batch)
2017-05-06 22:39:54.635542: step 74270, loss = 0.64 (1565.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:39:55.451267: step 74280, loss = 0.69 (1569.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:39:56.273204: step 74290, loss = 0.64 (1557.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:39:57.186819: step 74300, loss = 0.73 (1401.0 examples/sec; 0.091 sec/batch)
2017-05-06 22:39:57.910858: step 74310, loss = 0.82 (1767.9 examples/sec; 0.072 sec/batch)
2017-05-06 22:39:58.729790: step 74320, loss = 0.65 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:39:59.546621: step 74330, loss = 0.70 (1567.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:40:00.371858: step 74340, loss = 0.72 (1551.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:40:01.204306: step 74350, loss = 0.75 (1537.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:40:02.031228: step 74360, loss = 0.96 (1547.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:40:02.858626: step 74370, loss = 0.58 (1547.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:40:03.681587: step 74380, loss = 0.66 (1555.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:40:04.511853: step 74390, loss = 0.78 (1541.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:40:05.429669: step 74400, loss = 0.78 (1394.6 examples/sec; 0.092 sec/batch)
2017-05-06 22:40:06.159504: step 74410, loss = 0.75 (1753.8 examples/sec; 0.073 sec/batch)
2017-05-06 22:40:06.988369: step 74420, loss = 0.61 (1544.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:40:07.801827: step 74430, loss = 0.78 (1573.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:40:08.626418: step 74440, loss = 0.76 (1552.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:40:09.456972: step 74450, loss = 0.80 (1541.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:40:10.279352: step 74460, loss = 0.64 (1556.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:40:11.100137: step 74470, loss = 0.81 (1559.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:40:11.912418: step 74480, loss = 0.74 (1575.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:40:12.725927: step 74490, loss = 0.64 (1573.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:40:13.652182: step 74500, loss = 0.88 (1381.9 examples/sec; 0.093 sec/batch)
2017-05-06 22:40:14.369478: step 74510, loss = 0.82 (1784.5 examples/sec; 0.072 sec/batch)
2017-05-06 22:40:15.194423: step 74520, loss = 0.79 (1551.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:40:16.000275: step 74530, loss = 0.73 (1588.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:40:16.826941: step 74540, loss = 0.72 (1548.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:40:17.648012: step 74550, loss = 0.64 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:40:18.469127: step 74560, loss = 0.86 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:40:19.281990: step 74570, loss = 0.70 (1574.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:40:20.103835: step 74580, loss = 0.67 (1557.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:40:20.922840: step 74590, loss = 0.68 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:40:21.841592: step 74600, loss = 0.63 (1393.2 examples/sec; 0.092 sec/batch)
2017-05-06 22:40:22.555568: step 74610, loss = 0.68 (1792.8 examples/sec; 0.071 sec/batch)
2017-05-06 22:40:23.365748: step 74620, loss = 0.69 (1579.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:40:24.181007: step 74630, loss = 0.71 (1570.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:40:25.009426: step 74640, loss = 0.77 (1545.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:40:25.830475: step 74650, loss = 0.67 (1559.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:40:26.661391: step 74660, loss = 0.77 (1540.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:40:27.479184: step 74670, loss = 0.78 (1565.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:40:28.298278: step 74680, loss = 0.78 (1562.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:40:29.120628: step 74690, loss = 0.65 (1556.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:40:30.034994: step 74700, loss = 0.78 (1399.9 examples/sec; 0.091 sec/batch)
2017-05-06 22:40:30.762479: step 74710, loss = 0.81 (1759.5 examples/sec; 0.073 sec/batch)
2017-05-06 22:40:31.575530: step 74720, loss = 0.61 (1574.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:40:32.396749: step 74730, loss = 0.72 (1558.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:40:33.224354: step 74740, loss = 0.81 (1546.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:40:34.039612: step 74750, loss = 0.66 (1570.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:40:34.866116: step 74760, loss = 0.71 (1548.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:40:35.686836: step 74770, loss = 0.67 (1559.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:40:36.515328: step 74780, loss = 0.67 (1545.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:40:37.338568: step 74790, loss = 0.73 (1554.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:40:38.254168: step 74800, loss = 0.63 (1398.0 examples/sec; 0.092 sec/batch)
2017-05-06 22:40:38.982212: step 74810, loss = 0.75 (1758.1 examples/sec; 0.073 sec/batch)
2017-05-06 22:40:39.797174: step 74820, loss = 0.72 (1570.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:40:40.620070: step 74830, loss = 0.77 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:40:41.448376: step 74840, loss = 0.74 (1545.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:40:42.263761: step 74850, loss = 0.71 (1569.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:40:43.085908: step 74860, loss = 0.73 (1556.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:40:43.900748: step 74870, loss = 0.73 (1570.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:40:44.719814: step 74880, loss = 0.69 (1562.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:40:45.537028: step 74890, loss = 0.70 (1566.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:40:46.454417: step 74900, loss = 0.67 (1395.3 examples/sec; 0.092 sec/batch)
2017-05-06 22:40:47.175568: step 74910, loss = 0.62 (1774.9 examples/sec; 0.072 sec/batch)
2017-05-06 22:40:47.992079: step 74920, loss = 0.85 (1567.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:40:48.814916: step 74930, loss = 0.64 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:40:49.630494: step 74940, loss = 0.80 (1569.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:40:50.451787: step 74950, loss = 0.78 (1558.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:40:51.263710: step 74960, loss = 0.56 (1576.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:40:52.076022: step 74970, loss = 0.64 (1575.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:40:52.900190: step 74980, loss = 0.71 (1553.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:40:53.720425: step 74990, loss = 0.66 (1560.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:40:54.640934: step 75000, loss = 0.65 (1390.5 examples/sec; 0.092 sec/batch)
2017-05-06 22:40:55.360414: step 75010, loss = 0.75 (1779.1 examples/sec; 0.072 sec/batch)
2017-05-06 22:40:56.165407: step 75020, loss = 0.74 (1590.1 examples/sec; 0.080 sec/batch)
2017-05-06 22:40:56.969560: step 75030, loss = 0.85 (1591.7 examples/sec; 0.080 sec/batch)
2017-05-06 22:40:57.794355: step 75040, loss = 0.78 (1551.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:40:58.617078: step 75050, loss = 0.76 (1555.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:40:59.429091: step 75060, loss = 0.64 (1576.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:41:00.248792: step 75070, loss = 0.69 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:41:01.067067: step 75080, loss = 0.75 (1564.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:41:01.888092: step 75090, loss = 0.73 (1559.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:41:02.809760: step 75100, loss = 0.61 (1388.8 examples/sec; 0.092 sec/batch)
2017-05-06 22:41:03.531512: step 75110, loss = 0.67 (1773.5 examples/sec; 0.072 sec/batch)
2017-05-06 22:41:04.351121: step 75120, loss = 0.71 (1561.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:41:05.183301: step 75130, loss = 0.71 (1538.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:41:06.005121: step 75140, loss = 0.84 (1557.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:41:06.817405: step 75150, loss = 0.74 (1575.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:41:07.626317: step 75160, loss = 0.66 (1582.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:41:08.455392: step 75170, loss = 0.79 (1543.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:41:09.270898: step 75180, loss = 0.67 (1569.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:41:10.099002: step 75190, loss = 0.69 (1545.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:41:11.022175: step 75200, loss = 0.80 (1386.5 examples/sec; 0.092 sec/batch)
2017-05-06 22:41:11.739407: step 75210, loss = 0.67 (1784.7 examples/sec; 0.072 sec/batch)
2017-05-06 22:41:12.555777: step 75220, loss = 0.77 (1567.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:41:13.379052: step 75230, loss = 0.80 (1554.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:41:14.233005: step 75240, loss = 0.63 (1498.9 examples/sec; 0.085 sec/batch)
2017-05-06 22:41:15.037745: step 75250, loss = 0.59 (1590.6 examples/sec; 0.080 sec/batch)
2017-05-06 22:41:15.854150: step 75260, loss = 0.73 (1567.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:41:16.682478: step 75270, loss = 0.70 (1545.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:41:17.501187: step 75280, loss = 0.77 (1563.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:41:18.315021: step 75290, loss = 0.71 (1572.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:41:19.236934: step 75300, loss = 0.74 (1388.4 examples/sec; 0.092 sec/batch)
2017-05-06 22:41:19.953911: step 75310, loss = 0.81 (1785.3 examples/sec; 0.072 sec/batch)
2017-05-06 22:41:20.774006: step 75320, loss = 0.66 (1560.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:41:21.594020: step 75330, loss = 0.67 (1560.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:41:22.416764: step 75340, loss = 0.72 (1555.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:41:23.239651: step 75350, loss = 0.69 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:41:24.051481: step 75360, loss = 0.75 (1576.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:41:24.875078: step 75370, loss = 0.70 (1554.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:41:25.700176: step 75380, loss = 0.63 (1551.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:41:26.522718: step 75390, loss = 0.78 (1556.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:41:27.440123: step 75400, loss = 0.74 (1395.2 examples/sec; 0.092 sec/batch)
2017-05-06 22:41:28.158718: step 75410, loss = 0.71 (1781.3 examples/sec; 0.072 sec/batch)
2017-05-06 22:41:28.989705: step 75420, loss = 0.71 (1540.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:41:29.807812: step 75430, loss = 0.81 (1564.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:41:30.625574: step 75440, loss = 0.69 (1565.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:41:31.443574: step 75450, loss = 0.86 (1564.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:41:32.272777: step 75460, loss = 0.65 (1543.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:41:33.094531: step 75470, loss = 0.64 (1557.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:41:33.914827: step 75480, loss = 0.73 (1560.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:41:34.741771: step 75490, loss = 0.78 (1547.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:41:35.654088: step 75500, loss = 0.71 (1403.0 examples/sec; 0.091 sec/batch)
2017-05-06 22:41:36.378932: step 75510, loss = 0.72 (1765.9 examples/sec; 0.072 sec/batch)
2017-05-06 22:41:37.208486: step 75520, loss = 0.80 (1543.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:41:38.023016: step 75530, loss = 0.62 (1571.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:41:38.850350: step 75540, loss = 0.67 (1547.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:41:39.661894: step 75550, loss = 0.80 (1577.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:41:40.479123: step 75560, loss = 0.81 (1566.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:41:41.289492: step 75570, loss = 0.64 (1579.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:41:42.112563: step 75580, loss = 0.69 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:41:42.939647: step 75590, loss = 0.69 (1547.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:41:43.854992: step 75600, loss = 0.72 (1398.4 examples/sec; 0.092 sec/batch)
2017-05-06 22:41:44.573434: step 75610, loss = 0.59 (1781.6 examples/sec; 0.072 sec/batch)
2017-05-06 22:41:45.396626: step 75620, loss = 0.78 (1554.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:41:46.209556: step 75630, loss = 0.97 (1574.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:41:47.044645: step 75640, loss = 0.63 (1532.8 examples/sec; 0.084 sec/batch)
2017-05-06 22:41:47.858599: step 75650, loss = 0.77 (1572.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:41:48.673298: step 75660, loss = 0.67 (1571.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:41:49.499573: step 75670, loss = 0.84 (1549.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:41:50.330374: step 75680, loss = 0.76 (1540.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:41:51.157303: step 75690, loss = 0.67 (1547.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:41:52.060704: step 75700, loss = 0.68 (1416.9 examples/sec; 0.090 sec/batch)
2017-05-06 22:41:52.782403: step 75710, loss = 0.89 (1773.6 examples/sec; 0.072 sec/batch)
2017-05-06 22:41:53.605514: step 75720, loss = 0.73 (1555.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:41:54.430095: step 75730, loss = 0.69 (1552.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:41:55.254330: step 75740, loss = 0.60 (1553.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:41:56.074069: step 75750, loss = 0.65 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:41:56.901151: step 75760, loss = 0.69 (1547.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:41:57.728239: step 75770, loss = 0.55 (1547.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:41:58.562061: step 75780, loss = 0.62 (1535.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:41:59.382464: step 75790, loss = 0.61 (1560.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:42:00.307033: step 75800, loss = 0.79 (1384.4 examples/sec; 0.092 sec/batch)
2017-05-06 22:42:01.023363: step 75810, loss = 0.74 (1786.9 examples/sec; 0.072 sec/batch)
2017-05-06 22:42:01.850411: step 75820, loss = 0.74 (1547.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:42:02.678572: step 75830, loss = 0.71 (1545.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:42:03.493581: step 75840, loss = 0.74 (1570.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:42:04.310204: step 75850, loss = 0.82 (1567.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:42:05.126577: step 75860, loss = 0.77 (1567.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:42:05.947984: step 75870, loss = 0.70 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:42:06.766897: step 75880, loss = 0.72 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:42:07.591164: step 75890, loss = 0.79 (1552.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:42:08.516891: step 75900, loss = 0.71 (1382.7 examples/sec; 0.093 sec/batch)
2017-05-06 22:42:09.247864: step 75910, loss = 0.80 (1751.1 examples/sec; 0.073 sec/batch)
2017-05-06 22:42:10.070696: step 75920, loss = 0.73 (1555.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:42:10.897581: step 75930, loss = 0.77 (1548.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:42:11.714264: step 75940, loss = 0.73 (1567.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:42:12.541362: step 75950, loss = 0.82 (1547.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:42:13.360470: step 75960, loss = 0.77 (1562.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:42:14.187359: step 75970, loss = 0.70 (1548.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:42:15.008345: step 75980, loss = 0.78 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:42:15.842710: step 75990, loss = 0.76 (1534.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:42:16.758460: step 76000, loss = 0.78 (1397.8 examples/sec; 0.092 sec/batch)
2017-05-06 22:42:17.494885: step 76010, loss = 0.60 (1738.1 examples/sec; 0.074 sec/batch)
2017-05-06 22:42:18.309531: step 76020, loss = 0.89 (1571.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:42:19.128188: step 76030, loss = 0.80 (1563.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:42:19.939437: step 76040, loss = 0.84 (1577.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:42:20.765792: step 76050, loss = 0.73 (1549.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:42:21.588017: step 76060, loss = 0.63 (1556.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:42:22.409662: step 76070, loss = 0.73 (1557.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:42:23.223277: step 76080, loss = 0.58 (1573.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:42:24.031399: step 76090, loss = 0.66 (1583.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:42:24.944505: step 76100, loss = 0.66 (1401.8 examples/sec; 0.091 sec/batch)
2017-05-06 22:42:25.669963: step 76110, loss = 0.79 (1764.4 examples/sec; 0.073 sec/batch)
2017-05-06 22:42:26.493120: step 76120, loss = 0.90 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:42:27.300820: step 76130, loss = 0.74 (1584.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:42:28.110242: step 76140, loss = 0.60 (1581.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:42:28.925289: step 76150, loss = 0.63 (1570.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:42:29.747451: step 76160, loss = 0.80 (1556.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:42:30.567471: step 76170, loss = 0.71 (1560.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:42:31.386221: step 76180, loss = 0.67 (1563.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:42:32.204570: step 76190, loss = 0.75 (1564.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:42:33.131775: step 76200, loss = 0.88 (1380.5 examples/sec; 0.093 sec/batch)
2017-05-06 22:42:33.850756: step 76210, loss = 0.62 (1780.3 examples/sec; 0.072 sec/batch)
2017-05-06 22:42:34.672767: step 76220, loss = 0.63 (1557.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:42:35.521829: step 76230, loss = 0.66 (1507.6 examples/sec; 0.085 sec/batch)
2017-05-06 22:42:36.309925: step 76240, loss = 0.98 (1624.2 examples/sec; 0.079 sec/batch)
2017-05-06 22:42:37.136881: step 76250, loss = 0.83 (1547.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:42:37.964105: step 76260, loss = 0.74 (1547.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:42:38.774149: step 76270, loss = 0.79 (1580.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:42:39.587078: step 76280, loss = 0.88 (1574.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:42:40.405362: step 76290, loss = 0.71 (1564.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:42:41.333665: step 76300, loss = 0.80 (1378.9 examples/sec; 0.093 sec/batch)
2017-05-06 22:42:42.069885: step 76310, loss = 0.67 (1738.6 examples/sec; 0.074 sec/batch)
2017-05-06 22:42:42.891118: step 76320, loss = 0.82 (1558.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:42:43.712093: step 76330, loss = 0.74 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:42:44.524252: step 76340, loss = 0.72 (1576.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:42:45.337986: step 76350, loss = 0.67 (1573.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:42:46.152862: step 76360, loss = 0.83 (1570.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:42:46.973481: step 76370, loss = 0.71 (1559.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:42:47.790744: step 76380, loss = 0.80 (1566.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:42:48.610400: step 76390, loss = 0.75 (1561.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:42:49.526343: step 76400, loss = 0.75 (1397.5 examples/sec; 0.092 sec/batch)
2017-05-06 22:42:50.250392: step 76410, loss = 0.71 (1767.8 examples/sec; 0.072 sec/batch)
2017-05-06 22:42:51.070514: step 76420, loss = 0.59 (1560.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:42:51.885921: step 76430, loss = 0.85 (1569.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:42:52.709738: step 76440, loss = 0.74 (1553.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:42:53.540629: step 76450, loss = 0.74 (1540.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:42:54.356283: step 76460, loss = 0.65 (1569.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:42:55.180070: step 76470, loss = 0.70 (1553.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:42:55.994748: step 76480, loss = 0.71 (1571.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:42:56.825847: step 76490, loss = 0.65 (1540.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:42:57.743080: step 76500, loss = 0.82 (1395.5 examples/sec; 0.092 sec/batch)
2017-05-06 22:42:58.466473: step 76510, loss = 0.67 (1769.4 examples/sec; 0.072 sec/batch)
2017-05-06 22:42:59.285969: step 76520, loss = 0.64 (1561.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:43:00.096987: step 76530, loss = 0.72 (1578.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:43:00.918515: step 76540, loss = 0.68 (1558.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:43:01.751681: step 76550, loss = 0.68 (1536.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:43:02.570320: step 76560, loss = 0.85 (1563.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:43:03.386102: step 76570, loss = 0.54 (1569.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:43:04.202772: step 76580, loss = 0.64 (1567.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:43:05.034370: step 76590, loss = 0.70 (1539.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:43:05.955857: step 76600, loss = 0.74 (1389.1 examples/sec; 0.092 sec/batch)
2017-05-06 22:43:06.677705: step 76610, loss = 0.85 (1773.2 examples/sec; 0.072 sec/batch)
2017-05-06 22:43:07.496563: step 76620, loss = 0.76 (1563.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:43:08.316025: step 76630, loss = 0.89 (1562.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:43:09.146532: step 76640, loss = 0.69 (1541.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:43:09.966282: step 76650, loss = 0.81 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:43:10.787368: step 76660, loss = 0.80 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:43:11.597578: step 76670, loss = 0.84 (1579.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:43:12.411009: step 76680, loss = 0.67 (1573.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:43:13.229397: step 76690, loss = 0.77 (1564.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:43:14.144484: step 76700, loss = 0.61 (1398.8 examples/sec; 0.092 sec/batch)
2017-05-06 22:43:14.879516: step 76710, loss = 0.74 (1741.4 examples/sec; 0.074 sec/batch)
2017-05-06 22:43:15.700560: step 76720, loss = 0.74 (1559.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:43:16.529447: step 76730, loss = 0.65 (1544.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:43:17.358196: step 76740, loss = 0.75 (1544.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:43:18.186809: step 76750, loss = 0.72 (1544.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:43:19.014374: step 76760, loss = 0.54 (1546.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:43:19.830805: step 76770, loss = 0.52 (1567.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:43:20.653547: step 76780, loss = 0.68 (1555.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:43:21.480653: step 76790, loss = 0.76 (1547.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:43:22.414561: step 76800, loss = 0.67 (1370.6 examples/sec; 0.093 sec/batch)
2017-05-06 22:43:23.140079: step 76810, loss = 0.61 (1764.3 examples/sec; 0.073 sec/batch)
2017-05-06 22:43:23.952986: step 76820, loss = 0.60 (1574.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:43:24.779722: step 76830, loss = 0.65 (1548.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:43:25.607195: step 76840, loss = 0.66 (1546.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:43:26.428977: step 76850, loss = 0.84 (1557.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:43:27.252795: step 76860, loss = 0.74 (1553.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:43:28.064088: step 76870, loss = 0.57 (1577.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:43:28.887201: step 76880, loss = 0.63 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:43:29.711424: step 76890, loss = 0.75 (1553.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:43:30.639757: step 76900, loss = 0.66 (1378.8 examples/sec; 0.093 sec/batch)
2017-05-06 22:43:31.361131: step 76910, loss = 0.65 (1774.4 examples/sec; 0.072 sec/batch)
2017-05-06 22:43:32.175846: step 76920, loss = 0.73 (1571.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:43:33.003886: step 76930, loss = 0.68 (1545.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:43:33.822625: step 76940, loss = 0.88 (1563.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:43:34.641480: step 76950, loss = 0.72 (1563.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:43:35.459783: step 76960, loss = 0.65 (1564.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:43:36.273590: step 76970, loss = 0.76 (1572.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:43:37.103093: step 76980, loss = 0.79 (1543.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:43:37.911192: step 76990, loss = 0.53 (1584.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:43:38.845431: step 77000, loss = 0.76 (1370.1 examples/sec; 0.093 sec/batch)
2017-05-06 22:43:39.554105: step 77010, loss = 0.64 (1806.2 examples/sec; 0.071 sec/batch)
2017-05-06 22:43:40.369733: step 77020, loss = 0.76 (1569.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:43:41.185378: step 77030, loss = 0.70 (1569.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:43:42.004125: step 77040, loss = 0.81 (1563.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:43:42.822569: step 77050, loss = 0.64 (1563.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:43:43.636082: step 77060, loss = 0.76 (1573.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:43:44.460305: step 77070, loss = 0.66 (1553.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:43:45.274892: step 77080, loss = 0.69 (1571.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:43:46.101636: step 77090, loss = 0.84 (1548.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:43:47.029861: step 77100, loss = 0.66 (1379.0 examples/sec; 0.093 sec/batch)
2017-05-06 22:43:47.740449: step 77110, loss = 0.60 (1801.3 examples/sec; 0.071 sec/batch)
2017-05-06 22:43:48.562506: step 77120, loss = 0.77 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:43:49.376439: step 77130, loss = 0.77 (1572.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:43:50.204055: step 77140, loss = 0.66 (1546.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:43:51.029936: step 77150, loss = 0.80 (1549.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:43:51.838018: step 77160, loss = 0.72 (1584.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:43:52.665401: step 77170, loss = 0.91 (1547.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:43:53.486111: step 77180, loss = 0.75 (1559.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:43:54.314236: step 77190, loss = 0.70 (1545.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:43:55.244243: step 77200, loss = 0.69 (1376.3 examples/sec; 0.093 sec/batch)
2017-05-06 22:43:55.951886: step 77210, loss = 0.74 (1808.8 examples/sec; 0.071 sec/batch)
2017-05-06 22:43:56.788276: step 77220, loss = 0.79 (1530.4 examples/sec; 0.084 sec/batch)
2017-05-06 22:43:57.591116: step 77230, loss = 0.69 (1594.4 examples/sec; 0.080 sec/batch)
2017-05-06 22:43:58.403474: step 77240, loss = 0.70 (1575.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:43:59.228552: step 77250, loss = 0.91 (1551.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:44:00.041151: step 77260, loss = 0.74 (1575.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:44:00.859751: step 77270, loss = 0.66 (1563.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:44:01.684900: step 77280, loss = 0.72 (1551.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:44:02.506426: step 77290, loss = 0.72 (1558.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:44:03.418542: step 77300, loss = 0.87 (1403.3 examples/sec; 0.091 sec/batch)
2017-05-06 22:44:04.137147: step 77310, loss = 0.78 (1781.2 examples/sec; 0.072 sec/batch)
2017-05-06 22:44:04.965010: step 77320, loss = 0.76 (1546.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:44:05.785733: step 77330, loss = 0.57 (1559.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:44:06.606482: step 77340, loss = 0.72 (1559.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:44:07.412813: step 77350, loss = 0.71 (1587.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:44:08.236390: step 77360, loss = 0.70 (1554.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:44:09.061641: step 77370, loss = 0.70 (1551.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:44:09.879875: step 77380, loss = 0.75 (1564.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:44:10.705838: step 77390, loss = 0.73 (1549.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:44:11.618822: step 77400, loss = 0.83 (1402.0 examples/sec; 0.091 sec/batch)
2017-05-06 22:44:12.335794: step 77410, loss = 0.75 (1785.3 examples/sec; 0.072 sec/batch)
2017-05-06 22:44:13.164075: step 77420, loss = 0.74 (1545.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:44:13.979322: step 77430, loss = 0.86 (1570.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:44:14.794004: step 77440, loss = 0.83 (1571.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:44:15.611013: step 77450, loss = 0.76 (1566.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:44:16.424146: step 77460, loss = 0.77 (1574.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:44:17.253604: step 77470, loss = 0.56 (1543.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:44:18.082803: step 77480, loss = 0.68 (1543.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:44:18.912628: step 77490, loss = 0.67 (1542.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:44:19.812433: step 77500, loss = 0.69 (1422.5 examples/sec; 0.090 sec/batch)
2017-05-06 22:44:20.535770: step 77510, loss = 0.69 (1769.6 examples/sec; 0.072 sec/batch)
2017-05-06 22:44:21.360141: step 77520, loss = 0.84 (1552.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:44:22.169617: step 77530, loss = 0.65 (1581.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:44:22.989483: step 77540, loss = 0.74 (1561.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:44:23.800245: step 77550, loss = 0.87 (1578.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:44:24.614925: step 77560, loss = 0.80 (1571.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:44:25.433989: step 77570, loss = 0.75 (1562.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:44:26.248351: step 77580, loss = 0.74 (1571.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:44:27.061464: step 77590, loss = 0.65 (1574.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:44:27.963519: step 77600, loss = 0.66 (1419.0 examples/sec; 0.090 sec/batch)
2017-05-06 22:44:28.689982: step 77610, loss = 0.77 (1762.0 examples/sec; 0.073 sec/batch)
2017-05-06 22:44:29.522536: step 77620, loss = 0.75 (1537.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:44:30.343573: step 77630, loss = 0.75 (1559.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:44:31.164176: step 77640, loss = 0.80 (1559.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:44:31.990343: step 77650, loss = 0.79 (1549.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:44:32.811290: step 77660, loss = 0.66 (1559.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:44:33.624507: step 77670, loss = 0.74 (1574.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:44:34.453819: step 77680, loss = 0.76 (1543.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:44:35.274444: step 77690, loss = 0.65 (1559.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:44:36.211937: step 77700, loss = 0.65 (1365.3 examples/sec; 0.094 sec/batch)
2017-05-06 22:44:36.914608: step 77710, loss = 0.79 (1821.6 examples/sec; 0.070 sec/batch)
2017-05-06 22:44:37.746908: step 77720, loss = 0.75 (1537.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:44:38.563893: step 77730, loss = 0.83 (1566.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:44:39.378146: step 77740, loss = 0.73 (1572.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:44:40.185115: step 77750, loss = 0.74 (1586.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:44:41.005570: step 77760, loss = 0.61 (1560.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:44:41.832415: step 77770, loss = 0.72 (1548.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:44:42.647891: step 77780, loss = 0.65 (1569.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:44:43.462181: step 77790, loss = 0.74 (1571.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:44:44.385619: step 77800, loss = 0.87 (1386.1 examples/sec; 0.092 sec/batch)
2017-05-06 22:44:45.104353: step 77810, loss = 0.54 (1780.9 examples/sec; 0.072 sec/batch)
2017-05-06 22:44:45.935827: step 77820, loss = 0.61 (1539.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:44:46.752929: step 77830, loss = 0.83 (1566.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:44:47.566024: step 77840, loss = 0.73 (1574.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:44:48.385575: step 77850, loss = 0.70 (1561.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:44:49.201037: step 77860, loss = 0.69 (1569.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:44:50.020436: step 77870, loss = 0.80 (1562.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:44:50.840461: step 77880, loss = 0.79 (1560.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:44:51.651570: step 77890, loss = 0.82 (1578.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:44:52.569459: step 77900, loss = 0.75 (1394.5 examples/sec; 0.092 sec/batch)
2017-05-06 22:44:53.306707: step 77910, loss = 0.79 (1736.2 examples/sec; 0.074 sec/batch)
2017-05-06 22:44:54.136838: step 77920, loss = 0.74 (1541.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:44:54.961753: step 77930, loss = 0.84 (1551.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:44:55.776996: step 77940, loss = 0.63 (1570.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:44:56.594986: step 77950, loss = 0.71 (1564.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:44:57.413927: step 77960, loss = 0.68 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:44:58.221602: step 77970, loss = 0.71 (1584.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:44:59.053368: step 77980, loss = 0.81 (1538.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:44:59.854886: step 77990, loss = 0.77 (1597.0 examples/sec; 0.080 sec/batch)
2017-05-06 22:45:00.785190: step 78000, loss = 0.73 (1375.9 examples/sec; 0.093 sec/batch)
2017-05-06 22:45:01.518789: step 78010, loss = 0.83 (1744.8 examples/sec; 0.073 sec/batch)
2017-05-06 22:45:02.352575: step 78020, loss = 0.72 (1535.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:45:03.177653: step 78030, loss = 0.58 (1551.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:45:03.986006: step 78040, loss = 0.69 (1583.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:45:04.806488: step 78050, loss = 0.71 (1560.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:45:05.632440: step 78060, loss = 0.67 (1549.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:45:06.452155: step 78070, loss = 0.76 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:45:07.272143: step 78080, loss = 0.66 (1561.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:45:08.079064: step 78090, loss = 0.68 (1586.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:45:09.002020: step 78100, loss = 0.81 (1386.9 examples/sec; 0.092 sec/batch)
2017-05-06 22:45:09.719903: step 78110, loss = 0.59 (1783.0 examples/sec; 0.072 sec/batch)
2017-05-06 22:45:10.532963: step 78120, loss = 0.72 (1574.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:45:11.357689: step 78130, loss = 0.64 (1552.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:45:12.167073: step 78140, loss = 0.73 (1581.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:45:12.988773: step 78150, loss = 0.77 (1557.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:45:13.813723: step 78160, loss = 0.71 (1551.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:45:14.643712: step 78170, loss = 0.54 (1542.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:45:15.452178: step 78180, loss = 0.69 (1583.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:45:16.268522: step 78190, loss = 0.78 (1568.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:45:17.195449: step 78200, loss = 0.78 (1380.9 examples/sec; 0.093 sec/batch)
2017-05-06 22:45:17.942893: step 78210, loss = 0.79 (1712.5 examples/sec; 0.075 sec/batch)
2017-05-06 22:45:18.753034: step 78220, loss = 0.74 (1580.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:45:19.560544: step 78230, loss = 0.78 (1585.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:45:20.383641: step 78240, loss = 0.77 (1555.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:45:21.211950: step 78250, loss = 0.81 (1545.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:45:22.034460: step 78260, loss = 0.70 (1556.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:45:22.856581: step 78270, loss = 0.81 (1557.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:45:23.663578: step 78280, loss = 0.72 (1586.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:45:24.488812: step 78290, loss = 0.70 (1551.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:45:25.409424: step 78300, loss = 0.70 (1390.4 examples/sec; 0.092 sec/batch)
2017-05-06 22:45:26.152764: step 78310, loss = 0.65 (1722.0 examples/sec; 0.074 sec/batch)
2017-05-06 22:45:26.959525: step 78320, loss = 0.76 (1586.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:45:27.777610: step 78330, loss = 0.82 (1564.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:45:28.588588: step 78340, loss = 0.77 (1578.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:45:29.410522: step 78350, loss = 0.74 (1557.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:45:30.229031: step 78360, loss = 0.81 (1563.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:45:31.043296: step 78370, loss = 0.76 (1572.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:45:31.853445: step 78380, loss = 0.63 (1579.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:45:32.671251: step 78390, loss = 0.83 (1565.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:45:33.589958: step 78400, loss = 0.69 (1393.3 examples/sec; 0.092 sec/batch)
2017-05-06 22:45:34.317289: step 78410, loss = 0.66 (1759.8 examples/sec; 0.073 sec/batch)
2017-05-06 22:45:35.131876: step 78420, loss = 0.65 (1571.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:45:35.941642: step 78430, loss = 0.68 (1580.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:45:36.754653: step 78440, loss = 0.71 (1574.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:45:37.577106: step 78450, loss = 0.78 (1556.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:45:38.398147: step 78460, loss = 0.78 (1559.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:45:39.215402: step 78470, loss = 0.61 (1566.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:45:40.022100: step 78480, loss = 0.65 (1586.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:45:40.843679: step 78490, loss = 0.62 (1558.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:45:41.767445: step 78500, loss = 0.73 (1385.6 examples/sec; 0.092 sec/batch)
2017-05-06 22:45:42.486175: step 78510, loss = 0.76 (1780.9 examples/sec; 0.072 sec/batch)
2017-05-06 22:45:43.310710: step 78520, loss = 0.62 (1552.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:45:44.129552: step 78530, loss = 0.77 (1563.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:45:44.953133: step 78540, loss = 0.77 (1554.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:45:45.773727: step 78550, loss = 0.91 (1559.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:45:46.597266: step 78560, loss = 0.86 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:45:47.413615: step 78570, loss = 0.71 (1568.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:45:48.231035: step 78580, loss = 0.91 (1565.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:45:49.044902: step 78590, loss = 0.60 (1572.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:45:49.956038: step 78600, loss = 0.63 (1404.8 examples/sec; 0.091 sec/batch)
2017-05-06 22:45:50.694232: step 78610, loss = 0.83 (1734.0 examples/sec; 0.074 sec/batch)
2017-05-06 22:45:51.511439: step 78620, loss = 0.76 (1566.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:45:52.331411: step 78630, loss = 0.67 (1561.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:45:53.141578: step 78640, loss = 0.60 (1579.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:45:53.968238: step 78650, loss = 0.75 (1548.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:45:54.787890: step 78660, loss = 0.68 (1561.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:45:55.602792: step 78670, loss = 0.89 (1570.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:45:56.420470: step 78680, loss = 0.65 (1565.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:45:57.233825: step 78690, loss = 0.73 (1573.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:45:58.143985: step 78700, loss = 0.88 (1406.4 examples/sec; 0.091 sec/batch)
2017-05-06 22:45:58.877374: step 78710, loss = 0.84 (1745.3 examples/sec; 0.073 sec/batch)
2017-05-06 22:45:59.688186: step 78720, loss = 0.81 (1578.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:46:00.504434: step 78730, loss = 0.72 (1568.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:01.324794: step 78740, loss = 0.70 (1560.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:02.140755: step 78750, loss = 0.71 (1568.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:02.966281: step 78760, loss = 0.83 (1550.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:46:03.780379: step 78770, loss = 0.67 (1572.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:46:04.597324: step 78780, loss = 1.07 (1566.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:05.419823: step 78790, loss = 0.89 (1556.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:06.351530: step 78800, loss = 0.67 (1373.8 examples/sec; 0.093 sec/batch)
2017-05-06 22:46:07.077583: step 78810, loss = 0.73 (1762.9 examples/sec; 0.073 sec/batch)
2017-05-06 22:46:07.894437: step 78820, loss = 0.67 (1567.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:08.713507: step 78830, loss = 0.66 (1562.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:09.530241: step 78840, loss = 0.80 (1567.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:10.342616: step 78850, loss = 0.89 (1575.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:46:11.150691: step 78860, loss = 0.80 (1584.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:46:11.956672: step 78870, loss = 0.70 (1588.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:46:12.767966: step 78880, loss = 0.62 (1577.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:46:13.597546: step 78890, loss = 0.84 (1543.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:46:14.513063: step 78900, loss = 0.63 (1398.1 examples/sec; 0.092 sec/batch)
2017-05-06 22:46:15.233389: step 78910, loss = 0.59 (1776.9 examples/sec; 0.072 sec/batch)
2017-05-06 22:46:16.047841: step 78920, loss = 0.82 (1571.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:46:16.863372: step 78930, loss = 0.82 (1569.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:17.689263: step 78940, loss = 0.73 (1549.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:46:18.508131: step 78950, loss = 0.69 (1563.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:19.323366: step 78960, loss = 0.77 (1570.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:20.137887: step 78970, loss = 0.63 (1571.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:46:20.953899: step 78980, loss = 0.65 (1568.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:21.771194: step 78990, loss = 0.61 (1566.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:22.688611: step 79000, loss = 0.74 (1395.2 examples/sec; 0.092 sec/batch)
2017-05-06 22:46:23.407229: step 79010, loss = 0.84 (1781.2 examples/sec; 0.072 sec/batch)
2017-05-06 22:46:24.225184: step 79020, loss = 0.64 (1564.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:25.033847: step 79030, loss = 0.82 (1582.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:46:25.853707: step 79040, loss = 0.77 (1561.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:26.670583: step 79050, loss = 0.69 (1566.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:27.477097: step 79060, loss = 0.79 (1587.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:46:28.290655: step 79070, loss = 0.67 (1573.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:46:29.100677: step 79080, loss = 0.77 (1580.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:46:29.918931: step 79090, loss = 0.73 (1564.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:30.846309: step 79100, loss = 0.64 (1380.2 examples/sec; 0.093 sec/batch)
2017-05-06 22:46:31.566628: step 79110, loss = 0.71 (1777.0 examples/sec; 0.072 sec/batch)
2017-05-06 22:46:32.389896: step 79120, loss = 0.75 (1554.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:33.209368: step 79130, loss = 0.82 (1562.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:34.031183: step 79140, loss = 0.91 (1557.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:34.852314: step 79150, loss = 0.77 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:35.665910: step 79160, loss = 0.67 (1573.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:46:36.489387: step 79170, loss = 0.64 (1554.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:37.313676: step 79180, loss = 0.78 (1552.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:38.136267: step 79190, loss = 0.81 (1556.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:39.079402: step 79200, loss = 0.75 (1357.2 examples/sec; 0.094 sec/batch)
2017-05-06 22:46:39.791112: step 79210, loss = 0.68 (1798.5 examples/sec; 0.071 sec/batch)
2017-05-06 22:46:40.619294: step 79220, loss = 0.81 (1545.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:46:41.443485: step 79230, loss = 0.78 (1553.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:42.264143: step 79240, loss = 0.60 (1559.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:43.084418: step 79250, loss = 0.76 (1560.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:43.897108: step 79260, loss = 0.70 (1575.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:46:44.719939: step 79270, loss = 0.79 (1555.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:45.535122: step 79280, loss = 0.77 (1570.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:46.354641: step 79290, loss = 0.74 (1561.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:47.272373: step 79300, loss = 0.59 (1394.7 examples/sec; 0.092 sec/batch)
2017-05-06 22:46:47.985668: step 79310, loss = 0.66 (1794.5 examples/sec; 0.071 sec/batch)
2017-05-06 22:46:48.804459: step 79320, loss = 0.82 (1563.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:49.624239: step 79330, loss = 0.69 (1561.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:50.443514: step 79340, loss = 0.66 (1562.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:51.266831: step 79350, loss = 0.58 (1554.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:52.079697: step 79360, loss = 0.73 (1574.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:46:52.902728: step 79370, loss = 0.56 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:53.726505: step 79380, loss = 0.60 (1553.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:54.548706: step 79390, loss = 0.83 (1556.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:55.468541: step 79400, loss = 0.72 (1391.6 examples/sec; 0.092 sec/batch)
2017-05-06 22:46:56.189119: step 79410, loss = 0.83 (1776.4 examples/sec; 0.072 sec/batch)
2017-05-06 22:46:57.017271: step 79420, loss = 0.64 (1545.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:46:57.836072: step 79430, loss = 0.61 (1563.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:58.657231: step 79440, loss = 0.68 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:46:59.467734: step 79450, loss = 0.69 (1579.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:47:00.293025: step 79460, loss = 0.70 (1551.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:47:01.116189: step 79470, loss = 0.72 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:01.937297: step 79480, loss = 0.65 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:02.760360: step 79490, loss = 0.93 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:03.683477: step 79500, loss = 0.76 (1386.6 examples/sec; 0.092 sec/batch)
2017-05-06 22:47:04.419765: step 79510, loss = 0.65 (1738.5 examples/sec; 0.074 sec/batch)
2017-05-06 22:47:05.244326: step 79520, loss = 0.74 (1552.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:06.071391: step 79530, loss = 0.85 (1547.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:47:06.891460: step 79540, loss = 0.58 (1560.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:07.700367: step 79550, loss = 0.72 (1582.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:47:08.532062: step 79560, loss = 0.87 (1539.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:47:09.353518: step 79570, loss = 0.77 (1558.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:10.183352: step 79580, loss = 0.60 (1542.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:47:11.007505: step 79590, loss = 0.67 (1553.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:11.930765: step 79600, loss = 0.84 (1386.4 examples/sec; 0.092 sec/batch)
2017-05-06 22:47:12.657830: step 79610, loss = 0.66 (1760.5 examples/sec; 0.073 sec/batch)
2017-05-06 22:47:13.488491: step 79620, loss = 0.72 (1541.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:47:14.314927: step 79630, loss = 0.69 (1548.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:47:15.132508: step 79640, loss = 0.83 (1565.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:15.939699: step 79650, loss = 0.72 (1585.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:47:16.758847: step 79660, loss = 0.66 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:17.582769: step 79670, loss = 0.68 (1553.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:18.401412: step 79680, loss = 0.80 (1563.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:19.231690: step 79690, loss = 0.81 (1541.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:47:20.142321: step 79700, loss = 0.58 (1405.6 examples/sec; 0.091 sec/batch)
2017-05-06 22:47:20.863466: step 79710, loss = 0.64 (1775.0 examples/sec; 0.072 sec/batch)
2017-05-06 22:47:21.686143: step 79720, loss = 0.72 (1555.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:22.516119: step 79730, loss = 0.74 (1542.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:47:23.340308: step 79740, loss = 0.71 (1553.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:24.158720: step 79750, loss = 0.78 (1564.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:24.980067: step 79760, loss = 0.89 (1558.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:25.798529: step 79770, loss = 0.79 (1563.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:26.620694: step 79780, loss = 0.68 (1556.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:27.435823: step 79790, loss = 0.64 (1570.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:28.353449: step 79800, loss = 0.66 (1394.9 examples/sec; 0.092 sec/batch)
2017-05-06 22:47:29.080104: step 79810, loss = 0.75 (1761.5 examples/sec; 0.073 sec/batch)
2017-05-06 22:47:29.906370: step 79820, loss = 0.77 (1549.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:47:30.726346: step 79830, loss = 0.71 (1561.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:31.546298: step 79840, loss = 0.77 (1561.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:32.368119: step 79850, loss = 0.82 (1557.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:33.186188: step 79860, loss = 0.65 (1564.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:34.006964: step 79870, loss = 0.62 (1559.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:34.828396: step 79880, loss = 0.69 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:35.645861: step 79890, loss = 0.70 (1565.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:36.578074: step 79900, loss = 0.73 (1373.1 examples/sec; 0.093 sec/batch)
2017-05-06 22:47:37.286719: step 79910, loss = 0.69 (1806.2 examples/sec; 0.071 sec/batch)
2017-05-06 22:47:38.110695: step 79920, loss = 0.79 (1553.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:38.931282: step 79930, loss = 0.76 (1559.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:39.742086: step 79940, loss = 0.78 (1578.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:47:40.559952: step 79950, loss = 0.78 (1565.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:41.377507: step 79960, loss = 0.78 (1565.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:42.203909: step 79970, loss = 0.67 (1548.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:47:43.026694: step 79980, loss = 0.83 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:43.840167: step 79990, loss = 0.58 (1573.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:47:44.755157: step 80000, loss = 0.89 (1398.9 examples/sec; 0.091 sec/batch)
2017-05-06 22:47:45.477352: step 80010, loss = 0.60 (1772.4 examples/sec; 0.072 sec/batch)
2017-05-06 22:47:46.292224: step 80020, loss = 0.68 (1570.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:47:47.110545: step 80030, loss = 0.61 (1564.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:47.918417: step 80040, loss = 0.65 (1584.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:47:48.740987: step 80050, loss = 0.75 (1556.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:49.564995: step 80060, loss = 0.72 (1553.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:50.381980: step 80070, loss = 0.77 (1566.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:51.211066: step 80080, loss = 0.73 (1543.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:47:52.023401: step 80090, loss = 0.74 (1575.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:47:52.951313: step 80100, loss = 0.63 (1379.4 examples/sec; 0.093 sec/batch)
2017-05-06 22:47:53.674005: step 80110, loss = 0.73 (1771.2 examples/sec; 0.072 sec/batch)
2017-05-06 22:47:54.503739: step 80120, loss = 0.66 (1542.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:47:55.326411: step 80130, loss = 0.76 (1555.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:56.134716: step 80140, loss = 0.68 (1583.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:47:56.960809: step 80150, loss = 0.60 (1549.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:47:57.779744: step 80160, loss = 0.81 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:58.602068: step 80170, loss = 0.64 (1556.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:47:59.414242: step 80180, loss = 0.76 (1576.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:48:00.255860: step 80190, loss = 0.79 (1520.9 examples/sec; 0.084 sec/batch)
2017-05-06 22:48:01.151712: step 80200, loss = 0.80 (1428.8 examples/sec; 0.090 sec/batch)
2017-05-06 22:48:01.874405: step 80210, loss = 0.92 (1771.2 examples/sec; 0.072 sec/batch)
2017-05-06 22:48:02.693653: step 80220, loss = 0.70 (1562.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:48:03.515399: step 80230, loss = 0.68 (1557.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:48:04.342954: step 80240, loss = 0.76 (1546.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:48:05.161372: step 80250, loss = 0.64 (1564.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:48:05.977221: step 80260, loss = 0.75 (1568.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:48:06.806623: step 80270, loss = 0.67 (1543.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:48:07.620984: step 80280, loss = 0.73 (1571.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:48:08.446722: step 80290, loss = 0.60 (1550.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:48:09.361483: step 80300, loss = 0.73 (1399.3 examples/sec; 0.091 sec/batch)
2017-05-06 22:48:10.087750: step 80310, loss = 0.72 (1763.1 examples/sec; 0.073 sec/batch)
2017-05-06 22:48:10.913845: step 80320, loss = 0.77 (1549.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:48:11.728010: step 80330, loss = 0.78 (1572.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:48:12.549989: step 80340, loss = 0.65 (1557.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:48:13.402161: step 80350, loss = 0.71 (1502.1 examples/sec; 0.085 sec/batch)
2017-05-06 22:48:14.223744: step 80360, loss = 0.76 (1558.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:48:15.051374: step 80370, loss = 0.75 (1546.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:48:15.861790: step 80380, loss = 0.79 (1579.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:48:16.679968: step 80390, loss = 0.75 (1564.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:48:17.598685: step 80400, loss = 0.72 (1393.2 examples/sec; 0.092 sec/batch)
2017-05-06 22:48:18.329968: step 80410, loss = 0.62 (1750.4 examples/sec; 0.073 sec/batch)
2017-05-06 22:48:19.157172: step 80420, loss = 0.63 (1547.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:48:19.975797: step 80430, loss = 0.72 (1563.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:48:20.798921: step 80440, loss = 0.83 (1555.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:48:21.621940: step 80450, loss = 0.86 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:48:22.442299: step 80460, loss = 0.74 (1560.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:48:23.261287: step 80470, loss = 0.61 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:48:24.073398: step 80480, loss = 0.71 (1576.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:48:24.897883: step 80490, loss = 0.76 (1552.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:48:25.807768: step 80500, loss = 0.73 (1406.8 examples/sec; 0.091 sec/batch)
2017-05-06 22:48:26.523460: step 80510, loss = 0.79 (1788.5 examples/sec; 0.072 sec/batch)
2017-05-06 22:48:27.341129: step 80520, loss = 0.80 (1565.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:48:28.157986: step 80530, loss = 0.80 (1567.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:48:28.977405: step 80540, loss = 0.76 (1562.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:48:29.799797: step 80550, loss = 0.60 (1556.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:48:30.623328: step 80560, loss = 0.66 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:48:31.433009: step 80570, loss = 0.73 (1580.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:48:32.248259: step 80580, loss = 0.57 (1570.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:48:33.070893: step 80590, loss = 0.78 (1556.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:48:33.990887: step 80600, loss = 0.66 (1391.3 examples/sec; 0.092 sec/batch)
2017-05-06 22:48:34.719827: step 80610, loss = 0.64 (1756.0 examples/sec; 0.073 sec/batch)
2017-05-06 22:48:35.535114: step 80620, loss = 0.66 (1570.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:48:36.359834: step 80630, loss = 0.78 (1552.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:48:37.182860: step 80640, loss = 0.74 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:48:38.007067: step 80650, loss = 0.75 (1553.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:48:38.821629: step 80660, loss = 0.74 (1571.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:48:39.631188: step 80670, loss = 0.78 (1581.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:48:40.446129: step 80680, loss = 0.71 (1570.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:48:41.266589: step 80690, loss = 0.70 (1560.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:48:42.190108: step 80700, loss = 0.69 (1386.0 examples/sec; 0.092 sec/batch)
2017-05-06 22:48:42.908002: step 80710, loss = 0.72 (1783.0 examples/sec; 0.072 sec/batch)
2017-05-06 22:48:43.715320: step 80720, loss = 0.69 (1585.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:48:44.536783: step 80730, loss = 0.58 (1558.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:48:45.354944: step 80740, loss = 0.76 (1564.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:48:46.184714: step 80750, loss = 0.81 (1542.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:48:47.003757: step 80760, loss = 0.70 (1562.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:48:47.813201: step 80770, loss = 0.84 (1581.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:48:48.644551: step 80780, loss = 0.68 (1539.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:48:49.467693: step 80790, loss = 0.75 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:48:50.403380: step 80800, loss = 0.73 (1368.0 examples/sec; 0.094 sec/batch)
2017-05-06 22:48:51.106365: step 80810, loss = 0.59 (1820.8 examples/sec; 0.070 sec/batch)
2017-05-06 22:48:51.920324: step 80820, loss = 0.71 (1572.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:48:52.751096: step 80830, loss = 0.75 (1540.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:48:53.581483: step 80840, loss = 0.60 (1541.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:48:54.399113: step 80850, loss = 0.71 (1565.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:48:55.227819: step 80860, loss = 0.74 (1544.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:48:56.049095: step 80870, loss = 0.70 (1558.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:48:56.868917: step 80880, loss = 0.67 (1561.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:48:57.701864: step 80890, loss = 0.71 (1536.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:48:58.612278: step 80900, loss = 0.75 (1406.0 examples/sec; 0.091 sec/batch)
2017-05-06 22:48:59.331537: step 80910, loss = 0.67 (1779.6 examples/sec; 0.072 sec/batch)
2017-05-06 22:49:00.148252: step 80920, loss = 0.61 (1567.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:00.966595: step 80930, loss = 0.66 (1564.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:01.789378: step 80940, loss = 0.78 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:02.616305: step 80950, loss = 0.87 (1547.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:49:03.433606: step 80960, loss = 0.77 (1566.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:04.253176: step 80970, loss = 0.73 (1561.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:05.071928: step 80980, loss = 0.60 (1563.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:05.895177: step 80990, loss = 0.78 (1554.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:06.816406: step 81000, loss = 0.53 (1389.5 examples/sec; 0.092 sec/batch)
2017-05-06 22:49:07.524320: step 81010, loss = 0.72 (1808.1 examples/sec; 0.071 sec/batch)
2017-05-06 22:49:08.348232: step 81020, loss = 0.73 (1553.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:09.169531: step 81030, loss = 0.73 (1558.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:09.992710: step 81040, loss = 0.71 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:10.811378: step 81050, loss = 0.64 (1563.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:11.625351: step 81060, loss = 0.74 (1572.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:49:12.448420: step 81070, loss = 0.78 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:13.271725: step 81080, loss = 0.63 (1554.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:14.093034: step 81090, loss = 0.78 (1558.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:15.014033: step 81100, loss = 0.71 (1389.8 examples/sec; 0.092 sec/batch)
2017-05-06 22:49:15.729639: step 81110, loss = 0.79 (1788.7 examples/sec; 0.072 sec/batch)
2017-05-06 22:49:16.543316: step 81120, loss = 0.81 (1573.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:49:17.370155: step 81130, loss = 0.68 (1548.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:49:18.191629: step 81140, loss = 0.62 (1558.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:19.003806: step 81150, loss = 0.71 (1576.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:49:19.814635: step 81160, loss = 0.73 (1578.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:49:20.632392: step 81170, loss = 0.82 (1565.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:21.480409: step 81180, loss = 0.76 (1509.4 examples/sec; 0.085 sec/batch)
2017-05-06 22:49:22.277767: step 81190, loss = 0.79 (1605.3 examples/sec; 0.080 sec/batch)
2017-05-06 22:49:23.193827: step 81200, loss = 0.80 (1397.3 examples/sec; 0.092 sec/batch)
2017-05-06 22:49:23.912068: step 81210, loss = 0.72 (1782.2 examples/sec; 0.072 sec/batch)
2017-05-06 22:49:24.728213: step 81220, loss = 0.71 (1568.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:25.550291: step 81230, loss = 0.73 (1557.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:26.373830: step 81240, loss = 0.59 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:27.186857: step 81250, loss = 0.76 (1574.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:49:28.005297: step 81260, loss = 0.68 (1564.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:28.820771: step 81270, loss = 0.76 (1569.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:29.645602: step 81280, loss = 0.83 (1551.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:30.459865: step 81290, loss = 0.68 (1572.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:49:31.375736: step 81300, loss = 0.77 (1397.6 examples/sec; 0.092 sec/batch)
2017-05-06 22:49:32.099596: step 81310, loss = 0.83 (1768.3 examples/sec; 0.072 sec/batch)
2017-05-06 22:49:32.915931: step 81320, loss = 0.72 (1568.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:33.741364: step 81330, loss = 0.70 (1550.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:49:34.561108: step 81340, loss = 0.70 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:35.382258: step 81350, loss = 0.74 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:36.199728: step 81360, loss = 0.66 (1565.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:37.027550: step 81370, loss = 0.68 (1546.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:49:37.850669: step 81380, loss = 0.67 (1555.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:38.675593: step 81390, loss = 0.74 (1551.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:39.591922: step 81400, loss = 0.73 (1396.9 examples/sec; 0.092 sec/batch)
2017-05-06 22:49:40.316445: step 81410, loss = 0.81 (1766.7 examples/sec; 0.072 sec/batch)
2017-05-06 22:49:41.137091: step 81420, loss = 0.63 (1559.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:41.957888: step 81430, loss = 0.75 (1559.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:42.778101: step 81440, loss = 0.62 (1560.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:43.595755: step 81450, loss = 0.69 (1565.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:44.419502: step 81460, loss = 0.65 (1553.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:45.257572: step 81470, loss = 0.72 (1527.3 examples/sec; 0.084 sec/batch)
2017-05-06 22:49:46.084121: step 81480, loss = 0.72 (1548.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:49:46.907692: step 81490, loss = 0.69 (1554.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:47.822096: step 81500, loss = 0.65 (1399.8 examples/sec; 0.091 sec/batch)
2017-05-06 22:49:48.539534: step 81510, loss = 0.65 (1784.1 examples/sec; 0.072 sec/batch)
2017-05-06 22:49:49.356237: step 81520, loss = 0.79 (1567.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:50.185000: step 81530, loss = 0.75 (1544.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:49:51.011242: step 81540, loss = 0.81 (1549.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:49:51.824637: step 81550, loss = 0.72 (1573.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:49:52.647692: step 81560, loss = 0.70 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:53.468672: step 81570, loss = 0.73 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:54.284865: step 81580, loss = 0.73 (1568.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:55.103005: step 81590, loss = 0.83 (1564.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:49:56.025213: step 81600, loss = 0.67 (1388.0 examples/sec; 0.092 sec/batch)
2017-05-06 22:49:56.762286: step 81610, loss = 0.75 (1736.6 examples/sec; 0.074 sec/batch)
2017-05-06 22:49:57.588070: step 81620, loss = 0.54 (1550.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:49:58.402778: step 81630, loss = 0.76 (1571.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:49:59.231361: step 81640, loss = 0.63 (1544.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:50:00.042566: step 81650, loss = 0.70 (1577.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:50:00.871631: step 81660, loss = 0.77 (1543.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:50:01.707122: step 81670, loss = 0.64 (1532.0 examples/sec; 0.084 sec/batch)
2017-05-06 22:50:02.531888: step 81680, loss = 0.72 (1552.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:50:03.358414: step 81690, loss = 0.72 (1548.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:50:04.277093: step 81700, loss = 0.64 (1393.3 examples/sec; 0.092 sec/batch)
2017-05-06 22:50:04.998148: step 81710, loss = 0.66 (1775.2 examples/sec; 0.072 sec/batch)
2017-05-06 22:50:05.815103: step 81720, loss = 0.69 (1566.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:50:06.640270: step 81730, loss = 0.77 (1551.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:50:07.456343: step 81740, loss = 0.65 (1568.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:50:08.269543: step 81750, loss = 0.67 (1574.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:50:09.094124: step 81760, loss = 0.69 (1552.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:50:09.910180: step 81770, loss = 0.78 (1568.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:50:10.735470: step 81780, loss = 0.67 (1551.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:50:11.554789: step 81790, loss = 0.58 (1562.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:50:12.463098: step 81800, loss = 0.64 (1409.2 examples/sec; 0.091 sec/batch)
2017-05-06 22:50:13.184170: step 81810, loss = 0.75 (1775.1 examples/sec; 0.072 sec/batch)
2017-05-06 22:50:13.997957: step 81820, loss = 0.79 (1572.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:50:14.817925: step 81830, loss = 0.71 (1561.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:50:15.635697: step 81840, loss = 0.75 (1565.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:50:16.461086: step 81850, loss = 0.70 (1550.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:50:17.278963: step 81860, loss = 0.65 (1565.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:50:18.090234: step 81870, loss = 0.80 (1577.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:50:18.912646: step 81880, loss = 0.58 (1556.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:50:19.722919: step 81890, loss = 0.74 (1579.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:50:20.633613: step 81900, loss = 0.79 (1405.5 examples/sec; 0.091 sec/batch)
2017-05-06 22:50:21.350983: step 81910, loss = 0.67 (1784.3 examples/sec; 0.072 sec/batch)
2017-05-06 22:50:22.171156: step 81920, loss = 0.73 (1560.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:50:22.995306: step 81930, loss = 0.64 (1553.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:50:23.815642: step 81940, loss = 0.67 (1560.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:50:24.637701: step 81950, loss = 0.77 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:50:25.452172: step 81960, loss = 0.75 (1571.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:50:26.268303: step 81970, loss = 0.62 (1568.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:50:27.095295: step 81980, loss = 0.84 (1547.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:50:27.916480: step 81990, loss = 0.68 (1558.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:50:28.827166: step 82000, loss = 0.71 (1405.5 examples/sec; 0.091 sec/batch)
2017-05-06 22:50:29.558137: step 82010, loss = 0.78 (1751.1 examples/sec; 0.073 sec/batch)
2017-05-06 22:50:30.379954: step 82020, loss = 0.80 (1557.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:50:31.200841: step 82030, loss = 0.82 (1559.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:50:32.017553: step 82040, loss = 0.76 (1567.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:50:32.835839: step 82050, loss = 0.69 (1564.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:50:33.661463: step 82060, loss = 0.71 (1550.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:50:34.483437: step 82070, loss = 0.87 (1557.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:50:35.312727: step 82080, loss = 0.61 (1543.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:50:36.125032: step 82090, loss = 0.63 (1575.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:50:37.043509: step 82100, loss = 0.79 (1393.6 examples/sec; 0.092 sec/batch)
2017-05-06 22:50:37.770499: step 82110, loss = 0.68 (1760.7 examples/sec; 0.073 sec/batch)
2017-05-06 22:50:38.597981: step 82120, loss = 0.77 (1546.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:50:39.410377: step 82130, loss = 0.63 (1575.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:50:40.231519: step 82140, loss = 0.67 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:50:41.041925: step 82150, loss = 0.74 (1579.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:50:41.868419: step 82160, loss = 0.58 (1548.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:50:42.719505: step 82170, loss = 0.70 (1504.0 examples/sec; 0.085 sec/batch)
2017-05-06 22:50:43.510747: step 82180, loss = 0.80 (1617.7 examples/sec; 0.079 sec/batch)
2017-05-06 22:50:44.332118: step 82190, loss = 0.94 (1558.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:50:45.250264: step 82200, loss = 0.71 (1394.1 examples/sec; 0.092 sec/batch)
2017-05-06 22:50:45.972636: step 82210, loss = 0.62 (1771.9 examples/sec; 0.072 sec/batch)
2017-05-06 22:50:46.794535: step 82220, loss = 0.73 (1557.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:50:47.607655: step 82230, loss = 0.70 (1574.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:50:48.430341: step 82240, loss = 0.68 (1555.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:50:49.257241: step 82250, loss = 0.71 (1547.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:50:50.074347: step 82260, loss = 0.82 (1566.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:50:50.901169: step 82270, loss = 0.71 (1548.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:50:51.711841: step 82280, loss = 0.82 (1578.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:50:52.529435: step 82290, loss = 0.66 (1565.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:50:53.452294: step 82300, loss = 0.67 (1387.0 examples/sec; 0.092 sec/batch)
2017-05-06 22:50:54.181965: step 82310, loss = 0.72 (1754.2 examples/sec; 0.073 sec/batch)
2017-05-06 22:50:55.003883: step 82320, loss = 0.68 (1557.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:50:55.818663: step 82330, loss = 0.76 (1571.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:50:56.644118: step 82340, loss = 0.71 (1550.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:50:57.461770: step 82350, loss = 0.61 (1565.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:50:58.281762: step 82360, loss = 0.64 (1561.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:50:59.109511: step 82370, loss = 0.71 (1546.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:50:59.925209: step 82380, loss = 0.71 (1569.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:00.746639: step 82390, loss = 0.68 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:01.670607: step 82400, loss = 0.67 (1385.3 examples/sec; 0.092 sec/batch)
2017-05-06 22:51:02.398590: step 82410, loss = 0.69 (1758.3 examples/sec; 0.073 sec/batch)
2017-05-06 22:51:03.214175: step 82420, loss = 0.73 (1569.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:04.022155: step 82430, loss = 0.67 (1584.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:51:04.851550: step 82440, loss = 0.75 (1543.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:51:05.675124: step 82450, loss = 0.80 (1554.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:06.492702: step 82460, loss = 0.83 (1565.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:07.320461: step 82470, loss = 0.83 (1546.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:51:08.137458: step 82480, loss = 0.62 (1566.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:08.956234: step 82490, loss = 0.63 (1563.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:09.874306: step 82500, loss = 0.74 (1394.2 examples/sec; 0.092 sec/batch)
2017-05-06 22:51:10.603591: step 82510, loss = 0.60 (1755.2 examples/sec; 0.073 sec/batch)
2017-05-06 22:51:11.422417: step 82520, loss = 0.74 (1563.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:12.234718: step 82530, loss = 0.70 (1575.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:51:13.056512: step 82540, loss = 0.75 (1557.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:13.879131: step 82550, loss = 0.73 (1556.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:14.700901: step 82560, loss = 0.85 (1557.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:15.520086: step 82570, loss = 0.72 (1562.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:16.336648: step 82580, loss = 0.71 (1567.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:17.154287: step 82590, loss = 0.66 (1565.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:18.072030: step 82600, loss = 0.79 (1394.7 examples/sec; 0.092 sec/batch)
2017-05-06 22:51:18.787947: step 82610, loss = 0.64 (1787.9 examples/sec; 0.072 sec/batch)
2017-05-06 22:51:19.600939: step 82620, loss = 0.73 (1574.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:51:20.416892: step 82630, loss = 0.68 (1568.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:21.240219: step 82640, loss = 0.63 (1554.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:22.062859: step 82650, loss = 0.66 (1556.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:22.888698: step 82660, loss = 0.86 (1549.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:51:23.703943: step 82670, loss = 0.68 (1570.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:24.527015: step 82680, loss = 0.81 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:25.341662: step 82690, loss = 0.72 (1571.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:51:26.263777: step 82700, loss = 0.80 (1388.1 examples/sec; 0.092 sec/batch)
2017-05-06 22:51:26.980463: step 82710, loss = 0.67 (1786.0 examples/sec; 0.072 sec/batch)
2017-05-06 22:51:27.790397: step 82720, loss = 0.55 (1580.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:51:28.618432: step 82730, loss = 0.68 (1545.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:51:29.437490: step 82740, loss = 0.65 (1562.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:30.253913: step 82750, loss = 0.77 (1567.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:31.079198: step 82760, loss = 0.67 (1551.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:51:31.890401: step 82770, loss = 0.90 (1577.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:51:32.705560: step 82780, loss = 0.64 (1570.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:33.528387: step 82790, loss = 0.65 (1555.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:34.451988: step 82800, loss = 0.63 (1385.9 examples/sec; 0.092 sec/batch)
2017-05-06 22:51:35.168662: step 82810, loss = 0.71 (1786.0 examples/sec; 0.072 sec/batch)
2017-05-06 22:51:35.984776: step 82820, loss = 0.63 (1568.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:36.814210: step 82830, loss = 0.69 (1543.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:51:37.635337: step 82840, loss = 0.79 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:38.455529: step 82850, loss = 0.54 (1560.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:39.276230: step 82860, loss = 0.68 (1559.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:40.099181: step 82870, loss = 0.77 (1555.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:40.921394: step 82880, loss = 0.74 (1556.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:41.746624: step 82890, loss = 0.69 (1551.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:51:42.669950: step 82900, loss = 0.60 (1386.3 examples/sec; 0.092 sec/batch)
2017-05-06 22:51:43.394867: step 82910, loss = 0.70 (1765.7 examples/sec; 0.072 sec/batch)
2017-05-06 22:51:44.208135: step 82920, loss = 0.65 (1573.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:51:45.023156: step 82930, loss = 0.63 (1570.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:45.851739: step 82940, loss = 0.65 (1544.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:51:46.673940: step 82950, loss = 0.65 (1556.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:47.492906: step 82960, loss = 0.70 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:48.311406: step 82970, loss = 0.66 (1563.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:49.135916: step 82980, loss = 0.68 (1552.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:49.950831: step 82990, loss = 0.81 (1570.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:51:50.871143: step 83000, loss = 0.71 (1390.8 examples/sec; 0.092 sec/batch)
2017-05-06 22:51:51.583205: step 83010, loss = 0.68 (1797.6 examples/sec; 0.071 sec/batch)
2017-05-06 22:51:52.398862: step 83020, loss = 0.64 (1569.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:53.215742: step 83030, loss = 0.72 (1566.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:54.034950: step 83040, loss = 0.77 (1562.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:54.855595: step 83050, loss = 0.73 (1559.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:55.668594: step 83060, loss = 0.73 (1574.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:51:56.497422: step 83070, loss = 0.71 (1544.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:51:57.315458: step 83080, loss = 0.76 (1564.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:58.138054: step 83090, loss = 0.71 (1556.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:51:59.057078: step 83100, loss = 0.75 (1392.8 examples/sec; 0.092 sec/batch)
2017-05-06 22:51:59.777038: step 83110, loss = 0.73 (1777.8 examples/sec; 0.072 sec/batch)
2017-05-06 22:52:00.602856: step 83120, loss = 0.66 (1550.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:52:01.425893: step 83130, loss = 0.65 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:52:02.244964: step 83140, loss = 0.70 (1562.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:52:03.078906: step 83150, loss = 0.81 (1534.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:52:03.913599: step 83160, loss = 0.68 (1533.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:52:04.714303: step 83170, loss = 0.67 (1598.6 examples/sec; 0.080 sec/batch)
2017-05-06 22:52:05.527288: step 83180, loss = 0.70 (1574.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:52:06.350017: step 83190, loss = 0.68 (1555.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:52:07.266661: step 83200, loss = 0.68 (1396.4 examples/sec; 0.092 sec/batch)
2017-05-06 22:52:07.987586: step 83210, loss = 0.73 (1775.5 examples/sec; 0.072 sec/batch)
2017-05-06 22:52:08.810282: step 83220, loss = 0.63 (1555.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:52:09.625934: step 83230, loss = 0.72 (1569.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:52:10.452985: step 83240, loss = 0.73 (1547.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:52:11.278426: step 83250, loss = 0.70 (1550.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:52:12.096001: step 83260, loss = 0.75 (1565.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:52:12.913354: step 83270, loss = 0.81 (1566.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:52:13.739420: step 83280, loss = 0.76 (1549.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:52:14.554371: step 83290, loss = 0.74 (1570.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:52:15.471115: step 83300, loss = 0.56 (1396.2 examples/sec; 0.092 sec/batch)
2017-05-06 22:52:16.188772: step 83310, loss = 0.77 (1783.6 examples/sec; 0.072 sec/batch)
2017-05-06 22:52:17.013330: step 83320, loss = 0.73 (1552.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:52:17.841845: step 83330, loss = 0.72 (1544.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:52:18.658063: step 83340, loss = 0.64 (1568.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:52:19.478503: step 83350, loss = 0.80 (1560.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:52:20.310989: step 83360, loss = 0.70 (1537.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:52:21.125175: step 83370, loss = 0.80 (1572.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:52:21.951322: step 83380, loss = 0.86 (1549.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:52:22.773338: step 83390, loss = 0.71 (1557.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:52:23.685997: step 83400, loss = 0.76 (1402.5 examples/sec; 0.091 sec/batch)
2017-05-06 22:52:24.410948: step 83410, loss = 0.74 (1765.6 examples/sec; 0.072 sec/batch)
2017-05-06 22:52:25.224341: step 83420, loss = 0.72 (1573.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:52:26.047891: step 83430, loss = 0.75 (1554.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:52:26.866207: step 83440, loss = 0.71 (1564.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:52:27.680905: step 83450, loss = 0.91 (1571.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:52:28.501948: step 83460, loss = 0.66 (1559.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:52:29.323843: step 83470, loss = 0.73 (1557.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:52:30.143476: step 83480, loss = 0.82 (1561.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:52:30.965634: step 83490, loss = 0.67 (1556.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:52:31.879780: step 83500, loss = 0.71 (1400.2 examples/sec; 0.091 sec/batch)
2017-05-06 22:52:32.602548: step 83510, loss = 0.66 (1771.0 examples/sec; 0.072 sec/batch)
2017-05-06 22:52:33.423302: step 83520, loss = 0.80 (1559.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:52:34.242887: step 83530, loss = 0.65 (1561.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:52:35.072550: step 83540, loss = 0.50 (1542.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:52:35.882311: step 83550, loss = 0.74 (1580.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:52:36.705727: step 83560, loss = 0.64 (1554.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:52:37.533915: step 83570, loss = 0.75 (1545.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:52:38.355146: step 83580, loss = 0.71 (1558.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:52:39.174081: step 83590, loss = 0.64 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:52:40.099750: step 83600, loss = 0.69 (1382.8 examples/sec; 0.093 sec/batch)
2017-05-06 22:52:40.798513: step 83610, loss = 0.73 (1831.8 examples/sec; 0.070 sec/batch)
2017-05-06 22:52:41.607134: step 83620, loss = 0.61 (1583.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:52:42.420759: step 83630, loss = 0.73 (1573.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:52:43.243504: step 83640, loss = 0.79 (1555.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:52:44.048308: step 83650, loss = 0.80 (1590.5 examples/sec; 0.080 sec/batch)
2017-05-06 22:52:44.874621: step 83660, loss = 0.63 (1549.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:52:45.690923: step 83670, loss = 0.76 (1568.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:52:46.511453: step 83680, loss = 0.81 (1560.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:52:47.329797: step 83690, loss = 0.60 (1564.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:52:48.247731: step 83700, loss = 0.63 (1394.4 examples/sec; 0.092 sec/batch)
2017-05-06 22:52:48.956123: step 83710, loss = 0.74 (1806.9 examples/sec; 0.071 sec/batch)
2017-05-06 22:52:49.775126: step 83720, loss = 0.76 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:52:50.589706: step 83730, loss = 0.72 (1571.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:52:51.414795: step 83740, loss = 0.87 (1551.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:52:52.240824: step 83750, loss = 0.73 (1549.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:52:53.067228: step 83760, loss = 0.66 (1548.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:52:53.884320: step 83770, loss = 0.66 (1566.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:52:54.705220: step 83780, loss = 0.66 (1559.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:52:55.524013: step 83790, loss = 0.82 (1563.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:52:56.430032: step 83800, loss = 0.71 (1412.8 examples/sec; 0.091 sec/batch)
2017-05-06 22:52:57.155603: step 83810, loss = 0.83 (1764.1 examples/sec; 0.073 sec/batch)
2017-05-06 22:52:57.975061: step 83820, loss = 0.58 (1562.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:52:58.803255: step 83830, loss = 0.62 (1545.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:52:59.613913: step 83840, loss = 0.63 (1579.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:53:00.429916: step 83850, loss = 0.68 (1568.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:53:01.252872: step 83860, loss = 0.73 (1555.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:53:02.071077: step 83870, loss = 0.73 (1564.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:53:02.893428: step 83880, loss = 0.65 (1556.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:53:03.705421: step 83890, loss = 0.65 (1576.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:53:04.630815: step 83900, loss = 0.61 (1383.2 examples/sec; 0.093 sec/batch)
2017-05-06 22:53:05.342305: step 83910, loss = 0.70 (1799.0 examples/sec; 0.071 sec/batch)
2017-05-06 22:53:06.164281: step 83920, loss = 0.74 (1557.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:53:06.982035: step 83930, loss = 0.72 (1565.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:53:07.798487: step 83940, loss = 0.77 (1567.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:53:08.616682: step 83950, loss = 0.77 (1564.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:53:09.447723: step 83960, loss = 0.80 (1540.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:53:10.266995: step 83970, loss = 0.75 (1562.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:53:11.093435: step 83980, loss = 0.62 (1548.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:53:11.906989: step 83990, loss = 0.97 (1573.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:53:12.843525: step 84000, loss = 0.65 (1366.7 examples/sec; 0.094 sec/batch)
2017-05-06 22:53:13.533282: step 84010, loss = 0.58 (1855.7 examples/sec; 0.069 sec/batch)
2017-05-06 22:53:14.353563: step 84020, loss = 0.63 (1560.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:53:15.174473: step 84030, loss = 0.76 (1559.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:53:15.985189: step 84040, loss = 0.77 (1578.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:53:16.804920: step 84050, loss = 0.83 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:53:17.632811: step 84060, loss = 0.67 (1546.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:53:18.443235: step 84070, loss = 0.62 (1579.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:53:19.264154: step 84080, loss = 0.75 (1559.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:53:20.080648: step 84090, loss = 0.73 (1567.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:53:21.017279: step 84100, loss = 0.65 (1366.6 examples/sec; 0.094 sec/batch)
2017-05-06 22:53:21.731003: step 84110, loss = 0.66 (1793.4 examples/sec; 0.071 sec/batch)
2017-05-06 22:53:22.551101: step 84120, loss = 0.78 (1560.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:53:23.374571: step 84130, loss = 0.87 (1554.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:53:24.187902: step 84140, loss = 0.68 (1573.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:53:25.025959: step 84150, loss = 0.73 (1527.3 examples/sec; 0.084 sec/batch)
2017-05-06 22:53:25.829283: step 84160, loss = 0.60 (1593.4 examples/sec; 0.080 sec/batch)
2017-05-06 22:53:26.657609: step 84170, loss = 0.75 (1545.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:53:27.482312: step 84180, loss = 0.72 (1552.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:53:28.300762: step 84190, loss = 0.79 (1563.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:53:29.219095: step 84200, loss = 0.72 (1393.8 examples/sec; 0.092 sec/batch)
2017-05-06 22:53:29.958185: step 84210, loss = 0.79 (1731.9 examples/sec; 0.074 sec/batch)
2017-05-06 22:53:30.779816: step 84220, loss = 0.72 (1557.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:53:31.593101: step 84230, loss = 0.61 (1573.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:53:32.412461: step 84240, loss = 0.69 (1562.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:53:33.243618: step 84250, loss = 0.69 (1540.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:53:34.061167: step 84260, loss = 0.66 (1565.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:53:34.882987: step 84270, loss = 0.67 (1557.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:53:35.688833: step 84280, loss = 0.73 (1588.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:53:36.510234: step 84290, loss = 0.65 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:53:37.436426: step 84300, loss = 0.71 (1382.0 examples/sec; 0.093 sec/batch)
2017-05-06 22:53:38.149130: step 84310, loss = 0.70 (1796.0 examples/sec; 0.071 sec/batch)
2017-05-06 22:53:38.966108: step 84320, loss = 0.68 (1566.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:53:39.780144: step 84330, loss = 0.69 (1572.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:53:40.590587: step 84340, loss = 0.75 (1579.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:53:41.415563: step 84350, loss = 0.84 (1551.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:53:42.234921: step 84360, loss = 0.77 (1562.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:53:43.053152: step 84370, loss = 0.77 (1564.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:53:43.865561: step 84380, loss = 0.72 (1575.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:53:44.683327: step 84390, loss = 0.72 (1565.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:53:45.608256: step 84400, loss = 0.84 (1383.9 examples/sec; 0.092 sec/batch)
2017-05-06 22:53:46.326282: step 84410, loss = 0.65 (1782.7 examples/sec; 0.072 sec/batch)
2017-05-06 22:53:47.140830: step 84420, loss = 0.53 (1571.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:53:47.943111: step 84430, loss = 0.76 (1595.4 examples/sec; 0.080 sec/batch)
2017-05-06 22:53:48.757106: step 84440, loss = 0.62 (1572.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:53:49.571648: step 84450, loss = 0.64 (1571.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:53:50.395808: step 84460, loss = 0.74 (1553.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:53:51.209682: step 84470, loss = 0.69 (1572.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:53:52.023259: step 84480, loss = 0.81 (1573.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:53:52.843955: step 84490, loss = 0.80 (1559.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:53:53.761516: step 84500, loss = 0.66 (1395.0 examples/sec; 0.092 sec/batch)
2017-05-06 22:53:54.482888: step 84510, loss = 0.86 (1774.4 examples/sec; 0.072 sec/batch)
2017-05-06 22:53:55.297586: step 84520, loss = 0.67 (1571.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:53:56.108820: step 84530, loss = 0.64 (1577.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:53:56.925876: step 84540, loss = 0.70 (1566.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:53:57.752257: step 84550, loss = 0.74 (1548.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:53:58.566591: step 84560, loss = 0.71 (1571.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:53:59.384734: step 84570, loss = 0.90 (1564.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:00.203364: step 84580, loss = 0.74 (1563.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:01.023273: step 84590, loss = 0.67 (1561.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:01.960763: step 84600, loss = 0.72 (1365.3 examples/sec; 0.094 sec/batch)
2017-05-06 22:54:02.663057: step 84610, loss = 0.81 (1822.6 examples/sec; 0.070 sec/batch)
2017-05-06 22:54:03.478690: step 84620, loss = 0.63 (1569.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:04.298226: step 84630, loss = 0.71 (1561.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:05.120294: step 84640, loss = 0.70 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:05.951121: step 84650, loss = 0.71 (1540.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:54:06.775223: step 84660, loss = 0.90 (1553.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:07.594015: step 84670, loss = 0.77 (1563.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:08.415769: step 84680, loss = 0.55 (1557.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:09.234784: step 84690, loss = 0.78 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:10.162919: step 84700, loss = 0.79 (1379.1 examples/sec; 0.093 sec/batch)
2017-05-06 22:54:10.882824: step 84710, loss = 0.79 (1778.0 examples/sec; 0.072 sec/batch)
2017-05-06 22:54:11.694447: step 84720, loss = 0.76 (1577.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:54:12.509512: step 84730, loss = 0.72 (1570.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:13.326957: step 84740, loss = 0.61 (1565.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:14.150444: step 84750, loss = 0.65 (1554.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:14.966162: step 84760, loss = 0.66 (1569.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:15.778163: step 84770, loss = 0.63 (1576.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:54:16.595964: step 84780, loss = 0.72 (1565.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:17.413531: step 84790, loss = 0.79 (1565.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:18.330898: step 84800, loss = 0.80 (1395.3 examples/sec; 0.092 sec/batch)
2017-05-06 22:54:19.059361: step 84810, loss = 0.86 (1757.1 examples/sec; 0.073 sec/batch)
2017-05-06 22:54:19.867921: step 84820, loss = 0.57 (1583.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:54:20.684812: step 84830, loss = 0.63 (1566.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:21.507662: step 84840, loss = 0.56 (1555.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:22.331801: step 84850, loss = 0.71 (1553.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:23.148804: step 84860, loss = 0.69 (1566.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:23.953883: step 84870, loss = 0.58 (1589.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:54:24.770248: step 84880, loss = 0.62 (1567.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:25.595887: step 84890, loss = 0.75 (1550.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:54:26.523165: step 84900, loss = 0.60 (1380.4 examples/sec; 0.093 sec/batch)
2017-05-06 22:54:27.241229: step 84910, loss = 0.71 (1782.6 examples/sec; 0.072 sec/batch)
2017-05-06 22:54:28.049990: step 84920, loss = 0.76 (1582.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:54:28.871599: step 84930, loss = 0.67 (1557.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:29.698483: step 84940, loss = 0.71 (1548.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:54:30.516161: step 84950, loss = 0.89 (1565.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:31.327711: step 84960, loss = 0.73 (1577.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:54:32.143250: step 84970, loss = 0.66 (1569.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:32.965406: step 84980, loss = 0.77 (1556.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:33.778277: step 84990, loss = 0.77 (1574.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:54:34.704350: step 85000, loss = 0.78 (1382.2 examples/sec; 0.093 sec/batch)
2017-05-06 22:54:35.413246: step 85010, loss = 0.75 (1805.6 examples/sec; 0.071 sec/batch)
2017-05-06 22:54:36.230353: step 85020, loss = 0.80 (1566.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:37.047240: step 85030, loss = 0.60 (1566.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:37.864007: step 85040, loss = 0.57 (1567.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:38.683858: step 85050, loss = 0.80 (1561.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:39.502306: step 85060, loss = 0.65 (1563.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:40.320557: step 85070, loss = 0.75 (1564.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:41.148681: step 85080, loss = 0.76 (1545.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:54:41.967572: step 85090, loss = 0.66 (1563.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:42.885130: step 85100, loss = 0.77 (1395.0 examples/sec; 0.092 sec/batch)
2017-05-06 22:54:43.600844: step 85110, loss = 0.70 (1788.4 examples/sec; 0.072 sec/batch)
2017-05-06 22:54:44.417807: step 85120, loss = 0.70 (1566.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:45.235324: step 85130, loss = 0.68 (1565.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:46.082785: step 85140, loss = 0.84 (1510.4 examples/sec; 0.085 sec/batch)
2017-05-06 22:54:46.894158: step 85150, loss = 0.77 (1577.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:54:47.708751: step 85160, loss = 0.65 (1571.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:54:48.523224: step 85170, loss = 0.78 (1571.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:54:49.341210: step 85180, loss = 0.58 (1564.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:50.162325: step 85190, loss = 0.96 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:51.086119: step 85200, loss = 0.66 (1385.6 examples/sec; 0.092 sec/batch)
2017-05-06 22:54:51.797954: step 85210, loss = 0.66 (1798.2 examples/sec; 0.071 sec/batch)
2017-05-06 22:54:52.616097: step 85220, loss = 0.87 (1564.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:53.442009: step 85230, loss = 0.71 (1549.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:54:54.257975: step 85240, loss = 0.73 (1568.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:55.077504: step 85250, loss = 0.75 (1561.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:55.883893: step 85260, loss = 0.70 (1587.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:54:56.701412: step 85270, loss = 0.78 (1565.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:57.519877: step 85280, loss = 0.81 (1563.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:54:58.331297: step 85290, loss = 0.68 (1577.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:54:59.241619: step 85300, loss = 0.82 (1406.1 examples/sec; 0.091 sec/batch)
2017-05-06 22:54:59.966398: step 85310, loss = 0.86 (1766.1 examples/sec; 0.072 sec/batch)
2017-05-06 22:55:00.777723: step 85320, loss = 0.67 (1577.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:55:01.603097: step 85330, loss = 0.75 (1550.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:55:02.430828: step 85340, loss = 0.84 (1546.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:55:03.255924: step 85350, loss = 0.62 (1551.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:55:04.066175: step 85360, loss = 0.80 (1579.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:55:04.887798: step 85370, loss = 0.62 (1557.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:55:05.705299: step 85380, loss = 0.85 (1565.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:55:06.530749: step 85390, loss = 0.73 (1550.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:55:07.452519: step 85400, loss = 0.62 (1388.6 examples/sec; 0.092 sec/batch)
2017-05-06 22:55:08.156868: step 85410, loss = 0.80 (1817.3 examples/sec; 0.070 sec/batch)
2017-05-06 22:55:08.972014: step 85420, loss = 0.69 (1570.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:55:09.788745: step 85430, loss = 0.78 (1567.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:55:10.619817: step 85440, loss = 0.83 (1540.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:55:11.440655: step 85450, loss = 0.80 (1559.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:55:12.246618: step 85460, loss = 0.65 (1588.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:55:13.063449: step 85470, loss = 0.61 (1567.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:55:13.887091: step 85480, loss = 0.74 (1554.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:55:14.719167: step 85490, loss = 0.60 (1538.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:55:15.643943: step 85500, loss = 0.73 (1384.1 examples/sec; 0.092 sec/batch)
2017-05-06 22:55:16.365443: step 85510, loss = 0.71 (1774.1 examples/sec; 0.072 sec/batch)
2017-05-06 22:55:17.195209: step 85520, loss = 0.71 (1542.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:55:18.014557: step 85530, loss = 0.76 (1562.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:55:18.834271: step 85540, loss = 0.71 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:55:19.641934: step 85550, loss = 0.65 (1584.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:55:20.463971: step 85560, loss = 0.72 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:55:21.270140: step 85570, loss = 0.75 (1587.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:55:22.092606: step 85580, loss = 0.67 (1556.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:55:22.919253: step 85590, loss = 0.74 (1548.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:55:23.832664: step 85600, loss = 0.86 (1401.3 examples/sec; 0.091 sec/batch)
2017-05-06 22:55:24.563365: step 85610, loss = 0.79 (1751.8 examples/sec; 0.073 sec/batch)
2017-05-06 22:55:25.382320: step 85620, loss = 0.70 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:55:26.197377: step 85630, loss = 0.60 (1570.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:55:27.012235: step 85640, loss = 0.83 (1570.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:55:27.825433: step 85650, loss = 0.82 (1574.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:55:28.639520: step 85660, loss = 0.75 (1572.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:55:29.463507: step 85670, loss = 0.75 (1553.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:55:30.289455: step 85680, loss = 0.68 (1549.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:55:31.113224: step 85690, loss = 0.76 (1553.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:55:32.019346: step 85700, loss = 0.70 (1412.6 examples/sec; 0.091 sec/batch)
2017-05-06 22:55:32.748831: step 85710, loss = 0.69 (1754.7 examples/sec; 0.073 sec/batch)
2017-05-06 22:55:33.563950: step 85720, loss = 0.76 (1570.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:55:34.386234: step 85730, loss = 0.71 (1556.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:55:35.213770: step 85740, loss = 0.70 (1546.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:55:36.025435: step 85750, loss = 0.70 (1577.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:55:36.847648: step 85760, loss = 0.68 (1556.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:55:37.665326: step 85770, loss = 0.60 (1565.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:55:38.486389: step 85780, loss = 0.71 (1559.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:55:39.305979: step 85790, loss = 0.77 (1561.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:55:40.220063: step 85800, loss = 0.71 (1400.3 examples/sec; 0.091 sec/batch)
2017-05-06 22:55:40.947006: step 85810, loss = 0.67 (1760.8 examples/sec; 0.073 sec/batch)
2017-05-06 22:55:41.773707: step 85820, loss = 0.71 (1548.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:55:42.593437: step 85830, loss = 0.76 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:55:43.415609: step 85840, loss = 0.73 (1556.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:55:44.245831: step 85850, loss = 0.68 (1541.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:55:45.068898: step 85860, loss = 0.80 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:55:45.897380: step 85870, loss = 0.63 (1545.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:55:46.723686: step 85880, loss = 0.76 (1549.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:55:47.528577: step 85890, loss = 0.68 (1590.3 examples/sec; 0.080 sec/batch)
2017-05-06 22:55:48.455432: step 85900, loss = 0.73 (1381.0 examples/sec; 0.093 sec/batch)
2017-05-06 22:55:49.175608: step 85910, loss = 0.67 (1777.3 examples/sec; 0.072 sec/batch)
2017-05-06 22:55:49.991273: step 85920, loss = 0.65 (1569.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:55:50.819192: step 85930, loss = 0.71 (1546.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:55:51.642413: step 85940, loss = 0.74 (1554.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:55:52.463356: step 85950, loss = 0.76 (1559.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:55:53.286372: step 85960, loss = 0.71 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:55:54.096367: step 85970, loss = 0.74 (1580.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:55:54.924079: step 85980, loss = 0.74 (1546.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:55:55.736944: step 85990, loss = 0.66 (1574.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:55:56.653176: step 86000, loss = 0.76 (1397.0 examples/sec; 0.092 sec/batch)
2017-05-06 22:55:57.385922: step 86010, loss = 0.67 (1746.9 examples/sec; 0.073 sec/batch)
2017-05-06 22:55:58.203923: step 86020, loss = 0.78 (1564.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:55:59.025195: step 86030, loss = 0.82 (1558.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:55:59.843561: step 86040, loss = 0.74 (1564.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:00.665158: step 86050, loss = 0.82 (1557.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:01.492578: step 86060, loss = 0.70 (1547.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:56:02.316459: step 86070, loss = 0.76 (1553.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:03.143638: step 86080, loss = 0.76 (1547.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:56:03.957974: step 86090, loss = 0.83 (1571.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:56:04.881789: step 86100, loss = 0.68 (1385.6 examples/sec; 0.092 sec/batch)
2017-05-06 22:56:05.592481: step 86110, loss = 0.63 (1801.1 examples/sec; 0.071 sec/batch)
2017-05-06 22:56:06.408679: step 86120, loss = 0.66 (1568.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:07.258220: step 86130, loss = 0.74 (1506.7 examples/sec; 0.085 sec/batch)
2017-05-06 22:56:08.054000: step 86140, loss = 0.88 (1608.5 examples/sec; 0.080 sec/batch)
2017-05-06 22:56:08.881720: step 86150, loss = 0.72 (1546.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:56:09.708460: step 86160, loss = 0.72 (1548.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:56:10.532786: step 86170, loss = 0.84 (1552.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:11.347955: step 86180, loss = 0.62 (1570.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:12.159367: step 86190, loss = 0.64 (1577.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:56:13.080039: step 86200, loss = 0.76 (1390.3 examples/sec; 0.092 sec/batch)
2017-05-06 22:56:13.800729: step 86210, loss = 0.61 (1776.1 examples/sec; 0.072 sec/batch)
2017-05-06 22:56:14.623037: step 86220, loss = 0.75 (1556.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:15.445740: step 86230, loss = 0.68 (1555.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:16.265860: step 86240, loss = 0.67 (1560.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:17.082867: step 86250, loss = 0.88 (1566.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:17.900197: step 86260, loss = 0.78 (1566.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:18.723643: step 86270, loss = 0.67 (1554.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:19.536037: step 86280, loss = 0.66 (1575.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:56:20.354672: step 86290, loss = 0.58 (1563.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:21.283240: step 86300, loss = 0.76 (1378.5 examples/sec; 0.093 sec/batch)
2017-05-06 22:56:22.005803: step 86310, loss = 0.69 (1771.5 examples/sec; 0.072 sec/batch)
2017-05-06 22:56:22.829333: step 86320, loss = 0.66 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:23.650697: step 86330, loss = 0.91 (1558.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:24.475769: step 86340, loss = 0.70 (1551.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:56:25.288165: step 86350, loss = 0.67 (1575.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:56:26.105605: step 86360, loss = 0.70 (1565.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:26.919591: step 86370, loss = 0.62 (1572.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:56:27.733351: step 86380, loss = 0.67 (1572.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:56:28.562760: step 86390, loss = 0.82 (1543.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:56:29.484986: step 86400, loss = 0.67 (1387.9 examples/sec; 0.092 sec/batch)
2017-05-06 22:56:30.211902: step 86410, loss = 0.68 (1760.9 examples/sec; 0.073 sec/batch)
2017-05-06 22:56:31.041038: step 86420, loss = 0.60 (1543.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:56:31.857680: step 86430, loss = 0.70 (1567.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:32.685389: step 86440, loss = 0.65 (1546.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:56:33.503774: step 86450, loss = 0.85 (1564.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:34.329895: step 86460, loss = 0.69 (1549.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:56:35.148475: step 86470, loss = 0.70 (1563.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:35.957858: step 86480, loss = 0.61 (1581.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:56:36.778180: step 86490, loss = 0.67 (1560.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:37.690813: step 86500, loss = 0.61 (1402.5 examples/sec; 0.091 sec/batch)
2017-05-06 22:56:38.417149: step 86510, loss = 0.64 (1762.3 examples/sec; 0.073 sec/batch)
2017-05-06 22:56:39.235182: step 86520, loss = 0.65 (1564.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:40.061865: step 86530, loss = 0.68 (1548.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:56:40.881973: step 86540, loss = 0.66 (1560.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:41.704421: step 86550, loss = 0.78 (1556.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:42.525019: step 86560, loss = 0.79 (1559.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:43.349942: step 86570, loss = 0.60 (1551.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:44.162983: step 86580, loss = 0.74 (1574.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:56:44.985219: step 86590, loss = 0.60 (1556.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:45.909186: step 86600, loss = 0.62 (1385.3 examples/sec; 0.092 sec/batch)
2017-05-06 22:56:46.633422: step 86610, loss = 0.73 (1767.4 examples/sec; 0.072 sec/batch)
2017-05-06 22:56:47.449970: step 86620, loss = 0.81 (1567.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:48.269975: step 86630, loss = 0.65 (1561.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:49.083101: step 86640, loss = 0.73 (1574.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:56:49.905711: step 86650, loss = 0.71 (1556.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:50.728291: step 86660, loss = 0.57 (1556.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:51.542875: step 86670, loss = 0.79 (1571.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:56:52.368122: step 86680, loss = 0.57 (1551.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:56:53.189254: step 86690, loss = 0.65 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:54.106797: step 86700, loss = 0.63 (1395.0 examples/sec; 0.092 sec/batch)
2017-05-06 22:56:54.827800: step 86710, loss = 0.81 (1775.3 examples/sec; 0.072 sec/batch)
2017-05-06 22:56:55.639913: step 86720, loss = 0.75 (1576.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:56:56.460036: step 86730, loss = 0.66 (1560.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:57.281612: step 86740, loss = 0.64 (1558.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:58.103024: step 86750, loss = 0.73 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:58.927350: step 86760, loss = 0.74 (1552.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:56:59.746900: step 86770, loss = 0.72 (1561.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:00.565814: step 86780, loss = 0.69 (1563.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:01.391715: step 86790, loss = 0.74 (1549.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:57:02.313926: step 86800, loss = 0.65 (1388.0 examples/sec; 0.092 sec/batch)
2017-05-06 22:57:03.042444: step 86810, loss = 0.77 (1757.0 examples/sec; 0.073 sec/batch)
2017-05-06 22:57:03.862518: step 86820, loss = 0.67 (1560.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:04.683591: step 86830, loss = 0.69 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:05.503309: step 86840, loss = 0.57 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:06.322785: step 86850, loss = 0.70 (1562.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:07.149540: step 86860, loss = 0.71 (1548.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:57:07.965312: step 86870, loss = 0.77 (1569.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:08.784655: step 86880, loss = 0.85 (1562.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:09.607440: step 86890, loss = 0.72 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:10.534154: step 86900, loss = 0.58 (1381.2 examples/sec; 0.093 sec/batch)
2017-05-06 22:57:11.258490: step 86910, loss = 0.67 (1767.1 examples/sec; 0.072 sec/batch)
2017-05-06 22:57:12.069675: step 86920, loss = 0.75 (1578.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:57:12.904665: step 86930, loss = 0.75 (1532.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:57:13.729059: step 86940, loss = 0.69 (1552.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:14.556843: step 86950, loss = 0.67 (1546.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:57:15.375964: step 86960, loss = 0.79 (1562.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:16.191641: step 86970, loss = 0.77 (1569.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:17.011755: step 86980, loss = 0.73 (1560.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:17.834829: step 86990, loss = 0.59 (1555.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:18.775807: step 87000, loss = 0.74 (1360.3 examples/sec; 0.094 sec/batch)
2017-05-06 22:57:19.471536: step 87010, loss = 0.70 (1839.8 examples/sec; 0.070 sec/batch)
2017-05-06 22:57:20.287071: step 87020, loss = 0.60 (1569.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:21.112448: step 87030, loss = 0.87 (1550.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:57:21.937520: step 87040, loss = 0.88 (1551.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:57:22.755572: step 87050, loss = 0.67 (1564.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:23.572161: step 87060, loss = 0.68 (1567.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:24.386496: step 87070, loss = 0.65 (1571.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:57:25.207086: step 87080, loss = 0.76 (1559.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:26.028321: step 87090, loss = 0.69 (1558.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:26.947217: step 87100, loss = 0.69 (1393.0 examples/sec; 0.092 sec/batch)
2017-05-06 22:57:27.670645: step 87110, loss = 0.78 (1769.4 examples/sec; 0.072 sec/batch)
2017-05-06 22:57:28.510826: step 87120, loss = 0.71 (1523.5 examples/sec; 0.084 sec/batch)
2017-05-06 22:57:29.301653: step 87130, loss = 0.83 (1618.6 examples/sec; 0.079 sec/batch)
2017-05-06 22:57:30.118018: step 87140, loss = 0.73 (1567.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:30.933717: step 87150, loss = 0.67 (1569.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:31.745783: step 87160, loss = 0.71 (1576.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:57:32.565265: step 87170, loss = 0.91 (1562.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:33.390222: step 87180, loss = 0.67 (1551.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:34.208513: step 87190, loss = 0.67 (1564.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:35.134308: step 87200, loss = 0.77 (1382.6 examples/sec; 0.093 sec/batch)
2017-05-06 22:57:35.846757: step 87210, loss = 0.85 (1796.6 examples/sec; 0.071 sec/batch)
2017-05-06 22:57:36.690828: step 87220, loss = 0.74 (1516.5 examples/sec; 0.084 sec/batch)
2017-05-06 22:57:37.501684: step 87230, loss = 0.68 (1578.6 examples/sec; 0.081 sec/batch)
2017-05-06 22:57:38.315326: step 87240, loss = 0.73 (1573.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:57:39.138463: step 87250, loss = 0.81 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:39.950731: step 87260, loss = 0.54 (1575.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:57:40.771377: step 87270, loss = 0.65 (1559.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:41.588233: step 87280, loss = 0.75 (1567.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:42.409248: step 87290, loss = 0.75 (1559.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:43.326876: step 87300, loss = 0.68 (1394.9 examples/sec; 0.092 sec/batch)
2017-05-06 22:57:44.044251: step 87310, loss = 0.78 (1784.3 examples/sec; 0.072 sec/batch)
2017-05-06 22:57:44.866844: step 87320, loss = 0.82 (1556.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:45.690331: step 87330, loss = 0.78 (1554.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:46.511758: step 87340, loss = 0.72 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:47.330959: step 87350, loss = 0.69 (1562.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:48.141517: step 87360, loss = 0.66 (1579.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:57:48.963594: step 87370, loss = 0.95 (1557.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:49.785270: step 87380, loss = 0.74 (1557.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:50.606211: step 87390, loss = 0.73 (1559.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:51.523922: step 87400, loss = 0.72 (1394.8 examples/sec; 0.092 sec/batch)
2017-05-06 22:57:52.250896: step 87410, loss = 0.77 (1760.7 examples/sec; 0.073 sec/batch)
2017-05-06 22:57:53.078911: step 87420, loss = 0.88 (1545.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:57:53.903411: step 87430, loss = 0.81 (1552.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:54.727452: step 87440, loss = 0.69 (1553.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:55.539183: step 87450, loss = 0.62 (1576.9 examples/sec; 0.081 sec/batch)
2017-05-06 22:57:56.364729: step 87460, loss = 0.72 (1550.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:57:57.187099: step 87470, loss = 0.73 (1556.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:58.010370: step 87480, loss = 0.78 (1554.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:58.825407: step 87490, loss = 0.75 (1570.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:57:59.736125: step 87500, loss = 0.67 (1405.5 examples/sec; 0.091 sec/batch)
2017-05-06 22:58:00.458026: step 87510, loss = 0.66 (1773.1 examples/sec; 0.072 sec/batch)
2017-05-06 22:58:01.288640: step 87520, loss = 0.61 (1541.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:58:02.125919: step 87530, loss = 0.59 (1528.8 examples/sec; 0.084 sec/batch)
2017-05-06 22:58:02.947313: step 87540, loss = 0.66 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:58:03.768373: step 87550, loss = 0.74 (1559.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:58:04.587521: step 87560, loss = 0.87 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:58:05.407312: step 87570, loss = 0.76 (1561.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:58:06.222723: step 87580, loss = 0.65 (1569.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:58:07.049239: step 87590, loss = 0.90 (1548.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:58:07.980163: step 87600, loss = 0.77 (1375.0 examples/sec; 0.093 sec/batch)
2017-05-06 22:58:08.690748: step 87610, loss = 0.61 (1801.3 examples/sec; 0.071 sec/batch)
2017-05-06 22:58:09.513551: step 87620, loss = 0.68 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:58:10.330558: step 87630, loss = 0.96 (1566.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:58:11.149582: step 87640, loss = 0.68 (1562.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:58:11.959296: step 87650, loss = 0.67 (1580.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:58:12.788741: step 87660, loss = 0.75 (1543.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:58:13.633603: step 87670, loss = 0.68 (1515.0 examples/sec; 0.084 sec/batch)
2017-05-06 22:58:14.453194: step 87680, loss = 0.70 (1561.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:58:15.280493: step 87690, loss = 0.70 (1547.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:58:16.191188: step 87700, loss = 0.87 (1405.5 examples/sec; 0.091 sec/batch)
2017-05-06 22:58:16.913977: step 87710, loss = 0.80 (1770.9 examples/sec; 0.072 sec/batch)
2017-05-06 22:58:17.728098: step 87720, loss = 0.66 (1572.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:58:18.558392: step 87730, loss = 0.63 (1541.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:58:19.384417: step 87740, loss = 0.78 (1549.6 examples/sec; 0.083 sec/batch)
2017-05-06 22:58:20.200192: step 87750, loss = 0.71 (1569.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:58:21.026539: step 87760, loss = 0.77 (1549.0 examples/sec; 0.083 sec/batch)
2017-05-06 22:58:21.856979: step 87770, loss = 0.77 (1541.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:58:22.671380: step 87780, loss = 0.80 (1571.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:58:23.485064: step 87790, loss = 0.87 (1573.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:58:24.401933: step 87800, loss = 0.67 (1396.0 examples/sec; 0.092 sec/batch)
2017-05-06 22:58:25.114249: step 87810, loss = 0.83 (1796.9 examples/sec; 0.071 sec/batch)
2017-05-06 22:58:25.934333: step 87820, loss = 0.64 (1560.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:58:26.753646: step 87830, loss = 0.79 (1562.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:58:27.562081: step 87840, loss = 0.71 (1583.3 examples/sec; 0.081 sec/batch)
2017-05-06 22:58:28.382959: step 87850, loss = 0.80 (1559.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:58:29.206176: step 87860, loss = 0.67 (1554.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:58:30.031040: step 87870, loss = 0.67 (1551.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:58:30.855823: step 87880, loss = 0.93 (1551.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:58:31.671014: step 87890, loss = 0.68 (1570.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:58:32.583269: step 87900, loss = 0.64 (1403.1 examples/sec; 0.091 sec/batch)
2017-05-06 22:58:33.307414: step 87910, loss = 0.72 (1767.6 examples/sec; 0.072 sec/batch)
2017-05-06 22:58:34.131740: step 87920, loss = 0.74 (1552.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:58:34.949793: step 87930, loss = 0.69 (1564.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:58:35.762662: step 87940, loss = 0.78 (1574.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:58:36.590241: step 87950, loss = 0.86 (1546.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:58:37.417149: step 87960, loss = 0.72 (1547.9 examples/sec; 0.083 sec/batch)
2017-05-06 22:58:38.236463: step 87970, loss = 0.87 (1562.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:58:39.066098: step 87980, loss = 0.65 (1542.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:58:39.885173: step 87990, loss = 0.70 (1562.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:58:40.827832: step 88000, loss = 0.60 (1357.9 examples/sec; 0.094 sec/batch)
2017-05-06 22:58:41.526646: step 88010, loss = 0.52 (1831.7 examples/sec; 0.070 sec/batch)
2017-05-06 22:58:42.349129: step 88020, loss = 0.73 (1556.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:58:43.169869: step 88030, loss = 0.74 (1559.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:58:43.982223: step 88040, loss = 0.71 (1575.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:58:44.796748: step 88050, loss = 0.67 (1571.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:58:45.624967: step 88060, loss = 0.74 (1545.5 examples/sec; 0.083 sec/batch)
2017-05-06 22:58:46.447124: step 88070, loss = 0.64 (1556.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:58:47.264851: step 88080, loss = 0.66 (1565.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:58:48.078459: step 88090, loss = 0.65 (1573.2 examples/sec; 0.081 sec/batch)
2017-05-06 22:58:49.004432: step 88100, loss = 0.70 (1382.5 examples/sec; 0.093 sec/batch)
2017-05-06 22:58:49.747725: step 88110, loss = 0.68 (1721.8 examples/sec; 0.074 sec/batch)
2017-05-06 22:58:50.547307: step 88120, loss = 0.55 (1600.8 examples/sec; 0.080 sec/batch)
2017-05-06 22:58:51.378070: step 88130, loss = 0.72 (1540.8 examples/sec; 0.083 sec/batch)
2017-05-06 22:58:52.193924: step 88140, loss = 0.56 (1568.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:58:53.020694: step 88150, loss = 0.68 (1548.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:58:53.827570: step 88160, loss = 0.75 (1586.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:58:54.647431: step 88170, loss = 0.68 (1561.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:58:55.468536: step 88180, loss = 0.78 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:58:56.285992: step 88190, loss = 0.70 (1565.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:58:57.209501: step 88200, loss = 0.86 (1386.0 examples/sec; 0.092 sec/batch)
2017-05-06 22:58:57.948273: step 88210, loss = 0.73 (1732.6 examples/sec; 0.074 sec/batch)
2017-05-06 22:58:58.758003: step 88220, loss = 0.68 (1580.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:58:59.576954: step 88230, loss = 0.62 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:00.401378: step 88240, loss = 0.66 (1552.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:01.223737: step 88250, loss = 0.83 (1556.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:02.045696: step 88260, loss = 0.75 (1557.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:02.877798: step 88270, loss = 0.77 (1538.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:59:03.691458: step 88280, loss = 0.70 (1573.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:59:04.510556: step 88290, loss = 0.65 (1562.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:05.430980: step 88300, loss = 0.65 (1390.7 examples/sec; 0.092 sec/batch)
2017-05-06 22:59:06.147486: step 88310, loss = 0.69 (1786.5 examples/sec; 0.072 sec/batch)
2017-05-06 22:59:06.964518: step 88320, loss = 0.72 (1566.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:07.782261: step 88330, loss = 0.72 (1565.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:08.599455: step 88340, loss = 0.78 (1566.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:09.418119: step 88350, loss = 0.70 (1563.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:10.236214: step 88360, loss = 0.66 (1564.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:11.054708: step 88370, loss = 0.82 (1563.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:11.868749: step 88380, loss = 0.64 (1572.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:59:12.687221: step 88390, loss = 0.77 (1563.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:13.615262: step 88400, loss = 0.68 (1379.2 examples/sec; 0.093 sec/batch)
2017-05-06 22:59:14.330790: step 88410, loss = 0.63 (1788.9 examples/sec; 0.072 sec/batch)
2017-05-06 22:59:15.142243: step 88420, loss = 0.75 (1577.4 examples/sec; 0.081 sec/batch)
2017-05-06 22:59:15.952449: step 88430, loss = 0.80 (1579.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:59:16.768489: step 88440, loss = 0.74 (1568.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:17.592103: step 88450, loss = 0.61 (1554.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:18.415150: step 88460, loss = 0.77 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:19.240873: step 88470, loss = 0.61 (1550.2 examples/sec; 0.083 sec/batch)
2017-05-06 22:59:20.059365: step 88480, loss = 0.67 (1563.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:20.883809: step 88490, loss = 0.77 (1552.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:21.804782: step 88500, loss = 0.68 (1389.8 examples/sec; 0.092 sec/batch)
2017-05-06 22:59:22.538796: step 88510, loss = 0.69 (1743.8 examples/sec; 0.073 sec/batch)
2017-05-06 22:59:23.352501: step 88520, loss = 0.73 (1573.0 examples/sec; 0.081 sec/batch)
2017-05-06 22:59:24.163109: step 88530, loss = 0.74 (1579.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:59:24.982276: step 88540, loss = 0.67 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:25.800969: step 88550, loss = 0.59 (1563.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:26.627089: step 88560, loss = 0.73 (1549.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:59:27.447417: step 88570, loss = 0.76 (1560.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:28.259757: step 88580, loss = 0.55 (1575.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:59:29.088742: step 88590, loss = 0.68 (1544.1 examples/sec; 0.083 sec/batch)
2017-05-06 22:59:30.008356: step 88600, loss = 0.75 (1391.9 examples/sec; 0.092 sec/batch)
2017-05-06 22:59:30.726861: step 88610, loss = 0.65 (1781.5 examples/sec; 0.072 sec/batch)
2017-05-06 22:59:31.547621: step 88620, loss = 0.76 (1559.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:32.360945: step 88630, loss = 0.73 (1573.8 examples/sec; 0.081 sec/batch)
2017-05-06 22:59:33.178940: step 88640, loss = 0.86 (1564.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:33.994954: step 88650, loss = 0.64 (1568.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:34.808837: step 88660, loss = 0.68 (1572.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:59:35.622187: step 88670, loss = 0.62 (1573.7 examples/sec; 0.081 sec/batch)
2017-05-06 22:59:36.444250: step 88680, loss = 0.74 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:37.265999: step 88690, loss = 0.68 (1557.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:38.185634: step 88700, loss = 0.76 (1391.9 examples/sec; 0.092 sec/batch)
2017-05-06 22:59:38.909951: step 88710, loss = 0.62 (1767.2 examples/sec; 0.072 sec/batch)
2017-05-06 22:59:39.723641: step 88720, loss = 0.83 (1573.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:59:40.540561: step 88730, loss = 0.73 (1566.9 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:41.371030: step 88740, loss = 0.76 (1541.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:59:42.196105: step 88750, loss = 0.66 (1551.4 examples/sec; 0.083 sec/batch)
2017-05-06 22:59:43.016617: step 88760, loss = 0.69 (1560.0 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:43.831630: step 88770, loss = 0.71 (1570.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:44.650684: step 88780, loss = 0.79 (1562.8 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:45.468743: step 88790, loss = 0.83 (1564.7 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:46.386074: step 88800, loss = 0.92 (1395.4 examples/sec; 0.092 sec/batch)
2017-05-06 22:59:47.104453: step 88810, loss = 0.69 (1781.7 examples/sec; 0.072 sec/batch)
2017-05-06 22:59:47.922556: step 88820, loss = 0.64 (1564.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:48.744844: step 88830, loss = 0.69 (1556.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:49.563518: step 88840, loss = 0.81 (1563.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:50.384396: step 88850, loss = 0.70 (1559.3 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:51.205733: step 88860, loss = 0.76 (1558.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:52.013266: step 88870, loss = 0.95 (1585.1 examples/sec; 0.081 sec/batch)
2017-05-06 22:59:52.837198: step 88880, loss = 0.69 (1553.5 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:53.661626: step 88890, loss = 0.71 (1552.6 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:54.585787: step 88900, loss = 0.63 (1385.0 examples/sec; 0.092 sec/batch)
2017-05-06 22:59:55.309388: step 88910, loss = 0.55 (1768.9 examples/sec; 0.072 sec/batch)
2017-05-06 22:59:56.121807: step 88920, loss = 0.74 (1575.5 examples/sec; 0.081 sec/batch)
2017-05-06 22:59:56.948877: step 88930, loss = 0.64 (1547.7 examples/sec; 0.083 sec/batch)
2017-05-06 22:59:57.769155: step 88940, loss = 0.75 (1560.4 examples/sec; 0.082 sec/batch)
2017-05-06 22:59:58.598017: step 88950, loss = 0.64 (1544.3 examples/sec; 0.083 sec/batch)
2017-05-06 22:59:59.423897: step 88960, loss = 0.81 (1549.9 examples/sec; 0.083 sec/batch)
2017-05-06 23:00:00.247730: step 88970, loss = 0.70 (1553.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:01.069401: step 88980, loss = 0.67 (1557.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:01.893803: step 88990, loss = 0.88 (1552.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:02.817271: step 89000, loss = 0.71 (1386.1 examples/sec; 0.092 sec/batch)
2017-05-06 23:00:03.559513: step 89010, loss = 0.74 (1724.5 examples/sec; 0.074 sec/batch)
2017-05-06 23:00:04.384430: step 89020, loss = 0.76 (1551.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:05.206163: step 89030, loss = 0.80 (1557.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:06.033132: step 89040, loss = 0.69 (1547.8 examples/sec; 0.083 sec/batch)
2017-05-06 23:00:06.863273: step 89050, loss = 0.57 (1541.9 examples/sec; 0.083 sec/batch)
2017-05-06 23:00:07.688091: step 89060, loss = 0.78 (1551.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:08.511066: step 89070, loss = 0.61 (1555.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:09.331248: step 89080, loss = 0.76 (1560.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:10.156495: step 89090, loss = 0.68 (1551.0 examples/sec; 0.083 sec/batch)
2017-05-06 23:00:11.111023: step 89100, loss = 0.69 (1341.0 examples/sec; 0.095 sec/batch)
2017-05-06 23:00:11.804745: step 89110, loss = 0.54 (1845.1 examples/sec; 0.069 sec/batch)
2017-05-06 23:00:12.626548: step 89120, loss = 0.75 (1557.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:13.452885: step 89130, loss = 0.68 (1549.0 examples/sec; 0.083 sec/batch)
2017-05-06 23:00:14.277727: step 89140, loss = 0.60 (1551.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:15.091917: step 89150, loss = 0.65 (1572.1 examples/sec; 0.081 sec/batch)
2017-05-06 23:00:15.912877: step 89160, loss = 0.67 (1559.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:16.729327: step 89170, loss = 0.68 (1567.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:17.555351: step 89180, loss = 0.83 (1549.6 examples/sec; 0.083 sec/batch)
2017-05-06 23:00:18.383180: step 89190, loss = 0.55 (1546.2 examples/sec; 0.083 sec/batch)
2017-05-06 23:00:19.309427: step 89200, loss = 0.67 (1381.9 examples/sec; 0.093 sec/batch)
2017-05-06 23:00:20.033476: step 89210, loss = 0.65 (1767.8 examples/sec; 0.072 sec/batch)
2017-05-06 23:00:20.850850: step 89220, loss = 0.74 (1566.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:21.669286: step 89230, loss = 0.66 (1564.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:22.493478: step 89240, loss = 0.77 (1553.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:23.313911: step 89250, loss = 0.74 (1560.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:24.132835: step 89260, loss = 0.71 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:24.947894: step 89270, loss = 0.71 (1570.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:25.763424: step 89280, loss = 0.71 (1569.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:26.589525: step 89290, loss = 0.72 (1549.4 examples/sec; 0.083 sec/batch)
2017-05-06 23:00:27.504354: step 89300, loss = 0.78 (1399.2 examples/sec; 0.091 sec/batch)
2017-05-06 23:00:28.219910: step 89310, loss = 0.76 (1788.8 examples/sec; 0.072 sec/batch)
2017-05-06 23:00:29.040396: step 89320, loss = 0.68 (1560.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:29.865792: step 89330, loss = 0.58 (1550.8 examples/sec; 0.083 sec/batch)
2017-05-06 23:00:30.691083: step 89340, loss = 0.60 (1551.0 examples/sec; 0.083 sec/batch)
2017-05-06 23:00:31.514893: step 89350, loss = 0.71 (1553.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:32.329787: step 89360, loss = 0.63 (1570.8 examples/sec; 0.081 sec/batch)
2017-05-06 23:00:33.147485: step 89370, loss = 0.82 (1565.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:33.969518: step 89380, loss = 0.65 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:34.794217: step 89390, loss = 0.72 (1552.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:35.717626: step 89400, loss = 0.87 (1386.2 examples/sec; 0.092 sec/batch)
2017-05-06 23:00:36.435782: step 89410, loss = 0.80 (1782.4 examples/sec; 0.072 sec/batch)
2017-05-06 23:00:37.259214: step 89420, loss = 0.67 (1554.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:38.077507: step 89430, loss = 0.66 (1564.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:38.893890: step 89440, loss = 0.75 (1567.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:39.705449: step 89450, loss = 0.72 (1577.2 examples/sec; 0.081 sec/batch)
2017-05-06 23:00:40.519725: step 89460, loss = 0.63 (1572.0 examples/sec; 0.081 sec/batch)
2017-05-06 23:00:41.341655: step 89470, loss = 0.85 (1557.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:42.162592: step 89480, loss = 0.56 (1559.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:42.982788: step 89490, loss = 0.59 (1560.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:43.896339: step 89500, loss = 0.64 (1401.1 examples/sec; 0.091 sec/batch)
2017-05-06 23:00:44.613694: step 89510, loss = 0.69 (1784.3 examples/sec; 0.072 sec/batch)
2017-05-06 23:00:45.437172: step 89520, loss = 0.67 (1554.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:46.258259: step 89530, loss = 0.75 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:47.081160: step 89540, loss = 0.67 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:47.891895: step 89550, loss = 0.83 (1578.8 examples/sec; 0.081 sec/batch)
2017-05-06 23:00:48.717685: step 89560, loss = 0.68 (1550.0 examples/sec; 0.083 sec/batch)
2017-05-06 23:00:49.538464: step 89570, loss = 0.64 (1559.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:50.361403: step 89580, loss = 0.68 (1555.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:51.183636: step 89590, loss = 0.81 (1556.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:52.106217: step 89600, loss = 0.91 (1387.4 examples/sec; 0.092 sec/batch)
2017-05-06 23:00:52.819935: step 89610, loss = 0.77 (1793.4 examples/sec; 0.071 sec/batch)
2017-05-06 23:00:53.640913: step 89620, loss = 0.62 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:54.465842: step 89630, loss = 0.66 (1551.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:55.275358: step 89640, loss = 0.63 (1581.2 examples/sec; 0.081 sec/batch)
2017-05-06 23:00:56.091873: step 89650, loss = 0.74 (1567.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:56.913733: step 89660, loss = 0.79 (1557.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:57.734263: step 89670, loss = 0.68 (1560.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:58.556924: step 89680, loss = 0.79 (1555.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:00:59.384936: step 89690, loss = 0.61 (1545.9 examples/sec; 0.083 sec/batch)
2017-05-06 23:01:00.292993: step 89700, loss = 0.63 (1409.6 examples/sec; 0.091 sec/batch)
2017-05-06 23:01:01.011310: step 89710, loss = 0.78 (1781.9 examples/sec; 0.072 sec/batch)
2017-05-06 23:01:01.842852: step 89720, loss = 0.66 (1539.3 examples/sec; 0.083 sec/batch)
2017-05-06 23:01:02.663816: step 89730, loss = 0.77 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:03.476444: step 89740, loss = 0.77 (1575.1 examples/sec; 0.081 sec/batch)
2017-05-06 23:01:04.296963: step 89750, loss = 0.73 (1560.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:05.109104: step 89760, loss = 0.70 (1576.1 examples/sec; 0.081 sec/batch)
2017-05-06 23:01:05.934612: step 89770, loss = 0.78 (1550.6 examples/sec; 0.083 sec/batch)
2017-05-06 23:01:06.751345: step 89780, loss = 0.70 (1567.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:07.562702: step 89790, loss = 0.70 (1577.6 examples/sec; 0.081 sec/batch)
2017-05-06 23:01:08.486034: step 89800, loss = 0.87 (1386.3 examples/sec; 0.092 sec/batch)
2017-05-06 23:01:09.198574: step 89810, loss = 0.76 (1796.4 examples/sec; 0.071 sec/batch)
2017-05-06 23:01:10.016304: step 89820, loss = 0.77 (1565.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:10.839422: step 89830, loss = 0.82 (1555.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:11.659099: step 89840, loss = 0.68 (1561.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:12.472388: step 89850, loss = 0.77 (1573.9 examples/sec; 0.081 sec/batch)
2017-05-06 23:01:13.289306: step 89860, loss = 0.64 (1566.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:14.112371: step 89870, loss = 0.69 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:14.927173: step 89880, loss = 0.70 (1570.9 examples/sec; 0.081 sec/batch)
2017-05-06 23:01:15.746511: step 89890, loss = 0.74 (1562.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:16.678302: step 89900, loss = 0.71 (1373.7 examples/sec; 0.093 sec/batch)
2017-05-06 23:01:17.399608: step 89910, loss = 0.62 (1774.6 examples/sec; 0.072 sec/batch)
2017-05-06 23:01:18.216034: step 89920, loss = 0.76 (1567.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:19.039358: step 89930, loss = 0.72 (1554.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:19.860698: step 89940, loss = 0.85 (1558.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:20.680277: step 89950, loss = 0.63 (1561.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:21.495428: step 89960, loss = 0.78 (1570.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:22.315733: step 89970, loss = 0.69 (1560.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:23.136365: step 89980, loss = 0.82 (1559.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:23.951354: step 89990, loss = 0.86 (1570.6 examples/sec; 0.081 sec/batch)
2017-05-06 23:01:24.872794: step 90000, loss = 0.67 (1389.1 examples/sec; 0.092 sec/batch)
2017-05-06 23:01:25.599970: step 90010, loss = 0.71 (1760.2 examples/sec; 0.073 sec/batch)
2017-05-06 23:01:26.414894: step 90020, loss = 0.70 (1570.7 examples/sec; 0.081 sec/batch)
2017-05-06 23:01:27.228348: step 90030, loss = 0.75 (1573.5 examples/sec; 0.081 sec/batch)
2017-05-06 23:01:28.042260: step 90040, loss = 0.69 (1572.7 examples/sec; 0.081 sec/batch)
2017-05-06 23:01:28.865833: step 90050, loss = 0.82 (1554.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:29.684364: step 90060, loss = 0.60 (1563.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:30.499069: step 90070, loss = 0.64 (1571.1 examples/sec; 0.081 sec/batch)
2017-05-06 23:01:31.314828: step 90080, loss = 0.64 (1569.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:32.148097: step 90090, loss = 0.67 (1536.1 examples/sec; 0.083 sec/batch)
2017-05-06 23:01:33.046600: step 90100, loss = 0.71 (1424.6 examples/sec; 0.090 sec/batch)
2017-05-06 23:01:33.769262: step 90110, loss = 0.71 (1771.2 examples/sec; 0.072 sec/batch)
2017-05-06 23:01:34.595475: step 90120, loss = 0.54 (1549.2 examples/sec; 0.083 sec/batch)
2017-05-06 23:01:35.415518: step 90130, loss = 0.65 (1560.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:36.229069: step 90140, loss = 0.65 (1573.3 examples/sec; 0.081 sec/batch)
2017-05-06 23:01:37.046984: step 90150, loss = 0.59 (1565.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:37.865862: step 90160, loss = 0.79 (1563.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:38.691079: step 90170, loss = 0.76 (1551.1 examples/sec; 0.083 sec/batch)
2017-05-06 23:01:39.510145: step 90180, loss = 0.58 (1562.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:40.328607: step 90190, loss = 0.65 (1563.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:41.238455: step 90200, loss = 0.64 (1406.8 examples/sec; 0.091 sec/batch)
2017-05-06 23:01:41.969814: step 90210, loss = 0.84 (1750.2 examples/sec; 0.073 sec/batch)
2017-05-06 23:01:42.796552: step 90220, loss = 0.63 (1548.3 examples/sec; 0.083 sec/batch)
2017-05-06 23:01:43.602883: step 90230, loss = 0.56 (1587.4 examples/sec; 0.081 sec/batch)
2017-05-06 23:01:44.430796: step 90240, loss = 0.83 (1546.1 examples/sec; 0.083 sec/batch)
2017-05-06 23:01:45.249453: step 90250, loss = 1.01 (1563.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:46.065273: step 90260, loss = 0.64 (1569.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:46.886476: step 90270, loss = 0.78 (1558.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:47.700631: step 90280, loss = 0.76 (1572.2 examples/sec; 0.081 sec/batch)
2017-05-06 23:01:48.521934: step 90290, loss = 0.66 (1558.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:49.447424: step 90300, loss = 0.67 (1383.1 examples/sec; 0.093 sec/batch)
2017-05-06 23:01:50.165382: step 90310, loss = 0.68 (1782.8 examples/sec; 0.072 sec/batch)
2017-05-06 23:01:50.989204: step 90320, loss = 0.64 (1553.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:51.802854: step 90330, loss = 0.69 (1573.1 examples/sec; 0.081 sec/batch)
2017-05-06 23:01:52.623629: step 90340, loss = 0.97 (1559.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:53.448262: step 90350, loss = 0.68 (1552.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:54.268567: step 90360, loss = 0.72 (1560.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:55.092549: step 90370, loss = 0.60 (1553.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:55.903489: step 90380, loss = 0.71 (1578.4 examples/sec; 0.081 sec/batch)
2017-05-06 23:01:56.724829: step 90390, loss = 0.64 (1558.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:01:57.649461: step 90400, loss = 0.68 (1384.3 examples/sec; 0.092 sec/batch)
2017-05-06 23:01:58.373344: step 90410, loss = 0.67 (1768.3 examples/sec; 0.072 sec/batch)
2017-05-06 23:01:59.192394: step 90420, loss = 0.79 (1562.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:02:00.008826: step 90430, loss = 0.64 (1567.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:02:00.826015: step 90440, loss = 0.72 (1566.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:02:01.637782: step 90450, loss = 0.74 (1576.8 examples/sec; 0.081 sec/batch)
2017-05-06 23:02:02.456629: step 90460, loss = 0.82 (1563.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:02:03.276641: step 90470, loss = 0.75 (1560.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:02:04.089616: step 90480, loss = 0.75 (1574.5 examples/sec; 0.081 sec/batch)
2017-05-06 23:02:04.918840: step 90490, loss = 0.90 (1543.6 examples/sec; 0.083 sec/batch)
2017-05-06 23:02:05.834343: step 90500, loss = 0.73 (1398.1 examples/sec; 0.092 sec/batch)
2017-05-06 23:02:06.563310: step 90510, loss = 0.67 (1755.9 examples/sec; 0.073 sec/batch)
2017-05-06 23:02:07.379939: step 90520, loss = 0.55 (1567.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:02:08.189918: step 90530, loss = 0.66 (1580.3 examples/sec; 0.081 sec/batch)
2017-05-06 23:02:09.003013: step 90540, loss = 0.64 (1574.2 examples/sec; 0.081 sec/batch)
2017-05-06 23:02:09.819100: step 90550, loss = 0.67 (1568.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:02:10.643182: step 90560, loss = 0.62 (1553.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:02:11.459497: step 90570, loss = 0.71 (1568.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:02:12.277605: step 90580, loss = 0.69 (1564.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:02:13.094595: step 90590, loss = 0.71 (1566.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:02:14.042933: step 90600, loss = 0.79 (1349.7 examples/sec; 0.095 sec/batch)
2017-05-06 23:02:14.758549: step 90610, loss = 0.62 (1788.7 examples/sec; 0.072 sec/batch)
2017-05-06 23:02:15.591067: step 90620, loss = 0.72 (1537.5 examples/sec; 0.083 sec/batch)
2017-05-06 23:02:16.420091: step 90630, loss = 0.58 (1544.0 examples/sec; 0.083 sec/batch)
2017-05-06 23:02:17.248595: step 90640, loss = 0.67 (1545.0 examples/sec; 0.083 sec/batch)
2017-05-06 23:02:18.076495: step 90650, loss = 0.70 (1546.1 examples/sec; 0.083 sec/batch)
2017-05-06 23:02:18.905241: step 90660, loss = 0.66 (1544.5 examples/sec; 0.083 sec/batch)
2017-05-06 23:02:19.713028: step 90670, loss = 0.74 (1584.6 examples/sec; 0.081 sec/batch)
2017-05-06 23:02:20.540949: step 90680, loss = 0.69 (1546.0 examples/sec; 0.083 sec/batch)
2017-05-06 23:02:21.367797: step 90690, loss = 0.68 (1548.0 examples/sec; 0.083 sec/batch)
2017-05-06 23:02:22.282546: step 90700, loss = 0.67 (1399.3 examples/sec; 0.091 sec/batch)
2017-05-06 23:02:23.007354: step 90710, loss = 0.68 (1766.0 examples/sec; 0.072 sec/batch)
2017-05-06 23:02:23.823444: step 90720, loss = 0.56 (1568.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:02:24.642405: step 90730, loss = 0.73 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:02:25.466176: step 90740, loss = 0.60 (1553.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:02:26.286429: step 90750, loss = 0.77 (1560.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:02:27.102474: step 90760, loss = 0.76 (1568.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:02:27.912764: step 90770, loss = 0.71 (1579.7 examples/sec; 0.081 sec/batch)
2017-05-06 23:02:28.734661: step 90780, loss = 0.73 (1557.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:02:29.562612: step 90790, loss = 0.68 (1546.0 examples/sec; 0.083 sec/batch)
2017-05-06 23:02:30.474150: step 90800, loss = 0.86 (1404.2 examples/sec; 0.091 sec/batch)
2017-05-06 23:02:31.199069: step 90810, loss = 0.70 (1765.7 examples/sec; 0.072 sec/batch)
2017-05-06 23:02:32.021316: step 90820, loss = 0.71 (1556.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:02:32.839014: step 90830, loss = 0.70 (1565.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:02:33.652131: step 90840, loss = 0.71 (1574.2 examples/sec; 0.081 sec/batch)
2017-05-06 23:02:34.473344: step 90850, loss = 0.96 (1558.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:02:35.286632: step 90860, loss = 0.83 (1573.9 examples/sec; 0.081 sec/batch)
2017-05-06 23:02:36.114105: step 90870, loss = 0.65 (1546.9 examples/sec; 0.083 sec/batch)
2017-05-06 23:02:36.925493: step 90880, loss = 0.71 (1577.5 examples/sec; 0.081 sec/batch)
2017-05-06 23:02:37.755132: step 90890, loss = 0.80 (1542.8 examples/sec; 0.083 sec/batch)
2017-05-06 23:02:38.678090: step 90900, loss = 0.66 (1386.8 examples/sec; 0.092 sec/batch)
2017-05-06 23:02:39.401441: step 90910, loss = 0.57 (1769.5 examples/sec; 0.072 sec/batch)
2017-05-06 23:02:40.212273: step 90920, loss = 0.76 (1578.6 examples/sec; 0.081 sec/batch)
2017-05-06 23:02:41.025807: step 90930, loss = 0.76 (1573.4 examples/sec; 0.081 sec/batch)
2017-05-06 23:02:41.848540: step 90940, loss = 0.63 (1555.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:02:42.665342: step 90950, loss = 0.85 (1567.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:02:43.483010: step 90960, loss = 0.76 (1565.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:02:44.298251: step 90970, loss = 0.74 (1570.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:02:45.119196: step 90980, loss = 0.66 (1559.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:02:45.938554: step 90990, loss = 0.58 (1562.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:02:46.878296: step 91000, loss = 0.71 (1362.1 examples/sec; 0.094 sec/batch)
2017-05-06 23:02:47.572109: step 91010, loss = 0.78 (1844.9 examples/sec; 0.069 sec/batch)
2017-05-06 23:02:48.388686: step 91020, loss = 0.67 (1567.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:02:49.213775: step 91030, loss = 0.74 (1551.3 examples/sec; 0.083 sec/batch)
2017-05-06 23:02:50.028094: step 91040, loss = 0.69 (1571.9 examples/sec; 0.081 sec/batch)
2017-05-06 23:02:50.845457: step 91050, loss = 0.83 (1566.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:02:51.653488: step 91060, loss = 0.69 (1584.1 examples/sec; 0.081 sec/batch)
2017-05-06 23:02:52.476646: step 91070, loss = 0.59 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:02:53.323540: step 91080, loss = 0.77 (1511.4 examples/sec; 0.085 sec/batch)
2017-05-06 23:02:54.117016: step 91090, loss = 0.74 (1613.2 examples/sec; 0.079 sec/batch)
2017-05-06 23:02:55.045875: step 91100, loss = 0.79 (1378.0 examples/sec; 0.093 sec/batch)
2017-05-06 23:02:55.764473: step 91110, loss = 0.85 (1781.3 examples/sec; 0.072 sec/batch)
2017-05-06 23:02:56.593462: step 91120, loss = 0.69 (1544.0 examples/sec; 0.083 sec/batch)
2017-05-06 23:02:57.420099: step 91130, loss = 0.91 (1548.4 examples/sec; 0.083 sec/batch)
2017-05-06 23:02:58.233406: step 91140, loss = 0.64 (1573.8 examples/sec; 0.081 sec/batch)
2017-05-06 23:02:59.051641: step 91150, loss = 0.83 (1564.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:02:59.856839: step 91160, loss = 0.69 (1589.7 examples/sec; 0.081 sec/batch)
2017-05-06 23:03:00.676450: step 91170, loss = 0.73 (1561.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:01.507759: step 91180, loss = 0.61 (1539.7 examples/sec; 0.083 sec/batch)
2017-05-06 23:03:02.336888: step 91190, loss = 0.62 (1543.8 examples/sec; 0.083 sec/batch)
2017-05-06 23:03:03.261569: step 91200, loss = 0.79 (1384.3 examples/sec; 0.092 sec/batch)
2017-05-06 23:03:03.971248: step 91210, loss = 0.62 (1803.7 examples/sec; 0.071 sec/batch)
2017-05-06 23:03:04.792615: step 91220, loss = 0.65 (1558.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:05.612280: step 91230, loss = 0.78 (1561.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:06.436027: step 91240, loss = 0.68 (1553.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:07.257001: step 91250, loss = 0.69 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:08.072072: step 91260, loss = 0.63 (1570.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:08.883749: step 91270, loss = 0.65 (1577.0 examples/sec; 0.081 sec/batch)
2017-05-06 23:03:09.713228: step 91280, loss = 0.69 (1543.1 examples/sec; 0.083 sec/batch)
2017-05-06 23:03:10.534562: step 91290, loss = 0.66 (1558.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:11.451940: step 91300, loss = 0.80 (1395.3 examples/sec; 0.092 sec/batch)
2017-05-06 23:03:12.171223: step 91310, loss = 0.68 (1779.5 examples/sec; 0.072 sec/batch)
2017-05-06 23:03:12.994586: step 91320, loss = 0.87 (1554.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:13.820777: step 91330, loss = 0.88 (1549.3 examples/sec; 0.083 sec/batch)
2017-05-06 23:03:14.636460: step 91340, loss = 0.85 (1569.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:15.449850: step 91350, loss = 0.66 (1573.7 examples/sec; 0.081 sec/batch)
2017-05-06 23:03:16.266968: step 91360, loss = 0.66 (1566.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:17.079977: step 91370, loss = 0.72 (1574.4 examples/sec; 0.081 sec/batch)
2017-05-06 23:03:17.908211: step 91380, loss = 0.86 (1545.4 examples/sec; 0.083 sec/batch)
2017-05-06 23:03:18.741012: step 91390, loss = 0.67 (1537.0 examples/sec; 0.083 sec/batch)
2017-05-06 23:03:19.647093: step 91400, loss = 0.72 (1412.7 examples/sec; 0.091 sec/batch)
2017-05-06 23:03:20.376974: step 91410, loss = 0.63 (1753.7 examples/sec; 0.073 sec/batch)
2017-05-06 23:03:21.202950: step 91420, loss = 0.67 (1549.7 examples/sec; 0.083 sec/batch)
2017-05-06 23:03:22.016091: step 91430, loss = 0.63 (1574.1 examples/sec; 0.081 sec/batch)
2017-05-06 23:03:22.828382: step 91440, loss = 0.60 (1575.8 examples/sec; 0.081 sec/batch)
2017-05-06 23:03:23.644453: step 91450, loss = 0.71 (1568.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:24.462781: step 91460, loss = 0.84 (1564.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:25.283531: step 91470, loss = 0.82 (1559.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:26.102430: step 91480, loss = 0.85 (1563.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:26.923248: step 91490, loss = 0.78 (1559.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:27.831337: step 91500, loss = 0.68 (1409.5 examples/sec; 0.091 sec/batch)
2017-05-06 23:03:28.554202: step 91510, loss = 0.69 (1770.7 examples/sec; 0.072 sec/batch)
2017-05-06 23:03:29.375358: step 91520, loss = 0.60 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:30.196838: step 91530, loss = 0.57 (1558.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:31.016309: step 91540, loss = 0.84 (1562.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:31.833841: step 91550, loss = 0.75 (1565.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:32.655174: step 91560, loss = 0.69 (1558.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:33.464511: step 91570, loss = 0.88 (1581.5 examples/sec; 0.081 sec/batch)
2017-05-06 23:03:34.287780: step 91580, loss = 0.68 (1554.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:35.106418: step 91590, loss = 0.72 (1563.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:36.006210: step 91600, loss = 0.72 (1422.5 examples/sec; 0.090 sec/batch)
2017-05-06 23:03:36.735571: step 91610, loss = 0.68 (1755.0 examples/sec; 0.073 sec/batch)
2017-05-06 23:03:37.553360: step 91620, loss = 0.72 (1565.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:38.369205: step 91630, loss = 0.75 (1568.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:39.183738: step 91640, loss = 0.72 (1571.5 examples/sec; 0.081 sec/batch)
2017-05-06 23:03:39.995141: step 91650, loss = 0.75 (1577.5 examples/sec; 0.081 sec/batch)
2017-05-06 23:03:40.816266: step 91660, loss = 0.67 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:41.636474: step 91670, loss = 0.71 (1560.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:42.460159: step 91680, loss = 0.68 (1554.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:43.277290: step 91690, loss = 0.76 (1566.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:44.186590: step 91700, loss = 0.65 (1407.7 examples/sec; 0.091 sec/batch)
2017-05-06 23:03:44.911890: step 91710, loss = 0.71 (1764.8 examples/sec; 0.073 sec/batch)
2017-05-06 23:03:45.740838: step 91720, loss = 0.78 (1544.1 examples/sec; 0.083 sec/batch)
2017-05-06 23:03:46.558886: step 91730, loss = 0.68 (1564.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:47.373923: step 91740, loss = 0.73 (1570.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:48.190212: step 91750, loss = 0.77 (1568.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:49.011066: step 91760, loss = 0.69 (1559.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:49.829555: step 91770, loss = 0.61 (1563.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:50.652694: step 91780, loss = 0.67 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:51.470221: step 91790, loss = 0.64 (1565.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:52.394714: step 91800, loss = 0.71 (1384.5 examples/sec; 0.092 sec/batch)
2017-05-06 23:03:53.111853: step 91810, loss = 0.74 (1784.9 examples/sec; 0.072 sec/batch)
2017-05-06 23:03:53.931136: step 91820, loss = 0.69 (1562.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:54.744464: step 91830, loss = 0.86 (1573.8 examples/sec; 0.081 sec/batch)
2017-05-06 23:03:55.560490: step 91840, loss = 0.72 (1568.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:56.376726: step 91850, loss = 0.62 (1568.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:57.198214: step 91860, loss = 0.71 (1558.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:58.016811: step 91870, loss = 0.62 (1563.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:03:58.847999: step 91880, loss = 0.68 (1540.0 examples/sec; 0.083 sec/batch)
2017-05-06 23:03:59.657271: step 91890, loss = 0.68 (1581.7 examples/sec; 0.081 sec/batch)
2017-05-06 23:04:00.579882: step 91900, loss = 0.58 (1387.4 examples/sec; 0.092 sec/batch)
2017-05-06 23:04:01.305003: step 91910, loss = 0.60 (1765.2 examples/sec; 0.073 sec/batch)
2017-05-06 23:04:02.126343: step 91920, loss = 0.67 (1558.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:04:02.949514: step 91930, loss = 0.64 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:04:03.766828: step 91940, loss = 0.58 (1566.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:04:04.584673: step 91950, loss = 0.70 (1565.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:04:05.402427: step 91960, loss = 0.73 (1565.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:04:06.218478: step 91970, loss = 0.76 (1568.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:04:07.037856: step 91980, loss = 0.77 (1562.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:04:07.842376: step 91990, loss = 0.61 (1591.0 examples/sec; 0.080 sec/batch)
2017-05-06 23:04:08.765305: step 92000, loss = 0.61 (1386.9 examples/sec; 0.092 sec/batch)
2017-05-06 23:04:09.486183: step 92010, loss = 0.63 (1775.6 examples/sec; 0.072 sec/batch)
2017-05-06 23:04:10.318434: step 92020, loss = 0.62 (1538.0 examples/sec; 0.083 sec/batch)
2017-05-06 23:04:11.143814: step 92030, loss = 0.85 (1550.8 examples/sec; 0.083 sec/batch)
2017-05-06 23:04:11.953404: step 92040, loss = 0.69 (1581.1 examples/sec; 0.081 sec/batch)
2017-05-06 23:04:12.780560: step 92050, loss = 0.65 (1547.4 examples/sec; 0.083 sec/batch)
2017-05-06 23:04:13.598811: step 92060, loss = 0.71 (1564.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:04:14.458759: step 92070, loss = 0.79 (1488.4 examples/sec; 0.086 sec/batch)
2017-05-06 23:04:15.237460: step 92080, loss = 0.70 (1643.8 examples/sec; 0.078 sec/batch)
2017-05-06 23:04:16.050800: step 92090, loss = 0.77 (1573.8 examples/sec; 0.081 sec/batch)
2017-05-06 23:04:16.962584: step 92100, loss = 0.68 (1403.8 examples/sec; 0.091 sec/batch)
2017-05-06 23:04:17.690008: step 92110, loss = 0.62 (1759.6 examples/sec; 0.073 sec/batch)
2017-05-06 23:04:18.520815: step 92120, loss = 0.80 (1540.7 examples/sec; 0.083 sec/batch)
2017-05-06 23:04:19.336115: step 92130, loss = 0.62 (1570.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:04:20.160956: step 92140, loss = 0.83 (1551.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:04:20.986022: step 92150, loss = 0.69 (1551.4 examples/sec; 0.083 sec/batch)
2017-05-06 23:04:21.806096: step 92160, loss = 0.66 (1560.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:04:22.633838: step 92170, loss = 0.69 (1546.4 examples/sec; 0.083 sec/batch)
2017-05-06 23:04:23.447290: step 92180, loss = 0.65 (1573.5 examples/sec; 0.081 sec/batch)
2017-05-06 23:04:24.261967: step 92190, loss = 0.67 (1571.2 examples/sec; 0.081 sec/batch)
2017-05-06 23:04:25.182332: step 92200, loss = 0.62 (1390.8 examples/sec; 0.092 sec/batch)
2017-05-06 23:04:25.886988: step 92210, loss = 0.69 (1816.5 examples/sec; 0.070 sec/batch)
2017-05-06 23:04:26.708501: step 92220, loss = 0.81 (1558.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:04:27.521164: step 92230, loss = 0.85 (1575.1 examples/sec; 0.081 sec/batch)
2017-05-06 23:04:28.332039: step 92240, loss = 0.72 (1578.5 examples/sec; 0.081 sec/batch)
2017-05-06 23:04:29.149890: step 92250, loss = 0.67 (1565.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:04:29.973544: step 92260, loss = 0.76 (1554.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:04:30.800644: step 92270, loss = 0.76 (1547.6 examples/sec; 0.083 sec/batch)
2017-05-06 23:04:31.609705: step 92280, loss = 0.65 (1582.1 examples/sec; 0.081 sec/batch)
2017-05-06 23:04:32.428093: step 92290, loss = 0.66 (1564.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:04:33.344061: step 92300, loss = 0.65 (1397.4 examples/sec; 0.092 sec/batch)
2017-05-06 23:04:34.068021: step 92310, loss = 0.61 (1768.0 examples/sec; 0.072 sec/batch)
2017-05-06 23:04:34.894278: step 92320, loss = 0.69 (1549.2 examples/sec; 0.083 sec/batch)
2017-05-06 23:04:35.707566: step 92330, loss = 0.63 (1573.9 examples/sec; 0.081 sec/batch)
2017-05-06 23:04:36.532423: step 92340, loss = 0.54 (1551.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:04:37.350565: step 92350, loss = 0.77 (1564.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:04:38.175345: step 92360, loss = 0.75 (1551.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:04:38.993820: step 92370, loss = 0.87 (1563.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:04:39.812734: step 92380, loss = 0.72 (1563.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:04:40.628355: step 92390, loss = 0.64 (1569.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:04:41.547112: step 92400, loss = 0.66 (1393.2 examples/sec; 0.092 sec/batch)
2017-05-06 23:04:42.265950: step 92410, loss = 0.78 (1780.7 examples/sec; 0.072 sec/batch)
2017-05-06 23:04:43.084797: step 92420, loss = 0.68 (1563.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:04:43.900606: step 92430, loss = 0.76 (1569.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:04:44.705832: step 92440, loss = 0.77 (1589.6 examples/sec; 0.081 sec/batch)
2017-05-06 23:04:45.521726: step 92450, loss = 0.80 (1568.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:04:46.347061: step 92460, loss = 0.70 (1550.9 examples/sec; 0.083 sec/batch)
2017-05-06 23:04:47.169903: step 92470, loss = 0.80 (1555.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:04:47.979559: step 92480, loss = 0.73 (1580.9 examples/sec; 0.081 sec/batch)
2017-05-06 23:04:48.795497: step 92490, loss = 0.87 (1568.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:04:49.719019: step 92500, loss = 0.83 (1386.0 examples/sec; 0.092 sec/batch)
2017-05-06 23:04:50.441295: step 92510, loss = 0.84 (1772.2 examples/sec; 0.072 sec/batch)
2017-05-06 23:04:51.252967: step 92520, loss = 0.73 (1577.0 examples/sec; 0.081 sec/batch)
2017-05-06 23:04:52.068524: step 92530, loss = 0.84 (1569.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:04:52.888960: step 92540, loss = 0.87 (1560.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:04:53.712937: step 92550, loss = 0.65 (1553.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:04:54.541214: step 92560, loss = 0.62 (1545.4 examples/sec; 0.083 sec/batch)
2017-05-06 23:04:55.360711: step 92570, loss = 0.81 (1561.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:04:56.175646: step 92580, loss = 0.80 (1570.7 examples/sec; 0.081 sec/batch)
2017-05-06 23:04:56.996693: step 92590, loss = 0.70 (1559.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:04:57.920257: step 92600, loss = 0.68 (1385.9 examples/sec; 0.092 sec/batch)
2017-05-06 23:04:58.640806: step 92610, loss = 0.77 (1776.4 examples/sec; 0.072 sec/batch)
2017-05-06 23:04:59.461324: step 92620, loss = 0.62 (1560.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:00.273262: step 92630, loss = 0.74 (1576.5 examples/sec; 0.081 sec/batch)
2017-05-06 23:05:01.089234: step 92640, loss = 0.72 (1568.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:01.919271: step 92650, loss = 0.79 (1542.1 examples/sec; 0.083 sec/batch)
2017-05-06 23:05:02.742497: step 92660, loss = 0.64 (1554.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:03.557956: step 92670, loss = 0.64 (1569.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:04.373627: step 92680, loss = 0.78 (1569.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:05.190143: step 92690, loss = 0.63 (1567.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:06.111546: step 92700, loss = 0.70 (1389.2 examples/sec; 0.092 sec/batch)
2017-05-06 23:05:06.841196: step 92710, loss = 0.65 (1754.3 examples/sec; 0.073 sec/batch)
2017-05-06 23:05:07.666236: step 92720, loss = 0.64 (1551.4 examples/sec; 0.083 sec/batch)
2017-05-06 23:05:08.483473: step 92730, loss = 0.69 (1566.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:09.314247: step 92740, loss = 0.73 (1540.7 examples/sec; 0.083 sec/batch)
2017-05-06 23:05:10.134825: step 92750, loss = 0.70 (1559.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:10.954738: step 92760, loss = 0.75 (1561.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:11.776536: step 92770, loss = 0.73 (1557.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:12.598766: step 92780, loss = 0.86 (1556.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:13.418220: step 92790, loss = 0.67 (1562.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:14.352575: step 92800, loss = 0.83 (1369.9 examples/sec; 0.093 sec/batch)
2017-05-06 23:05:15.073587: step 92810, loss = 0.68 (1775.3 examples/sec; 0.072 sec/batch)
2017-05-06 23:05:15.879595: step 92820, loss = 0.59 (1588.1 examples/sec; 0.081 sec/batch)
2017-05-06 23:05:16.702157: step 92830, loss = 0.75 (1556.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:17.520944: step 92840, loss = 0.73 (1563.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:18.341203: step 92850, loss = 0.70 (1560.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:19.156747: step 92860, loss = 0.70 (1569.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:19.974217: step 92870, loss = 0.64 (1565.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:20.792726: step 92880, loss = 0.77 (1563.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:21.608154: step 92890, loss = 0.69 (1569.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:22.546002: step 92900, loss = 0.72 (1364.8 examples/sec; 0.094 sec/batch)
2017-05-06 23:05:23.249236: step 92910, loss = 0.58 (1820.2 examples/sec; 0.070 sec/batch)
2017-05-06 23:05:24.067185: step 92920, loss = 0.64 (1564.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:24.885706: step 92930, loss = 0.81 (1563.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:25.714637: step 92940, loss = 0.75 (1544.2 examples/sec; 0.083 sec/batch)
2017-05-06 23:05:26.537672: step 92950, loss = 0.74 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:27.364523: step 92960, loss = 0.67 (1548.0 examples/sec; 0.083 sec/batch)
2017-05-06 23:05:28.183555: step 92970, loss = 0.69 (1562.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:28.997164: step 92980, loss = 0.75 (1573.2 examples/sec; 0.081 sec/batch)
2017-05-06 23:05:29.813877: step 92990, loss = 0.77 (1567.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:30.736275: step 93000, loss = 0.72 (1387.7 examples/sec; 0.092 sec/batch)
2017-05-06 23:05:31.441231: step 93010, loss = 0.69 (1815.7 examples/sec; 0.070 sec/batch)
2017-05-06 23:05:32.264696: step 93020, loss = 0.73 (1554.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:33.078946: step 93030, loss = 0.62 (1572.0 examples/sec; 0.081 sec/batch)
2017-05-06 23:05:33.906800: step 93040, loss = 0.76 (1546.2 examples/sec; 0.083 sec/batch)
2017-05-06 23:05:34.725218: step 93050, loss = 0.72 (1564.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:35.560729: step 93060, loss = 0.91 (1532.0 examples/sec; 0.084 sec/batch)
2017-05-06 23:05:36.364094: step 93070, loss = 0.75 (1593.3 examples/sec; 0.080 sec/batch)
2017-05-06 23:05:37.187813: step 93080, loss = 0.83 (1554.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:38.011549: step 93090, loss = 0.80 (1553.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:38.934398: step 93100, loss = 0.76 (1387.0 examples/sec; 0.092 sec/batch)
2017-05-06 23:05:39.647294: step 93110, loss = 0.68 (1795.5 examples/sec; 0.071 sec/batch)
2017-05-06 23:05:40.467983: step 93120, loss = 0.61 (1559.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:41.285829: step 93130, loss = 0.58 (1565.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:42.104607: step 93140, loss = 0.65 (1563.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:42.920371: step 93150, loss = 0.87 (1569.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:43.741492: step 93160, loss = 0.81 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:44.549694: step 93170, loss = 0.80 (1583.8 examples/sec; 0.081 sec/batch)
2017-05-06 23:05:45.372759: step 93180, loss = 0.76 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:46.191317: step 93190, loss = 0.62 (1563.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:47.113490: step 93200, loss = 0.80 (1388.0 examples/sec; 0.092 sec/batch)
2017-05-06 23:05:47.826286: step 93210, loss = 0.64 (1795.7 examples/sec; 0.071 sec/batch)
2017-05-06 23:05:48.653146: step 93220, loss = 0.75 (1548.0 examples/sec; 0.083 sec/batch)
2017-05-06 23:05:49.475623: step 93230, loss = 0.62 (1556.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:50.291197: step 93240, loss = 0.76 (1569.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:51.108794: step 93250, loss = 0.81 (1565.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:51.929635: step 93260, loss = 0.62 (1559.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:52.747173: step 93270, loss = 0.82 (1565.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:53.573829: step 93280, loss = 0.65 (1548.4 examples/sec; 0.083 sec/batch)
2017-05-06 23:05:54.396579: step 93290, loss = 0.80 (1555.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:55.315368: step 93300, loss = 0.64 (1393.1 examples/sec; 0.092 sec/batch)
2017-05-06 23:05:56.040374: step 93310, loss = 0.64 (1765.5 examples/sec; 0.073 sec/batch)
2017-05-06 23:05:56.870781: step 93320, loss = 0.65 (1541.4 examples/sec; 0.083 sec/batch)
2017-05-06 23:05:57.705765: step 93330, loss = 0.73 (1533.0 examples/sec; 0.083 sec/batch)
2017-05-06 23:05:58.525137: step 93340, loss = 0.86 (1562.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:05:59.346018: step 93350, loss = 0.67 (1559.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:00.170631: step 93360, loss = 0.78 (1552.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:00.995035: step 93370, loss = 0.74 (1552.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:01.815678: step 93380, loss = 0.60 (1559.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:02.632882: step 93390, loss = 0.65 (1566.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:03.556653: step 93400, loss = 0.69 (1385.6 examples/sec; 0.092 sec/batch)
2017-05-06 23:06:04.277207: step 93410, loss = 0.77 (1776.4 examples/sec; 0.072 sec/batch)
2017-05-06 23:06:05.088080: step 93420, loss = 0.68 (1578.5 examples/sec; 0.081 sec/batch)
2017-05-06 23:06:05.909804: step 93430, loss = 0.78 (1557.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:06.727235: step 93440, loss = 0.66 (1565.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:07.539244: step 93450, loss = 0.77 (1576.3 examples/sec; 0.081 sec/batch)
2017-05-06 23:06:08.364413: step 93460, loss = 0.64 (1551.2 examples/sec; 0.083 sec/batch)
2017-05-06 23:06:09.196022: step 93470, loss = 0.68 (1539.2 examples/sec; 0.083 sec/batch)
2017-05-06 23:06:10.016199: step 93480, loss = 0.90 (1560.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:10.838239: step 93490, loss = 0.63 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:11.755710: step 93500, loss = 0.75 (1395.1 examples/sec; 0.092 sec/batch)
2017-05-06 23:06:12.476976: step 93510, loss = 0.73 (1774.7 examples/sec; 0.072 sec/batch)
2017-05-06 23:06:13.309439: step 93520, loss = 0.77 (1537.6 examples/sec; 0.083 sec/batch)
2017-05-06 23:06:14.120454: step 93530, loss = 0.57 (1578.3 examples/sec; 0.081 sec/batch)
2017-05-06 23:06:14.936653: step 93540, loss = 0.75 (1568.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:15.754906: step 93550, loss = 0.58 (1564.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:16.575969: step 93560, loss = 0.70 (1559.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:17.393865: step 93570, loss = 0.59 (1565.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:18.212993: step 93580, loss = 0.57 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:19.031480: step 93590, loss = 0.80 (1563.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:19.945547: step 93600, loss = 0.69 (1400.3 examples/sec; 0.091 sec/batch)
2017-05-06 23:06:20.672339: step 93610, loss = 0.74 (1761.2 examples/sec; 0.073 sec/batch)
2017-05-06 23:06:21.492453: step 93620, loss = 0.62 (1560.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:22.315905: step 93630, loss = 0.64 (1554.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:23.135754: step 93640, loss = 0.81 (1561.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:23.950100: step 93650, loss = 0.62 (1571.8 examples/sec; 0.081 sec/batch)
2017-05-06 23:06:24.779879: step 93660, loss = 0.81 (1542.6 examples/sec; 0.083 sec/batch)
2017-05-06 23:06:25.593890: step 93670, loss = 0.71 (1572.5 examples/sec; 0.081 sec/batch)
2017-05-06 23:06:26.424790: step 93680, loss = 0.61 (1540.5 examples/sec; 0.083 sec/batch)
2017-05-06 23:06:27.244733: step 93690, loss = 0.74 (1561.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:28.159035: step 93700, loss = 0.83 (1400.0 examples/sec; 0.091 sec/batch)
2017-05-06 23:06:28.883871: step 93710, loss = 0.81 (1765.9 examples/sec; 0.072 sec/batch)
2017-05-06 23:06:29.704958: step 93720, loss = 0.56 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:30.518248: step 93730, loss = 0.72 (1573.9 examples/sec; 0.081 sec/batch)
2017-05-06 23:06:31.334949: step 93740, loss = 0.50 (1567.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:32.147832: step 93750, loss = 0.56 (1574.7 examples/sec; 0.081 sec/batch)
2017-05-06 23:06:32.979126: step 93760, loss = 0.70 (1539.8 examples/sec; 0.083 sec/batch)
2017-05-06 23:06:33.805842: step 93770, loss = 0.64 (1548.3 examples/sec; 0.083 sec/batch)
2017-05-06 23:06:34.622409: step 93780, loss = 0.66 (1567.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:35.443461: step 93790, loss = 0.62 (1559.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:36.375599: step 93800, loss = 0.73 (1373.2 examples/sec; 0.093 sec/batch)
2017-05-06 23:06:37.075951: step 93810, loss = 0.74 (1827.7 examples/sec; 0.070 sec/batch)
2017-05-06 23:06:37.892681: step 93820, loss = 0.62 (1567.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:38.721312: step 93830, loss = 0.69 (1544.7 examples/sec; 0.083 sec/batch)
2017-05-06 23:06:39.531217: step 93840, loss = 0.69 (1580.4 examples/sec; 0.081 sec/batch)
2017-05-06 23:06:40.346444: step 93850, loss = 0.69 (1570.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:41.169498: step 93860, loss = 0.65 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:41.995321: step 93870, loss = 0.77 (1550.0 examples/sec; 0.083 sec/batch)
2017-05-06 23:06:42.813083: step 93880, loss = 0.68 (1565.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:43.629767: step 93890, loss = 0.69 (1567.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:44.552435: step 93900, loss = 0.67 (1387.3 examples/sec; 0.092 sec/batch)
2017-05-06 23:06:45.272016: step 93910, loss = 0.80 (1778.8 examples/sec; 0.072 sec/batch)
2017-05-06 23:06:46.090325: step 93920, loss = 0.58 (1564.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:46.911183: step 93930, loss = 0.90 (1559.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:47.720452: step 93940, loss = 0.73 (1581.7 examples/sec; 0.081 sec/batch)
2017-05-06 23:06:48.542220: step 93950, loss = 0.74 (1557.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:49.360582: step 93960, loss = 0.65 (1564.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:50.179633: step 93970, loss = 0.68 (1562.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:50.996690: step 93980, loss = 0.62 (1566.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:51.812194: step 93990, loss = 0.57 (1569.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:52.731949: step 94000, loss = 0.63 (1391.7 examples/sec; 0.092 sec/batch)
2017-05-06 23:06:53.460954: step 94010, loss = 0.78 (1755.8 examples/sec; 0.073 sec/batch)
2017-05-06 23:06:54.276180: step 94020, loss = 0.77 (1570.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:55.101770: step 94030, loss = 0.80 (1550.4 examples/sec; 0.083 sec/batch)
2017-05-06 23:06:55.918963: step 94040, loss = 0.58 (1566.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:56.763832: step 94050, loss = 0.68 (1515.0 examples/sec; 0.084 sec/batch)
2017-05-06 23:06:57.571264: step 94060, loss = 0.79 (1585.3 examples/sec; 0.081 sec/batch)
2017-05-06 23:06:58.388661: step 94070, loss = 0.74 (1566.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:06:59.198436: step 94080, loss = 0.74 (1580.7 examples/sec; 0.081 sec/batch)
2017-05-06 23:07:00.008669: step 94090, loss = 0.59 (1579.8 examples/sec; 0.081 sec/batch)
2017-05-06 23:07:00.928302: step 94100, loss = 0.64 (1391.9 examples/sec; 0.092 sec/batch)
2017-05-06 23:07:01.656215: step 94110, loss = 0.56 (1758.4 examples/sec; 0.073 sec/batch)
2017-05-06 23:07:02.473296: step 94120, loss = 0.65 (1566.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:07:03.283227: step 94130, loss = 0.80 (1580.4 examples/sec; 0.081 sec/batch)
2017-05-06 23:07:04.100955: step 94140, loss = 0.75 (1565.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:07:04.933761: step 94150, loss = 0.63 (1537.0 examples/sec; 0.083 sec/batch)
2017-05-06 23:07:05.747367: step 94160, loss = 0.83 (1573.2 examples/sec; 0.081 sec/batch)
2017-05-06 23:07:06.573350: step 94170, loss = 0.70 (1549.7 examples/sec; 0.083 sec/batch)
2017-05-06 23:07:07.388431: step 94180, loss = 0.58 (1570.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:07:08.207946: step 94190, loss = 0.78 (1561.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:07:09.127134: step 94200, loss = 0.59 (1392.5 examples/sec; 0.092 sec/batch)
2017-05-06 23:07:09.851901: step 94210, loss = 0.70 (1766.1 examples/sec; 0.072 sec/batch)
2017-05-06 23:07:10.679000: step 94220, loss = 0.60 (1547.6 examples/sec; 0.083 sec/batch)
2017-05-06 23:07:11.497546: step 94230, loss = 0.61 (1563.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:07:12.315795: step 94240, loss = 0.72 (1564.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:07:13.141067: step 94250, loss = 0.72 (1551.1 examples/sec; 0.083 sec/batch)
2017-05-06 23:07:13.969754: step 94260, loss = 0.71 (1544.6 examples/sec; 0.083 sec/batch)
2017-05-06 23:07:14.797568: step 94270, loss = 0.70 (1546.2 examples/sec; 0.083 sec/batch)
2017-05-06 23:07:15.620517: step 94280, loss = 0.77 (1555.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:07:16.449601: step 94290, loss = 0.64 (1543.9 examples/sec; 0.083 sec/batch)
2017-05-06 23:07:17.357763: step 94300, loss = 0.73 (1409.4 examples/sec; 0.091 sec/batch)
2017-05-06 23:07:18.080308: step 94310, loss = 0.65 (1771.5 examples/sec; 0.072 sec/batch)
2017-05-06 23:07:18.899022: step 94320, loss = 0.75 (1563.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:07:19.711398: step 94330, loss = 0.83 (1575.6 examples/sec; 0.081 sec/batch)
2017-05-06 23:07:20.530885: step 94340, loss = 0.62 (1562.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:07:21.357131: step 94350, loss = 0.79 (1549.2 examples/sec; 0.083 sec/batch)
2017-05-06 23:07:22.175505: step 94360, loss = 0.65 (1564.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:07:22.998645: step 94370, loss = 0.67 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:07:23.816364: step 94380, loss = 0.69 (1565.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:07:24.639310: step 94390, loss = 0.76 (1555.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:07:25.556142: step 94400, loss = 0.89 (1396.1 examples/sec; 0.092 sec/batch)
2017-05-06 23:07:26.281929: step 94410, loss = 0.78 (1763.6 examples/sec; 0.073 sec/batch)
2017-05-06 23:07:27.100915: step 94420, loss = 0.72 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:07:27.911344: step 94430, loss = 0.70 (1579.4 examples/sec; 0.081 sec/batch)
2017-05-06 23:07:28.738620: step 94440, loss = 0.66 (1547.2 examples/sec; 0.083 sec/batch)
2017-05-06 23:07:29.555198: step 94450, loss = 0.65 (1567.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:07:30.367874: step 94460, loss = 0.59 (1575.0 examples/sec; 0.081 sec/batch)
2017-05-06 23:07:31.193731: step 94470, loss = 0.73 (1549.9 examples/sec; 0.083 sec/batch)
2017-05-06 23:07:32.004692: step 94480, loss = 0.82 (1578.4 examples/sec; 0.081 sec/batch)
2017-05-06 23:07:32.825612: step 94490, loss = 0.59 (1559.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:07:33.740081: step 94500, loss = 0.77 (1399.7 examples/sec; 0.091 sec/batch)
2017-05-06 23:07:34.500692: step 94510, loss = 0.87 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-06 23:07:35.288321: step 94520, loss = 0.64 (1625.1 examples/sec; 0.079 sec/batch)
2017-05-06 23:07:36.106440: step 94530, loss = 0.75 (1564.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:07:36.925953: step 94540, loss = 0.87 (1561.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:07:37.755554: step 94550, loss = 0.71 (1542.9 examples/sec; 0.083 sec/batch)
2017-05-06 23:07:38.583731: step 94560, loss = 0.75 (1545.6 examples/sec; 0.083 sec/batch)
2017-05-06 23:07:39.394171: step 94570, loss = 0.76 (1579.4 examples/sec; 0.081 sec/batch)
2017-05-06 23:07:40.212068: step 94580, loss = 0.57 (1565.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:07:41.035528: step 94590, loss = 0.88 (1554.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:07:41.949425: step 94600, loss = 0.72 (1400.6 examples/sec; 0.091 sec/batch)
2017-05-06 23:07:42.680159: step 94610, loss = 0.80 (1751.7 examples/sec; 0.073 sec/batch)
2017-05-06 23:07:43.494297: step 94620, loss = 0.77 (1572.2 examples/sec; 0.081 sec/batch)
2017-05-06 23:07:44.307404: step 94630, loss = 0.58 (1574.2 examples/sec; 0.081 sec/batch)
2017-05-06 23:07:45.130268: step 94640, loss = 0.73 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:07:45.949495: step 94650, loss = 0.74 (1562.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:07:46.774979: step 94660, loss = 0.79 (1550.6 examples/sec; 0.083 sec/batch)
2017-05-06 23:07:47.586074: step 94670, loss = 0.64 (1578.1 examples/sec; 0.081 sec/batch)
2017-05-06 23:07:48.411704: step 94680, loss = 0.74 (1550.3 examples/sec; 0.083 sec/batch)
2017-05-06 23:07:49.234309: step 94690, loss = 0.71 (1556.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:07:50.145059: step 94700, loss = 0.56 (1405.5 examples/sec; 0.091 sec/batch)
2017-05-06 23:07:50.870234: step 94710, loss = 0.78 (1765.1 examples/sec; 0.073 sec/batch)
2017-05-06 23:07:51.686381: step 94720, loss = 0.64 (1568.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:07:52.504545: step 94730, loss = 0.75 (1564.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:07:53.323795: step 94740, loss = 0.60 (1562.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:07:54.146715: step 94750, loss = 0.73 (1555.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:07:54.966150: step 94760, loss = 0.67 (1562.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:07:55.784189: step 94770, loss = 0.65 (1564.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:07:56.601631: step 94780, loss = 0.76 (1565.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:07:57.423672: step 94790, loss = 0.61 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:07:58.344521: step 94800, loss = 0.67 (1390.0 examples/sec; 0.092 sec/batch)
2017-05-06 23:07:59.063788: step 94810, loss = 0.73 (1779.6 examples/sec; 0.072 sec/batch)
2017-05-06 23:07:59.878422: step 94820, loss = 0.79 (1571.3 examples/sec; 0.081 sec/batch)
2017-05-06 23:08:00.702371: step 94830, loss = 0.70 (1553.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:01.524251: step 94840, loss = 0.65 (1557.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:02.358956: step 94850, loss = 0.58 (1533.5 examples/sec; 0.083 sec/batch)
2017-05-06 23:08:03.176379: step 94860, loss = 0.76 (1565.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:03.985859: step 94870, loss = 0.75 (1581.2 examples/sec; 0.081 sec/batch)
2017-05-06 23:08:04.805832: step 94880, loss = 0.75 (1561.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:05.618673: step 94890, loss = 0.76 (1574.7 examples/sec; 0.081 sec/batch)
2017-05-06 23:08:06.545008: step 94900, loss = 0.80 (1381.8 examples/sec; 0.093 sec/batch)
2017-05-06 23:08:07.264519: step 94910, loss = 0.57 (1779.0 examples/sec; 0.072 sec/batch)
2017-05-06 23:08:08.083811: step 94920, loss = 0.66 (1562.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:08.906334: step 94930, loss = 0.71 (1556.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:09.725718: step 94940, loss = 0.80 (1562.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:10.552365: step 94950, loss = 0.75 (1548.4 examples/sec; 0.083 sec/batch)
2017-05-06 23:08:11.373378: step 94960, loss = 0.74 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:12.191128: step 94970, loss = 0.65 (1565.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:13.018529: step 94980, loss = 0.76 (1547.0 examples/sec; 0.083 sec/batch)
2017-05-06 23:08:13.866313: step 94990, loss = 0.69 (1509.8 examples/sec; 0.085 sec/batch)
2017-05-06 23:08:14.790779: step 95000, loss = 0.60 (1384.6 examples/sec; 0.092 sec/batch)
2017-05-06 23:08:15.505949: step 95010, loss = 0.63 (1789.8 examples/sec; 0.072 sec/batch)
2017-05-06 23:08:16.320609: step 95020, loss = 0.75 (1571.2 examples/sec; 0.081 sec/batch)
2017-05-06 23:08:17.148172: step 95030, loss = 0.91 (1546.7 examples/sec; 0.083 sec/batch)
2017-05-06 23:08:17.991311: step 95040, loss = 0.64 (1518.1 examples/sec; 0.084 sec/batch)
2017-05-06 23:08:18.796418: step 95050, loss = 0.84 (1589.8 examples/sec; 0.081 sec/batch)
2017-05-06 23:08:19.612745: step 95060, loss = 0.74 (1568.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:20.426828: step 95070, loss = 0.70 (1572.3 examples/sec; 0.081 sec/batch)
2017-05-06 23:08:21.249975: step 95080, loss = 0.64 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:22.066395: step 95090, loss = 0.80 (1567.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:22.987446: step 95100, loss = 0.94 (1389.7 examples/sec; 0.092 sec/batch)
2017-05-06 23:08:23.695209: step 95110, loss = 0.69 (1808.5 examples/sec; 0.071 sec/batch)
2017-05-06 23:08:24.516018: step 95120, loss = 0.76 (1559.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:25.342107: step 95130, loss = 0.70 (1549.5 examples/sec; 0.083 sec/batch)
2017-05-06 23:08:26.155584: step 95140, loss = 0.69 (1573.5 examples/sec; 0.081 sec/batch)
2017-05-06 23:08:26.974654: step 95150, loss = 0.70 (1562.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:27.786503: step 95160, loss = 0.54 (1576.7 examples/sec; 0.081 sec/batch)
2017-05-06 23:08:28.603773: step 95170, loss = 0.80 (1566.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:29.425983: step 95180, loss = 0.61 (1556.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:30.245798: step 95190, loss = 0.73 (1561.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:31.165202: step 95200, loss = 0.77 (1392.2 examples/sec; 0.092 sec/batch)
2017-05-06 23:08:31.880325: step 95210, loss = 0.72 (1789.9 examples/sec; 0.072 sec/batch)
2017-05-06 23:08:32.696459: step 95220, loss = 0.72 (1568.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:33.514817: step 95230, loss = 0.72 (1564.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:34.345977: step 95240, loss = 0.69 (1540.0 examples/sec; 0.083 sec/batch)
2017-05-06 23:08:35.155532: step 95250, loss = 0.71 (1581.1 examples/sec; 0.081 sec/batch)
2017-05-06 23:08:35.971768: step 95260, loss = 0.64 (1568.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:36.797905: step 95270, loss = 0.76 (1549.4 examples/sec; 0.083 sec/batch)
2017-05-06 23:08:37.624441: step 95280, loss = 0.72 (1548.6 examples/sec; 0.083 sec/batch)
2017-05-06 23:08:38.449282: step 95290, loss = 0.57 (1551.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:39.367902: step 95300, loss = 0.72 (1393.4 examples/sec; 0.092 sec/batch)
2017-05-06 23:08:40.087990: step 95310, loss = 0.69 (1777.6 examples/sec; 0.072 sec/batch)
2017-05-06 23:08:40.909992: step 95320, loss = 0.63 (1557.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:41.736508: step 95330, loss = 0.66 (1548.7 examples/sec; 0.083 sec/batch)
2017-05-06 23:08:42.562172: step 95340, loss = 0.69 (1550.3 examples/sec; 0.083 sec/batch)
2017-05-06 23:08:43.379692: step 95350, loss = 0.66 (1565.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:44.188892: step 95360, loss = 0.67 (1581.8 examples/sec; 0.081 sec/batch)
2017-05-06 23:08:45.006175: step 95370, loss = 0.69 (1566.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:45.831021: step 95380, loss = 0.72 (1551.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:46.651189: step 95390, loss = 0.68 (1560.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:47.575762: step 95400, loss = 0.60 (1384.4 examples/sec; 0.092 sec/batch)
2017-05-06 23:08:48.292019: step 95410, loss = 0.72 (1787.1 examples/sec; 0.072 sec/batch)
2017-05-06 23:08:49.107818: step 95420, loss = 0.66 (1569.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:49.925483: step 95430, loss = 0.77 (1565.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:50.745024: step 95440, loss = 0.61 (1561.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:51.561096: step 95450, loss = 0.62 (1568.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:52.373221: step 95460, loss = 0.87 (1576.1 examples/sec; 0.081 sec/batch)
2017-05-06 23:08:53.187688: step 95470, loss = 0.70 (1571.6 examples/sec; 0.081 sec/batch)
2017-05-06 23:08:54.003525: step 95480, loss = 0.82 (1569.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:54.822779: step 95490, loss = 0.67 (1562.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:55.740525: step 95500, loss = 0.66 (1394.7 examples/sec; 0.092 sec/batch)
2017-05-06 23:08:56.463928: step 95510, loss = 0.71 (1769.4 examples/sec; 0.072 sec/batch)
2017-05-06 23:08:57.286308: step 95520, loss = 0.73 (1556.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:58.107674: step 95530, loss = 0.68 (1558.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:08:58.934846: step 95540, loss = 0.77 (1547.5 examples/sec; 0.083 sec/batch)
2017-05-06 23:08:59.746140: step 95550, loss = 0.69 (1577.7 examples/sec; 0.081 sec/batch)
2017-05-06 23:09:00.574264: step 95560, loss = 0.72 (1545.7 examples/sec; 0.083 sec/batch)
2017-05-06 23:09:01.401494: step 95570, loss = 0.64 (1547.3 examples/sec; 0.083 sec/batch)
2017-05-06 23:09:02.226508: step 95580, loss = 0.60 (1551.5 examples/sec; 0.083 sec/batch)
2017-05-06 23:09:03.053220: step 95590, loss = 0.57 (1548.3 examples/sec; 0.083 sec/batch)
2017-05-06 23:09:03.958899: step 95600, loss = 0.83 (1413.3 examples/sec; 0.091 sec/batch)
2017-05-06 23:09:04.692932: step 95610, loss = 0.65 (1743.8 examples/sec; 0.073 sec/batch)
2017-05-06 23:09:05.520310: step 95620, loss = 0.90 (1547.1 examples/sec; 0.083 sec/batch)
2017-05-06 23:09:06.346609: step 95630, loss = 0.59 (1549.1 examples/sec; 0.083 sec/batch)
2017-05-06 23:09:07.175670: step 95640, loss = 0.65 (1543.9 examples/sec; 0.083 sec/batch)
2017-05-06 23:09:07.984762: step 95650, loss = 0.82 (1582.0 examples/sec; 0.081 sec/batch)
2017-05-06 23:09:08.802950: step 95660, loss = 0.75 (1564.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:09.623954: step 95670, loss = 0.81 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:10.447782: step 95680, loss = 0.71 (1553.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:11.271305: step 95690, loss = 0.78 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:12.193576: step 95700, loss = 0.60 (1387.9 examples/sec; 0.092 sec/batch)
2017-05-06 23:09:12.915800: step 95710, loss = 0.69 (1772.3 examples/sec; 0.072 sec/batch)
2017-05-06 23:09:13.737814: step 95720, loss = 0.60 (1557.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:14.561400: step 95730, loss = 0.65 (1554.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:15.375906: step 95740, loss = 0.92 (1571.5 examples/sec; 0.081 sec/batch)
2017-05-06 23:09:16.195247: step 95750, loss = 0.79 (1562.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:17.011031: step 95760, loss = 0.57 (1569.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:17.835502: step 95770, loss = 0.68 (1552.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:18.654824: step 95780, loss = 0.62 (1562.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:19.475448: step 95790, loss = 0.78 (1559.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:20.389489: step 95800, loss = 0.93 (1400.4 examples/sec; 0.091 sec/batch)
2017-05-06 23:09:21.113592: step 95810, loss = 0.57 (1767.7 examples/sec; 0.072 sec/batch)
2017-05-06 23:09:21.941669: step 95820, loss = 0.76 (1545.8 examples/sec; 0.083 sec/batch)
2017-05-06 23:09:22.763150: step 95830, loss = 0.59 (1558.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:23.586977: step 95840, loss = 0.73 (1553.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:24.406124: step 95850, loss = 0.73 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:25.226412: step 95860, loss = 0.55 (1560.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:26.052276: step 95870, loss = 0.79 (1549.9 examples/sec; 0.083 sec/batch)
2017-05-06 23:09:26.873725: step 95880, loss = 0.83 (1558.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:27.684574: step 95890, loss = 0.64 (1578.6 examples/sec; 0.081 sec/batch)
2017-05-06 23:09:28.601475: step 95900, loss = 0.68 (1396.0 examples/sec; 0.092 sec/batch)
2017-05-06 23:09:29.331332: step 95910, loss = 0.68 (1753.8 examples/sec; 0.073 sec/batch)
2017-05-06 23:09:30.152304: step 95920, loss = 0.79 (1559.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:30.969331: step 95930, loss = 0.74 (1566.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:31.784919: step 95940, loss = 0.77 (1569.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:32.616882: step 95950, loss = 0.66 (1538.5 examples/sec; 0.083 sec/batch)
2017-05-06 23:09:33.444286: step 95960, loss = 0.91 (1547.0 examples/sec; 0.083 sec/batch)
2017-05-06 23:09:34.261672: step 95970, loss = 0.59 (1566.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:35.077241: step 95980, loss = 0.84 (1569.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:35.890794: step 95990, loss = 0.68 (1573.4 examples/sec; 0.081 sec/batch)
2017-05-06 23:09:36.806733: step 96000, loss = 0.63 (1397.5 examples/sec; 0.092 sec/batch)
2017-05-06 23:09:37.531656: step 96010, loss = 0.61 (1765.7 examples/sec; 0.072 sec/batch)
2017-05-06 23:09:38.354432: step 96020, loss = 0.77 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:39.203302: step 96030, loss = 0.68 (1507.9 examples/sec; 0.085 sec/batch)
2017-05-06 23:09:40.005924: step 96040, loss = 0.66 (1594.8 examples/sec; 0.080 sec/batch)
2017-05-06 23:09:40.816647: step 96050, loss = 0.87 (1578.8 examples/sec; 0.081 sec/batch)
2017-05-06 23:09:41.635959: step 96060, loss = 0.73 (1562.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:42.455824: step 96070, loss = 0.56 (1561.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:43.270808: step 96080, loss = 0.68 (1570.6 examples/sec; 0.081 sec/batch)
2017-05-06 23:09:44.089416: step 96090, loss = 0.64 (1563.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:45.010129: step 96100, loss = 0.67 (1390.2 examples/sec; 0.092 sec/batch)
2017-05-06 23:09:45.731993: step 96110, loss = 0.54 (1773.2 examples/sec; 0.072 sec/batch)
2017-05-06 23:09:46.551653: step 96120, loss = 0.72 (1561.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:47.383962: step 96130, loss = 0.62 (1537.9 examples/sec; 0.083 sec/batch)
2017-05-06 23:09:48.197810: step 96140, loss = 0.75 (1572.8 examples/sec; 0.081 sec/batch)
2017-05-06 23:09:49.011062: step 96150, loss = 0.73 (1574.0 examples/sec; 0.081 sec/batch)
2017-05-06 23:09:49.830724: step 96160, loss = 0.84 (1561.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:50.655503: step 96170, loss = 0.78 (1551.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:51.470583: step 96180, loss = 0.71 (1570.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:52.293602: step 96190, loss = 0.73 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:53.209371: step 96200, loss = 0.73 (1397.7 examples/sec; 0.092 sec/batch)
2017-05-06 23:09:53.936546: step 96210, loss = 0.68 (1760.3 examples/sec; 0.073 sec/batch)
2017-05-06 23:09:54.755607: step 96220, loss = 0.58 (1562.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:55.565822: step 96230, loss = 0.75 (1579.8 examples/sec; 0.081 sec/batch)
2017-05-06 23:09:56.381497: step 96240, loss = 0.70 (1569.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:57.203988: step 96250, loss = 0.76 (1556.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:58.020409: step 96260, loss = 0.73 (1567.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:58.839376: step 96270, loss = 0.85 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:09:59.656027: step 96280, loss = 0.71 (1567.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:00.477110: step 96290, loss = 0.65 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:01.395231: step 96300, loss = 0.61 (1394.2 examples/sec; 0.092 sec/batch)
2017-05-06 23:10:02.124778: step 96310, loss = 0.71 (1754.5 examples/sec; 0.073 sec/batch)
2017-05-06 23:10:02.952988: step 96320, loss = 0.78 (1545.5 examples/sec; 0.083 sec/batch)
2017-05-06 23:10:03.772721: step 96330, loss = 0.71 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:04.592738: step 96340, loss = 0.96 (1560.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:05.404154: step 96350, loss = 0.77 (1577.5 examples/sec; 0.081 sec/batch)
2017-05-06 23:10:06.223417: step 96360, loss = 0.82 (1562.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:07.038851: step 96370, loss = 0.68 (1569.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:07.844619: step 96380, loss = 0.84 (1588.6 examples/sec; 0.081 sec/batch)
2017-05-06 23:10:08.674492: step 96390, loss = 0.52 (1542.4 examples/sec; 0.083 sec/batch)
2017-05-06 23:10:09.616193: step 96400, loss = 0.78 (1359.2 examples/sec; 0.094 sec/batch)
2017-05-06 23:10:10.318089: step 96410, loss = 0.82 (1823.6 examples/sec; 0.070 sec/batch)
2017-05-06 23:10:11.141311: step 96420, loss = 0.63 (1554.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:11.960111: step 96430, loss = 0.63 (1563.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:12.779291: step 96440, loss = 0.72 (1562.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:13.597951: step 96450, loss = 0.55 (1563.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:14.412216: step 96460, loss = 0.78 (1572.0 examples/sec; 0.081 sec/batch)
2017-05-06 23:10:15.237511: step 96470, loss = 0.85 (1551.0 examples/sec; 0.083 sec/batch)
2017-05-06 23:10:16.048233: step 96480, loss = 0.68 (1578.9 examples/sec; 0.081 sec/batch)
2017-05-06 23:10:16.862974: step 96490, loss = 0.80 (1571.0 examples/sec; 0.081 sec/batch)
2017-05-06 23:10:17.780860: step 96500, loss = 0.72 (1394.5 examples/sec; 0.092 sec/batch)
2017-05-06 23:10:18.500621: step 96510, loss = 0.72 (1778.4 examples/sec; 0.072 sec/batch)
2017-05-06 23:10:19.322360: step 96520, loss = 0.67 (1557.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:20.131667: step 96530, loss = 0.83 (1581.6 examples/sec; 0.081 sec/batch)
2017-05-06 23:10:20.949693: step 96540, loss = 0.91 (1564.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:21.767426: step 96550, loss = 0.69 (1565.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:22.584442: step 96560, loss = 0.87 (1566.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:23.402783: step 96570, loss = 0.68 (1564.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:24.213871: step 96580, loss = 0.71 (1578.1 examples/sec; 0.081 sec/batch)
2017-05-06 23:10:25.037368: step 96590, loss = 0.68 (1554.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:25.965092: step 96600, loss = 0.86 (1379.7 examples/sec; 0.093 sec/batch)
2017-05-06 23:10:26.685196: step 96610, loss = 0.70 (1777.5 examples/sec; 0.072 sec/batch)
2017-05-06 23:10:27.490517: step 96620, loss = 0.80 (1589.4 examples/sec; 0.081 sec/batch)
2017-05-06 23:10:28.317709: step 96630, loss = 0.70 (1547.4 examples/sec; 0.083 sec/batch)
2017-05-06 23:10:29.138399: step 96640, loss = 0.74 (1559.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:29.951420: step 96650, loss = 0.75 (1574.4 examples/sec; 0.081 sec/batch)
2017-05-06 23:10:30.770994: step 96660, loss = 0.68 (1561.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:31.581921: step 96670, loss = 0.59 (1578.4 examples/sec; 0.081 sec/batch)
2017-05-06 23:10:32.393212: step 96680, loss = 0.63 (1577.7 examples/sec; 0.081 sec/batch)
2017-05-06 23:10:33.216524: step 96690, loss = 0.63 (1554.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:34.132081: step 96700, loss = 0.64 (1398.1 examples/sec; 0.092 sec/batch)
2017-05-06 23:10:34.864822: step 96710, loss = 0.80 (1746.8 examples/sec; 0.073 sec/batch)
2017-05-06 23:10:35.688239: step 96720, loss = 0.72 (1554.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:36.506984: step 96730, loss = 0.63 (1563.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:37.336811: step 96740, loss = 0.70 (1542.5 examples/sec; 0.083 sec/batch)
2017-05-06 23:10:38.156087: step 96750, loss = 0.62 (1562.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:38.982746: step 96760, loss = 0.76 (1548.4 examples/sec; 0.083 sec/batch)
2017-05-06 23:10:39.790108: step 96770, loss = 0.65 (1585.4 examples/sec; 0.081 sec/batch)
2017-05-06 23:10:40.614763: step 96780, loss = 0.78 (1552.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:41.431310: step 96790, loss = 0.70 (1567.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:42.345805: step 96800, loss = 0.62 (1399.7 examples/sec; 0.091 sec/batch)
2017-05-06 23:10:43.066446: step 96810, loss = 0.75 (1776.2 examples/sec; 0.072 sec/batch)
2017-05-06 23:10:43.877230: step 96820, loss = 0.66 (1578.7 examples/sec; 0.081 sec/batch)
2017-05-06 23:10:44.698910: step 96830, loss = 0.93 (1557.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:45.521548: step 96840, loss = 0.68 (1556.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:46.338943: step 96850, loss = 0.70 (1566.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:47.160754: step 96860, loss = 0.96 (1557.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:47.974159: step 96870, loss = 0.65 (1573.6 examples/sec; 0.081 sec/batch)
2017-05-06 23:10:48.808748: step 96880, loss = 0.69 (1533.7 examples/sec; 0.083 sec/batch)
2017-05-06 23:10:49.629670: step 96890, loss = 0.80 (1559.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:50.544545: step 96900, loss = 0.53 (1399.1 examples/sec; 0.091 sec/batch)
2017-05-06 23:10:51.274353: step 96910, loss = 0.71 (1753.9 examples/sec; 0.073 sec/batch)
2017-05-06 23:10:52.089110: step 96920, loss = 0.60 (1571.0 examples/sec; 0.081 sec/batch)
2017-05-06 23:10:52.909671: step 96930, loss = 0.67 (1559.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:53.726749: step 96940, loss = 0.87 (1566.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:54.537558: step 96950, loss = 0.63 (1578.7 examples/sec; 0.081 sec/batch)
2017-05-06 23:10:55.358174: step 96960, loss = 0.76 (1559.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:56.177669: step 96970, loss = 0.74 (1561.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:57.001458: step 96980, loss = 0.76 (1553.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:57.825740: step 96990, loss = 0.79 (1552.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:10:58.750728: step 97000, loss = 0.73 (1383.8 examples/sec; 0.092 sec/batch)
2017-05-06 23:10:59.474428: step 97010, loss = 0.65 (1768.7 examples/sec; 0.072 sec/batch)
2017-05-06 23:11:00.317607: step 97020, loss = 0.73 (1518.1 examples/sec; 0.084 sec/batch)
2017-05-06 23:11:01.122202: step 97030, loss = 0.70 (1590.9 examples/sec; 0.080 sec/batch)
2017-05-06 23:11:01.933567: step 97040, loss = 0.72 (1577.6 examples/sec; 0.081 sec/batch)
2017-05-06 23:11:02.753261: step 97050, loss = 0.73 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:11:03.569482: step 97060, loss = 0.75 (1568.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:11:04.383607: step 97070, loss = 0.76 (1572.2 examples/sec; 0.081 sec/batch)
2017-05-06 23:11:05.209628: step 97080, loss = 0.73 (1549.6 examples/sec; 0.083 sec/batch)
2017-05-06 23:11:06.026585: step 97090, loss = 0.69 (1566.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:11:06.952395: step 97100, loss = 0.75 (1382.6 examples/sec; 0.093 sec/batch)
2017-05-06 23:11:07.664568: step 97110, loss = 0.70 (1797.3 examples/sec; 0.071 sec/batch)
2017-05-06 23:11:08.495400: step 97120, loss = 0.70 (1540.6 examples/sec; 0.083 sec/batch)
2017-05-06 23:11:09.306173: step 97130, loss = 0.65 (1578.7 examples/sec; 0.081 sec/batch)
2017-05-06 23:11:10.132467: step 97140, loss = 0.78 (1549.1 examples/sec; 0.083 sec/batch)
2017-05-06 23:11:10.946387: step 97150, loss = 0.80 (1572.6 examples/sec; 0.081 sec/batch)
2017-05-06 23:11:11.750761: step 97160, loss = 0.55 (1591.3 examples/sec; 0.080 sec/batch)
2017-05-06 23:11:12.564838: step 97170, loss = 0.92 (1572.3 examples/sec; 0.081 sec/batch)
2017-05-06 23:11:13.380985: step 97180, loss = 0.64 (1568.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:11:14.200627: step 97190, loss = 0.70 (1561.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:11:15.136094: step 97200, loss = 0.60 (1368.3 examples/sec; 0.094 sec/batch)
2017-05-06 23:11:15.824647: step 97210, loss = 0.68 (1858.9 examples/sec; 0.069 sec/batch)
2017-05-06 23:11:16.656973: step 97220, loss = 0.61 (1537.9 examples/sec; 0.083 sec/batch)
2017-05-06 23:11:17.477664: step 97230, loss = 0.62 (1559.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:11:18.306566: step 97240, loss = 0.64 (1544.2 examples/sec; 0.083 sec/batch)
2017-05-06 23:11:19.115543: step 97250, loss = 0.68 (1582.2 examples/sec; 0.081 sec/batch)
2017-05-06 23:11:19.919835: step 97260, loss = 0.74 (1591.5 examples/sec; 0.080 sec/batch)
2017-05-06 23:11:20.736151: step 97270, loss = 0.65 (1568.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:11:21.545165: step 97280, loss = 0.82 (1582.2 examples/sec; 0.081 sec/batch)
2017-05-06 23:11:22.374356: step 97290, loss = 0.63 (1543.7 examples/sec; 0.083 sec/batch)
2017-05-06 23:11:23.292250: step 97300, loss = 0.62 (1394.5 examples/sec; 0.092 sec/batch)
2017-05-06 23:11:24.003929: step 97310, loss = 0.73 (1798.6 examples/sec; 0.071 sec/batch)
2017-05-06 23:11:24.818817: step 97320, loss = 0.88 (1570.8 examples/sec; 0.081 sec/batch)
2017-05-06 23:11:25.641110: step 97330, loss = 0.64 (1556.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:11:26.464230: step 97340, loss = 0.70 (1555.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:11:27.281934: step 97350, loss = 0.77 (1565.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:11:28.094175: step 97360, loss = 0.70 (1575.9 examples/sec; 0.081 sec/batch)
2017-05-06 23:11:28.919319: step 97370, loss = 0.64 (1551.2 examples/sec; 0.083 sec/batch)
2017-05-06 23:11:29.736971: step 97380, loss = 0.81 (1565.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:11:30.558290: step 97390, loss = 0.67 (1558.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:11:31.496774: step 97400, loss = 0.96 (1363.9 examples/sec; 0.094 sec/batch)
2017-05-06 23:11:32.199483: step 97410, loss = 0.73 (1821.5 examples/sec; 0.070 sec/batch)
2017-05-06 23:11:33.022706: step 97420, loss = 0.67 (1554.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:11:33.843847: step 97430, loss = 0.62 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:11:34.668762: step 97440, loss = 0.78 (1551.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:11:35.487970: step 97450, loss = 0.71 (1562.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:11:36.299486: step 97460, loss = 0.73 (1577.3 examples/sec; 0.081 sec/batch)
2017-05-06 23:11:37.109347: step 97470, loss = 0.68 (1580.5 examples/sec; 0.081 sec/batch)
2017-05-06 23:11:37.928037: step 97480, loss = 0.56 (1563.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:11:38.748933: step 97490, loss = 0.80 (1559.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:11:39.661537: step 97500, loss = 0.69 (1402.6 examples/sec; 0.091 sec/batch)
2017-05-06 23:11:40.378088: step 97510, loss = 0.69 (1786.4 examples/sec; 0.072 sec/batch)
2017-05-06 23:11:41.194962: step 97520, loss = 0.63 (1566.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:11:42.012106: step 97530, loss = 0.73 (1566.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:11:42.829729: step 97540, loss = 0.80 (1565.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:11:43.636767: step 97550, loss = 0.72 (1586.1 examples/sec; 0.081 sec/batch)
2017-05-06 23:11:44.462341: step 97560, loss = 0.57 (1550.4 examples/sec; 0.083 sec/batch)
2017-05-06 23:11:45.277768: step 97570, loss = 0.65 (1569.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:11:46.095259: step 97580, loss = 0.72 (1565.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:11:46.926712: step 97590, loss = 0.77 (1539.5 examples/sec; 0.083 sec/batch)
2017-05-06 23:11:47.827832: step 97600, loss = 0.74 (1420.5 examples/sec; 0.090 sec/batch)
2017-05-06 23:11:48.552851: step 97610, loss = 0.56 (1765.5 examples/sec; 0.073 sec/batch)
2017-05-06 23:11:49.375696: step 97620, loss = 0.65 (1555.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:11:50.199672: step 97630, loss = 0.74 (1553.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:11:51.016029: step 97640, loss = 0.70 (1567.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:11:51.823888: step 97650, loss = 0.56 (1584.4 examples/sec; 0.081 sec/batch)
2017-05-06 23:11:52.644950: step 97660, loss = 0.75 (1559.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:11:53.465746: step 97670, loss = 0.73 (1559.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:11:54.282335: step 97680, loss = 0.66 (1567.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:11:55.095777: step 97690, loss = 0.68 (1573.6 examples/sec; 0.081 sec/batch)
2017-05-06 23:11:56.003282: step 97700, loss = 0.63 (1410.5 examples/sec; 0.091 sec/batch)
2017-05-06 23:11:56.719830: step 97710, loss = 0.65 (1786.3 examples/sec; 0.072 sec/batch)
2017-05-06 23:11:57.543463: step 97720, loss = 0.68 (1554.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:11:58.367087: step 97730, loss = 0.73 (1554.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:11:59.186237: step 97740, loss = 0.79 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:11:59.988677: step 97750, loss = 0.58 (1595.1 examples/sec; 0.080 sec/batch)
2017-05-06 23:12:00.800835: step 97760, loss = 0.72 (1576.1 examples/sec; 0.081 sec/batch)
2017-05-06 23:12:01.619465: step 97770, loss = 0.67 (1563.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:12:02.435880: step 97780, loss = 0.76 (1567.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:12:03.259140: step 97790, loss = 0.81 (1554.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:12:04.171293: step 97800, loss = 0.68 (1403.3 examples/sec; 0.091 sec/batch)
2017-05-06 23:12:04.892781: step 97810, loss = 0.99 (1774.1 examples/sec; 0.072 sec/batch)
2017-05-06 23:12:05.714098: step 97820, loss = 0.89 (1558.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:12:06.533533: step 97830, loss = 0.77 (1562.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:12:07.353856: step 97840, loss = 0.75 (1560.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:12:08.168230: step 97850, loss = 0.80 (1571.7 examples/sec; 0.081 sec/batch)
2017-05-06 23:12:08.981504: step 97860, loss = 0.82 (1573.9 examples/sec; 0.081 sec/batch)
2017-05-06 23:12:09.794504: step 97870, loss = 0.63 (1574.4 examples/sec; 0.081 sec/batch)
2017-05-06 23:12:10.619377: step 97880, loss = 0.83 (1551.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:12:11.430906: step 97890, loss = 0.89 (1577.3 examples/sec; 0.081 sec/batch)
2017-05-06 23:12:12.345570: step 97900, loss = 0.80 (1399.4 examples/sec; 0.091 sec/batch)
2017-05-06 23:12:13.065869: step 97910, loss = 0.65 (1777.0 examples/sec; 0.072 sec/batch)
2017-05-06 23:12:13.894206: step 97920, loss = 0.80 (1545.3 examples/sec; 0.083 sec/batch)
2017-05-06 23:12:14.716219: step 97930, loss = 0.63 (1557.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:12:15.534726: step 97940, loss = 0.59 (1563.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:12:16.354148: step 97950, loss = 0.66 (1562.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:12:17.174290: step 97960, loss = 0.66 (1560.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:12:17.988163: step 97970, loss = 0.59 (1572.7 examples/sec; 0.081 sec/batch)
2017-05-06 23:12:18.804461: step 97980, loss = 0.63 (1568.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:12:19.604778: step 97990, loss = 0.71 (1599.4 examples/sec; 0.080 sec/batch)
2017-05-06 23:12:20.528439: step 98000, loss = 0.72 (1385.8 examples/sec; 0.092 sec/batch)
2017-05-06 23:12:21.269979: step 98010, loss = 0.73 (1726.1 examples/sec; 0.074 sec/batch)
2017-05-06 23:12:22.063117: step 98020, loss = 0.77 (1613.8 examples/sec; 0.079 sec/batch)
2017-05-06 23:12:22.869969: step 98030, loss = 0.63 (1586.4 examples/sec; 0.081 sec/batch)
2017-05-06 23:12:23.680267: step 98040, loss = 0.67 (1579.7 examples/sec; 0.081 sec/batch)
2017-05-06 23:12:24.497297: step 98050, loss = 0.83 (1566.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:12:25.317274: step 98060, loss = 0.64 (1561.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:12:26.140324: step 98070, loss = 0.76 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:12:26.960878: step 98080, loss = 0.67 (1559.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:12:27.770783: step 98090, loss = 0.68 (1580.4 examples/sec; 0.081 sec/batch)
2017-05-06 23:12:28.695745: step 98100, loss = 0.74 (1383.8 examples/sec; 0.092 sec/batch)
2017-05-06 23:12:29.427276: step 98110, loss = 0.71 (1749.8 examples/sec; 0.073 sec/batch)
2017-05-06 23:12:30.241170: step 98120, loss = 0.51 (1572.7 examples/sec; 0.081 sec/batch)
2017-05-06 23:12:31.056530: step 98130, loss = 0.62 (1569.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:12:31.875441: step 98140, loss = 0.60 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:12:32.687076: step 98150, loss = 0.72 (1577.1 examples/sec; 0.081 sec/batch)
2017-05-06 23:12:33.513073: step 98160, loss = 0.66 (1549.7 examples/sec; 0.083 sec/batch)
2017-05-06 23:12:34.325383: step 98170, loss = 0.63 (1575.7 examples/sec; 0.081 sec/batch)
2017-05-06 23:12:35.135685: step 98180, loss = 0.77 (1579.6 examples/sec; 0.081 sec/batch)
2017-05-06 23:12:35.942070: step 98190, loss = 0.83 (1587.3 examples/sec; 0.081 sec/batch)
2017-05-06 23:12:36.866770: step 98200, loss = 0.59 (1384.2 examples/sec; 0.092 sec/batch)
2017-05-06 23:12:37.592359: step 98210, loss = 0.72 (1764.1 examples/sec; 0.073 sec/batch)
2017-05-06 23:12:38.412035: step 98220, loss = 0.70 (1561.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:12:39.224486: step 98230, loss = 0.86 (1575.5 examples/sec; 0.081 sec/batch)
2017-05-06 23:12:40.033706: step 98240, loss = 0.80 (1581.8 examples/sec; 0.081 sec/batch)
2017-05-06 23:12:40.854580: step 98250, loss = 0.66 (1559.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:12:41.669921: step 98260, loss = 0.63 (1569.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:12:42.491771: step 98270, loss = 0.73 (1557.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:12:43.309004: step 98280, loss = 0.76 (1566.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:12:44.118646: step 98290, loss = 0.68 (1580.9 examples/sec; 0.081 sec/batch)
2017-05-06 23:12:45.028185: step 98300, loss = 0.68 (1407.3 examples/sec; 0.091 sec/batch)
2017-05-06 23:12:45.751608: step 98310, loss = 0.70 (1769.4 examples/sec; 0.072 sec/batch)
2017-05-06 23:12:46.579526: step 98320, loss = 0.57 (1546.0 examples/sec; 0.083 sec/batch)
2017-05-06 23:12:47.393241: step 98330, loss = 0.67 (1573.0 examples/sec; 0.081 sec/batch)
2017-05-06 23:12:48.209117: step 98340, loss = 0.69 (1568.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:12:49.031402: step 98350, loss = 0.55 (1556.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:12:49.849610: step 98360, loss = 0.62 (1564.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:12:50.668286: step 98370, loss = 0.73 (1563.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:12:51.479757: step 98380, loss = 0.73 (1577.4 examples/sec; 0.081 sec/batch)
2017-05-06 23:12:52.296762: step 98390, loss = 0.79 (1566.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:12:53.212575: step 98400, loss = 0.63 (1397.7 examples/sec; 0.092 sec/batch)
2017-05-06 23:12:53.946108: step 98410, loss = 0.68 (1745.0 examples/sec; 0.073 sec/batch)
2017-05-06 23:12:54.768669: step 98420, loss = 0.64 (1556.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:12:55.573291: step 98430, loss = 0.78 (1590.8 examples/sec; 0.080 sec/batch)
2017-05-06 23:12:56.392250: step 98440, loss = 0.77 (1563.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:12:57.211730: step 98450, loss = 0.64 (1562.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:12:58.039350: step 98460, loss = 0.78 (1546.6 examples/sec; 0.083 sec/batch)
2017-05-06 23:12:58.849826: step 98470, loss = 0.78 (1579.3 examples/sec; 0.081 sec/batch)
2017-05-06 23:12:59.662652: step 98480, loss = 0.79 (1574.7 examples/sec; 0.081 sec/batch)
2017-05-06 23:13:00.482912: step 98490, loss = 0.74 (1560.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:01.399255: step 98500, loss = 0.79 (1396.9 examples/sec; 0.092 sec/batch)
2017-05-06 23:13:02.129871: step 98510, loss = 0.80 (1752.0 examples/sec; 0.073 sec/batch)
2017-05-06 23:13:02.945810: step 98520, loss = 0.76 (1568.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:03.763402: step 98530, loss = 0.58 (1565.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:04.575933: step 98540, loss = 0.73 (1575.3 examples/sec; 0.081 sec/batch)
2017-05-06 23:13:05.411041: step 98550, loss = 0.68 (1532.7 examples/sec; 0.084 sec/batch)
2017-05-06 23:13:06.229348: step 98560, loss = 0.67 (1564.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:07.043605: step 98570, loss = 0.88 (1572.0 examples/sec; 0.081 sec/batch)
2017-05-06 23:13:07.855147: step 98580, loss = 0.72 (1577.3 examples/sec; 0.081 sec/batch)
2017-05-06 23:13:08.672121: step 98590, loss = 0.61 (1566.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:09.595556: step 98600, loss = 0.80 (1386.1 examples/sec; 0.092 sec/batch)
2017-05-06 23:13:10.317288: step 98610, loss = 0.66 (1773.5 examples/sec; 0.072 sec/batch)
2017-05-06 23:13:11.135384: step 98620, loss = 0.74 (1564.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:11.945631: step 98630, loss = 0.71 (1579.8 examples/sec; 0.081 sec/batch)
2017-05-06 23:13:12.754701: step 98640, loss = 0.72 (1582.1 examples/sec; 0.081 sec/batch)
2017-05-06 23:13:13.576329: step 98650, loss = 0.73 (1557.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:14.400704: step 98660, loss = 0.86 (1552.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:15.223554: step 98670, loss = 0.66 (1555.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:16.032747: step 98680, loss = 0.68 (1581.8 examples/sec; 0.081 sec/batch)
2017-05-06 23:13:16.850272: step 98690, loss = 0.76 (1565.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:17.767282: step 98700, loss = 0.60 (1395.8 examples/sec; 0.092 sec/batch)
2017-05-06 23:13:18.495869: step 98710, loss = 0.79 (1756.8 examples/sec; 0.073 sec/batch)
2017-05-06 23:13:19.322733: step 98720, loss = 0.74 (1548.0 examples/sec; 0.083 sec/batch)
2017-05-06 23:13:20.132877: step 98730, loss = 0.79 (1580.0 examples/sec; 0.081 sec/batch)
2017-05-06 23:13:20.946238: step 98740, loss = 0.66 (1573.7 examples/sec; 0.081 sec/batch)
2017-05-06 23:13:21.768510: step 98750, loss = 0.66 (1556.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:22.584449: step 98760, loss = 0.58 (1568.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:23.398821: step 98770, loss = 0.85 (1571.8 examples/sec; 0.081 sec/batch)
2017-05-06 23:13:24.225409: step 98780, loss = 0.66 (1548.5 examples/sec; 0.083 sec/batch)
2017-05-06 23:13:25.040478: step 98790, loss = 0.65 (1570.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:25.954119: step 98800, loss = 0.78 (1401.0 examples/sec; 0.091 sec/batch)
2017-05-06 23:13:26.681744: step 98810, loss = 0.75 (1759.1 examples/sec; 0.073 sec/batch)
2017-05-06 23:13:27.488713: step 98820, loss = 0.78 (1586.2 examples/sec; 0.081 sec/batch)
2017-05-06 23:13:28.295217: step 98830, loss = 0.56 (1587.1 examples/sec; 0.081 sec/batch)
2017-05-06 23:13:29.116918: step 98840, loss = 0.74 (1557.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:29.931915: step 98850, loss = 0.75 (1570.6 examples/sec; 0.081 sec/batch)
2017-05-06 23:13:30.748260: step 98860, loss = 0.74 (1568.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:31.563735: step 98870, loss = 0.66 (1569.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:32.379815: step 98880, loss = 0.68 (1568.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:33.197867: step 98890, loss = 0.76 (1564.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:34.131222: step 98900, loss = 0.72 (1371.4 examples/sec; 0.093 sec/batch)
2017-05-06 23:13:34.832810: step 98910, loss = 0.68 (1824.4 examples/sec; 0.070 sec/batch)
2017-05-06 23:13:35.642471: step 98920, loss = 0.66 (1580.9 examples/sec; 0.081 sec/batch)
2017-05-06 23:13:36.461464: step 98930, loss = 0.77 (1562.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:37.277231: step 98940, loss = 0.64 (1569.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:38.101756: step 98950, loss = 0.65 (1552.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:38.911677: step 98960, loss = 0.73 (1580.3 examples/sec; 0.081 sec/batch)
2017-05-06 23:13:39.720690: step 98970, loss = 0.66 (1582.2 examples/sec; 0.081 sec/batch)
2017-05-06 23:13:40.537785: step 98980, loss = 0.70 (1566.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:41.356639: step 98990, loss = 0.60 (1563.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:42.299539: step 99000, loss = 0.65 (1357.5 examples/sec; 0.094 sec/batch)
2017-05-06 23:13:43.009117: step 99010, loss = 0.61 (1803.9 examples/sec; 0.071 sec/batch)
2017-05-06 23:13:43.820924: step 99020, loss = 0.81 (1576.7 examples/sec; 0.081 sec/batch)
2017-05-06 23:13:44.641115: step 99030, loss = 0.82 (1560.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:45.462256: step 99040, loss = 0.82 (1558.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:46.280456: step 99050, loss = 0.74 (1564.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:47.095773: step 99060, loss = 0.70 (1569.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:47.911688: step 99070, loss = 0.67 (1568.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:48.728886: step 99080, loss = 0.63 (1566.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:49.546068: step 99090, loss = 0.75 (1566.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:50.476633: step 99100, loss = 0.53 (1375.5 examples/sec; 0.093 sec/batch)
2017-05-06 23:13:51.190036: step 99110, loss = 0.65 (1794.2 examples/sec; 0.071 sec/batch)
2017-05-06 23:13:51.994816: step 99120, loss = 0.58 (1590.5 examples/sec; 0.080 sec/batch)
2017-05-06 23:13:52.812946: step 99130, loss = 0.74 (1564.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:53.631462: step 99140, loss = 0.65 (1563.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:54.449135: step 99150, loss = 0.66 (1565.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:55.266569: step 99160, loss = 0.60 (1565.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:56.082900: step 99170, loss = 0.69 (1568.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:56.902667: step 99180, loss = 0.75 (1561.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:57.718375: step 99190, loss = 0.67 (1569.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:13:58.653307: step 99200, loss = 0.68 (1369.1 examples/sec; 0.093 sec/batch)
2017-05-06 23:13:59.363837: step 99210, loss = 0.73 (1801.5 examples/sec; 0.071 sec/batch)
2017-05-06 23:14:00.186746: step 99220, loss = 0.71 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:01.011864: step 99230, loss = 0.75 (1551.3 examples/sec; 0.083 sec/batch)
2017-05-06 23:14:01.830112: step 99240, loss = 0.68 (1564.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:02.656019: step 99250, loss = 0.70 (1549.8 examples/sec; 0.083 sec/batch)
2017-05-06 23:14:03.469773: step 99260, loss = 0.56 (1573.0 examples/sec; 0.081 sec/batch)
2017-05-06 23:14:04.280806: step 99270, loss = 0.76 (1578.2 examples/sec; 0.081 sec/batch)
2017-05-06 23:14:05.097471: step 99280, loss = 0.64 (1567.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:05.916659: step 99290, loss = 0.70 (1562.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:06.848088: step 99300, loss = 0.70 (1374.2 examples/sec; 0.093 sec/batch)
2017-05-06 23:14:07.565498: step 99310, loss = 0.76 (1784.2 examples/sec; 0.072 sec/batch)
2017-05-06 23:14:08.378218: step 99320, loss = 0.69 (1575.0 examples/sec; 0.081 sec/batch)
2017-05-06 23:14:09.198735: step 99330, loss = 0.56 (1560.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:10.009158: step 99340, loss = 0.69 (1579.4 examples/sec; 0.081 sec/batch)
2017-05-06 23:14:10.823942: step 99350, loss = 0.76 (1571.0 examples/sec; 0.081 sec/batch)
2017-05-06 23:14:11.636722: step 99360, loss = 0.69 (1574.8 examples/sec; 0.081 sec/batch)
2017-05-06 23:14:12.454747: step 99370, loss = 0.71 (1564.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:13.276776: step 99380, loss = 0.72 (1557.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:14.096483: step 99390, loss = 0.62 (1561.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:15.015154: step 99400, loss = 0.81 (1393.3 examples/sec; 0.092 sec/batch)
2017-05-06 23:14:15.725855: step 99410, loss = 0.90 (1801.0 examples/sec; 0.071 sec/batch)
2017-05-06 23:14:16.546949: step 99420, loss = 0.70 (1558.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:17.366712: step 99430, loss = 0.57 (1561.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:18.187973: step 99440, loss = 0.67 (1558.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:19.012820: step 99450, loss = 0.62 (1551.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:19.818332: step 99460, loss = 0.88 (1589.1 examples/sec; 0.081 sec/batch)
2017-05-06 23:14:20.639486: step 99470, loss = 0.68 (1558.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:21.458636: step 99480, loss = 0.60 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:22.282011: step 99490, loss = 0.73 (1554.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:23.202952: step 99500, loss = 0.60 (1389.9 examples/sec; 0.092 sec/batch)
2017-05-06 23:14:23.919880: step 99510, loss = 0.68 (1785.4 examples/sec; 0.072 sec/batch)
2017-05-06 23:14:24.742924: step 99520, loss = 0.89 (1555.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:25.569185: step 99530, loss = 0.72 (1549.1 examples/sec; 0.083 sec/batch)
2017-05-06 23:14:26.386108: step 99540, loss = 0.73 (1566.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:27.208851: step 99550, loss = 0.65 (1555.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:28.020271: step 99560, loss = 0.69 (1577.5 examples/sec; 0.081 sec/batch)
2017-05-06 23:14:28.841172: step 99570, loss = 0.76 (1559.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:29.666132: step 99580, loss = 0.72 (1551.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:30.484625: step 99590, loss = 0.83 (1563.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:31.424921: step 99600, loss = 0.94 (1361.3 examples/sec; 0.094 sec/batch)
2017-05-06 23:14:32.112328: step 99610, loss = 0.74 (1862.1 examples/sec; 0.069 sec/batch)
2017-05-06 23:14:32.927591: step 99620, loss = 0.73 (1570.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:33.749584: step 99630, loss = 0.74 (1557.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:34.568683: step 99640, loss = 0.68 (1562.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:35.391351: step 99650, loss = 0.71 (1555.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:36.204791: step 99660, loss = 0.63 (1573.6 examples/sec; 0.081 sec/batch)
2017-05-06 23:14:37.025744: step 99670, loss = 0.62 (1559.2 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:37.850208: step 99680, loss = 0.69 (1552.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:38.667969: step 99690, loss = 0.76 (1565.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:39.586653: step 99700, loss = 0.74 (1393.3 examples/sec; 0.092 sec/batch)
2017-05-06 23:14:40.298071: step 99710, loss = 0.70 (1799.2 examples/sec; 0.071 sec/batch)
2017-05-06 23:14:41.127374: step 99720, loss = 0.72 (1543.5 examples/sec; 0.083 sec/batch)
2017-05-06 23:14:41.950785: step 99730, loss = 0.74 (1554.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:42.771377: step 99740, loss = 0.82 (1559.8 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:43.587913: step 99750, loss = 0.77 (1567.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:44.408345: step 99760, loss = 0.79 (1560.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:45.227489: step 99770, loss = 0.93 (1562.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:46.054629: step 99780, loss = 0.71 (1547.5 examples/sec; 0.083 sec/batch)
2017-05-06 23:14:46.867815: step 99790, loss = 0.74 (1574.1 examples/sec; 0.081 sec/batch)
2017-05-06 23:14:47.795147: step 99800, loss = 0.75 (1380.3 examples/sec; 0.093 sec/batch)
2017-05-06 23:14:48.494384: step 99810, loss = 0.74 (1830.6 examples/sec; 0.070 sec/batch)
2017-05-06 23:14:49.320373: step 99820, loss = 0.72 (1549.7 examples/sec; 0.083 sec/batch)
2017-05-06 23:14:50.145948: step 99830, loss = 0.65 (1550.4 examples/sec; 0.083 sec/batch)
2017-05-06 23:14:50.967306: step 99840, loss = 0.58 (1558.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:51.769349: step 99850, loss = 0.71 (1595.9 examples/sec; 0.080 sec/batch)
2017-05-06 23:14:52.592150: step 99860, loss = 0.83 (1555.7 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:53.416586: step 99870, loss = 0.69 (1552.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:54.238994: step 99880, loss = 0.68 (1556.4 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:55.061113: step 99890, loss = 0.76 (1557.0 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:55.976259: step 99900, loss = 0.69 (1398.7 examples/sec; 0.092 sec/batch)
2017-05-06 23:14:56.713035: step 99910, loss = 0.75 (1737.3 examples/sec; 0.074 sec/batch)
2017-05-06 23:14:57.535932: step 99920, loss = 0.62 (1555.5 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:58.360719: step 99930, loss = 0.72 (1551.9 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:59.177224: step 99940, loss = 0.63 (1567.6 examples/sec; 0.082 sec/batch)
2017-05-06 23:14:59.993494: step 99950, loss = 0.64 (1568.1 examples/sec; 0.082 sec/batch)
2017-05-06 23:15:00.814903: step 99960, loss = 0.99 (1558.3 examples/sec; 0.082 sec/batch)
2017-05-06 23:15:01.650387: step 99970, loss = 0.66 (1532.1 examples/sec; 0.084 sec/batch)
2017-05-06 23:15:02.465015: step 99980, loss = 0.76 (1571.3 examples/sec; 0.081 sec/batch)
2017-05-06 23:15:03.311564: step 99990, loss = 0.60 (1512.0 examples/sec; 0.085 sec/batch)
2017-05-06 23:15:04.193578: step 100000, loss = 0.69 (1451.2 examples/sec; 0.088 sec/batch)
--- 8224.66697216 seconds ---
