Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Received variable values from ps
Connected to both PSs
2017-05-09 20:59:39.850280: step 0, loss = 4.68 (92.9 examples/sec; 1.377 sec/batch)
2017-05-09 20:59:40.786288: step 10, loss = 4.63 (1367.5 examples/sec; 0.094 sec/batch)
2017-05-09 20:59:41.936245: step 20, loss = 4.58 (1113.1 examples/sec; 0.115 sec/batch)
2017-05-09 20:59:43.091665: step 30, loss = 4.52 (1107.8 examples/sec; 0.116 sec/batch)
2017-05-09 20:59:44.252426: step 40, loss = 5.18 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-09 20:59:45.423579: step 50, loss = 4.50 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-09 20:59:46.574875: step 60, loss = 4.41 (1111.8 examples/sec; 0.115 sec/batch)
2017-05-09 20:59:47.733010: step 70, loss = 4.29 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-09 20:59:48.878315: step 80, loss = 4.29 (1117.6 examples/sec; 0.115 sec/batch)
2017-05-09 20:59:50.046696: step 90, loss = 4.23 (1095.5 examples/sec; 0.117 sec/batch)
2017-05-09 20:59:51.334530: step 100, loss = 4.19 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 20:59:52.321436: step 110, loss = 4.13 (1297.0 examples/sec; 0.099 sec/batch)
2017-05-09 20:59:53.611757: step 120, loss = 4.31 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 20:59:54.882260: step 130, loss = 4.12 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-09 20:59:56.157404: step 140, loss = 4.16 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 20:59:57.458874: step 150, loss = 4.00 (983.5 examples/sec; 0.130 sec/batch)
2017-05-09 20:59:58.771239: step 160, loss = 3.88 (975.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:00:00.097201: step 170, loss = 4.01 (965.3 examples/sec; 0.133 sec/batch)
2017-05-09 21:00:01.384260: step 180, loss = 3.89 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:00:02.690846: step 190, loss = 3.90 (979.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:00:04.098776: step 200, loss = 3.86 (909.1 examples/sec; 0.141 sec/batch)
2017-05-09 21:00:05.260101: step 210, loss = 3.93 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-09 21:00:06.561227: step 220, loss = 3.61 (983.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:00:07.866216: step 230, loss = 3.61 (980.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:00:09.162945: step 240, loss = 3.53 (987.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:00:10.469872: step 250, loss = 3.62 (979.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:00:11.757017: step 260, loss = 3.48 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:00:13.053608: step 270, loss = 3.39 (987.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:00:14.377265: step 280, loss = 4.02 (967.0 examples/sec; 0.132 sec/batch)
2017-05-09 21:00:15.679861: step 290, loss = 3.36 (982.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:00:17.111620: step 300, loss = 3.36 (894.0 examples/sec; 0.143 sec/batch)
2017-05-09 21:00:18.339162: step 310, loss = 3.39 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-09 21:00:19.623710: step 320, loss = 3.45 (996.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:00:20.924877: step 330, loss = 3.34 (983.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:00:22.203385: step 340, loss = 3.19 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:00:23.492074: step 350, loss = 3.06 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:00:24.788988: step 360, loss = 3.42 (987.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:00:26.110165: step 370, loss = 3.20 (968.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:00:27.396310: step 380, loss = 3.24 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:00:28.675788: step 390, loss = 3.24 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:00:30.085252: step 400, loss = 3.17 (908.1 examples/sec; 0.141 sec/batch)
2017-05-09 21:00:31.318391: step 410, loss = 3.11 (1038.0 examples/sec; 0.123 sec/batch)
2017-05-09 21:00:32.601989: step 420, loss = 3.11 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:00:33.886683: step 430, loss = 3.20 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:00:35.198125: step 440, loss = 2.84 (976.0 examples/sec;E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 23 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
 0.131 sec/batch)
2017-05-09 21:00:36.487066: step 450, loss = 2.93 (993.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:00:37.798058: step 460, loss = 2.92 (976.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:00:39.109816: step 470, loss = 3.00 (975.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:00:40.432528: step 480, loss = 2.77 (967.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:00:41.716935: step 490, loss = 3.14 (996.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:00:43.163209: step 500, loss = 2.85 (885.0 examples/sec; 0.145 sec/batch)
2017-05-09 21:00:44.373379: step 510, loss = 2.78 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-09 21:00:45.657384: step 520, loss = 3.09 (996.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:00:46.971089: step 530, loss = 2.73 (974.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:00:48.254042: step 540, loss = 2.65 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:00:49.559069: step 550, loss = 2.70 (980.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:00:50.860298: step 560, loss = 2.57 (983.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:00:52.146904: step 570, loss = 2.68 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:00:53.474818: step 580, loss = 2.55 (963.9 examples/sec; 0.133 sec/batch)
2017-05-09 21:00:54.782311: step 590, loss = 2.51 (979.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:00:56.178366: step 600, loss = 2.64 (916.9 examples/sec; 0.140 sec/batch)
2017-05-09 21:00:57.377374: step 610, loss = 2.87 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-09 21:00:58.663323: step 620, loss = 2.70 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:00:59.960171: step 630, loss = 2.76 (987.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:01:01.285084: step 640, loss = 2.53 (966.1 examples/sec; 0.132 sec/batch)
2017-05-09 21:01:02.596568: step 650, loss = 2.53 (976.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:01:03.880097: step 660, loss = 2.48 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:01:05.167720: step 670, loss = 2.33 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:01:06.449863: step 680, loss = 2.37 (998.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:01:07.732713: step 690, loss = 2.61 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:01:09.151625: step 700, loss = 2.43 (902.1 examples/sec; 0.142 sec/batch)
2017-05-09 21:01:10.334314: step 710, loss = 2.28 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-09 21:01:11.619972: step 720, loss = 2.27 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:01:12.917043: step 730, loss = 2.62 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:01:14.265767: step 740, loss = 2.45 (949.0 examples/sec; 0.135 sec/batch)
2017-05-09 21:01:15.553597: step 750, loss = 2.32 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:01:16.851766: step 760, loss = 2.75 (986.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:01:18.128012: step 770, loss = 2.26 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:01:19.449226: step 780, loss = 2.36 (968.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:01:20.753145: step 790, loss = 2.06 (981.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:01:22.142145: step 800, loss = 2.23 (921.5 examples/sec; 0.139 sec/batch)
2017-05-09 21:01:23.318156: step 810, loss = 2.25 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-09 21:01:24.587482: step 820, loss = 2.08 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:01:25.903176: step 830, loss = 2.02 (972.9 examples/sec; 0.132 sec/batch)
2017-05-09 21:01:27.170177: step 840, loss = 2.15 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:01:28.460103: step 850, loss = 2.36 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:01:29.754234: step 860, loss = 2.20 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:01:31.070348: step 870, loss = 2.06 (972.6 examples/sec; 0.132 sec/batch)
2017-05-09 21:01:32.380999: step 880, loss = 2.17 (976.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:01:33.665708: step 890, loss = 2.07 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:01:35.070839: step 900, loss = 1.99 (910.9 examples/sec; 0.141 sec/batch)
2017-05-09 21:01:36.237872: step 910, loss = 2.24 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-09 21:01:37.531197: step 920, loss = 1.90 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:01:38.797081: step 930, loss = 2.16 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:01:40.099565: step 940, loss = 1.83 (982.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:01:41.448781: step 950, loss = 2.09 (948.7 examples/sec; 0.135 sec/batch)
2017-05-09 21:01:42.738876: step 960, loss = 1.86 (992.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:01:44.039848: step 970, loss = 2.15 (983.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:01:45.331024: step 980, loss = 1.89 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:01:46.616936: step 990, loss = 2.01 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:01:48.034845: step 1000, loss = 1.98 (902.7 examples/sec; 0.142 sec/batch)
2017-05-09 21:01:49.233967: step 1010, loss = 2.16 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-09 21:01:50.511637: step 1020, loss = 1.82 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:01:51.812182: step 1030, loss = 2.01 (984.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:01:53.150843: step 1040, loss = 1.99 (956.2 examples/sec; 0.134 sec/batch)
2017-05-09 21:01:54.462094: step 1050, loss = 2.25 (976.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:01:55.746435: step 1060, loss = 1.94 (996.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:01:57.033002: step 1070, loss = 1.88 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:01:58.321693: step 1080, loss = 1.80 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:01:59.609671: step 1090, loss = 1.77 (993.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:02:01.004643: step 1100, loss = 1.99 (917.6 examples/sec; 0.139 sec/batch)
2017-05-09 21:02:02.173536: step 1110, loss = 1.71 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-09 21:02:03.459059: step 1120, loss = 1.87 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:02:04.742208: step 1130, loss = 1.65 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:02:06.040690: step 1140, loss = 2.00 (985.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:02:07.337426: step 1150, loss = 1.74 (987.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:02:08.644690: step 1160, loss = 2.31 (979.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:02:09.960679: step 1170, loss = 1.93 (972.6 examples/sec; 0.132 sec/batch)
2017-05-09 21:02:11.266853: step 1180, loss = 1.90 (980.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:02:12.582540: step 1190, loss = 1.72 (972.9 examples/sec; 0.132 sec/batch)
2017-05-09 21:02:13.991010: step 1200, loss = 1.69 (908.8 examples/sec; 0.141 sec/batch)
2017-05-09 21:02:15.199435: step 1210, loss = 1.90 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-09 21:02:16.510786: step 1220, loss = 1.66 (976.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:02:17.822368: step 1230, loss = 1.83 (975.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:02:19.145957: step 1240, loss = 1.66 (967.1 examples/sec; 0.132 sec/batch)
2017-05-09 21:02:20.431673: step 1250, loss = 1.77 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:02:21.718718: step 1260, loss = 1.53 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:02:23.016973: step 1270, loss = 1.60 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:02:24.306154: step 1280, loss = 1.86 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:02:25.605997: step 1290, loss = 1.63 (984.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:02:26.995899: step 1300, loss = 1.60 (920.9 examples/sec; 0.139 sec/batch)
2017-05-09 21:02:28.176298: step 1310, loss = 1.60 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-09 21:02:29.498033: step 1320, loss = 1.71 (968.4 examples/sec; 0.132 sec/batch)
2017-05-09 21:02:30.789910: step 1330, loss = 1.49 (990.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:02:32.077417: step 1340, loss = 1.60 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:02:33.394729: step 1350, loss = 1.68 (971.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:02:34.691379: step 1360, loss = 1.49 (987.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:02:36.012012: step 1370, loss = 1.84 (969.2 examples/sec; 0.132 sec/batch)
2017-05-09 21:02:37.322060: step 1380, loss = 1.86 (977.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:02:38.627847: step 1390, loss = 1.53 (980.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:02:40.054557: step 1400, loss = 1.33 (897.2 examples/sec; 0.143 sec/batch)
2017-05-09 21:02:41.253411: step 1410, loss = 1.63 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-09 21:02:42.557400: step 1420, loss = 1.62 (981.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:02:43.887979: step 1430, loss = 2.01 (962.0 examples/sec; 0.133 sec/batch)
2017-05-09 21:02:45.180301: step 1440, loss = 1.66 (990.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:02:46.470301: step 1450, loss = 1.50 (992.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:02:47.761802: step 1460, loss = 1.43 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:02:49.069514: step 1470, loss = 1.49 (978.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:02:50.386614: step 1480, loss = 1.57 (971.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:02:51.687431: step 1490, loss = 1.53 (984.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:02:53.101107: step 1500, loss = 1.58 (905.4 examples/sec; 0.141 sec/batch)
2017-05-09 21:02:54.297324: step 1510, loss = 2.00 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-09 21:02:55.581123: step 1520, loss = 1.41 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:02:56.890270: step 1530, loss = 1.45 (977.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:02:58.181369: step 1540, loss = 1.50 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:02:59.470011: step 1550, loss = 1.44 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:03:00.753564: step 1560, loss = 1.49 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:03:02.040851: step 1570, loss = 1.42 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:03:03.377702: step 1580, loss = 1.76 (957.5 examples/sec; 0.134 sec/batch)
2017-05-09 21:03:04.674930: step 1590, loss = 1.48 (986.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:03:06.057592: step 1600, loss = 1.39 (925.7 examples/sec; 0.138 sec/batch)
2017-05-09 21:03:07.269168: step 1610, loss = 1.46 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-09 21:03:08.544037: step 1620, loss = 1.48 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:03:09.838501: step 1630, loss = 1.40 (988.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:03:11.146097: step 1640, loss = 1.46 (978.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:03:12.439234: step 1650, loss = 1.69 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:03:13.744684: step 1660, loss = 1.36 (980.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:03:15.035494: step 1670, loss = 1.47 (991.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:03:16.334311: step 1680, loss = 1.61 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:03:17.641329: step 1690, loss = 1.29 (979.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:03:19.025885: step 1700, loss = 1.32 (924.5 examples/sec; 0.138 sec/batch)
2017-05-09 21:03:20.238082: step 1710, loss = 1.49 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-09 21:03:21.556219: step 1720, loss = 1.50 (971.1 examples/sec; 0.132 sec/batch)
2017-05-09 21:03:22.859329: step 1730, loss = 1.54 (982.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:03:24.127922: step 1740, loss = 1.40 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:03:25.460053: step 1750, loss = 1.59 (960.9 examples/sec; 0.133 sec/batch)
2017-05-09 21:03:26.742852: step 1760, loss = 1.47 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:03:28.033850: step 1770, loss = 1.36 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:03:29.356506: step 1780, loss = 1.53 (967.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:03:30.654041: step 1790, loss = 1.32 (986.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:03:32.070271: step 1800, loss = 1.16 (903.8 examples/sec; 0.142 sec/batch)
2017-05-09 21:03:33.233824: step 1810, loss = 1.24 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-09 21:03:34.522953: step 1820, loss = 1.46 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:03:35.80E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 43 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
9590: step 1830, loss = 1.52 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:03:37.096281: step 1840, loss = 1.47 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:03:38.397032: step 1850, loss = 1.40 (984.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:03:39.686688: step 1860, loss = 1.28 (992.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:03:40.975499: step 1870, loss = 1.21 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:03:42.314584: step 1880, loss = 1.43 (955.9 examples/sec; 0.134 sec/batch)
2017-05-09 21:03:43.594161: step 1890, loss = 1.47 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:03:44.985153: step 1900, loss = 1.31 (920.2 examples/sec; 0.139 sec/batch)
2017-05-09 21:03:46.170991: step 1910, loss = 1.34 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-09 21:03:47.459533: step 1920, loss = 1.57 (993.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:03:48.750751: step 1930, loss = 1.24 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:03:50.064136: step 1940, loss = 1.40 (974.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:03:51.358153: step 1950, loss = 1.26 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:03:52.678252: step 1960, loss = 1.15 (969.6 examples/sec; 0.132 sec/batch)
2017-05-09 21:03:53.945419: step 1970, loss = 1.18 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:03:55.243735: step 1980, loss = 1.46 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:03:56.526365: step 1990, loss = 1.19 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:03:57.904882: step 2000, loss = 1.26 (928.5 examples/sec; 0.138 sec/batch)
2017-05-09 21:03:59.102281: step 2010, loss = 1.23 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-09 21:04:00.412336: step 2020, loss = 1.44 (977.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:04:01.741891: step 2030, loss = 1.62 (962.7 examples/sec; 0.133 sec/batch)
2017-05-09 21:04:03.023776: step 2040, loss = 1.33 (998.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:04:04.315734: step 2050, loss = 1.14 (990.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:04:05.610852: step 2060, loss = 1.41 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:04:06.942896: step 2070, loss = 1.28 (960.9 examples/sec; 0.133 sec/batch)
2017-05-09 21:04:08.236575: step 2080, loss = 1.16 (989.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:04:09.528697: step 2090, loss = 1.25 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:04:10.940697: step 2100, loss = 1.38 (906.5 examples/sec; 0.141 sec/batch)
2017-05-09 21:04:12.134866: step 2110, loss = 1.27 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-09 21:04:13.434259: step 2120, loss = 1.21 (985.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:04:14.730872: step 2130, loss = 1.33 (987.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:04:16.055258: step 2140, loss = 1.35 (966.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:04:17.367400: step 2150, loss = 1.19 (975.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:04:18.680467: step 2160, loss = 1.39 (974.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:04:19.972404: step 2170, loss = 1.14 (990.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:04:21.282828: step 2180, loss = 1.24 (976.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:04:22.606347: step 2190, loss = 1.24 (967.1 examples/sec; 0.132 sec/batch)
2017-05-09 21:04:23.999150: step 2200, loss = 1.21 (919.0 examples/sec; 0.139 sec/batch)
2017-05-09 21:04:25.190629: step 2210, loss = 1.07 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-09 21:04:26.499273: step 2220, loss = 1.24 (978.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:04:27.781087: step 2230, loss = 1.26 (998.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:04:29.076887: step 2240, loss = 1.19 (987.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:04:30.358833: step 2250, loss = 1.19 (998.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:04:31.655314: step 2260, loss = 1.27 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:04:32.964027: step 2270, loss = 1.23 (978.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:04:34.274339: step 2280, loss = 1.29 (976.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:04:35.572354: step 2290, loss = 1.16 (986.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:04:36.965227: step 2300, loss = 1.22 (919.0 examples/sec; 0.139 sec/batch)
2017-05-09 21:04:38.146103: step 2310, loss = 1.02 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-09 21:04:39.408002: step 2320, loss = 1.35 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-09 21:04:40.690184: step 2330, loss = 1.40 (998.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:04:41.999751: step 2340, loss = 1.39 (977.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:04:43.294430: step 2350, loss = 1.41 (988.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:04:44.591736: step 2360, loss = 1.29 (986.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:04:45.890601: step 2370, loss = 1.29 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:04:47.193141: step 2380, loss = 1.01 (982.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:04:48.481492: step 2390, loss = 1.03 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:04:49.871878: step 2400, loss = 1.28 (920.6 examples/sec; 0.139 sec/batch)
2017-05-09 21:04:51.069296: step 2410, loss = 1.29 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-09 21:04:52.356990: step 2420, loss = 1.19 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:04:53.636475: step 2430, loss = 1.14 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:04:54.940702: step 2440, loss = 1.32 (981.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:04:56.236697: step 2450, loss = 1.27 (987.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:04:57.561955: step 2460, loss = 1.44 (965.8 examples/sec; 0.133 sec/batch)
2017-05-09 21:04:58.864693: step 2470, loss = 1.34 (982.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:05:00.150594: step 2480, loss = 1.07 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:05:01.444319: step 2490, loss = 1.27 (989.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:05:02.841788: step 2500, loss = 1.19 (915.9 examples/sec; 0.140 sec/batch)
2017-05-09 21:05:04.052334: step 2510, loss = 1.23 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-09 21:05:05.354964: step 2520, loss = 1.14 (982.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:05:06.678301: step 2530, loss = 1.60 (967.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:05:07.985322: step 2540, loss = 1.30 (979.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:05:09.256597: step 2550, loss = 1.45 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:05:10.555326: step 2560, loss = 1.31 (985.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:05:11.862229: step 2570, loss = 1.28 (979.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:05:13.144776: step 2580, loss = 1.20 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:05:14.428551: step 2590, loss = 1.07 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:05:15.824087: step 2600, loss = 1.17 (917.2 examples/sec; 0.140 sec/batch)
2017-05-09 21:05:17.013689: step 2610, loss = 1.18 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-09 21:05:18.336580: step 2620, loss = 1.48 (967.6 examples/sec; 0.132 sec/batch)
2017-05-09 21:05:19.643640: step 2630, loss = 1.06 (979.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:05:20.939571: step 2640, loss = 1.28 (987.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:05:22.215967: step 2650, loss = 1.09 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:05:23.504767: step 2660, loss = 1.30 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:05:24.813150: step 2670, loss = 1.18 (978.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:05:26.101177: step 2680, loss = 1.35 (993.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:05:27.409835: step 2690, loss = 1.31 (978.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:05:28.799064: step 2700, loss = 1.21 (921.4 examples/sec; 0.139 sec/batch)
2017-05-09 21:05:30.001422: step 2710, loss = 1.16 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-09 21:05:31.318329: step 2720, loss = 1.13 (972.0 examples/sec; 0.132 sec/batch)
2017-05-09 21:05:32.609558: step 2730, loss = 1.08 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:05:33.901398: step 2740, loss = 1.12 (990.8 examples/sec; 0.129 sec/batch)
2017-05-09 E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 63 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
21:05:35.206835: step 2750, loss = 1.10 (980.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:05:36.528429: step 2760, loss = 1.04 (968.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:05:37.829121: step 2770, loss = 1.30 (984.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:05:39.124257: step 2780, loss = 1.01 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:05:40.453873: step 2790, loss = 1.26 (962.7 examples/sec; 0.133 sec/batch)
2017-05-09 21:05:41.855176: step 2800, loss = 1.30 (913.4 examples/sec; 0.140 sec/batch)
2017-05-09 21:05:43.103615: step 2810, loss = 1.43 (1025.3 examples/sec; 0.125 sec/batch)
2017-05-09 21:05:44.408761: step 2820, loss = 1.77 (980.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:05:45.699700: step 2830, loss = 0.99 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:05:47.023612: step 2840, loss = 1.09 (966.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:05:48.323230: step 2850, loss = 1.09 (984.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:05:49.628534: step 2860, loss = 1.20 (980.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:05:50.924561: step 2870, loss = 1.04 (987.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:05:52.203918: step 2880, loss = 1.19 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:05:53.497140: step 2890, loss = 1.19 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:05:54.897660: step 2900, loss = 1.33 (913.9 examples/sec; 0.140 sec/batch)
2017-05-09 21:05:56.110669: step 2910, loss = 1.12 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-09 21:05:57.420485: step 2920, loss = 0.99 (977.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:05:58.749402: step 2930, loss = 1.29 (963.2 examples/sec; 0.133 sec/batch)
2017-05-09 21:06:00.049144: step 2940, loss = 1.10 (984.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:06:01.352184: step 2950, loss = 0.97 (982.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:06:02.631119: step 2960, loss = 0.94 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:06:03.911912: step 2970, loss = 1.37 (999.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:06:05.195731: step 2980, loss = 1.20 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:06:06.509543: step 2990, loss = 1.18 (974.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:06:07.917418: step 3000, loss = 1.09 (909.2 examples/sec; 0.141 sec/batch)
2017-05-09 21:06:09.119199: step 3010, loss = 0.89 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-09 21:06:10.395487: step 3020, loss = 1.09 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:06:11.690725: step 3030, loss = 1.14 (988.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:06:12.998698: step 3040, loss = 1.16 (978.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:06:14.286529: step 3050, loss = 1.06 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:06:15.563279: step 3060, loss = 1.09 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:06:16.864315: step 3070, loss = 1.22 (983.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:06:18.177701: step 3080, loss = 1.37 (974.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:06:19.494368: step 3090, loss = 1.08 (972.2 examples/sec; 0.132 sec/batch)
2017-05-09 21:06:20.898347: step 3100, loss = 1.15 (911.7 examples/sec; 0.140 sec/batch)
2017-05-09 21:06:22.113607: step 3110, loss = 1.00 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-09 21:06:23.406837: step 3120, loss = 1.23 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:06:24.687877: step 3130, loss = 1.22 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:06:26.000977: step 3140, loss = 1.17 (974.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:06:27.302133: step 3150, loss = 0.94 (983.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:06:28.598606: step 3160, loss = 1.06 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:06:29.882046: step 3170, loss = 1.42 (997.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:06:31.172818: step 3180, loss = 1.14 (991.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:06:32.451867: step 3190, loss = 1.11 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:06:33.836669: step 3200, loss = 1.19 (924.3 examples/sec; 0.138 sec/batch)
2017-05-09 21:06:35.020219: step 3210, loss = 1.28 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-09 21:06:36.299893: step 3220, loss = 1.16 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:06:37.594989: step 3230, loss = 1.13 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:06:38.893483: step 3240, loss = 1.12 (985.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:06:40.185387: step 3250, loss = 1.12 (990.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:06:41.504556: step 3260, loss = 1.19 (970.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:06:42.789318: step 3270, loss = 1.26 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:06:44.093897: step 3280, loss = 1.09 (981.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:06:45.383740: step 3290, loss = 1.12 (992.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:06:46.781562: step 3300, loss = 1.28 (915.7 examples/sec; 0.140 sec/batch)
2017-05-09 21:06:47.981636: step 3310, loss = 1.01 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-09 21:06:49.288058: step 3320, loss = 1.05 (979.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:06:50.580997: step 3330, loss = 1.10 (990.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:06:51.872047: step 3340, loss = 1.19 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:06:53.171195: step 3350, loss = 1.03 (985.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:06:54.483020: step 3360, loss = 1.03 (975.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:06:55.795808: step 3370, loss = 1.05 (975.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:06:57.087869: step 3380, loss = 0.96 (990.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:06:58.374905: step 3390, loss = 1.23 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:06:59.757975: step 3400, loss = 1.01 (925.5 examples/sec; 0.138 sec/batch)
2017-05-09 21:07:00.962103: step 3410, loss = 1.13 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-09 21:07:02.274583: step 3420, loss = 1.42 (975.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:07:03.602410: step 3430, loss = 1.23 (964.0 examples/sec; 0.133 sec/batch)
2017-05-09 21:07:04.900639: step 3440, loss = 0.99 (986.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:07:06.193835: step 3450, loss = 1.15 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:07:07.495221: step 3460, loss = 1.07 (983.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:07:08.916615: step 3470, loss = 1.04 (900.5 examples/sec; 0.142 sec/batch)
2017-05-09 21:07:10.193132: step 3480, loss = 0.98 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:07:11.634989: step 3490, loss = 1.49 (887.7 examples/sec; 0.144 sec/batch)
2017-05-09 21:07:13.048076: step 3500, loss = 0.89 (905.8 examples/sec; 0.141 sec/batch)
2017-05-09 21:07:14.258302: step 3510, loss = 1.02 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-09 21:07:15.540672: step 3520, loss = 1.01 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:07:16.832348: step 3530, loss = 1.35 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:07:18.129860: step 3540, loss = 1.09 (986.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:07:19.448490: step 3550, loss = 1.14 (970.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:07:20.757590: step 3560, loss = 1.12 (977.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:07:22.050646: step 3570, loss = 1.09 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:07:23.342961: step 3580, loss = 1.18 (990.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:07:24.642510: step 3590, loss = 0.96 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:07:26.038754: step 3600, loss = 1.02 (916.7 examples/sec; 0.140 sec/batch)
2017-05-09 21:07:27.269379: step 3610, loss = 1.22 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-09 21:07:28.544582: step 3620, loss = 0.97 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:07:29.830964: step 3630, loss = 1.10 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:07:31.138900: step 3640, loss = 1.12 (978.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:07:32.405021: step 3650, loss = 1.23 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:07:33.688843: step 3660, loss = 0.99 (997.0 examples/sec; 0.128 sec/batcE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 83 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
h)
2017-05-09 21:07:34.966655: step 3670, loss = 0.99 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:07:36.246179: step 3680, loss = 1.04 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:07:37.531739: step 3690, loss = 1.08 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:07:38.913765: step 3700, loss = 0.94 (926.2 examples/sec; 0.138 sec/batch)
2017-05-09 21:07:40.147937: step 3710, loss = 0.82 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-09 21:07:41.425752: step 3720, loss = 1.16 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:07:42.731038: step 3730, loss = 1.10 (980.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:07:44.008815: step 3740, loss = 1.21 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:07:45.385135: step 3750, loss = 1.15 (930.0 examples/sec; 0.138 sec/batch)
2017-05-09 21:07:46.676258: step 3760, loss = 1.06 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:07:47.959332: step 3770, loss = 1.07 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:07:49.261892: step 3780, loss = 1.02 (982.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:07:50.554712: step 3790, loss = 1.20 (990.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:07:51.924370: step 3800, loss = 1.10 (934.5 examples/sec; 0.137 sec/batch)
2017-05-09 21:07:53.109933: step 3810, loss = 0.95 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-09 21:07:54.420140: step 3820, loss = 1.21 (976.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:07:55.735853: step 3830, loss = 1.14 (972.9 examples/sec; 0.132 sec/batch)
2017-05-09 21:07:57.055316: step 3840, loss = 1.23 (970.1 examples/sec; 0.132 sec/batch)
2017-05-09 21:07:58.347901: step 3850, loss = 0.99 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:07:59.619881: step 3860, loss = 1.12 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:08:00.921226: step 3870, loss = 1.15 (983.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:08:02.196168: step 3880, loss = 0.95 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:08:03.483053: step 3890, loss = 0.97 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:08:04.891644: step 3900, loss = 1.10 (908.7 examples/sec; 0.141 sec/batch)
2017-05-09 21:08:06.083634: step 3910, loss = 1.22 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-09 21:08:07.393568: step 3920, loss = 0.90 (977.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:08:08.696419: step 3930, loss = 1.06 (982.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:08:09.989914: step 3940, loss = 1.27 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:08:11.267807: step 3950, loss = 0.98 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:08:12.576933: step 3960, loss = 1.03 (977.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:08:13.908333: step 3970, loss = 1.03 (961.4 examples/sec; 0.133 sec/batch)
2017-05-09 21:08:15.229963: step 3980, loss = 0.88 (968.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:08:16.570324: step 3990, loss = 1.14 (955.0 examples/sec; 0.134 sec/batch)
2017-05-09 21:08:17.954371: step 4000, loss = 1.16 (924.8 examples/sec; 0.138 sec/batch)
2017-05-09 21:08:19.404366: step 4010, loss = 1.13 (882.8 examples/sec; 0.145 sec/batch)
2017-05-09 21:08:20.696185: step 4020, loss = 1.17 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:08:21.999538: step 4030, loss = 0.91 (982.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:08:23.322741: step 4040, loss = 1.34 (967.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:08:24.623552: step 4050, loss = 0.98 (984.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:08:25.958472: step 4060, loss = 1.06 (958.9 examples/sec; 0.133 sec/batch)
2017-05-09 21:08:27.460770: step 4070, loss = 1.04 (852.0 examples/sec; 0.150 sec/batch)
2017-05-09 21:08:28.761712: step 4080, loss = 0.95 (983.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:08:30.037530: step 4090, loss = 1.16 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:08:31.436564: step 4100, loss = 0.92 (914.9 examples/sec; 0.140 sec/batch)
2017-05-09 21:08:32.618918: step 4110, loss = 1.05 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-09 21:08:33.899000: step 4120, loss = 1.10 (999.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:08:35.203355: step 4130, loss = 0.90 (981.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:08:36.594896: step 4140, loss = 1.04 (919.9 examples/sec; 0.139 sec/batch)
2017-05-09 21:08:37.906631: step 4150, loss = 1.14 (975.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:08:39.212069: step 4160, loss = 0.97 (980.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:08:40.524739: step 4170, loss = 1.14 (975.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:08:41.862281: step 4180, loss = 0.95 (957.0 examples/sec; 0.134 sec/batch)
2017-05-09 21:08:43.154729: step 4190, loss = 1.09 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:08:44.538517: step 4200, loss = 1.29 (925.0 examples/sec; 0.138 sec/batch)
2017-05-09 21:08:45.757547: step 4210, loss = 1.07 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-09 21:08:47.054040: step 4220, loss = 1.00 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:08:48.356053: step 4230, loss = 0.86 (983.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:08:49.685915: step 4240, loss = 1.52 (962.5 examples/sec; 0.133 sec/batch)
2017-05-09 21:08:50.999728: step 4250, loss = 1.22 (974.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:08:52.300711: step 4260, loss = 1.13 (983.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:08:53.621517: step 4270, loss = 1.19 (969.1 examples/sec; 0.132 sec/batch)
2017-05-09 21:08:54.916985: step 4280, loss = 1.03 (988.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:08:56.234247: step 4290, loss = 1.03 (971.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:08:57.615165: step 4300, loss = 0.99 (926.9 examples/sec; 0.138 sec/batch)
2017-05-09 21:08:58.832835: step 4310, loss = 1.19 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-09 21:09:00.113834: step 4320, loss = 0.90 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:09:01.405534: step 4330, loss = 1.06 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:09:02.701832: step 4340, loss = 1.11 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:09:03.993358: step 4350, loss = 0.98 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:09:05.290437: step 4360, loss = 1.20 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:09:06.569768: step 4370, loss = 1.09 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:09:07.875583: step 4380, loss = 1.12 (980.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:09:09.194104: step 4390, loss = 1.05 (970.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:09:10.629918: step 4400, loss = 1.32 (891.5 examples/sec; 0.144 sec/batch)
2017-05-09 21:09:11.824159: step 4410, loss = 1.19 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-09 21:09:13.099980: step 4420, loss = 1.11 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:09:14.418112: step 4430, loss = 1.13 (971.1 examples/sec; 0.132 sec/batch)
2017-05-09 21:09:15.715295: step 4440, loss = 1.08 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:09:17.002306: step 4450, loss = 1.08 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:09:18.299692: step 4460, loss = 1.02 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:09:19.573517: step 4470, loss = 1.01 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-09 21:09:20.889381: step 4480, loss = 0.88 (972.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:09:22.412074: step 4490, loss = 1.16 (840.6 examples/sec; 0.152 sec/batch)
2017-05-09 21:09:23.795832: step 4500, loss = 1.04 (925.0 examples/sec; 0.138 sec/batch)
2017-05-09 21:09:25.014216: step 4510, loss = 1.04 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-09 21:09:26.309292: step 4520, loss = 1.32 (988.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:09:27.587279: step 4530, loss = 1.11 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:09:28.893872: step 4540, loss = 1.04 (979.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:09:30.188427: step 4550, loss = 1.00 (988.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:09:31.477772: step 4560, loss = 0.87 (992.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:09:32.774660: step 4570, loss = 1.04 (987.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:09:34.080063: step 4580, loss = 1.07 (980.5 examples/secE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 104 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
; 0.131 sec/batch)
2017-05-09 21:09:35.414549: step 4590, loss = 1.28 (959.2 examples/sec; 0.133 sec/batch)
2017-05-09 21:09:36.818816: step 4600, loss = 1.00 (911.5 examples/sec; 0.140 sec/batch)
2017-05-09 21:09:38.016869: step 4610, loss = 1.12 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-09 21:09:39.309531: step 4620, loss = 1.04 (990.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:09:40.615435: step 4630, loss = 0.98 (980.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:09:41.926876: step 4640, loss = 1.06 (976.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:09:43.244063: step 4650, loss = 1.14 (971.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:09:44.556661: step 4660, loss = 1.27 (975.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:09:45.855653: step 4670, loss = 1.02 (985.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:09:47.176490: step 4680, loss = 1.07 (969.1 examples/sec; 0.132 sec/batch)
2017-05-09 21:09:48.500105: step 4690, loss = 1.02 (967.0 examples/sec; 0.132 sec/batch)
2017-05-09 21:09:49.890054: step 4700, loss = 1.14 (920.9 examples/sec; 0.139 sec/batch)
2017-05-09 21:09:51.060168: step 4710, loss = 1.13 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-09 21:09:52.362249: step 4720, loss = 1.11 (983.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:09:53.639237: step 4730, loss = 0.94 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:09:54.928849: step 4740, loss = 0.95 (992.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:09:56.228411: step 4750, loss = 1.08 (984.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:09:57.510819: step 4760, loss = 1.07 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:09:58.814303: step 4770, loss = 1.05 (982.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:10:00.089782: step 4780, loss = 1.05 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:10:01.378099: step 4790, loss = 0.96 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:10:02.799091: step 4800, loss = 1.05 (900.8 examples/sec; 0.142 sec/batch)
2017-05-09 21:10:04.029631: step 4810, loss = 1.04 (1040.2 examples/sec; 0.123 sec/batch)
2017-05-09 21:10:05.296406: step 4820, loss = 1.18 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:10:06.595078: step 4830, loss = 0.94 (985.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:10:07.892386: step 4840, loss = 0.90 (986.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:10:09.224724: step 4850, loss = 1.00 (960.7 examples/sec; 0.133 sec/batch)
2017-05-09 21:10:10.542801: step 4860, loss = 1.04 (971.1 examples/sec; 0.132 sec/batch)
2017-05-09 21:10:11.821102: step 4870, loss = 0.93 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:10:13.113365: step 4880, loss = 1.09 (990.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:10:14.417231: step 4890, loss = 1.11 (981.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:10:15.809907: step 4900, loss = 1.10 (919.1 examples/sec; 0.139 sec/batch)
2017-05-09 21:10:17.007379: step 4910, loss = 0.78 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-09 21:10:18.309069: step 4920, loss = 0.86 (983.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:10:19.646995: step 4930, loss = 1.21 (956.7 examples/sec; 0.134 sec/batch)
2017-05-09 21:10:20.949736: step 4940, loss = 1.12 (982.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:10:22.246846: step 4950, loss = 1.12 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:10:23.554354: step 4960, loss = 1.01 (979.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:10:24.828233: step 4970, loss = 1.09 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-09 21:10:26.110949: step 4980, loss = 0.91 (997.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:10:27.427227: step 4990, loss = 1.05 (972.4 examples/sec; 0.132 sec/batch)
2017-05-09 21:10:28.839207: step 5000, loss = 1.05 (906.5 examples/sec; 0.141 sec/batch)
2017-05-09 21:10:30.049305: step 5010, loss = 1.08 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-09 21:10:31.310888: step 5020, loss = 0.95 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-09 21:10:32.601340: step 5030, loss = 1.00 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:10:33.885971: step 5040, loss = 0.90 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:10:35.169834: step 5050, loss = 0.98 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:10:36.440527: step 5060, loss = 1.19 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:10:37.738193: step 5070, loss = 1.00 (986.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:10:39.015086: step 5080, loss = 1.00 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:10:40.297170: step 5090, loss = 0.97 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:10:41.658301: step 5100, loss = 0.90 (940.4 examples/sec; 0.136 sec/batch)
2017-05-09 21:10:42.863216: step 5110, loss = 1.23 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-09 21:10:44.162641: step 5120, loss = 1.05 (985.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:10:45.452210: step 5130, loss = 0.87 (992.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:10:46.734240: step 5140, loss = 1.05 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:10:48.033742: step 5150, loss = 0.99 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:10:49.316790: step 5160, loss = 1.21 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:10:50.595552: step 5170, loss = 1.06 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:10:51.878571: step 5180, loss = 0.87 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:10:53.172177: step 5190, loss = 1.00 (989.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:10:54.566295: step 5200, loss = 0.97 (918.1 examples/sec; 0.139 sec/batch)
2017-05-09 21:10:55.752239: step 5210, loss = 1.03 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-09 21:10:57.073285: step 5220, loss = 1.05 (968.9 examples/sec; 0.132 sec/batch)
2017-05-09 21:10:58.407811: step 5230, loss = 1.13 (959.1 examples/sec; 0.133 sec/batch)
2017-05-09 21:10:59.728616: step 5240, loss = 1.04 (969.1 examples/sec; 0.132 sec/batch)
2017-05-09 21:11:01.023772: step 5250, loss = 1.01 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:11:02.295420: step 5260, loss = 0.99 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:11:03.590656: step 5270, loss = 1.02 (988.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:11:04.868210: step 5280, loss = 1.10 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:11:06.197855: step 5290, loss = 0.98 (962.7 examples/sec; 0.133 sec/batch)
2017-05-09 21:11:07.600878: step 5300, loss = 1.03 (912.3 examples/sec; 0.140 sec/batch)
2017-05-09 21:11:08.801837: step 5310, loss = 0.95 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-09 21:11:10.099801: step 5320, loss = 1.01 (986.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:11:11.366922: step 5330, loss = 0.91 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:11:12.659276: step 5340, loss = 1.06 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:11:13.947056: step 5350, loss = 1.20 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:11:15.267706: step 5360, loss = 0.93 (969.2 examples/sec; 0.132 sec/batch)
2017-05-09 21:11:16.551576: step 5370, loss = 0.95 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:11:17.850997: step 5380, loss = 1.15 (985.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:11:19.138197: step 5390, loss = 0.90 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:11:20.537392: step 5400, loss = 0.94 (914.8 examples/sec; 0.140 sec/batch)
2017-05-09 21:11:21.730124: step 5410, loss = 1.05 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-09 21:11:23.013557: step 5420, loss = 1.09 (997.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:11:24.317876: step 5430, loss = 1.33 (981.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:11:25.600192: step 5440, loss = 0.94 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:11:26.902959: step 5450, loss = 1.01 (982.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:11:28.218606: step 5460, loss = 1.06 (972.9 examples/sec; 0.132 sec/batch)
2017-05-09 21:11:29.510133: step 5470, loss = 0.92 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:11:30.807979: step 5480, loss = 0.87 (986.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:11:32.103256: step 5490, loss = 0.98 (988.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:11:33.490780: step 5500, loss = 1.00 (9E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 124 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
22.5 examples/sec; 0.139 sec/batch)
2017-05-09 21:11:34.671950: step 5510, loss = 0.87 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-09 21:11:36.005354: step 5520, loss = 1.21 (960.0 examples/sec; 0.133 sec/batch)
2017-05-09 21:11:37.314670: step 5530, loss = 1.04 (977.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:11:38.603243: step 5540, loss = 1.11 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:11:39.928936: step 5550, loss = 0.94 (965.5 examples/sec; 0.133 sec/batch)
2017-05-09 21:11:41.207613: step 5560, loss = 1.00 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:11:42.496719: step 5570, loss = 1.06 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:11:43.809891: step 5580, loss = 1.10 (974.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:11:45.121848: step 5590, loss = 1.04 (975.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:11:46.518651: step 5600, loss = 1.16 (916.4 examples/sec; 0.140 sec/batch)
2017-05-09 21:11:47.685683: step 5610, loss = 0.90 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-09 21:11:48.986538: step 5620, loss = 0.99 (984.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:11:50.274706: step 5630, loss = 0.94 (993.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:11:51.566128: step 5640, loss = 0.91 (991.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:11:52.900294: step 5650, loss = 0.93 (959.4 examples/sec; 0.133 sec/batch)
2017-05-09 21:11:54.201832: step 5660, loss = 1.11 (983.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:11:55.505042: step 5670, loss = 0.87 (982.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:11:56.798773: step 5680, loss = 1.11 (989.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:11:58.106988: step 5690, loss = 0.91 (978.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:11:59.504531: step 5700, loss = 0.82 (915.9 examples/sec; 0.140 sec/batch)
2017-05-09 21:12:00.713225: step 5710, loss = 1.13 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-09 21:12:02.031591: step 5720, loss = 1.19 (970.9 examples/sec; 0.132 sec/batch)
2017-05-09 21:12:03.351083: step 5730, loss = 0.88 (970.1 examples/sec; 0.132 sec/batch)
2017-05-09 21:12:04.627649: step 5740, loss = 0.89 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:12:05.935659: step 5750, loss = 0.97 (978.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:12:07.242519: step 5760, loss = 1.09 (979.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:12:08.551224: step 5770, loss = 1.18 (978.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:12:09.873939: step 5780, loss = 0.98 (967.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:12:11.162951: step 5790, loss = 0.87 (993.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:12:12.552348: step 5800, loss = 1.18 (921.3 examples/sec; 0.139 sec/batch)
2017-05-09 21:12:13.754178: step 5810, loss = 1.06 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-09 21:12:15.050582: step 5820, loss = 1.05 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:12:16.350029: step 5830, loss = 1.10 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:12:17.627827: step 5840, loss = 0.96 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:12:18.961730: step 5850, loss = 0.90 (959.6 examples/sec; 0.133 sec/batch)
2017-05-09 21:12:20.233758: step 5860, loss = 1.30 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:12:21.533154: step 5870, loss = 1.07 (985.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:12:22.863876: step 5880, loss = 0.96 (961.9 examples/sec; 0.133 sec/batch)
2017-05-09 21:12:24.164142: step 5890, loss = 0.85 (984.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:12:25.566285: step 5900, loss = 0.93 (912.9 examples/sec; 0.140 sec/batch)
2017-05-09 21:12:26.749428: step 5910, loss = 1.02 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-09 21:12:28.051194: step 5920, loss = 1.03 (983.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:12:29.356819: step 5930, loss = 0.97 (980.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:12:30.643291: step 5940, loss = 0.97 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:12:31.937768: step 5950, loss = 1.17 (988.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:12:33.231025: step 5960, loss = 0.90 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:12:34.556614: step 5970, loss = 1.10 (965.6 examples/sec; 0.133 sec/batch)
2017-05-09 21:12:35.844718: step 5980, loss = 0.94 (993.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:12:37.157613: step 5990, loss = 0.95 (974.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:12:38.576897: step 6000, loss = 1.03 (901.9 examples/sec; 0.142 sec/batch)
2017-05-09 21:12:39.788485: step 6010, loss = 0.91 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-09 21:12:41.066709: step 6020, loss = 0.92 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:12:42.347921: step 6030, loss = 1.14 (999.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:12:43.648162: step 6040, loss = 1.24 (984.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:12:44.954653: step 6050, loss = 1.02 (979.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:12:46.248137: step 6060, loss = 1.10 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:12:47.545068: step 6070, loss = 1.07 (986.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:12:48.841470: step 6080, loss = 1.02 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:12:50.156083: step 6090, loss = 0.99 (973.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:12:51.556090: step 6100, loss = 0.89 (914.3 examples/sec; 0.140 sec/batch)
2017-05-09 21:12:52.804118: step 6110, loss = 1.00 (1025.6 examples/sec; 0.125 sec/batch)
2017-05-09 21:12:54.110534: step 6120, loss = 1.14 (979.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:12:55.399139: step 6130, loss = 0.97 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:12:56.706761: step 6140, loss = 0.98 (978.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:12:57.995861: step 6150, loss = 0.90 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:12:59.282539: step 6160, loss = 0.90 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:13:00.591109: step 6170, loss = 1.01 (978.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:13:01.875513: step 6180, loss = 0.86 (996.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:13:03.193814: step 6190, loss = 1.28 (970.9 examples/sec; 0.132 sec/batch)
2017-05-09 21:13:04.597671: step 6200, loss = 1.10 (911.8 examples/sec; 0.140 sec/batch)
2017-05-09 21:13:05.788293: step 6210, loss = 1.08 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-09 21:13:07.104131: step 6220, loss = 0.81 (972.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:13:08.399634: step 6230, loss = 1.18 (988.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:13:09.742668: step 6240, loss = 1.01 (953.1 examples/sec; 0.134 sec/batch)
2017-05-09 21:13:11.051828: step 6250, loss = 0.99 (977.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:13:12.340696: step 6260, loss = 1.03 (993.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:13:13.627821: step 6270, loss = 1.20 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:13:14.913837: step 6280, loss = 1.07 (995.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:13:16.212215: step 6290, loss = 0.88 (985.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:13:17.595961: step 6300, loss = 0.91 (925.0 examples/sec; 0.138 sec/batch)
2017-05-09 21:13:18.790035: step 6310, loss = 0.76 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-09 21:13:20.100892: step 6320, loss = 1.09 (976.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:13:21.408054: step 6330, loss = 0.90 (979.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:13:22.737033: step 6340, loss = 1.02 (963.1 examples/sec; 0.133 sec/batch)
2017-05-09 21:13:24.042864: step 6350, loss = 1.11 (980.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:13:25.347340: step 6360, loss = 0.76 (981.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:13:26.668030: step 6370, loss = 1.25 (969.2 examples/sec; 0.132 sec/batch)
2017-05-09 21:13:27.991008: step 6380, loss = 1.76 (967.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:13:29.304552: step 6390, loss = 1.13 (974.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:13:30.691276: step 6400, loss = 0.98 (923.0 examples/sec; 0.139 sec/batch)
2017-05-09 21:13:31.895278: step 6410, loss = 0.90 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-09 21:13:33.191591: step 6420, loss = 0.83 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:13:34.496623: step 6430, loss = 0.89 (980.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:13:35.823717: step 6440, loss = 1.03 (964.5 examples/sec; 0.133 sec/batch)
2017-05-09 21:13:37.130771: step 6450, loss = 1.08 (979.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:13:38.407868: step 6460, loss = 1.12 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:13:39.735399: step 6470, loss = 1.01 (964.2 examples/sec; 0.133 sec/batch)
2017-05-09 21:13:41.039386: step 6480, loss = 1.16 (981.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:13:42.340677: step 6490, loss = 0.81 (983.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:13:43.723090: step 6500, loss = 0.99 (925.9 examples/sec; 0.138 sec/batch)
2017-05-09 21:13:44.925252: step 6510, loss = 0.92 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-09 21:13:46.231269: step 6520, loss = 1.12 (980.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:13:47.532612: step 6530, loss = 0.90 (983.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:13:48.802807: step 6540, loss = 0.87 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:13:50.104140: step 6550, loss = 0.96 (983.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:13:51.397440: step 6560, loss = 0.88 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:13:52.684742: step 6570, loss = 0.93 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:13:53.978018: step 6580, loss = 1.02 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:13:55.298324: step 6590, loss = 0.94 (969.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:13:56.698005: step 6600, loss = 1.06 (914.5 examples/sec; 0.140 sec/batch)
2017-05-09 21:13:57.896710: step 6610, loss = 1.02 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-09 21:13:59.181928: step 6620, loss = 1.07 (995.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:14:00.485934: step 6630, loss = 0.97 (981.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:14:01.764690: step 6640, loss = 1.00 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:14:03.087428: step 6650, loss = 1.01 (967.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:14:04.388892: step 6660, loss = 1.09 (983.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:14:05.682006: step 6670, loss = 0.95 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:14:07.010800: step 6680, loss = 0.95 (963.3 examples/sec; 0.133 sec/batch)
2017-05-09 21:14:08.322183: step 6690, loss = 0.96 (976.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:14:09.729645: step 6700, loss = 1.08 (909.4 examples/sec; 0.141 sec/batch)
2017-05-09 21:14:10.944366: step 6710, loss = 1.00 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-09 21:14:12.237089: step 6720, loss = 0.82 (990.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:14:13.542733: step 6730, loss = 0.90 (980.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:14:14.834163: step 6740, loss = 0.91 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:14:16.133130: step 6750, loss = 1.00 (985.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:14:17.428141: step 6760, loss = 1.00 (988.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:14:18.738428: step 6770, loss = 0.94 (976.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:14:20.028646: step 6780, loss = 1.00 (992.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:14:21.322755: step 6790, loss = 0.84 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:14:22.726709: step 6800, loss = 1.07 (911.7 examples/sec; 0.140 sec/batch)
2017-05-09 21:14:23.905584: step 6810, loss = 0.92 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-09 21:14:25.184018: step 6820, loss = 0.83 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:14:26.501498: step 6830, loss = 0.83 (971.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:14:27.809105: step 6840, loss = 0.92 (978.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:14:29.137836: step 6850, loss = 0.97 (963.3 examples/sec; 0.133 sec/batch)
2017-05-09 21:14:30.433860: step 6860, loss = 1.05 (987.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:14:31.720216: step 6870, loss = 1.04 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:14:33.025321: step 688E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 144 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
0, loss = 0.91 (980.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:14:34.322776: step 6890, loss = 0.95 (986.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:14:35.693380: step 6900, loss = 1.14 (933.9 examples/sec; 0.137 sec/batch)
2017-05-09 21:14:36.882908: step 6910, loss = 0.86 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-09 21:14:38.158240: step 6920, loss = 1.05 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:14:39.463190: step 6930, loss = 1.12 (980.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:14:40.786357: step 6940, loss = 1.28 (967.4 examples/sec; 0.132 sec/batch)
2017-05-09 21:14:42.090754: step 6950, loss = 1.09 (981.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:14:43.390570: step 6960, loss = 0.88 (984.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:14:44.667280: step 6970, loss = 1.02 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:14:45.962234: step 6980, loss = 0.93 (988.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:14:47.249195: step 6990, loss = 0.92 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:14:48.651505: step 7000, loss = 0.89 (912.8 examples/sec; 0.140 sec/batch)
2017-05-09 21:14:49.865563: step 7010, loss = 0.99 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-09 21:14:51.185497: step 7020, loss = 0.97 (969.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:14:52.483020: step 7030, loss = 0.94 (986.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:14:53.762667: step 7040, loss = 1.11 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:14:55.073673: step 7050, loss = 1.28 (976.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:14:56.384795: step 7060, loss = 1.01 (976.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:14:57.689508: step 7070, loss = 0.91 (981.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:14:58.978267: step 7080, loss = 0.80 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:15:00.236015: step 7090, loss = 1.16 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-09 21:15:01.612906: step 7100, loss = 1.02 (929.6 examples/sec; 0.138 sec/batch)
2017-05-09 21:15:02.832911: step 7110, loss = 1.00 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-09 21:15:04.133932: step 7120, loss = 0.95 (983.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:15:05.446608: step 7130, loss = 1.01 (975.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:15:06.739713: step 7140, loss = 1.00 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:15:08.035147: step 7150, loss = 0.83 (988.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:15:09.358285: step 7160, loss = 0.87 (967.4 examples/sec; 0.132 sec/batch)
2017-05-09 21:15:10.681438: step 7170, loss = 1.02 (967.4 examples/sec; 0.132 sec/batch)
2017-05-09 21:15:12.008841: step 7180, loss = 1.05 (964.3 examples/sec; 0.133 sec/batch)
2017-05-09 21:15:13.290030: step 7190, loss = 1.09 (999.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:15:14.711590: step 7200, loss = 0.87 (900.4 examples/sec; 0.142 sec/batch)
2017-05-09 21:15:15.939438: step 7210, loss = 1.02 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-09 21:15:17.225239: step 7220, loss = 0.93 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:15:18.591787: step 7230, loss = 0.90 (936.7 examples/sec; 0.137 sec/batch)
2017-05-09 21:15:19.910876: step 7240, loss = 0.84 (970.4 examples/sec; 0.132 sec/batch)
2017-05-09 21:15:21.215694: step 7250, loss = 0.92 (981.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:15:22.771934: step 7260, loss = 0.86 (822.5 examples/sec; 0.156 sec/batch)
2017-05-09 21:15:24.066500: step 7270, loss = 0.91 (988.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:15:25.349718: step 7280, loss = 0.95 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:15:26.655613: step 7290, loss = 1.00 (980.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:15:28.118264: step 7300, loss = 1.00 (875.1 examples/sec; 0.146 sec/batch)
2017-05-09 21:15:29.291711: step 7310, loss = 1.20 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-09 21:15:30.620425: step 7320, loss = 0.99 (963.3 examples/sec; 0.133 sec/batch)
2017-05-09 21:15:31.925437: step 7330, loss = 1.03 (980.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:15:33.209870: step 7340, loss = 0.86 (996.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:15:34.534288: step 7350, loss = 0.95 (966.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:15:35.794542: step 7360, loss = 0.99 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-09 21:15:37.076315: step 7370, loss = 0.91 (998.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:15:38.361348: step 7380, loss = 0.97 (996.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:15:39.670742: step 7390, loss = 0.80 (977.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:15:41.058940: step 7400, loss = 0.90 (922.1 examples/sec; 0.139 sec/batch)
2017-05-09 21:15:42.279144: step 7410, loss = 1.08 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-09 21:15:43.581211: step 7420, loss = 0.83 (983.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:15:44.878186: step 7430, loss = 0.92 (986.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:15:46.184091: step 7440, loss = 1.08 (980.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:15:47.506698: step 7450, loss = 1.18 (967.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:15:48.830035: step 7460, loss = 1.21 (967.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:15:50.111857: step 7470, loss = 0.89 (998.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:15:51.391395: step 7480, loss = 0.80 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:15:52.673829: step 7490, loss = 0.85 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:15:54.075098: step 7500, loss = 0.92 (913.5 examples/sec; 0.140 sec/batch)
2017-05-09 21:15:55.270328: step 7510, loss = 0.99 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-09 21:15:56.593047: step 7520, loss = 1.05 (967.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:15:57.895668: step 7530, loss = 0.91 (982.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:15:59.222585: step 7540, loss = 0.84 (964.6 examples/sec; 0.133 sec/batch)
2017-05-09 21:16:00.512872: step 7550, loss = 1.13 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:16:01.805934: step 7560, loss = 0.97 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:16:03.145981: step 7570, loss = 0.94 (955.2 examples/sec; 0.134 sec/batch)
2017-05-09 21:16:04.434061: step 7580, loss = 0.86 (993.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:16:05.706377: step 7590, loss = 0.95 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:16:07.093540: step 7600, loss = 1.16 (922.7 examples/sec; 0.139 sec/batch)
2017-05-09 21:16:08.322402: step 7610, loss = 0.87 (1041.6 examples/sec; 0.123 sec/batch)
2017-05-09 21:16:09.608756: step 7620, loss = 1.00 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:16:10.920665: step 7630, loss = 0.87 (975.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:16:12.229284: step 7640, loss = 0.97 (978.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:16:13.566113: step 7650, loss = 0.92 (957.5 examples/sec; 0.134 sec/batch)
2017-05-09 21:16:14.864934: step 7660, loss = 1.01 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:16:16.142947: step 7670, loss = 0.82 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:16:17.428043: step 7680, loss = 0.93 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:16:18.693960: step 7690, loss = 0.80 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:16:20.091385: step 7700, loss = 1.08 (916.0 examples/sec; 0.140 sec/batch)
2017-05-09 21:16:21.286267: step 7710, loss = 0.97 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-09 21:16:22.583674: step 7720, loss = 1.09 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:16:23.872553: step 7730, loss = 0.99 (993.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:16:25.192547: step 7740, loss = 0.94 (969.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:16:26.489028: step 7750, loss = 0.91 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:16:27.778812: step 7760, loss = 0.83 (992.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:16:29.066829: step 7770, loss = 0.88 (993.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:16:30.368675: step 7780, loss = 0.93 (983.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:16:31.660665: step 7790, loss = 0.92 (990.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:16:33.04E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 164 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
1572: step 7800, loss = 1.00 (926.9 examples/sec; 0.138 sec/batch)
2017-05-09 21:16:34.244882: step 7810, loss = 0.99 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-09 21:16:35.548240: step 7820, loss = 0.84 (982.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:16:36.841694: step 7830, loss = 0.90 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:16:38.143403: step 7840, loss = 0.80 (983.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:16:39.461778: step 7850, loss = 0.96 (970.9 examples/sec; 0.132 sec/batch)
2017-05-09 21:16:40.768545: step 7860, loss = 0.75 (979.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:16:42.042175: step 7870, loss = 1.02 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:16:43.328251: step 7880, loss = 0.98 (995.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:16:44.613841: step 7890, loss = 1.14 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:16:46.008429: step 7900, loss = 0.86 (917.8 examples/sec; 0.139 sec/batch)
2017-05-09 21:16:47.210064: step 7910, loss = 0.92 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-09 21:16:48.520655: step 7920, loss = 0.90 (976.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:16:49.814999: step 7930, loss = 1.04 (988.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:16:51.090894: step 7940, loss = 0.82 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:16:52.376313: step 7950, loss = 0.87 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:16:53.673597: step 7960, loss = 1.17 (986.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:16:54.961577: step 7970, loss = 0.88 (993.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:16:56.245167: step 7980, loss = 0.91 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:16:57.530133: step 7990, loss = 0.87 (996.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:16:58.937271: step 8000, loss = 1.12 (909.6 examples/sec; 0.141 sec/batch)
2017-05-09 21:17:00.135251: step 8010, loss = 0.89 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-09 21:17:01.437048: step 8020, loss = 0.87 (983.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:17:02.726366: step 8030, loss = 0.96 (992.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:17:04.013210: step 8040, loss = 1.06 (994.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:17:05.306217: step 8050, loss = 1.01 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:17:06.618356: step 8060, loss = 0.87 (975.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:17:07.919089: step 8070, loss = 1.02 (984.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:17:09.229231: step 8080, loss = 1.17 (977.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:17:10.545150: step 8090, loss = 0.99 (972.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:17:11.946974: step 8100, loss = 0.94 (913.1 examples/sec; 0.140 sec/batch)
2017-05-09 21:17:13.147131: step 8110, loss = 1.27 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-09 21:17:14.461223: step 8120, loss = 0.84 (974.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:17:15.764508: step 8130, loss = 0.98 (982.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:17:17.065141: step 8140, loss = 0.85 (984.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:17:18.364959: step 8150, loss = 0.93 (984.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:17:19.661351: step 8160, loss = 1.11 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:17:20.975145: step 8170, loss = 0.98 (974.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:17:22.286337: step 8180, loss = 0.81 (976.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:17:23.615646: step 8190, loss = 1.02 (962.9 examples/sec; 0.133 sec/batch)
2017-05-09 21:17:25.011208: step 8200, loss = 1.02 (917.2 examples/sec; 0.140 sec/batch)
2017-05-09 21:17:26.199932: step 8210, loss = 1.17 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-09 21:17:27.489301: step 8220, loss = 0.90 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:17:28.790000: step 8230, loss = 1.03 (984.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:17:30.083501: step 8240, loss = 0.86 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:17:31.370715: step 8250, loss = 0.95 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:17:32.698985: step 8260, loss = 1.07 (963.7 examples/sec; 0.133 sec/batch)
2017-05-09 21:17:34.021247: step 8270, loss = 1.16 (968.0 examples/sec; 0.132 sec/batch)
2017-05-09 21:17:35.311125: step 8280, loss = 0.92 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:17:36.592958: step 8290, loss = 1.20 (998.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:17:37.996819: step 8300, loss = 0.86 (911.8 examples/sec; 0.140 sec/batch)
2017-05-09 21:17:39.232553: step 8310, loss = 0.90 (1035.8 examples/sec; 0.124 sec/batch)
2017-05-09 21:17:40.518240: step 8320, loss = 1.02 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:17:41.813802: step 8330, loss = 0.94 (988.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:17:43.110642: step 8340, loss = 0.83 (987.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:17:44.399869: step 8350, loss = 0.92 (992.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:17:45.704196: step 8360, loss = 0.88 (981.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:17:46.992016: step 8370, loss = 1.02 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:17:48.271396: step 8380, loss = 0.84 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:17:49.561939: step 8390, loss = 0.78 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:17:50.952845: step 8400, loss = 1.01 (920.3 examples/sec; 0.139 sec/batch)
2017-05-09 21:17:52.176384: step 8410, loss = 0.95 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-09 21:17:53.490309: step 8420, loss = 0.81 (974.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:17:54.844060: step 8430, loss = 0.82 (945.5 examples/sec; 0.135 sec/batch)
2017-05-09 21:17:56.161941: step 8440, loss = 0.87 (971.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:17:57.471944: step 8450, loss = 0.99 (977.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:17:58.789146: step 8460, loss = 1.14 (971.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:18:00.114938: step 8470, loss = 1.02 (965.5 examples/sec; 0.133 sec/batch)
2017-05-09 21:18:01.433574: step 8480, loss = 0.82 (970.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:18:02.739705: step 8490, loss = 0.96 (980.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:18:04.127573: step 8500, loss = 1.00 (922.3 examples/sec; 0.139 sec/batch)
2017-05-09 21:18:05.325923: step 8510, loss = 0.86 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-09 21:18:06.643565: step 8520, loss = 0.82 (971.4 examples/sec; 0.132 sec/batch)
2017-05-09 21:18:07.940603: step 8530, loss = 1.06 (986.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:18:09.275735: step 8540, loss = 1.00 (958.7 examples/sec; 0.134 sec/batch)
2017-05-09 21:18:10.575121: step 8550, loss = 1.02 (985.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:18:11.873284: step 8560, loss = 0.96 (986.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:18:13.170050: step 8570, loss = 0.86 (987.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:18:14.477044: step 8580, loss = 0.86 (979.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:18:15.784739: step 8590, loss = 1.00 (978.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:18:17.173616: step 8600, loss = 1.00 (921.6 examples/sec; 0.139 sec/batch)
2017-05-09 21:18:18.398254: step 8610, loss = 0.87 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-09 21:18:19.717585: step 8620, loss = 0.98 (970.2 examples/sec; 0.132 sec/batch)
2017-05-09 21:18:21.013800: step 8630, loss = 0.94 (987.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:18:22.326015: step 8640, loss = 1.02 (975.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:18:23.659303: step 8650, loss = 0.83 (960.0 examples/sec; 0.133 sec/batch)
2017-05-09 21:18:24.969210: step 8660, loss = 0.94 (977.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:18:26.290755: step 8670, loss = 1.08 (968.6 examples/sec; 0.132 sec/batch)
2017-05-09 21:18:27.643634: step 8680, loss = 0.92 (946.1 examples/sec; 0.135 sec/batch)
2017-05-09 21:18:28.983180: step 8690, loss = 1.05 (955.5 examples/sec; 0.134 sec/batch)
2017-05-09 21:18:30.495118: step 8700, loss = 1.01 (846.6 examples/sec; 0.151 sec/batch)
2017-05-09 21:18:32.000134: step 8710, loss = 0.85 (850.5 examples/sec; 0.151 sec/batch)
2017-05-09 21:E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 184 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
18:33.536758: step 8720, loss = 1.04 (833.0 examples/sec; 0.154 sec/batch)
2017-05-09 21:18:34.868346: step 8730, loss = 1.00 (961.3 examples/sec; 0.133 sec/batch)
2017-05-09 21:18:36.169262: step 8740, loss = 0.85 (983.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:18:37.468303: step 8750, loss = 0.81 (985.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:18:38.775771: step 8760, loss = 0.96 (979.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:18:40.097112: step 8770, loss = 1.01 (968.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:18:41.402021: step 8780, loss = 0.82 (980.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:18:42.711298: step 8790, loss = 0.98 (977.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:18:44.120247: step 8800, loss = 0.87 (908.5 examples/sec; 0.141 sec/batch)
2017-05-09 21:18:45.313802: step 8810, loss = 0.86 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-09 21:18:46.614182: step 8820, loss = 0.81 (984.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:18:47.903441: step 8830, loss = 0.98 (992.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:18:49.215475: step 8840, loss = 0.84 (975.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:18:50.498502: step 8850, loss = 1.03 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:18:51.777187: step 8860, loss = 0.80 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:18:53.087146: step 8870, loss = 1.38 (977.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:18:54.433893: step 8880, loss = 0.95 (950.4 examples/sec; 0.135 sec/batch)
2017-05-09 21:18:55.731297: step 8890, loss = 0.85 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:18:57.101657: step 8900, loss = 1.00 (934.1 examples/sec; 0.137 sec/batch)
2017-05-09 21:18:58.312398: step 8910, loss = 0.89 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-09 21:18:59.612928: step 8920, loss = 1.07 (984.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:19:00.946486: step 8930, loss = 1.14 (959.9 examples/sec; 0.133 sec/batch)
2017-05-09 21:19:02.237276: step 8940, loss = 1.05 (991.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:19:03.552126: step 8950, loss = 0.83 (973.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:19:04.880705: step 8960, loss = 1.03 (963.4 examples/sec; 0.133 sec/batch)
2017-05-09 21:19:06.171204: step 8970, loss = 1.06 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:19:07.483478: step 8980, loss = 1.00 (975.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:19:08.755832: step 8990, loss = 1.08 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:19:10.150858: step 9000, loss = 1.06 (917.5 examples/sec; 0.140 sec/batch)
2017-05-09 21:19:11.377107: step 9010, loss = 0.99 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-09 21:19:12.696690: step 9020, loss = 1.14 (970.0 examples/sec; 0.132 sec/batch)
2017-05-09 21:19:14.008689: step 9030, loss = 0.99 (975.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:19:15.313651: step 9040, loss = 1.03 (980.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:19:16.606422: step 9050, loss = 0.93 (990.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:19:17.919222: step 9060, loss = 1.01 (975.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:19:19.236709: step 9070, loss = 0.83 (971.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:19:20.540390: step 9080, loss = 0.92 (981.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:19:21.850758: step 9090, loss = 1.09 (976.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:19:23.245660: step 9100, loss = 0.95 (917.6 examples/sec; 0.139 sec/batch)
2017-05-09 21:19:24.431679: step 9110, loss = 0.87 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-09 21:19:25.716989: step 9120, loss = 0.86 (995.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:19:27.013154: step 9130, loss = 0.93 (987.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:19:28.307143: step 9140, loss = 0.92 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:19:29.597153: step 9150, loss = 0.88 (992.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:19:30.873198: step 9160, loss = 1.01 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:19:32.159089: step 9170, loss = 0.94 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:19:33.470381: step 9180, loss = 1.06 (976.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:19:34.749584: step 9190, loss = 0.93 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:19:36.119503: step 9200, loss = 0.81 (934.4 examples/sec; 0.137 sec/batch)
2017-05-09 21:19:37.310684: step 9210, loss = 1.11 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-09 21:19:38.599147: step 9220, loss = 0.85 (993.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:19:39.970549: step 9230, loss = 1.07 (933.3 examples/sec; 0.137 sec/batch)
2017-05-09 21:19:41.222718: step 9240, loss = 0.81 (1022.2 examples/sec; 0.125 sec/batch)
2017-05-09 21:19:42.530491: step 9250, loss = 1.07 (978.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:19:43.803852: step 9260, loss = 0.92 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:19:45.104801: step 9270, loss = 0.88 (983.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:19:46.396141: step 9280, loss = 0.94 (991.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:19:47.716255: step 9290, loss = 0.73 (969.6 examples/sec; 0.132 sec/batch)
2017-05-09 21:19:49.094522: step 9300, loss = 1.01 (928.7 examples/sec; 0.138 sec/batch)
2017-05-09 21:19:50.287977: step 9310, loss = 1.02 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-09 21:19:51.611605: step 9320, loss = 1.05 (967.0 examples/sec; 0.132 sec/batch)
2017-05-09 21:19:52.938418: step 9330, loss = 0.93 (964.7 examples/sec; 0.133 sec/batch)
2017-05-09 21:19:54.262412: step 9340, loss = 0.94 (966.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:19:55.575495: step 9350, loss = 0.90 (974.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:19:56.853623: step 9360, loss = 0.84 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:19:58.131749: step 9370, loss = 0.89 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:19:59.462955: step 9380, loss = 1.02 (961.5 examples/sec; 0.133 sec/batch)
2017-05-09 21:20:00.798732: step 9390, loss = 0.95 (958.2 examples/sec; 0.134 sec/batch)
2017-05-09 21:20:02.193394: step 9400, loss = 0.84 (917.8 examples/sec; 0.139 sec/batch)
2017-05-09 21:20:03.380243: step 9410, loss = 0.89 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-09 21:20:04.679783: step 9420, loss = 0.86 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:20:05.995969: step 9430, loss = 0.85 (972.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:20:07.278330: step 9440, loss = 1.08 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:20:08.569206: step 9450, loss = 1.04 (991.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:20:09.865026: step 9460, loss = 0.92 (987.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:20:11.162363: step 9470, loss = 1.02 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:20:12.448875: step 9480, loss = 1.00 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:20:13.738089: step 9490, loss = 0.77 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:20:15.126470: step 9500, loss = 0.91 (922.0 examples/sec; 0.139 sec/batch)
2017-05-09 21:20:16.346745: step 9510, loss = 0.81 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-09 21:20:17.650111: step 9520, loss = 0.97 (982.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:20:18.987031: step 9530, loss = 1.00 (957.4 examples/sec; 0.134 sec/batch)
2017-05-09 21:20:20.254564: step 9540, loss = 0.90 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-09 21:20:21.545322: step 9550, loss = 0.86 (991.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:20:22.820772: step 9560, loss = 0.93 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:20:24.111229: step 9570, loss = 0.88 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:20:25.414811: step 9580, loss = 1.01 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:20:26.700844: step 9590, loss = 0.77 (995.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:20:28.098229: step 9600, loss = 0.93 (916.0 examples/sec; 0.140 sec/batch)
2017-05-09 21:20:29.309295: step 9610, loss = 0.99 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-09 21:20:30.616943: step 9620, loss = 1.21 (978.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:20:31.931610: step 9630, loss = 1.01 (973.6 examples/sec; 0.131 sec/batch)E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 205 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU

2017-05-09 21:20:33.227607: step 9640, loss = 0.79 (987.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:20:34.514726: step 9650, loss = 1.04 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:20:35.815676: step 9660, loss = 0.74 (983.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:20:37.127199: step 9670, loss = 1.06 (976.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:20:38.430731: step 9680, loss = 1.01 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:20:39.735599: step 9690, loss = 1.00 (980.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:20:41.125245: step 9700, loss = 1.00 (921.1 examples/sec; 0.139 sec/batch)
2017-05-09 21:20:42.355193: step 9710, loss = 0.82 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-09 21:20:43.643504: step 9720, loss = 0.84 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:20:44.925102: step 9730, loss = 0.76 (998.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:20:46.231315: step 9740, loss = 0.98 (979.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:20:47.511146: step 9750, loss = 0.82 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:20:48.810075: step 9760, loss = 1.03 (985.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:20:50.113362: step 9770, loss = 0.83 (982.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:20:51.399504: step 9780, loss = 0.97 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:20:52.695317: step 9790, loss = 0.96 (987.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:20:54.121376: step 9800, loss = 1.09 (897.6 examples/sec; 0.143 sec/batch)
2017-05-09 21:20:55.318727: step 9810, loss = 1.07 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-09 21:20:56.605920: step 9820, loss = 0.95 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:20:57.913383: step 9830, loss = 0.83 (979.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:20:59.198834: step 9840, loss = 0.93 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:21:00.476591: step 9850, loss = 0.85 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:21:01.762160: step 9860, loss = 0.78 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:21:03.055181: step 9870, loss = 0.96 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:21:04.332578: step 9880, loss = 0.97 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:21:05.631280: step 9890, loss = 1.09 (985.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:21:07.036802: step 9900, loss = 0.84 (910.7 examples/sec; 0.141 sec/batch)
2017-05-09 21:21:08.228515: step 9910, loss = 1.07 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-09 21:21:09.524856: step 9920, loss = 0.83 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:21:10.814419: step 9930, loss = 0.87 (992.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:21:12.130148: step 9940, loss = 1.22 (972.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:21:13.434097: step 9950, loss = 0.77 (981.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:21:14.712550: step 9960, loss = 0.89 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:21:16.007249: step 9970, loss = 0.94 (988.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:21:17.321605: step 9980, loss = 1.04 (973.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:21:18.628067: step 9990, loss = 0.81 (979.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:21:20.008626: step 10000, loss = 0.80 (927.2 examples/sec; 0.138 sec/batch)
2017-05-09 21:21:21.235303: step 10010, loss = 1.09 (1043.5 examples/sec; 0.123 sec/batch)
2017-05-09 21:21:22.533525: step 10020, loss = 0.87 (986.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:21:23.821947: step 10030, loss = 1.00 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:21:25.123016: step 10040, loss = 0.87 (983.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:21:26.421018: step 10050, loss = 1.07 (986.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:21:27.745952: step 10060, loss = 0.86 (966.1 examples/sec; 0.132 sec/batch)
2017-05-09 21:21:29.058934: step 10070, loss = 0.93 (974.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:21:30.359256: step 10080, loss = 0.81 (984.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:21:31.640889: step 10090, loss = 0.79 (998.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:21:33.046723: step 10100, loss = 0.86 (910.5 examples/sec; 0.141 sec/batch)
2017-05-09 21:21:34.256796: step 10110, loss = 0.87 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-09 21:21:35.561893: step 10120, loss = 0.81 (980.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:21:36.845357: step 10130, loss = 0.93 (997.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:21:38.134731: step 10140, loss = 0.95 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:21:39.449699: step 10150, loss = 1.00 (973.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:21:40.700849: step 10160, loss = 0.74 (1023.1 examples/sec; 0.125 sec/batch)
2017-05-09 21:21:42.014947: step 10170, loss = 1.09 (974.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:21:43.334514: step 10180, loss = 0.82 (970.0 examples/sec; 0.132 sec/batch)
2017-05-09 21:21:44.622338: step 10190, loss = 0.95 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:21:46.008275: step 10200, loss = 0.86 (923.6 examples/sec; 0.139 sec/batch)
2017-05-09 21:21:47.231526: step 10210, loss = 0.85 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-09 21:21:48.533345: step 10220, loss = 0.81 (983.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:21:49.832094: step 10230, loss = 1.14 (985.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:21:51.126830: step 10240, loss = 1.00 (988.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:21:52.446419: step 10250, loss = 0.82 (970.0 examples/sec; 0.132 sec/batch)
2017-05-09 21:21:53.756478: step 10260, loss = 0.84 (977.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:21:55.063764: step 10270, loss = 1.04 (979.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:21:56.374005: step 10280, loss = 1.00 (976.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:21:57.681939: step 10290, loss = 0.82 (978.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:21:59.080516: step 10300, loss = 0.99 (915.2 examples/sec; 0.140 sec/batch)
2017-05-09 21:22:00.298240: step 10310, loss = 0.90 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-09 21:22:01.625008: step 10320, loss = 0.82 (964.8 examples/sec; 0.133 sec/batch)
2017-05-09 21:22:02.908910: step 10330, loss = 0.89 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:22:04.205220: step 10340, loss = 0.98 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:22:05.521096: step 10350, loss = 0.98 (972.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:22:06.828559: step 10360, loss = 0.97 (979.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:22:08.108097: step 10370, loss = 1.10 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:22:09.410513: step 10380, loss = 0.97 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:22:10.705270: step 10390, loss = 0.90 (988.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:22:12.120395: step 10400, loss = 0.87 (904.5 examples/sec; 0.142 sec/batch)
2017-05-09 21:22:13.314813: step 10410, loss = 1.05 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-09 21:22:14.611190: step 10420, loss = 1.17 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:22:15.933106: step 10430, loss = 1.83 (968.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:22:17.246326: step 10440, loss = 0.84 (974.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:22:18.545866: step 10450, loss = 0.80 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:22:19.848522: step 10460, loss = 0.85 (982.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:22:21.171273: step 10470, loss = 1.08 (967.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:22:22.480657: step 10480, loss = 0.91 (977.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:22:23.782614: step 10490, loss = 0.85 (983.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:22:25.179762: step 10500, loss = 0.97 (916.2 examples/sec; 0.140 sec/batch)
2017-05-09 21:22:26.381856: step 10510, loss = 0.93 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-09 21:22:27.670038: step 10520, loss = 0.91 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:22:28.966056: step 10530, loss = 0.88 (987.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:22:30.255549: step 10540, loss = 0.85 (992.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:22:31.57E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 225 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
1226: step 10550, loss = 1.12 (972.9 examples/sec; 0.132 sec/batch)
2017-05-09 21:22:32.857712: step 10560, loss = 0.93 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:22:34.149375: step 10570, loss = 0.82 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:22:35.429502: step 10580, loss = 0.97 (999.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:22:36.706226: step 10590, loss = 1.13 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:22:38.106427: step 10600, loss = 0.94 (914.2 examples/sec; 0.140 sec/batch)
2017-05-09 21:22:39.314765: step 10610, loss = 0.77 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-09 21:22:40.618016: step 10620, loss = 0.89 (982.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:22:41.920175: step 10630, loss = 0.98 (983.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:22:43.270121: step 10640, loss = 0.90 (948.2 examples/sec; 0.135 sec/batch)
2017-05-09 21:22:44.561313: step 10650, loss = 0.85 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:22:45.898560: step 10660, loss = 0.93 (957.2 examples/sec; 0.134 sec/batch)
2017-05-09 21:22:47.181334: step 10670, loss = 0.87 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:22:48.480760: step 10680, loss = 0.82 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:22:49.787734: step 10690, loss = 0.82 (979.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:22:51.204159: step 10700, loss = 0.85 (903.7 examples/sec; 0.142 sec/batch)
2017-05-09 21:22:52.420620: step 10710, loss = 0.94 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-09 21:22:53.702272: step 10720, loss = 0.93 (998.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:22:55.007791: step 10730, loss = 0.82 (980.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:22:56.302767: step 10740, loss = 0.76 (988.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:22:57.589630: step 10750, loss = 0.83 (994.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:22:58.880759: step 10760, loss = 0.91 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:23:00.214402: step 10770, loss = 1.01 (959.8 examples/sec; 0.133 sec/batch)
2017-05-09 21:23:01.522683: step 10780, loss = 0.97 (978.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:23:02.812877: step 10790, loss = 0.71 (992.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:23:04.189738: step 10800, loss = 0.92 (929.6 examples/sec; 0.138 sec/batch)
2017-05-09 21:23:05.402309: step 10810, loss = 1.02 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-09 21:23:06.697368: step 10820, loss = 0.89 (988.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:23:08.009362: step 10830, loss = 1.05 (975.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:23:09.293763: step 10840, loss = 0.87 (996.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:23:10.579830: step 10850, loss = 1.01 (995.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:23:11.905698: step 10860, loss = 1.11 (965.4 examples/sec; 0.133 sec/batch)
2017-05-09 21:23:13.213543: step 10870, loss = 1.07 (978.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:23:14.546970: step 10880, loss = 0.76 (959.9 examples/sec; 0.133 sec/batch)
2017-05-09 21:23:15.838547: step 10890, loss = 0.94 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:23:17.218523: step 10900, loss = 0.93 (927.6 examples/sec; 0.138 sec/batch)
2017-05-09 21:23:18.431952: step 10910, loss = 0.95 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-09 21:23:19.720615: step 10920, loss = 0.86 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:23:21.008817: step 10930, loss = 0.98 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:23:22.314413: step 10940, loss = 0.98 (980.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:23:23.592495: step 10950, loss = 0.90 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:23:24.906338: step 10960, loss = 0.93 (974.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:23:26.245108: step 10970, loss = 1.10 (956.1 examples/sec; 0.134 sec/batch)
2017-05-09 21:23:27.528125: step 10980, loss = 0.90 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:23:28.829390: step 10990, loss = 0.97 (983.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:23:30.222927: step 11000, loss = 1.02 (918.5 examples/sec; 0.139 sec/batch)
2017-05-09 21:23:31.439978: step 11010, loss = 0.91 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-09 21:23:32.728510: step 11020, loss = 0.84 (993.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:23:34.015819: step 11030, loss = 0.83 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:23:35.320088: step 11040, loss = 1.10 (981.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:23:36.649600: step 11050, loss = 1.11 (962.7 examples/sec; 0.133 sec/batch)
2017-05-09 21:23:37.960787: step 11060, loss = 1.13 (976.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:23:39.260079: step 11070, loss = 0.91 (985.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:23:40.570294: step 11080, loss = 0.95 (976.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:23:41.885995: step 11090, loss = 0.85 (972.9 examples/sec; 0.132 sec/batch)
2017-05-09 21:23:43.304593: step 11100, loss = 0.85 (902.3 examples/sec; 0.142 sec/batch)
2017-05-09 21:23:44.528636: step 11110, loss = 1.02 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-09 21:23:45.820266: step 11120, loss = 0.80 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:23:47.148661: step 11130, loss = 0.87 (963.6 examples/sec; 0.133 sec/batch)
2017-05-09 21:23:48.451319: step 11140, loss = 0.91 (982.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:23:49.766738: step 11150, loss = 0.98 (973.1 examples/sec; 0.132 sec/batch)
2017-05-09 21:23:51.088411: step 11160, loss = 0.83 (968.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:23:52.404696: step 11170, loss = 0.71 (972.4 examples/sec; 0.132 sec/batch)
2017-05-09 21:23:53.724944: step 11180, loss = 0.72 (969.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:23:55.014015: step 11190, loss = 0.89 (993.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:23:56.409816: step 11200, loss = 0.97 (917.0 examples/sec; 0.140 sec/batch)
2017-05-09 21:23:57.605271: step 11210, loss = 0.77 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-09 21:23:58.903224: step 11220, loss = 0.88 (986.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:24:00.184533: step 11230, loss = 1.21 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:24:01.484054: step 11240, loss = 0.90 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:24:02.779992: step 11250, loss = 1.02 (987.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:24:04.067694: step 11260, loss = 0.83 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:24:05.368832: step 11270, loss = 0.85 (983.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:24:06.675610: step 11280, loss = 1.01 (979.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:24:07.965314: step 11290, loss = 0.94 (992.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:24:09.376668: step 11300, loss = 1.07 (906.9 examples/sec; 0.141 sec/batch)
2017-05-09 21:24:10.615670: step 11310, loss = 0.98 (1033.1 examples/sec; 0.124 sec/batch)
2017-05-09 21:24:11.896201: step 11320, loss = 0.91 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:24:13.206136: step 11330, loss = 0.93 (977.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:24:14.506492: step 11340, loss = 0.84 (984.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:24:15.787765: step 11350, loss = 0.94 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:24:17.079417: step 11360, loss = 0.91 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:24:18.375561: step 11370, loss = 0.93 (987.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:24:19.658735: step 11380, loss = 1.06 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:24:20.957037: step 11390, loss = 0.86 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:24:22.344360: step 11400, loss = 0.81 (922.6 examples/sec; 0.139 sec/batch)
2017-05-09 21:24:23.581809: step 11410, loss = 0.89 (1034.4 examples/sec; 0.124 sec/batch)
2017-05-09 21:24:24.883404: step 11420, loss = 0.82 (983.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:24:26.200015: step 11430, loss = 0.87 (972.2 examples/sec; 0.132 sec/batch)
2017-05-09 21:24:27.505340: step 11440, loss = 0.79 (980.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:24:28.796880: step 11450, loss = 0.79 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 21E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 245 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
:24:30.093954: step 11460, loss = 0.93 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:24:31.385044: step 11470, loss = 0.85 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:24:32.710954: step 11480, loss = 1.14 (965.4 examples/sec; 0.133 sec/batch)
2017-05-09 21:24:33.998388: step 11490, loss = 0.94 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:24:35.392787: step 11500, loss = 0.70 (918.0 examples/sec; 0.139 sec/batch)
2017-05-09 21:24:36.596843: step 11510, loss = 0.90 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-09 21:24:37.935931: step 11520, loss = 0.85 (955.9 examples/sec; 0.134 sec/batch)
2017-05-09 21:24:39.233075: step 11530, loss = 0.95 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:24:40.561706: step 11540, loss = 1.18 (963.4 examples/sec; 0.133 sec/batch)
2017-05-09 21:24:41.904779: step 11550, loss = 0.78 (953.0 examples/sec; 0.134 sec/batch)
2017-05-09 21:24:43.184062: step 11560, loss = 0.93 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:24:44.483565: step 11570, loss = 0.99 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:24:45.804941: step 11580, loss = 1.09 (968.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:24:47.109091: step 11590, loss = 1.40 (981.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:24:48.511372: step 11600, loss = 0.93 (912.8 examples/sec; 0.140 sec/batch)
2017-05-09 21:24:49.685869: step 11610, loss = 0.88 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-09 21:24:50.972677: step 11620, loss = 0.97 (994.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:24:52.243701: step 11630, loss = 0.89 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:24:53.543664: step 11640, loss = 0.97 (984.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:24:54.837854: step 11650, loss = 0.96 (989.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:24:56.115835: step 11660, loss = 0.89 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:24:57.397960: step 11670, loss = 1.01 (998.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:24:58.689603: step 11680, loss = 0.85 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:24:59.961216: step 11690, loss = 0.98 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:25:01.369254: step 11700, loss = 1.10 (909.1 examples/sec; 0.141 sec/batch)
2017-05-09 21:25:02.567990: step 11710, loss = 1.09 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-09 21:25:03.863322: step 11720, loss = 0.89 (988.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:25:05.171734: step 11730, loss = 0.80 (978.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:25:06.462147: step 11740, loss = 1.01 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:25:07.763235: step 11750, loss = 1.17 (983.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:25:09.061480: step 11760, loss = 1.02 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:25:10.396596: step 11770, loss = 0.90 (958.7 examples/sec; 0.134 sec/batch)
2017-05-09 21:25:11.703206: step 11780, loss = 1.00 (979.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:25:13.005677: step 11790, loss = 0.87 (982.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:25:14.389727: step 11800, loss = 1.13 (924.8 examples/sec; 0.138 sec/batch)
2017-05-09 21:25:15.566243: step 11810, loss = 0.97 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-09 21:25:16.869183: step 11820, loss = 0.96 (982.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:25:18.156925: step 11830, loss = 0.85 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:25:19.464523: step 11840, loss = 0.85 (978.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:25:20.800175: step 11850, loss = 0.93 (958.3 examples/sec; 0.134 sec/batch)
2017-05-09 21:25:22.128264: step 11860, loss = 0.88 (963.8 examples/sec; 0.133 sec/batch)
2017-05-09 21:25:23.430510: step 11870, loss = 0.75 (982.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:25:24.732424: step 11880, loss = 0.98 (983.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:25:26.015253: step 11890, loss = 0.90 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:25:27.414874: step 11900, loss = 0.91 (914.5 examples/sec; 0.140 sec/batch)
2017-05-09 21:25:28.605282: step 11910, loss = 1.05 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-09 21:25:29.895557: step 11920, loss = 0.85 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:25:31.178561: step 11930, loss = 0.95 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:25:32.479299: step 11940, loss = 0.85 (984.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:25:33.774478: step 11950, loss = 0.75 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:25:35.067302: step 11960, loss = 0.94 (990.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:25:36.343709: step 11970, loss = 0.77 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:25:37.638097: step 11980, loss = 1.09 (988.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:25:38.934747: step 11990, loss = 0.99 (987.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:25:40.331534: step 12000, loss = 0.86 (916.4 examples/sec; 0.140 sec/batch)
2017-05-09 21:25:41.541877: step 12010, loss = 0.99 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-09 21:25:42.878068: step 12020, loss = 0.81 (957.9 examples/sec; 0.134 sec/batch)
2017-05-09 21:25:44.189500: step 12030, loss = 1.03 (976.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:25:45.510587: step 12040, loss = 1.18 (968.9 examples/sec; 0.132 sec/batch)
2017-05-09 21:25:46.843778: step 12050, loss = 0.92 (960.1 examples/sec; 0.133 sec/batch)
2017-05-09 21:25:48.147780: step 12060, loss = 0.68 (981.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:25:49.456302: step 12070, loss = 0.89 (978.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:25:50.763832: step 12080, loss = 0.97 (978.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:25:52.062427: step 12090, loss = 1.03 (985.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:25:53.471047: step 12100, loss = 1.03 (908.7 examples/sec; 0.141 sec/batch)
2017-05-09 21:25:54.713969: step 12110, loss = 1.01 (1029.8 examples/sec; 0.124 sec/batch)
2017-05-09 21:25:55.994297: step 12120, loss = 0.88 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:25:57.314675: step 12130, loss = 0.78 (969.4 examples/sec; 0.132 sec/batch)
2017-05-09 21:25:58.627948: step 12140, loss = 0.95 (974.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:25:59.962925: step 12150, loss = 1.23 (958.8 examples/sec; 0.133 sec/batch)
2017-05-09 21:26:01.255409: step 12160, loss = 0.87 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:26:02.585790: step 12170, loss = 0.81 (962.1 examples/sec; 0.133 sec/batch)
2017-05-09 21:26:03.878293: step 12180, loss = 1.00 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:26:05.169238: step 12190, loss = 0.85 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:26:06.538500: step 12200, loss = 0.88 (934.8 examples/sec; 0.137 sec/batch)
2017-05-09 21:26:07.732847: step 12210, loss = 0.99 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-09 21:26:09.040524: step 12220, loss = 0.78 (978.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:26:10.337823: step 12230, loss = 0.93 (986.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:26:11.633229: step 12240, loss = 1.03 (988.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:26:12.922108: step 12250, loss = 0.96 (993.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:26:14.226738: step 12260, loss = 1.13 (981.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:26:15.491132: step 12270, loss = 0.80 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-09 21:26:16.775848: step 12280, loss = 0.92 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:26:18.093842: step 12290, loss = 1.00 (971.2 examples/sec; 0.132 sec/batch)
2017-05-09 21:26:19.496217: step 12300, loss = 0.83 (912.7 examples/sec; 0.140 sec/batch)
2017-05-09 21:26:20.727724: step 12310, loss = 0.86 (1039.4 examples/sec; 0.123 sec/batch)
2017-05-09 21:26:22.052481: step 12320, loss = 1.05 (966.2 examples/sec; 0.132 sec/batch)
2017-05-09 21:26:23.346063: step 12330, loss = 0.93 (989.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:26:24.635470: step 12340, loss = 0.98 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:26:25.959118: step 12350, loss = 0.89 (967.0 examples/sec; 0.132 sec/batch)
2017-05-09 21:26:27.298477: step 12360, loss = 0.82 (955.7 examples/sec; 0.134 sec/batch)
2017-05-09 21:26:28.613146: step 12370, loss = 0.81 (973.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:26:29.914536: step 12380, loss = 0.82 (983.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:26:31.216627: step 12390, loss = 0.92 (983.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:26:32.604276: step 12400, loss = 0.82 (922.4 examples/sec; 0.139 sec/batch)
2017-05-09 21:26:33.801883: step 12410, loss = 0.99 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-09 21:26:35.085631: step 12420, loss = 0.69 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:26:36.372423: step 12430, loss = 0.76 (994.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:26:37.700755: step 12440, loss = 0.91 (963.6 examples/sec; 0.133 sec/batch)
2017-05-09 21:26:39.026637: step 12450, loss = 0.86 (965.4 examples/sec; 0.133 sec/batch)
2017-05-09 21:26:40.335984: step 12460, loss = 0.85 (977.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:26:41.614107: step 12470, loss = 0.98 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:26:42.917629: step 12480, loss = 0.89 (982.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:26:44.200094: step 12490, loss = 0.79 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:26:45.614285: step 12500, loss = 0.83 (905.1 examples/sec; 0.141 sec/batch)
2017-05-09 21:26:46.845674: step 12510, loss = 1.14 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-09 21:26:48.145622: step 12520, loss = 0.72 (984.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:26:49.459209: step 12530, loss = 0.95 (974.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:26:50.784348: step 12540, loss = 0.79 (965.9 examples/sec; 0.133 sec/batch)
2017-05-09 21:26:52.080560: step 12550, loss = 0.92 (987.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:26:53.395319: step 12560, loss = 0.94 (973.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:26:54.687520: step 12570, loss = 0.91 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:26:55.997166: step 12580, loss = 0.94 (977.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:26:57.309250: step 12590, loss = 1.14 (975.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:26:58.732591: step 12600, loss = 1.08 (899.3 examples/sec; 0.142 sec/batch)
2017-05-09 21:26:59.940369: step 12610, loss = 0.85 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-09 21:27:01.260693: step 12620, loss = 0.92 (969.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:27:02.539256: step 12630, loss = 0.97 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:27:03.840081: step 12640, loss = 0.96 (984.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:27:05.140297: step 12650, loss = 0.91 (984.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:27:06.444427: step 12660, loss = 0.99 (981.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:27:07.739767: step 12670, loss = 1.00 (988.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:27:09.039938: step 12680, loss = 0.81 (984.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:27:10.337146: step 12690, loss = 0.84 (986.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:27:11.769515: step 12700, loss = 1.04 (893.6 examples/sec; 0.143 sec/batch)
2017-05-09 21:27:12.966378: step 12710, loss = 0.80 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-09 21:27:14.288642: step 12720, loss = 0.94 (968.0 examples/sec; 0.132 sec/batch)
2017-05-09 21:27:15.562803: step 12730, loss = 0.81 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:27:16.846552: step 12740, loss = 0.97 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:27:18.129779: step 12750, loss = 1.02 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:27:19.406917: step 12760, loss = 0.91 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:27:20.688619: step 12770, loss = 1.03 (998.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:27:21.988141: step 12780, loss = 0.97 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:27:23.289255: step 12790, loss = 0.80 (983.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:27:24.700500: step 12800, loss = 1.15 (907.0 examples/sec; 0.141 sec/batch)
2017-05-09 21:27:25.918294: step 12810, loss = 0.87 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-09 21:27:27.254062: step 1282E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 265 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
0, loss = 0.77 (958.3 examples/sec; 0.134 sec/batch)
2017-05-09 21:27:28.574584: step 12830, loss = 0.96 (969.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:27:29.880848: step 12840, loss = 0.90 (979.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:27:31.181896: step 12850, loss = 0.82 (983.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:27:32.468102: step 12860, loss = 0.84 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:27:33.763427: step 12870, loss = 0.76 (988.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:27:35.072012: step 12880, loss = 0.80 (978.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:27:36.377888: step 12890, loss = 0.94 (980.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:27:37.773503: step 12900, loss = 0.91 (917.2 examples/sec; 0.140 sec/batch)
2017-05-09 21:27:38.962123: step 12910, loss = 0.92 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-09 21:27:40.239108: step 12920, loss = 0.94 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:27:41.532064: step 12930, loss = 0.85 (990.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:27:42.818250: step 12940, loss = 0.95 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:27:44.133071: step 12950, loss = 1.12 (973.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:27:45.461279: step 12960, loss = 0.85 (963.7 examples/sec; 0.133 sec/batch)
2017-05-09 21:27:46.736160: step 12970, loss = 0.84 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:27:48.022365: step 12980, loss = 0.94 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:27:49.330172: step 12990, loss = 0.83 (978.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:27:50.748759: step 13000, loss = 1.00 (902.3 examples/sec; 0.142 sec/batch)
2017-05-09 21:27:51.962788: step 13010, loss = 1.01 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-09 21:27:53.264436: step 13020, loss = 0.83 (983.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:27:54.590635: step 13030, loss = 0.85 (965.2 examples/sec; 0.133 sec/batch)
2017-05-09 21:27:55.895688: step 13040, loss = 0.96 (980.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:27:57.178104: step 13050, loss = 0.94 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:27:58.481279: step 13060, loss = 0.95 (982.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:27:59.791598: step 13070, loss = 1.18 (976.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:28:01.111897: step 13080, loss = 0.82 (969.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:28:02.448598: step 13090, loss = 0.83 (957.6 examples/sec; 0.134 sec/batch)
2017-05-09 21:28:03.855817: step 13100, loss = 0.88 (909.6 examples/sec; 0.141 sec/batch)
2017-05-09 21:28:05.089975: step 13110, loss = 1.07 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-09 21:28:06.375819: step 13120, loss = 1.09 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:28:07.678123: step 13130, loss = 1.00 (982.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:28:08.956239: step 13140, loss = 0.90 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:28:10.251215: step 13150, loss = 0.80 (988.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:28:11.553646: step 13160, loss = 1.00 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:28:12.823825: step 13170, loss = 0.82 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:28:14.167996: step 13180, loss = 0.99 (952.3 examples/sec; 0.134 sec/batch)
2017-05-09 21:28:15.486706: step 13190, loss = 1.14 (970.6 examples/sec; 0.132 sec/batch)
2017-05-09 21:28:16.901227: step 13200, loss = 0.92 (904.9 examples/sec; 0.141 sec/batch)
2017-05-09 21:28:18.111729: step 13210, loss = 0.89 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-09 21:28:19.409945: step 13220, loss = 0.83 (986.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:28:20.702823: step 13230, loss = 0.88 (990.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:28:21.995293: step 13240, loss = 1.12 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:28:23.299079: step 13250, loss = 0.99 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:28:24.597945: step 13260, loss = 1.02 (985.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:28:25.911531: step 13270, loss = 0.94 (974.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:28:27.194578: step 13280, loss = 1.11 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:28:28.493314: step 13290, loss = 0.87 (985.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:28:29.901702: step 13300, loss = 1.05 (908.8 examples/sec; 0.141 sec/batch)
2017-05-09 21:28:31.112281: step 13310, loss = 0.90 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-09 21:28:32.451099: step 13320, loss = 0.99 (956.1 examples/sec; 0.134 sec/batch)
2017-05-09 21:28:33.748243: step 13330, loss = 0.92 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:28:35.049405: step 13340, loss = 0.83 (983.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:28:36.362394: step 13350, loss = 0.84 (974.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:28:37.658512: step 13360, loss = 0.75 (987.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:28:38.945663: step 13370, loss = 0.67 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:28:40.251420: step 13380, loss = 0.88 (980.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:28:41.523199: step 13390, loss = 0.86 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-09 21:28:42.922268: step 13400, loss = 0.87 (914.9 examples/sec; 0.140 sec/batch)
2017-05-09 21:28:44.158975: step 13410, loss = 0.88 (1035.0 examples/sec; 0.124 sec/batch)
2017-05-09 21:28:45.477790: step 13420, loss = 0.90 (970.6 examples/sec; 0.132 sec/batch)
2017-05-09 21:28:46.768377: step 13430, loss = 0.85 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:28:48.046288: step 13440, loss = 0.93 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:28:49.334017: step 13450, loss = 0.72 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:28:50.645388: step 13460, loss = 0.96 (976.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:28:51.944912: step 13470, loss = 1.01 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:28:53.253959: step 13480, loss = 1.02 (977.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:28:54.564243: step 13490, loss = 0.90 (976.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:28:55.969608: step 13500, loss = 0.78 (910.8 examples/sec; 0.141 sec/batch)
2017-05-09 21:28:57.167761: step 13510, loss = 0.92 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-09 21:28:58.468431: step 13520, loss = 0.79 (984.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:28:59.795237: step 13530, loss = 0.84 (964.7 examples/sec; 0.133 sec/batch)
2017-05-09 21:29:01.115059: step 13540, loss = 0.89 (969.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:29:02.420546: step 13550, loss = 1.00 (980.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:29:03.724110: step 13560, loss = 0.71 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:29:05.032129: step 13570, loss = 0.91 (978.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:29:06.331123: step 13580, loss = 0.96 (985.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:29:07.610304: step 13590, loss = 0.82 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:29:09.002329: step 13600, loss = 0.85 (919.5 examples/sec; 0.139 sec/batch)
2017-05-09 21:29:10.205393: step 13610, loss = 0.86 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-09 21:29:11.508936: step 13620, loss = 0.94 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:29:12.794987: step 13630, loss = 0.90 (995.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:29:14.123509: step 13640, loss = 0.78 (963.5 examples/sec; 0.133 sec/batch)
2017-05-09 21:29:15.442855: step 13650, loss = 1.04 (970.2 examples/sec; 0.132 sec/batch)
2017-05-09 21:29:16.730894: step 13660, loss = 0.97 (993.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:29:18.005109: step 13670, loss = 0.91 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-09 21:29:19.309560: step 13680, loss = 0.75 (981.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:29:20.601687: step 13690, loss = 0.92 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:29:22.016644: step 13700, loss = 1.05 (904.6 examples/sec; 0.141 sec/batch)
2017-05-09 21:29:23.219054: step 13710, loss = 0.88 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-09 21:29:24.571895: step 13720, loss = 0.81 (946.2 examples/sec; 0.135 sec/batch)
2017-05-09 21:29:25.88E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 285 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
6812: step 13730, loss = 0.91 (973.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:29:27.201243: step 13740, loss = 0.99 (973.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:29:28.482873: step 13750, loss = 0.96 (998.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:29:29.772825: step 13760, loss = 0.78 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:29:31.099696: step 13770, loss = 0.82 (964.7 examples/sec; 0.133 sec/batch)
2017-05-09 21:29:32.397409: step 13780, loss = 0.90 (986.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:29:33.739294: step 13790, loss = 0.81 (953.9 examples/sec; 0.134 sec/batch)
2017-05-09 21:29:35.146893: step 13800, loss = 0.81 (909.3 examples/sec; 0.141 sec/batch)
2017-05-09 21:29:36.341925: step 13810, loss = 0.84 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-09 21:29:37.635879: step 13820, loss = 0.85 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:29:38.930026: step 13830, loss = 0.98 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:29:40.240100: step 13840, loss = 0.82 (977.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:29:41.525400: step 13850, loss = 0.93 (995.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:29:42.827696: step 13860, loss = 0.75 (982.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:29:44.086665: step 13870, loss = 0.80 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-09 21:29:45.393872: step 13880, loss = 0.89 (979.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:29:46.679087: step 13890, loss = 0.89 (995.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:29:48.067044: step 13900, loss = 1.07 (922.2 examples/sec; 0.139 sec/batch)
2017-05-09 21:29:49.315150: step 13910, loss = 1.10 (1025.5 examples/sec; 0.125 sec/batch)
2017-05-09 21:29:50.631326: step 13920, loss = 1.06 (972.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:29:51.893400: step 13930, loss = 0.86 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-09 21:29:53.187800: step 13940, loss = 0.92 (988.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:29:54.486840: step 13950, loss = 0.99 (985.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:29:55.794748: step 13960, loss = 0.91 (978.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:29:57.102113: step 13970, loss = 0.96 (979.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:29:58.391012: step 13980, loss = 0.96 (993.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:29:59.704348: step 13990, loss = 0.93 (974.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:30:01.093789: step 14000, loss = 1.48 (921.2 examples/sec; 0.139 sec/batch)
2017-05-09 21:30:02.335371: step 14010, loss = 0.85 (1030.9 examples/sec; 0.124 sec/batch)
2017-05-09 21:30:03.663261: step 14020, loss = 0.76 (963.9 examples/sec; 0.133 sec/batch)
2017-05-09 21:30:04.967670: step 14030, loss = 1.03 (981.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:30:06.255243: step 14040, loss = 0.97 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:30:07.558170: step 14050, loss = 0.77 (982.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:30:08.886900: step 14060, loss = 1.04 (963.3 examples/sec; 0.133 sec/batch)
2017-05-09 21:30:10.220779: step 14070, loss = 1.02 (959.6 examples/sec; 0.133 sec/batch)
2017-05-09 21:30:11.489866: step 14080, loss = 0.91 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:30:12.802549: step 14090, loss = 0.83 (975.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:30:14.193238: step 14100, loss = 0.96 (920.4 examples/sec; 0.139 sec/batch)
2017-05-09 21:30:15.412293: step 14110, loss = 0.87 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-09 21:30:16.733762: step 14120, loss = 0.86 (968.6 examples/sec; 0.132 sec/batch)
2017-05-09 21:30:18.046861: step 14130, loss = 0.81 (974.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:30:19.347683: step 14140, loss = 0.99 (984.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:30:20.637772: step 14150, loss = 0.96 (992.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:30:21.914592: step 14160, loss = 0.86 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:30:23.233725: step 14170, loss = 0.82 (970.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:30:24.558994: step 14180, loss = 1.01 (965.8 examples/sec; 0.133 sec/batch)
2017-05-09 21:30:25.845831: step 14190, loss = 1.05 (994.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:30:27.240636: step 14200, loss = 0.91 (917.7 examples/sec; 0.139 sec/batch)
2017-05-09 21:30:28.411907: step 14210, loss = 1.00 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-09 21:30:29.715566: step 14220, loss = 0.86 (981.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:30:31.003596: step 14230, loss = 0.96 (993.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:30:32.283292: step 14240, loss = 0.91 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:30:33.605950: step 14250, loss = 0.75 (967.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:30:34.900999: step 14260, loss = 0.91 (988.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:30:36.244544: step 14270, loss = 0.68 (952.7 examples/sec; 0.134 sec/batch)
2017-05-09 21:30:37.540994: step 14280, loss = 0.99 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:30:38.842042: step 14290, loss = 0.85 (983.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:30:40.258236: step 14300, loss = 0.81 (903.8 examples/sec; 0.142 sec/batch)
2017-05-09 21:30:41.476926: step 14310, loss = 0.98 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-09 21:30:42.799522: step 14320, loss = 0.98 (967.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:30:44.105545: step 14330, loss = 0.77 (980.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:30:45.421217: step 14340, loss = 0.77 (972.9 examples/sec; 0.132 sec/batch)
2017-05-09 21:30:46.736563: step 14350, loss = 0.94 (973.1 examples/sec; 0.132 sec/batch)
2017-05-09 21:30:48.033618: step 14360, loss = 0.84 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:30:49.339312: step 14370, loss = 0.85 (980.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:30:50.626300: step 14380, loss = 0.90 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:30:51.914175: step 14390, loss = 0.79 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:30:53.313768: step 14400, loss = 0.99 (914.6 examples/sec; 0.140 sec/batch)
2017-05-09 21:30:54.549804: step 14410, loss = 0.83 (1035.6 examples/sec; 0.124 sec/batch)
2017-05-09 21:30:55.827060: step 14420, loss = 0.99 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:30:57.110860: step 14430, loss = 0.82 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:30:58.416854: step 14440, loss = 0.92 (980.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:30:59.732914: step 14450, loss = 0.95 (972.6 examples/sec; 0.132 sec/batch)
2017-05-09 21:31:01.058605: step 14460, loss = 0.83 (965.5 examples/sec; 0.133 sec/batch)
2017-05-09 21:31:02.323721: step 14470, loss = 1.06 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-09 21:31:03.594080: step 14480, loss = 0.90 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:31:04.879926: step 14490, loss = 0.97 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:31:06.272975: step 14500, loss = 0.76 (918.8 examples/sec; 0.139 sec/batch)
2017-05-09 21:31:07.465934: step 14510, loss = 0.76 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-09 21:31:08.751474: step 14520, loss = 0.86 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:31:10.043112: step 14530, loss = 0.89 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:31:11.324041: step 14540, loss = 0.92 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:31:12.604899: step 14550, loss = 0.91 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:31:13.890787: step 14560, loss = 0.71 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:31:15.194721: step 14570, loss = 0.87 (981.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:31:16.487555: step 14580, loss = 0.84 (990.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:31:17.770400: step 14590, loss = 0.81 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:31:19.149135: step 14600, loss = 0.99 (928.4 examples/sec; 0.138 sec/batch)
2017-05-09 21:31:20.345003: step 14610, loss = 1.01 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-09 21:31:21.653313: step 14620, loss = 0.86 (978.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:31:22.940965: step 14630, loss = 0.94 (994.1 examples/sec; 0.129 sec/batch)
2017-05E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 304 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
-09 21:31:24.219628: step 14640, loss = 0.74 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:31:25.497015: step 14650, loss = 0.79 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:31:26.811415: step 14660, loss = 0.95 (973.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:31:28.096874: step 14670, loss = 0.97 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:31:29.388765: step 14680, loss = 0.86 (990.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:31:30.671979: step 14690, loss = 0.83 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:31:32.067792: step 14700, loss = 0.99 (917.0 examples/sec; 0.140 sec/batch)
2017-05-09 21:31:33.259169: step 14710, loss = 0.86 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-09 21:31:34.603963: step 14720, loss = 1.05 (951.8 examples/sec; 0.134 sec/batch)
2017-05-09 21:31:35.907818: step 14730, loss = 0.86 (981.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:31:37.223991: step 14740, loss = 0.95 (972.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:31:38.544650: step 14750, loss = 0.84 (969.2 examples/sec; 0.132 sec/batch)
2017-05-09 21:31:39.845237: step 14760, loss = 1.05 (984.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:31:41.172720: step 14770, loss = 0.92 (964.2 examples/sec; 0.133 sec/batch)
2017-05-09 21:31:42.472291: step 14780, loss = 0.95 (984.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:31:43.771843: step 14790, loss = 0.88 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:31:45.180817: step 14800, loss = 0.72 (908.5 examples/sec; 0.141 sec/batch)
2017-05-09 21:31:46.389307: step 14810, loss = 0.93 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-09 21:31:47.669147: step 14820, loss = 0.97 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:31:48.932858: step 14830, loss = 0.94 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-09 21:31:50.244676: step 14840, loss = 0.80 (975.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:31:51.524041: step 14850, loss = 0.87 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:31:52.828373: step 14860, loss = 0.82 (981.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:31:54.144740: step 14870, loss = 0.85 (972.4 examples/sec; 0.132 sec/batch)
2017-05-09 21:31:55.453817: step 14880, loss = 0.93 (977.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:31:56.761416: step 14890, loss = 1.01 (978.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:31:58.159064: step 14900, loss = 0.74 (915.8 examples/sec; 0.140 sec/batch)
2017-05-09 21:31:59.344215: step 14910, loss = 0.77 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-09 21:32:00.636354: step 14920, loss = 0.87 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:32:01.903711: step 14930, loss = 0.85 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:32:03.178117: step 14940, loss = 0.84 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:32:04.480129: step 14950, loss = 0.83 (983.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:32:05.760631: step 14960, loss = 1.03 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:32:07.023691: step 14970, loss = 0.73 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-09 21:32:08.314217: step 14980, loss = 0.82 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:32:09.616382: step 14990, loss = 1.06 (983.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:32:11.012250: step 15000, loss = 0.90 (917.0 examples/sec; 0.140 sec/batch)
2017-05-09 21:32:12.228227: step 15010, loss = 1.21 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-09 21:32:13.554587: step 15020, loss = 0.87 (965.1 examples/sec; 0.133 sec/batch)
2017-05-09 21:32:14.825206: step 15030, loss = 0.90 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:32:16.168363: step 15040, loss = 0.98 (953.0 examples/sec; 0.134 sec/batch)
2017-05-09 21:32:17.457492: step 15050, loss = 0.87 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:32:18.777697: step 15060, loss = 1.04 (969.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:32:20.072837: step 15070, loss = 0.94 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:32:21.404755: step 15080, loss = 0.95 (961.0 examples/sec; 0.133 sec/batch)
2017-05-09 21:32:22.696691: step 15090, loss = 1.13 (990.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:32:24.098194: step 15100, loss = 1.02 (913.3 examples/sec; 0.140 sec/batch)
2017-05-09 21:32:25.290528: step 15110, loss = 0.96 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-09 21:32:26.575880: step 15120, loss = 0.80 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:32:27.888356: step 15130, loss = 0.91 (975.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:32:29.175769: step 15140, loss = 0.87 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:32:30.515856: step 15150, loss = 0.98 (955.2 examples/sec; 0.134 sec/batch)
2017-05-09 21:32:31.840200: step 15160, loss = 0.99 (966.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:32:33.146635: step 15170, loss = 1.12 (979.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:32:34.471321: step 15180, loss = 0.93 (966.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:32:35.800888: step 15190, loss = 1.00 (962.7 examples/sec; 0.133 sec/batch)
2017-05-09 21:32:37.206606: step 15200, loss = 1.00 (910.6 examples/sec; 0.141 sec/batch)
2017-05-09 21:32:38.409368: step 15210, loss = 0.77 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-09 21:32:39.714485: step 15220, loss = 0.92 (980.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:32:40.997533: step 15230, loss = 0.90 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:32:42.272873: step 15240, loss = 0.83 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:32:43.563517: step 15250, loss = 0.91 (991.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:32:44.861232: step 15260, loss = 0.78 (986.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:32:46.153403: step 15270, loss = 0.77 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:32:47.475784: step 15280, loss = 0.98 (967.9 examples/sec; 0.132 sec/batch)
2017-05-09 21:32:48.797906: step 15290, loss = 0.96 (968.1 examples/sec; 0.132 sec/batch)
2017-05-09 21:32:50.185273: step 15300, loss = 0.87 (922.6 examples/sec; 0.139 sec/batch)
2017-05-09 21:32:51.425938: step 15310, loss = 0.89 (1031.7 examples/sec; 0.124 sec/batch)
2017-05-09 21:32:52.741424: step 15320, loss = 0.87 (973.0 examples/sec; 0.132 sec/batch)
2017-05-09 21:32:54.035714: step 15330, loss = 0.93 (989.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:32:55.327924: step 15340, loss = 0.79 (990.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:32:56.614145: step 15350, loss = 0.88 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:32:57.891260: step 15360, loss = 0.84 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:32:59.188572: step 15370, loss = 0.82 (986.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:33:00.484689: step 15380, loss = 0.89 (987.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:33:01.785347: step 15390, loss = 0.96 (984.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:33:03.185778: step 15400, loss = 1.02 (914.0 examples/sec; 0.140 sec/batch)
2017-05-09 21:33:04.393758: step 15410, loss = 0.83 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-09 21:33:05.667913: step 15420, loss = 0.88 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:33:06.937552: step 15430, loss = 0.92 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:33:08.219401: step 15440, loss = 1.08 (998.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:33:09.514385: step 15450, loss = 0.85 (988.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:33:10.807905: step 15460, loss = 0.85 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:33:12.096347: step 15470, loss = 0.81 (993.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:33:13.386463: step 15480, loss = 1.01 (992.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:33:14.661365: step 15490, loss = 0.86 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:33:16.073989: step 15500, loss = 0.88 (906.1 examples/sec; 0.141 sec/batch)
2017-05-09 21:33:17.283520: step 15510, loss = 0.77 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-09 21:33:18.562868: step 15520, loss = 0.92 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:33:19.874182: step 15530, loss = 0.95 (976.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:33:21.195702: step 15540, loss = 1.08 (968.6 examples/sec; 0.E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 324 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
132 sec/batch)
2017-05-09 21:33:22.502965: step 15550, loss = 0.89 (979.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:33:23.792656: step 15560, loss = 0.91 (992.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:33:25.061131: step 15570, loss = 0.91 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:33:26.375852: step 15580, loss = 0.77 (973.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:33:27.700215: step 15590, loss = 1.03 (966.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:33:29.086902: step 15600, loss = 0.95 (923.1 examples/sec; 0.139 sec/batch)
2017-05-09 21:33:30.274022: step 15610, loss = 0.90 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-09 21:33:31.581791: step 15620, loss = 0.80 (978.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:33:32.893232: step 15630, loss = 0.91 (976.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:33:34.216499: step 15640, loss = 0.98 (967.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:33:35.507791: step 15650, loss = 0.95 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:33:36.804718: step 15660, loss = 0.85 (986.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:33:38.106699: step 15670, loss = 0.99 (983.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:33:39.417309: step 15680, loss = 0.90 (976.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:33:40.714152: step 15690, loss = 1.01 (987.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:33:42.083086: step 15700, loss = 1.27 (935.0 examples/sec; 0.137 sec/batch)
2017-05-09 21:33:43.288547: step 15710, loss = 1.11 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-09 21:33:44.615151: step 15720, loss = 0.83 (964.9 examples/sec; 0.133 sec/batch)
2017-05-09 21:33:45.906293: step 15730, loss = 0.81 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:33:47.219645: step 15740, loss = 0.88 (974.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:33:48.532156: step 15750, loss = 0.97 (975.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:33:49.822725: step 15760, loss = 0.90 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:33:51.127900: step 15770, loss = 0.87 (980.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:33:52.418646: step 15780, loss = 0.87 (991.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:33:53.728927: step 15790, loss = 0.89 (976.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:33:55.129812: step 15800, loss = 0.82 (913.7 examples/sec; 0.140 sec/batch)
2017-05-09 21:33:56.303491: step 15810, loss = 0.90 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-09 21:33:57.578014: step 15820, loss = 0.83 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:33:58.866090: step 15830, loss = 0.78 (993.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:34:00.149548: step 15840, loss = 0.78 (997.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:34:01.440842: step 15850, loss = 0.79 (991.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:34:02.744841: step 15860, loss = 1.03 (981.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:34:04.046079: step 15870, loss = 0.77 (983.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:34:05.355920: step 15880, loss = 0.84 (977.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:34:06.663244: step 15890, loss = 0.75 (979.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:34:08.054496: step 15900, loss = 0.83 (920.0 examples/sec; 0.139 sec/batch)
2017-05-09 21:34:09.260786: step 15910, loss = 0.88 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-09 21:34:10.586505: step 15920, loss = 0.69 (965.5 examples/sec; 0.133 sec/batch)
2017-05-09 21:34:11.898648: step 15930, loss = 0.81 (975.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:34:13.190436: step 15940, loss = 0.92 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:34:14.490395: step 15950, loss = 0.93 (984.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:34:15.793304: step 15960, loss = 0.81 (982.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:34:17.104624: step 15970, loss = 0.87 (976.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:34:18.423372: step 15980, loss = 0.84 (970.6 examples/sec; 0.132 sec/batch)
2017-05-09 21:34:19.718025: step 15990, loss = 0.88 (988.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:34:21.115020: step 16000, loss = 0.72 (916.3 examples/sec; 0.140 sec/batch)
2017-05-09 21:34:22.299732: step 16010, loss = 0.90 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-09 21:34:23.583255: step 16020, loss = 0.80 (997.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:34:24.887707: step 16030, loss = 0.86 (981.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:34:26.167671: step 16040, loss = 0.84 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:34:27.457825: step 16050, loss = 0.95 (992.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:34:28.764279: step 16060, loss = 0.83 (979.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:34:30.051233: step 16070, loss = 0.85 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:34:31.336275: step 16080, loss = 0.92 (996.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:34:32.612793: step 16090, loss = 0.72 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:34:34.004904: step 16100, loss = 0.86 (919.5 examples/sec; 0.139 sec/batch)
2017-05-09 21:34:35.191326: step 16110, loss = 0.80 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-09 21:34:36.485078: step 16120, loss = 0.68 (989.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:34:37.771258: step 16130, loss = 0.79 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:34:39.061242: step 16140, loss = 0.82 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:34:40.332305: step 16150, loss = 0.99 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:34:41.596196: step 16160, loss = 0.96 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-09 21:34:42.876716: step 16170, loss = 1.21 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:34:44.190872: step 16180, loss = 0.97 (974.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:34:45.473650: step 16190, loss = 0.79 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:34:46.858879: step 16200, loss = 0.80 (924.0 examples/sec; 0.139 sec/batch)
2017-05-09 21:34:48.053576: step 16210, loss = 0.93 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-09 21:34:49.334983: step 16220, loss = 0.91 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:34:50.622283: step 16230, loss = 0.79 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:34:51.931461: step 16240, loss = 0.75 (977.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:34:53.218215: step 16250, loss = 0.94 (994.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:34:54.510880: step 16260, loss = 0.97 (990.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:34:55.795653: step 16270, loss = 0.77 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:34:57.097999: step 16280, loss = 0.75 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:34:58.392086: step 16290, loss = 0.79 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:34:59.782276: step 16300, loss = 0.89 (920.7 examples/sec; 0.139 sec/batch)
2017-05-09 21:35:00.989152: step 16310, loss = 0.97 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-09 21:35:02.328880: step 16320, loss = 0.79 (955.4 examples/sec; 0.134 sec/batch)
2017-05-09 21:35:03.621763: step 16330, loss = 0.91 (990.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:35:04.931508: step 16340, loss = 0.82 (977.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:35:06.219083: step 16350, loss = 0.93 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:35:07.533766: step 16360, loss = 1.04 (973.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:35:08.866688: step 16370, loss = 0.92 (960.3 examples/sec; 0.133 sec/batch)
2017-05-09 21:35:10.177264: step 16380, loss = 0.90 (976.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:35:11.500555: step 16390, loss = 0.98 (967.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:35:12.890183: step 16400, loss = 1.02 (921.1 examples/sec; 0.139 sec/batch)
2017-05-09 21:35:14.075578: step 16410, loss = 0.93 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-09 21:35:15.390064: step 16420, loss = 0.87 (973.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:35:16.662437: step 16430, loss = 0.76 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:35:17.978327: step 16440, loss = 1.02 (972.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:35:19.310385: step 16450, loss = 0.70 (960.9 exE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 344 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
amples/sec; 0.133 sec/batch)
2017-05-09 21:35:20.609177: step 16460, loss = 0.92 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:35:21.887771: step 16470, loss = 0.70 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:35:23.211312: step 16480, loss = 0.99 (967.1 examples/sec; 0.132 sec/batch)
2017-05-09 21:35:24.466893: step 16490, loss = 0.95 (1019.4 examples/sec; 0.126 sec/batch)
2017-05-09 21:35:25.853576: step 16500, loss = 0.87 (923.1 examples/sec; 0.139 sec/batch)
2017-05-09 21:35:27.063827: step 16510, loss = 0.93 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-09 21:35:28.371811: step 16520, loss = 0.83 (978.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:35:29.720352: step 16530, loss = 1.16 (949.2 examples/sec; 0.135 sec/batch)
2017-05-09 21:35:31.016415: step 16540, loss = 0.71 (987.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:35:32.299587: step 16550, loss = 0.89 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:35:33.600343: step 16560, loss = 0.92 (984.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:35:34.909996: step 16570, loss = 0.85 (977.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:35:36.211144: step 16580, loss = 0.84 (983.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:35:37.497725: step 16590, loss = 0.95 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:35:38.906839: step 16600, loss = 0.95 (908.4 examples/sec; 0.141 sec/batch)
2017-05-09 21:35:40.115947: step 16610, loss = 0.90 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-09 21:35:41.407647: step 16620, loss = 0.92 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:35:42.706521: step 16630, loss = 0.92 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:35:44.017054: step 16640, loss = 0.94 (976.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:35:45.310122: step 16650, loss = 1.03 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:35:46.622036: step 16660, loss = 0.89 (975.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:35:47.907788: step 16670, loss = 0.92 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:35:49.209303: step 16680, loss = 0.89 (983.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:35:50.511649: step 16690, loss = 0.77 (982.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:35:51.895811: step 16700, loss = 0.89 (924.7 examples/sec; 0.138 sec/batch)
2017-05-09 21:35:53.088060: step 16710, loss = 0.84 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-09 21:35:54.393475: step 16720, loss = 0.93 (980.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:35:55.684859: step 16730, loss = 0.69 (991.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:35:56.989730: step 16740, loss = 0.94 (980.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:35:58.278500: step 16750, loss = 0.89 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:35:59.566931: step 16760, loss = 0.85 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:36:00.885045: step 16770, loss = 0.85 (971.1 examples/sec; 0.132 sec/batch)
2017-05-09 21:36:02.219379: step 16780, loss = 1.15 (959.3 examples/sec; 0.133 sec/batch)
2017-05-09 21:36:03.508280: step 16790, loss = 0.87 (993.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:36:04.908573: step 16800, loss = 0.74 (914.1 examples/sec; 0.140 sec/batch)
2017-05-09 21:36:06.120651: step 16810, loss = 0.77 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-09 21:36:07.420505: step 16820, loss = 1.13 (984.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:36:08.715968: step 16830, loss = 0.78 (988.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:36:09.990119: step 16840, loss = 1.00 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:36:11.304657: step 16850, loss = 0.90 (973.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:36:12.612898: step 16860, loss = 0.88 (978.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:36:13.907709: step 16870, loss = 0.86 (988.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:36:15.193054: step 16880, loss = 0.76 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:36:16.497664: step 16890, loss = 0.83 (981.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:36:17.879771: step 16900, loss = 1.05 (926.1 examples/sec; 0.138 sec/batch)
2017-05-09 21:36:19.078650: step 16910, loss = 0.99 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-09 21:36:20.403348: step 16920, loss = 0.91 (966.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:36:21.723544: step 16930, loss = 0.90 (969.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:36:23.020138: step 16940, loss = 1.06 (987.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:36:24.333343: step 16950, loss = 0.92 (974.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:36:25.639476: step 16960, loss = 1.17 (980.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:36:26.963747: step 16970, loss = 1.00 (966.6 examples/sec; 0.132 sec/batch)
2017-05-09 21:36:28.250352: step 16980, loss = 0.85 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:36:29.531807: step 16990, loss = 0.83 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:36:30.926062: step 17000, loss = 0.79 (918.1 examples/sec; 0.139 sec/batch)
2017-05-09 21:36:32.164677: step 17010, loss = 0.96 (1033.4 examples/sec; 0.124 sec/batch)
2017-05-09 21:36:33.440632: step 17020, loss = 0.87 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:36:34.731796: step 17030, loss = 0.81 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:36:35.997966: step 17040, loss = 0.97 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:36:37.300094: step 17050, loss = 0.81 (983.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:36:38.615762: step 17060, loss = 0.90 (972.9 examples/sec; 0.132 sec/batch)
2017-05-09 21:36:39.934361: step 17070, loss = 0.81 (970.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:36:41.228336: step 17080, loss = 0.76 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:36:42.525569: step 17090, loss = 0.97 (986.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:36:43.925536: step 17100, loss = 0.79 (914.3 examples/sec; 0.140 sec/batch)
2017-05-09 21:36:45.120814: step 17110, loss = 0.78 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-09 21:36:46.423573: step 17120, loss = 0.76 (982.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:36:47.716735: step 17130, loss = 0.93 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:36:49.018568: step 17140, loss = 0.86 (983.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:36:50.316897: step 17150, loss = 0.91 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:36:51.613611: step 17160, loss = 0.86 (987.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:36:52.929351: step 17170, loss = 1.15 (972.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:36:54.257852: step 17180, loss = 0.80 (963.5 examples/sec; 0.133 sec/batch)
2017-05-09 21:36:55.555352: step 17190, loss = 0.90 (986.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:36:56.945932: step 17200, loss = 0.85 (920.5 examples/sec; 0.139 sec/batch)
2017-05-09 21:36:58.171351: step 17210, loss = 0.93 (1044.5 examples/sec; 0.123 sec/batch)
2017-05-09 21:36:59.455725: step 17220, loss = 0.95 (996.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:37:00.767159: step 17230, loss = 0.86 (976.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:37:02.074810: step 17240, loss = 0.87 (978.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:37:03.371391: step 17250, loss = 1.18 (987.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:37:04.633151: step 17260, loss = 0.82 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-09 21:37:05.931356: step 17270, loss = 0.84 (986.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:37:07.216292: step 17280, loss = 0.85 (996.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:37:08.504277: step 17290, loss = 0.82 (993.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:37:09.867201: step 17300, loss = 0.81 (939.2 examples/sec; 0.136 sec/batch)
2017-05-09 21:37:11.057012: step 17310, loss = 0.80 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-09 21:37:12.347074: step 17320, loss = 0.65 (992.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:37:13.638620: step 17330, loss = 0.94 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:37:14.918341: step 17340, loss = 1.05 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:37:16.205210: step 17350, loss = 0.84 (994.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:37:17.506313: step 17360, loss = E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 364 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
0.78 (983.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:37:18.789253: step 17370, loss = 0.85 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:37:20.090253: step 17380, loss = 0.72 (983.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:37:21.383337: step 17390, loss = 1.14 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:37:22.785593: step 17400, loss = 0.93 (912.8 examples/sec; 0.140 sec/batch)
2017-05-09 21:37:24.001245: step 17410, loss = 0.76 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-09 21:37:25.302334: step 17420, loss = 0.94 (983.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:37:26.571814: step 17430, loss = 0.79 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:37:27.891336: step 17440, loss = 0.79 (970.0 examples/sec; 0.132 sec/batch)
2017-05-09 21:37:29.192694: step 17450, loss = 0.80 (983.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:37:30.509168: step 17460, loss = 0.78 (972.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:37:31.780754: step 17470, loss = 0.93 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:37:33.107635: step 17480, loss = 1.04 (964.7 examples/sec; 0.133 sec/batch)
2017-05-09 21:37:34.400668: step 17490, loss = 0.91 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:37:35.809491: step 17500, loss = 0.82 (908.6 examples/sec; 0.141 sec/batch)
2017-05-09 21:37:37.023489: step 17510, loss = 0.77 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-09 21:37:38.307378: step 17520, loss = 0.78 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:37:39.581660: step 17530, loss = 0.83 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-09 21:37:40.896826: step 17540, loss = 0.88 (973.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:37:42.226817: step 17550, loss = 0.71 (962.4 examples/sec; 0.133 sec/batch)
2017-05-09 21:37:43.513146: step 17560, loss = 0.89 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:37:44.803350: step 17570, loss = 0.95 (992.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:37:46.125699: step 17580, loss = 0.85 (968.0 examples/sec; 0.132 sec/batch)
2017-05-09 21:37:47.415347: step 17590, loss = 1.08 (992.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:37:48.835578: step 17600, loss = 0.90 (901.3 examples/sec; 0.142 sec/batch)
2017-05-09 21:37:50.056371: step 17610, loss = 0.79 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-09 21:37:51.353186: step 17620, loss = 0.90 (987.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:37:52.635215: step 17630, loss = 0.70 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:37:53.946765: step 17640, loss = 0.92 (975.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:37:55.221160: step 17650, loss = 0.75 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:37:56.522831: step 17660, loss = 0.84 (983.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:37:57.839580: step 17670, loss = 0.87 (972.1 examples/sec; 0.132 sec/batch)
2017-05-09 21:37:59.160741: step 17680, loss = 0.85 (968.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:38:00.472682: step 17690, loss = 0.88 (975.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:38:01.867728: step 17700, loss = 0.83 (917.5 examples/sec; 0.140 sec/batch)
2017-05-09 21:38:03.091602: step 17710, loss = 0.81 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-09 21:38:04.367847: step 17720, loss = 0.72 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:38:05.661046: step 17730, loss = 0.83 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:38:06.979275: step 17740, loss = 0.83 (971.0 examples/sec; 0.132 sec/batch)
2017-05-09 21:38:08.301085: step 17750, loss = 0.74 (968.4 examples/sec; 0.132 sec/batch)
2017-05-09 21:38:09.600050: step 17760, loss = 0.72 (985.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:38:10.906053: step 17770, loss = 0.90 (980.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:38:12.200129: step 17780, loss = 0.88 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:38:13.498997: step 17790, loss = 1.03 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:38:14.898236: step 17800, loss = 1.04 (914.8 examples/sec; 0.140 sec/batch)
2017-05-09 21:38:16.129394: step 17810, loss = 0.84 (1039.7 examples/sec; 0.123 sec/batch)
2017-05-09 21:38:17.416420: step 17820, loss = 1.01 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:38:18.711320: step 17830, loss = 0.97 (988.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:38:20.031975: step 17840, loss = 0.81 (969.2 examples/sec; 0.132 sec/batch)
2017-05-09 21:38:21.334167: step 17850, loss = 1.14 (983.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:38:22.675130: step 17860, loss = 0.77 (954.5 examples/sec; 0.134 sec/batch)
2017-05-09 21:38:23.960262: step 17870, loss = 0.88 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:38:25.243713: step 17880, loss = 0.99 (997.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:38:26.535468: step 17890, loss = 1.04 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:38:27.917783: step 17900, loss = 0.88 (926.0 examples/sec; 0.138 sec/batch)
2017-05-09 21:38:29.129256: step 17910, loss = 0.94 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-09 21:38:30.441545: step 17920, loss = 0.87 (975.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:38:31.751845: step 17930, loss = 1.05 (976.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:38:33.068176: step 17940, loss = 0.94 (972.4 examples/sec; 0.132 sec/batch)
2017-05-09 21:38:34.369329: step 17950, loss = 0.98 (983.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:38:35.664854: step 17960, loss = 0.96 (988.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:38:36.968238: step 17970, loss = 0.96 (982.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:38:38.285226: step 17980, loss = 0.72 (971.9 examples/sec; 0.132 sec/batch)
2017-05-09 21:38:39.572221: step 17990, loss = 0.77 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:38:40.941556: step 18000, loss = 1.08 (934.8 examples/sec; 0.137 sec/batch)
2017-05-09 21:38:42.155217: step 18010, loss = 0.77 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-09 21:38:43.450491: step 18020, loss = 0.89 (988.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:38:44.727613: step 18030, loss = 1.16 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:38:46.018636: step 18040, loss = 0.90 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:38:47.312631: step 18050, loss = 0.90 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:38:48.608907: step 18060, loss = 0.82 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:38:49.904616: step 18070, loss = 0.94 (987.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:38:51.234520: step 18080, loss = 1.04 (962.5 examples/sec; 0.133 sec/batch)
2017-05-09 21:38:52.545307: step 18090, loss = 0.92 (976.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:38:53.910867: step 18100, loss = 0.84 (937.3 examples/sec; 0.137 sec/batch)
2017-05-09 21:38:55.139656: step 18110, loss = 0.80 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-09 21:38:56.447317: step 18120, loss = 0.81 (978.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:38:57.742620: step 18130, loss = 0.84 (988.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:38:59.025955: step 18140, loss = 0.99 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:39:00.313961: step 18150, loss = 0.70 (993.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:39:01.591736: step 18160, loss = 0.87 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:39:02.879673: step 18170, loss = 0.85 (993.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:39:04.179131: step 18180, loss = 0.77 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:39:05.456506: step 18190, loss = 0.86 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:39:06.841728: step 18200, loss = 0.90 (924.0 examples/sec; 0.139 sec/batch)
2017-05-09 21:39:08.031831: step 18210, loss = 0.85 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-09 21:39:09.287021: step 18220, loss = 0.72 (1019.8 examples/sec; 0.126 sec/batch)
2017-05-09 21:39:10.570495: step 18230, loss = 1.02 (997.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:39:11.858590: step 18240, loss = 0.78 (993.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:39:13.185549: step 18250, loss = 0.79 (964.6 examples/sec; 0.133 sec/batch)
2017-05-09 21:39:14.448242: step 18260, loss = 0.73 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-09 21:39:15.732188: step 18270, loss = 1.04 (996.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:39:17.013495: step 18280, loss = 0.91 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:39:18.329944: step 18290, loss = 0.77 (972.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:39:19.729163: step 18300, loss = 0.69 (914.8 examples/sec; 0.140 sec/batch)
2017-05-09 21:39:20.951905: step 18310, loss = 0.86 (1046.8 examples/sec; 0.122 sec/batch)
2017-05-09 21:39:22.281523: step 18320, loss = 0.89 (962.7 examples/sec; 0.133 sec/batch)
2017-05-09 21:39:23.577404: step 18330, loss = 0.82 (987.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:39:24.877214: step 18340, loss = 0.79 (984.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:39:26.169489: step 18350, loss = 0.89 (990.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:39:27.476321: step 18360, loss = 0.79 (979.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:39:28.777300: step 18370, loss = 0.82 (983.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:39:30.073139: step 18380, loss = 0.93 (987.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:39:31.368649: step 18390, loss = 0.82 (988.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:39:32.761761: step 18400, loss = 0.82 (918.8 examples/sec; 0.139 sec/batch)
2017-05-09 21:39:33.998584: step 18410, loss = 0.79 (1034.9 examples/sec; 0.124 sec/batch)
2017-05-09 21:39:35.296911: step 18420, loss = 0.84 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:39:36.618157: step 18430, loss = 0.72 (968.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:39:37.959384: step 18440, loss = 1.05 (954.4 examples/sec; 0.134 sec/batch)
2017-05-09 21:39:39.244042: step 18450, loss = 0.76 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:39:40.588284: step 18460, loss = 0.71 (952.2 examples/sec; 0.134 sec/batch)
2017-05-09 21:39:41.866187: step 18470, loss = 0.77 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:39:43.154641: step 18480, loss = 0.86 (993.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:39:44.445829: step 18490, loss = 0.71 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:39:45.816590: step 18500, loss = 0.89 (933.8 examples/sec; 0.137 sec/batch)
2017-05-09 21:39:47.026871: step 18510, loss = 1.05 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-09 21:39:48.322111: step 18520, loss = 0.82 (988.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:39:49.603296: step 18530, loss = 0.74 (999.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:39:50.902092: step 18540, loss = 0.85 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:39:52.182695: step 18550, loss = 0.92 (999.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:39:53.452541: step 18560, loss = 1.09 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:39:54.734209: step 18570, loss = 0.81 (998.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:39:56.009384: step 18580, loss = 0.93 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:39:57.299086: step 18590, loss = 0.88 (992.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:39:58.688260: step 18600, loss = 0.73 (921.4 examples/sec; 0.139 sec/batch)
2017-05-09 21:39:59.887869: step 18610, loss = 0.85 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-09 21:40:01.167473: step 18620, loss = 0.83 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:40:02.470638: step 18630, loss = 0.78 (982.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:40:03.763222: step 18640, loss = 1.05 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:40:05.042799: step 18650, loss = 0.90 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:40:06.352896: step 18660, loss = 1.03 (977.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:40:07.654266: step 18670, loss = 1.04 (983.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:40:08.914115: step 18680, loss = 0.88 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-09 21:40:10.195117: step 18690, loss = 0.91 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:40:11.563945: step 18700, loss = 1.09 (935.1 examples/sec; 0.137 sec/batch)
2017-05-09 21:40:12.761365: step 18710, loss = 0.95 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-09 21:40:14.044669: step 18720, loss = 0.90 (997.4 examplE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 385 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
es/sec; 0.128 sec/batch)
2017-05-09 21:40:15.332500: step 18730, loss = 0.80 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:40:16.597534: step 18740, loss = 0.91 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-09 21:40:17.879873: step 18750, loss = 0.84 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:40:19.167505: step 18760, loss = 0.85 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:40:20.456941: step 18770, loss = 0.87 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:40:21.732668: step 18780, loss = 0.87 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:40:23.026849: step 18790, loss = 0.79 (989.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:40:24.392621: step 18800, loss = 0.81 (937.2 examples/sec; 0.137 sec/batch)
2017-05-09 21:40:25.563063: step 18810, loss = 0.83 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-09 21:40:26.868259: step 18820, loss = 0.93 (980.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:40:28.173710: step 18830, loss = 0.90 (980.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:40:29.456911: step 18840, loss = 0.74 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:40:30.754647: step 18850, loss = 0.93 (986.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:40:32.039616: step 18860, loss = 0.74 (996.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:40:33.336852: step 18870, loss = 0.83 (986.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:40:34.618711: step 18880, loss = 0.85 (998.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:40:35.899546: step 18890, loss = 0.63 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:40:37.291700: step 18900, loss = 0.72 (919.4 examples/sec; 0.139 sec/batch)
2017-05-09 21:40:38.493085: step 18910, loss = 0.84 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-09 21:40:39.750693: step 18920, loss = 0.94 (1017.8 examples/sec; 0.126 sec/batch)
2017-05-09 21:40:41.043430: step 18930, loss = 0.78 (990.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:40:42.336258: step 18940, loss = 1.01 (990.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:40:43.615141: step 18950, loss = 0.72 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:40:44.884834: step 18960, loss = 0.68 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:40:46.187064: step 18970, loss = 0.95 (982.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:40:47.482238: step 18980, loss = 0.74 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:40:48.766760: step 18990, loss = 0.89 (996.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:40:50.130486: step 19000, loss = 0.87 (938.6 examples/sec; 0.136 sec/batch)
2017-05-09 21:40:51.318044: step 19010, loss = 0.78 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-09 21:40:52.600416: step 19020, loss = 0.71 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:40:53.897558: step 19030, loss = 0.77 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:40:55.170709: step 19040, loss = 0.90 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:40:56.480495: step 19050, loss = 1.04 (977.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:40:57.821796: step 19060, loss = 1.00 (954.3 examples/sec; 0.134 sec/batch)
2017-05-09 21:40:59.122948: step 19070, loss = 0.84 (983.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:41:00.449655: step 19080, loss = 1.12 (964.8 examples/sec; 0.133 sec/batch)
2017-05-09 21:41:01.751548: step 19090, loss = 0.84 (983.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:41:03.254322: step 19100, loss = 0.88 (851.8 examples/sec; 0.150 sec/batch)
2017-05-09 21:41:04.483855: step 19110, loss = 0.92 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-09 21:41:05.787242: step 19120, loss = 0.95 (982.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:41:07.065146: step 19130, loss = 0.85 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:41:08.311873: step 19140, loss = 0.77 (1026.7 examples/sec; 0.125 sec/batch)
2017-05-09 21:41:09.611415: step 19150, loss = 0.88 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:41:10.941756: step 19160, loss = 0.81 (962.2 examples/sec; 0.133 sec/batch)
2017-05-09 21:41:12.243369: step 19170, loss = 1.05 (983.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:41:13.545867: step 19180, loss = 0.72 (982.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:41:14.846591: step 19190, loss = 0.79 (984.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:41:16.233639: step 19200, loss = 0.97 (922.8 examples/sec; 0.139 sec/batch)
2017-05-09 21:41:17.449569: step 19210, loss = 0.93 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-09 21:41:18.752963: step 19220, loss = 0.75 (982.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:41:20.027422: step 19230, loss = 0.77 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:41:21.322863: step 19240, loss = 0.92 (988.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:41:22.601575: step 19250, loss = 1.01 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:41:23.884713: step 19260, loss = 0.92 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:41:25.173591: step 19270, loss = 0.86 (993.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:41:26.456902: step 19280, loss = 0.95 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:41:27.757682: step 19290, loss = 0.89 (984.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:41:29.153658: step 19300, loss = 0.75 (916.9 examples/sec; 0.140 sec/batch)
2017-05-09 21:41:30.344064: step 19310, loss = 0.99 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-09 21:41:31.627767: step 19320, loss = 1.03 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:41:32.920959: step 19330, loss = 0.98 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:41:34.195169: step 19340, loss = 0.89 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-09 21:41:35.483976: step 19350, loss = 1.00 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:41:36.769594: step 19360, loss = 0.71 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:41:38.054406: step 19370, loss = 0.84 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:41:39.335541: step 19380, loss = 0.88 (999.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:41:40.587891: step 19390, loss = 1.12 (1022.1 examples/sec; 0.125 sec/batch)
2017-05-09 21:41:41.960295: step 19400, loss = 0.86 (932.7 examples/sec; 0.137 sec/batch)
2017-05-09 21:41:43.170626: step 19410, loss = 0.81 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-09 21:41:44.454644: step 19420, loss = 0.82 (996.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:41:45.732513: step 19430, loss = 0.73 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:41:47.004485: step 19440, loss = 0.89 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:41:48.291835: step 19450, loss = 0.89 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:41:49.568685: step 19460, loss = 0.81 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:41:50.854207: step 19470, loss = 0.90 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:41:52.140739: step 19480, loss = 0.85 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:41:53.461059: step 19490, loss = 0.82 (969.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:41:54.884961: step 19500, loss = 1.06 (898.9 examples/sec; 0.142 sec/batch)
2017-05-09 21:41:56.089717: step 19510, loss = 0.96 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-09 21:41:57.381436: step 19520, loss = 0.93 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:41:58.713500: step 19530, loss = 1.00 (960.9 examples/sec; 0.133 sec/batch)
2017-05-09 21:41:59.986133: step 19540, loss = 0.78 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-09 21:42:01.328578: step 19550, loss = 0.76 (953.5 examples/sec; 0.134 sec/batch)
2017-05-09 21:42:02.620670: step 19560, loss = 0.73 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:42:03.910706: step 19570, loss = 0.89 (992.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:42:05.187664: step 19580, loss = 0.94 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:42:06.501649: step 19590, loss = 0.89 (974.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:42:07.879151: step 19600, loss = 0.78 (929.2 examples/sec; 0.138 sec/batch)
2017-05-09 21:42:09.086634: step 19610, loss = 0.68 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-09 21:42:10.366520: step 19620, loss = 0.71 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:42:11.636218: step 19630, E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 405 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
loss = 0.76 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:42:12.909384: step 19640, loss = 0.67 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:42:14.220700: step 19650, loss = 0.76 (976.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:42:15.502664: step 19660, loss = 0.81 (998.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:42:16.791291: step 19670, loss = 0.87 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:42:18.067730: step 19680, loss = 0.75 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:42:19.368126: step 19690, loss = 0.76 (984.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:42:20.751989: step 19700, loss = 0.80 (924.9 examples/sec; 0.138 sec/batch)
2017-05-09 21:42:21.959921: step 19710, loss = 0.90 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-09 21:42:23.246823: step 19720, loss = 1.02 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:42:24.515724: step 19730, loss = 0.86 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:42:25.804052: step 19740, loss = 0.78 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:42:27.079301: step 19750, loss = 0.82 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:42:28.369799: step 19760, loss = 0.92 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:42:29.649678: step 19770, loss = 0.83 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:42:30.928582: step 19780, loss = 0.70 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:42:32.214172: step 19790, loss = 0.79 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:42:33.589126: step 19800, loss = 0.95 (930.9 examples/sec; 0.137 sec/batch)
2017-05-09 21:42:34.786168: step 19810, loss = 0.83 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-09 21:42:36.052071: step 19820, loss = 0.81 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:42:37.340431: step 19830, loss = 0.63 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:42:38.615338: step 19840, loss = 0.84 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:42:39.878240: step 19850, loss = 0.81 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-09 21:42:41.170476: step 19860, loss = 0.94 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:42:42.455287: step 19870, loss = 1.13 (996.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:42:43.774544: step 19880, loss = 1.04 (970.2 examples/sec; 0.132 sec/batch)
2017-05-09 21:42:45.059180: step 19890, loss = 0.79 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:42:46.468884: step 19900, loss = 0.91 (908.0 examples/sec; 0.141 sec/batch)
2017-05-09 21:42:47.631600: step 19910, loss = 0.92 (1100.9 examples/sec; 0.116 sec/batch)
2017-05-09 21:42:48.942929: step 19920, loss = 0.99 (976.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:42:50.253656: step 19930, loss = 1.06 (976.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:42:51.564021: step 19940, loss = 0.89 (976.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:42:52.868506: step 19950, loss = 0.73 (981.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:42:54.122628: step 19960, loss = 1.07 (1020.6 examples/sec; 0.125 sec/batch)
2017-05-09 21:42:55.407342: step 19970, loss = 0.76 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:42:56.699006: step 19980, loss = 0.72 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:42:57.976916: step 19990, loss = 0.93 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:42:59.369506: step 20000, loss = 0.99 (919.2 examples/sec; 0.139 sec/batch)
2017-05-09 21:43:00.557238: step 20010, loss = 0.76 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-09 21:43:01.841386: step 20020, loss = 0.86 (996.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:43:03.102664: step 20030, loss = 0.83 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-09 21:43:04.386493: step 20040, loss = 0.80 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:43:05.667554: step 20050, loss = 0.98 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:43:06.960746: step 20060, loss = 0.84 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:43:08.237934: step 20070, loss = 0.62 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:43:09.516037: step 20080, loss = 0.85 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:43:10.795438: step 20090, loss = 0.85 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:43:12.220147: step 20100, loss = 0.79 (898.4 examples/sec; 0.142 sec/batch)
2017-05-09 21:43:13.417096: step 20110, loss = 0.89 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-09 21:43:14.711392: step 20120, loss = 0.82 (989.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:43:16.009127: step 20130, loss = 0.99 (986.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:43:17.292849: step 20140, loss = 0.73 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:43:18.580195: step 20150, loss = 0.86 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:43:19.887217: step 20160, loss = 0.73 (979.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:43:21.183715: step 20170, loss = 0.83 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:43:22.482359: step 20180, loss = 0.69 (985.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:43:23.805068: step 20190, loss = 0.97 (967.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:43:25.242047: step 20200, loss = 0.84 (890.8 examples/sec; 0.144 sec/batch)
2017-05-09 21:43:26.434748: step 20210, loss = 1.07 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-09 21:43:27.729338: step 20220, loss = 1.12 (988.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:43:29.032942: step 20230, loss = 0.79 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:43:30.337937: step 20240, loss = 0.99 (980.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:43:31.606897: step 20250, loss = 0.97 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:43:32.879879: step 20260, loss = 0.92 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-09 21:43:34.173851: step 20270, loss = 0.76 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:43:35.474398: step 20280, loss = 0.83 (984.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:43:36.748838: step 20290, loss = 0.77 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:43:38.123954: step 20300, loss = 0.79 (930.8 examples/sec; 0.138 sec/batch)
2017-05-09 21:43:39.327701: step 20310, loss = 0.84 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-09 21:43:40.607662: step 20320, loss = 0.93 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:43:41.894706: step 20330, loss = 0.88 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:43:43.172518: step 20340, loss = 0.90 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:43:44.454547: step 20350, loss = 1.13 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:43:45.789901: step 20360, loss = 0.90 (958.5 examples/sec; 0.134 sec/batch)
2017-05-09 21:43:47.066734: step 20370, loss = 0.90 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:43:48.359082: step 20380, loss = 0.85 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:43:49.648386: step 20390, loss = 0.82 (992.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:43:51.030921: step 20400, loss = 0.87 (925.8 examples/sec; 0.138 sec/batch)
2017-05-09 21:43:52.210307: step 20410, loss = 0.94 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-09 21:43:53.500570: step 20420, loss = 0.86 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:43:54.777951: step 20430, loss = 0.84 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:43:56.088750: step 20440, loss = 0.95 (976.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:43:57.358946: step 20450, loss = 0.96 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:43:58.637279: step 20460, loss = 0.86 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:43:59.902846: step 20470, loss = 0.81 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:44:01.184854: step 20480, loss = 0.83 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:44:02.474719: step 20490, loss = 0.84 (992.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:44:03.849719: step 20500, loss = 0.87 (930.9 examples/sec; 0.137 sec/batch)
2017-05-09 21:44:05.065087: step 20510, loss = 0.75 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-09 21:44:06.378215: step 20520, loss = 0.95 (974.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:44:07.660812: step 20530, loss = 0.94 (998.0 examples/sec; 0.128 sec/batch)
2017-05E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 425 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
-09 21:44:08.937843: step 20540, loss = 0.91 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:44:10.239211: step 20550, loss = 0.87 (983.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:44:11.538386: step 20560, loss = 0.73 (985.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:44:12.811502: step 20570, loss = 0.84 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:44:14.083629: step 20580, loss = 0.87 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:44:15.381979: step 20590, loss = 1.01 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:44:16.765160: step 20600, loss = 0.97 (925.4 examples/sec; 0.138 sec/batch)
2017-05-09 21:44:17.959034: step 20610, loss = 0.80 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-09 21:44:19.244720: step 20620, loss = 0.73 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:44:20.515281: step 20630, loss = 0.83 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:44:21.782945: step 20640, loss = 0.85 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:44:23.074245: step 20650, loss = 0.77 (991.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:44:24.356122: step 20660, loss = 0.75 (998.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:44:25.641917: step 20670, loss = 0.99 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:44:26.959655: step 20680, loss = 0.87 (971.4 examples/sec; 0.132 sec/batch)
2017-05-09 21:44:28.231633: step 20690, loss = 0.85 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:44:29.597894: step 20700, loss = 0.74 (936.9 examples/sec; 0.137 sec/batch)
2017-05-09 21:44:30.763971: step 20710, loss = 0.69 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-09 21:44:32.061964: step 20720, loss = 0.89 (986.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:44:33.354741: step 20730, loss = 0.74 (990.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:44:34.638244: step 20740, loss = 0.68 (997.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:44:35.910737: step 20750, loss = 0.78 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:44:37.200171: step 20760, loss = 0.69 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:44:38.486115: step 20770, loss = 1.20 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:44:39.776621: step 20780, loss = 0.76 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:44:41.079414: step 20790, loss = 0.86 (982.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:44:42.483394: step 20800, loss = 0.91 (911.7 examples/sec; 0.140 sec/batch)
2017-05-09 21:44:43.691880: step 20810, loss = 1.10 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-09 21:44:45.025290: step 20820, loss = 0.83 (959.9 examples/sec; 0.133 sec/batch)
2017-05-09 21:44:46.278998: step 20830, loss = 0.81 (1021.0 examples/sec; 0.125 sec/batch)
2017-05-09 21:44:47.592334: step 20840, loss = 0.80 (974.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:44:48.911845: step 20850, loss = 0.95 (970.0 examples/sec; 0.132 sec/batch)
2017-05-09 21:44:50.238903: step 20860, loss = 0.96 (964.5 examples/sec; 0.133 sec/batch)
2017-05-09 21:44:51.542051: step 20870, loss = 0.97 (982.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:44:52.820333: step 20880, loss = 0.68 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:44:54.068600: step 20890, loss = 0.76 (1025.4 examples/sec; 0.125 sec/batch)
2017-05-09 21:44:55.450768: step 20900, loss = 0.74 (926.1 examples/sec; 0.138 sec/batch)
2017-05-09 21:44:56.682873: step 20910, loss = 0.92 (1038.9 examples/sec; 0.123 sec/batch)
2017-05-09 21:44:57.989832: step 20920, loss = 1.01 (979.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:44:59.281359: step 20930, loss = 0.83 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:45:00.554543: step 20940, loss = 0.77 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:45:01.845994: step 20950, loss = 0.91 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:45:03.141618: step 20960, loss = 0.87 (987.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:45:04.428912: step 20970, loss = 0.85 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:45:05.749577: step 20980, loss = 0.80 (969.2 examples/sec; 0.132 sec/batch)
2017-05-09 21:45:07.055543: step 20990, loss = 0.70 (980.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:45:08.438759: step 21000, loss = 0.74 (925.4 examples/sec; 0.138 sec/batch)
2017-05-09 21:45:09.639488: step 21010, loss = 0.98 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-09 21:45:10.957170: step 21020, loss = 1.03 (971.4 examples/sec; 0.132 sec/batch)
2017-05-09 21:45:12.262130: step 21030, loss = 0.77 (980.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:45:13.586290: step 21040, loss = 0.82 (966.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:45:14.887984: step 21050, loss = 0.65 (983.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:45:16.177675: step 21060, loss = 0.83 (992.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:45:17.485528: step 21070, loss = 0.81 (978.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:45:18.771248: step 21080, loss = 0.81 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:45:20.073579: step 21090, loss = 0.89 (982.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:45:21.482996: step 21100, loss = 0.75 (908.2 examples/sec; 0.141 sec/batch)
2017-05-09 21:45:22.706658: step 21110, loss = 0.87 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-09 21:45:23.999317: step 21120, loss = 0.95 (990.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:45:25.316498: step 21130, loss = 1.04 (971.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:45:26.626827: step 21140, loss = 0.96 (976.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:45:27.880870: step 21150, loss = 0.83 (1020.7 examples/sec; 0.125 sec/batch)
2017-05-09 21:45:29.167003: step 21160, loss = 0.86 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:45:30.453947: step 21170, loss = 0.90 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:45:31.741953: step 21180, loss = 0.85 (993.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:45:33.068559: step 21190, loss = 0.78 (964.9 examples/sec; 0.133 sec/batch)
2017-05-09 21:45:34.461114: step 21200, loss = 0.83 (919.2 examples/sec; 0.139 sec/batch)
2017-05-09 21:45:35.651252: step 21210, loss = 0.91 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-09 21:45:36.927936: step 21220, loss = 0.86 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:45:38.211952: step 21230, loss = 0.62 (996.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:45:39.487080: step 21240, loss = 0.82 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:45:40.794981: step 21250, loss = 1.01 (978.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:45:42.074758: step 21260, loss = 0.91 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:45:43.376509: step 21270, loss = 0.85 (983.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:45:44.683510: step 21280, loss = 0.90 (979.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:45:45.974381: step 21290, loss = 0.75 (991.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:45:47.334425: step 21300, loss = 0.97 (941.1 examples/sec; 0.136 sec/batch)
2017-05-09 21:45:48.520340: step 21310, loss = 0.87 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-09 21:45:49.809321: step 21320, loss = 0.83 (993.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:45:51.112262: step 21330, loss = 0.98 (982.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:45:52.412799: step 21340, loss = 0.89 (984.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:45:53.726859: step 21350, loss = 0.85 (974.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:45:54.995001: step 21360, loss = 0.83 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:45:56.257263: step 21370, loss = 0.93 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-09 21:45:57.541268: step 21380, loss = 0.74 (996.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:45:58.824561: step 21390, loss = 0.70 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:46:00.219733: step 21400, loss = 0.72 (917.4 examples/sec; 0.140 sec/batch)
2017-05-09 21:46:01.413416: step 21410, loss = 0.75 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-09 21:46:02.696218: step 21420, loss = 0.87 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:46:03.996504: step 21430, loss = 0.85 (984.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:46:05.318944: step 21440, loss = 0.68 (967.9 examples/sec; E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 445 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
0.132 sec/batch)
2017-05-09 21:46:06.613050: step 21450, loss = 1.07 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:46:07.897116: step 21460, loss = 0.98 (996.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:46:09.183315: step 21470, loss = 0.72 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:46:10.493958: step 21480, loss = 0.83 (976.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:46:11.770845: step 21490, loss = 0.84 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:46:13.170001: step 21500, loss = 0.91 (914.8 examples/sec; 0.140 sec/batch)
2017-05-09 21:46:14.377920: step 21510, loss = 0.86 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-09 21:46:15.652114: step 21520, loss = 1.01 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:46:16.935859: step 21530, loss = 0.79 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:46:18.225027: step 21540, loss = 0.84 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:46:19.510073: step 21550, loss = 0.85 (996.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:46:20.799971: step 21560, loss = 0.77 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:46:22.095065: step 21570, loss = 0.94 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:46:23.378243: step 21580, loss = 0.83 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:46:24.679631: step 21590, loss = 0.94 (983.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:46:26.079436: step 21600, loss = 0.85 (914.4 examples/sec; 0.140 sec/batch)
2017-05-09 21:46:27.285616: step 21610, loss = 1.06 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-09 21:46:28.600083: step 21620, loss = 0.83 (973.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:46:29.892372: step 21630, loss = 0.77 (990.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:46:31.207823: step 21640, loss = 0.71 (973.0 examples/sec; 0.132 sec/batch)
2017-05-09 21:46:32.483124: step 21650, loss = 0.93 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:46:33.783373: step 21660, loss = 0.84 (984.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:46:35.076488: step 21670, loss = 0.89 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:46:36.346738: step 21680, loss = 0.70 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:46:37.649327: step 21690, loss = 0.88 (982.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:46:39.059199: step 21700, loss = 0.84 (907.9 examples/sec; 0.141 sec/batch)
2017-05-09 21:46:40.262584: step 21710, loss = 0.85 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-09 21:46:41.530227: step 21720, loss = 0.84 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:46:42.824774: step 21730, loss = 1.00 (988.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:46:44.089115: step 21740, loss = 0.83 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-09 21:46:45.392324: step 21750, loss = 0.72 (982.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:46:46.685944: step 21760, loss = 0.77 (989.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:46:48.010398: step 21770, loss = 1.12 (966.4 examples/sec; 0.132 sec/batch)
2017-05-09 21:46:49.316838: step 21780, loss = 0.76 (979.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:46:50.603861: step 21790, loss = 0.88 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:46:51.992622: step 21800, loss = 0.81 (921.7 examples/sec; 0.139 sec/batch)
2017-05-09 21:46:53.190987: step 21810, loss = 0.88 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-09 21:46:54.470668: step 21820, loss = 0.74 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:46:55.768353: step 21830, loss = 0.75 (986.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:46:57.044588: step 21840, loss = 0.88 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:46:58.322252: step 21850, loss = 0.95 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:46:59.617647: step 21860, loss = 0.72 (988.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:47:00.938474: step 21870, loss = 0.97 (969.1 examples/sec; 0.132 sec/batch)
2017-05-09 21:47:02.222664: step 21880, loss = 0.67 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:47:03.529586: step 21890, loss = 0.94 (979.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:47:04.916830: step 21900, loss = 0.76 (922.7 examples/sec; 0.139 sec/batch)
2017-05-09 21:47:06.116951: step 21910, loss = 0.97 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-09 21:47:07.402263: step 21920, loss = 0.88 (995.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:47:08.694835: step 21930, loss = 0.75 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:47:09.981674: step 21940, loss = 0.90 (994.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:47:11.275097: step 21950, loss = 0.81 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:47:12.573110: step 21960, loss = 0.97 (986.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:47:13.868955: step 21970, loss = 0.75 (987.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:47:15.159216: step 21980, loss = 0.71 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:47:16.454287: step 21990, loss = 0.71 (988.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:47:17.844059: step 22000, loss = 0.95 (921.0 examples/sec; 0.139 sec/batch)
2017-05-09 21:47:19.073491: step 22010, loss = 1.00 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-09 21:47:20.395568: step 22020, loss = 1.08 (968.1 examples/sec; 0.132 sec/batch)
2017-05-09 21:47:21.685189: step 22030, loss = 0.73 (992.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:47:22.996229: step 22040, loss = 0.78 (976.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:47:24.301592: step 22050, loss = 0.76 (980.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:47:25.610630: step 22060, loss = 1.04 (977.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:47:26.922884: step 22070, loss = 0.92 (975.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:47:28.212299: step 22080, loss = 0.71 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:47:29.509140: step 22090, loss = 0.73 (987.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:47:30.902692: step 22100, loss = 0.68 (918.5 examples/sec; 0.139 sec/batch)
2017-05-09 21:47:32.093159: step 22110, loss = 0.89 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-09 21:47:33.383636: step 22120, loss = 0.96 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:47:34.691661: step 22130, loss = 0.78 (978.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:47:35.998552: step 22140, loss = 0.83 (979.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:47:37.285319: step 22150, loss = 0.84 (994.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:47:38.565760: step 22160, loss = 0.95 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:47:39.838078: step 22170, loss = 0.99 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 21:47:41.133986: step 22180, loss = 0.82 (987.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:47:42.450604: step 22190, loss = 0.83 (972.2 examples/sec; 0.132 sec/batch)
2017-05-09 21:47:43.834022: step 22200, loss = 0.95 (925.2 examples/sec; 0.138 sec/batch)
2017-05-09 21:47:45.021962: step 22210, loss = 0.76 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-09 21:47:46.325737: step 22220, loss = 0.81 (981.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:47:47.607297: step 22230, loss = 0.88 (998.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:47:48.892072: step 22240, loss = 0.71 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:47:50.193180: step 22250, loss = 1.02 (983.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:47:51.521765: step 22260, loss = 0.98 (963.4 examples/sec; 0.133 sec/batch)
2017-05-09 21:47:52.814346: step 22270, loss = 0.90 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:47:54.153042: step 22280, loss = 0.98 (956.2 examples/sec; 0.134 sec/batch)
2017-05-09 21:47:55.452686: step 22290, loss = 0.94 (984.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:47:56.850988: step 22300, loss = 0.74 (915.4 examples/sec; 0.140 sec/batch)
2017-05-09 21:47:58.048676: step 22310, loss = 0.86 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-09 21:47:59.350729: step 22320, loss = 0.90 (983.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:48:00.649614: step 22330, loss = 0.77 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:48:01.954795: step 22340, loss = 0.87 (980.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:48:03.217518: step 22350, loss = 0.79 (101E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 465 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
3.7 examples/sec; 0.126 sec/batch)
2017-05-09 21:48:04.504759: step 22360, loss = 0.89 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:48:05.822166: step 22370, loss = 0.64 (971.6 examples/sec; 0.132 sec/batch)
2017-05-09 21:48:07.121861: step 22380, loss = 0.96 (984.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:48:08.397763: step 22390, loss = 0.83 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:48:09.784085: step 22400, loss = 0.64 (923.3 examples/sec; 0.139 sec/batch)
2017-05-09 21:48:10.976504: step 22410, loss = 0.84 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-09 21:48:12.274625: step 22420, loss = 0.74 (986.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:48:13.564967: step 22430, loss = 0.96 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:48:14.845983: step 22440, loss = 0.81 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:48:16.133980: step 22450, loss = 0.76 (993.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:48:17.437391: step 22460, loss = 0.87 (982.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:48:18.728418: step 22470, loss = 0.71 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:48:20.013955: step 22480, loss = 0.87 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:48:21.308817: step 22490, loss = 0.86 (988.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:48:22.683069: step 22500, loss = 0.65 (931.4 examples/sec; 0.137 sec/batch)
2017-05-09 21:48:23.862911: step 22510, loss = 1.05 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-09 21:48:25.179496: step 22520, loss = 0.76 (972.2 examples/sec; 0.132 sec/batch)
2017-05-09 21:48:26.480039: step 22530, loss = 1.39 (984.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:48:27.778144: step 22540, loss = 0.80 (986.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:48:29.061181: step 22550, loss = 0.79 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:48:30.365397: step 22560, loss = 0.77 (981.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:48:31.659166: step 22570, loss = 0.84 (989.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:48:32.979484: step 22580, loss = 0.74 (969.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:48:34.282769: step 22590, loss = 0.85 (982.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:48:35.662722: step 22600, loss = 0.99 (927.6 examples/sec; 0.138 sec/batch)
2017-05-09 21:48:36.893437: step 22610, loss = 0.75 (1040.0 examples/sec; 0.123 sec/batch)
2017-05-09 21:48:38.201696: step 22620, loss = 0.84 (978.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:48:39.465648: step 22630, loss = 0.77 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-09 21:48:40.745160: step 22640, loss = 0.90 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 21:48:42.071183: step 22650, loss = 0.95 (965.3 examples/sec; 0.133 sec/batch)
2017-05-09 21:48:43.365282: step 22660, loss = 0.91 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:48:44.658975: step 22670, loss = 0.88 (989.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:48:45.982003: step 22680, loss = 0.82 (967.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:48:47.279664: step 22690, loss = 1.24 (986.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:48:48.644684: step 22700, loss = 0.78 (937.7 examples/sec; 0.137 sec/batch)
2017-05-09 21:48:49.853397: step 22710, loss = 0.86 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-09 21:48:51.198491: step 22720, loss = 0.82 (951.6 examples/sec; 0.135 sec/batch)
2017-05-09 21:48:52.516441: step 22730, loss = 0.91 (971.2 examples/sec; 0.132 sec/batch)
2017-05-09 21:48:53.788576: step 22740, loss = 0.88 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-09 21:48:55.085164: step 22750, loss = 0.73 (987.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:48:56.370163: step 22760, loss = 0.80 (996.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:48:57.668526: step 22770, loss = 0.77 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:48:58.970106: step 22780, loss = 0.83 (983.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:49:00.254326: step 22790, loss = 0.80 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:49:01.637725: step 22800, loss = 0.85 (925.3 examples/sec; 0.138 sec/batch)
2017-05-09 21:49:02.837175: step 22810, loss = 0.77 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-09 21:49:04.138804: step 22820, loss = 0.83 (983.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:49:05.458279: step 22830, loss = 0.88 (970.1 examples/sec; 0.132 sec/batch)
2017-05-09 21:49:06.740724: step 22840, loss = 0.76 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:49:08.018109: step 22850, loss = 0.75 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:49:09.319709: step 22860, loss = 0.80 (983.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:49:10.637507: step 22870, loss = 1.04 (971.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:49:11.940635: step 22880, loss = 0.80 (982.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:49:13.265096: step 22890, loss = 1.23 (966.4 examples/sec; 0.132 sec/batch)
2017-05-09 21:49:14.658040: step 22900, loss = 0.83 (918.9 examples/sec; 0.139 sec/batch)
2017-05-09 21:49:15.871922: step 22910, loss = 0.87 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-09 21:49:17.189212: step 22920, loss = 0.70 (971.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:49:18.486614: step 22930, loss = 0.73 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:49:19.786771: step 22940, loss = 0.69 (984.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:49:21.096646: step 22950, loss = 0.95 (977.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:49:22.394183: step 22960, loss = 0.83 (986.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:49:23.679711: step 22970, loss = 0.93 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:49:25.002869: step 22980, loss = 0.88 (967.4 examples/sec; 0.132 sec/batch)
2017-05-09 21:49:26.300849: step 22990, loss = 0.81 (986.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:49:27.707038: step 23000, loss = 1.02 (910.3 examples/sec; 0.141 sec/batch)
2017-05-09 21:49:28.908805: step 23010, loss = 0.80 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-09 21:49:30.232119: step 23020, loss = 0.89 (967.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:49:31.516921: step 23030, loss = 0.78 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:49:32.813819: step 23040, loss = 0.83 (987.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:49:34.126187: step 23050, loss = 0.82 (975.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:49:35.420312: step 23060, loss = 0.88 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:49:36.705173: step 23070, loss = 0.77 (996.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:49:37.982977: step 23080, loss = 0.98 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:49:39.297819: step 23090, loss = 0.97 (973.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:49:40.691347: step 23100, loss = 0.87 (918.5 examples/sec; 0.139 sec/batch)
2017-05-09 21:49:41.882999: step 23110, loss = 0.94 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-09 21:49:43.211810: step 23120, loss = 0.79 (963.3 examples/sec; 0.133 sec/batch)
2017-05-09 21:49:44.523954: step 23130, loss = 0.71 (975.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:49:45.844984: step 23140, loss = 0.94 (968.9 examples/sec; 0.132 sec/batch)
2017-05-09 21:49:47.122787: step 23150, loss = 0.83 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:49:48.409567: step 23160, loss = 0.80 (994.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:49:49.697810: step 23170, loss = 0.86 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:49:50.981922: step 23180, loss = 0.67 (996.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:49:52.293746: step 23190, loss = 0.68 (975.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:49:53.682518: step 23200, loss = 0.73 (921.7 examples/sec; 0.139 sec/batch)
2017-05-09 21:49:54.897386: step 23210, loss = 0.76 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-09 21:49:56.189269: step 23220, loss = 0.97 (990.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:49:57.538731: step 23230, loss = 0.93 (948.5 examples/sec; 0.135 sec/batch)
2017-05-09 21:49:58.829681: step 23240, loss = 0.86 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:50:00.130332: step 23250, loss = 0.71 (984.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:50:01.421294: step 23260, loss = 0.83 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:50:02.735554: step 23270, loss = 0.89 (973.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:50:04.004067: step 23280, loss = 0.81 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:50:05.295143: step 23290, loss = 0.78 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:50:06.673510: step 23300, loss = 0.96 (928.6 examples/sec; 0.138 sec/batch)
2017-05-09 21:50:07.851957: step 23310, loss = 0.80 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-09 21:50:09.140905: step 23320, loss = 0.86 (993.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:50:10.422140: step 23330, loss = 0.90 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:50:11.718567: step 23340, loss = 0.73 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:50:13.005074: step 23350, loss = 0.69 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:50:14.309698: step 23360, loss = 0.94 (981.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:50:15.600838: step 23370, loss = 0.84 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:50:16.893260: step 23380, loss = 0.94 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:50:18.179595: step 23390, loss = 0.79 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:50:19.578869: step 23400, loss = 0.83 (914.8 examples/sec; 0.140 sec/batch)
2017-05-09 21:50:20.777329: step 23410, loss = 0.92 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-09 21:50:22.117049: step 23420, loss = 0.81 (955.4 examples/sec; 0.134 sec/batch)
2017-05-09 21:50:23.410081: step 23430, loss = 0.90 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:50:24.684637: step 23440, loss = 0.79 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:50:25.970950: step 23450, loss = 0.76 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:50:27.244399: step 23460, loss = 0.75 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:50:28.528010: step 23470, loss = 0.80 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:50:29.800765: step 23480, loss = 0.76 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-09 21:50:31.101067: step 23490, loss = 0.72 (984.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:50:32.475147: step 23500, loss = 0.96 (931.5 examples/sec; 0.137 sec/batch)
2017-05-09 21:50:33.651892: step 23510, loss = 0.89 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-09 21:50:34.940753: step 23520, loss = 0.87 (993.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:50:36.252442: step 23530, loss = 0.83 (975.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:50:37.532480: step 23540, loss = 0.84 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:50:38.808874: step 23550, loss = 0.96 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:50:40.109508: step 23560, loss = 0.75 (984.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:50:41.403273: step 23570, loss = 0.93 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:50:42.708733: step 23580, loss = 0.99 (980.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:50:43.993621: step 23590, loss = 1.05 (996.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:50:45.398378: step 23600, loss = 0.92 (911.2 examples/sec; 0.140 sec/batch)
2017-05-09 21:50:46.637650: step 23610, loss = 1.12 (1032.9 examples/sec; 0.124 sec/batch)
2017-05-09 21:50:47.928553: step 23620, loss = 0.92 (991.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:50:49.232511: step 23630, loss = 0.98 (981.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:50:50.510182: step 23640, loss = 0.77 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:50:51.832949: step 23650, loss = 0.86 (967.7 examples/sec; 0.132 sec/batch)
2017-05-09 21:50:53.141110: step 23660, loss = 1.01 (978.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:50:54.431399: step 23670, loss = 1.03 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:50:55.717000: step 23680, loss = 0.85 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:50:57.025790: step 23690, loss = 0.82 (978.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:50:58.396402: step 23700, loss = 0.77 (933.9 examples/sec; 0.137 sec/batch)
2017-05-09 21:50:59.572498: step 23710, loss = 0.89 (1088.3 examples/sec; 0.E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 486 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
118 sec/batch)
2017-05-09 21:51:00.849029: step 23720, loss = 0.84 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:51:02.124598: step 23730, loss = 0.74 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:51:03.419829: step 23740, loss = 0.83 (988.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:51:04.725267: step 23750, loss = 0.80 (980.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:51:06.031818: step 23760, loss = 0.75 (979.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:51:07.349838: step 23770, loss = 0.89 (971.2 examples/sec; 0.132 sec/batch)
2017-05-09 21:51:08.661343: step 23780, loss = 0.95 (976.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:51:09.934870: step 23790, loss = 0.96 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:51:11.345139: step 23800, loss = 1.06 (907.6 examples/sec; 0.141 sec/batch)
2017-05-09 21:51:12.556873: step 23810, loss = 0.88 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-09 21:51:13.862027: step 23820, loss = 0.82 (980.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:51:15.199878: step 23830, loss = 0.88 (956.8 examples/sec; 0.134 sec/batch)
2017-05-09 21:51:16.565390: step 23840, loss = 0.88 (937.4 examples/sec; 0.137 sec/batch)
2017-05-09 21:51:17.865250: step 23850, loss = 0.82 (984.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:51:19.196280: step 23860, loss = 0.95 (961.7 examples/sec; 0.133 sec/batch)
2017-05-09 21:51:20.477533: step 23870, loss = 0.80 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:51:21.775400: step 23880, loss = 0.73 (986.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:51:23.067433: step 23890, loss = 0.80 (990.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:51:24.467251: step 23900, loss = 0.80 (914.4 examples/sec; 0.140 sec/batch)
2017-05-09 21:51:25.763677: step 23910, loss = 0.94 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:51:27.074872: step 23920, loss = 0.85 (976.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:51:28.377624: step 23930, loss = 1.09 (982.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:51:29.667531: step 23940, loss = 0.81 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:51:30.961286: step 23950, loss = 0.83 (989.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:51:32.264671: step 23960, loss = 0.63 (982.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:51:33.548109: step 23970, loss = 0.69 (997.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:51:34.856609: step 23980, loss = 0.73 (978.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:51:36.170115: step 23990, loss = 0.83 (974.5 examples/sec; 0.131 sec/batch)
2017-05-09 21:51:37.564599: step 24000, loss = 0.73 (917.9 examples/sec; 0.139 sec/batch)
2017-05-09 21:51:38.756831: step 24010, loss = 0.80 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-09 21:51:40.042253: step 24020, loss = 1.02 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:51:41.349498: step 24030, loss = 0.74 (979.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:51:42.640247: step 24040, loss = 0.89 (991.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:51:43.939243: step 24050, loss = 0.67 (985.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:51:45.228912: step 24060, loss = 0.97 (992.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:51:46.537662: step 24070, loss = 0.80 (978.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:51:47.851275: step 24080, loss = 0.64 (974.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:51:49.161551: step 24090, loss = 0.86 (976.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:51:50.568595: step 24100, loss = 0.92 (909.7 examples/sec; 0.141 sec/batch)
2017-05-09 21:51:51.757857: step 24110, loss = 0.84 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-09 21:51:53.058868: step 24120, loss = 1.01 (983.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:51:54.352380: step 24130, loss = 0.86 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:51:55.667405: step 24140, loss = 0.88 (973.4 examples/sec; 0.132 sec/batch)
2017-05-09 21:51:56.964000: step 24150, loss = 0.90 (987.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:51:58.288430: step 24160, loss = 0.82 (966.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:51:59.567598: step 24170, loss = 0.77 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:52:00.840067: step 24180, loss = 0.80 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:52:02.145012: step 24190, loss = 0.92 (980.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:52:03.566998: step 24200, loss = 0.77 (900.1 examples/sec; 0.142 sec/batch)
2017-05-09 21:52:04.772914: step 24210, loss = 0.89 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-09 21:52:06.049548: step 24220, loss = 0.81 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:52:07.327083: step 24230, loss = 0.91 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:52:08.630945: step 24240, loss = 0.90 (981.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:52:09.924006: step 24250, loss = 0.74 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:52:11.213420: step 24260, loss = 0.77 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:52:12.515273: step 24270, loss = 1.21 (983.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:52:13.820837: step 24280, loss = 0.81 (980.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:52:15.116135: step 24290, loss = 0.76 (988.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:52:16.502777: step 24300, loss = 0.81 (923.1 examples/sec; 0.139 sec/batch)
2017-05-09 21:52:17.677805: step 24310, loss = 0.69 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-09 21:52:18.973277: step 24320, loss = 0.73 (988.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:52:20.378179: step 24330, loss = 0.88 (911.1 examples/sec; 0.140 sec/batch)
2017-05-09 21:52:21.672245: step 24340, loss = 1.15 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:52:22.973861: step 24350, loss = 0.76 (983.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:52:24.255210: step 24360, loss = 0.75 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:52:25.519771: step 24370, loss = 0.78 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-09 21:52:26.789674: step 24380, loss = 0.85 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:52:28.083200: step 24390, loss = 0.77 (989.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:52:29.460608: step 24400, loss = 0.87 (929.3 examples/sec; 0.138 sec/batch)
2017-05-09 21:52:30.634533: step 24410, loss = 0.89 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-09 21:52:31.903012: step 24420, loss = 0.92 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:52:33.197983: step 24430, loss = 0.66 (988.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:52:34.508386: step 24440, loss = 0.75 (976.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:52:35.801767: step 24450, loss = 0.74 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:52:37.110902: step 24460, loss = 0.84 (977.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:52:38.381191: step 24470, loss = 0.83 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-09 21:52:39.684787: step 24480, loss = 0.67 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:52:40.975484: step 24490, loss = 0.91 (991.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:52:42.380992: step 24500, loss = 0.78 (910.7 examples/sec; 0.141 sec/batch)
2017-05-09 21:52:43.604566: step 24510, loss = 0.89 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-09 21:52:44.935892: step 24520, loss = 0.79 (961.4 examples/sec; 0.133 sec/batch)
2017-05-09 21:52:46.231956: step 24530, loss = 0.97 (987.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:52:47.517906: step 24540, loss = 0.75 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:52:48.800437: step 24550, loss = 0.77 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:52:50.087026: step 24560, loss = 0.74 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:52:51.356506: step 24570, loss = 0.97 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:52:52.668860: step 24580, loss = 0.84 (975.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:52:53.991282: step 24590, loss = 0.81 (967.9 examples/sec; 0.132 sec/batch)
2017-05-09 21:52:55.404259: step 24600, loss = 0.89 (905.9 examples/sec; 0.141 sec/batch)
2017-05-09 21:52:56.610537: step 24610, loss = 0.97 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-09 21:52:57.885621: step 24620, loss = 0.77 (1003E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 506 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:52:59.175524: step 24630, loss = 0.72 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:53:00.485500: step 24640, loss = 0.87 (977.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:53:01.782938: step 24650, loss = 0.76 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:53:03.087868: step 24660, loss = 0.77 (980.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:53:04.391902: step 24670, loss = 0.87 (981.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:53:05.668983: step 24680, loss = 1.00 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:53:06.953754: step 24690, loss = 0.91 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:53:08.327994: step 24700, loss = 0.97 (931.4 examples/sec; 0.137 sec/batch)
2017-05-09 21:53:09.519969: step 24710, loss = 0.98 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-09 21:53:10.806708: step 24720, loss = 0.97 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:53:12.095800: step 24730, loss = 0.71 (993.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:53:13.399649: step 24740, loss = 0.81 (981.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:53:14.701836: step 24750, loss = 0.89 (983.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:53:16.004282: step 24760, loss = 0.94 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:53:17.286944: step 24770, loss = 0.82 (997.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:53:18.574322: step 24780, loss = 1.02 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:53:19.888959: step 24790, loss = 0.81 (973.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:53:21.303368: step 24800, loss = 1.03 (905.0 examples/sec; 0.141 sec/batch)
2017-05-09 21:53:22.498678: step 24810, loss = 0.82 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-09 21:53:23.794838: step 24820, loss = 0.79 (987.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:53:25.086545: step 24830, loss = 0.74 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:53:26.355291: step 24840, loss = 0.96 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:53:27.659164: step 24850, loss = 1.00 (981.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:53:28.993354: step 24860, loss = 0.88 (959.4 examples/sec; 0.133 sec/batch)
2017-05-09 21:53:30.315528: step 24870, loss = 0.82 (968.1 examples/sec; 0.132 sec/batch)
2017-05-09 21:53:31.624507: step 24880, loss = 0.70 (977.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:53:32.909971: step 24890, loss = 0.72 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:53:34.253751: step 24900, loss = 0.92 (952.5 examples/sec; 0.134 sec/batch)
2017-05-09 21:53:35.456906: step 24910, loss = 0.73 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-09 21:53:36.738825: step 24920, loss = 0.94 (998.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:53:38.034269: step 24930, loss = 0.71 (988.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:53:39.320755: step 24940, loss = 0.85 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:53:40.620938: step 24950, loss = 0.84 (984.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:53:41.917624: step 24960, loss = 0.84 (987.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:53:43.217166: step 24970, loss = 0.87 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:53:44.533299: step 24980, loss = 0.76 (972.5 examples/sec; 0.132 sec/batch)
2017-05-09 21:53:45.849038: step 24990, loss = 0.86 (972.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:53:47.202986: step 25000, loss = 0.92 (945.4 examples/sec; 0.135 sec/batch)
2017-05-09 21:53:48.423911: step 25010, loss = 0.95 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-09 21:53:49.725382: step 25020, loss = 1.00 (983.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:53:51.036749: step 25030, loss = 0.72 (976.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:53:52.377828: step 25040, loss = 0.78 (954.5 examples/sec; 0.134 sec/batch)
2017-05-09 21:53:53.801614: step 25050, loss = 0.90 (899.0 examples/sec; 0.142 sec/batch)
2017-05-09 21:53:55.100242: step 25060, loss = 0.80 (985.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:53:56.373377: step 25070, loss = 0.94 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:53:57.658762: step 25080, loss = 0.87 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:53:58.960853: step 25090, loss = 0.75 (983.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:54:00.339916: step 25100, loss = 0.92 (928.2 examples/sec; 0.138 sec/batch)
2017-05-09 21:54:01.537633: step 25110, loss = 0.83 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-09 21:54:02.816496: step 25120, loss = 0.92 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:54:04.104976: step 25130, loss = 0.80 (993.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:54:05.423968: step 25140, loss = 0.89 (970.4 examples/sec; 0.132 sec/batch)
2017-05-09 21:54:06.725364: step 25150, loss = 0.89 (983.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:54:08.005715: step 25160, loss = 0.86 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:54:09.274418: step 25170, loss = 0.88 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:54:10.591146: step 25180, loss = 0.89 (972.1 examples/sec; 0.132 sec/batch)
2017-05-09 21:54:11.884429: step 25190, loss = 1.10 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:54:13.266767: step 25200, loss = 0.81 (926.0 examples/sec; 0.138 sec/batch)
2017-05-09 21:54:14.463041: step 25210, loss = 0.79 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-09 21:54:15.738019: step 25220, loss = 0.82 (1003.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:54:17.026533: step 25230, loss = 0.77 (993.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:54:18.345688: step 25240, loss = 0.75 (970.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:54:19.649543: step 25250, loss = 0.91 (981.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:54:20.937757: step 25260, loss = 0.96 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:54:22.258961: step 25270, loss = 0.87 (968.8 examples/sec; 0.132 sec/batch)
2017-05-09 21:54:23.545748: step 25280, loss = 0.75 (994.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:54:24.871424: step 25290, loss = 0.82 (965.5 examples/sec; 0.133 sec/batch)
2017-05-09 21:54:26.264067: step 25300, loss = 0.91 (919.1 examples/sec; 0.139 sec/batch)
2017-05-09 21:54:27.449705: step 25310, loss = 0.75 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-09 21:54:28.741725: step 25320, loss = 0.89 (990.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:54:30.024049: step 25330, loss = 0.95 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:54:31.335339: step 25340, loss = 0.83 (976.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:54:32.607625: step 25350, loss = 0.71 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-09 21:54:33.902092: step 25360, loss = 0.87 (988.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:54:35.207420: step 25370, loss = 0.76 (980.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:54:36.497148: step 25380, loss = 0.71 (992.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:54:37.766030: step 25390, loss = 0.74 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-09 21:54:39.157763: step 25400, loss = 0.82 (919.7 examples/sec; 0.139 sec/batch)
2017-05-09 21:54:40.364568: step 25410, loss = 0.81 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-09 21:54:41.658789: step 25420, loss = 0.75 (989.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:54:42.930782: step 25430, loss = 0.84 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:54:44.221756: step 25440, loss = 0.95 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:54:45.501013: step 25450, loss = 0.81 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:54:46.782836: step 25460, loss = 0.87 (998.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:54:48.063266: step 25470, loss = 0.78 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:54:49.366862: step 25480, loss = 0.93 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:54:50.650419: step 25490, loss = 1.00 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:54:52.023245: step 25500, loss = 0.87 (932.4 examples/sec; 0.137 sec/batch)
2017-05-09 21:54:53.206744: step 25510, loss = 1.00 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-09 21:54:54.509429: step 25520, loss = 0.87 (982.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:54:55.803471: step 25530,E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 526 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
 loss = 0.88 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:54:57.058545: step 25540, loss = 0.89 (1019.8 examples/sec; 0.126 sec/batch)
2017-05-09 21:54:58.343385: step 25550, loss = 0.82 (996.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:54:59.628530: step 25560, loss = 0.76 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 21:55:00.887044: step 25570, loss = 0.79 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-09 21:55:02.219772: step 25580, loss = 0.80 (960.4 examples/sec; 0.133 sec/batch)
2017-05-09 21:55:03.487988: step 25590, loss = 0.83 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-09 21:55:04.858702: step 25600, loss = 0.92 (933.8 examples/sec; 0.137 sec/batch)
2017-05-09 21:55:06.070524: step 25610, loss = 0.86 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-09 21:55:07.356246: step 25620, loss = 0.82 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:55:08.631430: step 25630, loss = 0.77 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:55:09.910251: step 25640, loss = 0.84 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:55:11.190075: step 25650, loss = 0.75 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:55:12.482638: step 25660, loss = 0.66 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:55:13.780540: step 25670, loss = 0.66 (986.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:55:15.106266: step 25680, loss = 0.94 (965.5 examples/sec; 0.133 sec/batch)
2017-05-09 21:55:16.394637: step 25690, loss = 0.70 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 21:55:17.781949: step 25700, loss = 0.89 (922.6 examples/sec; 0.139 sec/batch)
2017-05-09 21:55:18.965878: step 25710, loss = 0.65 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-09 21:55:20.249866: step 25720, loss = 0.81 (996.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:55:21.534046: step 25730, loss = 0.80 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:55:22.814904: step 25740, loss = 1.08 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:55:24.127157: step 25750, loss = 0.81 (975.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:55:25.437668: step 25760, loss = 0.83 (976.7 examples/sec; 0.131 sec/batch)
2017-05-09 21:55:26.743391: step 25770, loss = 0.76 (980.3 examples/sec; 0.131 sec/batch)
2017-05-09 21:55:28.042885: step 25780, loss = 0.87 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:55:29.354483: step 25790, loss = 0.85 (975.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:55:30.754322: step 25800, loss = 0.68 (914.4 examples/sec; 0.140 sec/batch)
2017-05-09 21:55:31.944489: step 25810, loss = 0.98 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-09 21:55:33.244788: step 25820, loss = 0.86 (984.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:55:34.562425: step 25830, loss = 0.99 (971.4 examples/sec; 0.132 sec/batch)
2017-05-09 21:55:35.840939: step 25840, loss = 0.87 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-09 21:55:37.117432: step 25850, loss = 0.92 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:55:38.400614: step 25860, loss = 0.86 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:55:39.715580: step 25870, loss = 1.01 (973.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:55:41.020117: step 25880, loss = 0.82 (981.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:55:42.292582: step 25890, loss = 0.71 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:55:43.658198: step 25900, loss = 0.92 (937.3 examples/sec; 0.137 sec/batch)
2017-05-09 21:55:44.871381: step 25910, loss = 0.92 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-09 21:55:46.172081: step 25920, loss = 1.12 (984.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:55:47.462062: step 25930, loss = 0.74 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 21:55:48.756098: step 25940, loss = 0.81 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:55:50.056418: step 25950, loss = 0.80 (984.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:55:51.336723: step 25960, loss = 0.91 (999.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:55:52.659062: step 25970, loss = 0.88 (968.0 examples/sec; 0.132 sec/batch)
2017-05-09 21:55:53.947794: step 25980, loss = 0.82 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:55:55.227199: step 25990, loss = 0.74 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:55:56.583807: step 26000, loss = 0.73 (943.5 examples/sec; 0.136 sec/batch)
2017-05-09 21:55:57.770815: step 26010, loss = 0.79 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-09 21:55:59.055883: step 26020, loss = 0.79 (996.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:56:00.338851: step 26030, loss = 0.75 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:56:01.619011: step 26040, loss = 0.88 (999.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:56:02.918185: step 26050, loss = 0.86 (985.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:56:04.215339: step 26060, loss = 0.87 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 21:56:05.528552: step 26070, loss = 0.72 (974.8 examples/sec; 0.131 sec/batch)
2017-05-09 21:56:06.821482: step 26080, loss = 0.92 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:56:08.130416: step 26090, loss = 0.82 (977.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:56:09.570115: step 26100, loss = 0.88 (889.1 examples/sec; 0.144 sec/batch)
2017-05-09 21:56:10.724227: step 26110, loss = 0.79 (1109.1 examples/sec; 0.115 sec/batch)
2017-05-09 21:56:12.004093: step 26120, loss = 0.90 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:56:13.310316: step 26130, loss = 1.15 (979.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:56:14.618353: step 26140, loss = 0.82 (978.6 examples/sec; 0.131 sec/batch)
2017-05-09 21:56:15.924616: step 26150, loss = 0.87 (979.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:56:17.197694: step 26160, loss = 0.87 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-09 21:56:18.496024: step 26170, loss = 1.10 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:56:19.793920: step 26180, loss = 0.83 (986.2 examples/sec; 0.130 sec/batch)
2017-05-09 21:56:21.065242: step 26190, loss = 0.81 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-09 21:56:22.454180: step 26200, loss = 0.93 (921.6 examples/sec; 0.139 sec/batch)
2017-05-09 21:56:23.633314: step 26210, loss = 0.94 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-09 21:56:24.939410: step 26220, loss = 0.84 (980.0 examples/sec; 0.131 sec/batch)
2017-05-09 21:56:26.235095: step 26230, loss = 0.89 (987.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:56:27.518609: step 26240, loss = 0.83 (997.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:56:28.807997: step 26250, loss = 0.76 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:56:30.086655: step 26260, loss = 0.79 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-09 21:56:31.419441: step 26270, loss = 1.18 (960.4 examples/sec; 0.133 sec/batch)
2017-05-09 21:56:32.717540: step 26280, loss = 0.70 (986.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:56:34.017674: step 26290, loss = 0.76 (984.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:56:35.422734: step 26300, loss = 0.88 (911.0 examples/sec; 0.141 sec/batch)
2017-05-09 21:56:36.604320: step 26310, loss = 0.84 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-09 21:56:37.894821: step 26320, loss = 0.64 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:56:39.184316: step 26330, loss = 0.79 (992.6 examples/sec; 0.129 sec/batch)
2017-05-09 21:56:40.480547: step 26340, loss = 0.87 (987.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:56:41.783911: step 26350, loss = 1.06 (982.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:56:43.104395: step 26360, loss = 0.68 (969.3 examples/sec; 0.132 sec/batch)
2017-05-09 21:56:44.410102: step 26370, loss = 0.83 (980.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:56:45.677404: step 26380, loss = 0.83 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:56:46.968533: step 26390, loss = 0.95 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:56:48.338749: step 26400, loss = 0.86 (934.2 examples/sec; 0.137 sec/batch)
2017-05-09 21:56:49.526530: step 26410, loss = 0.75 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-09 21:56:50.823409: step 26420, loss = 0.79 (987.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:56:52.104885: step 26430, loss = 1.05 (998.8 examples/sec; 0.128 sec/batch)
2017-05-09 21:56:E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 546 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
53.398262: step 26440, loss = 0.82 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:56:54.667069: step 26450, loss = 0.84 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-09 21:56:55.939636: step 26460, loss = 0.82 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-09 21:56:57.223372: step 26470, loss = 0.95 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:56:58.540252: step 26480, loss = 0.80 (972.0 examples/sec; 0.132 sec/batch)
2017-05-09 21:56:59.821936: step 26490, loss = 0.80 (998.7 examples/sec; 0.128 sec/batch)
2017-05-09 21:57:01.215868: step 26500, loss = 0.87 (918.3 examples/sec; 0.139 sec/batch)
2017-05-09 21:57:02.400002: step 26510, loss = 0.75 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-09 21:57:03.694564: step 26520, loss = 0.78 (988.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:57:04.975086: step 26530, loss = 0.88 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:57:06.279375: step 26540, loss = 0.68 (981.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:57:07.567834: step 26550, loss = 1.01 (993.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:57:08.858385: step 26560, loss = 0.81 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:57:10.142400: step 26570, loss = 0.72 (996.9 examples/sec; 0.128 sec/batch)
2017-05-09 21:57:11.471175: step 26580, loss = 0.69 (963.3 examples/sec; 0.133 sec/batch)
2017-05-09 21:57:12.754212: step 26590, loss = 0.89 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:57:14.148574: step 26600, loss = 0.85 (918.0 examples/sec; 0.139 sec/batch)
2017-05-09 21:57:15.356338: step 26610, loss = 0.81 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-09 21:57:16.634926: step 26620, loss = 0.88 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:57:17.937961: step 26630, loss = 0.80 (982.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:57:19.279463: step 26640, loss = 0.78 (954.2 examples/sec; 0.134 sec/batch)
2017-05-09 21:57:20.580271: step 26650, loss = 0.96 (984.0 examples/sec; 0.130 sec/batch)
2017-05-09 21:57:21.887580: step 26660, loss = 0.85 (979.1 examples/sec; 0.131 sec/batch)
2017-05-09 21:57:23.245915: step 26670, loss = 0.75 (942.3 examples/sec; 0.136 sec/batch)
2017-05-09 21:57:24.547569: step 26680, loss = 0.88 (983.4 examples/sec; 0.130 sec/batch)
2017-05-09 21:57:25.834216: step 26690, loss = 0.84 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 21:57:27.199570: step 26700, loss = 0.72 (937.5 examples/sec; 0.137 sec/batch)
2017-05-09 21:57:28.412191: step 26710, loss = 0.70 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-09 21:57:29.709157: step 26720, loss = 0.77 (986.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:57:30.997079: step 26730, loss = 0.77 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:57:32.337743: step 26740, loss = 0.76 (954.7 examples/sec; 0.134 sec/batch)
2017-05-09 21:57:33.980096: step 26750, loss = 1.06 (779.4 examples/sec; 0.164 sec/batch)
2017-05-09 21:57:35.507025: step 26760, loss = 0.76 (838.3 examples/sec; 0.153 sec/batch)
2017-05-09 21:57:37.119086: step 26770, loss = 0.74 (794.0 examples/sec; 0.161 sec/batch)
2017-05-09 21:57:38.753262: step 26780, loss = 0.92 (783.3 examples/sec; 0.163 sec/batch)
2017-05-09 21:57:40.290750: step 26790, loss = 0.68 (832.5 examples/sec; 0.154 sec/batch)
2017-05-09 21:57:42.216826: step 26800, loss = 1.02 (664.6 examples/sec; 0.193 sec/batch)
2017-05-09 21:57:43.672940: step 26810, loss = 0.72 (879.0 examples/sec; 0.146 sec/batch)
2017-05-09 21:57:45.182884: step 26820, loss = 0.73 (847.7 examples/sec; 0.151 sec/batch)
2017-05-09 21:57:46.771424: step 26830, loss = 0.76 (805.8 examples/sec; 0.159 sec/batch)
2017-05-09 21:57:48.328281: step 26840, loss = 1.05 (822.2 examples/sec; 0.156 sec/batch)
2017-05-09 21:57:49.790825: step 26850, loss = 0.88 (875.2 examples/sec; 0.146 sec/batch)
2017-05-09 21:57:51.323201: step 26860, loss = 0.90 (835.3 examples/sec; 0.153 sec/batch)
2017-05-09 21:57:53.066231: step 26870, loss = 0.78 (734.4 examples/sec; 0.174 sec/batch)
2017-05-09 21:57:54.853701: step 26880, loss = 0.74 (716.1 examples/sec; 0.179 sec/batch)
2017-05-09 21:57:56.534611: step 26890, loss = 0.68 (761.5 examples/sec; 0.168 sec/batch)
2017-05-09 21:57:58.400419: step 26900, loss = 0.94 (686.0 examples/sec; 0.187 sec/batch)
2017-05-09 21:57:59.771175: step 26910, loss = 0.81 (933.8 examples/sec; 0.137 sec/batch)
2017-05-09 21:58:01.434244: step 26920, loss = 0.97 (769.7 examples/sec; 0.166 sec/batch)
2017-05-09 21:58:02.960687: step 26930, loss = 0.72 (838.6 examples/sec; 0.153 sec/batch)
2017-05-09 21:58:04.505784: step 26940, loss = 0.95 (828.4 examples/sec; 0.155 sec/batch)
2017-05-09 21:58:06.091129: step 26950, loss = 0.86 (807.4 examples/sec; 0.159 sec/batch)
2017-05-09 21:58:07.777540: step 26960, loss = 0.75 (759.0 examples/sec; 0.169 sec/batch)
2017-05-09 21:58:09.371789: step 26970, loss = 0.69 (802.9 examples/sec; 0.159 sec/batch)
2017-05-09 21:58:10.930909: step 26980, loss = 0.83 (821.0 examples/sec; 0.156 sec/batch)
2017-05-09 21:58:12.693098: step 26990, loss = 0.79 (726.4 examples/sec; 0.176 sec/batch)
2017-05-09 21:58:14.531833: step 27000, loss = 0.81 (696.1 examples/sec; 0.184 sec/batch)
2017-05-09 21:58:15.871819: step 27010, loss = 0.88 (955.2 examples/sec; 0.134 sec/batch)
2017-05-09 21:58:17.355656: step 27020, loss = 0.92 (862.6 examples/sec; 0.148 sec/batch)
2017-05-09 21:58:18.988811: step 27030, loss = 0.70 (783.8 examples/sec; 0.163 sec/batch)
2017-05-09 21:58:20.624651: step 27040, loss = 0.80 (782.5 examples/sec; 0.164 sec/batch)
2017-05-09 21:58:22.141507: step 27050, loss = 0.68 (843.8 examples/sec; 0.152 sec/batch)
2017-05-09 21:58:23.708202: step 27060, loss = 0.66 (817.0 examples/sec; 0.157 sec/batch)
2017-05-09 21:58:25.378411: step 27070, loss = 0.68 (766.4 examples/sec; 0.167 sec/batch)
2017-05-09 21:58:27.090838: step 27080, loss = 0.88 (747.5 examples/sec; 0.171 sec/batch)
2017-05-09 21:58:28.721590: step 27090, loss = 0.94 (784.9 examples/sec; 0.163 sec/batch)
2017-05-09 21:58:30.411387: step 27100, loss = 0.86 (757.5 examples/sec; 0.169 sec/batch)
2017-05-09 21:58:31.755968: step 27110, loss = 0.82 (952.0 examples/sec; 0.134 sec/batch)
2017-05-09 21:58:33.402697: step 27120, loss = 0.77 (777.3 examples/sec; 0.165 sec/batch)
2017-05-09 21:58:34.894153: step 27130, loss = 0.79 (858.2 examples/sec; 0.149 sec/batch)
2017-05-09 21:58:36.472234: step 27140, loss = 0.69 (811.1 examples/sec; 0.158 sec/batch)
2017-05-09 21:58:38.065189: step 27150, loss = 1.08 (803.5 examples/sec; 0.159 sec/batch)
2017-05-09 21:58:39.341231: step 27160, loss = 0.80 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:58:40.625622: step 27170, loss = 0.89 (996.6 examples/sec; 0.128 sec/batch)
2017-05-09 21:58:42.351329: step 27180, loss = 0.77 (741.7 examples/sec; 0.173 sec/batch)
2017-05-09 21:58:43.944283: step 27190, loss = 0.89 (803.5 examples/sec; 0.159 sec/batch)
2017-05-09 21:58:45.792337: step 27200, loss = 0.93 (692.6 examples/sec; 0.185 sec/batch)
2017-05-09 21:58:47.089603: step 27210, loss = 0.90 (986.7 examples/sec; 0.130 sec/batch)
2017-05-09 21:58:48.665821: step 27220, loss = 1.06 (812.1 examples/sec; 0.158 sec/batch)
2017-05-09 21:58:50.282482: step 27230, loss = 0.66 (791.8 examples/sec; 0.162 sec/batch)
2017-05-09 21:58:51.917304: step 27240, loss = 0.95 (782.9 examples/sec; 0.163 sec/batch)
2017-05-09 21:58:53.420500: step 27250, loss = 0.83 (851.5 examples/sec; 0.150 sec/batch)
2017-05-09 21:58:54.842305: step 27260, loss = 0.91 (900.3 examples/sec; 0.142 sec/batch)
2017-05-09 21:58:56.222575: step 27270, loss = 0.93 (927.4 examples/sec; 0.138 sec/batch)
2017-05-09 21:58:57.603112: step 27280, loss = 0.78 (927.2 examples/sec; 0.138 sec/batch)
2017-05-09 21:58:58.926752: step 27290, loss = 0.87 (967.0 examples/sec; 0.132 sec/batch)
2017-05-09 21:59:00.340793: step 27300, loss = 0.86 (905.2 examples/sec; 0.141 sec/batch)
2017-05-09 21:59:01.561663: step 27310, loss = 1.03 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-09 21:59:02.881797: step 27320, loss = 1.00 (969.6 examples/sec; 0.132 sec/batch)
2017-05-09 21:59:04.175772: step 27330, loss = 0.73 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 21:59:05.475038: step 27340, loss = 0.79 (985.2 examples/sec; 0.130 sec/batch)
2017-05-09 E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 565 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
21:59:06.807901: step 27350, loss = 0.76 (960.3 examples/sec; 0.133 sec/batch)
2017-05-09 21:59:08.121773: step 27360, loss = 0.67 (974.2 examples/sec; 0.131 sec/batch)
2017-05-09 21:59:09.462110: step 27370, loss = 0.83 (955.0 examples/sec; 0.134 sec/batch)
2017-05-09 21:59:10.804783: step 27380, loss = 0.95 (953.3 examples/sec; 0.134 sec/batch)
2017-05-09 21:59:12.606979: step 27390, loss = 0.92 (710.2 examples/sec; 0.180 sec/batch)
2017-05-09 21:59:14.385985: step 27400, loss = 0.86 (719.5 examples/sec; 0.178 sec/batch)
2017-05-09 21:59:15.702255: step 27410, loss = 0.76 (972.4 examples/sec; 0.132 sec/batch)
2017-05-09 21:59:17.221666: step 27420, loss = 0.91 (842.4 examples/sec; 0.152 sec/batch)
2017-05-09 21:59:18.731634: step 27430, loss = 0.99 (847.7 examples/sec; 0.151 sec/batch)
2017-05-09 21:59:20.431910: step 27440, loss = 0.76 (752.8 examples/sec; 0.170 sec/batch)
2017-05-09 21:59:21.915003: step 27450, loss = 0.76 (863.1 examples/sec; 0.148 sec/batch)
2017-05-09 21:59:23.195599: step 27460, loss = 0.71 (999.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:59:24.493877: step 27470, loss = 0.86 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:59:25.785626: step 27480, loss = 0.95 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 21:59:27.093835: step 27490, loss = 0.75 (978.4 examples/sec; 0.131 sec/batch)
2017-05-09 21:59:28.503385: step 27500, loss = 0.82 (908.1 examples/sec; 0.141 sec/batch)
2017-05-09 21:59:29.733061: step 27510, loss = 0.65 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-09 21:59:31.024229: step 27520, loss = 0.95 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 21:59:32.306177: step 27530, loss = 0.93 (998.5 examples/sec; 0.128 sec/batch)
2017-05-09 21:59:33.588565: step 27540, loss = 0.84 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 21:59:34.916791: step 27550, loss = 0.82 (963.7 examples/sec; 0.133 sec/batch)
2017-05-09 21:59:36.221762: step 27560, loss = 0.69 (980.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:59:37.536758: step 27570, loss = 0.83 (973.4 examples/sec; 0.132 sec/batch)
2017-05-09 21:59:38.825607: step 27580, loss = 0.71 (993.1 examples/sec; 0.129 sec/batch)
2017-05-09 21:59:40.201206: step 27590, loss = 0.80 (930.5 examples/sec; 0.138 sec/batch)
2017-05-09 21:59:41.575041: step 27600, loss = 1.00 (931.7 examples/sec; 0.137 sec/batch)
2017-05-09 21:59:42.780509: step 27610, loss = 0.93 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-09 21:59:44.054256: step 27620, loss = 0.90 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-09 21:59:45.330059: step 27630, loss = 0.61 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-09 21:59:46.624705: step 27640, loss = 0.69 (988.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:59:47.922520: step 27650, loss = 0.98 (986.3 examples/sec; 0.130 sec/batch)
2017-05-09 21:59:49.227221: step 27660, loss = 0.61 (981.1 examples/sec; 0.130 sec/batch)
2017-05-09 21:59:50.529528: step 27670, loss = 0.98 (982.9 examples/sec; 0.130 sec/batch)
2017-05-09 21:59:51.838417: step 27680, loss = 0.80 (977.9 examples/sec; 0.131 sec/batch)
2017-05-09 21:59:53.141185: step 27690, loss = 1.02 (982.5 examples/sec; 0.130 sec/batch)
2017-05-09 21:59:54.554213: step 27700, loss = 0.95 (905.8 examples/sec; 0.141 sec/batch)
2017-05-09 21:59:55.765279: step 27710, loss = 0.99 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-09 21:59:57.064013: step 27720, loss = 0.81 (985.6 examples/sec; 0.130 sec/batch)
2017-05-09 21:59:58.357343: step 27730, loss = 0.90 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 21:59:59.659127: step 27740, loss = 0.86 (983.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:00:00.965696: step 27750, loss = 0.88 (979.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:00:02.311350: step 27760, loss = 0.79 (951.2 examples/sec; 0.135 sec/batch)
2017-05-09 22:00:03.622235: step 27770, loss = 0.78 (976.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:00:04.916874: step 27780, loss = 0.76 (988.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:00:06.228345: step 27790, loss = 0.81 (976.0 examples/sec; 0.131 sec/batch)
2017-05-09 22:00:07.645506: step 27800, loss = 1.04 (903.2 examples/sec; 0.142 sec/batch)
2017-05-09 22:00:08.865415: step 27810, loss = 0.76 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-09 22:00:10.151691: step 27820, loss = 0.96 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:00:11.431679: step 27830, loss = 0.86 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:00:12.722140: step 27840, loss = 0.78 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:00:14.034803: step 27850, loss = 0.67 (975.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:00:15.319534: step 27860, loss = 0.77 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:00:16.624094: step 27870, loss = 0.85 (981.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:00:17.938120: step 27880, loss = 0.79 (974.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:00:19.246811: step 27890, loss = 0.72 (978.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:00:20.616205: step 27900, loss = 0.84 (934.7 examples/sec; 0.137 sec/batch)
2017-05-09 22:00:21.848723: step 27910, loss = 0.73 (1038.5 examples/sec; 0.123 sec/batch)
2017-05-09 22:00:23.136429: step 27920, loss = 0.84 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:00:24.425354: step 27930, loss = 0.89 (993.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:00:25.713047: step 27940, loss = 0.69 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:00:27.032079: step 27950, loss = 0.75 (970.4 examples/sec; 0.132 sec/batch)
2017-05-09 22:00:28.323850: step 27960, loss = 0.83 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:00:29.650570: step 27970, loss = 0.93 (964.8 examples/sec; 0.133 sec/batch)
2017-05-09 22:00:30.953942: step 27980, loss = 0.84 (982.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:00:32.237415: step 27990, loss = 0.81 (997.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:00:33.651040: step 28000, loss = 0.79 (905.5 examples/sec; 0.141 sec/batch)
2017-05-09 22:00:34.840767: step 28010, loss = 0.82 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-09 22:00:36.128211: step 28020, loss = 0.79 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:00:37.402879: step 28030, loss = 0.85 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:00:38.688537: step 28040, loss = 0.89 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:00:39.983887: step 28050, loss = 0.72 (988.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:00:41.256229: step 28060, loss = 0.88 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:00:42.541825: step 28070, loss = 0.88 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:00:43.819164: step 28080, loss = 0.84 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:00:45.107051: step 28090, loss = 0.75 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:00:46.494200: step 28100, loss = 0.86 (922.8 examples/sec; 0.139 sec/batch)
2017-05-09 22:00:47.686310: step 28110, loss = 0.84 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-09 22:00:48.964199: step 28120, loss = 0.85 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:00:50.274772: step 28130, loss = 0.87 (976.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:00:51.560075: step 28140, loss = 0.77 (995.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:00:52.871023: step 28150, loss = 0.95 (976.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:00:54.159683: step 28160, loss = 0.82 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:00:55.438241: step 28170, loss = 0.73 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:00:56.843719: step 28180, loss = 0.84 (910.7 examples/sec; 0.141 sec/batch)
2017-05-09 22:00:58.129579: step 28190, loss = 0.96 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:00:59.508037: step 28200, loss = 0.85 (928.6 examples/sec; 0.138 sec/batch)
2017-05-09 22:01:00.686276: step 28210, loss = 0.83 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-09 22:01:01.990574: step 28220, loss = 0.79 (981.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:01:03.282016: step 28230, loss = 0.93 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:01:04.574438: step 28240, loss = 0.82 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:01:05.873228: step 28250, loss = 0.85 (985.5 examples/sec; 0.130 sec/batcE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 585 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
h)
2017-05-09 22:01:07.181089: step 28260, loss = 0.89 (978.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:01:08.477573: step 28270, loss = 0.64 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:01:09.824564: step 28280, loss = 0.86 (950.3 examples/sec; 0.135 sec/batch)
2017-05-09 22:01:11.119949: step 28290, loss = 0.86 (988.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:01:12.477552: step 28300, loss = 0.76 (942.8 examples/sec; 0.136 sec/batch)
2017-05-09 22:01:13.710168: step 28310, loss = 0.82 (1038.4 examples/sec; 0.123 sec/batch)
2017-05-09 22:01:15.119503: step 28320, loss = 1.03 (908.2 examples/sec; 0.141 sec/batch)
2017-05-09 22:01:16.441208: step 28330, loss = 0.77 (968.4 examples/sec; 0.132 sec/batch)
2017-05-09 22:01:17.727637: step 28340, loss = 0.82 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:01:19.002897: step 28350, loss = 0.76 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:01:20.277225: step 28360, loss = 0.81 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:01:21.554805: step 28370, loss = 0.87 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:01:22.839046: step 28380, loss = 0.84 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:01:24.127281: step 28390, loss = 0.80 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:01:25.496760: step 28400, loss = 0.65 (934.7 examples/sec; 0.137 sec/batch)
2017-05-09 22:01:26.682621: step 28410, loss = 0.72 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-09 22:01:27.949682: step 28420, loss = 0.74 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:01:29.236631: step 28430, loss = 1.08 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:01:30.538083: step 28440, loss = 1.03 (983.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:01:31.847303: step 28450, loss = 0.86 (977.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:01:33.129275: step 28460, loss = 0.98 (998.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:01:34.439147: step 28470, loss = 0.95 (977.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:01:35.728304: step 28480, loss = 0.83 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:01:37.016223: step 28490, loss = 0.83 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:01:38.404976: step 28500, loss = 0.86 (921.7 examples/sec; 0.139 sec/batch)
2017-05-09 22:01:39.606084: step 28510, loss = 0.96 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-09 22:01:40.918567: step 28520, loss = 0.89 (975.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:01:42.225151: step 28530, loss = 0.89 (979.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:01:43.507624: step 28540, loss = 0.83 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:01:44.806663: step 28550, loss = 0.66 (985.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:01:46.256275: step 28560, loss = 0.68 (883.0 examples/sec; 0.145 sec/batch)
2017-05-09 22:01:47.723216: step 28570, loss = 0.70 (872.6 examples/sec; 0.147 sec/batch)
2017-05-09 22:01:49.203575: step 28580, loss = 0.72 (864.7 examples/sec; 0.148 sec/batch)
2017-05-09 22:01:50.530501: step 28590, loss = 0.77 (964.6 examples/sec; 0.133 sec/batch)
2017-05-09 22:01:51.909817: step 28600, loss = 0.76 (928.0 examples/sec; 0.138 sec/batch)
2017-05-09 22:01:53.095328: step 28610, loss = 1.00 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-09 22:01:54.372896: step 28620, loss = 1.01 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:01:55.643643: step 28630, loss = 0.76 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:01:56.954859: step 28640, loss = 0.83 (976.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:01:58.224996: step 28650, loss = 0.86 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:01:59.507812: step 28660, loss = 0.70 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:02:00.806084: step 28670, loss = 0.86 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:02:02.083886: step 28680, loss = 0.98 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:02:03.391492: step 28690, loss = 0.71 (978.9 examples/sec; 0.131 sec/batch)
2017-05-09 22:02:04.773114: step 28700, loss = 0.84 (926.4 examples/sec; 0.138 sec/batch)
2017-05-09 22:02:05.980689: step 28710, loss = 0.85 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-09 22:02:07.261656: step 28720, loss = 0.67 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:02:08.554684: step 28730, loss = 0.75 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:02:09.861329: step 28740, loss = 0.68 (979.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:02:11.140679: step 28750, loss = 0.79 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:02:12.451807: step 28760, loss = 0.87 (976.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:02:13.736498: step 28770, loss = 0.79 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:02:15.012475: step 28780, loss = 0.87 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:02:16.330791: step 28790, loss = 1.02 (970.9 examples/sec; 0.132 sec/batch)
2017-05-09 22:02:17.751738: step 28800, loss = 0.83 (900.8 examples/sec; 0.142 sec/batch)
2017-05-09 22:02:18.970562: step 28810, loss = 0.84 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-09 22:02:20.276345: step 28820, loss = 0.91 (980.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:02:21.588010: step 28830, loss = 0.76 (975.9 examples/sec; 0.131 sec/batch)
2017-05-09 22:02:22.916936: step 28840, loss = 0.84 (963.2 examples/sec; 0.133 sec/batch)
2017-05-09 22:02:24.222488: step 28850, loss = 0.70 (980.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:02:25.513621: step 28860, loss = 0.80 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:02:26.795088: step 28870, loss = 0.87 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:02:28.104074: step 28880, loss = 0.94 (977.9 examples/sec; 0.131 sec/batch)
2017-05-09 22:02:29.379985: step 28890, loss = 0.93 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:02:30.773011: step 28900, loss = 0.76 (918.9 examples/sec; 0.139 sec/batch)
2017-05-09 22:02:31.978164: step 28910, loss = 0.76 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-09 22:02:33.269365: step 28920, loss = 0.81 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:02:34.591363: step 28930, loss = 0.96 (968.2 examples/sec; 0.132 sec/batch)
2017-05-09 22:02:35.881284: step 28940, loss = 0.76 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:02:37.182424: step 28950, loss = 0.80 (983.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:02:38.480131: step 28960, loss = 1.05 (986.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:02:39.809698: step 28970, loss = 0.81 (962.7 examples/sec; 0.133 sec/batch)
2017-05-09 22:02:41.109130: step 28980, loss = 0.94 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:02:42.405492: step 28990, loss = 1.05 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:02:43.795190: step 29000, loss = 0.66 (921.1 examples/sec; 0.139 sec/batch)
2017-05-09 22:02:45.016714: step 29010, loss = 0.65 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-09 22:02:46.341864: step 29020, loss = 0.91 (965.9 examples/sec; 0.133 sec/batch)
2017-05-09 22:02:47.635266: step 29030, loss = 1.07 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:02:48.978768: step 29040, loss = 0.66 (952.7 examples/sec; 0.134 sec/batch)
2017-05-09 22:02:50.282251: step 29050, loss = 0.77 (982.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:02:51.598330: step 29060, loss = 0.80 (972.6 examples/sec; 0.132 sec/batch)
2017-05-09 22:02:52.873650: step 29070, loss = 0.89 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:02:54.218659: step 29080, loss = 1.03 (951.7 examples/sec; 0.135 sec/batch)
2017-05-09 22:02:55.550293: step 29090, loss = 1.10 (961.2 examples/sec; 0.133 sec/batch)
2017-05-09 22:02:56.948858: step 29100, loss = 0.83 (915.2 examples/sec; 0.140 sec/batch)
2017-05-09 22:02:58.149052: step 29110, loss = 0.81 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-09 22:02:59.420279: step 29120, loss = 0.68 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:03:00.744532: step 29130, loss = 0.80 (966.6 examples/sec; 0.132 sec/batch)
2017-05-09 22:03:02.072709: step 29140, loss = 1.25 (963.7 examples/sec; 0.133 sec/batch)
2017-05-09 22:03:03.497809: step 29150, loss = 0.69 (898.2 examples/sec; 0.143 sec/batch)
2017-05-09 22:03:04.835171: step 29160, loss = 1.00 (957.1 examples/sec; 0.134 sec/batch)
2017-05-09 22:03:06.160496: step 29170, loss = 0.73 (965.8 examples/sec; 0.133 sec/batch)
2017-05-09 22:03:07.439883: step 29180, loss = 0.88 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:03:08.734924: step 29190, loss = 0.80 (988.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:03:10.128624: step 29200, loss = 0.70 (918.4 examples/sec; 0.139 sec/batch)
2017-05-09 22:03:11.315125: step 29210, loss = 0.72 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-09 22:03:12.597727: step 29220, loss = 0.79 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:03:13.894207: step 29230, loss = 1.10 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:03:15.197522: step 29240, loss = 0.75 (982.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:03:16.465762: step 29250, loss = 0.79 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:03:17.767955: step 29260, loss = 0.87 (982.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:03:19.065673: step 29270, loss = 0.90 (986.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:03:20.384642: step 29280, loss = 0.84 (970.4 examples/sec; 0.132 sec/batch)
2017-05-09 22:03:21.692612: step 29290, loss = 0.85 (978.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:03:23.091012: step 29300, loss = 0.76 (915.3 examples/sec; 0.140 sec/batch)
2017-05-09 22:03:24.264505: step 29310, loss = 0.92 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-09 22:03:25.532830: step 29320, loss = 0.75 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:03:26.815832: step 29330, loss = 0.85 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:03:28.096744: step 29340, loss = 0.86 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:03:29.403996: step 29350, loss = 0.88 (979.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:03:30.682342: step 29360, loss = 0.84 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:03:31.969990: step 29370, loss = 0.93 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:03:33.281912: step 29380, loss = 1.01 (975.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:03:34.596890: step 29390, loss = 1.03 (973.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:03:36.012730: step 29400, loss = 0.90 (904.1 examples/sec; 0.142 sec/batch)
2017-05-09 22:03:37.211626: step 29410, loss = 0.79 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-09 22:03:38.506842: step 29420, loss = 0.87 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:03:39.794114: step 29430, loss = 0.94 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:03:41.083795: step 29440, loss = 0.67 (992.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:03:42.405280: step 29450, loss = 0.88 (968.6 examples/sec; 0.132 sec/batch)
2017-05-09 22:03:43.691985: step 29460, loss = 0.84 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:03:45.001536: step 29470, loss = 0.76 (977.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:03:46.314864: step 29480, loss = 0.79 (974.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:03:47.618359: step 29490, loss = 0.93 (982.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:03:49.019682: step 29500, loss = 0.74 (913.4 examples/sec; 0.140 sec/batch)
2017-05-09 22:03:50.184716: step 29510, loss = 0.92 (1098.7 examples/sec; 0.117 sec/batch)
2017-05-09 22:03:51.482802: step 29520, loss = 0.80 (986.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:03:52.786522: step 29530, loss = 0.86 (981.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:03:54.050420: step 29540, loss = 0.70 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-09 22:03:55.362212: step 29550, loss = 0.81 (975.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:03:56.695667: step 29560, loss = 0.77 (959.9 examples/sec; 0.133 sec/batch)
2017-05-09 22:03:57.980769: step 29570, loss = 1.03 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:03:59.268873: step 29580, loss = 0.78 (993.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:04:00.561537: step 29590, loss = 0.71 (990.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:04:01.945744: step 29600, loss = 0.74 (924.7 examples/sec; 0.138 sec/batch)
2017-05-09 22:04:03.147689: step 29610, loss = 0.71 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-09 22E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 605 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
:04:04.431741: step 29620, loss = 0.77 (996.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:04:05.722136: step 29630, loss = 0.85 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:04:07.023624: step 29640, loss = 0.91 (983.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:04:08.291063: step 29650, loss = 0.95 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:04:09.583935: step 29660, loss = 0.84 (990.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:04:10.863216: step 29670, loss = 0.78 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:04:12.164689: step 29680, loss = 0.81 (983.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:04:13.480267: step 29690, loss = 0.82 (973.0 examples/sec; 0.132 sec/batch)
2017-05-09 22:04:14.860192: step 29700, loss = 0.77 (927.6 examples/sec; 0.138 sec/batch)
2017-05-09 22:04:16.045421: step 29710, loss = 0.80 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-09 22:04:17.340647: step 29720, loss = 0.80 (988.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:04:18.640880: step 29730, loss = 0.81 (984.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:04:19.939751: step 29740, loss = 0.71 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:04:21.239520: step 29750, loss = 0.85 (984.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:04:22.516066: step 29760, loss = 0.84 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:04:23.834948: step 29770, loss = 0.98 (970.5 examples/sec; 0.132 sec/batch)
2017-05-09 22:04:25.150927: step 29780, loss = 0.84 (972.7 examples/sec; 0.132 sec/batch)
2017-05-09 22:04:26.441783: step 29790, loss = 0.74 (991.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:04:27.816852: step 29800, loss = 0.66 (930.9 examples/sec; 0.138 sec/batch)
2017-05-09 22:04:28.997505: step 29810, loss = 0.86 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-09 22:04:30.311547: step 29820, loss = 0.79 (974.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:04:31.602081: step 29830, loss = 0.90 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:04:32.903282: step 29840, loss = 0.73 (983.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:04:34.180418: step 29850, loss = 0.80 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:04:35.484586: step 29860, loss = 0.87 (981.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:04:36.765334: step 29870, loss = 0.90 (999.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:04:38.063287: step 29880, loss = 0.88 (986.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:04:39.362348: step 29890, loss = 1.01 (985.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:04:40.754876: step 29900, loss = 0.98 (919.2 examples/sec; 0.139 sec/batch)
2017-05-09 22:04:41.959421: step 29910, loss = 0.58 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-09 22:04:43.281780: step 29920, loss = 0.74 (968.0 examples/sec; 0.132 sec/batch)
2017-05-09 22:04:44.538112: step 29930, loss = 0.88 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-09 22:04:45.834082: step 29940, loss = 0.80 (987.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:04:47.139776: step 29950, loss = 0.77 (980.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:04:48.446635: step 29960, loss = 0.93 (979.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:04:49.720402: step 29970, loss = 0.69 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:04:51.015414: step 29980, loss = 0.80 (988.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:04:52.304563: step 29990, loss = 0.73 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:04:53.718426: step 30000, loss = 0.86 (905.3 examples/sec; 0.141 sec/batch)
2017-05-09 22:04:54.911655: step 30010, loss = 1.01 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-09 22:04:56.193670: step 30020, loss = 0.77 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:04:57.519324: step 30030, loss = 0.83 (965.6 examples/sec; 0.133 sec/batch)
2017-05-09 22:04:58.813524: step 30040, loss = 0.84 (989.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:05:00.109288: step 30050, loss = 0.83 (987.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:05:01.386548: step 30060, loss = 0.67 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:05:02.666385: step 30070, loss = 0.77 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:05:03.936155: step 30080, loss = 0.78 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:05:05.215629: step 30090, loss = 0.94 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:05:06.640226: step 30100, loss = 0.86 (898.5 examples/sec; 0.142 sec/batch)
2017-05-09 22:05:07.799758: step 30110, loss = 0.71 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-09 22:05:09.093783: step 30120, loss = 0.66 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:05:10.371859: step 30130, loss = 0.80 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:05:11.661964: step 30140, loss = 0.82 (992.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:05:12.942791: step 30150, loss = 0.88 (999.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:05:14.236940: step 30160, loss = 0.77 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:05:15.536383: step 30170, loss = 0.87 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:05:16.870877: step 30180, loss = 0.82 (959.2 examples/sec; 0.133 sec/batch)
2017-05-09 22:05:18.202517: step 30190, loss = 0.87 (961.2 examples/sec; 0.133 sec/batch)
2017-05-09 22:05:19.577826: step 30200, loss = 0.80 (930.7 examples/sec; 0.138 sec/batch)
2017-05-09 22:05:20.801392: step 30210, loss = 0.86 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-09 22:05:22.117686: step 30220, loss = 0.82 (972.4 examples/sec; 0.132 sec/batch)
2017-05-09 22:05:23.423026: step 30230, loss = 0.71 (980.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:05:24.696360: step 30240, loss = 0.74 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:05:25.984433: step 30250, loss = 0.86 (993.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:05:27.259888: step 30260, loss = 0.76 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:05:28.544036: step 30270, loss = 0.63 (996.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:05:29.821808: step 30280, loss = 0.89 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:05:31.111147: step 30290, loss = 0.93 (992.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:05:32.522233: step 30300, loss = 0.90 (907.1 examples/sec; 0.141 sec/batch)
2017-05-09 22:05:33.656330: step 30310, loss = 0.80 (1128.6 examples/sec; 0.113 sec/batch)
2017-05-09 22:05:34.964310: step 30320, loss = 0.82 (978.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:05:36.274189: step 30330, loss = 0.85 (977.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:05:37.567698: step 30340, loss = 0.77 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:05:38.880035: step 30350, loss = 0.94 (975.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:05:40.177099: step 30360, loss = 0.89 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:05:41.471920: step 30370, loss = 0.78 (988.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:05:42.770186: step 30380, loss = 0.88 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:05:44.067311: step 30390, loss = 0.81 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:05:45.479916: step 30400, loss = 0.99 (906.1 examples/sec; 0.141 sec/batch)
2017-05-09 22:05:46.699389: step 30410, loss = 0.90 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-09 22:05:48.001091: step 30420, loss = 0.92 (983.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:05:49.290205: step 30430, loss = 0.90 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:05:50.580711: step 30440, loss = 0.77 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:05:51.854799: step 30450, loss = 0.78 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:05:53.139335: step 30460, loss = 0.78 (996.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:05:54.430952: step 30470, loss = 0.96 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:05:55.719585: step 30480, loss = 0.67 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:05:57.012137: step 30490, loss = 0.55 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:05:58.412462: step 30500, loss = 0.79 (914.1 examples/sec; 0.140 sec/batch)
2017-05-09 22:05:59.631305: step 30510, loss = 0.92 (1050.2 examples/sec; 0.122 sec/batch)
2017-05-09 22:06:00.920305: step 30520, loss = 0.81 (993.0 examples/sec; 0.129 seE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 625 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
c/batch)
2017-05-09 22:06:02.200720: step 30530, loss = 0.83 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:06:03.481598: step 30540, loss = 0.95 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:06:04.770737: step 30550, loss = 0.98 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:06:06.069523: step 30560, loss = 0.78 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:06:07.344638: step 30570, loss = 0.82 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:06:08.609138: step 30580, loss = 0.70 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-09 22:06:09.915313: step 30590, loss = 0.78 (980.0 examples/sec; 0.131 sec/batch)
2017-05-09 22:06:11.298777: step 30600, loss = 0.79 (925.2 examples/sec; 0.138 sec/batch)
2017-05-09 22:06:12.530180: step 30610, loss = 0.70 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-09 22:06:13.827059: step 30620, loss = 0.96 (987.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:06:15.114431: step 30630, loss = 0.80 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:06:16.420586: step 30640, loss = 0.74 (980.0 examples/sec; 0.131 sec/batch)
2017-05-09 22:06:17.714066: step 30650, loss = 0.78 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:06:18.988683: step 30660, loss = 0.92 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:06:20.277493: step 30670, loss = 0.94 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:06:21.588946: step 30680, loss = 0.77 (976.0 examples/sec; 0.131 sec/batch)
2017-05-09 22:06:22.907861: step 30690, loss = 0.73 (970.5 examples/sec; 0.132 sec/batch)
2017-05-09 22:06:24.274072: step 30700, loss = 0.88 (936.9 examples/sec; 0.137 sec/batch)
2017-05-09 22:06:25.481693: step 30710, loss = 0.85 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-09 22:06:26.776573: step 30720, loss = 0.74 (988.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:06:28.077122: step 30730, loss = 0.74 (984.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:06:29.360514: step 30740, loss = 0.61 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:06:30.682152: step 30750, loss = 0.81 (968.5 examples/sec; 0.132 sec/batch)
2017-05-09 22:06:31.959264: step 30760, loss = 0.78 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:06:33.253368: step 30770, loss = 0.83 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:06:34.546314: step 30780, loss = 0.88 (990.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:06:35.833996: step 30790, loss = 0.65 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:06:37.208580: step 30800, loss = 0.79 (931.2 examples/sec; 0.137 sec/batch)
2017-05-09 22:06:38.405905: step 30810, loss = 0.71 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-09 22:06:39.696878: step 30820, loss = 0.81 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:06:41.015128: step 30830, loss = 0.81 (971.0 examples/sec; 0.132 sec/batch)
2017-05-09 22:06:42.284436: step 30840, loss = 0.81 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:06:43.609675: step 30850, loss = 0.75 (965.9 examples/sec; 0.133 sec/batch)
2017-05-09 22:06:44.878542: step 30860, loss = 0.71 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:06:46.162418: step 30870, loss = 0.84 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:06:47.468522: step 30880, loss = 0.81 (980.0 examples/sec; 0.131 sec/batch)
2017-05-09 22:06:48.736122: step 30890, loss = 0.79 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:06:50.122039: step 30900, loss = 0.77 (923.6 examples/sec; 0.139 sec/batch)
2017-05-09 22:06:51.306823: step 30910, loss = 0.73 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-09 22:06:52.592551: step 30920, loss = 0.76 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:06:53.876221: step 30930, loss = 0.82 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:06:55.170827: step 30940, loss = 0.93 (988.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:06:56.448884: step 30950, loss = 0.76 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:06:57.722184: step 30960, loss = 0.82 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:06:58.995085: step 30970, loss = 0.77 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:07:00.299318: step 30980, loss = 0.99 (981.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:07:01.577767: step 30990, loss = 0.75 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:07:02.971313: step 31000, loss = 0.95 (918.5 examples/sec; 0.139 sec/batch)
2017-05-09 22:07:04.162997: step 31010, loss = 1.00 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-09 22:07:05.461530: step 31020, loss = 0.96 (985.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:07:06.752220: step 31030, loss = 0.97 (991.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:07:08.033637: step 31040, loss = 0.89 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:07:09.335326: step 31050, loss = 0.61 (983.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:07:10.618931: step 31060, loss = 1.06 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:07:11.923027: step 31070, loss = 0.79 (981.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:07:13.246792: step 31080, loss = 0.84 (966.9 examples/sec; 0.132 sec/batch)
2017-05-09 22:07:14.538056: step 31090, loss = 0.81 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:07:15.941403: step 31100, loss = 0.71 (912.1 examples/sec; 0.140 sec/batch)
2017-05-09 22:07:17.120322: step 31110, loss = 0.72 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-09 22:07:18.416902: step 31120, loss = 0.81 (987.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:07:19.715945: step 31130, loss = 0.74 (985.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:07:21.039306: step 31140, loss = 0.89 (967.2 examples/sec; 0.132 sec/batch)
2017-05-09 22:07:22.368825: step 31150, loss = 0.91 (962.8 examples/sec; 0.133 sec/batch)
2017-05-09 22:07:23.676545: step 31160, loss = 1.04 (978.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:07:24.987256: step 31170, loss = 0.80 (976.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:07:26.292290: step 31180, loss = 0.96 (980.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:07:27.602488: step 31190, loss = 0.77 (977.0 examples/sec; 0.131 sec/batch)
2017-05-09 22:07:29.004586: step 31200, loss = 0.95 (912.9 examples/sec; 0.140 sec/batch)
2017-05-09 22:07:30.204590: step 31210, loss = 0.85 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-09 22:07:31.488272: step 31220, loss = 0.82 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:07:32.752627: step 31230, loss = 0.71 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-09 22:07:34.054963: step 31240, loss = 0.82 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:07:35.348588: step 31250, loss = 0.85 (989.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:07:36.659611: step 31260, loss = 0.78 (976.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:07:37.956624: step 31270, loss = 0.84 (986.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:07:39.274668: step 31280, loss = 0.79 (971.1 examples/sec; 0.132 sec/batch)
2017-05-09 22:07:40.561786: step 31290, loss = 0.98 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:07:41.961355: step 31300, loss = 0.94 (914.6 examples/sec; 0.140 sec/batch)
2017-05-09 22:07:43.146113: step 31310, loss = 0.92 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-09 22:07:44.428578: step 31320, loss = 0.83 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:07:45.718280: step 31330, loss = 0.73 (992.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:07:47.021936: step 31340, loss = 0.91 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:07:48.335821: step 31350, loss = 0.74 (974.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:07:49.648107: step 31360, loss = 0.74 (975.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:07:50.929090: step 31370, loss = 0.74 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:07:52.240199: step 31380, loss = 0.88 (976.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:07:53.523838: step 31390, loss = 0.81 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:07:54.892366: step 31400, loss = 0.96 (935.3 examples/sec; 0.137 sec/batch)
2017-05-09 22:07:56.106212: step 31410, loss = 0.76 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-09 22:07:57.403992: step 31420, loss = 0.83 (986.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:07:58.709358: step 31430, loss = 0.96 (980.6 exaE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 645 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
mples/sec; 0.131 sec/batch)
2017-05-09 22:08:00.003148: step 31440, loss = 0.77 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:08:01.296028: step 31450, loss = 0.89 (990.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:08:02.577922: step 31460, loss = 0.78 (998.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:08:03.861128: step 31470, loss = 0.87 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:08:05.168065: step 31480, loss = 0.83 (979.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:08:06.501904: step 31490, loss = 0.78 (959.6 examples/sec; 0.133 sec/batch)
2017-05-09 22:08:07.882557: step 31500, loss = 0.85 (927.1 examples/sec; 0.138 sec/batch)
2017-05-09 22:08:09.101704: step 31510, loss = 0.78 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-09 22:08:10.364984: step 31520, loss = 0.88 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-09 22:08:11.646967: step 31530, loss = 0.91 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:08:12.924741: step 31540, loss = 0.77 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:08:14.256119: step 31550, loss = 0.94 (961.4 examples/sec; 0.133 sec/batch)
2017-05-09 22:08:15.563330: step 31560, loss = 0.82 (979.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:08:16.856520: step 31570, loss = 0.80 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:08:18.150754: step 31580, loss = 0.79 (989.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:08:19.432150: step 31590, loss = 0.77 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:08:20.806200: step 31600, loss = 0.69 (931.6 examples/sec; 0.137 sec/batch)
2017-05-09 22:08:22.012423: step 31610, loss = 0.78 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-09 22:08:23.307730: step 31620, loss = 0.90 (988.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:08:24.585877: step 31630, loss = 0.71 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:08:25.883964: step 31640, loss = 0.73 (986.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:08:27.150855: step 31650, loss = 0.90 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:08:28.431611: step 31660, loss = 0.72 (999.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:08:29.719375: step 31670, loss = 0.85 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:08:31.026177: step 31680, loss = 0.85 (979.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:08:32.343419: step 31690, loss = 0.71 (971.7 examples/sec; 0.132 sec/batch)
2017-05-09 22:08:33.755737: step 31700, loss = 0.82 (906.3 examples/sec; 0.141 sec/batch)
2017-05-09 22:08:34.966875: step 31710, loss = 0.71 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-09 22:08:36.275566: step 31720, loss = 0.78 (978.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:08:37.573754: step 31730, loss = 0.97 (986.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:08:38.874048: step 31740, loss = 0.75 (984.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:08:40.192722: step 31750, loss = 1.02 (970.7 examples/sec; 0.132 sec/batch)
2017-05-09 22:08:41.479229: step 31760, loss = 0.92 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:08:42.772216: step 31770, loss = 0.71 (990.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:08:44.105398: step 31780, loss = 0.87 (960.1 examples/sec; 0.133 sec/batch)
2017-05-09 22:08:45.407762: step 31790, loss = 1.04 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:08:46.786695: step 31800, loss = 0.99 (928.3 examples/sec; 0.138 sec/batch)
2017-05-09 22:08:47.972533: step 31810, loss = 0.84 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-09 22:08:49.252986: step 31820, loss = 0.86 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:08:50.564336: step 31830, loss = 0.94 (976.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:08:51.889981: step 31840, loss = 0.88 (965.6 examples/sec; 0.133 sec/batch)
2017-05-09 22:08:53.211760: step 31850, loss = 0.71 (968.4 examples/sec; 0.132 sec/batch)
2017-05-09 22:08:54.525731: step 31860, loss = 0.75 (974.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:08:55.819661: step 31870, loss = 0.79 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:08:57.118742: step 31880, loss = 0.91 (985.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:08:58.428295: step 31890, loss = 0.98 (977.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:08:59.787091: step 31900, loss = 0.91 (942.0 examples/sec; 0.136 sec/batch)
2017-05-09 22:09:00.976473: step 31910, loss = 0.94 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-09 22:09:02.303535: step 31920, loss = 0.80 (964.5 examples/sec; 0.133 sec/batch)
2017-05-09 22:09:03.576544: step 31930, loss = 0.86 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:09:04.858703: step 31940, loss = 0.78 (998.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:09:06.165990: step 31950, loss = 0.74 (979.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:09:07.452483: step 31960, loss = 0.87 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:09:08.743383: step 31970, loss = 0.78 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:09:10.025512: step 31980, loss = 0.72 (998.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:09:11.319940: step 31990, loss = 0.69 (988.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:09:12.713996: step 32000, loss = 0.80 (918.2 examples/sec; 0.139 sec/batch)
2017-05-09 22:09:13.931504: step 32010, loss = 0.88 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-09 22:09:15.252388: step 32020, loss = 0.80 (969.1 examples/sec; 0.132 sec/batch)
2017-05-09 22:09:16.533760: step 32030, loss = 0.87 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:09:17.829396: step 32040, loss = 0.90 (987.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:09:19.115870: step 32050, loss = 0.73 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:09:20.440568: step 32060, loss = 0.93 (966.3 examples/sec; 0.132 sec/batch)
2017-05-09 22:09:21.727837: step 32070, loss = 0.74 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:09:23.028282: step 32080, loss = 0.81 (984.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:09:24.286612: step 32090, loss = 0.79 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-09 22:09:25.688543: step 32100, loss = 0.76 (913.0 examples/sec; 0.140 sec/batch)
2017-05-09 22:09:26.927725: step 32110, loss = 0.89 (1032.9 examples/sec; 0.124 sec/batch)
2017-05-09 22:09:28.238986: step 32120, loss = 0.90 (976.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:09:29.525833: step 32130, loss = 0.77 (994.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:09:30.788329: step 32140, loss = 0.77 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-09 22:09:32.081121: step 32150, loss = 0.91 (990.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:09:33.389171: step 32160, loss = 0.72 (978.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:09:34.675811: step 32170, loss = 0.72 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:09:35.971908: step 32180, loss = 0.61 (987.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:09:37.290007: step 32190, loss = 0.90 (971.1 examples/sec; 0.132 sec/batch)
2017-05-09 22:09:38.685965: step 32200, loss = 0.93 (916.9 examples/sec; 0.140 sec/batch)
2017-05-09 22:09:39.921052: step 32210, loss = 0.85 (1036.4 examples/sec; 0.124 sec/batch)
2017-05-09 22:09:41.221647: step 32220, loss = 0.82 (984.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:09:42.536320: step 32230, loss = 0.85 (973.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:09:43.819589: step 32240, loss = 0.89 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:09:45.108704: step 32250, loss = 0.86 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:09:46.433433: step 32260, loss = 1.00 (966.2 examples/sec; 0.132 sec/batch)
2017-05-09 22:09:47.732114: step 32270, loss = 0.93 (985.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:09:49.052255: step 32280, loss = 0.78 (969.6 examples/sec; 0.132 sec/batch)
2017-05-09 22:09:50.341953: step 32290, loss = 0.80 (992.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:09:51.774011: step 32300, loss = 0.85 (893.8 examples/sec; 0.143 sec/batch)
2017-05-09 22:09:52.926403: step 32310, loss = 0.84 (1110.7 examples/sec; 0.115 sec/batch)
2017-05-09 22:09:54.209533: step 32320, loss = 0.72 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:09:55.497247: step 32330, loss = 0.81 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:09:56.790661: step 32340, loss = 0E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 666 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
.83 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:09:58.084769: step 32350, loss = 0.73 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:09:59.365493: step 32360, loss = 0.99 (999.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:10:00.656667: step 32370, loss = 0.75 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:10:01.967202: step 32380, loss = 0.57 (976.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:10:03.261389: step 32390, loss = 0.75 (989.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:10:04.640404: step 32400, loss = 0.90 (928.2 examples/sec; 0.138 sec/batch)
2017-05-09 22:10:05.833615: step 32410, loss = 0.86 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-09 22:10:07.108583: step 32420, loss = 0.68 (1003.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:10:08.371680: step 32430, loss = 0.98 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-09 22:10:09.657680: step 32440, loss = 0.65 (995.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:10:10.944042: step 32450, loss = 0.85 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:10:12.217157: step 32460, loss = 0.80 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:10:13.680969: step 32470, loss = 0.77 (874.4 examples/sec; 0.146 sec/batch)
2017-05-09 22:10:15.024760: step 32480, loss = 0.87 (952.5 examples/sec; 0.134 sec/batch)
2017-05-09 22:10:16.351680: step 32490, loss = 0.88 (964.6 examples/sec; 0.133 sec/batch)
2017-05-09 22:10:17.725158: step 32500, loss = 0.69 (931.9 examples/sec; 0.137 sec/batch)
2017-05-09 22:10:18.899701: step 32510, loss = 0.69 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-09 22:10:20.192578: step 32520, loss = 0.72 (990.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:10:21.496545: step 32530, loss = 0.73 (981.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:10:22.767676: step 32540, loss = 0.81 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:10:24.071128: step 32550, loss = 0.70 (982.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:10:25.358431: step 32560, loss = 0.95 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:10:26.667233: step 32570, loss = 0.92 (978.0 examples/sec; 0.131 sec/batch)
2017-05-09 22:10:27.969162: step 32580, loss = 0.73 (983.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:10:29.316285: step 32590, loss = 0.90 (950.2 examples/sec; 0.135 sec/batch)
2017-05-09 22:10:30.728314: step 32600, loss = 0.77 (906.5 examples/sec; 0.141 sec/batch)
2017-05-09 22:10:31.897779: step 32610, loss = 0.77 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-09 22:10:33.173484: step 32620, loss = 0.72 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:10:34.457079: step 32630, loss = 0.80 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:10:35.754059: step 32640, loss = 0.90 (986.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:10:37.043108: step 32650, loss = 0.91 (993.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:10:38.325150: step 32660, loss = 0.68 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:10:39.611409: step 32670, loss = 0.71 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:10:40.883711: step 32680, loss = 0.73 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:10:42.187507: step 32690, loss = 0.75 (981.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:10:43.573141: step 32700, loss = 0.78 (923.8 examples/sec; 0.139 sec/batch)
2017-05-09 22:10:44.770691: step 32710, loss = 0.93 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-09 22:10:46.051666: step 32720, loss = 0.87 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:10:47.332011: step 32730, loss = 0.75 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:10:48.631542: step 32740, loss = 0.73 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:10:49.947617: step 32750, loss = 0.75 (972.6 examples/sec; 0.132 sec/batch)
2017-05-09 22:10:51.258993: step 32760, loss = 1.01 (976.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:10:52.549205: step 32770, loss = 0.68 (992.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:10:53.842594: step 32780, loss = 0.89 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:10:55.117402: step 32790, loss = 0.80 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:10:56.500655: step 32800, loss = 0.73 (925.4 examples/sec; 0.138 sec/batch)
2017-05-09 22:10:57.691562: step 32810, loss = 0.76 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-09 22:10:58.966305: step 32820, loss = 0.85 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:11:00.247130: step 32830, loss = 0.78 (999.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:11:01.567759: step 32840, loss = 0.93 (969.2 examples/sec; 0.132 sec/batch)
2017-05-09 22:11:02.870216: step 32850, loss = 0.85 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:11:04.154408: step 32860, loss = 0.87 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:11:05.437032: step 32870, loss = 0.78 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:11:06.731396: step 32880, loss = 1.16 (988.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:11:08.003230: step 32890, loss = 0.89 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:11:09.424317: step 32900, loss = 0.98 (900.7 examples/sec; 0.142 sec/batch)
2017-05-09 22:11:10.634184: step 32910, loss = 0.81 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-09 22:11:11.916949: step 32920, loss = 0.76 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:11:13.229093: step 32930, loss = 1.05 (975.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:11:14.508787: step 32940, loss = 0.71 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:11:15.795838: step 32950, loss = 0.85 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:11:17.101618: step 32960, loss = 0.68 (980.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:11:18.399571: step 32970, loss = 0.83 (986.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:11:19.688832: step 32980, loss = 0.75 (992.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:11:20.970293: step 32990, loss = 0.85 (998.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:11:22.347834: step 33000, loss = 0.79 (929.2 examples/sec; 0.138 sec/batch)
2017-05-09 22:11:23.534300: step 33010, loss = 0.85 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-09 22:11:24.807495: step 33020, loss = 0.83 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:11:26.102950: step 33030, loss = 1.04 (988.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:11:27.380102: step 33040, loss = 0.73 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:11:28.673655: step 33050, loss = 0.66 (989.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:11:29.962419: step 33060, loss = 0.80 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:11:31.253642: step 33070, loss = 0.65 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:11:32.541583: step 33080, loss = 0.68 (993.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:11:33.859750: step 33090, loss = 0.85 (971.0 examples/sec; 0.132 sec/batch)
2017-05-09 22:11:35.291166: step 33100, loss = 1.59 (894.2 examples/sec; 0.143 sec/batch)
2017-05-09 22:11:36.498328: step 33110, loss = 0.69 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-09 22:11:37.748093: step 33120, loss = 0.82 (1024.2 examples/sec; 0.125 sec/batch)
2017-05-09 22:11:39.072598: step 33130, loss = 0.99 (966.4 examples/sec; 0.132 sec/batch)
2017-05-09 22:11:40.374760: step 33140, loss = 0.86 (983.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:11:41.667852: step 33150, loss = 0.72 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:11:42.991667: step 33160, loss = 1.08 (966.9 examples/sec; 0.132 sec/batch)
2017-05-09 22:11:44.286415: step 33170, loss = 0.74 (988.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:11:45.580433: step 33180, loss = 0.66 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:11:46.878588: step 33190, loss = 0.88 (986.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:11:48.271317: step 33200, loss = 1.10 (919.1 examples/sec; 0.139 sec/batch)
2017-05-09 22:11:49.470415: step 33210, loss = 0.83 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-09 22:11:50.752429: step 33220, loss = 0.80 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:11:52.057313: step 33230, loss = 0.98 (980.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:11:53.371301: step 33240, loss = 0.79 (974.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:11:54.652792: step 33250, loss = 0.78 (998.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:11:55.963038: step 33260, loss = 0.82 (976.9 examples/sec; 0.131 sec/batch)
2017-05-09 22:11:57.278602: step 33270, loss = 0.79 (973.0 examples/sec; 0.132 sec/batch)
2017-05-09 22:11:58.578705: step 33280, loss = 0.79 (984.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:11:59.866959: step 33290, loss = 1.05 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:12:01.297673: step 33300, loss = 0.78 (894.7 examples/sec; 0.143 sec/batch)
2017-05-09 22:12:02.506434: step 33310, loss = 0.72 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-09 22:12:03.801111: step 33320, loss = 0.83 (988.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:12:05.097552: step 33330, loss = 0.65 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:12:06.371184: step 33340, loss = 0.86 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:12:07.637219: step 33350, loss = 0.83 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:12:08.929005: step 33360, loss = 0.88 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:12:10.212340: step 33370, loss = 0.84 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:12:11.503290: step 33380, loss = 0.76 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:12:12.818422: step 33390, loss = 0.75 (973.3 examples/sec; 0.132 sec/batch)
2017-05-09 22:12:14.194110: step 33400, loss = 0.90 (930.4 examples/sec; 0.138 sec/batch)
2017-05-09 22:12:15.393987: step 33410, loss = 0.83 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-09 22:12:16.708146: step 33420, loss = 1.08 (974.0 examples/sec; 0.131 sec/batch)
2017-05-09 22:12:18.015662: step 33430, loss = 0.75 (979.0 examples/sec; 0.131 sec/batch)
2017-05-09 22:12:19.312942: step 33440, loss = 0.75 (986.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:12:20.590556: step 33450, loss = 0.90 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:12:21.881566: step 33460, loss = 0.86 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:12:23.163833: step 33470, loss = 0.90 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:12:24.433751: step 33480, loss = 0.70 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:12:25.712357: step 33490, loss = 0.80 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:12:27.136849: step 33500, loss = 0.82 (898.6 examples/sec; 0.142 sec/batch)
2017-05-09 22:12:28.293503: step 33510, loss = 0.75 (1106.6 examples/sec; 0.116 sec/batch)
2017-05-09 22:12:29.589904: step 33520, loss = 0.80 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:12:30.884920: step 33530, loss = 0.66 (988.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:12:32.189830: step 33540, loss = 0.82 (980.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:12:33.478578: step 33550, loss = 0.92 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:12:34.814981: step 33560, loss = 0.88 (957.8 examples/sec; 0.134 sec/batch)
2017-05-09 22:12:36.106838: step 33570, loss = 0.73 (990.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:12:37.387069: step 33580, loss = 0.85 (999.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:12:38.707924: step 33590, loss = 0.84 (969.1 examples/sec; 0.132 sec/batch)
2017-05-09 22:12:40.096506: step 33600, loss = 0.65 (921.8 examples/sec; 0.139 sec/batch)
2017-05-09 22:12:41.285409: step 33610, loss = 0.85 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-09 22:12:42.569909: step 33620, loss = 0.74 (996.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:12:43.846625: step 33630, loss = 0.84 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:12:45.123532: step 33640, loss = 0.80 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:12:46.428560: step 33650, loss = 0.85 (980.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:12:47.713074: step 33660, loss = 0.78 (996.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:12:49.011035: step 33670, loss = 0.76 (986.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:12:50.294853: step 33680, loss = 0.60 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:12:51.606722: step 33690, loss = 0.76 (975.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:12:52.994843: step 33700, loss = 0.86 (922.1 examE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 686 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
ples/sec; 0.139 sec/batch)
2017-05-09 22:12:54.229547: step 33710, loss = 0.75 (1036.7 examples/sec; 0.123 sec/batch)
2017-05-09 22:12:55.524641: step 33720, loss = 0.89 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:12:56.805978: step 33730, loss = 0.72 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:12:58.084281: step 33740, loss = 0.69 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:12:59.373165: step 33750, loss = 0.82 (993.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:13:00.656741: step 33760, loss = 0.80 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:13:01.944219: step 33770, loss = 0.63 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:13:03.235970: step 33780, loss = 0.73 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:13:04.525828: step 33790, loss = 0.85 (992.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:13:05.923490: step 33800, loss = 0.83 (915.8 examples/sec; 0.140 sec/batch)
2017-05-09 22:13:07.154369: step 33810, loss = 0.71 (1039.9 examples/sec; 0.123 sec/batch)
2017-05-09 22:13:08.439764: step 33820, loss = 0.74 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:13:09.736652: step 33830, loss = 0.80 (987.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:13:11.012208: step 33840, loss = 0.81 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:13:12.300602: step 33850, loss = 0.79 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:13:13.581268: step 33860, loss = 0.80 (999.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:13:14.898039: step 33870, loss = 0.82 (972.1 examples/sec; 0.132 sec/batch)
2017-05-09 22:13:16.174104: step 33880, loss = 0.91 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:13:17.484037: step 33890, loss = 0.86 (977.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:13:18.887654: step 33900, loss = 1.10 (911.9 examples/sec; 0.140 sec/batch)
2017-05-09 22:13:20.126850: step 33910, loss = 0.81 (1032.9 examples/sec; 0.124 sec/batch)
2017-05-09 22:13:21.405533: step 33920, loss = 0.86 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:13:22.685618: step 33930, loss = 0.82 (999.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:13:23.987239: step 33940, loss = 0.69 (983.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:13:25.250745: step 33950, loss = 0.93 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-09 22:13:26.526763: step 33960, loss = 0.85 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:13:27.799837: step 33970, loss = 0.72 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:13:29.086355: step 33980, loss = 0.67 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:13:30.374323: step 33990, loss = 0.71 (993.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:13:31.758816: step 34000, loss = 0.77 (924.5 examples/sec; 0.138 sec/batch)
2017-05-09 22:13:32.971270: step 34010, loss = 0.86 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-09 22:13:34.272264: step 34020, loss = 0.72 (983.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:13:35.541830: step 34030, loss = 0.87 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:13:36.825421: step 34040, loss = 0.79 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:13:38.113283: step 34050, loss = 0.89 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:13:39.391603: step 34060, loss = 0.60 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:13:40.663927: step 34070, loss = 0.71 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:13:41.924871: step 34080, loss = 0.77 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-09 22:13:43.212647: step 34090, loss = 0.77 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:13:44.639908: step 34100, loss = 0.72 (896.8 examples/sec; 0.143 sec/batch)
2017-05-09 22:13:45.807297: step 34110, loss = 0.80 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-09 22:13:47.111323: step 34120, loss = 0.87 (981.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:13:48.380313: step 34130, loss = 0.66 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:13:49.661973: step 34140, loss = 0.82 (998.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:13:50.954883: step 34150, loss = 0.92 (990.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:13:52.237156: step 34160, loss = 0.73 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:13:53.528399: step 34170, loss = 0.86 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:13:54.845276: step 34180, loss = 1.00 (972.0 examples/sec; 0.132 sec/batch)
2017-05-09 22:13:56.156010: step 34190, loss = 0.91 (976.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:13:57.565192: step 34200, loss = 0.89 (908.3 examples/sec; 0.141 sec/batch)
2017-05-09 22:13:58.759832: step 34210, loss = 0.71 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-09 22:14:00.041065: step 34220, loss = 0.59 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:14:01.334583: step 34230, loss = 0.74 (989.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:14:02.628333: step 34240, loss = 0.85 (989.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:14:03.937062: step 34250, loss = 0.65 (978.0 examples/sec; 0.131 sec/batch)
2017-05-09 22:14:05.242689: step 34260, loss = 0.86 (980.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:14:06.550989: step 34270, loss = 0.77 (978.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:14:07.836615: step 34280, loss = 0.79 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:14:09.116913: step 34290, loss = 0.76 (999.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:14:10.496819: step 34300, loss = 0.67 (927.6 examples/sec; 0.138 sec/batch)
2017-05-09 22:14:11.691102: step 34310, loss = 0.71 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-09 22:14:12.953597: step 34320, loss = 0.80 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-09 22:14:14.253330: step 34330, loss = 0.80 (984.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:14:15.539935: step 34340, loss = 0.77 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:14:16.829683: step 34350, loss = 0.72 (992.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:14:18.129146: step 34360, loss = 0.89 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:14:19.423958: step 34370, loss = 0.72 (988.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:14:20.691594: step 34380, loss = 0.71 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:14:21.990066: step 34390, loss = 0.97 (985.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:14:23.368927: step 34400, loss = 0.98 (928.3 examples/sec; 0.138 sec/batch)
2017-05-09 22:14:24.590857: step 34410, loss = 0.90 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-09 22:14:25.884205: step 34420, loss = 0.82 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:14:27.193552: step 34430, loss = 0.80 (977.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:14:28.488625: step 34440, loss = 0.85 (988.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:14:29.775920: step 34450, loss = 0.78 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:14:31.083330: step 34460, loss = 0.85 (979.0 examples/sec; 0.131 sec/batch)
2017-05-09 22:14:32.356070: step 34470, loss = 0.83 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:14:33.653158: step 34480, loss = 0.83 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:14:34.972106: step 34490, loss = 0.83 (970.5 examples/sec; 0.132 sec/batch)
2017-05-09 22:14:36.365894: step 34500, loss = 0.67 (918.4 examples/sec; 0.139 sec/batch)
2017-05-09 22:14:37.592144: step 34510, loss = 0.75 (1043.8 examples/sec; 0.123 sec/batch)
2017-05-09 22:14:38.895613: step 34520, loss = 0.82 (982.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:14:40.195174: step 34530, loss = 0.87 (984.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:14:41.489420: step 34540, loss = 0.63 (989.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:14:42.808355: step 34550, loss = 0.86 (970.5 examples/sec; 0.132 sec/batch)
2017-05-09 22:14:44.125180: step 34560, loss = 0.90 (972.0 examples/sec; 0.132 sec/batch)
2017-05-09 22:14:45.424170: step 34570, loss = 0.79 (985.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:14:46.707570: step 34580, loss = 0.73 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:14:47.992637: step 34590, loss = 0.84 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:14:49.418723: step 34600, loss = 1.01 (897.6 examples/sec; 0.143 sec/batch)
2017-05-09 22:14:50.648295: step 34610, lE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 706 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
oss = 0.67 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-09 22:14:51.934814: step 34620, loss = 0.92 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:14:53.207229: step 34630, loss = 0.71 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:14:54.507755: step 34640, loss = 0.84 (984.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:14:55.820142: step 34650, loss = 0.71 (975.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:14:57.107117: step 34660, loss = 0.98 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:14:58.402252: step 34670, loss = 0.72 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:14:59.691432: step 34680, loss = 0.78 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:15:00.999420: step 34690, loss = 0.85 (978.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:15:02.423428: step 34700, loss = 0.93 (898.9 examples/sec; 0.142 sec/batch)
2017-05-09 22:15:03.614686: step 34710, loss = 0.80 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-09 22:15:04.913335: step 34720, loss = 0.88 (985.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:15:06.183029: step 34730, loss = 0.77 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:15:07.474997: step 34740, loss = 0.61 (990.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:15:08.755961: step 34750, loss = 0.75 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:15:10.047579: step 34760, loss = 0.97 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:15:11.335339: step 34770, loss = 0.81 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:15:12.643936: step 34780, loss = 0.91 (978.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:15:13.928151: step 34790, loss = 0.69 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:15:15.329121: step 34800, loss = 0.71 (913.7 examples/sec; 0.140 sec/batch)
2017-05-09 22:15:16.559727: step 34810, loss = 0.83 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-09 22:15:17.848722: step 34820, loss = 0.93 (993.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:15:19.118210: step 34830, loss = 0.77 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:15:20.435758: step 34840, loss = 0.73 (971.5 examples/sec; 0.132 sec/batch)
2017-05-09 22:15:21.712821: step 34850, loss = 0.82 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:15:23.003173: step 34860, loss = 0.92 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:15:24.271182: step 34870, loss = 0.95 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:15:25.569522: step 34880, loss = 0.98 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:15:26.861090: step 34890, loss = 0.81 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:15:28.233867: step 34900, loss = 0.68 (932.4 examples/sec; 0.137 sec/batch)
2017-05-09 22:15:29.437423: step 34910, loss = 0.66 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-09 22:15:30.724373: step 34920, loss = 0.95 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:15:32.005578: step 34930, loss = 0.86 (999.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:15:33.289748: step 34940, loss = 0.87 (996.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:15:34.600681: step 34950, loss = 0.74 (976.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:15:35.905941: step 34960, loss = 0.88 (980.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:15:37.222106: step 34970, loss = 0.80 (972.5 examples/sec; 0.132 sec/batch)
2017-05-09 22:15:38.500516: step 34980, loss = 0.86 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:15:39.779716: step 34990, loss = 1.07 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:15:41.168916: step 35000, loss = 0.85 (921.4 examples/sec; 0.139 sec/batch)
2017-05-09 22:15:42.409914: step 35010, loss = 0.82 (1031.4 examples/sec; 0.124 sec/batch)
2017-05-09 22:15:43.691536: step 35020, loss = 0.69 (998.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:15:44.975566: step 35030, loss = 0.90 (996.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:15:46.284831: step 35040, loss = 0.83 (977.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:15:47.565095: step 35050, loss = 0.79 (999.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:15:48.853874: step 35060, loss = 0.84 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:15:50.172702: step 35070, loss = 0.98 (970.6 examples/sec; 0.132 sec/batch)
2017-05-09 22:15:51.465319: step 35080, loss = 0.74 (990.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:15:52.771774: step 35090, loss = 0.63 (979.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:15:54.146043: step 35100, loss = 0.84 (931.4 examples/sec; 0.137 sec/batch)
2017-05-09 22:15:55.340874: step 35110, loss = 0.90 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-09 22:15:56.670508: step 35120, loss = 0.88 (962.7 examples/sec; 0.133 sec/batch)
2017-05-09 22:15:57.948739: step 35130, loss = 0.82 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:15:59.246725: step 35140, loss = 0.76 (986.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:16:00.537771: step 35150, loss = 0.79 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:16:01.806099: step 35160, loss = 0.64 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:16:03.076210: step 35170, loss = 0.78 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:16:04.350778: step 35180, loss = 0.88 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:16:05.648177: step 35190, loss = 0.79 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:16:07.051493: step 35200, loss = 0.89 (912.1 examples/sec; 0.140 sec/batch)
2017-05-09 22:16:08.224783: step 35210, loss = 0.60 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-09 22:16:09.507683: step 35220, loss = 0.84 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:16:10.805838: step 35230, loss = 0.82 (986.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:16:12.086047: step 35240, loss = 0.70 (999.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:16:13.362900: step 35250, loss = 0.69 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:16:14.645646: step 35260, loss = 0.85 (997.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:16:15.930246: step 35270, loss = 0.72 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:16:17.224075: step 35280, loss = 0.71 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:16:18.514726: step 35290, loss = 0.84 (991.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:16:19.892793: step 35300, loss = 0.72 (928.8 examples/sec; 0.138 sec/batch)
2017-05-09 22:16:21.091299: step 35310, loss = 0.77 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-09 22:16:22.402026: step 35320, loss = 0.77 (976.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:16:23.685797: step 35330, loss = 0.79 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:16:24.986551: step 35340, loss = 0.84 (984.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:16:26.269459: step 35350, loss = 0.78 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:16:27.537778: step 35360, loss = 0.82 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:16:28.817584: step 35370, loss = 0.95 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:16:30.134285: step 35380, loss = 0.77 (972.1 examples/sec; 0.132 sec/batch)
2017-05-09 22:16:31.440108: step 35390, loss = 0.82 (980.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:16:32.852667: step 35400, loss = 0.69 (906.2 examples/sec; 0.141 sec/batch)
2017-05-09 22:16:34.064463: step 35410, loss = 0.91 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-09 22:16:35.341186: step 35420, loss = 0.73 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:16:36.633381: step 35430, loss = 0.86 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:16:37.937018: step 35440, loss = 0.78 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:16:39.223706: step 35450, loss = 0.69 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:16:40.516413: step 35460, loss = 0.70 (990.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:16:41.822043: step 35470, loss = 0.81 (980.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:16:43.122794: step 35480, loss = 0.77 (984.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:16:44.462392: step 35490, loss = 0.90 (955.5 examples/sec; 0.134 sec/batch)
2017-05-09 22:16:45.840209: step 35500, loss = 0.65 (929.0 examples/sec; 0.138 sec/batch)
2017-05-09 22:16:47.016326: step 35510, loss = 0.76 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-09 22:16:4E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 726 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
8.330761: step 35520, loss = 1.14 (973.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:16:49.618963: step 35530, loss = 0.82 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:16:50.952221: step 35540, loss = 0.87 (960.1 examples/sec; 0.133 sec/batch)
2017-05-09 22:16:52.256402: step 35550, loss = 0.82 (981.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:16:53.567227: step 35560, loss = 0.84 (976.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:16:54.864134: step 35570, loss = 0.76 (987.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:16:56.134022: step 35580, loss = 0.83 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:16:57.435022: step 35590, loss = 0.76 (983.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:16:58.808179: step 35600, loss = 0.94 (932.2 examples/sec; 0.137 sec/batch)
2017-05-09 22:16:59.992216: step 35610, loss = 0.86 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-09 22:17:01.282579: step 35620, loss = 0.69 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:17:02.560307: step 35630, loss = 0.95 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:17:03.849797: step 35640, loss = 0.73 (992.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:17:05.130644: step 35650, loss = 0.82 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:17:06.434497: step 35660, loss = 0.77 (981.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:17:07.728045: step 35670, loss = 0.70 (989.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:17:09.047724: step 35680, loss = 0.72 (969.9 examples/sec; 0.132 sec/batch)
2017-05-09 22:17:10.345172: step 35690, loss = 1.07 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:17:11.744620: step 35700, loss = 0.74 (914.6 examples/sec; 0.140 sec/batch)
2017-05-09 22:17:12.942857: step 35710, loss = 0.77 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-09 22:17:14.219003: step 35720, loss = 0.63 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:17:15.521890: step 35730, loss = 0.93 (982.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:17:16.847858: step 35740, loss = 0.67 (965.3 examples/sec; 0.133 sec/batch)
2017-05-09 22:17:18.145034: step 35750, loss = 0.96 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:17:19.429541: step 35760, loss = 0.84 (996.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:17:20.690910: step 35770, loss = 0.82 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-09 22:17:21.973933: step 35780, loss = 0.68 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:17:23.247359: step 35790, loss = 0.78 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:17:24.637056: step 35800, loss = 0.77 (921.1 examples/sec; 0.139 sec/batch)
2017-05-09 22:17:25.842307: step 35810, loss = 0.78 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-09 22:17:27.119788: step 35820, loss = 0.77 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:17:28.395951: step 35830, loss = 0.85 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:17:29.710430: step 35840, loss = 0.95 (973.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:17:31.002064: step 35850, loss = 0.93 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:17:32.299708: step 35860, loss = 0.89 (986.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:17:33.573658: step 35870, loss = 0.79 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:17:34.885238: step 35880, loss = 0.78 (975.9 examples/sec; 0.131 sec/batch)
2017-05-09 22:17:36.192238: step 35890, loss = 1.11 (979.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:17:37.570472: step 35900, loss = 0.73 (928.7 examples/sec; 0.138 sec/batch)
2017-05-09 22:17:38.777638: step 35910, loss = 0.67 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-09 22:17:40.076820: step 35920, loss = 0.98 (985.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:17:41.368299: step 35930, loss = 0.73 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:17:42.641676: step 35940, loss = 0.66 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:17:43.910198: step 35950, loss = 0.75 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:17:45.192803: step 35960, loss = 1.00 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:17:46.486856: step 35970, loss = 0.75 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:17:47.763286: step 35980, loss = 0.81 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:17:49.058140: step 35990, loss = 0.74 (988.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:17:50.445948: step 36000, loss = 0.86 (922.3 examples/sec; 0.139 sec/batch)
2017-05-09 22:17:51.666935: step 36010, loss = 0.71 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-09 22:17:52.958490: step 36020, loss = 0.77 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:17:54.239539: step 36030, loss = 0.81 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:17:55.536202: step 36040, loss = 0.61 (987.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:17:56.841997: step 36050, loss = 0.75 (980.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:17:58.131729: step 36060, loss = 0.93 (992.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:17:59.418717: step 36070, loss = 0.92 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:18:00.733435: step 36080, loss = 0.72 (973.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:18:02.028534: step 36090, loss = 0.66 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:18:03.404510: step 36100, loss = 0.75 (930.3 examples/sec; 0.138 sec/batch)
2017-05-09 22:18:04.597735: step 36110, loss = 0.73 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-09 22:18:05.894791: step 36120, loss = 0.89 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:18:07.165710: step 36130, loss = 0.90 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:18:08.439776: step 36140, loss = 0.79 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:18:09.735722: step 36150, loss = 0.95 (987.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:18:11.028500: step 36160, loss = 0.77 (990.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:18:12.299705: step 36170, loss = 0.87 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:18:13.595604: step 36180, loss = 0.83 (987.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:18:14.896561: step 36190, loss = 0.67 (983.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:18:16.288979: step 36200, loss = 0.78 (919.3 examples/sec; 0.139 sec/batch)
2017-05-09 22:18:17.485757: step 36210, loss = 0.85 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-09 22:18:18.783207: step 36220, loss = 0.58 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:18:20.130848: step 36230, loss = 0.82 (949.8 examples/sec; 0.135 sec/batch)
2017-05-09 22:18:21.421855: step 36240, loss = 0.74 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:18:22.715335: step 36250, loss = 0.80 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:18:24.010483: step 36260, loss = 0.90 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:18:25.312867: step 36270, loss = 0.78 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:18:26.635676: step 36280, loss = 0.83 (967.6 examples/sec; 0.132 sec/batch)
2017-05-09 22:18:27.915665: step 36290, loss = 0.79 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:18:29.275988: step 36300, loss = 0.80 (941.0 examples/sec; 0.136 sec/batch)
2017-05-09 22:18:30.490431: step 36310, loss = 0.84 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-09 22:18:31.765776: step 36320, loss = 0.81 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:18:33.072076: step 36330, loss = 1.04 (979.9 examples/sec; 0.131 sec/batch)
2017-05-09 22:18:34.365856: step 36340, loss = 0.78 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:18:35.660519: step 36350, loss = 0.81 (988.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:18:36.927471: step 36360, loss = 0.74 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:18:38.216872: step 36370, loss = 1.01 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:18:39.507772: step 36380, loss = 0.85 (991.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:18:40.787005: step 36390, loss = 0.63 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:18:42.192020: step 36400, loss = 0.83 (911.0 examples/sec; 0.141 sec/batch)
2017-05-09 22:18:43.419172: step 36410, loss = 0.76 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-09 22:18:44.728224: step 36420, loss = 0.84 (977.8 examples/sec; 0.131 sec/E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 746 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
batch)
2017-05-09 22:18:46.013377: step 36430, loss = 0.69 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:18:47.305244: step 36440, loss = 0.72 (990.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:18:48.571239: step 36450, loss = 0.69 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:18:49.859783: step 36460, loss = 0.82 (993.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:18:51.170646: step 36470, loss = 0.93 (976.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:18:52.443181: step 36480, loss = 0.62 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:18:53.736608: step 36490, loss = 0.74 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:18:55.122239: step 36500, loss = 0.74 (923.8 examples/sec; 0.139 sec/batch)
2017-05-09 22:18:56.303021: step 36510, loss = 0.83 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-09 22:18:57.580904: step 36520, loss = 0.69 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:18:58.896545: step 36530, loss = 0.86 (972.9 examples/sec; 0.132 sec/batch)
2017-05-09 22:19:00.215634: step 36540, loss = 0.80 (970.4 examples/sec; 0.132 sec/batch)
2017-05-09 22:19:01.520077: step 36550, loss = 0.78 (981.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:19:02.799125: step 36560, loss = 0.91 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:19:04.064891: step 36570, loss = 0.70 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:19:05.335388: step 36580, loss = 0.82 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:19:06.606800: step 36590, loss = 0.75 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:19:07.990879: step 36600, loss = 0.82 (924.8 examples/sec; 0.138 sec/batch)
2017-05-09 22:19:09.181479: step 36610, loss = 1.20 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-09 22:19:10.470082: step 36620, loss = 0.80 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:19:11.750061: step 36630, loss = 0.73 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:19:13.048051: step 36640, loss = 0.81 (986.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:19:14.340930: step 36650, loss = 0.91 (990.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:19:15.607893: step 36660, loss = 0.81 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:19:16.890422: step 36670, loss = 0.80 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:19:18.168436: step 36680, loss = 0.89 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:19:19.471016: step 36690, loss = 0.71 (982.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:19:20.868756: step 36700, loss = 0.70 (915.8 examples/sec; 0.140 sec/batch)
2017-05-09 22:19:22.061783: step 36710, loss = 0.65 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-09 22:19:23.374328: step 36720, loss = 0.84 (975.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:19:24.671642: step 36730, loss = 0.77 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:19:25.952728: step 36740, loss = 0.88 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:19:27.254035: step 36750, loss = 0.92 (983.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:19:28.511507: step 36760, loss = 0.78 (1018.0 examples/sec; 0.126 sec/batch)
2017-05-09 22:19:29.810217: step 36770, loss = 0.72 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:19:31.093257: step 36780, loss = 0.87 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:19:32.354601: step 36790, loss = 0.73 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-09 22:19:33.752180: step 36800, loss = 0.71 (915.9 examples/sec; 0.140 sec/batch)
2017-05-09 22:19:34.955899: step 36810, loss = 0.85 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-09 22:19:36.251069: step 36820, loss = 0.99 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:19:37.542859: step 36830, loss = 0.70 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:19:38.831269: step 36840, loss = 0.85 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:19:40.101938: step 36850, loss = 0.74 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:19:41.414376: step 36860, loss = 0.74 (975.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:19:42.706633: step 36870, loss = 0.88 (990.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:19:43.993974: step 36880, loss = 0.84 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:19:45.308315: step 36890, loss = 0.91 (973.9 examples/sec; 0.131 sec/batch)
2017-05-09 22:19:46.692336: step 36900, loss = 1.06 (924.8 examples/sec; 0.138 sec/batch)
2017-05-09 22:19:47.886023: step 36910, loss = 0.84 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-09 22:19:49.184008: step 36920, loss = 0.64 (986.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:19:50.481753: step 36930, loss = 0.76 (986.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:19:51.784124: step 36940, loss = 1.11 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:19:53.062557: step 36950, loss = 0.79 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:19:54.355260: step 36960, loss = 0.84 (990.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:19:55.673960: step 36970, loss = 0.84 (970.7 examples/sec; 0.132 sec/batch)
2017-05-09 22:19:56.979280: step 36980, loss = 0.77 (980.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:19:58.279689: step 36990, loss = 0.80 (984.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:19:59.649658: step 37000, loss = 0.73 (934.3 examples/sec; 0.137 sec/batch)
2017-05-09 22:20:00.823012: step 37010, loss = 0.79 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-09 22:20:02.100897: step 37020, loss = 0.80 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:20:03.393490: step 37030, loss = 0.71 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:20:04.680129: step 37040, loss = 0.76 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:20:05.947040: step 37050, loss = 0.83 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:20:07.230637: step 37060, loss = 0.77 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:20:08.519542: step 37070, loss = 0.83 (993.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:20:09.802325: step 37080, loss = 0.78 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:20:11.087127: step 37090, loss = 0.76 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:20:12.462261: step 37100, loss = 0.85 (930.8 examples/sec; 0.138 sec/batch)
2017-05-09 22:20:13.666531: step 37110, loss = 0.80 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-09 22:20:14.966607: step 37120, loss = 0.75 (984.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:20:16.245334: step 37130, loss = 0.83 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:20:17.572527: step 37140, loss = 0.66 (964.4 examples/sec; 0.133 sec/batch)
2017-05-09 22:20:18.866918: step 37150, loss = 0.83 (988.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:20:20.152645: step 37160, loss = 0.82 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:20:21.447059: step 37170, loss = 0.68 (988.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:20:22.752054: step 37180, loss = 0.75 (980.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:20:24.025211: step 37190, loss = 0.87 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:20:25.443687: step 37200, loss = 0.81 (902.4 examples/sec; 0.142 sec/batch)
2017-05-09 22:20:26.649206: step 37210, loss = 0.76 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-09 22:20:27.915921: step 37220, loss = 0.87 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:20:29.208712: step 37230, loss = 0.71 (990.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:20:30.524048: step 37240, loss = 0.66 (973.1 examples/sec; 0.132 sec/batch)
2017-05-09 22:20:31.827203: step 37250, loss = 0.71 (982.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:20:33.151293: step 37260, loss = 0.65 (966.7 examples/sec; 0.132 sec/batch)
2017-05-09 22:20:34.461640: step 37270, loss = 0.66 (976.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:20:35.738362: step 37280, loss = 0.76 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:20:37.016263: step 37290, loss = 0.81 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:20:38.380268: step 37300, loss = 0.83 (938.4 examples/sec; 0.136 sec/batch)
2017-05-09 22:20:39.569742: step 37310, loss = 0.67 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-09 22:20:40.846556: step 37320, loss = 0.85 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:20:42.144192: step 37330, loss = 0.76 (9E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 767 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
86.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:20:43.456296: step 37340, loss = 0.78 (975.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:20:44.754522: step 37350, loss = 0.81 (986.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:20:46.044027: step 37360, loss = 0.87 (992.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:20:47.346865: step 37370, loss = 0.59 (982.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:20:48.639905: step 37380, loss = 0.89 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:20:49.939917: step 37390, loss = 0.67 (984.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:20:51.335909: step 37400, loss = 0.82 (916.9 examples/sec; 0.140 sec/batch)
2017-05-09 22:20:52.542322: step 37410, loss = 0.70 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-09 22:20:53.833286: step 37420, loss = 0.78 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:20:55.099956: step 37430, loss = 0.77 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:20:56.402110: step 37440, loss = 0.78 (983.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:20:57.689432: step 37450, loss = 0.87 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:20:59.003855: step 37460, loss = 0.98 (973.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:21:00.275261: step 37470, loss = 0.73 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:21:01.584238: step 37480, loss = 0.73 (977.9 examples/sec; 0.131 sec/batch)
2017-05-09 22:21:02.875387: step 37490, loss = 0.79 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:21:04.264060: step 37500, loss = 0.90 (921.7 examples/sec; 0.139 sec/batch)
2017-05-09 22:21:05.477969: step 37510, loss = 0.84 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-09 22:21:06.803499: step 37520, loss = 0.68 (965.7 examples/sec; 0.133 sec/batch)
2017-05-09 22:21:08.078403: step 37530, loss = 0.83 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:21:09.358188: step 37540, loss = 0.80 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:21:10.643096: step 37550, loss = 0.77 (996.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:21:11.919627: step 37560, loss = 0.83 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:21:13.209988: step 37570, loss = 0.97 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:21:14.493411: step 37580, loss = 0.92 (997.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:21:15.770432: step 37590, loss = 0.84 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:21:17.145769: step 37600, loss = 0.66 (930.7 examples/sec; 0.138 sec/batch)
2017-05-09 22:21:18.337027: step 37610, loss = 0.79 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-09 22:21:19.613605: step 37620, loss = 0.72 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:21:20.916768: step 37630, loss = 0.72 (982.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:21:22.193743: step 37640, loss = 0.66 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:21:23.512375: step 37650, loss = 0.79 (970.7 examples/sec; 0.132 sec/batch)
2017-05-09 22:21:24.800975: step 37660, loss = 0.83 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:21:26.101152: step 37670, loss = 0.65 (984.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:21:27.391389: step 37680, loss = 0.92 (992.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:21:28.672694: step 37690, loss = 0.75 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:21:30.059155: step 37700, loss = 0.86 (923.2 examples/sec; 0.139 sec/batch)
2017-05-09 22:21:31.243816: step 37710, loss = 0.77 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-09 22:21:32.508927: step 37720, loss = 0.82 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:21:33.804601: step 37730, loss = 0.80 (987.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:21:35.096013: step 37740, loss = 0.85 (991.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:21:36.415102: step 37750, loss = 0.89 (970.4 examples/sec; 0.132 sec/batch)
2017-05-09 22:21:37.720551: step 37760, loss = 0.78 (980.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:21:39.014688: step 37770, loss = 0.83 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:21:40.335662: step 37780, loss = 0.75 (969.0 examples/sec; 0.132 sec/batch)
2017-05-09 22:21:41.658431: step 37790, loss = 0.87 (967.7 examples/sec; 0.132 sec/batch)
2017-05-09 22:21:43.045913: step 37800, loss = 0.72 (922.5 examples/sec; 0.139 sec/batch)
2017-05-09 22:21:44.255663: step 37810, loss = 0.74 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-09 22:21:45.541169: step 37820, loss = 0.77 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:21:46.823668: step 37830, loss = 0.90 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:21:48.095511: step 37840, loss = 0.99 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:21:49.384858: step 37850, loss = 0.86 (992.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:21:50.692651: step 37860, loss = 0.81 (978.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:21:51.978663: step 37870, loss = 0.78 (995.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:21:53.305005: step 37880, loss = 0.69 (965.1 examples/sec; 0.133 sec/batch)
2017-05-09 22:21:54.600550: step 37890, loss = 0.83 (988.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:21:55.977428: step 37900, loss = 0.87 (929.6 examples/sec; 0.138 sec/batch)
2017-05-09 22:21:57.205969: step 37910, loss = 0.62 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-09 22:21:58.516996: step 37920, loss = 0.76 (976.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:21:59.822517: step 37930, loss = 0.70 (980.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:22:01.119897: step 37940, loss = 0.69 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:22:02.392664: step 37950, loss = 0.77 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:22:03.675553: step 37960, loss = 0.72 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:22:04.961380: step 37970, loss = 0.61 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:22:06.241687: step 37980, loss = 0.71 (999.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:22:07.527393: step 37990, loss = 0.75 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:22:08.919210: step 38000, loss = 0.74 (919.7 examples/sec; 0.139 sec/batch)
2017-05-09 22:22:10.106597: step 38010, loss = 0.90 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-09 22:22:11.407623: step 38020, loss = 0.85 (983.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:22:12.700072: step 38030, loss = 0.86 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:22:14.002425: step 38040, loss = 0.88 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:22:15.332919: step 38050, loss = 0.83 (962.0 examples/sec; 0.133 sec/batch)
2017-05-09 22:22:16.616891: step 38060, loss = 0.86 (996.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:22:17.902440: step 38070, loss = 0.74 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:22:19.204161: step 38080, loss = 0.76 (983.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:22:20.532799: step 38090, loss = 0.95 (963.4 examples/sec; 0.133 sec/batch)
2017-05-09 22:22:21.959420: step 38100, loss = 0.65 (897.2 examples/sec; 0.143 sec/batch)
2017-05-09 22:22:23.159243: step 38110, loss = 0.81 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-09 22:22:24.449661: step 38120, loss = 0.87 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:22:25.715179: step 38130, loss = 0.78 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:22:27.015543: step 38140, loss = 0.87 (984.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:22:28.273727: step 38150, loss = 0.75 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-09 22:22:29.595848: step 38160, loss = 0.71 (968.1 examples/sec; 0.132 sec/batch)
2017-05-09 22:22:30.892877: step 38170, loss = 0.67 (986.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:22:32.156292: step 38180, loss = 0.92 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-09 22:22:33.439898: step 38190, loss = 0.86 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:22:34.821545: step 38200, loss = 0.83 (926.4 examples/sec; 0.138 sec/batch)
2017-05-09 22:22:36.014362: step 38210, loss = 0.85 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-09 22:22:37.299833: step 38220, loss = 0.78 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:22:38.587014: step 38230, loss = 0.75 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:22:39.885852: step 38240, loss = 0.77 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:22:41.183983: step 38250, loss = 0.82 (986.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:22:42.502148: step 38260, loss = 0.83 (971.0 examples/sec; 0.132 sec/batch)
2017-05-09 22:22:43.794518: step 38270, loss = 0.80 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:22:45.100355: step 38280, loss = 0.68 (980.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:22:46.400049: step 38290, loss = 0.82 (984.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:22:47.805811: step 38300, loss = 0.95 (910.5 examples/sec; 0.141 sec/batch)
2017-05-09 22:22:49.013043: step 38310, loss = 0.75 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-09 22:22:50.337170: step 38320, loss = 0.79 (966.7 examples/sec; 0.132 sec/batch)
2017-05-09 22:22:51.658834: step 38330, loss = 0.79 (968.5 examples/sec; 0.132 sec/batch)
2017-05-09 22:22:52.951505: step 38340, loss = 0.78 (990.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:22:54.255076: step 38350, loss = 0.71 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:22:55.553395: step 38360, loss = 0.63 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:22:56.853994: step 38370, loss = 0.83 (984.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:22:58.147416: step 38380, loss = 0.84 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:22:59.424290: step 38390, loss = 0.69 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:23:00.829950: step 38400, loss = 0.71 (910.6 examples/sec; 0.141 sec/batch)
2017-05-09 22:23:02.042301: step 38410, loss = 0.79 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-09 22:23:03.354598: step 38420, loss = 0.73 (975.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:23:04.653112: step 38430, loss = 0.85 (985.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:23:05.919249: step 38440, loss = 0.85 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:23:07.227621: step 38450, loss = 0.90 (978.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:23:08.521887: step 38460, loss = 0.85 (989.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:23:09.823069: step 38470, loss = 0.96 (983.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:23:11.138486: step 38480, loss = 0.88 (973.1 examples/sec; 0.132 sec/batch)
2017-05-09 22:23:12.439664: step 38490, loss = 0.81 (983.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:23:13.817217: step 38500, loss = 0.92 (929.2 examples/sec; 0.138 sec/batch)
2017-05-09 22:23:15.026494: step 38510, loss = 0.99 (1058.5 examples/sec; 0.121 sec/batch)
2017-05-09 22:23:16.351746: step 38520, loss = 0.94 (965.9 examples/sec; 0.133 sec/batch)
2017-05-09 22:23:17.662909: step 38530, loss = 0.74 (976.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:23:18.957619: step 38540, loss = 0.78 (988.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:23:20.249130: step 38550, loss = 0.89 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:23:21.541242: step 38560, loss = 1.02 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:23:22.842278: step 38570, loss = 0.84 (983.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:23:24.152151: step 38580, loss = 0.92 (977.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:23:25.462654: step 38590, loss = 0.85 (976.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:23:26.845108: step 38600, loss = 0.71 (925.9 examples/sec; 0.138 sec/batch)
2017-05-09 22:23:28.025859: step 38610, loss = 0.73 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-09 22:23:29.309901: step 38620, loss = 0.78 (996.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:23:30.606680: step 38630, loss = 0.80 (987.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:23:31.893037: step 38640, loss = 0.79 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:23:33.296197: step 38650, loss = 0.75 (912.2 examples/sec; 0.140 sec/batch)
2017-05-09 22:23:34.599170: step 38660, loss = 0.76 (982.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:23:35.861370: step 38670, loss = 1.02 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-09 22:23:37.150929: step 38680, loss = 0.89 (992.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:23:38.439782: step 38690, loss = 0.85 (993.1 examples/sec;E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 787 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
 0.129 sec/batch)
2017-05-09 22:23:39.843203: step 38700, loss = 0.81 (912.0 examples/sec; 0.140 sec/batch)
2017-05-09 22:23:41.059934: step 38710, loss = 0.90 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-09 22:23:42.395295: step 38720, loss = 0.85 (958.5 examples/sec; 0.134 sec/batch)
2017-05-09 22:23:43.671896: step 38730, loss = 0.85 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:23:44.965055: step 38740, loss = 0.77 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:23:46.248842: step 38750, loss = 0.71 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:23:47.521151: step 38760, loss = 0.68 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:23:48.804380: step 38770, loss = 0.71 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:23:50.122083: step 38780, loss = 1.02 (971.4 examples/sec; 0.132 sec/batch)
2017-05-09 22:23:51.398823: step 38790, loss = 0.84 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:23:52.787591: step 38800, loss = 0.71 (921.7 examples/sec; 0.139 sec/batch)
2017-05-09 22:23:53.994970: step 38810, loss = 0.87 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-09 22:23:55.273283: step 38820, loss = 0.80 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:23:56.562199: step 38830, loss = 0.67 (993.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:23:57.830672: step 38840, loss = 0.86 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:23:59.104531: step 38850, loss = 0.92 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:24:00.399523: step 38860, loss = 0.79 (988.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:24:01.679714: step 38870, loss = 0.84 (999.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:24:02.988953: step 38880, loss = 0.74 (977.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:24:04.277208: step 38890, loss = 0.82 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:24:05.656136: step 38900, loss = 0.80 (928.3 examples/sec; 0.138 sec/batch)
2017-05-09 22:24:06.852122: step 38910, loss = 0.81 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-09 22:24:08.127574: step 38920, loss = 0.80 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:24:09.433486: step 38930, loss = 0.71 (980.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:24:10.746513: step 38940, loss = 1.18 (974.9 examples/sec; 0.131 sec/batch)
2017-05-09 22:24:12.068712: step 38950, loss = 0.93 (968.1 examples/sec; 0.132 sec/batch)
2017-05-09 22:24:13.360820: step 38960, loss = 1.04 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:24:14.668576: step 38970, loss = 0.69 (978.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:24:15.971328: step 38980, loss = 0.85 (982.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:24:17.261089: step 38990, loss = 0.85 (992.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:24:18.662633: step 39000, loss = 0.89 (913.3 examples/sec; 0.140 sec/batch)
2017-05-09 22:24:19.848720: step 39010, loss = 0.70 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-09 22:24:21.140672: step 39020, loss = 0.76 (990.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:24:22.417280: step 39030, loss = 0.92 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:24:23.724153: step 39040, loss = 0.72 (979.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:24:25.018006: step 39050, loss = 0.77 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:24:26.320087: step 39060, loss = 0.79 (983.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:24:27.599021: step 39070, loss = 1.05 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:24:28.909905: step 39080, loss = 0.71 (976.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:24:30.238036: step 39090, loss = 1.03 (963.8 examples/sec; 0.133 sec/batch)
2017-05-09 22:24:31.624101: step 39100, loss = 0.63 (923.5 examples/sec; 0.139 sec/batch)
2017-05-09 22:24:32.813136: step 39110, loss = 0.75 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-09 22:24:34.106174: step 39120, loss = 0.82 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:24:35.398343: step 39130, loss = 0.67 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:24:36.702139: step 39140, loss = 0.89 (981.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:24:38.011191: step 39150, loss = 0.88 (977.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:24:39.334946: step 39160, loss = 0.84 (966.9 examples/sec; 0.132 sec/batch)
2017-05-09 22:24:40.643624: step 39170, loss = 0.65 (978.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:24:41.925184: step 39180, loss = 0.72 (998.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:24:43.253248: step 39190, loss = 0.95 (963.8 examples/sec; 0.133 sec/batch)
2017-05-09 22:24:44.657876: step 39200, loss = 0.72 (911.3 examples/sec; 0.140 sec/batch)
2017-05-09 22:24:45.863159: step 39210, loss = 0.84 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-09 22:24:47.160356: step 39220, loss = 0.79 (986.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:24:48.461761: step 39230, loss = 0.72 (983.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:24:49.772819: step 39240, loss = 0.98 (976.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:24:51.071072: step 39250, loss = 0.86 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:24:52.371869: step 39260, loss = 0.74 (984.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:24:53.664254: step 39270, loss = 0.69 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:24:54.953429: step 39280, loss = 0.92 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:24:56.257410: step 39290, loss = 0.87 (981.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:24:57.663342: step 39300, loss = 0.80 (910.4 examples/sec; 0.141 sec/batch)
2017-05-09 22:24:58.857532: step 39310, loss = 0.73 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-09 22:25:00.173503: step 39320, loss = 0.79 (972.7 examples/sec; 0.132 sec/batch)
2017-05-09 22:25:01.469659: step 39330, loss = 0.78 (987.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:25:02.760989: step 39340, loss = 0.80 (991.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:25:04.059772: step 39350, loss = 0.70 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:25:05.352839: step 39360, loss = 0.93 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:25:06.659721: step 39370, loss = 0.94 (979.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:25:07.938894: step 39380, loss = 0.86 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:25:09.223375: step 39390, loss = 0.83 (996.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:25:10.598208: step 39400, loss = 0.71 (931.0 examples/sec; 0.137 sec/batch)
2017-05-09 22:25:11.786321: step 39410, loss = 0.79 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-09 22:25:13.070722: step 39420, loss = 0.73 (996.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:25:14.357861: step 39430, loss = 0.79 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:25:15.638707: step 39440, loss = 0.74 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:25:16.921527: step 39450, loss = 0.81 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:25:18.223714: step 39460, loss = 0.88 (983.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:25:19.514217: step 39470, loss = 0.80 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:25:20.799493: step 39480, loss = 0.84 (995.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:25:22.099951: step 39490, loss = 0.79 (984.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:25:23.502612: step 39500, loss = 0.86 (912.6 examples/sec; 0.140 sec/batch)
2017-05-09 22:25:24.731587: step 39510, loss = 0.84 (1041.5 examples/sec; 0.123 sec/batch)
2017-05-09 22:25:26.042859: step 39520, loss = 0.96 (976.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:25:27.344465: step 39530, loss = 0.78 (983.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:25:28.664534: step 39540, loss = 0.75 (969.6 examples/sec; 0.132 sec/batch)
2017-05-09 22:25:29.954302: step 39550, loss = 0.76 (992.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:25:31.268129: step 39560, loss = 0.94 (974.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:25:32.553192: step 39570, loss = 0.81 (996.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:25:33.836121: step 39580, loss = 0.81 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:25:35.141024: step 39590, loss = 0.91 (980.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:25:36.535085: step 39600, loss = 0.99 (91E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 807 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
8.2 examples/sec; 0.139 sec/batch)
2017-05-09 22:25:37.773304: step 39610, loss = 1.15 (1033.7 examples/sec; 0.124 sec/batch)
2017-05-09 22:25:39.066789: step 39620, loss = 0.75 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:25:40.363035: step 39630, loss = 0.76 (987.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:25:41.649032: step 39640, loss = 0.76 (995.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:25:42.953220: step 39650, loss = 0.84 (981.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:25:44.252061: step 39660, loss = 0.79 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:25:45.539180: step 39670, loss = 0.77 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:25:46.847765: step 39680, loss = 0.94 (978.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:25:48.125718: step 39690, loss = 0.78 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:25:49.519941: step 39700, loss = 0.77 (918.1 examples/sec; 0.139 sec/batch)
2017-05-09 22:25:50.707664: step 39710, loss = 0.83 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-09 22:25:51.995221: step 39720, loss = 0.83 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:25:53.279904: step 39730, loss = 0.89 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:25:54.600497: step 39740, loss = 0.78 (969.3 examples/sec; 0.132 sec/batch)
2017-05-09 22:25:55.913574: step 39750, loss = 0.84 (974.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:25:57.204855: step 39760, loss = 0.85 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:25:58.512188: step 39770, loss = 0.88 (979.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:25:59.802140: step 39780, loss = 1.00 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:26:01.118147: step 39790, loss = 0.73 (972.6 examples/sec; 0.132 sec/batch)
2017-05-09 22:26:02.501075: step 39800, loss = 0.77 (925.6 examples/sec; 0.138 sec/batch)
2017-05-09 22:26:03.704743: step 39810, loss = 0.94 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-09 22:26:04.994554: step 39820, loss = 0.79 (992.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:26:06.293406: step 39830, loss = 1.02 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:26:07.579086: step 39840, loss = 0.75 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:26:08.880322: step 39850, loss = 0.72 (983.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:26:10.188747: step 39860, loss = 0.78 (978.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:26:11.522750: step 39870, loss = 0.79 (959.5 examples/sec; 0.133 sec/batch)
2017-05-09 22:26:12.827279: step 39880, loss = 1.05 (981.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:26:14.132229: step 39890, loss = 0.88 (980.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:26:15.527963: step 39900, loss = 0.79 (917.1 examples/sec; 0.140 sec/batch)
2017-05-09 22:26:16.758952: step 39910, loss = 0.82 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-09 22:26:18.044794: step 39920, loss = 0.90 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:26:19.326655: step 39930, loss = 0.74 (998.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:26:20.633842: step 39940, loss = 0.67 (979.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:26:21.920736: step 39950, loss = 0.84 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:26:23.205826: step 39960, loss = 0.92 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:26:24.497363: step 39970, loss = 0.72 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:26:25.778726: step 39980, loss = 0.77 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:26:27.055902: step 39990, loss = 0.85 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:26:28.428822: step 40000, loss = 0.79 (932.3 examples/sec; 0.137 sec/batch)
2017-05-09 22:26:29.629881: step 40010, loss = 0.69 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-09 22:26:30.917074: step 40020, loss = 0.77 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:26:32.209341: step 40030, loss = 0.74 (990.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:26:33.489703: step 40040, loss = 0.88 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:26:34.777760: step 40050, loss = 0.72 (993.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:26:36.059612: step 40060, loss = 0.74 (998.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:26:37.367655: step 40070, loss = 0.76 (978.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:26:38.695182: step 40080, loss = 0.77 (964.2 examples/sec; 0.133 sec/batch)
2017-05-09 22:26:39.992659: step 40090, loss = 0.75 (986.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:26:41.372143: step 40100, loss = 0.66 (927.9 examples/sec; 0.138 sec/batch)
2017-05-09 22:26:42.572417: step 40110, loss = 0.80 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-09 22:26:43.845040: step 40120, loss = 0.84 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:26:45.141661: step 40130, loss = 0.79 (987.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:26:46.416037: step 40140, loss = 0.66 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:26:47.717198: step 40150, loss = 0.83 (983.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:26:48.998149: step 40160, loss = 0.70 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:26:50.303246: step 40170, loss = 0.75 (980.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:26:51.604795: step 40180, loss = 1.13 (983.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:26:52.899416: step 40190, loss = 0.79 (988.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:26:54.280586: step 40200, loss = 0.71 (926.7 examples/sec; 0.138 sec/batch)
2017-05-09 22:26:55.468175: step 40210, loss = 0.66 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-09 22:26:56.730145: step 40220, loss = 0.97 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-09 22:26:58.049684: step 40230, loss = 0.68 (970.0 examples/sec; 0.132 sec/batch)
2017-05-09 22:26:59.349583: step 40240, loss = 0.81 (984.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:27:00.637489: step 40250, loss = 0.83 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:27:01.938487: step 40260, loss = 0.91 (983.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:27:03.202956: step 40270, loss = 0.95 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-09 22:27:04.495842: step 40280, loss = 0.79 (990.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:27:05.794829: step 40290, loss = 0.85 (985.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:27:07.180819: step 40300, loss = 0.86 (923.5 examples/sec; 0.139 sec/batch)
2017-05-09 22:27:08.373137: step 40310, loss = 0.84 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-09 22:27:09.661950: step 40320, loss = 0.71 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:27:10.938240: step 40330, loss = 0.83 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:27:12.202549: step 40340, loss = 0.87 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-09 22:27:13.485081: step 40350, loss = 0.72 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:27:14.804206: step 40360, loss = 0.77 (970.3 examples/sec; 0.132 sec/batch)
2017-05-09 22:27:16.099404: step 40370, loss = 0.73 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:27:17.382717: step 40380, loss = 0.75 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:27:18.698841: step 40390, loss = 0.61 (972.6 examples/sec; 0.132 sec/batch)
2017-05-09 22:27:20.087006: step 40400, loss = 0.84 (922.1 examples/sec; 0.139 sec/batch)
2017-05-09 22:27:21.283165: step 40410, loss = 0.74 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-09 22:27:22.577262: step 40420, loss = 0.71 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:27:23.851743: step 40430, loss = 0.80 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:27:25.140713: step 40440, loss = 0.87 (993.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:27:26.421059: step 40450, loss = 0.65 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:27:27.740213: step 40460, loss = 0.74 (970.3 examples/sec; 0.132 sec/batch)
2017-05-09 22:27:29.051700: step 40470, loss = 0.77 (976.0 examples/sec; 0.131 sec/batch)
2017-05-09 22:27:30.351526: step 40480, loss = 0.73 (984.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:27:31.660629: step 40490, loss = 0.94 (977.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:27:33.023798: step 40500, loss = 0.78 (939.0 examples/sec; 0.136 sec/batch)
2017-05-09 22:27:34.256140: step 40510,E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 827 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
 loss = 0.67 (1038.7 examples/sec; 0.123 sec/batch)
2017-05-09 22:27:35.576348: step 40520, loss = 0.92 (969.5 examples/sec; 0.132 sec/batch)
2017-05-09 22:27:36.850125: step 40530, loss = 0.80 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:27:38.129202: step 40540, loss = 0.80 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:27:39.423586: step 40550, loss = 0.85 (988.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:27:40.721017: step 40560, loss = 0.73 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:27:42.006942: step 40570, loss = 0.81 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:27:43.314706: step 40580, loss = 0.62 (978.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:27:44.605095: step 40590, loss = 0.73 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:27:45.991995: step 40600, loss = 0.71 (922.9 examples/sec; 0.139 sec/batch)
2017-05-09 22:27:47.214032: step 40610, loss = 0.76 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-09 22:27:48.554424: step 40620, loss = 0.79 (954.9 examples/sec; 0.134 sec/batch)
2017-05-09 22:27:49.839232: step 40630, loss = 0.77 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:27:51.108669: step 40640, loss = 0.97 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:27:52.410367: step 40650, loss = 0.67 (983.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:27:53.691021: step 40660, loss = 0.78 (999.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:27:54.993385: step 40670, loss = 0.84 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:27:56.303979: step 40680, loss = 0.73 (976.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:27:57.577383: step 40690, loss = 0.83 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:27:58.989911: step 40700, loss = 0.71 (906.2 examples/sec; 0.141 sec/batch)
2017-05-09 22:28:00.165402: step 40710, loss = 0.70 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-09 22:28:01.472766: step 40720, loss = 0.90 (979.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:28:02.752241: step 40730, loss = 0.69 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:28:04.029623: step 40740, loss = 0.84 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:28:05.310049: step 40750, loss = 0.84 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:28:06.625553: step 40760, loss = 0.84 (973.0 examples/sec; 0.132 sec/batch)
2017-05-09 22:28:07.900951: step 40770, loss = 0.74 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:28:09.177171: step 40780, loss = 0.95 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:28:10.474253: step 40790, loss = 0.93 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:28:11.841586: step 40800, loss = 0.86 (936.1 examples/sec; 0.137 sec/batch)
2017-05-09 22:28:13.040788: step 40810, loss = 0.76 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-09 22:28:14.319804: step 40820, loss = 0.73 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:28:15.610099: step 40830, loss = 0.71 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:28:16.866523: step 40840, loss = 0.98 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-09 22:28:18.201083: step 40850, loss = 0.71 (959.1 examples/sec; 0.133 sec/batch)
2017-05-09 22:28:19.500790: step 40860, loss = 0.86 (984.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:28:20.827073: step 40870, loss = 0.97 (965.1 examples/sec; 0.133 sec/batch)
2017-05-09 22:28:22.141686: step 40880, loss = 0.84 (973.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:28:23.427744: step 40890, loss = 0.73 (995.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:28:24.796383: step 40900, loss = 0.88 (935.2 examples/sec; 0.137 sec/batch)
2017-05-09 22:28:26.001913: step 40910, loss = 0.91 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-09 22:28:27.274438: step 40920, loss = 0.77 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:28:28.562580: step 40930, loss = 0.86 (993.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:28:29.864497: step 40940, loss = 0.79 (983.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:28:31.154376: step 40950, loss = 0.82 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:28:32.432920: step 40960, loss = 0.54 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:28:33.780056: step 40970, loss = 1.01 (950.2 examples/sec; 0.135 sec/batch)
2017-05-09 22:28:35.053352: step 40980, loss = 0.72 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:28:36.339328: step 40990, loss = 0.75 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:28:37.712569: step 41000, loss = 0.79 (932.1 examples/sec; 0.137 sec/batch)
2017-05-09 22:28:38.905555: step 41010, loss = 0.54 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-09 22:28:40.198894: step 41020, loss = 0.75 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:28:41.493283: step 41030, loss = 0.84 (988.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:28:42.780775: step 41040, loss = 0.75 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:28:44.086921: step 41050, loss = 0.72 (980.0 examples/sec; 0.131 sec/batch)
2017-05-09 22:28:45.381085: step 41060, loss = 0.75 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:28:46.673113: step 41070, loss = 0.83 (990.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:28:47.963425: step 41080, loss = 0.81 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:28:49.280855: step 41090, loss = 0.77 (971.6 examples/sec; 0.132 sec/batch)
2017-05-09 22:28:50.684341: step 41100, loss = 0.90 (912.0 examples/sec; 0.140 sec/batch)
2017-05-09 22:28:51.898660: step 41110, loss = 0.87 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-09 22:28:53.178526: step 41120, loss = 0.87 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:28:54.463324: step 41130, loss = 0.72 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:28:55.744352: step 41140, loss = 0.71 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:28:57.034322: step 41150, loss = 0.84 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:28:58.336094: step 41160, loss = 0.78 (983.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:28:59.623090: step 41170, loss = 0.64 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:29:00.908226: step 41180, loss = 0.76 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:29:02.189548: step 41190, loss = 0.82 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:29:03.585193: step 41200, loss = 0.60 (917.1 examples/sec; 0.140 sec/batch)
2017-05-09 22:29:04.793820: step 41210, loss = 0.64 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-09 22:29:06.075385: step 41220, loss = 0.68 (998.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:29:07.348172: step 41230, loss = 0.76 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:29:08.644505: step 41240, loss = 0.86 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:29:09.938632: step 41250, loss = 1.05 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:29:11.215077: step 41260, loss = 0.76 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:29:12.513467: step 41270, loss = 0.89 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:29:13.804861: step 41280, loss = 0.79 (991.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:29:15.121167: step 41290, loss = 0.98 (972.4 examples/sec; 0.132 sec/batch)
2017-05-09 22:29:16.534631: step 41300, loss = 0.72 (905.6 examples/sec; 0.141 sec/batch)
2017-05-09 22:29:17.737214: step 41310, loss = 0.93 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-09 22:29:19.041334: step 41320, loss = 0.75 (981.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:29:20.326828: step 41330, loss = 0.85 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:29:21.623327: step 41340, loss = 0.70 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:29:22.909986: step 41350, loss = 0.81 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:29:24.202956: step 41360, loss = 0.72 (990.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:29:25.486861: step 41370, loss = 0.81 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:29:26.811894: step 41380, loss = 1.08 (966.0 examples/sec; 0.133 sec/batch)
2017-05-09 22:29:28.134603: step 41390, loss = 0.93 (967.7 examples/sec; 0.132 sec/batch)
2017-05-09 22:29:29.516066: step 41400, loss = 0.82 (926.6 examples/sec; 0.138 sec/batch)
2017-05-09 22:29:30.683001: step 41410, loss = 0.83 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-09 22:2E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 848 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
9:31.954039: step 41420, loss = 0.96 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:29:33.231672: step 41430, loss = 0.79 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:29:34.535789: step 41440, loss = 0.80 (981.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:29:35.802966: step 41450, loss = 0.78 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:29:37.078304: step 41460, loss = 0.77 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:29:38.349685: step 41470, loss = 0.74 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:29:39.633252: step 41480, loss = 0.66 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:29:40.985308: step 41490, loss = 0.77 (946.7 examples/sec; 0.135 sec/batch)
2017-05-09 22:29:42.367140: step 41500, loss = 0.76 (926.3 examples/sec; 0.138 sec/batch)
2017-05-09 22:29:43.575513: step 41510, loss = 0.75 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-09 22:29:44.892962: step 41520, loss = 0.82 (971.6 examples/sec; 0.132 sec/batch)
2017-05-09 22:29:46.199540: step 41530, loss = 0.77 (979.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:29:47.484107: step 41540, loss = 0.71 (996.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:29:48.781736: step 41550, loss = 0.65 (986.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:29:50.057396: step 41560, loss = 0.69 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:29:51.332758: step 41570, loss = 0.95 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:29:52.616282: step 41580, loss = 0.60 (997.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:29:53.913829: step 41590, loss = 0.70 (986.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:29:55.320044: step 41600, loss = 0.88 (910.2 examples/sec; 0.141 sec/batch)
2017-05-09 22:29:56.552749: step 41610, loss = 0.75 (1038.4 examples/sec; 0.123 sec/batch)
2017-05-09 22:29:57.843000: step 41620, loss = 0.74 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:29:59.128197: step 41630, loss = 0.78 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:30:00.422848: step 41640, loss = 0.77 (988.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:30:01.717378: step 41650, loss = 0.66 (988.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:30:03.028041: step 41660, loss = 1.04 (976.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:30:04.307178: step 41670, loss = 0.93 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:30:05.590057: step 41680, loss = 1.00 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:30:06.877559: step 41690, loss = 1.02 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:30:08.282322: step 41700, loss = 0.75 (911.2 examples/sec; 0.140 sec/batch)
2017-05-09 22:30:09.480537: step 41710, loss = 0.87 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-09 22:30:10.779888: step 41720, loss = 0.77 (985.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:30:12.094744: step 41730, loss = 0.74 (973.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:30:13.387555: step 41740, loss = 0.63 (990.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:30:14.676388: step 41750, loss = 0.78 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:30:15.948731: step 41760, loss = 0.60 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:30:17.244325: step 41770, loss = 0.87 (988.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:30:18.537709: step 41780, loss = 0.68 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:30:19.825104: step 41790, loss = 0.90 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:30:21.221591: step 41800, loss = 0.73 (916.6 examples/sec; 0.140 sec/batch)
2017-05-09 22:30:22.414749: step 41810, loss = 0.84 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-09 22:30:23.689181: step 41820, loss = 0.71 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:30:24.968503: step 41830, loss = 0.81 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:30:26.243105: step 41840, loss = 0.77 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:30:27.518513: step 41850, loss = 0.73 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:30:28.824763: step 41860, loss = 0.72 (979.9 examples/sec; 0.131 sec/batch)
2017-05-09 22:30:30.110915: step 41870, loss = 0.84 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:30:31.403386: step 41880, loss = 0.88 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:30:32.711099: step 41890, loss = 0.86 (978.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:30:34.128939: step 41900, loss = 1.10 (902.8 examples/sec; 0.142 sec/batch)
2017-05-09 22:30:35.334747: step 41910, loss = 0.73 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-09 22:30:36.624973: step 41920, loss = 0.71 (992.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:30:37.904573: step 41930, loss = 0.82 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:30:39.152685: step 41940, loss = 0.75 (1025.5 examples/sec; 0.125 sec/batch)
2017-05-09 22:30:40.441023: step 41950, loss = 0.71 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:30:41.753807: step 41960, loss = 0.85 (975.0 examples/sec; 0.131 sec/batch)
2017-05-09 22:30:43.040071: step 41970, loss = 0.92 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:30:44.332047: step 41980, loss = 0.71 (990.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:30:45.632135: step 41990, loss = 0.70 (984.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:30:47.030341: step 42000, loss = 0.72 (915.5 examples/sec; 0.140 sec/batch)
2017-05-09 22:30:48.221270: step 42010, loss = 0.86 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-09 22:30:49.532050: step 42020, loss = 0.75 (976.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:30:50.840390: step 42030, loss = 0.91 (978.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:30:52.118480: step 42040, loss = 0.87 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:30:53.406635: step 42050, loss = 0.77 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:30:54.737101: step 42060, loss = 0.80 (962.1 examples/sec; 0.133 sec/batch)
2017-05-09 22:30:56.039725: step 42070, loss = 0.66 (982.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:30:57.378638: step 42080, loss = 0.83 (956.0 examples/sec; 0.134 sec/batch)
2017-05-09 22:30:58.676797: step 42090, loss = 0.86 (986.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:31:00.032211: step 42100, loss = 0.80 (944.4 examples/sec; 0.136 sec/batch)
2017-05-09 22:31:01.242770: step 42110, loss = 0.88 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-09 22:31:02.541011: step 42120, loss = 1.03 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:31:03.829318: step 42130, loss = 0.91 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:31:05.149539: step 42140, loss = 0.82 (969.5 examples/sec; 0.132 sec/batch)
2017-05-09 22:31:06.421095: step 42150, loss = 0.72 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:31:07.706153: step 42160, loss = 0.79 (996.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:31:09.009111: step 42170, loss = 0.91 (982.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:31:10.313298: step 42180, loss = 0.71 (981.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:31:11.591338: step 42190, loss = 0.61 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:31:12.981881: step 42200, loss = 0.82 (920.5 examples/sec; 0.139 sec/batch)
2017-05-09 22:31:14.168797: step 42210, loss = 0.87 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-09 22:31:15.445137: step 42220, loss = 0.93 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:31:16.736326: step 42230, loss = 0.76 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:31:18.016744: step 42240, loss = 0.96 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:31:19.293985: step 42250, loss = 0.81 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:31:20.603045: step 42260, loss = 0.83 (977.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:31:21.886239: step 42270, loss = 0.73 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:31:23.213808: step 42280, loss = 1.00 (964.2 examples/sec; 0.133 sec/batch)
2017-05-09 22:31:24.503351: step 42290, loss = 0.78 (992.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:31:25.900079: step 42300, loss = 0.58 (916.4 examples/sec; 0.140 sec/batch)
2017-05-09 22:31:27.078937: step 42310, loss = 0.70 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-09 22:31:28.383140: step 42320, loss = 0.85 (981.4 examples/sec; 0.130E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 868 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
 sec/batch)
2017-05-09 22:31:29.684417: step 42330, loss = 0.89 (983.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:31:30.962668: step 42340, loss = 0.66 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:31:32.234102: step 42350, loss = 0.73 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:31:33.547879: step 42360, loss = 0.93 (974.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:31:34.854027: step 42370, loss = 0.84 (980.0 examples/sec; 0.131 sec/batch)
2017-05-09 22:31:36.133628: step 42380, loss = 0.80 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:31:37.420320: step 42390, loss = 0.71 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:31:38.805754: step 42400, loss = 0.82 (923.9 examples/sec; 0.139 sec/batch)
2017-05-09 22:31:39.989678: step 42410, loss = 0.85 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-09 22:31:41.261685: step 42420, loss = 0.79 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:31:42.539251: step 42430, loss = 0.88 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:31:43.834963: step 42440, loss = 0.78 (987.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:31:45.145770: step 42450, loss = 0.76 (976.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:31:46.429221: step 42460, loss = 0.87 (997.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:31:47.717411: step 42470, loss = 0.87 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:31:49.011995: step 42480, loss = 0.72 (988.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:31:50.333952: step 42490, loss = 0.78 (968.3 examples/sec; 0.132 sec/batch)
2017-05-09 22:31:51.702371: step 42500, loss = 0.95 (935.4 examples/sec; 0.137 sec/batch)
2017-05-09 22:31:52.907853: step 42510, loss = 0.98 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-09 22:31:54.205067: step 42520, loss = 0.94 (986.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:31:55.495937: step 42530, loss = 0.68 (991.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:31:56.777224: step 42540, loss = 0.88 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:31:58.057692: step 42550, loss = 0.67 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:31:59.339821: step 42560, loss = 0.75 (998.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:32:00.613246: step 42570, loss = 0.74 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:32:01.896254: step 42580, loss = 0.76 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:32:03.185203: step 42590, loss = 0.58 (993.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:32:04.564646: step 42600, loss = 0.67 (927.9 examples/sec; 0.138 sec/batch)
2017-05-09 22:32:05.767367: step 42610, loss = 0.83 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-09 22:32:07.066023: step 42620, loss = 0.80 (985.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:32:08.390801: step 42630, loss = 1.02 (966.2 examples/sec; 0.132 sec/batch)
2017-05-09 22:32:09.682846: step 42640, loss = 0.84 (990.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:32:10.973980: step 42650, loss = 0.66 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:32:12.247439: step 42660, loss = 0.76 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:32:13.531955: step 42670, loss = 0.69 (996.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:32:14.806069: step 42680, loss = 0.94 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:32:16.089901: step 42690, loss = 0.76 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:32:17.480735: step 42700, loss = 0.76 (920.3 examples/sec; 0.139 sec/batch)
2017-05-09 22:32:18.703815: step 42710, loss = 0.74 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-09 22:32:20.018516: step 42720, loss = 1.47 (973.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:32:21.281994: step 42730, loss = 0.64 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-09 22:32:22.592681: step 42740, loss = 0.67 (976.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:32:23.893236: step 42750, loss = 0.75 (984.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:32:25.191964: step 42760, loss = 0.80 (985.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:32:26.456649: step 42770, loss = 0.85 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-09 22:32:27.726962: step 42780, loss = 0.64 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:32:29.012118: step 42790, loss = 0.65 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:32:30.401433: step 42800, loss = 0.65 (921.3 examples/sec; 0.139 sec/batch)
2017-05-09 22:32:31.591596: step 42810, loss = 0.80 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-09 22:32:32.917543: step 42820, loss = 0.74 (965.4 examples/sec; 0.133 sec/batch)
2017-05-09 22:32:34.215487: step 42830, loss = 0.96 (986.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:32:35.540090: step 42840, loss = 1.08 (966.3 examples/sec; 0.132 sec/batch)
2017-05-09 22:32:36.816219: step 42850, loss = 0.81 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:32:38.127892: step 42860, loss = 1.08 (975.9 examples/sec; 0.131 sec/batch)
2017-05-09 22:32:39.435199: step 42870, loss = 0.84 (979.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:32:40.755707: step 42880, loss = 1.06 (969.3 examples/sec; 0.132 sec/batch)
2017-05-09 22:32:42.080312: step 42890, loss = 0.76 (966.3 examples/sec; 0.132 sec/batch)
2017-05-09 22:32:43.472702: step 42900, loss = 0.73 (919.3 examples/sec; 0.139 sec/batch)
2017-05-09 22:32:44.665589: step 42910, loss = 0.93 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-09 22:32:45.956655: step 42920, loss = 0.88 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:32:47.270032: step 42930, loss = 0.78 (974.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:32:48.584594: step 42940, loss = 0.78 (973.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:32:49.865767: step 42950, loss = 0.79 (999.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:32:51.148811: step 42960, loss = 0.70 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:32:52.427884: step 42970, loss = 0.75 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:32:53.707419: step 42980, loss = 0.84 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:32:55.001516: step 42990, loss = 0.78 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:32:56.376861: step 43000, loss = 0.83 (930.7 examples/sec; 0.138 sec/batch)
2017-05-09 22:32:57.583637: step 43010, loss = 0.76 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-09 22:32:58.892684: step 43020, loss = 0.80 (977.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:33:00.167927: step 43030, loss = 0.65 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:33:01.474681: step 43040, loss = 0.67 (979.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:33:02.768359: step 43050, loss = 0.69 (989.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:33:04.073925: step 43060, loss = 1.17 (980.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:33:05.375541: step 43070, loss = 0.76 (983.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:33:06.656101: step 43080, loss = 0.73 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:33:07.943687: step 43090, loss = 0.82 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:33:09.313634: step 43100, loss = 0.76 (934.3 examples/sec; 0.137 sec/batch)
2017-05-09 22:33:10.512584: step 43110, loss = 0.66 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-09 22:33:11.780199: step 43120, loss = 0.81 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:33:13.094853: step 43130, loss = 0.74 (973.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:33:14.404723: step 43140, loss = 0.83 (977.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:33:15.674575: step 43150, loss = 0.68 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:33:16.976759: step 43160, loss = 0.89 (983.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:33:18.291886: step 43170, loss = 0.88 (973.3 examples/sec; 0.132 sec/batch)
2017-05-09 22:33:19.583345: step 43180, loss = 0.79 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:33:20.848871: step 43190, loss = 0.82 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:33:22.238247: step 43200, loss = 0.91 (921.3 examples/sec; 0.139 sec/batch)
2017-05-09 22:33:23.425950: step 43210, loss = 0.73 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-09 22:33:24.693581: step 43220, loss = 0.90 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:33:25.972135: step 43230, loss = 0.74 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:33:27.285476: step 43240, loss = 0.68 (974.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:33:28.543747: step 43250, loss = 0.82 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-09 22:33:29.822736: step 43260, loss = 0.82 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:33:31.085927: step 43270, loss = 0.85 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-09 22:33:32.374571: step 43280, loss = 0.75 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:33:33.661025: step 43290, loss = 0.88 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:33:35.045675: step 43300, loss = 0.73 (924.4 examples/sec; 0.138 sec/batch)
2017-05-09 22:33:36.246220: step 43310, loss = 0.94 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-09 22:33:37.510130: step 43320, loss = 0.71 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-09 22:33:38.786770: step 43330, loss = 0.81 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:33:40.062985: step 43340, loss = 0.80 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:33:41.411076: step 43350, loss = 0.88 (949.5 examples/sec; 0.135 sec/batch)
2017-05-09 22:33:42.705011: step 43360, loss = 0.59 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:33:43.965028: step 43370, loss = 0.76 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-09 22:33:45.301037: step 43380, loss = 0.67 (958.1 examples/sec; 0.134 sec/batch)
2017-05-09 22:33:46.598847: step 43390, loss = 0.82 (986.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:33:47.958534: step 43400, loss = 0.71 (941.4 examples/sec; 0.136 sec/batch)
2017-05-09 22:33:49.148979: step 43410, loss = 0.92 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-09 22:33:50.439674: step 43420, loss = 0.81 (991.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:33:51.735316: step 43430, loss = 0.90 (987.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:33:53.016900: step 43440, loss = 0.79 (998.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:33:54.324206: step 43450, loss = 0.91 (979.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:33:55.624156: step 43460, loss = 0.93 (984.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:33:56.921703: step 43470, loss = 0.96 (986.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:33:58.215717: step 43480, loss = 0.79 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:33:59.510493: step 43490, loss = 0.74 (988.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:34:00.885382: step 43500, loss = 0.90 (931.0 examples/sec; 0.137 sec/batch)
2017-05-09 22:34:02.100420: step 43510, loss = 1.01 (1053.5 examples/sec; 0.122 sec/batch)
2017-05-09 22:34:03.373666: step 43520, loss = 0.69 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:34:04.653967: step 43530, loss = 0.83 (999.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:34:05.949863: step 43540, loss = 0.78 (987.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:34:07.242870: step 43550, loss = 0.93 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:34:08.517677: step 43560, loss = 0.89 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:34:09.799750: step 43570, loss = 0.80 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:34:11.086428: step 43580, loss = 0.78 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:34:12.396192: step 43590, loss = 0.76 (977.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:34:13.790336: step 43600, loss = 0.71 (918.1 examples/sec; 0.139 sec/batch)
2017-05-09 22:34:15.021751: step 43610, loss = 0.93 (1039.5 examples/sec; 0.123 sec/batch)
2017-05-09 22:34:16.312589: step 43620, loss = 0.74 (991.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:34:17.609576: step 43630, loss = 0.65 (986.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:34:18.917117: step 43640, loss = 0.91 (978.9 examples/sec; 0.131 sec/batch)
2017-05-09 22:34:20.176212: step 43650, loss = 0.98 (1016.6 examples/sec; 0.126 sec/batch)
2017-05-09 22:34:21.455221: step 43660, loss = 0.73 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:34:22.773732: step 43670, loss = 0.63 (970.8 examples/sec; 0.132 sec/batch)
2017-05-09 22:34:24.047672: step 43680, loss = 0.75 (1004.8 examples/sec; 0.127 seE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 888 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
c/batch)
2017-05-09 22:34:25.322535: step 43690, loss = 0.71 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:34:26.683372: step 43700, loss = 1.04 (940.6 examples/sec; 0.136 sec/batch)
2017-05-09 22:34:27.902321: step 43710, loss = 0.73 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-09 22:34:29.195741: step 43720, loss = 0.80 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:34:30.487972: step 43730, loss = 0.89 (990.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:34:31.799385: step 43740, loss = 0.73 (976.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:34:33.112831: step 43750, loss = 0.92 (974.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:34:34.369052: step 43760, loss = 0.68 (1018.9 examples/sec; 0.126 sec/batch)
2017-05-09 22:34:35.679403: step 43770, loss = 0.72 (976.9 examples/sec; 0.131 sec/batch)
2017-05-09 22:34:36.971681: step 43780, loss = 0.88 (990.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:34:38.265630: step 43790, loss = 0.81 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:34:39.633552: step 43800, loss = 0.67 (935.7 examples/sec; 0.137 sec/batch)
2017-05-09 22:34:40.804898: step 43810, loss = 0.70 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-09 22:34:42.088040: step 43820, loss = 0.72 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:34:43.381173: step 43830, loss = 0.72 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:34:44.659229: step 43840, loss = 0.85 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:34:45.973217: step 43850, loss = 0.95 (974.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:34:47.269554: step 43860, loss = 0.90 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:34:48.585858: step 43870, loss = 0.99 (972.4 examples/sec; 0.132 sec/batch)
2017-05-09 22:34:49.895271: step 43880, loss = 0.82 (977.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:34:51.214603: step 43890, loss = 0.83 (970.2 examples/sec; 0.132 sec/batch)
2017-05-09 22:34:52.597062: step 43900, loss = 0.89 (925.9 examples/sec; 0.138 sec/batch)
2017-05-09 22:34:53.770786: step 43910, loss = 0.75 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-09 22:34:55.039801: step 43920, loss = 0.89 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:34:56.317286: step 43930, loss = 0.77 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:34:57.612136: step 43940, loss = 0.75 (988.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:34:58.912688: step 43950, loss = 0.72 (984.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:35:00.212942: step 43960, loss = 0.86 (984.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:35:01.514931: step 43970, loss = 0.77 (983.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:35:02.834666: step 43980, loss = 0.76 (969.9 examples/sec; 0.132 sec/batch)
2017-05-09 22:35:04.116937: step 43990, loss = 0.63 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:35:05.494760: step 44000, loss = 0.72 (929.0 examples/sec; 0.138 sec/batch)
2017-05-09 22:35:06.715091: step 44010, loss = 0.82 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-09 22:35:08.006284: step 44020, loss = 0.81 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:35:09.317101: step 44030, loss = 0.66 (976.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:35:10.608236: step 44040, loss = 0.73 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:35:11.920146: step 44050, loss = 0.82 (975.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:35:13.189277: step 44060, loss = 0.80 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:35:14.487225: step 44070, loss = 0.65 (986.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:35:15.785755: step 44080, loss = 0.88 (985.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:35:17.081730: step 44090, loss = 0.75 (987.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:35:18.469682: step 44100, loss = 0.93 (922.2 examples/sec; 0.139 sec/batch)
2017-05-09 22:35:19.673948: step 44110, loss = 0.74 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-09 22:35:20.958234: step 44120, loss = 0.92 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:35:22.281955: step 44130, loss = 0.82 (967.0 examples/sec; 0.132 sec/batch)
2017-05-09 22:35:23.594425: step 44140, loss = 0.77 (975.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:35:24.883915: step 44150, loss = 0.56 (992.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:35:26.191133: step 44160, loss = 0.70 (979.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:35:27.496574: step 44170, loss = 0.94 (980.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:35:28.764133: step 44180, loss = 0.73 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:35:30.026882: step 44190, loss = 0.90 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-09 22:35:31.416801: step 44200, loss = 0.65 (920.9 examples/sec; 0.139 sec/batch)
2017-05-09 22:35:32.612123: step 44210, loss = 0.72 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-09 22:35:33.905575: step 44220, loss = 1.00 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:35:35.190676: step 44230, loss = 0.91 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:35:36.482074: step 44240, loss = 0.95 (991.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:35:37.777924: step 44250, loss = 0.85 (987.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:35:39.043842: step 44260, loss = 0.68 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:35:40.316781: step 44270, loss = 0.67 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:35:41.608052: step 44280, loss = 0.71 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:35:42.911916: step 44290, loss = 0.70 (981.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:35:44.292882: step 44300, loss = 0.74 (926.9 examples/sec; 0.138 sec/batch)
2017-05-09 22:35:45.473690: step 44310, loss = 0.74 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-09 22:35:46.772312: step 44320, loss = 0.82 (985.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:35:48.042751: step 44330, loss = 0.73 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:35:49.321145: step 44340, loss = 0.64 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:35:50.643712: step 44350, loss = 1.11 (967.8 examples/sec; 0.132 sec/batch)
2017-05-09 22:35:51.941081: step 44360, loss = 0.72 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:35:53.243439: step 44370, loss = 0.97 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:35:54.526618: step 44380, loss = 0.73 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:35:55.805911: step 44390, loss = 0.80 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:35:57.213027: step 44400, loss = 0.78 (909.7 examples/sec; 0.141 sec/batch)
2017-05-09 22:35:58.426242: step 44410, loss = 0.67 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-09 22:35:59.729775: step 44420, loss = 0.74 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:36:01.017996: step 44430, loss = 0.69 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:36:02.302103: step 44440, loss = 0.84 (996.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:36:03.565028: step 44450, loss = 0.77 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-09 22:36:04.880229: step 44460, loss = 0.64 (973.2 examples/sec; 0.132 sec/batch)
2017-05-09 22:36:06.145356: step 44470, loss = 0.73 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:36:07.412752: step 44480, loss = 0.59 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:36:08.706720: step 44490, loss = 0.81 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:36:10.084004: step 44500, loss = 0.87 (929.4 examples/sec; 0.138 sec/batch)
2017-05-09 22:36:11.291840: step 44510, loss = 0.84 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-09 22:36:12.558272: step 44520, loss = 0.79 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:36:13.838036: step 44530, loss = 0.64 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:36:15.131606: step 44540, loss = 0.68 (989.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:36:16.436425: step 44550, loss = 0.91 (981.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:36:17.750789: step 44560, loss = 0.95 (973.9 examples/sec; 0.131 sec/batch)
2017-05-09 22:36:19.057554: step 44570, loss = 0.70 (979.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:36:20.334433: step 44580, loss = 0.79 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:36:21.635791: step 44590, loss = 0.57 (98E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 908 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
3.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:36:23.009871: step 44600, loss = 0.80 (931.5 examples/sec; 0.137 sec/batch)
2017-05-09 22:36:24.189537: step 44610, loss = 0.71 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-09 22:36:25.485188: step 44620, loss = 0.81 (987.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:36:26.761569: step 44630, loss = 0.67 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:36:28.059286: step 44640, loss = 0.77 (986.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:36:29.347174: step 44650, loss = 0.91 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:36:30.624006: step 44660, loss = 0.95 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:36:31.931723: step 44670, loss = 0.72 (978.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:36:33.248596: step 44680, loss = 0.86 (972.0 examples/sec; 0.132 sec/batch)
2017-05-09 22:36:34.579101: step 44690, loss = 1.02 (962.0 examples/sec; 0.133 sec/batch)
2017-05-09 22:36:35.961635: step 44700, loss = 0.76 (925.8 examples/sec; 0.138 sec/batch)
2017-05-09 22:36:37.138023: step 44710, loss = 0.75 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-09 22:36:38.428582: step 44720, loss = 0.86 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:36:39.713733: step 44730, loss = 0.76 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:36:40.989185: step 44740, loss = 0.76 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:36:42.274968: step 44750, loss = 0.83 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:36:43.552827: step 44760, loss = 0.84 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:36:44.864512: step 44770, loss = 0.77 (975.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:36:46.202314: step 44780, loss = 0.90 (956.8 examples/sec; 0.134 sec/batch)
2017-05-09 22:36:47.495795: step 44790, loss = 0.70 (989.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:36:48.890534: step 44800, loss = 0.76 (917.7 examples/sec; 0.139 sec/batch)
2017-05-09 22:36:50.120117: step 44810, loss = 0.79 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-09 22:36:51.398206: step 44820, loss = 0.78 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:36:52.673893: step 44830, loss = 0.95 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:36:53.989985: step 44840, loss = 0.61 (972.6 examples/sec; 0.132 sec/batch)
2017-05-09 22:36:55.281727: step 44850, loss = 0.72 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:36:56.556027: step 44860, loss = 0.78 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:36:57.844185: step 44870, loss = 0.71 (993.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:36:59.118351: step 44880, loss = 0.69 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:37:00.401696: step 44890, loss = 0.80 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:37:01.784162: step 44900, loss = 0.89 (925.9 examples/sec; 0.138 sec/batch)
2017-05-09 22:37:02.969328: step 44910, loss = 0.91 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-09 22:37:04.264411: step 44920, loss = 0.73 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:37:05.539580: step 44930, loss = 0.80 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:37:06.848296: step 44940, loss = 0.91 (978.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:37:08.156296: step 44950, loss = 0.65 (978.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:37:09.439733: step 44960, loss = 0.72 (997.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:37:10.729272: step 44970, loss = 0.81 (992.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:37:12.019744: step 44980, loss = 0.89 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:37:13.328249: step 44990, loss = 0.73 (978.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:37:14.703141: step 45000, loss = 0.90 (931.0 examples/sec; 0.137 sec/batch)
2017-05-09 22:37:15.921888: step 45010, loss = 0.94 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-09 22:37:17.228048: step 45020, loss = 0.80 (980.0 examples/sec; 0.131 sec/batch)
2017-05-09 22:37:18.516831: step 45030, loss = 0.67 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:37:19.804111: step 45040, loss = 0.66 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:37:21.077705: step 45050, loss = 0.71 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:37:22.347882: step 45060, loss = 0.71 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:37:23.616882: step 45070, loss = 0.78 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:37:24.890843: step 45080, loss = 0.78 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:37:26.171973: step 45090, loss = 0.68 (999.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:37:27.533823: step 45100, loss = 0.76 (939.9 examples/sec; 0.136 sec/batch)
2017-05-09 22:37:28.728435: step 45110, loss = 0.71 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-09 22:37:30.011305: step 45120, loss = 0.70 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:37:31.291472: step 45130, loss = 0.82 (999.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:37:32.572291: step 45140, loss = 0.79 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:37:33.867906: step 45150, loss = 0.76 (987.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:37:35.162816: step 45160, loss = 0.75 (988.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:37:36.427748: step 45170, loss = 0.68 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-09 22:37:37.730759: step 45180, loss = 0.68 (982.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:37:39.057358: step 45190, loss = 0.72 (964.9 examples/sec; 0.133 sec/batch)
2017-05-09 22:37:40.437295: step 45200, loss = 0.71 (927.6 examples/sec; 0.138 sec/batch)
2017-05-09 22:37:41.642727: step 45210, loss = 0.70 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-09 22:37:42.916699: step 45220, loss = 0.87 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:37:44.178397: step 45230, loss = 0.62 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-09 22:37:45.490763: step 45240, loss = 0.62 (975.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:37:46.784626: step 45250, loss = 0.82 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:37:48.047358: step 45260, loss = 0.72 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-09 22:37:49.317039: step 45270, loss = 0.80 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:37:50.599602: step 45280, loss = 1.04 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:37:51.873214: step 45290, loss = 0.89 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:37:53.241966: step 45300, loss = 0.68 (935.2 examples/sec; 0.137 sec/batch)
2017-05-09 22:37:54.485792: step 45310, loss = 0.76 (1029.1 examples/sec; 0.124 sec/batch)
2017-05-09 22:37:55.775249: step 45320, loss = 0.72 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:37:57.062811: step 45330, loss = 0.86 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:37:58.355168: step 45340, loss = 0.75 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:37:59.631972: step 45350, loss = 0.74 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:38:00.929046: step 45360, loss = 0.58 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:38:02.198472: step 45370, loss = 0.80 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:38:03.492491: step 45380, loss = 0.83 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:38:04.774899: step 45390, loss = 0.88 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:38:06.138490: step 45400, loss = 0.74 (938.7 examples/sec; 0.136 sec/batch)
2017-05-09 22:38:07.382859: step 45410, loss = 0.83 (1028.6 examples/sec; 0.124 sec/batch)
2017-05-09 22:38:08.685191: step 45420, loss = 0.82 (982.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:38:09.961839: step 45430, loss = 0.91 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:38:11.255054: step 45440, loss = 0.89 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:38:12.542694: step 45450, loss = 0.72 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:38:13.818084: step 45460, loss = 0.73 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:38:15.131773: step 45470, loss = 0.98 (974.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:38:16.439252: step 45480, loss = 0.97 (979.0 examples/sec; 0.131 sec/batch)
2017-05-09 22:38:17.742882: step 45490, loss = 0.69 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:38:19.13321E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 928 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
5: step 45500, loss = 0.69 (920.6 examples/sec; 0.139 sec/batch)
2017-05-09 22:38:20.319501: step 45510, loss = 0.80 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-09 22:38:21.614722: step 45520, loss = 0.80 (988.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:38:22.887837: step 45530, loss = 0.96 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:38:24.196527: step 45540, loss = 0.72 (978.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:38:25.489410: step 45550, loss = 0.76 (990.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:38:26.787896: step 45560, loss = 0.70 (985.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:38:28.058168: step 45570, loss = 0.78 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:38:29.375233: step 45580, loss = 0.85 (971.9 examples/sec; 0.132 sec/batch)
2017-05-09 22:38:30.686115: step 45590, loss = 0.98 (976.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:38:32.064996: step 45600, loss = 0.83 (928.3 examples/sec; 0.138 sec/batch)
2017-05-09 22:38:33.277641: step 45610, loss = 0.69 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-09 22:38:34.570322: step 45620, loss = 0.85 (990.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:38:35.848562: step 45630, loss = 1.00 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:38:37.115300: step 45640, loss = 0.82 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:38:38.391645: step 45650, loss = 0.64 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:38:39.658484: step 45660, loss = 0.80 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:38:40.958196: step 45670, loss = 0.85 (984.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:38:42.251373: step 45680, loss = 0.71 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:38:43.538576: step 45690, loss = 0.76 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:38:44.930287: step 45700, loss = 0.83 (919.7 examples/sec; 0.139 sec/batch)
2017-05-09 22:38:46.169965: step 45710, loss = 0.99 (1032.5 examples/sec; 0.124 sec/batch)
2017-05-09 22:38:47.459355: step 45720, loss = 0.69 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:38:48.756372: step 45730, loss = 0.87 (986.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:38:50.053502: step 45740, loss = 0.88 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:38:51.355022: step 45750, loss = 0.70 (983.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:38:52.657018: step 45760, loss = 0.89 (983.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:38:53.924020: step 45770, loss = 0.90 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:38:55.239149: step 45780, loss = 0.84 (973.3 examples/sec; 0.132 sec/batch)
2017-05-09 22:38:56.580340: step 45790, loss = 0.76 (954.4 examples/sec; 0.134 sec/batch)
2017-05-09 22:38:57.948128: step 45800, loss = 0.79 (935.8 examples/sec; 0.137 sec/batch)
2017-05-09 22:38:59.157691: step 45810, loss = 0.71 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-09 22:39:00.460332: step 45820, loss = 0.88 (982.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:39:01.730368: step 45830, loss = 0.86 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:39:03.012360: step 45840, loss = 0.67 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:39:04.305299: step 45850, loss = 0.77 (990.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:39:05.610325: step 45860, loss = 0.67 (980.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:39:06.908341: step 45870, loss = 0.57 (986.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:39:08.200638: step 45880, loss = 0.89 (990.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:39:09.495097: step 45890, loss = 0.71 (988.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:39:10.888416: step 45900, loss = 0.84 (918.7 examples/sec; 0.139 sec/batch)
2017-05-09 22:39:12.089431: step 45910, loss = 0.63 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-09 22:39:13.400403: step 45920, loss = 0.85 (976.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:39:14.732821: step 45930, loss = 0.86 (960.7 examples/sec; 0.133 sec/batch)
2017-05-09 22:39:16.041975: step 45940, loss = 0.72 (977.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:39:17.333396: step 45950, loss = 0.95 (991.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:39:18.655003: step 45960, loss = 0.84 (968.5 examples/sec; 0.132 sec/batch)
2017-05-09 22:39:19.950428: step 45970, loss = 0.66 (988.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:39:21.285741: step 45980, loss = 0.82 (958.6 examples/sec; 0.134 sec/batch)
2017-05-09 22:39:22.593328: step 45990, loss = 0.96 (978.9 examples/sec; 0.131 sec/batch)
2017-05-09 22:39:23.976931: step 46000, loss = 0.75 (925.1 examples/sec; 0.138 sec/batch)
2017-05-09 22:39:25.188030: step 46010, loss = 0.80 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-09 22:39:26.465771: step 46020, loss = 0.77 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:39:27.731588: step 46030, loss = 0.57 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:39:29.017092: step 46040, loss = 0.66 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:39:30.307613: step 46050, loss = 0.70 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:39:31.596485: step 46060, loss = 0.88 (993.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:39:32.890842: step 46070, loss = 0.75 (988.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:39:34.205677: step 46080, loss = 0.94 (973.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:39:35.520361: step 46090, loss = 0.75 (973.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:39:36.907774: step 46100, loss = 0.83 (922.6 examples/sec; 0.139 sec/batch)
2017-05-09 22:39:38.123073: step 46110, loss = 0.67 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-09 22:39:39.417677: step 46120, loss = 0.73 (988.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:39:40.758899: step 46130, loss = 0.84 (954.4 examples/sec; 0.134 sec/batch)
2017-05-09 22:39:42.075121: step 46140, loss = 1.00 (972.5 examples/sec; 0.132 sec/batch)
2017-05-09 22:39:43.390151: step 46150, loss = 0.71 (973.4 examples/sec; 0.132 sec/batch)
2017-05-09 22:39:44.680520: step 46160, loss = 0.79 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:39:46.007630: step 46170, loss = 0.85 (964.5 examples/sec; 0.133 sec/batch)
2017-05-09 22:39:47.301426: step 46180, loss = 0.76 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:39:48.601717: step 46190, loss = 0.97 (984.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:39:50.071187: step 46200, loss = 0.90 (871.1 examples/sec; 0.147 sec/batch)
2017-05-09 22:39:51.216719: step 46210, loss = 0.72 (1117.4 examples/sec; 0.115 sec/batch)
2017-05-09 22:39:52.498441: step 46220, loss = 0.76 (998.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:39:53.787780: step 46230, loss = 0.77 (992.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:39:55.085187: step 46240, loss = 0.67 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:39:56.383635: step 46250, loss = 0.78 (985.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:39:57.652378: step 46260, loss = 0.89 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:39:58.950127: step 46270, loss = 0.81 (986.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:40:00.230026: step 46280, loss = 0.98 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:40:01.510404: step 46290, loss = 0.84 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:40:02.906064: step 46300, loss = 0.79 (917.1 examples/sec; 0.140 sec/batch)
2017-05-09 22:40:04.136091: step 46310, loss = 0.87 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-09 22:40:05.441493: step 46320, loss = 0.71 (980.6 examples/sec; 0.131 sec/batch)
2017-05-09 22:40:06.722366: step 46330, loss = 0.73 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:40:07.994264: step 46340, loss = 0.91 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:40:09.287786: step 46350, loss = 0.78 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:40:10.581497: step 46360, loss = 0.81 (989.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:40:11.872488: step 46370, loss = 0.83 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:40:13.181624: step 46380, loss = 0.75 (977.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:40:14.475055: step 46390, loss = 0.70 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:40:15.863242: step 46400, loss = 0.78 (922.1 examples/sec; 0.139 sec/batch)
2017-E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 949 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
05-09 22:40:17.109173: step 46410, loss = 0.80 (1027.3 examples/sec; 0.125 sec/batch)
2017-05-09 22:40:18.404339: step 46420, loss = 0.84 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:40:19.685997: step 46430, loss = 0.87 (998.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:40:21.004314: step 46440, loss = 0.77 (970.9 examples/sec; 0.132 sec/batch)
2017-05-09 22:40:22.301227: step 46450, loss = 0.64 (987.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:40:23.566616: step 46460, loss = 0.73 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:40:24.880968: step 46470, loss = 0.72 (973.9 examples/sec; 0.131 sec/batch)
2017-05-09 22:40:26.166803: step 46480, loss = 0.86 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:40:27.472485: step 46490, loss = 1.07 (980.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:40:28.878097: step 46500, loss = 0.78 (910.6 examples/sec; 0.141 sec/batch)
2017-05-09 22:40:30.079670: step 46510, loss = 0.60 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-09 22:40:31.381282: step 46520, loss = 1.16 (983.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:40:32.656193: step 46530, loss = 0.79 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:40:33.922113: step 46540, loss = 0.71 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:40:35.209248: step 46550, loss = 0.57 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:40:36.492558: step 46560, loss = 1.05 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:40:37.780659: step 46570, loss = 0.78 (993.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:40:39.074018: step 46580, loss = 0.82 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:40:40.363870: step 46590, loss = 0.79 (992.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:40:41.729902: step 46600, loss = 0.78 (937.0 examples/sec; 0.137 sec/batch)
2017-05-09 22:40:42.932096: step 46610, loss = 0.90 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-09 22:40:44.205046: step 46620, loss = 0.83 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:40:45.484276: step 46630, loss = 0.80 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:40:46.777749: step 46640, loss = 0.86 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:40:48.066048: step 46650, loss = 0.80 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:40:49.349952: step 46660, loss = 0.81 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:40:50.612718: step 46670, loss = 0.69 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-09 22:40:51.893491: step 46680, loss = 0.70 (999.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:40:53.191440: step 46690, loss = 0.85 (986.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:40:54.588151: step 46700, loss = 0.76 (916.4 examples/sec; 0.140 sec/batch)
2017-05-09 22:40:55.798657: step 46710, loss = 0.68 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-09 22:40:57.069190: step 46720, loss = 0.73 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:40:58.385520: step 46730, loss = 0.73 (972.4 examples/sec; 0.132 sec/batch)
2017-05-09 22:40:59.680881: step 46740, loss = 0.82 (988.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:41:00.965183: step 46750, loss = 0.70 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:41:02.264980: step 46760, loss = 0.75 (984.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:41:03.558440: step 46770, loss = 0.70 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:41:04.837152: step 46780, loss = 0.63 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:41:06.110506: step 46790, loss = 0.83 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:41:07.470968: step 46800, loss = 0.85 (940.9 examples/sec; 0.136 sec/batch)
2017-05-09 22:41:08.664928: step 46810, loss = 0.84 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-09 22:41:09.968570: step 46820, loss = 0.69 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:41:11.260325: step 46830, loss = 0.78 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:41:12.571850: step 46840, loss = 0.74 (976.0 examples/sec; 0.131 sec/batch)
2017-05-09 22:41:13.854771: step 46850, loss = 0.83 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:41:15.162384: step 46860, loss = 1.07 (978.9 examples/sec; 0.131 sec/batch)
2017-05-09 22:41:16.483257: step 46870, loss = 0.64 (969.1 examples/sec; 0.132 sec/batch)
2017-05-09 22:41:17.770752: step 46880, loss = 0.68 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:41:19.051160: step 46890, loss = 0.86 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:41:20.437868: step 46900, loss = 0.69 (923.0 examples/sec; 0.139 sec/batch)
2017-05-09 22:41:21.642984: step 46910, loss = 0.89 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-09 22:41:22.928915: step 46920, loss = 0.81 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:41:24.224388: step 46930, loss = 0.65 (988.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:41:25.527950: step 46940, loss = 0.73 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:41:26.845499: step 46950, loss = 0.67 (971.5 examples/sec; 0.132 sec/batch)
2017-05-09 22:41:28.133502: step 46960, loss = 0.81 (993.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:41:29.428943: step 46970, loss = 0.67 (988.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:41:30.715378: step 46980, loss = 0.68 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:41:31.996835: step 46990, loss = 0.68 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:41:33.364733: step 47000, loss = 0.75 (935.7 examples/sec; 0.137 sec/batch)
2017-05-09 22:41:34.560872: step 47010, loss = 0.74 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-09 22:41:35.858928: step 47020, loss = 0.76 (986.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:41:37.164070: step 47030, loss = 0.87 (980.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:41:38.450742: step 47040, loss = 0.61 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:41:39.712944: step 47050, loss = 0.62 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-09 22:41:40.982336: step 47060, loss = 0.62 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:41:42.261481: step 47070, loss = 0.67 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:41:43.534040: step 47080, loss = 0.92 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:41:44.832551: step 47090, loss = 0.82 (985.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:41:46.199295: step 47100, loss = 0.84 (936.5 examples/sec; 0.137 sec/batch)
2017-05-09 22:41:47.381866: step 47110, loss = 0.94 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-09 22:41:48.653256: step 47120, loss = 0.82 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:41:49.946394: step 47130, loss = 0.86 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:41:51.222854: step 47140, loss = 0.74 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:41:52.511191: step 47150, loss = 0.63 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:41:53.788047: step 47160, loss = 0.72 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:41:55.080195: step 47170, loss = 0.91 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:41:56.351021: step 47180, loss = 0.84 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:41:57.615138: step 47190, loss = 0.69 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-09 22:41:59.038298: step 47200, loss = 0.92 (899.4 examples/sec; 0.142 sec/batch)
2017-05-09 22:42:00.182127: step 47210, loss = 0.89 (1119.0 examples/sec; 0.114 sec/batch)
2017-05-09 22:42:01.457134: step 47220, loss = 0.83 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:42:02.731748: step 47230, loss = 0.81 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:42:04.014419: step 47240, loss = 0.83 (997.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:42:05.298851: step 47250, loss = 0.69 (996.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:42:06.605313: step 47260, loss = 0.74 (979.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:42:07.892956: step 47270, loss = 0.88 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:42:09.186290: step 47280, loss = 0.70 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:42:10.439169: step 47290, loss = 0.76 (1021.6 examples/sec; 0.125 sec/batch)
2017-05-09 22:42:11.826656: step 47300, loss = 0.77 (922.5 examples/sec; 0.139 sec/batch)
2017-05-09 22:42:13.020328: step 47310, loss = 0.72 (1072.3 exampleE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 969 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
s/sec; 0.119 sec/batch)
2017-05-09 22:42:14.302852: step 47320, loss = 0.83 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:42:15.586515: step 47330, loss = 0.73 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:42:16.871376: step 47340, loss = 0.72 (996.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:42:18.145667: step 47350, loss = 0.84 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:42:19.441078: step 47360, loss = 0.84 (988.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:42:20.757235: step 47370, loss = 0.69 (972.5 examples/sec; 0.132 sec/batch)
2017-05-09 22:42:22.044225: step 47380, loss = 0.93 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:42:23.351584: step 47390, loss = 0.89 (979.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:42:24.733354: step 47400, loss = 0.80 (926.3 examples/sec; 0.138 sec/batch)
2017-05-09 22:42:25.941243: step 47410, loss = 0.93 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-09 22:42:27.245856: step 47420, loss = 0.62 (981.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:42:28.538844: step 47430, loss = 0.78 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:42:29.867850: step 47440, loss = 0.75 (963.1 examples/sec; 0.133 sec/batch)
2017-05-09 22:42:31.146605: step 47450, loss = 1.16 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:42:32.474955: step 47460, loss = 0.81 (963.6 examples/sec; 0.133 sec/batch)
2017-05-09 22:42:33.765089: step 47470, loss = 0.85 (992.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:42:35.044591: step 47480, loss = 0.89 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:42:36.310078: step 47490, loss = 0.86 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-09 22:42:37.693716: step 47500, loss = 0.83 (925.1 examples/sec; 0.138 sec/batch)
2017-05-09 22:42:38.881366: step 47510, loss = 0.62 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-09 22:42:40.153298: step 47520, loss = 0.83 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-09 22:42:41.445033: step 47530, loss = 0.71 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:42:42.746870: step 47540, loss = 0.72 (983.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:42:44.034862: step 47550, loss = 0.80 (993.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:42:45.312074: step 47560, loss = 0.73 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:42:46.592857: step 47570, loss = 0.76 (999.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:42:47.886016: step 47580, loss = 0.75 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:42:49.177182: step 47590, loss = 0.88 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:42:50.552016: step 47600, loss = 0.83 (931.0 examples/sec; 0.137 sec/batch)
2017-05-09 22:42:51.730012: step 47610, loss = 0.72 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-09 22:42:53.037920: step 47620, loss = 0.89 (978.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:42:54.335377: step 47630, loss = 0.77 (986.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:42:55.618389: step 47640, loss = 0.61 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:42:56.905007: step 47650, loss = 0.71 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:42:58.228766: step 47660, loss = 0.82 (966.9 examples/sec; 0.132 sec/batch)
2017-05-09 22:42:59.510603: step 47670, loss = 0.63 (998.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:43:00.784082: step 47680, loss = 0.64 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-09 22:43:02.065897: step 47690, loss = 0.78 (998.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:43:03.444875: step 47700, loss = 0.68 (928.2 examples/sec; 0.138 sec/batch)
2017-05-09 22:43:04.614311: step 47710, loss = 0.88 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-09 22:43:05.887663: step 47720, loss = 0.79 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:43:07.168518: step 47730, loss = 0.69 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:43:08.464901: step 47740, loss = 0.84 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:43:09.750782: step 47750, loss = 0.68 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:43:11.040498: step 47760, loss = 0.86 (992.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:43:12.315367: step 47770, loss = 0.71 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-09 22:43:13.633207: step 47780, loss = 0.86 (971.3 examples/sec; 0.132 sec/batch)
2017-05-09 22:43:14.923141: step 47790, loss = 0.78 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:43:16.297260: step 47800, loss = 0.71 (931.5 examples/sec; 0.137 sec/batch)
2017-05-09 22:43:17.503590: step 47810, loss = 0.65 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-09 22:43:18.801377: step 47820, loss = 0.74 (986.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:43:20.088717: step 47830, loss = 0.67 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:43:21.396985: step 47840, loss = 0.92 (978.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:43:22.709156: step 47850, loss = 0.94 (975.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:43:24.034712: step 47860, loss = 0.94 (965.6 examples/sec; 0.133 sec/batch)
2017-05-09 22:43:25.304242: step 47870, loss = 0.78 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:43:26.625560: step 47880, loss = 0.79 (968.7 examples/sec; 0.132 sec/batch)
2017-05-09 22:43:27.918114: step 47890, loss = 0.73 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:43:29.284314: step 47900, loss = 0.69 (936.9 examples/sec; 0.137 sec/batch)
2017-05-09 22:43:30.474533: step 47910, loss = 0.78 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-09 22:43:31.774671: step 47920, loss = 0.88 (984.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:43:33.043680: step 47930, loss = 0.72 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:43:34.338019: step 47940, loss = 0.87 (988.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:43:35.615284: step 47950, loss = 0.69 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:43:36.911026: step 47960, loss = 0.60 (987.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:43:38.196888: step 47970, loss = 0.90 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:43:39.477570: step 47980, loss = 0.64 (999.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:43:40.789410: step 47990, loss = 0.74 (975.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:43:42.163281: step 48000, loss = 0.89 (931.7 examples/sec; 0.137 sec/batch)
2017-05-09 22:43:43.357087: step 48010, loss = 0.65 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-09 22:43:44.647705: step 48020, loss = 0.64 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:43:45.967518: step 48030, loss = 0.71 (969.8 examples/sec; 0.132 sec/batch)
2017-05-09 22:43:47.263345: step 48040, loss = 0.55 (987.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:43:48.556029: step 48050, loss = 0.63 (990.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:43:49.880428: step 48060, loss = 0.87 (966.5 examples/sec; 0.132 sec/batch)
2017-05-09 22:43:51.183566: step 48070, loss = 0.72 (982.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:43:52.449334: step 48080, loss = 0.88 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:43:53.714973: step 48090, loss = 0.63 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-09 22:43:55.097784: step 48100, loss = 0.76 (925.7 examples/sec; 0.138 sec/batch)
2017-05-09 22:43:56.286981: step 48110, loss = 0.70 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-09 22:43:57.603705: step 48120, loss = 0.85 (972.1 examples/sec; 0.132 sec/batch)
2017-05-09 22:43:58.907349: step 48130, loss = 1.06 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:44:00.204060: step 48140, loss = 0.85 (987.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:44:01.506218: step 48150, loss = 0.74 (983.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:44:02.826917: step 48160, loss = 0.78 (969.2 examples/sec; 0.132 sec/batch)
2017-05-09 22:44:04.136407: step 48170, loss = 0.72 (977.5 examples/sec; 0.131 sec/batch)
2017-05-09 22:44:05.442494: step 48180, loss = 0.84 (980.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:44:06.725891: step 48190, loss = 0.72 (997.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:44:08.106624: step 48200, loss = 0.83 (927.0 examples/sec; 0.138 sec/batch)
2017-05-09 22:44:09.277366: step 48210, loss = 0.81 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-09 22:44:10.583483: step 48220, loss = 0.61 (980.0 examples/sec; 0.131 sec/batch)
2017-05-09 22:44:11.865320: step 48230, loss = 0.66 (998.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:44:13.163368: step 48240, loss = 0.74 (986.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:44:14.466241: step 48250, loss = 0.76 (982.4 examples/sec; 0.130 sec/batch)
2017-05-09 22:44:15.733903: step 48260, loss = 0.76 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:44:17.014503: step 48270, loss = 0.79 (999.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:44:18.300876: step 48280, loss = 0.64 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:44:19.566051: step 48290, loss = 0.72 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-09 22:44:20.940160: step 48300, loss = 0.97 (931.5 examples/sec; 0.137 sec/batch)
2017-05-09 22:44:22.164483: step 48310, loss = 0.86 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-09 22:44:23.466347: step 48320, loss = 0.83 (983.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:44:24.747409: step 48330, loss = 0.83 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:44:26.022509: step 48340, loss = 0.71 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:44:27.296641: step 48350, loss = 0.85 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-09 22:44:28.579821: step 48360, loss = 0.90 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:44:29.874217: step 48370, loss = 0.89 (988.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:44:31.188841: step 48380, loss = 0.96 (973.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:44:32.445176: step 48390, loss = 0.67 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-09 22:44:33.841749: step 48400, loss = 0.75 (916.5 examples/sec; 0.140 sec/batch)
2017-05-09 22:44:35.062392: step 48410, loss = 0.66 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-09 22:44:36.339597: step 48420, loss = 0.66 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:44:37.633989: step 48430, loss = 0.69 (988.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:44:38.961000: step 48440, loss = 0.74 (964.6 examples/sec; 0.133 sec/batch)
2017-05-09 22:44:40.267886: step 48450, loss = 0.72 (979.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:44:41.553401: step 48460, loss = 0.73 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:44:42.860803: step 48470, loss = 0.76 (979.0 examples/sec; 0.131 sec/batch)
2017-05-09 22:44:44.148637: step 48480, loss = 0.82 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:44:45.429776: step 48490, loss = 0.98 (999.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:44:46.803433: step 48500, loss = 0.86 (931.8 examples/sec; 0.137 sec/batch)
2017-05-09 22:44:48.012351: step 48510, loss = 0.81 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-09 22:44:49.333183: step 48520, loss = 0.82 (969.1 examples/sec; 0.132 sec/batch)
2017-05-09 22:44:50.613327: step 48530, loss = 0.73 (999.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:44:51.915539: step 48540, loss = 0.83 (982.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:44:53.206369: step 48550, loss = 0.84 (991.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:44:54.524906: step 48560, loss = 0.74 (970.8 examples/sec; 0.132 sec/batch)
2017-05-09 22:44:55.813855: step 48570, loss = 0.84 (993.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:44:57.072233: step 48580, loss = 0.84 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-09 22:44:58.357706: step 48590, loss = 0.81 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:44:59.745967: step 48600, loss = 0.76 (922.0 examples/sec; 0.139 sec/batch)
2017-05-09 22:45:00.947486: step 48610, loss = 0.84 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-09 22:45:02.267801: step 48620, loss = 0.80 (969.5 examples/sec; 0.132 sec/batch)
2017-05-09 22:45:03.525749: step 48630, loss = 0.72 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-09 22:45:04.811158: step 48640, loss = 0.95 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:45:06.117015: step 48650, loss = 0.89 (980.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:45:07.394616: step 48660, loss = 0.84 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:45:08.708433: step 48670, loss = 0.73 (974.3 examples/sec; 0.131E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 989 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
 sec/batch)
2017-05-09 22:45:10.006356: step 48680, loss = 0.84 (986.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:45:11.320923: step 48690, loss = 0.85 (973.7 examples/sec; 0.131 sec/batch)
2017-05-09 22:45:12.687281: step 48700, loss = 0.64 (936.8 examples/sec; 0.137 sec/batch)
2017-05-09 22:45:13.890652: step 48710, loss = 0.79 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-09 22:45:15.188196: step 48720, loss = 0.83 (986.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:45:16.517296: step 48730, loss = 0.79 (963.1 examples/sec; 0.133 sec/batch)
2017-05-09 22:45:17.825699: step 48740, loss = 0.78 (978.3 examples/sec; 0.131 sec/batch)
2017-05-09 22:45:19.122349: step 48750, loss = 0.82 (987.2 examples/sec; 0.130 sec/batch)
2017-05-09 22:45:20.416900: step 48760, loss = 0.75 (988.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:45:21.730488: step 48770, loss = 0.77 (974.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:45:23.030064: step 48780, loss = 0.75 (984.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:45:24.323845: step 48790, loss = 0.96 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:45:25.718534: step 48800, loss = 0.70 (917.8 examples/sec; 0.139 sec/batch)
2017-05-09 22:45:26.918137: step 48810, loss = 0.82 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-09 22:45:28.195935: step 48820, loss = 0.87 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:45:29.479668: step 48830, loss = 0.64 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 22:45:30.764838: step 48840, loss = 0.57 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:45:32.071112: step 48850, loss = 0.87 (979.9 examples/sec; 0.131 sec/batch)
2017-05-09 22:45:33.368113: step 48860, loss = 0.80 (986.9 examples/sec; 0.130 sec/batch)
2017-05-09 22:45:34.674077: step 48870, loss = 0.86 (980.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:45:35.974226: step 48880, loss = 0.77 (984.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:45:37.277712: step 48890, loss = 0.80 (982.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:45:38.637413: step 48900, loss = 0.83 (941.4 examples/sec; 0.136 sec/batch)
2017-05-09 22:45:39.818845: step 48910, loss = 0.64 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-09 22:45:41.114422: step 48920, loss = 0.76 (988.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:45:42.400211: step 48930, loss = 0.65 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:45:43.695555: step 48940, loss = 0.65 (988.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:45:44.982003: step 48950, loss = 0.75 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:45:46.294791: step 48960, loss = 0.74 (975.0 examples/sec; 0.131 sec/batch)
2017-05-09 22:45:47.551214: step 48970, loss = 0.70 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-09 22:45:48.836887: step 48980, loss = 0.96 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:45:50.128819: step 48990, loss = 0.74 (990.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:45:51.505258: step 49000, loss = 0.63 (929.9 examples/sec; 0.138 sec/batch)
2017-05-09 22:45:52.689822: step 49010, loss = 0.95 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-09 22:45:53.987247: step 49020, loss = 0.58 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 22:45:55.262954: step 49030, loss = 0.85 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:45:56.555114: step 49040, loss = 0.78 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:45:57.842592: step 49050, loss = 0.62 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 22:45:59.125895: step 49060, loss = 0.73 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:46:00.404020: step 49070, loss = 0.74 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-09 22:46:01.666683: step 49080, loss = 0.73 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-09 22:46:02.957751: step 49090, loss = 0.70 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:46:04.370013: step 49100, loss = 0.82 (906.3 examples/sec; 0.141 sec/batch)
2017-05-09 22:46:05.533876: step 49110, loss = 0.82 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-09 22:46:06.816928: step 49120, loss = 0.75 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 22:46:08.102284: step 49130, loss = 0.72 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:46:09.382038: step 49140, loss = 0.88 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:46:10.659701: step 49150, loss = 0.61 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:46:11.950820: step 49160, loss = 0.76 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:46:13.264978: step 49170, loss = 0.84 (974.0 examples/sec; 0.131 sec/batch)
2017-05-09 22:46:14.556110: step 49180, loss = 0.76 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 22:46:15.863381: step 49190, loss = 0.95 (979.1 examples/sec; 0.131 sec/batch)
2017-05-09 22:46:17.250803: step 49200, loss = 0.79 (922.6 examples/sec; 0.139 sec/batch)
2017-05-09 22:46:18.431079: step 49210, loss = 0.69 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-09 22:46:19.706844: step 49220, loss = 0.64 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-09 22:46:20.989195: step 49230, loss = 0.92 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:46:22.289813: step 49240, loss = 0.90 (984.1 examples/sec; 0.130 sec/batch)
2017-05-09 22:46:23.591982: step 49250, loss = 0.94 (983.0 examples/sec; 0.130 sec/batch)
2017-05-09 22:46:24.878841: step 49260, loss = 0.91 (994.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:46:26.206319: step 49270, loss = 0.85 (964.2 examples/sec; 0.133 sec/batch)
2017-05-09 22:46:27.494639: step 49280, loss = 0.88 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:46:28.790869: step 49290, loss = 0.83 (987.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:46:30.187506: step 49300, loss = 0.73 (916.5 examples/sec; 0.140 sec/batch)
2017-05-09 22:46:31.387663: step 49310, loss = 0.64 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-09 22:46:32.688719: step 49320, loss = 0.80 (983.8 examples/sec; 0.130 sec/batch)
2017-05-09 22:46:33.991217: step 49330, loss = 0.83 (982.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:46:35.268825: step 49340, loss = 0.75 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 22:46:36.559746: step 49350, loss = 0.95 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:46:37.854185: step 49360, loss = 0.94 (988.8 examples/sec; 0.129 sec/batch)
2017-05-09 22:46:39.198803: step 49370, loss = 0.79 (951.9 examples/sec; 0.134 sec/batch)
2017-05-09 22:46:40.491918: step 49380, loss = 0.75 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:46:41.823417: step 49390, loss = 1.08 (961.3 examples/sec; 0.133 sec/batch)
2017-05-09 22:46:43.201300: step 49400, loss = 0.75 (929.0 examples/sec; 0.138 sec/batch)
2017-05-09 22:46:44.407886: step 49410, loss = 0.78 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-09 22:46:45.680424: step 49420, loss = 0.63 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-09 22:46:46.970858: step 49430, loss = 0.74 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:46:48.245961: step 49440, loss = 0.81 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:46:49.512303: step 49450, loss = 0.61 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-09 22:46:50.802730: step 49460, loss = 0.82 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:46:52.099977: step 49470, loss = 0.75 (986.7 examples/sec; 0.130 sec/batch)
2017-05-09 22:46:53.373342: step 49480, loss = 0.77 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-09 22:46:54.662550: step 49490, loss = 0.70 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 22:46:56.078994: step 49500, loss = 0.78 (903.7 examples/sec; 0.142 sec/batch)
2017-05-09 22:46:57.274496: step 49510, loss = 0.84 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-09 22:46:58.569420: step 49520, loss = 0.83 (988.5 examples/sec; 0.129 sec/batch)
2017-05-09 22:46:59.856972: step 49530, loss = 0.80 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:47:01.166813: step 49540, loss = 0.69 (977.2 examples/sec; 0.131 sec/batch)
2017-05-09 22:47:02.469830: step 49550, loss = 0.72 (982.3 examples/sec; 0.130 sec/batch)
2017-05-09 22:47:03.747574: step 49560, loss = 0.81 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-09 22:47:05.055877: step 49570, loss = 0.88 (978.4 examples/sec; 0.131 sec/batch)
2017-05-09 22:47:06.363505: step 49580, loss = 0.71 (978.E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1009 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1016 events to /tmp/cifar10_train/events.out.tfevents.1494377966.GHC44.GHC.ANDREW.CMU.EDU
9 examples/sec; 0.131 sec/batch)
2017-05-09 22:47:07.680100: step 49590, loss = 0.72 (972.2 examples/sec; 0.132 sec/batch)
2017-05-09 22:47:09.055612: step 49600, loss = 0.71 (930.6 examples/sec; 0.138 sec/batch)
2017-05-09 22:47:10.263642: step 49610, loss = 0.70 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-09 22:47:11.543120: step 49620, loss = 0.76 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 22:47:12.825731: step 49630, loss = 0.83 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:47:14.113032: step 49640, loss = 0.74 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 22:47:15.390178: step 49650, loss = 0.82 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-09 22:47:16.675661: step 49660, loss = 0.78 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 22:47:17.984689: step 49670, loss = 0.79 (977.8 examples/sec; 0.131 sec/batch)
2017-05-09 22:47:19.261239: step 49680, loss = 0.78 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:47:20.556034: step 49690, loss = 0.83 (988.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:47:22.005549: step 49700, loss = 0.86 (883.1 examples/sec; 0.145 sec/batch)
2017-05-09 22:47:23.180549: step 49710, loss = 0.83 (1089.4 examples/sec; 0.118 sec/batch)
2017-05-09 22:47:24.470844: step 49720, loss = 0.64 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 22:47:25.735057: step 49730, loss = 0.87 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-09 22:47:27.037881: step 49740, loss = 0.70 (982.5 examples/sec; 0.130 sec/batch)
2017-05-09 22:47:28.300109: step 49750, loss = 0.68 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-09 22:47:29.590244: step 49760, loss = 0.52 (992.1 examples/sec; 0.129 sec/batch)
2017-05-09 22:47:30.873153: step 49770, loss = 0.74 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 22:47:32.158851: step 49780, loss = 0.65 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 22:47:33.441404: step 49790, loss = 0.75 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 22:47:34.808538: step 49800, loss = 0.92 (936.3 examples/sec; 0.137 sec/batch)
2017-05-09 22:47:35.845431: step 49810, loss = 0.62 (1234.5 examples/sec; 0.104 sec/batch)
2017-05-09 22:47:36.727121: step 49820, loss = 0.87 (1451.8 examples/sec; 0.088 sec/batch)
2017-05-09 22:47:37.606709: step 49830, loss = 0.61 (1455.2 examples/sec; 0.088 sec/batch)
2017-05-09 22:47:38.497411: step 49840, loss = 0.84 (1437.1 examples/sec; 0.089 sec/batch)
2017-05-09 22:47:39.385336: step 49850, loss = 0.74 (1441.6 examples/sec; 0.089 sec/batch)
2017-05-09 22:47:40.272808: step 49860, loss = 0.73 (1442.3 examples/sec; 0.089 sec/batch)
2017-05-09 22:47:41.163180: step 49870, loss = 0.75 (1437.6 examples/sec; 0.089 sec/batch)
2017-05-09 22:47:42.040765: step 49880, loss = 0.85 (1458.5 examples/sec; 0.088 sec/batch)
2017-05-09 22:47:42.927239: step 49890, loss = 0.62 (1443.9 examples/sec; 0.089 sec/batch)
2017-05-09 22:47:43.887957: step 49900, loss = 0.80 (1332.3 examples/sec; 0.096 sec/batch)
2017-05-09 22:47:44.671599: step 49910, loss = 0.77 (1633.4 examples/sec; 0.078 sec/batch)
2017-05-09 22:47:45.558070: step 49920, loss = 0.60 (1443.9 examples/sec; 0.089 sec/batch)
2017-05-09 22:47:46.426936: step 49930, loss = 0.68 (1473.2 examples/sec; 0.087 sec/batch)
2017-05-09 22:47:47.314004: step 49940, loss = 0.62 (1443.0 examples/sec; 0.089 sec/batch)
2017-05-09 22:47:48.172725: step 49950, loss = 0.71 (1490.6 examples/sec; 0.086 sec/batch)
2017-05-09 22:47:49.058699: step 49960, loss = 0.60 (1444.7 examples/sec; 0.089 sec/batch)
2017-05-09 22:47:49.951462: step 49970, loss = 0.88 (1433.7 examples/sec; 0.089 sec/batch)
2017-05-09 22:47:50.841375: step 49980, loss = 0.65 (1438.3 examples/sec; 0.089 sec/batch)
2017-05-09 22:47:51.727794: step 49990, loss = 0.83 (1444.0 examples/sec; 0.089 sec/batch)
2017-05-09 22:47:52.710216: step 50000, loss = 0.79 (1302.9 examples/sec; 0.098 sec/batch)
--- 6507.18118596 seconds ---
