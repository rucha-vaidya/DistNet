Connecting to port  12361
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Received variable values from ps
2017-05-07 20:12:37.672248: step 0, loss = 4.67 (80.7 examples/sec; 1.585 sec/batch)
2017-05-07 20:12:38.621012: step 10, loss = 4.62 (1349.2 examples/sec; 0.095 sec/batch)
2017-05-07 20:12:39.764127: step 20, loss = 4.55 (1119.7 examples/sec; 0.114 sec/batch)
2017-05-07 20:12:40.885291: step 30, loss = 4.65 (1141.7 examples/sec; 0.112 sec/batch)
2017-05-07 20:12:42.020060: step 40, loss = 4.44 (1128.0 examples/sec; 0.113 sec/batch)
2017-05-07 20:12:43.066503: step 50, loss = 4.32 (1223.2 examples/sec; 0.105 sec/batch)
2017-05-07 20:12:44.331558: step 60, loss = 4.42 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:12:45.587625: step 70, loss = 4.30 (1019.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:12:46.839154: step 80, loss = 4.14 (1022.7 examples/sec; 0.125 sec/batch)
2017-05-07 20:12:48.115414: step 90, loss = 4.41 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:12:49.535730: step 100, loss = 4.07 (901.2 examples/sec; 0.142 sec/batch)
2017-05-07 20:12:50.676706: step 110, loss = 4.22 (1121.8 examples/sec; 0.114 sec/batch)
2017-05-07 20:12:51.975152: step 120, loss = 4.10 (985.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:12:53.258025: step 130, loss = 4.03 (997.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:12:54.515153: step 140, loss = 4.07 (1018.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:12:55.786502: step 150, loss = 3.84 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:12:57.055096: step 160, loss = 3.83 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:12:58.325637: step 170, loss = 3.71 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:12:59.587574: step 180, loss = 3.60 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:13:00.839867: step 190, loss = 3.64 (1022.1 examples/sec; 0.125 sec/batch)
2017-05-07 20:13:02.204224: step 200, loss = 3.62 (938.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:13:03.352061: step 210, loss = 3.68 (1115.1 examples/sec; 0.115 sec/batch)
2017-05-07 20:13:04.621421: step 220, loss = 3.65 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:13:05.916297: step 230, loss = 3.66 (988.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:13:07.212315: step 240, loss = 3.63 (987.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:13:08.481250: step 250, loss = 3.53 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:13:09.751873: step 260, loss = 3.62 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:13:11.019408: step 270, loss = 3.47 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:13:12.306141: step 280, loss = 3.36 (994.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:13:13.567136: step 290, loss = 3.24 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:13:14.971515: step 300, loss = 3.39 (911.4 examples/sec; 0.140 sec/batch)
2017-05-07 20:13:16.114484: step 310, loss = 3.24 (1119.9 examples/sec; 0.114 sec/batch)
2017-05-07 20:13:17.367753: step 320, loss = 3.22 (1021.3 examples/sec; 0.125 sec/batch)
2017-05-07 20:13:18.663124: step 330, loss = 3.14 (988.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:13:19.960882: step 340, loss = 3.06 (986.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:13:21.258293: step 350, loss = 3.24 (986.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:13:22.525269: step 360, loss = 3.37 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:13:23.810589: step 370, loss = 3.24 (995.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:13:25.106947: step 380, loss = 3.16 (987.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:13:26.367736: step 390, loss = 2.93 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:13:27.751186: step 400, loss = 3.20 (925.2 examples/sec; 0.138 sec/batch)
2017-05-07 20:13:28.936010: step 410, loss = 2.98 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-07 20:13:30.207553: step 420, loss = 3.10 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:13:31.511214: step 430, loss = 2.91 (981.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:13:32.808626: step 440, loss = 2.98 (986.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:13:34.081063: step 450, loss = 2.87 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:13:35.359130: step 460, loss = 2.70 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:13:36.645293: step 470, loss = 2.94 (995.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:13:37.895480: step 480, loss = 3.01 (1023.8 examples/sec; 0.125 sec/batch)
2017-05-07 20:13:39.188163: step 490, loss = 2.94 (990.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:13:40.559985: step 500, loss = 2.74 (933.1 examples/sec; 0.137 sec/batch)
2017-05-07 20:13:41.735112: step 510, loss = 2.86 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-07 20:13:43.040153: step 520, loss = 2.61 (980.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:13:44.291188: step 530, loss = 2.65 (1023.1 examples/sec; 0.125 sec/batch)
2017-05-07 20:13:45.577717: step 540, loss = 2.66 (994.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:13:46.844107: step 550, loss = 2.60 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:13:48.122277: step 560, loss = 2.97 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:13:49.409925: step 570, loss = 2.63 (994.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:13:50.701320: step 580, loss = 2.66 (991.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:13:51.995533: step 590, loss = 2.62 (989.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:13:53.370637: step 600, loss = 2.58 (930.8 examples/sec; 0.138 sec/batch)
2017-05-07 20:13:54.552444: step 610, loss = 2.66 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-07 20:13:55.865213: step 620, loss = 2.73 (975.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:13:57.151793: step 630, loss = 2.78 (994.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:13:58.435607: step 640, loss = 2.58 (997.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:13:59.721199: step 650, loss = 2.50 (995.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:14:00.984565: step 660, loss = 2.52 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:14:02.233139: step 670, loss = 2.34 (1025.2 examples/sec; 0.125 sec/batch)
2017-05-07 20:14:03.523098: step 680, loss = 2.19 (992.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:14:04.825197: step 690, loss = 2.39 (983.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:14:06.193039: step 700, loss = 2.31 (935.8 examples/sec; 0.137 sec/batch)
2017-05-07 20:14:07.399829: step 710, loss = 2.27 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-07 20:14:08.702387: step 720, loss = 2.33 (982.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:14:09.972405: step 730, loss = 2.40 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:14:11.243760: step 740, loss = 2.16 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:14:12.507012: step 750, loss = 2.39 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:14:13.776241: step 760, loss = 2.28 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:14:15.069914: step 770, loss = 2.29 (989.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:14:16.341697: step 780, loss = 2.29 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:14:17.603320: step 790, loss = 2.55 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:14:18.985677: step 800, loss = 2.06 (926.0 examples/sec; 0.138 sec/batch)
2017-05-07 20:14:20.192872: step 810, loss = 2.16 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-07 20:14:21.500000: step 820, loss = 2.12 (979.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:14:22.763184: step 830, loss = 2.28 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:14:24.045063: step 840, loss = 2.09 (998.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:14:25.315752: step 850, loss = 1.95 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:14:26.595601: step 860, loss = 2.16 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:14:27.866708: step 870, loss = 2.34 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:14:29.131078: step 880, loss = 2.22 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:14:30.398911: step 890, loss = 2.07 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:14:31.765396: step 900, loss = 2.09 (936.7 examples/sec; 0.137 seE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 23 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
c/batch)
2017-05-07 20:14:32.942054: step 910, loss = 1.95 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-07 20:14:34.205324: step 920, loss = 1.98 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:14:35.477135: step 930, loss = 1.98 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:14:36.750590: step 940, loss = 1.98 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:14:37.985889: step 950, loss = 2.08 (1036.2 examples/sec; 0.124 sec/batch)
2017-05-07 20:14:39.287720: step 960, loss = 2.03 (983.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:14:40.587510: step 970, loss = 1.82 (984.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:14:41.859192: step 980, loss = 2.19 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:14:43.176011: step 990, loss = 2.05 (972.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:14:44.542273: step 1000, loss = 1.82 (936.8 examples/sec; 0.137 sec/batch)
2017-05-07 20:14:45.710456: step 1010, loss = 1.92 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-07 20:14:47.003441: step 1020, loss = 2.09 (990.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:14:48.283678: step 1030, loss = 1.87 (999.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:14:49.561788: step 1040, loss = 1.87 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:14:50.858343: step 1050, loss = 1.93 (987.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:14:52.169585: step 1060, loss = 1.99 (976.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:14:53.446653: step 1070, loss = 1.81 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:14:54.714019: step 1080, loss = 1.87 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:14:55.999836: step 1090, loss = 2.06 (995.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:14:57.366427: step 1100, loss = 1.84 (936.6 examples/sec; 0.137 sec/batch)
2017-05-07 20:14:58.534079: step 1110, loss = 1.64 (1096.2 examples/sec; 0.117 sec/batch)
2017-05-07 20:14:59.818355: step 1120, loss = 1.94 (996.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:15:01.107910: step 1130, loss = 2.10 (992.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:15:02.378597: step 1140, loss = 1.82 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:15:03.648726: step 1150, loss = 1.81 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:15:04.986470: step 1160, loss = 1.70 (956.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:15:06.278329: step 1170, loss = 1.99 (990.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:15:07.573361: step 1180, loss = 1.88 (988.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:15:08.876959: step 1190, loss = 1.94 (981.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:15:10.286709: step 1200, loss = 1.72 (908.0 examples/sec; 0.141 sec/batch)
2017-05-07 20:15:11.476433: step 1210, loss = 1.89 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-07 20:15:12.748948: step 1220, loss = 1.73 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:15:14.017004: step 1230, loss = 1.83 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:15:15.295101: step 1240, loss = 1.71 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:15:16.590093: step 1250, loss = 1.72 (988.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:15:17.912056: step 1260, loss = 1.80 (968.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:15:19.214748: step 1270, loss = 1.80 (982.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:15:20.498585: step 1280, loss = 1.65 (997.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:15:21.800050: step 1290, loss = 1.75 (983.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:15:23.191204: step 1300, loss = 2.07 (920.1 examples/sec; 0.139 sec/batch)
2017-05-07 20:15:24.390529: step 1310, loss = 1.65 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-07 20:15:25.670631: step 1320, loss = 1.77 (999.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:15:26.961585: step 1330, loss = 1.56 (991.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:15:28.258859: step 1340, loss = 2.03 (986.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:15:29.521032: step 1350, loss = 1.60 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:15:30.824551: step 1360, loss = 1.71 (981.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:15:32.107720: step 1370, loss = 1.61 (997.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:15:33.380538: step 1380, loss = 2.02 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:15:34.652094: step 1390, loss = 1.54 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:15:36.016722: step 1400, loss = 1.44 (938.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:15:37.182665: step 1410, loss = 1.33 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-07 20:15:38.483332: step 1420, loss = 1.44 (984.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:15:39.771819: step 1430, loss = 1.70 (993.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:15:41.108559: step 1440, loss = 1.76 (957.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:15:42.364953: step 1450, loss = 1.39 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:15:43.668415: step 1460, loss = 1.65 (982.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:15:44.972110: step 1470, loss = 1.86 (981.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:15:46.285412: step 1480, loss = 1.64 (974.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:15:47.573497: step 1490, loss = 1.37 (993.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:15:48.977714: step 1500, loss = 1.53 (911.5 examples/sec; 0.140 sec/batch)
2017-05-07 20:15:50.148001: step 1510, loss = 1.61 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-07 20:15:51.426357: step 1520, loss = 1.30 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:15:52.691155: step 1530, loss = 1.54 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:15:53.960845: step 1540, loss = 1.53 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:15:55.242812: step 1550, loss = 1.58 (998.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:15:56.531466: step 1560, loss = 1.47 (993.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:15:57.790835: step 1570, loss = 1.38 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:15:59.067181: step 1580, loss = 1.57 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:16:00.339545: step 1590, loss = 1.27 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:16:01.707723: step 1600, loss = 1.42 (935.5 examples/sec; 0.137 sec/batch)
2017-05-07 20:16:02.862221: step 1610, loss = 1.35 (1108.7 examples/sec; 0.115 sec/batch)
2017-05-07 20:16:04.166012: step 1620, loss = 1.51 (981.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:16:05.471449: step 1630, loss = 1.78 (980.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:16:06.771198: step 1640, loss = 1.64 (984.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:16:08.082445: step 1650, loss = 1.55 (976.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:16:09.354840: step 1660, loss = 1.52 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:16:10.636927: step 1670, loss = 1.46 (998.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:16:11.917618: step 1680, loss = 1.36 (999.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:16:13.215343: step 1690, loss = 1.56 (986.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:16:14.581185: step 1700, loss = 1.37 (937.1 examples/sec; 0.137 sec/batch)
2017-05-07 20:16:15.754441: step 1710, loss = 1.47 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-07 20:16:17.025443: step 1720, loss = 1.47 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:16:18.316958: step 1730, loss = 1.26 (990.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:16:19.623699: step 1740, loss = 1.59 (979.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:16:20.943821: step 1750, loss = 1.40 (969.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:16:22.229904: step 1760, loss = 1.28 (995.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:16:23.497845: step 1770, loss = 1.41 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:16:24.789800: step 1780, loss = 1.52 (990.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:16:26.099462: step 1790, loss = 1.53 (977.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:16:27.462620: step 1800, loss = 1.26 (939.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:16:28.680556: step 1810, loss = 1.49 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-07 20:16:29.975747: step 1820, loss = 1.34 (988.3E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 43 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
 examples/sec; 0.130 sec/batch)
2017-05-07 20:16:31.260540: step 1830, loss = 1.40 (996.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:16:32.548534: step 1840, loss = 1.52 (993.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:16:33.822409: step 1850, loss = 1.37 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:16:35.093572: step 1860, loss = 1.35 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:16:36.373423: step 1870, loss = 1.30 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:16:37.632021: step 1880, loss = 1.25 (1017.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:16:38.919791: step 1890, loss = 1.17 (994.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:16:40.302830: step 1900, loss = 1.59 (925.5 examples/sec; 0.138 sec/batch)
2017-05-07 20:16:41.500210: step 1910, loss = 1.51 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-07 20:16:42.767282: step 1920, loss = 1.44 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:16:44.042823: step 1930, loss = 1.41 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:16:45.334857: step 1940, loss = 1.25 (990.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:16:46.622077: step 1950, loss = 1.45 (994.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:16:47.923131: step 1960, loss = 1.51 (983.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:16:49.202329: step 1970, loss = 1.73 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:16:50.471169: step 1980, loss = 1.48 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:16:51.739943: step 1990, loss = 1.17 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:16:53.140549: step 2000, loss = 1.49 (913.9 examples/sec; 0.140 sec/batch)
2017-05-07 20:16:54.331911: step 2010, loss = 1.39 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-07 20:16:55.581032: step 2020, loss = 1.37 (1024.7 examples/sec; 0.125 sec/batch)
2017-05-07 20:16:56.856309: step 2030, loss = 1.20 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:16:58.120331: step 2040, loss = 1.42 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:16:59.420075: step 2050, loss = 1.28 (984.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:17:00.692020: step 2060, loss = 1.22 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:17:01.956365: step 2070, loss = 1.26 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:17:03.261527: step 2080, loss = 1.14 (980.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:17:04.534423: step 2090, loss = 1.49 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:17:05.897833: step 2100, loss = 1.34 (938.8 examples/sec; 0.136 sec/batch)
2017-05-07 20:17:07.097235: step 2110, loss = 1.08 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-07 20:17:08.380725: step 2120, loss = 1.33 (997.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:17:09.642459: step 2130, loss = 1.14 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:17:10.922149: step 2140, loss = 1.05 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:17:12.184533: step 2150, loss = 1.20 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:17:13.468805: step 2160, loss = 1.41 (996.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:17:14.773511: step 2170, loss = 1.16 (981.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:17:16.081735: step 2180, loss = 1.40 (978.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:17:17.360303: step 2190, loss = 1.19 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:17:18.721593: step 2200, loss = 1.22 (940.3 examples/sec; 0.136 sec/batch)
2017-05-07 20:17:19.906714: step 2210, loss = 1.16 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-07 20:17:21.176311: step 2220, loss = 1.28 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:17:22.442831: step 2230, loss = 1.21 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:17:23.728051: step 2240, loss = 1.08 (995.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:17:25.020286: step 2250, loss = 1.39 (990.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:17:26.323114: step 2260, loss = 1.39 (982.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:17:27.609738: step 2270, loss = 1.34 (994.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:17:28.892149: step 2280, loss = 1.36 (998.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:17:30.176332: step 2290, loss = 1.49 (996.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:17:31.560062: step 2300, loss = 1.35 (925.0 examples/sec; 0.138 sec/batch)
2017-05-07 20:17:32.775657: step 2310, loss = 1.22 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-07 20:17:34.069212: step 2320, loss = 1.13 (989.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:17:35.378624: step 2330, loss = 1.35 (977.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:17:36.665856: step 2340, loss = 1.12 (994.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:17:37.986469: step 2350, loss = 1.34 (969.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:17:39.298129: step 2360, loss = 1.31 (975.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:17:40.572563: step 2370, loss = 1.16 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:17:41.847584: step 2380, loss = 1.21 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:17:43.140288: step 2390, loss = 1.24 (990.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:17:44.505143: step 2400, loss = 1.11 (937.8 examples/sec; 0.136 sec/batch)
2017-05-07 20:17:45.676788: step 2410, loss = 1.20 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-07 20:17:46.945864: step 2420, loss = 1.17 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:17:48.214304: step 2430, loss = 1.18 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:17:49.503246: step 2440, loss = 1.20 (993.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:17:50.791533: step 2450, loss = 1.36 (993.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:17:52.056560: step 2460, loss = 1.20 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:17:53.344682: step 2470, loss = 1.72 (993.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:17:54.636537: step 2480, loss = 1.40 (990.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:17:55.916044: step 2490, loss = 1.24 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:17:57.301698: step 2500, loss = 1.30 (923.8 examples/sec; 0.139 sec/batch)
2017-05-07 20:17:58.485496: step 2510, loss = 1.28 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-07 20:17:59.757033: step 2520, loss = 1.34 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:18:01.062767: step 2530, loss = 1.26 (980.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:18:02.312053: step 2540, loss = 1.29 (1024.6 examples/sec; 0.125 sec/batch)
2017-05-07 20:18:03.577265: step 2550, loss = 1.28 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:18:04.865378: step 2560, loss = 1.42 (993.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:18:06.155368: step 2570, loss = 1.33 (992.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:18:07.409397: step 2580, loss = 1.27 (1020.7 examples/sec; 0.125 sec/batch)
2017-05-07 20:18:08.679269: step 2590, loss = 1.14 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:18:10.040555: step 2600, loss = 1.16 (940.3 examples/sec; 0.136 sec/batch)
2017-05-07 20:18:11.230344: step 2610, loss = 1.23 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-07 20:18:12.489401: step 2620, loss = 1.16 (1016.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:18:13.773413: step 2630, loss = 1.13 (996.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:18:15.056840: step 2640, loss = 1.12 (997.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:18:16.321297: step 2650, loss = 1.16 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:18:17.602906: step 2660, loss = 1.06 (998.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:18:18.890052: step 2670, loss = 1.28 (994.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:18:20.201482: step 2680, loss = 1.37 (976.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:18:21.476545: step 2690, loss = 1.15 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:18:22.854588: step 2700, loss = 1.22 (928.9 examples/sec; 0.138 sec/batch)
2017-05-07 20:18:24.007454: step 2710, loss = 1.10 (1110.3 examples/sec; 0.115 sec/batch)
2017-05-07 20:18:25.277180: step 2720, loss = 1.04 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:18:26.579638: step 2730, loss = 1.21 (982.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:18E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 63 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
:27.838981: step 2740, loss = 1.05 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:18:29.121909: step 2750, loss = 1.24 (997.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:18:30.385407: step 2760, loss = 1.25 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:18:31.635313: step 2770, loss = 1.34 (1024.1 examples/sec; 0.125 sec/batch)
2017-05-07 20:18:32.916362: step 2780, loss = 1.14 (999.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:18:34.183806: step 2790, loss = 1.07 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:18:35.552930: step 2800, loss = 1.22 (934.9 examples/sec; 0.137 sec/batch)
2017-05-07 20:18:36.738760: step 2810, loss = 1.17 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-07 20:18:38.028947: step 2820, loss = 1.11 (992.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:18:39.350281: step 2830, loss = 1.36 (968.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:18:40.644549: step 2840, loss = 1.36 (989.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:18:41.914887: step 2850, loss = 1.41 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:18:43.194243: step 2860, loss = 1.15 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:18:44.464238: step 2870, loss = 1.32 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:18:45.768789: step 2880, loss = 1.29 (981.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:18:47.095060: step 2890, loss = 1.33 (965.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:18:48.472546: step 2900, loss = 1.20 (929.2 examples/sec; 0.138 sec/batch)
2017-05-07 20:18:49.662330: step 2910, loss = 1.06 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-07 20:18:50.954693: step 2920, loss = 1.33 (990.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:18:52.240108: step 2930, loss = 1.13 (995.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:18:53.530170: step 2940, loss = 1.10 (992.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:18:54.795775: step 2950, loss = 1.23 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:18:56.098711: step 2960, loss = 1.01 (982.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:18:57.380136: step 2970, loss = 1.06 (998.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:18:58.647106: step 2980, loss = 1.11 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:18:59.961649: step 2990, loss = 1.02 (973.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:19:01.359416: step 3000, loss = 1.19 (915.7 examples/sec; 0.140 sec/batch)
2017-05-07 20:19:02.544480: step 3010, loss = 1.06 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-07 20:19:03.829328: step 3020, loss = 1.07 (996.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:19:05.129789: step 3030, loss = 1.23 (984.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:19:06.422123: step 3040, loss = 1.29 (990.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:19:07.694619: step 3050, loss = 1.26 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:19:08.970829: step 3060, loss = 1.17 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:19:10.287828: step 3070, loss = 1.14 (971.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:19:11.580575: step 3080, loss = 1.41 (990.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:19:12.867833: step 3090, loss = 1.41 (994.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:19:14.227413: step 3100, loss = 1.26 (941.5 examples/sec; 0.136 sec/batch)
2017-05-07 20:19:15.425311: step 3110, loss = 1.42 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-07 20:19:16.711676: step 3120, loss = 1.07 (995.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:19:17.989228: step 3130, loss = 1.17 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:19:19.277879: step 3140, loss = 1.39 (993.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:19:20.586920: step 3150, loss = 1.44 (977.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:19:21.895224: step 3160, loss = 1.30 (978.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:19:23.190152: step 3170, loss = 1.25 (988.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:19:24.505201: step 3180, loss = 1.32 (973.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:19:25.800619: step 3190, loss = 1.18 (988.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:19:27.173265: step 3200, loss = 1.15 (932.5 examples/sec; 0.137 sec/batch)
2017-05-07 20:19:28.405912: step 3210, loss = 1.31 (1038.4 examples/sec; 0.123 sec/batch)
2017-05-07 20:19:29.694489: step 3220, loss = 1.26 (993.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:19:31.007925: step 3230, loss = 1.11 (974.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:19:32.282203: step 3240, loss = 1.06 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:19:33.588279: step 3250, loss = 1.19 (980.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:19:34.853210: step 3260, loss = 1.23 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:19:36.116268: step 3270, loss = 1.00 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:19:37.376097: step 3280, loss = 0.93 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:19:38.641752: step 3290, loss = 1.16 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:19:40.034399: step 3300, loss = 1.23 (919.1 examples/sec; 0.139 sec/batch)
2017-05-07 20:19:41.209922: step 3310, loss = 1.14 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-07 20:19:42.470449: step 3320, loss = 0.99 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:19:43.759333: step 3330, loss = 0.97 (993.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:19:45.052339: step 3340, loss = 1.62 (989.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:19:46.298341: step 3350, loss = 1.17 (1027.3 examples/sec; 0.125 sec/batch)
2017-05-07 20:19:47.569661: step 3360, loss = 1.13 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:19:48.844810: step 3370, loss = 1.16 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:19:50.097874: step 3380, loss = 1.25 (1021.5 examples/sec; 0.125 sec/batch)
2017-05-07 20:19:51.400613: step 3390, loss = 1.31 (982.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:19:52.788392: step 3400, loss = 1.25 (922.3 examples/sec; 0.139 sec/batch)
2017-05-07 20:19:54.012207: step 3410, loss = 1.26 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-07 20:19:55.300598: step 3420, loss = 1.19 (993.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:19:56.598998: step 3430, loss = 1.46 (985.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:19:57.885075: step 3440, loss = 1.09 (995.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:19:59.151413: step 3450, loss = 1.12 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:20:00.443015: step 3460, loss = 1.22 (991.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:20:01.731882: step 3470, loss = 1.18 (993.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:20:03.046778: step 3480, loss = 1.02 (973.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:20:04.363010: step 3490, loss = 1.12 (972.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:20:05.728617: step 3500, loss = 1.27 (937.3 examples/sec; 0.137 sec/batch)
2017-05-07 20:20:06.946193: step 3510, loss = 1.07 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-07 20:20:08.233805: step 3520, loss = 1.25 (994.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:20:09.511327: step 3530, loss = 1.17 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:20:10.798775: step 3540, loss = 1.16 (994.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:20:12.093281: step 3550, loss = 1.10 (988.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:20:13.354122: step 3560, loss = 0.97 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:20:14.605515: step 3570, loss = 0.81 (1022.9 examples/sec; 0.125 sec/batch)
2017-05-07 20:20:15.890722: step 3580, loss = 1.28 (996.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:20:17.174634: step 3590, loss = 1.04 (996.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:20:18.531992: step 3600, loss = 1.49 (943.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:20:19.691688: step 3610, loss = 0.95 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-07 20:20:20.979685: step 3620, loss = 1.07 (993.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:20:22.248953: step 3630, loss = 1.16 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:20:23.527667: step 3640, loss = 1.04 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:20:24.795133: step 3650, loss = 1.05 (1009.9 examples/sec;E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 83 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
 0.127 sec/batch)
2017-05-07 20:20:26.070137: step 3660, loss = 1.17 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:20:27.367154: step 3670, loss = 0.80 (986.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:20:28.647335: step 3680, loss = 0.98 (999.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:20:29.929475: step 3690, loss = 1.07 (998.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:20:31.286998: step 3700, loss = 1.10 (942.9 examples/sec; 0.136 sec/batch)
2017-05-07 20:20:32.500847: step 3710, loss = 1.56 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-07 20:20:33.800060: step 3720, loss = 1.09 (985.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:20:35.081726: step 3730, loss = 0.96 (998.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:20:36.359189: step 3740, loss = 1.06 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:20:37.622797: step 3750, loss = 1.18 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:20:38.891322: step 3760, loss = 1.22 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:20:40.192313: step 3770, loss = 1.05 (983.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:20:41.464490: step 3780, loss = 1.08 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:20:42.749146: step 3790, loss = 0.83 (996.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:20:44.130448: step 3800, loss = 1.13 (926.7 examples/sec; 0.138 sec/batch)
2017-05-07 20:20:45.318846: step 3810, loss = 1.20 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-07 20:20:46.638533: step 3820, loss = 1.08 (969.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:20:47.960657: step 3830, loss = 1.16 (968.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:20:49.253706: step 3840, loss = 1.20 (989.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:20:50.533173: step 3850, loss = 0.91 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:20:51.797555: step 3860, loss = 0.96 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:20:53.067935: step 3870, loss = 1.16 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:20:54.342366: step 3880, loss = 1.19 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:20:55.626185: step 3890, loss = 1.04 (997.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:20:56.999819: step 3900, loss = 1.13 (931.8 examples/sec; 0.137 sec/batch)
2017-05-07 20:20:58.168118: step 3910, loss = 0.99 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-07 20:20:59.439659: step 3920, loss = 1.11 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:21:00.726391: step 3930, loss = 0.89 (994.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:21:01.994121: step 3940, loss = 1.13 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:21:03.263631: step 3950, loss = 1.15 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:21:04.534577: step 3960, loss = 1.08 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:21:05.828584: step 3970, loss = 0.94 (989.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:21:07.100419: step 3980, loss = 1.22 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:21:08.386387: step 3990, loss = 1.27 (995.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:21:09.779163: step 4000, loss = 1.12 (919.0 examples/sec; 0.139 sec/batch)
2017-05-07 20:21:10.940558: step 4010, loss = 1.21 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-07 20:21:12.246668: step 4020, loss = 1.04 (980.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:21:13.500791: step 4030, loss = 1.02 (1020.6 examples/sec; 0.125 sec/batch)
2017-05-07 20:21:14.771066: step 4040, loss = 1.15 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:21:16.065575: step 4050, loss = 1.00 (988.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:21:17.367300: step 4060, loss = 1.10 (983.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:21:18.654789: step 4070, loss = 0.95 (994.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:21:19.933127: step 4080, loss = 1.09 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:21:21.244903: step 4090, loss = 1.14 (975.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:21:22.608459: step 4100, loss = 1.36 (938.7 examples/sec; 0.136 sec/batch)
2017-05-07 20:21:23.825853: step 4110, loss = 1.07 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-07 20:21:25.126928: step 4120, loss = 1.05 (983.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:21:26.436958: step 4130, loss = 1.09 (977.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:21:27.741010: step 4140, loss = 0.99 (981.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:21:29.038360: step 4150, loss = 1.08 (986.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:21:30.335441: step 4160, loss = 1.14 (986.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:21:31.619745: step 4170, loss = 1.20 (996.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:21:32.936073: step 4180, loss = 1.08 (972.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:21:34.249842: step 4190, loss = 1.05 (974.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:21:35.622911: step 4200, loss = 1.21 (932.2 examples/sec; 0.137 sec/batch)
2017-05-07 20:21:36.847096: step 4210, loss = 1.05 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-07 20:21:38.128338: step 4220, loss = 0.90 (999.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:21:39.430824: step 4230, loss = 1.11 (982.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:21:40.729471: step 4240, loss = 1.19 (985.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:21:42.026224: step 4250, loss = 0.94 (987.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:21:43.326237: step 4260, loss = 0.84 (984.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:21:44.616933: step 4270, loss = 1.29 (991.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:21:45.919140: step 4280, loss = 1.24 (982.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:21:47.208612: step 4290, loss = 1.07 (992.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:21:48.596133: step 4300, loss = 0.97 (922.5 examples/sec; 0.139 sec/batch)
2017-05-07 20:21:49.771326: step 4310, loss = 1.09 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-07 20:21:51.028203: step 4320, loss = 1.00 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:21:52.302886: step 4330, loss = 1.07 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:21:53.584448: step 4340, loss = 1.05 (998.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:21:54.875454: step 4350, loss = 1.00 (991.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:21:56.142398: step 4360, loss = 1.12 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:21:57.421726: step 4370, loss = 0.96 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:21:58.687479: step 4380, loss = 1.10 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:21:59.963146: step 4390, loss = 1.47 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:22:01.351155: step 4400, loss = 1.14 (922.2 examples/sec; 0.139 sec/batch)
2017-05-07 20:22:02.552314: step 4410, loss = 1.09 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-07 20:22:03.817646: step 4420, loss = 0.81 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:22:05.091770: step 4430, loss = 1.07 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:22:06.377174: step 4440, loss = 1.10 (995.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:22:07.645058: step 4450, loss = 1.00 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:22:08.920873: step 4460, loss = 1.07 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:22:10.207504: step 4470, loss = 0.88 (994.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:22:11.509738: step 4480, loss = 1.07 (982.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:22:12.799567: step 4490, loss = 1.02 (992.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:22:14.156039: step 4500, loss = 1.44 (943.6 examples/sec; 0.136 sec/batch)
2017-05-07 20:22:15.363596: step 4510, loss = 1.11 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-07 20:22:16.641516: step 4520, loss = 1.18 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:22:17.953179: step 4530, loss = 1.25 (975.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:22:19.233044: step 4540, loss = 1.16 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:22:20.521346: step 4550, loss = 1.08 (993.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:22:21.800593: step 4560, loss = 0.90 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:22:23.073343: step 457E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 104 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
0, loss = 0.90 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:22:24.350858: step 4580, loss = 1.07 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:22:25.598256: step 4590, loss = 1.20 (1026.1 examples/sec; 0.125 sec/batch)
2017-05-07 20:22:27.006959: step 4600, loss = 0.93 (908.6 examples/sec; 0.141 sec/batch)
2017-05-07 20:22:28.185567: step 4610, loss = 1.28 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-07 20:22:29.478666: step 4620, loss = 0.99 (989.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:22:30.783650: step 4630, loss = 0.90 (980.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:22:32.083355: step 4640, loss = 0.98 (984.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:22:33.359895: step 4650, loss = 0.88 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:22:34.632848: step 4660, loss = 1.01 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:22:35.907920: step 4670, loss = 1.10 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:22:37.173377: step 4680, loss = 1.10 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:22:38.471880: step 4690, loss = 1.28 (985.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:22:39.860386: step 4700, loss = 0.97 (921.9 examples/sec; 0.139 sec/batch)
2017-05-07 20:22:41.072298: step 4710, loss = 1.04 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-07 20:22:42.345533: step 4720, loss = 1.16 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:22:43.621694: step 4730, loss = 1.01 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:22:44.899344: step 4740, loss = 1.16 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:22:46.166685: step 4750, loss = 0.92 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:22:47.437481: step 4760, loss = 1.10 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:22:48.707767: step 4770, loss = 1.07 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:22:49.998659: step 4780, loss = 1.11 (991.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:22:51.272387: step 4790, loss = 1.04 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:22:52.649082: step 4800, loss = 0.91 (929.8 examples/sec; 0.138 sec/batch)
2017-05-07 20:22:53.805000: step 4810, loss = 1.21 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-07 20:22:55.096219: step 4820, loss = 1.03 (991.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:22:56.432494: step 4830, loss = 1.01 (957.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:22:57.694670: step 4840, loss = 0.98 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:22:58.959624: step 4850, loss = 1.11 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:23:00.262302: step 4860, loss = 0.92 (982.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:23:01.538869: step 4870, loss = 0.86 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:23:02.809534: step 4880, loss = 1.03 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:23:04.107414: step 4890, loss = 1.18 (986.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:23:05.483797: step 4900, loss = 1.07 (930.0 examples/sec; 0.138 sec/batch)
2017-05-07 20:23:06.684241: step 4910, loss = 0.85 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-07 20:23:07.967260: step 4920, loss = 1.26 (997.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:23:09.252541: step 4930, loss = 1.06 (995.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:23:10.537657: step 4940, loss = 1.04 (996.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:23:11.832432: step 4950, loss = 1.04 (988.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:23:13.107750: step 4960, loss = 1.11 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:23:14.397262: step 4970, loss = 1.00 (992.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:23:15.714054: step 4980, loss = 1.50 (972.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:23:16.991355: step 4990, loss = 1.24 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:23:18.393914: step 5000, loss = 1.06 (912.6 examples/sec; 0.140 sec/batch)
2017-05-07 20:23:19.584624: step 5010, loss = 1.27 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-07 20:23:20.865873: step 5020, loss = 1.09 (999.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:23:22.135496: step 5030, loss = 1.15 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:23:23.417536: step 5040, loss = 1.18 (998.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:23:24.719428: step 5050, loss = 1.12 (983.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:23:26.003240: step 5060, loss = 1.06 (997.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:23:27.321554: step 5070, loss = 1.11 (970.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:23:28.577500: step 5080, loss = 1.15 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:23:29.838343: step 5090, loss = 0.92 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:23:31.207822: step 5100, loss = 1.02 (934.7 examples/sec; 0.137 sec/batch)
2017-05-07 20:23:32.383212: step 5110, loss = 1.02 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-07 20:23:33.649807: step 5120, loss = 0.97 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:23:34.926191: step 5130, loss = 1.35 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:23:36.226212: step 5140, loss = 1.23 (984.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:23:37.522839: step 5150, loss = 1.50 (987.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:23:38.809422: step 5160, loss = 1.08 (994.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:23:40.113875: step 5170, loss = 0.96 (981.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:23:41.427740: step 5180, loss = 0.96 (974.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:23:42.732144: step 5190, loss = 1.08 (981.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:23:44.127739: step 5200, loss = 0.93 (917.2 examples/sec; 0.140 sec/batch)
2017-05-07 20:23:45.343234: step 5210, loss = 1.11 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-07 20:23:46.620309: step 5220, loss = 1.00 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:23:47.881159: step 5230, loss = 0.91 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:23:49.148478: step 5240, loss = 1.00 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:23:50.434814: step 5250, loss = 1.06 (995.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:23:51.716295: step 5260, loss = 1.00 (998.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:23:52.983018: step 5270, loss = 1.12 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:23:54.252263: step 5280, loss = 1.05 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:23:55.529127: step 5290, loss = 0.97 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:23:56.883110: step 5300, loss = 1.08 (945.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:23:58.101097: step 5310, loss = 1.09 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-07 20:23:59.386868: step 5320, loss = 1.14 (995.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:24:00.690350: step 5330, loss = 1.04 (982.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:24:01.962813: step 5340, loss = 1.03 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:24:03.239948: step 5350, loss = 1.23 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:24:04.536149: step 5360, loss = 1.05 (987.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:24:05.826541: step 5370, loss = 1.06 (991.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:24:07.113221: step 5380, loss = 1.10 (994.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:24:08.389678: step 5390, loss = 1.09 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:24:09.757040: step 5400, loss = 1.08 (936.1 examples/sec; 0.137 sec/batch)
2017-05-07 20:24:10.955169: step 5410, loss = 0.93 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-07 20:24:12.234085: step 5420, loss = 0.94 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:24:13.523433: step 5430, loss = 0.97 (992.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:24:14.828061: step 5440, loss = 1.11 (981.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:24:16.123566: step 5450, loss = 1.00 (988.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:24:17.413281: step 5460, loss = 1.17 (992.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:24:18.691363: step 5470, loss = 1.29 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:24:19.979372: step 5480, loss = 1.06 (993.8 examples/sec; 0.129 sec/baE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 124 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
tch)
2017-05-07 20:24:21.274294: step 5490, loss = 1.07 (988.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:24:22.647628: step 5500, loss = 0.98 (932.0 examples/sec; 0.137 sec/batch)
2017-05-07 20:24:23.854081: step 5510, loss = 0.90 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-07 20:24:25.172710: step 5520, loss = 1.05 (970.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:24:26.464048: step 5530, loss = 0.92 (991.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:24:27.742510: step 5540, loss = 1.45 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:24:29.019010: step 5550, loss = 1.02 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:24:30.309898: step 5560, loss = 0.99 (991.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:24:31.618022: step 5570, loss = 0.93 (978.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:24:32.926334: step 5580, loss = 1.05 (978.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:24:34.210586: step 5590, loss = 1.22 (996.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:24:35.616044: step 5600, loss = 1.21 (910.7 examples/sec; 0.141 sec/batch)
2017-05-07 20:24:36.794072: step 5610, loss = 1.10 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-07 20:24:38.081493: step 5620, loss = 1.03 (994.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:24:39.364039: step 5630, loss = 1.03 (998.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:24:40.640543: step 5640, loss = 0.97 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:24:41.926946: step 5650, loss = 1.02 (995.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:24:43.198790: step 5660, loss = 1.04 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:24:44.464753: step 5670, loss = 1.36 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:24:45.708524: step 5680, loss = 1.07 (1029.1 examples/sec; 0.124 sec/batch)
2017-05-07 20:24:46.990541: step 5690, loss = 1.03 (998.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:24:48.369011: step 5700, loss = 0.97 (928.6 examples/sec; 0.138 sec/batch)
2017-05-07 20:24:49.587004: step 5710, loss = 1.12 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-07 20:24:50.900347: step 5720, loss = 1.11 (974.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:24:52.196938: step 5730, loss = 0.91 (987.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:24:53.493008: step 5740, loss = 1.03 (987.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:24:54.786541: step 5750, loss = 1.28 (989.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:24:56.097182: step 5760, loss = 0.89 (976.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:24:57.370524: step 5770, loss = 0.96 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:24:58.682434: step 5780, loss = 0.89 (975.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:24:59.970749: step 5790, loss = 1.26 (993.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:25:01.364297: step 5800, loss = 1.17 (918.5 examples/sec; 0.139 sec/batch)
2017-05-07 20:25:02.596172: step 5810, loss = 0.94 (1039.1 examples/sec; 0.123 sec/batch)
2017-05-07 20:25:03.860067: step 5820, loss = 0.95 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:25:05.135657: step 5830, loss = 1.21 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:25:06.403522: step 5840, loss = 0.90 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:25:07.687135: step 5850, loss = 0.88 (997.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:25:08.979328: step 5860, loss = 1.04 (990.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:25:10.266767: step 5870, loss = 0.96 (994.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:25:11.581256: step 5880, loss = 0.97 (973.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:25:12.872337: step 5890, loss = 1.46 (991.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:25:14.264001: step 5900, loss = 0.98 (919.8 examples/sec; 0.139 sec/batch)
2017-05-07 20:25:15.490121: step 5910, loss = 1.08 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-07 20:25:16.800165: step 5920, loss = 1.06 (977.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:25:18.065549: step 5930, loss = 1.31 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:25:19.350341: step 5940, loss = 0.97 (996.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:25:20.634382: step 5950, loss = 0.99 (996.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:25:21.931275: step 5960, loss = 1.07 (987.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:25:23.221993: step 5970, loss = 0.99 (991.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:25:24.530385: step 5980, loss = 0.98 (978.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:25:25.835898: step 5990, loss = 1.11 (980.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:25:27.212086: step 6000, loss = 1.06 (930.1 examples/sec; 0.138 sec/batch)
2017-05-07 20:25:28.424402: step 6010, loss = 1.08 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-07 20:25:29.700502: step 6020, loss = 1.10 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:25:31.021094: step 6030, loss = 0.90 (969.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:25:32.325634: step 6040, loss = 0.93 (981.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:25:33.619606: step 6050, loss = 1.24 (989.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:25:34.900701: step 6060, loss = 1.04 (999.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:25:36.188321: step 6070, loss = 0.94 (994.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:25:37.462763: step 6080, loss = 1.15 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:25:38.736147: step 6090, loss = 1.05 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:25:40.105184: step 6100, loss = 1.02 (935.0 examples/sec; 0.137 sec/batch)
2017-05-07 20:25:41.295382: step 6110, loss = 1.05 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-07 20:25:42.569131: step 6120, loss = 1.08 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:25:43.850658: step 6130, loss = 1.01 (998.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:25:45.133298: step 6140, loss = 0.98 (997.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:25:46.406509: step 6150, loss = 1.21 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:25:47.695642: step 6160, loss = 0.91 (992.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:25:48.986041: step 6170, loss = 1.03 (991.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:25:50.257524: step 6180, loss = 1.01 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:25:51.532380: step 6190, loss = 0.88 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:25:52.904206: step 6200, loss = 1.14 (933.1 examples/sec; 0.137 sec/batch)
2017-05-07 20:25:54.085775: step 6210, loss = 1.01 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-07 20:25:55.362088: step 6220, loss = 1.11 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:25:56.641890: step 6230, loss = 0.95 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:25:57.899024: step 6240, loss = 0.91 (1018.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:25:59.162022: step 6250, loss = 0.99 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:26:00.450034: step 6260, loss = 0.82 (993.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:26:01.751329: step 6270, loss = 1.52 (983.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:26:03.049026: step 6280, loss = 2.10 (986.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:26:04.323434: step 6290, loss = 1.18 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:26:05.706259: step 6300, loss = 1.12 (925.6 examples/sec; 0.138 sec/batch)
2017-05-07 20:26:06.878568: step 6310, loss = 0.92 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-07 20:26:08.132714: step 6320, loss = 1.02 (1020.6 examples/sec; 0.125 sec/batch)
2017-05-07 20:26:09.439516: step 6330, loss = 1.32 (979.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:26:10.696610: step 6340, loss = 1.18 (1018.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:26:12.007797: step 6350, loss = 1.23 (976.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:26:13.301355: step 6360, loss = 1.14 (989.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:26:14.589992: step 6370, loss = 1.14 (993.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:26:15.908673: step 6380, loss = 0.99 (970.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:26:17.215312: step 6390, loss = 0.84 (979.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:26:18.588239: step 6400, loss = 0.95 (932.3 examples/sec; 0.137 sec/batch)
2017-05-07 20:26:19.812597: step 6410, loss = 0.89 (1045.4 examples/sec; 0.122 sec/batch)
2017-05-07 20:26:21.116429: step 6420, loss = 0.98 (981.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:26:22.408802: step 6430, loss = 1.05 (990.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:26:23.715866: step 6440, loss = 1.20 (979.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:26:25.029238: step 6450, loss = 1.27 (974.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:26:26.319646: step 6460, loss = 0.99 (991.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:26:27.600387: step 6470, loss = 1.03 (999.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:26:28.911399: step 6480, loss = 1.02 (976.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:26:30.161031: step 6490, loss = 0.90 (1024.3 examples/sec; 0.125 sec/batch)
2017-05-07 20:26:31.520954: step 6500, loss = 1.06 (941.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:26:32.727274: step 6510, loss = 0.91 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-07 20:26:34.038071: step 6520, loss = 1.08 (976.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:26:35.314725: step 6530, loss = 0.99 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:26:36.611298: step 6540, loss = 1.13 (987.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:26:37.903302: step 6550, loss = 1.06 (990.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:26:39.175135: step 6560, loss = 0.97 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:26:40.466437: step 6570, loss = 1.05 (991.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:26:41.749673: step 6580, loss = 1.15 (997.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:26:43.056309: step 6590, loss = 0.84 (979.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:26:44.436630: step 6600, loss = 0.91 (927.3 examples/sec; 0.138 sec/batch)
2017-05-07 20:26:45.623779: step 6610, loss = 1.08 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-07 20:26:46.913485: step 6620, loss = 1.16 (992.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:26:48.185478: step 6630, loss = 0.98 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:26:49.455338: step 6640, loss = 0.88 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:26:50.732554: step 6650, loss = 0.89 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:26:52.025151: step 6660, loss = 1.06 (990.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:26:53.304427: step 6670, loss = 1.00 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:26:54.602640: step 6680, loss = 0.85 (986.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:26:55.878168: step 6690, loss = 1.02 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:26:57.261514: step 6700, loss = 1.25 (925.3 examples/sec; 0.138 sec/batch)
2017-05-07 20:26:58.473099: step 6710, loss = 1.07 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-07 20:26:59.743616: step 6720, loss = 1.03 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:27:01.021429: step 6730, loss = 1.04 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:27:02.344698: step 6740, loss = 1.00 (967.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:27:03.634796: step 6750, loss = 0.96 (992.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:27:04.917581: step 6760, loss = 0.95 (997.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:27:06.198603: step 6770, loss = 1.01 (999.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:27:07.513819: step 6780, loss = 1.50 (973.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:27:08.822454: step 6790, loss = 0.97 (978.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:27:10.213100: step 6800, loss = 1.17 (920.4 examples/sec; 0.139 sec/batch)
2017-05-07 20:27:11.394639: step 6810, loss = 0.94 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-07 20:27:12.712323: step 6820, loss = 1.09 (971.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:27:13.988006: step 6830, loss = 1.02 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:27:15.304702: step 6840, loss = 1.18 (972.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:27:16.599489: step 6850, loss = 1.02 (988.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:27:17.876093: step 6860, lE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 144 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
oss = 1.07 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:27:19.156717: step 6870, loss = 1.13 (999.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:27:20.441614: step 6880, loss = 0.89 (996.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:27:21.730406: step 6890, loss = 1.01 (993.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:27:23.122382: step 6900, loss = 1.07 (919.6 examples/sec; 0.139 sec/batch)
2017-05-07 20:27:24.327313: step 6910, loss = 1.02 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-07 20:27:25.584891: step 6920, loss = 0.82 (1017.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:27:26.866687: step 6930, loss = 1.22 (998.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:27:28.151867: step 6940, loss = 1.04 (996.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:27:29.410373: step 6950, loss = 0.87 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:27:30.688861: step 6960, loss = 0.88 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:27:32.002321: step 6970, loss = 0.82 (974.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:27:33.319885: step 6980, loss = 1.06 (971.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:27:34.627921: step 6990, loss = 1.06 (978.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:27:36.009996: step 7000, loss = 0.91 (926.2 examples/sec; 0.138 sec/batch)
2017-05-07 20:27:37.208559: step 7010, loss = 1.13 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-07 20:27:38.528658: step 7020, loss = 0.92 (969.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:27:39.812874: step 7030, loss = 1.11 (996.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:27:41.096864: step 7040, loss = 0.96 (996.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:27:42.374880: step 7050, loss = 0.89 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:27:43.669386: step 7060, loss = 1.19 (988.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:27:44.945153: step 7070, loss = 1.00 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:27:46.226678: step 7080, loss = 1.01 (998.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:27:47.513943: step 7090, loss = 1.00 (994.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:27:48.907065: step 7100, loss = 0.67 (918.8 examples/sec; 0.139 sec/batch)
2017-05-07 20:27:50.066674: step 7110, loss = 1.05 (1103.8 examples/sec; 0.116 sec/batch)
2017-05-07 20:27:51.378593: step 7120, loss = 1.11 (975.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:27:52.694712: step 7130, loss = 1.14 (972.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:27:53.979035: step 7140, loss = 0.98 (996.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:27:55.259631: step 7150, loss = 0.77 (999.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:27:56.555910: step 7160, loss = 0.96 (987.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:27:57.835777: step 7170, loss = 1.02 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:27:59.108464: step 7180, loss = 0.94 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:28:00.391660: step 7190, loss = 1.38 (997.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:28:01.774318: step 7200, loss = 0.94 (925.8 examples/sec; 0.138 sec/batch)
2017-05-07 20:28:02.968850: step 7210, loss = 0.95 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-07 20:28:04.284253: step 7220, loss = 1.20 (973.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:28:05.555592: step 7230, loss = 1.15 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:28:06.847206: step 7240, loss = 1.15 (991.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:28:08.146417: step 7250, loss = 1.01 (985.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:28:09.447014: step 7260, loss = 1.12 (984.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:28:10.731169: step 7270, loss = 0.95 (996.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:28:12.018911: step 7280, loss = 0.93 (994.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:28:13.299056: step 7290, loss = 0.87 (999.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:28:14.704206: step 7300, loss = 0.93 (910.9 examples/sec; 0.141 sec/batch)
2017-05-07 20:28:15.927073: step 7310, loss = 1.15 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-07 20:28:17.242703: step 7320, loss = 0.98 (972.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:28:18.537068: step 7330, loss = 1.04 (988.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:28:19.838174: step 7340, loss = 0.93 (983.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:28:21.123606: step 7350, loss = 1.03 (995.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:28:22.394508: step 7360, loss = 0.91 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:28:23.686421: step 7370, loss = 0.89 (990.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:28:24.965644: step 7380, loss = 0.95 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:28:26.228478: step 7390, loss = 1.05 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:28:27.613803: step 7400, loss = 1.13 (924.0 examples/sec; 0.139 sec/batch)
2017-05-07 20:28:28.823233: step 7410, loss = 1.20 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-07 20:28:30.107617: step 7420, loss = 0.84 (996.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:28:31.399471: step 7430, loss = 1.08 (990.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:28:32.680519: step 7440, loss = 1.03 (999.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:28:33.956351: step 7450, loss = 0.89 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:28:35.260922: step 7460, loss = 1.08 (981.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:28:36.571044: step 7470, loss = 1.05 (977.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:28:37.857466: step 7480, loss = 1.31 (995.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:28:39.131394: step 7490, loss = 1.23 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:28:40.493396: step 7500, loss = 0.97 (939.8 examples/sec; 0.136 sec/batch)
2017-05-07 20:28:41.709704: step 7510, loss = 0.97 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-07 20:28:42.982959: step 7520, loss = 1.02 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:28:44.255324: step 7530, loss = 0.83 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:28:45.554025: step 7540, loss = 1.01 (985.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:28:46.829571: step 7550, loss = 0.99 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:28:48.108333: step 7560, loss = 0.97 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:28:49.419663: step 7570, loss = 0.90 (976.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:28:50.720665: step 7580, loss = 1.12 (983.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:28:51.996418: step 7590, loss = 0.87 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:28:53.391235: step 7600, loss = 0.98 (917.7 examples/sec; 0.139 sec/batch)
2017-05-07 20:28:54.609770: step 7610, loss = 0.94 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-07 20:28:55.896703: step 7620, loss = 1.05 (994.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:28:57.174618: step 7630, loss = 1.17 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:28:58.437253: step 7640, loss = 0.80 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:28:59.740439: step 7650, loss = 0.79 (982.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:29:01.049791: step 7660, loss = 0.91 (977.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:29:02.348582: step 7670, loss = 1.12 (985.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:29:03.621353: step 7680, loss = 0.99 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:29:04.899652: step 7690, loss = 0.86 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:29:06.244333: step 7700, loss = 1.04 (951.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:29:07.428753: step 7710, loss = 0.99 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-07 20:29:08.694852: step 7720, loss = 0.82 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:29:09.928216: step 7730, loss = 0.80 (1037.8 examples/sec; 0.123 sec/batch)
2017-05-07 20:29:11.213492: step 7740, loss = 1.22 (995.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:29:12.491567: step 7750, loss = 0.87 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:29:13.803241: step 7760, loss = 0.99 (975.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:29:15.104253: step 7770, loss = 0.91 (983.8 examples/sec; 0.130 sec/batch)
2017-05-0E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 164 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
7 20:29:16.407278: step 7780, loss = 1.07 (982.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:29:17.694284: step 7790, loss = 1.02 (994.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:29:19.081340: step 7800, loss = 1.07 (922.8 examples/sec; 0.139 sec/batch)
2017-05-07 20:29:20.285709: step 7810, loss = 1.18 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-07 20:29:21.544164: step 7820, loss = 1.00 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:29:22.840820: step 7830, loss = 1.07 (987.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:29:24.114173: step 7840, loss = 1.06 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:29:25.404053: step 7850, loss = 1.00 (992.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:29:26.706960: step 7860, loss = 1.04 (982.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:29:27.993187: step 7870, loss = 1.02 (995.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:29:29.287787: step 7880, loss = 1.16 (988.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:29:30.576806: step 7890, loss = 0.83 (993.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:29:31.977387: step 7900, loss = 0.92 (913.9 examples/sec; 0.140 sec/batch)
2017-05-07 20:29:33.192739: step 7910, loss = 1.04 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-07 20:29:34.469699: step 7920, loss = 0.92 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:29:35.726494: step 7930, loss = 0.97 (1018.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:29:37.006511: step 7940, loss = 0.91 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:29:38.301670: step 7950, loss = 0.96 (988.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:29:39.628759: step 7960, loss = 0.91 (964.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:29:40.912221: step 7970, loss = 0.96 (997.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:29:42.197073: step 7980, loss = 0.98 (996.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:29:43.502632: step 7990, loss = 1.11 (980.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:29:44.906758: step 8000, loss = 1.13 (911.6 examples/sec; 0.140 sec/batch)
2017-05-07 20:29:46.103088: step 8010, loss = 0.98 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-07 20:29:47.393296: step 8020, loss = 1.07 (992.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:29:48.710301: step 8030, loss = 1.07 (971.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:29:50.016890: step 8040, loss = 0.94 (979.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:29:51.276294: step 8050, loss = 1.02 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:29:52.588638: step 8060, loss = 1.20 (975.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:29:53.863089: step 8070, loss = 0.93 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:29:55.148058: step 8080, loss = 1.03 (996.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:29:56.467629: step 8090, loss = 1.00 (970.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:29:57.837809: step 8100, loss = 0.75 (934.2 examples/sec; 0.137 sec/batch)
2017-05-07 20:29:59.044096: step 8110, loss = 0.86 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-07 20:30:00.332877: step 8120, loss = 0.98 (993.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:30:01.596554: step 8130, loss = 1.01 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:30:02.895355: step 8140, loss = 1.27 (985.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:30:04.233295: step 8150, loss = 0.92 (956.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:30:05.489511: step 8160, loss = 1.05 (1018.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:30:06.775373: step 8170, loss = 1.13 (995.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:30:08.058603: step 8180, loss = 1.37 (997.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:30:09.356602: step 8190, loss = 0.93 (986.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:30:10.735637: step 8200, loss = 0.81 (928.2 examples/sec; 0.138 sec/batch)
2017-05-07 20:30:11.968938: step 8210, loss = 1.08 (1037.9 examples/sec; 0.123 sec/batch)
2017-05-07 20:30:13.298595: step 8220, loss = 0.96 (962.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:30:14.551569: step 8230, loss = 0.86 (1021.6 examples/sec; 0.125 sec/batch)
2017-05-07 20:30:15.851682: step 8240, loss = 0.98 (984.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:30:17.138628: step 8250, loss = 0.92 (994.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:30:18.401707: step 8260, loss = 0.95 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:30:19.686648: step 8270, loss = 1.07 (996.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:30:20.966279: step 8280, loss = 1.09 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:30:22.247931: step 8290, loss = 0.92 (998.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:30:23.652889: step 8300, loss = 0.95 (911.1 examples/sec; 0.140 sec/batch)
2017-05-07 20:30:24.858566: step 8310, loss = 0.87 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-07 20:30:26.151073: step 8320, loss = 0.96 (990.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:30:27.470337: step 8330, loss = 1.01 (970.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:30:28.740237: step 8340, loss = 0.96 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:30:30.040252: step 8350, loss = 1.20 (984.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:30:31.332996: step 8360, loss = 0.96 (990.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:30:32.644230: step 8370, loss = 0.91 (976.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:30:33.912751: step 8380, loss = 0.85 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:30:35.168661: step 8390, loss = 1.02 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:30:36.538925: step 8400, loss = 1.09 (934.1 examples/sec; 0.137 sec/batch)
2017-05-07 20:30:37.749173: step 8410, loss = 0.92 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-07 20:30:39.062825: step 8420, loss = 1.18 (974.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:30:40.352544: step 8430, loss = 0.95 (992.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:30:41.610939: step 8440, loss = 0.85 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:30:42.935536: step 8450, loss = 1.17 (966.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:30:44.213736: step 8460, loss = 0.95 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:30:45.524917: step 8470, loss = 0.98 (976.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:30:46.829753: step 8480, loss = 0.97 (981.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:30:48.148212: step 8490, loss = 1.06 (970.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:30:49.535061: step 8500, loss = 1.03 (923.0 examples/sec; 0.139 sec/batch)
2017-05-07 20:30:50.732523: step 8510, loss = 0.75 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-07 20:30:52.028577: step 8520, loss = 0.96 (987.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:30:53.322639: step 8530, loss = 0.97 (989.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:30:54.609160: step 8540, loss = 0.96 (994.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:30:55.890201: step 8550, loss = 1.05 (999.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:30:57.157651: step 8560, loss = 0.97 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:30:58.416061: step 8570, loss = 0.92 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:30:59.692447: step 8580, loss = 0.98 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:31:00.953472: step 8590, loss = 0.91 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:31:02.313287: step 8600, loss = 0.97 (941.3 examples/sec; 0.136 sec/batch)
2017-05-07 20:31:03.482059: step 8610, loss = 0.91 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-07 20:31:04.737234: step 8620, loss = 1.03 (1019.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:31:05.998419: step 8630, loss = 1.21 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:31:07.285295: step 8640, loss = 1.12 (994.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:31:08.572813: step 8650, loss = 1.11 (994.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:31:09.832805: step 8660, loss = 0.91 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:31:11.089360: step 8670, loss = 0.84 (1018.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:31:12.369002: step 8680, loss = 0.91 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:31:13.635283: step 8690, loss = 0.86 (1010.8 examplesE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 184 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
/sec; 0.127 sec/batch)
2017-05-07 20:31:15.009366: step 8700, loss = 0.79 (931.5 examples/sec; 0.137 sec/batch)
2017-05-07 20:31:16.187158: step 8710, loss = 0.84 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-07 20:31:17.465348: step 8720, loss = 0.85 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:31:18.723100: step 8730, loss = 0.88 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:31:20.006735: step 8740, loss = 0.88 (997.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:31:21.270369: step 8750, loss = 0.98 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:31:22.557398: step 8760, loss = 0.95 (994.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:31:23.833134: step 8770, loss = 0.92 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:31:25.123151: step 8780, loss = 1.24 (991.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:31:26.381073: step 8790, loss = 1.06 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:31:27.760047: step 8800, loss = 1.03 (928.2 examples/sec; 0.138 sec/batch)
2017-05-07 20:31:28.917232: step 8810, loss = 1.08 (1106.1 examples/sec; 0.116 sec/batch)
2017-05-07 20:31:30.194512: step 8820, loss = 0.89 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:31:31.449408: step 8830, loss = 1.31 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-07 20:31:32.716039: step 8840, loss = 1.03 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:31:34.001893: step 8850, loss = 1.17 (995.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:31:35.270773: step 8860, loss = 1.07 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:31:36.542596: step 8870, loss = 0.99 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:31:37.800621: step 8880, loss = 1.10 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:31:39.094428: step 8890, loss = 1.14 (989.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:31:40.489622: step 8900, loss = 1.09 (917.4 examples/sec; 0.140 sec/batch)
2017-05-07 20:31:41.656513: step 8910, loss = 0.94 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-07 20:31:42.922846: step 8920, loss = 0.99 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:31:44.195720: step 8930, loss = 1.14 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:31:45.481412: step 8940, loss = 1.05 (995.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:31:46.775510: step 8950, loss = 0.98 (989.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:31:48.069802: step 8960, loss = 1.13 (989.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:31:49.325507: step 8970, loss = 1.08 (1019.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:31:50.609919: step 8980, loss = 0.88 (996.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:31:51.891268: step 8990, loss = 0.85 (998.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:31:53.267377: step 9000, loss = 0.95 (930.2 examples/sec; 0.138 sec/batch)
2017-05-07 20:31:54.443571: step 9010, loss = 1.00 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-07 20:31:55.727415: step 9020, loss = 0.91 (997.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:31:56.991445: step 9030, loss = 0.95 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:31:58.258313: step 9040, loss = 0.82 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:31:59.517552: step 9050, loss = 1.34 (1016.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:32:00.784056: step 9060, loss = 0.98 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:32:02.062042: step 9070, loss = 1.00 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:32:03.312683: step 9080, loss = 1.04 (1023.5 examples/sec; 0.125 sec/batch)
2017-05-07 20:32:04.600259: step 9090, loss = 1.09 (994.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:32:05.960544: step 9100, loss = 0.94 (941.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:32:07.116168: step 9110, loss = 0.74 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-07 20:32:08.387933: step 9120, loss = 1.18 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:32:09.701552: step 9130, loss = 1.18 (974.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:32:11.008043: step 9140, loss = 0.87 (979.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:32:12.310636: step 9150, loss = 1.06 (982.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:32:13.584378: step 9160, loss = 1.01 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:32:14.872701: step 9170, loss = 0.90 (993.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:32:16.163273: step 9180, loss = 0.93 (991.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:32:17.420256: step 9190, loss = 1.13 (1018.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:32:18.780613: step 9200, loss = 0.82 (940.9 examples/sec; 0.136 sec/batch)
2017-05-07 20:32:19.952520: step 9210, loss = 0.83 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-07 20:32:21.232770: step 9220, loss = 0.76 (999.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:32:22.510828: step 9230, loss = 1.15 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:32:23.780415: step 9240, loss = 0.97 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:32:25.062045: step 9250, loss = 1.06 (998.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:32:26.302591: step 9260, loss = 0.87 (1031.8 examples/sec; 0.124 sec/batch)
2017-05-07 20:32:27.549719: step 9270, loss = 1.01 (1026.4 examples/sec; 0.125 sec/batch)
2017-05-07 20:32:28.834824: step 9280, loss = 1.10 (996.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:32:30.095408: step 9290, loss = 0.80 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:32:31.469875: step 9300, loss = 0.82 (931.3 examples/sec; 0.137 sec/batch)
2017-05-07 20:32:32.644156: step 9310, loss = 0.86 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-07 20:32:33.917348: step 9320, loss = 0.85 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:32:35.188130: step 9330, loss = 0.94 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:32:36.454826: step 9340, loss = 0.92 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:32:37.799151: step 9350, loss = 0.78 (952.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:32:39.038533: step 9360, loss = 1.16 (1032.8 examples/sec; 0.124 sec/batch)
2017-05-07 20:32:40.330519: step 9370, loss = 0.84 (990.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:32:41.647474: step 9380, loss = 0.90 (971.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:32:42.957948: step 9390, loss = 0.92 (976.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:32:44.365348: step 9400, loss = 1.11 (909.5 examples/sec; 0.141 sec/batch)
2017-05-07 20:32:45.535610: step 9410, loss = 0.91 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-07 20:32:46.800907: step 9420, loss = 0.87 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:32:48.056192: step 9430, loss = 0.92 (1019.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:32:49.380220: step 9440, loss = 1.28 (966.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:32:50.651002: step 9450, loss = 0.91 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:32:51.895400: step 9460, loss = 0.91 (1028.6 examples/sec; 0.124 sec/batch)
2017-05-07 20:32:53.175914: step 9470, loss = 0.99 (999.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:32:54.424549: step 9480, loss = 1.09 (1025.1 examples/sec; 0.125 sec/batch)
2017-05-07 20:32:55.711585: step 9490, loss = 1.19 (994.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:32:57.096573: step 9500, loss = 1.10 (924.2 examples/sec; 0.138 sec/batch)
2017-05-07 20:32:58.269727: step 9510, loss = 0.98 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-07 20:32:59.540471: step 9520, loss = 1.04 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:33:00.821069: step 9530, loss = 0.74 (999.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:33:02.097044: step 9540, loss = 0.99 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:33:03.364662: step 9550, loss = 0.92 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:33:04.649448: step 9560, loss = 0.89 (996.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:33:05.929730: step 9570, loss = 0.82 (999.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:33:07.213318: step 9580, loss = 0.84 (997.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:33:08.483916: step 9590, loss = 0.86 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:33:09.841244: step 9600, loss = 0.82 (943.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:33:11.E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 205 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
002205: step 9610, loss = 1.11 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-07 20:33:12.283073: step 9620, loss = 0.98 (999.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:33:13.556377: step 9630, loss = 0.88 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:33:14.822370: step 9640, loss = 1.17 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:33:16.093510: step 9650, loss = 0.94 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:33:17.346607: step 9660, loss = 0.96 (1021.5 examples/sec; 0.125 sec/batch)
2017-05-07 20:33:18.610698: step 9670, loss = 1.05 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:33:19.903590: step 9680, loss = 1.08 (990.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:33:21.178280: step 9690, loss = 0.90 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:33:22.541330: step 9700, loss = 0.91 (939.1 examples/sec; 0.136 sec/batch)
2017-05-07 20:33:23.715861: step 9710, loss = 1.02 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-07 20:33:25.006728: step 9720, loss = 1.08 (991.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:33:26.270929: step 9730, loss = 0.82 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:33:27.541190: step 9740, loss = 0.90 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:33:28.806694: step 9750, loss = 0.98 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:33:30.067700: step 9760, loss = 1.15 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:33:31.343294: step 9770, loss = 0.77 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:33:32.614763: step 9780, loss = 0.96 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:33:33.891383: step 9790, loss = 0.84 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:33:35.264195: step 9800, loss = 0.96 (932.4 examples/sec; 0.137 sec/batch)
2017-05-07 20:33:36.426999: step 9810, loss = 0.79 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-07 20:33:37.683198: step 9820, loss = 0.91 (1018.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:33:38.994724: step 9830, loss = 0.91 (976.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:33:40.266547: step 9840, loss = 1.01 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:33:41.574768: step 9850, loss = 1.52 (978.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:33:42.841082: step 9860, loss = 0.85 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:33:44.115135: step 9870, loss = 1.05 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:33:45.383717: step 9880, loss = 0.96 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:33:46.667697: step 9890, loss = 1.03 (996.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:33:48.046707: step 9900, loss = 0.96 (928.2 examples/sec; 0.138 sec/batch)
2017-05-07 20:33:49.229165: step 9910, loss = 0.89 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-07 20:33:50.471214: step 9920, loss = 0.90 (1030.6 examples/sec; 0.124 sec/batch)
2017-05-07 20:33:51.742955: step 9930, loss = 0.91 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:33:53.002853: step 9940, loss = 0.96 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:33:54.265298: step 9950, loss = 0.97 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:33:55.536889: step 9960, loss = 0.84 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:33:56.793596: step 9970, loss = 1.07 (1018.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:33:58.053359: step 9980, loss = 0.91 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:33:59.343068: step 9990, loss = 1.04 (992.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:34:00.739728: step 10000, loss = 1.16 (916.5 examples/sec; 0.140 sec/batch)
2017-05-07 20:34:01.905226: step 10010, loss = 0.83 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-07 20:34:03.168273: step 10020, loss = 1.05 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:34:04.441536: step 10030, loss = 0.82 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:34:05.722348: step 10040, loss = 0.87 (999.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:34:06.981264: step 10050, loss = 0.99 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:34:08.251064: step 10060, loss = 1.07 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:34:09.501666: step 10070, loss = 1.55 (1023.5 examples/sec; 0.125 sec/batch)
2017-05-07 20:34:10.790689: step 10080, loss = 0.89 (993.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:34:12.091611: step 10090, loss = 0.76 (983.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:34:13.492510: step 10100, loss = 1.03 (913.7 examples/sec; 0.140 sec/batch)
2017-05-07 20:34:14.684888: step 10110, loss = 0.74 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-07 20:34:15.929725: step 10120, loss = 0.94 (1028.2 examples/sec; 0.124 sec/batch)
2017-05-07 20:34:17.194400: step 10130, loss = 0.92 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:34:18.479951: step 10140, loss = 0.83 (995.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:34:19.744607: step 10150, loss = 0.97 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:34:21.016938: step 10160, loss = 0.99 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:34:22.272082: step 10170, loss = 0.80 (1019.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:34:23.557371: step 10180, loss = 1.00 (995.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:34:24.854644: step 10190, loss = 1.12 (986.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:34:26.237586: step 10200, loss = 0.95 (925.6 examples/sec; 0.138 sec/batch)
2017-05-07 20:34:27.438877: step 10210, loss = 1.02 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-07 20:34:28.706076: step 10220, loss = 0.96 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:34:29.990224: step 10230, loss = 0.74 (996.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:34:31.259269: step 10240, loss = 0.79 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:34:32.559906: step 10250, loss = 1.19 (984.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:34:33.813421: step 10260, loss = 0.84 (1021.1 examples/sec; 0.125 sec/batch)
2017-05-07 20:34:35.069044: step 10270, loss = 0.89 (1019.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:34:36.337955: step 10280, loss = 1.00 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:34:37.630958: step 10290, loss = 0.92 (989.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:34:39.003529: step 10300, loss = 1.38 (932.6 examples/sec; 0.137 sec/batch)
2017-05-07 20:34:40.178220: step 10310, loss = 1.02 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-07 20:34:41.455522: step 10320, loss = 0.87 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:34:42.720681: step 10330, loss = 0.88 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:34:43.966369: step 10340, loss = 1.15 (1027.5 examples/sec; 0.125 sec/batch)
2017-05-07 20:34:45.268877: step 10350, loss = 0.80 (982.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:34:46.532710: step 10360, loss = 1.14 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:34:47.780519: step 10370, loss = 0.90 (1025.8 examples/sec; 0.125 sec/batch)
2017-05-07 20:34:49.046948: step 10380, loss = 0.75 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:34:50.334902: step 10390, loss = 0.82 (993.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:34:51.683009: step 10400, loss = 0.96 (949.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:34:52.880458: step 10410, loss = 0.90 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-07 20:34:54.150421: step 10420, loss = 0.91 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:34:55.418883: step 10430, loss = 0.95 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:34:56.683252: step 10440, loss = 0.92 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:34:57.986276: step 10450, loss = 1.04 (982.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:34:59.257217: step 10460, loss = 0.83 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:35:00.536938: step 10470, loss = 0.87 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:35:01.826865: step 10480, loss = 0.94 (992.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:35:03.073899: step 10490, loss = 0.75 (1026.4 examples/sec; 0.125 sec/batch)
2017-05-07 20:35:04.444826: step 10500, loss = 0.95 (933.7 examples/sec; 0.137 sec/batch)
2017-05-07 20:35:05.620495: step 10510, loss = 1.03 (1088.7 examples/sec; 0.118 sec/batch)E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 225 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU

2017-05-07 20:35:06.885230: step 10520, loss = 0.71 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:35:08.147984: step 10530, loss = 0.93 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:35:09.412601: step 10540, loss = 1.17 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:35:10.698515: step 10550, loss = 1.11 (995.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:35:11.975895: step 10560, loss = 0.88 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:35:13.267236: step 10570, loss = 0.89 (991.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:35:14.527557: step 10580, loss = 0.69 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:35:15.796004: step 10590, loss = 1.11 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:35:17.160543: step 10600, loss = 0.90 (938.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:35:18.300715: step 10610, loss = 0.88 (1122.6 examples/sec; 0.114 sec/batch)
2017-05-07 20:35:19.575280: step 10620, loss = 1.20 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:35:20.846448: step 10630, loss = 1.21 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:35:22.098106: step 10640, loss = 0.98 (1022.6 examples/sec; 0.125 sec/batch)
2017-05-07 20:35:23.370458: step 10650, loss = 0.88 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:35:24.626861: step 10660, loss = 0.73 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:35:25.866142: step 10670, loss = 0.88 (1032.9 examples/sec; 0.124 sec/batch)
2017-05-07 20:35:27.160946: step 10680, loss = 0.84 (988.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:35:28.477355: step 10690, loss = 0.90 (972.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:35:29.845922: step 10700, loss = 0.93 (935.3 examples/sec; 0.137 sec/batch)
2017-05-07 20:35:31.051045: step 10710, loss = 1.03 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-07 20:35:32.327731: step 10720, loss = 1.17 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:35:33.609300: step 10730, loss = 0.88 (998.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:35:34.867843: step 10740, loss = 0.87 (1017.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:35:36.141105: step 10750, loss = 1.01 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:35:37.437868: step 10760, loss = 0.90 (987.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:35:38.734587: step 10770, loss = 1.16 (987.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:35:40.020439: step 10780, loss = 1.09 (995.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:35:41.294153: step 10790, loss = 0.88 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:35:42.666390: step 10800, loss = 0.89 (932.8 examples/sec; 0.137 sec/batch)
2017-05-07 20:35:43.830445: step 10810, loss = 0.82 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-07 20:35:45.093011: step 10820, loss = 1.01 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:35:46.373402: step 10830, loss = 0.91 (999.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:35:47.645418: step 10840, loss = 0.93 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:35:48.921204: step 10850, loss = 0.89 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:35:50.184369: step 10860, loss = 0.80 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:35:51.454871: step 10870, loss = 0.98 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:35:52.750722: step 10880, loss = 0.95 (987.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:35:54.009672: step 10890, loss = 0.90 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:35:55.369275: step 10900, loss = 1.01 (941.5 examples/sec; 0.136 sec/batch)
2017-05-07 20:35:56.559678: step 10910, loss = 0.72 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-07 20:35:57.831996: step 10920, loss = 0.86 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:35:59.110264: step 10930, loss = 0.84 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:36:00.399130: step 10940, loss = 0.97 (993.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:36:01.636313: step 10950, loss = 0.96 (1034.6 examples/sec; 0.124 sec/batch)
2017-05-07 20:36:02.921753: step 10960, loss = 0.88 (995.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:36:04.197185: step 10970, loss = 0.91 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:36:05.456315: step 10980, loss = 0.80 (1016.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:36:06.716927: step 10990, loss = 0.82 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:36:08.072813: step 11000, loss = 0.93 (944.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:36:09.244605: step 11010, loss = 0.86 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-07 20:36:10.523453: step 11020, loss = 1.18 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:36:11.818941: step 11030, loss = 1.05 (988.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:36:13.083755: step 11040, loss = 0.95 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:36:14.350609: step 11050, loss = 0.83 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:36:15.621608: step 11060, loss = 1.19 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:36:16.902239: step 11070, loss = 0.84 (999.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:36:18.149003: step 11080, loss = 0.99 (1026.7 examples/sec; 0.125 sec/batch)
2017-05-07 20:36:19.428790: step 11090, loss = 0.86 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:36:20.808121: step 11100, loss = 0.97 (928.0 examples/sec; 0.138 sec/batch)
2017-05-07 20:36:21.975437: step 11110, loss = 0.70 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-07 20:36:23.251533: step 11120, loss = 1.11 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:36:24.534071: step 11130, loss = 0.96 (998.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:36:25.787500: step 11140, loss = 0.98 (1021.2 examples/sec; 0.125 sec/batch)
2017-05-07 20:36:27.058845: step 11150, loss = 1.10 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:36:28.361290: step 11160, loss = 0.98 (982.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:36:29.671827: step 11170, loss = 1.28 (976.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:36:30.973130: step 11180, loss = 0.86 (983.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:36:32.235666: step 11190, loss = 0.81 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:36:33.601534: step 11200, loss = 1.07 (937.1 examples/sec; 0.137 sec/batch)
2017-05-07 20:36:34.796916: step 11210, loss = 0.81 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-07 20:36:36.082344: step 11220, loss = 1.07 (995.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:36:37.405378: step 11230, loss = 0.90 (967.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:36:38.663997: step 11240, loss = 0.85 (1017.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:36:39.937322: step 11250, loss = 0.96 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:36:41.194826: step 11260, loss = 0.78 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:36:42.469676: step 11270, loss = 0.95 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:36:43.735055: step 11280, loss = 0.99 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:36:45.011934: step 11290, loss = 1.03 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:36:46.376591: step 11300, loss = 1.05 (938.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:36:47.539497: step 11310, loss = 0.94 (1100.7 examples/sec; 0.116 sec/batch)
2017-05-07 20:36:48.815250: step 11320, loss = 1.01 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:36:50.088061: step 11330, loss = 0.87 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:36:51.359468: step 11340, loss = 1.08 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:36:52.631063: step 11350, loss = 0.92 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:36:53.917805: step 11360, loss = 1.05 (994.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:36:55.163526: step 11370, loss = 0.80 (1027.5 examples/sec; 0.125 sec/batch)
2017-05-07 20:36:56.431127: step 11380, loss = 1.02 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:36:57.735791: step 11390, loss = 1.17 (981.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:36:59.117446: step 11400, loss = 0.93 (926.4 examples/sec; 0.138 sec/batch)
2017-05-07 20:37:00.292680: step 11410, loss = 0.90 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-07 20:37:01.546640: step 11420, loss = 0.81 (1020.8 examples/sec; 0.125 sec/batch)
2017-05-07 20:37:02.817440: step 11430, loss = 0.88 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:37:04.100623: step 11440, loss = 1.00 (997.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:37:05.375149: step 11450, loss = 0.82 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:37:06.641408: step 11460, loss = 0.86 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:37:07.927922: step 11470, loss = 1.00 (994.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:37:09.199230: step 11480, loss = 0.96 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:37:10.461902: step 11490, loss = 0.83 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:37:11.827953: step 11500, loss = 0.91 (937.0 examples/sec; 0.137 sec/batch)
2017-05-07 20:37:12.996254: step 11510, loss = 0.96 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-07 20:37:14.240851: step 11520, loss = 1.16 (1028.5 examples/sec; 0.124 sec/batch)
2017-05-07 20:37:15.569750: step 11530, loss = 0.94 (963.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:37:16.874616: step 11540, loss = 0.79 (980.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:37:18.162439: step 11550, loss = 1.15 (993.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:37:19.478678: step 11560, loss = 1.32 (972.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:37:20.766640: step 11570, loss = 0.96 (993.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:37:22.060165: step 11580, loss = 1.13 (989.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:37:23.332066: step 11590, loss = 1.01 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:37:24.696594: step 11600, loss = 1.04 (938.1 examples/sec; 0.136 sec/batch)
2017-05-07 20:37:25.875934: step 11610, loss = 0.84 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-07 20:37:27.137229: step 11620, loss = 0.90 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:37:28.414583: step 11630, loss = 0.93 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:37:29.702907: step 11640, loss = 0.75 (993.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:37:30.978321: step 11650, loss = 0.99 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:37:32.257938: step 11660, loss = 1.05 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:37:33.528629: step 11670, loss = 0.83 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:37:34.776553: step 11680, loss = 1.06 (1025.7 examples/sec; 0.125 sec/batch)
2017-05-07 20:37:36.030611: step 11690, loss = 0.95 (1020.7 examples/sec; 0.125 sec/batch)
2017-05-07 20:37:37.404396: step 11700, loss = 0.91 (931.7 examples/sec; 0.137 sec/batch)
2017-05-07 20:37:38.597910: step 11710, loss = 0.93 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-07 20:37:39.893359: step 11720, loss = 0.96 (988.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:37:41.184339: step 11730, loss = 1.07 (991.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:37:42.458541: step 11740, loss = 1.08 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:37:43.748869: step 11750, loss = 1.00 (992.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:37:45.012452: step 11760, loss = 1.02 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:37:46.278225: step 11770, loss = 0.76 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:37:47.554517: step 11780, loss = 0.91 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:37:48.812700: step 11790, loss = 0.94 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:37:50.175748: step 11800, loss = 0.90 (939.1 examples/sec; 0.136 sec/batch)
2017-05-07 20:37:51.362348: step 11810, loss = 1.04 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-07 20:37:52.623233: step 11820, loss = 1.13 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:37:53.887924: step 11830, loss = 1.11 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:37:55.162630: step 11840, loss = 1.94 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:37:56.437899: step 11850, loss = 0.98 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:37:57.693545: step 11860, loss = 0.84 (1019.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:37:58.968811: step 11870, loss = 0.E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 245 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
96 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:38:00.228514: step 11880, loss = 1.03 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:38:01.479337: step 11890, loss = 0.86 (1023.3 examples/sec; 0.125 sec/batch)
2017-05-07 20:38:02.849607: step 11900, loss = 1.04 (934.1 examples/sec; 0.137 sec/batch)
2017-05-07 20:38:04.024289: step 11910, loss = 1.00 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-07 20:38:05.321154: step 11920, loss = 0.87 (987.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:38:06.573656: step 11930, loss = 0.92 (1022.0 examples/sec; 0.125 sec/batch)
2017-05-07 20:38:07.873440: step 11940, loss = 0.89 (984.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:38:09.192407: step 11950, loss = 1.03 (970.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:38:10.489302: step 11960, loss = 1.02 (987.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:38:11.759057: step 11970, loss = 0.75 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:38:13.016878: step 11980, loss = 0.89 (1017.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:38:14.274074: step 11990, loss = 0.86 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:38:15.637205: step 12000, loss = 0.77 (939.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:38:16.784348: step 12010, loss = 1.29 (1115.8 examples/sec; 0.115 sec/batch)
2017-05-07 20:38:18.055482: step 12020, loss = 0.95 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:38:19.324936: step 12030, loss = 0.97 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:38:20.589453: step 12040, loss = 0.85 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:38:21.840352: step 12050, loss = 0.93 (1023.3 examples/sec; 0.125 sec/batch)
2017-05-07 20:38:23.102499: step 12060, loss = 1.14 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:38:24.385791: step 12070, loss = 1.00 (997.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:38:25.657259: step 12080, loss = 0.79 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:38:26.929594: step 12090, loss = 0.89 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:38:28.311023: step 12100, loss = 0.83 (926.6 examples/sec; 0.138 sec/batch)
2017-05-07 20:38:29.494256: step 12110, loss = 1.01 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-07 20:38:30.726477: step 12120, loss = 0.76 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-07 20:38:32.047806: step 12130, loss = 0.91 (968.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:38:33.334477: step 12140, loss = 1.01 (994.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:38:34.634318: step 12150, loss = 1.09 (984.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:38:35.878349: step 12160, loss = 0.87 (1028.9 examples/sec; 0.124 sec/batch)
2017-05-07 20:38:37.162800: step 12170, loss = 0.93 (996.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:38:38.440821: step 12180, loss = 0.91 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:38:39.713477: step 12190, loss = 1.15 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:38:41.074049: step 12200, loss = 0.75 (940.8 examples/sec; 0.136 sec/batch)
2017-05-07 20:38:42.253811: step 12210, loss = 0.81 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-07 20:38:43.547297: step 12220, loss = 1.02 (989.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:38:44.819201: step 12230, loss = 0.89 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:38:46.090673: step 12240, loss = 0.91 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:38:47.352719: step 12250, loss = 0.89 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:38:48.622880: step 12260, loss = 1.04 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:38:49.911461: step 12270, loss = 0.95 (993.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:38:51.184036: step 12280, loss = 0.95 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:38:52.448749: step 12290, loss = 0.91 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:38:53.821087: step 12300, loss = 1.11 (932.7 examples/sec; 0.137 sec/batch)
2017-05-07 20:38:54.981622: step 12310, loss = 0.87 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-07 20:38:56.243459: step 12320, loss = 1.19 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:38:57.488736: step 12330, loss = 0.90 (1027.9 examples/sec; 0.125 sec/batch)
2017-05-07 20:38:58.733951: step 12340, loss = 0.94 (1027.9 examples/sec; 0.125 sec/batch)
2017-05-07 20:39:00.019562: step 12350, loss = 0.96 (995.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:39:01.298401: step 12360, loss = 1.02 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:39:02.556927: step 12370, loss = 1.04 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:39:03.825739: step 12380, loss = 0.86 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:39:05.094997: step 12390, loss = 0.81 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:39:06.461299: step 12400, loss = 0.84 (936.8 examples/sec; 0.137 sec/batch)
2017-05-07 20:39:07.627903: step 12410, loss = 1.01 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-07 20:39:08.893301: step 12420, loss = 0.83 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:39:10.135521: step 12430, loss = 1.03 (1030.4 examples/sec; 0.124 sec/batch)
2017-05-07 20:39:11.390265: step 12440, loss = 0.78 (1020.1 examples/sec; 0.125 sec/batch)
2017-05-07 20:39:12.635478: step 12450, loss = 0.76 (1027.9 examples/sec; 0.125 sec/batch)
2017-05-07 20:39:13.924513: step 12460, loss = 0.99 (993.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:39:15.245285: step 12470, loss = 0.97 (969.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:39:16.509429: step 12480, loss = 0.84 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:39:17.776384: step 12490, loss = 0.84 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:39:19.121551: step 12500, loss = 0.83 (951.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:39:20.270671: step 12510, loss = 0.96 (1113.9 examples/sec; 0.115 sec/batch)
2017-05-07 20:39:21.541754: step 12520, loss = 1.05 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:39:22.797926: step 12530, loss = 0.88 (1019.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:39:24.075774: step 12540, loss = 1.02 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:39:25.345535: step 12550, loss = 1.03 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:39:26.601375: step 12560, loss = 0.81 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:39:27.863154: step 12570, loss = 0.74 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:39:29.130542: step 12580, loss = 0.82 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:39:30.392015: step 12590, loss = 0.91 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:39:31.761869: step 12600, loss = 1.17 (934.4 examples/sec; 0.137 sec/batch)
2017-05-07 20:39:32.932107: step 12610, loss = 1.04 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-07 20:39:34.219096: step 12620, loss = 0.89 (994.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:39:35.490105: step 12630, loss = 0.76 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:39:36.752989: step 12640, loss = 0.90 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:39:38.029042: step 12650, loss = 0.82 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:39:39.317156: step 12660, loss = 1.02 (993.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:39:40.606118: step 12670, loss = 1.00 (993.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:39:41.858880: step 12680, loss = 0.93 (1021.8 examples/sec; 0.125 sec/batch)
2017-05-07 20:39:43.122210: step 12690, loss = 0.96 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:39:44.469365: step 12700, loss = 0.85 (950.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:39:45.651631: step 12710, loss = 1.07 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-07 20:39:46.917370: step 12720, loss = 0.87 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:39:48.178828: step 12730, loss = 0.99 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:39:49.438766: step 12740, loss = 0.93 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:39:50.705433: step 12750, loss = 1.02 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:39:51.961483: step 12760, loss = 0.80 (1019.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:39:53.242083: step 12770, loss = 0.85 (999.5 examples/sec; 0.12E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 265 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
8 sec/batch)
2017-05-07 20:39:54.528404: step 12780, loss = 0.90 (995.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:39:55.796167: step 12790, loss = 0.81 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:39:57.145771: step 12800, loss = 0.89 (948.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:39:58.322104: step 12810, loss = 0.76 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-07 20:39:59.596512: step 12820, loss = 1.03 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:40:00.891357: step 12830, loss = 0.80 (988.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:40:02.182863: step 12840, loss = 1.13 (991.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:40:03.433516: step 12850, loss = 0.92 (1023.5 examples/sec; 0.125 sec/batch)
2017-05-07 20:40:04.718008: step 12860, loss = 0.74 (996.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:40:05.971348: step 12870, loss = 0.73 (1021.3 examples/sec; 0.125 sec/batch)
2017-05-07 20:40:07.227831: step 12880, loss = 1.04 (1018.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:40:08.508172: step 12890, loss = 1.08 (999.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:40:09.895802: step 12900, loss = 0.88 (922.4 examples/sec; 0.139 sec/batch)
2017-05-07 20:40:11.083778: step 12910, loss = 0.90 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-07 20:40:12.342795: step 12920, loss = 0.65 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:40:13.615124: step 12930, loss = 0.72 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:40:14.862757: step 12940, loss = 0.93 (1025.9 examples/sec; 0.125 sec/batch)
2017-05-07 20:40:16.137095: step 12950, loss = 1.01 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:40:17.455163: step 12960, loss = 0.90 (971.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:40:18.744650: step 12970, loss = 1.05 (992.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:40:19.995576: step 12980, loss = 1.09 (1023.2 examples/sec; 0.125 sec/batch)
2017-05-07 20:40:21.256732: step 12990, loss = 0.89 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:40:22.609353: step 13000, loss = 0.85 (946.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:40:23.774413: step 13010, loss = 1.07 (1098.7 examples/sec; 0.117 sec/batch)
2017-05-07 20:40:25.088299: step 13020, loss = 0.97 (974.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:40:26.379015: step 13030, loss = 0.83 (991.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:40:27.669695: step 13040, loss = 0.95 (991.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:40:28.937855: step 13050, loss = 0.77 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:40:30.220579: step 13060, loss = 1.04 (997.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:40:31.492201: step 13070, loss = 1.19 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:40:32.760242: step 13080, loss = 0.87 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:40:34.027441: step 13090, loss = 0.83 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:40:35.393860: step 13100, loss = 1.01 (936.8 examples/sec; 0.137 sec/batch)
2017-05-07 20:40:36.563371: step 13110, loss = 0.88 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-07 20:40:37.850357: step 13120, loss = 1.00 (994.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:40:39.135213: step 13130, loss = 1.02 (996.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:40:40.413444: step 13140, loss = 0.94 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:40:41.667442: step 13150, loss = 0.97 (1020.7 examples/sec; 0.125 sec/batch)
2017-05-07 20:40:42.940562: step 13160, loss = 0.92 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:40:44.211808: step 13170, loss = 0.90 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:40:45.483081: step 13180, loss = 0.89 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:40:46.742767: step 13190, loss = 0.87 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:40:48.094693: step 13200, loss = 0.90 (946.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:40:49.269307: step 13210, loss = 0.91 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-07 20:40:50.534370: step 13220, loss = 1.11 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:40:51.786591: step 13230, loss = 0.83 (1022.2 examples/sec; 0.125 sec/batch)
2017-05-07 20:40:53.043858: step 13240, loss = 1.02 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:40:54.300396: step 13250, loss = 1.20 (1018.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:40:55.573814: step 13260, loss = 1.11 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:40:56.863725: step 13270, loss = 0.96 (992.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:40:58.118119: step 13280, loss = 0.96 (1020.4 examples/sec; 0.125 sec/batch)
2017-05-07 20:40:59.401956: step 13290, loss = 0.77 (997.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:41:00.756050: step 13300, loss = 0.99 (945.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:41:01.931901: step 13310, loss = 0.93 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-07 20:41:03.186008: step 13320, loss = 0.98 (1020.6 examples/sec; 0.125 sec/batch)
2017-05-07 20:41:04.456039: step 13330, loss = 0.91 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:41:05.699885: step 13340, loss = 0.65 (1029.1 examples/sec; 0.124 sec/batch)
2017-05-07 20:41:06.967571: step 13350, loss = 1.10 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:41:08.246615: step 13360, loss = 1.02 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:41:09.506435: step 13370, loss = 1.10 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:41:10.762748: step 13380, loss = 0.89 (1018.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:41:12.020544: step 13390, loss = 0.77 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:41:13.407421: step 13400, loss = 0.90 (922.9 examples/sec; 0.139 sec/batch)
2017-05-07 20:41:14.585310: step 13410, loss = 1.09 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-07 20:41:15.839134: step 13420, loss = 0.73 (1020.9 examples/sec; 0.125 sec/batch)
2017-05-07 20:41:17.102694: step 13430, loss = 0.73 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:41:18.349612: step 13440, loss = 0.71 (1026.5 examples/sec; 0.125 sec/batch)
2017-05-07 20:41:19.613103: step 13450, loss = 0.97 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:41:20.897261: step 13460, loss = 1.15 (996.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:41:22.155698: step 13470, loss = 0.78 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:41:23.420932: step 13480, loss = 0.83 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:41:24.682120: step 13490, loss = 0.86 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:41:26.044178: step 13500, loss = 0.90 (939.8 examples/sec; 0.136 sec/batch)
2017-05-07 20:41:27.220292: step 13510, loss = 0.94 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-07 20:41:28.479893: step 13520, loss = 0.75 (1016.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:41:29.740481: step 13530, loss = 0.81 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:41:31.002211: step 13540, loss = 0.96 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:41:32.277674: step 13550, loss = 0.83 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:41:33.551495: step 13560, loss = 0.95 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:41:34.809589: step 13570, loss = 0.82 (1017.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:41:36.108281: step 13580, loss = 0.98 (985.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:41:37.398423: step 13590, loss = 0.91 (992.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:41:38.747829: step 13600, loss = 0.74 (948.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:41:39.927202: step 13610, loss = 0.87 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-07 20:41:41.199374: step 13620, loss = 0.87 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:41:42.465244: step 13630, loss = 0.84 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:41:43.738769: step 13640, loss = 0.97 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:41:45.004458: step 13650, loss = 1.18 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:41:46.299710: step 13660, loss = 0.89 (988.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:41:47.602017: step 13670, loss = 1.02 (982.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:41:4E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 285 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
8.900258: step 13680, loss = 0.82 (985.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:41:50.151302: step 13690, loss = 1.05 (1023.1 examples/sec; 0.125 sec/batch)
2017-05-07 20:41:51.516757: step 13700, loss = 1.08 (937.4 examples/sec; 0.137 sec/batch)
2017-05-07 20:41:52.707164: step 13710, loss = 0.82 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-07 20:41:53.974804: step 13720, loss = 0.73 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:41:55.232566: step 13730, loss = 0.89 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:41:56.504002: step 13740, loss = 0.98 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:41:57.752868: step 13750, loss = 1.02 (1024.9 examples/sec; 0.125 sec/batch)
2017-05-07 20:41:59.022634: step 13760, loss = 0.90 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:42:00.298550: step 13770, loss = 0.98 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:42:01.554747: step 13780, loss = 1.05 (1019.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:42:02.825306: step 13790, loss = 0.76 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:42:04.194002: step 13800, loss = 0.90 (935.2 examples/sec; 0.137 sec/batch)
2017-05-07 20:42:05.386281: step 13810, loss = 1.06 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-07 20:42:06.686370: step 13820, loss = 0.99 (984.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:42:07.957105: step 13830, loss = 1.12 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:42:09.205861: step 13840, loss = 0.73 (1025.0 examples/sec; 0.125 sec/batch)
2017-05-07 20:42:10.458384: step 13850, loss = 0.83 (1021.9 examples/sec; 0.125 sec/batch)
2017-05-07 20:42:11.721628: step 13860, loss = 0.89 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:42:12.995214: step 13870, loss = 1.18 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:42:14.243113: step 13880, loss = 1.13 (1025.7 examples/sec; 0.125 sec/batch)
2017-05-07 20:42:15.513848: step 13890, loss = 1.02 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:42:16.911696: step 13900, loss = 0.74 (915.7 examples/sec; 0.140 sec/batch)
2017-05-07 20:42:18.078838: step 13910, loss = 0.86 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-07 20:42:19.366154: step 13920, loss = 0.94 (994.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:42:20.638726: step 13930, loss = 1.04 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:42:21.899663: step 13940, loss = 1.09 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:42:23.163044: step 13950, loss = 0.91 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:42:24.432532: step 13960, loss = 0.90 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:42:25.699190: step 13970, loss = 1.02 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:42:26.982949: step 13980, loss = 1.07 (997.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:42:28.267918: step 13990, loss = 0.84 (996.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:42:29.605871: step 14000, loss = 1.05 (956.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:42:30.783640: step 14010, loss = 0.82 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-07 20:42:32.064378: step 14020, loss = 0.88 (999.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:42:33.337588: step 14030, loss = 1.17 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:42:34.586028: step 14040, loss = 0.69 (1025.3 examples/sec; 0.125 sec/batch)
2017-05-07 20:42:35.842664: step 14050, loss = 1.03 (1018.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:42:37.104783: step 14060, loss = 0.87 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:42:38.395656: step 14070, loss = 0.85 (991.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:42:39.682019: step 14080, loss = 0.76 (995.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:42:40.943758: step 14090, loss = 0.93 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:42:42.310897: step 14100, loss = 0.82 (936.3 examples/sec; 0.137 sec/batch)
2017-05-07 20:42:43.469069: step 14110, loss = 0.95 (1105.2 examples/sec; 0.116 sec/batch)
2017-05-07 20:42:44.732650: step 14120, loss = 0.93 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:42:45.995889: step 14130, loss = 0.92 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:42:47.248004: step 14140, loss = 0.85 (1022.3 examples/sec; 0.125 sec/batch)
2017-05-07 20:42:48.514168: step 14150, loss = 0.96 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:42:49.784108: step 14160, loss = 0.82 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:42:51.047351: step 14170, loss = 0.96 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:42:52.328128: step 14180, loss = 0.83 (999.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:42:53.581980: step 14190, loss = 0.85 (1020.9 examples/sec; 0.125 sec/batch)
2017-05-07 20:42:54.947209: step 14200, loss = 0.86 (937.6 examples/sec; 0.137 sec/batch)
2017-05-07 20:42:56.132100: step 14210, loss = 0.87 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-07 20:42:57.403070: step 14220, loss = 1.07 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:42:58.664827: step 14230, loss = 0.93 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:42:59.945644: step 14240, loss = 0.82 (999.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:43:01.208462: step 14250, loss = 0.87 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:43:02.460387: step 14260, loss = 0.99 (1022.4 examples/sec; 0.125 sec/batch)
2017-05-07 20:43:03.746319: step 14270, loss = 0.77 (995.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:43:05.029605: step 14280, loss = 0.87 (997.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:43:06.314606: step 14290, loss = 0.86 (996.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:43:07.672053: step 14300, loss = 0.93 (942.9 examples/sec; 0.136 sec/batch)
2017-05-07 20:43:08.861869: step 14310, loss = 0.91 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-07 20:43:10.181033: step 14320, loss = 1.16 (970.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:43:11.492899: step 14330, loss = 0.86 (975.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:43:12.790149: step 14340, loss = 0.90 (986.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:43:14.085277: step 14350, loss = 0.88 (988.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:43:15.334346: step 14360, loss = 1.04 (1024.8 examples/sec; 0.125 sec/batch)
2017-05-07 20:43:16.597844: step 14370, loss = 0.84 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:43:17.827378: step 14380, loss = 0.80 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-07 20:43:19.115892: step 14390, loss = 0.89 (993.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:43:20.515645: step 14400, loss = 0.97 (914.4 examples/sec; 0.140 sec/batch)
2017-05-07 20:43:21.663400: step 14410, loss = 1.11 (1115.2 examples/sec; 0.115 sec/batch)
2017-05-07 20:43:22.938670: step 14420, loss = 0.76 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:43:24.209867: step 14430, loss = 0.86 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:43:25.470452: step 14440, loss = 1.00 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:43:26.732169: step 14450, loss = 0.87 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:43:28.014783: step 14460, loss = 1.04 (998.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:43:29.284056: step 14470, loss = 0.83 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:43:30.541767: step 14480, loss = 0.99 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:43:31.809441: step 14490, loss = 0.89 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:43:33.195082: step 14500, loss = 0.82 (923.8 examples/sec; 0.139 sec/batch)
2017-05-07 20:43:34.361365: step 14510, loss = 0.80 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-07 20:43:35.643589: step 14520, loss = 0.95 (998.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:43:36.920919: step 14530, loss = 0.76 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:43:38.170374: step 14540, loss = 0.84 (1024.5 examples/sec; 0.125 sec/batch)
2017-05-07 20:43:39.434080: step 14550, loss = 1.00 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:43:40.694728: step 14560, loss = 0.81 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:43:41.957338: step 14570, loss = 0.93 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:43:43.210738: step 14580, loss = 0.E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 306 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
86 (1021.2 examples/sec; 0.125 sec/batch)
2017-05-07 20:43:44.496394: step 14590, loss = 1.14 (995.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:43:45.863338: step 14600, loss = 0.98 (936.4 examples/sec; 0.137 sec/batch)
2017-05-07 20:43:47.025358: step 14610, loss = 0.97 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-07 20:43:48.302483: step 14620, loss = 0.94 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:43:49.557091: step 14630, loss = 0.81 (1020.2 examples/sec; 0.125 sec/batch)
2017-05-07 20:43:50.810915: step 14640, loss = 0.89 (1020.9 examples/sec; 0.125 sec/batch)
2017-05-07 20:43:52.080593: step 14650, loss = 0.79 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:43:53.354187: step 14660, loss = 0.93 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:43:54.635607: step 14670, loss = 0.87 (998.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:43:55.893549: step 14680, loss = 0.86 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:43:57.146493: step 14690, loss = 1.07 (1021.6 examples/sec; 0.125 sec/batch)
2017-05-07 20:43:58.521545: step 14700, loss = 0.82 (930.9 examples/sec; 0.138 sec/batch)
2017-05-07 20:43:59.715095: step 14710, loss = 1.09 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-07 20:44:00.998511: step 14720, loss = 0.84 (997.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:44:02.267946: step 14730, loss = 0.84 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:44:03.558056: step 14740, loss = 1.05 (992.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:44:04.849766: step 14750, loss = 0.87 (990.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:44:06.106782: step 14760, loss = 0.99 (1018.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:44:07.368482: step 14770, loss = 0.85 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:44:08.629407: step 14780, loss = 1.12 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:44:09.884926: step 14790, loss = 0.91 (1019.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:44:11.238491: step 14800, loss = 0.93 (945.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:44:12.415242: step 14810, loss = 0.76 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-07 20:44:13.681647: step 14820, loss = 0.95 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:44:14.948030: step 14830, loss = 0.70 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:44:16.214808: step 14840, loss = 0.95 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:44:17.473838: step 14850, loss = 0.89 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:44:18.740776: step 14860, loss = 0.92 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:44:20.003571: step 14870, loss = 0.95 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:44:21.271520: step 14880, loss = 0.84 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:44:22.523694: step 14890, loss = 0.89 (1022.2 examples/sec; 0.125 sec/batch)
2017-05-07 20:44:23.920707: step 14900, loss = 0.98 (916.2 examples/sec; 0.140 sec/batch)
2017-05-07 20:44:25.128890: step 14910, loss = 0.87 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-07 20:44:26.394722: step 14920, loss = 0.92 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:44:27.660816: step 14930, loss = 0.70 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:44:28.936426: step 14940, loss = 0.94 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:44:30.211719: step 14950, loss = 0.86 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:44:31.493509: step 14960, loss = 1.20 (998.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:44:32.777537: step 14970, loss = 1.01 (996.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:44:34.049872: step 14980, loss = 0.86 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:44:35.323602: step 14990, loss = 0.97 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:44:36.675605: step 15000, loss = 0.82 (946.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:44:37.836644: step 15010, loss = 1.02 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-07 20:44:39.099625: step 15020, loss = 0.88 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:44:40.353380: step 15030, loss = 0.84 (1020.9 examples/sec; 0.125 sec/batch)
2017-05-07 20:44:41.626642: step 15040, loss = 0.97 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:44:42.878121: step 15050, loss = 1.13 (1022.8 examples/sec; 0.125 sec/batch)
2017-05-07 20:44:44.157277: step 15060, loss = 0.99 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:44:45.444669: step 15070, loss = 0.87 (994.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:44:46.761133: step 15080, loss = 1.04 (972.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:44:48.021217: step 15090, loss = 0.96 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:44:49.399522: step 15100, loss = 0.94 (928.7 examples/sec; 0.138 sec/batch)
2017-05-07 20:44:50.615365: step 15110, loss = 0.91 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-07 20:44:51.906869: step 15120, loss = 0.92 (991.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:44:53.181799: step 15130, loss = 1.11 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:44:54.450921: step 15140, loss = 0.88 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:44:55.769859: step 15150, loss = 0.92 (970.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:44:57.038579: step 15160, loss = 0.97 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:44:58.320526: step 15170, loss = 0.83 (998.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:44:59.588737: step 15180, loss = 0.63 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:45:00.853200: step 15190, loss = 0.73 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:45:02.226816: step 15200, loss = 0.94 (931.8 examples/sec; 0.137 sec/batch)
2017-05-07 20:45:03.433105: step 15210, loss = 0.86 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-07 20:45:04.723133: step 15220, loss = 0.88 (992.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:45:05.975095: step 15230, loss = 0.72 (1022.4 examples/sec; 0.125 sec/batch)
2017-05-07 20:45:07.247479: step 15240, loss = 0.82 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:45:08.512047: step 15250, loss = 0.97 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:45:09.783475: step 15260, loss = 0.86 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:45:11.066940: step 15270, loss = 0.91 (997.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:45:12.339886: step 15280, loss = 1.01 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:45:13.616381: step 15290, loss = 0.87 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:45:14.973892: step 15300, loss = 0.96 (942.9 examples/sec; 0.136 sec/batch)
2017-05-07 20:45:16.170561: step 15310, loss = 0.97 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-07 20:45:17.476356: step 15320, loss = 0.87 (980.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:45:18.777629: step 15330, loss = 1.18 (983.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:45:20.047116: step 15340, loss = 0.85 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:45:21.316837: step 15350, loss = 1.15 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:45:22.622919: step 15360, loss = 0.78 (980.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:45:23.894331: step 15370, loss = 1.09 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:45:25.165217: step 15380, loss = 1.06 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:45:26.442119: step 15390, loss = 0.95 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:45:27.826823: step 15400, loss = 0.97 (924.4 examples/sec; 0.138 sec/batch)
2017-05-07 20:45:28.994358: step 15410, loss = 0.77 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-07 20:45:30.272168: step 15420, loss = 1.06 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:45:31.580375: step 15430, loss = 0.97 (978.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:45:32.869359: step 15440, loss = 0.99 (993.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:45:34.141645: step 15450, loss = 0.97 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:45:35.441039: step 15460, loss = 0.92 (985.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:45:36.710121: step 15470, loss = 1.08 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:45:38.026536: step 15480, loss = 0.88 (972.3 examples/sec; 0.132 sE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 326 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
ec/batch)
2017-05-07 20:45:39.309313: step 15490, loss = 1.08 (997.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:45:40.674448: step 15500, loss = 0.93 (937.6 examples/sec; 0.137 sec/batch)
2017-05-07 20:45:41.854097: step 15510, loss = 0.79 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-07 20:45:43.160318: step 15520, loss = 0.93 (979.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:45:44.447624: step 15530, loss = 0.82 (994.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:45:45.698340: step 15540, loss = 0.81 (1023.4 examples/sec; 0.125 sec/batch)
2017-05-07 20:45:46.979078: step 15550, loss = 0.79 (999.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:45:48.250748: step 15560, loss = 0.84 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:45:49.539565: step 15570, loss = 0.82 (993.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:45:50.807427: step 15580, loss = 0.82 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:45:52.076518: step 15590, loss = 0.92 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:45:53.439587: step 15600, loss = 0.79 (939.1 examples/sec; 0.136 sec/batch)
2017-05-07 20:45:54.624434: step 15610, loss = 0.96 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-07 20:45:55.923322: step 15620, loss = 0.87 (985.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:45:57.210269: step 15630, loss = 1.17 (994.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:45:58.471509: step 15640, loss = 1.00 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:45:59.767125: step 15650, loss = 0.96 (987.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:46:01.048828: step 15660, loss = 0.77 (998.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:46:02.333025: step 15670, loss = 1.01 (996.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:46:03.612823: step 15680, loss = 1.11 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:46:04.878508: step 15690, loss = 0.71 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:46:06.246380: step 15700, loss = 0.90 (935.8 examples/sec; 0.137 sec/batch)
2017-05-07 20:46:07.430317: step 15710, loss = 0.97 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-07 20:46:08.718707: step 15720, loss = 0.77 (993.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:46:10.009331: step 15730, loss = 1.05 (991.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:46:11.252349: step 15740, loss = 0.95 (1029.8 examples/sec; 0.124 sec/batch)
2017-05-07 20:46:12.537997: step 15750, loss = 0.94 (995.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:46:13.811450: step 15760, loss = 0.79 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:46:15.089171: step 15770, loss = 0.92 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:46:16.397585: step 15780, loss = 0.94 (978.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:46:17.685061: step 15790, loss = 0.96 (994.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:46:19.062150: step 15800, loss = 0.96 (929.5 examples/sec; 0.138 sec/batch)
2017-05-07 20:46:20.264114: step 15810, loss = 0.97 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-07 20:46:21.546757: step 15820, loss = 0.86 (997.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:46:22.820218: step 15830, loss = 1.01 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:46:24.097251: step 15840, loss = 0.89 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:46:25.389012: step 15850, loss = 0.84 (990.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:46:26.664650: step 15860, loss = 0.90 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:46:27.964640: step 15870, loss = 0.89 (984.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:46:29.263360: step 15880, loss = 1.05 (985.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:46:30.552585: step 15890, loss = 0.95 (992.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:46:31.954775: step 15900, loss = 0.98 (912.9 examples/sec; 0.140 sec/batch)
2017-05-07 20:46:33.140592: step 15910, loss = 1.10 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-07 20:46:34.404248: step 15920, loss = 0.77 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:46:35.668564: step 15930, loss = 1.19 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:46:36.964299: step 15940, loss = 0.98 (987.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:46:38.234681: step 15950, loss = 0.83 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:46:39.535878: step 15960, loss = 0.76 (983.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:46:40.830341: step 15970, loss = 0.85 (988.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:46:42.098371: step 15980, loss = 0.87 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:46:43.418545: step 15990, loss = 0.94 (969.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:46:44.801023: step 16000, loss = 0.88 (925.9 examples/sec; 0.138 sec/batch)
2017-05-07 20:46:45.984592: step 16010, loss = 0.81 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-07 20:46:47.271534: step 16020, loss = 0.89 (994.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:46:48.550650: step 16030, loss = 1.00 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:46:49.843188: step 16040, loss = 0.92 (990.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:46:51.138667: step 16050, loss = 0.85 (988.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:46:52.443087: step 16060, loss = 0.84 (981.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:46:53.759304: step 16070, loss = 0.85 (972.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:46:55.043118: step 16080, loss = 0.82 (997.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:46:56.370382: step 16090, loss = 1.01 (964.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:46:57.764357: step 16100, loss = 1.01 (918.2 examples/sec; 0.139 sec/batch)
2017-05-07 20:46:58.957885: step 16110, loss = 0.88 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-07 20:47:00.263162: step 16120, loss = 0.90 (980.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:47:01.538280: step 16130, loss = 0.89 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:47:02.828709: step 16140, loss = 0.91 (991.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:47:04.120787: step 16150, loss = 0.88 (990.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:47:05.399943: step 16160, loss = 0.85 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:47:06.660491: step 16170, loss = 0.80 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:47:07.935960: step 16180, loss = 0.88 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:47:09.216782: step 16190, loss = 1.00 (999.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:47:10.603098: step 16200, loss = 0.93 (923.3 examples/sec; 0.139 sec/batch)
2017-05-07 20:47:11.825231: step 16210, loss = 0.97 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-07 20:47:13.136819: step 16220, loss = 0.94 (975.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:47:14.405879: step 16230, loss = 0.80 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:47:15.656213: step 16240, loss = 0.79 (1023.7 examples/sec; 0.125 sec/batch)
2017-05-07 20:47:16.944144: step 16250, loss = 0.91 (993.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:47:18.208278: step 16260, loss = 0.83 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:47:19.461684: step 16270, loss = 0.72 (1021.2 examples/sec; 0.125 sec/batch)
2017-05-07 20:47:20.741189: step 16280, loss = 1.02 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:47:22.013298: step 16290, loss = 0.95 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:47:23.393072: step 16300, loss = 1.03 (927.7 examples/sec; 0.138 sec/batch)
2017-05-07 20:47:24.558936: step 16310, loss = 0.92 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-07 20:47:25.831628: step 16320, loss = 0.89 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:47:27.102393: step 16330, loss = 0.87 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:47:28.370385: step 16340, loss = 1.04 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:47:29.634476: step 16350, loss = 0.90 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:47:30.932444: step 16360, loss = 0.91 (986.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:47:32.224117: step 16370, loss = 0.91 (991.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:47:33.508653: step 16380, loss = 0.83 (996.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:47:34.807669: step 16390, loss = 0.81 (985.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:47:36.182902: step 16400, loss = 0.88 (930.8 examples/sec; 0.138 sec/batch)
2017-05-07 20:47:37.368302: step 16410, loss = 0.73 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-07 20:47:38.629802: step 16420, loss = 0.91 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:47:39.915524: step 16430, loss = 1.12 (995.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:47:41.197716: step 16440, loss = 0.97 (998.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:47:42.463006: step 16450, loss = 0.91 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:47:43.740398: step 16460, loss = 0.94 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:47:45.023373: step 16470, loss = 0.83 (997.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:47:46.326747: step 16480, loss = 1.15 (982.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:47:47.622901: step 16490, loss = 1.22 (987.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:47:49.000168: step 16500, loss = 0.99 (929.4 examples/sec; 0.138 sec/batch)
2017-05-07 20:47:50.181628: step 16510, loss = 1.12 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-07 20:47:51.469346: step 16520, loss = 0.83 (994.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:47:52.734334: step 16530, loss = 0.83 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:47:53.995561: step 16540, loss = 0.93 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:47:55.278409: step 16550, loss = 0.75 (997.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:47:56.574516: step 16560, loss = 0.84 (987.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:47:57.859141: step 16570, loss = 0.92 (996.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:47:59.125842: step 16580, loss = 0.89 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:48:00.402820: step 16590, loss = 0.85 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:48:01.738573: step 16600, loss = 0.82 (958.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:48:02.929231: step 16610, loss = 0.77 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-07 20:48:04.200534: step 16620, loss = 0.75 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:48:05.475497: step 16630, loss = 0.88 (1003.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:48:06.752211: step 16640, loss = 0.88 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:48:08.018823: step 16650, loss = 0.97 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:48:09.284549: step 16660, loss = 0.72 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:48:10.524913: step 16670, loss = 0.85 (1031.9 examples/sec; 0.124 sec/batch)
2017-05-07 20:48:11.819549: step 16680, loss = 0.97 (988.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:48:13.079325: step 16690, loss = 1.23 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:48:14.443713: step 16700, loss = 0.89 (938.1 examples/sec; 0.136 sec/batch)
2017-05-07 20:48:15.640385: step 16710, loss = 0.73 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-07 20:48:16.929256: step 16720, loss = 1.07 (993.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:48:18.201774: step 16730, loss = 1.01 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:48:19.487955: step 16740, loss = 0.93 (995.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:48:20.784894: step 16750, loss = 0.76 (986.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:48:22.037091: step 16760, loss = 0.87 (1022.2 examples/sec; 0.125 sec/batch)
2017-05-07 20:48:23.322598: step 16770, loss = 1.03 (995.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:48:24.587492: step 16780, loss = 0.79 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:48:25.846257: step 16790, loss = 0.91 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:48:27.208082: step 16800, loss = 0.80 (939.9 examples/sec; 0.136 sec/batch)
2017-05-07 20:48:28.383182: step 16810, loss = 0.95 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-07 20:48:29.641630: step 16820, loss = 0.97 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:48:30.891605: step 16830, loss = 0.82 (1024.0 examples/sec; 0.125 sec/batch)
2017-05-07 20:48:32.173835: step 16840, loss = 0.91 (998.3 exaE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 346 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
mples/sec; 0.128 sec/batch)
2017-05-07 20:48:33.424719: step 16850, loss = 0.90 (1023.3 examples/sec; 0.125 sec/batch)
2017-05-07 20:48:34.669478: step 16860, loss = 0.79 (1028.3 examples/sec; 0.124 sec/batch)
2017-05-07 20:48:35.945144: step 16870, loss = 0.94 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:48:37.213514: step 16880, loss = 1.01 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:48:38.498619: step 16890, loss = 0.91 (996.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:48:39.846392: step 16900, loss = 0.94 (949.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:48:41.018532: step 16910, loss = 0.81 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-07 20:48:42.275272: step 16920, loss = 0.86 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:48:43.547473: step 16930, loss = 0.84 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:48:44.818815: step 16940, loss = 0.85 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:48:46.098162: step 16950, loss = 1.00 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:48:47.387305: step 16960, loss = 1.17 (992.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:48:48.632200: step 16970, loss = 0.96 (1028.2 examples/sec; 0.124 sec/batch)
2017-05-07 20:48:49.918470: step 16980, loss = 0.84 (995.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:48:51.191773: step 16990, loss = 0.89 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:48:52.554494: step 17000, loss = 0.79 (939.3 examples/sec; 0.136 sec/batch)
2017-05-07 20:48:53.731208: step 17010, loss = 1.11 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-07 20:48:54.996146: step 17020, loss = 0.80 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:48:56.267446: step 17030, loss = 0.87 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:48:57.524955: step 17040, loss = 0.91 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:48:58.799158: step 17050, loss = 0.96 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:49:00.053625: step 17060, loss = 0.91 (1020.3 examples/sec; 0.125 sec/batch)
2017-05-07 20:49:01.314718: step 17070, loss = 1.13 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:49:02.598355: step 17080, loss = 0.90 (997.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:49:03.896938: step 17090, loss = 0.77 (985.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:49:05.279845: step 17100, loss = 0.95 (925.6 examples/sec; 0.138 sec/batch)
2017-05-07 20:49:06.446053: step 17110, loss = 0.93 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-07 20:49:07.721765: step 17120, loss = 1.05 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:49:08.995757: step 17130, loss = 0.76 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:49:10.267457: step 17140, loss = 0.80 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:49:11.536347: step 17150, loss = 0.98 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:49:12.794782: step 17160, loss = 0.97 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:49:14.042143: step 17170, loss = 0.86 (1026.2 examples/sec; 0.125 sec/batch)
2017-05-07 20:49:15.321189: step 17180, loss = 0.86 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:49:16.607199: step 17190, loss = 0.90 (995.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:49:17.952518: step 17200, loss = 0.85 (951.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:49:19.118454: step 17210, loss = 0.81 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-07 20:49:20.367198: step 17220, loss = 0.76 (1025.0 examples/sec; 0.125 sec/batch)
2017-05-07 20:49:21.630612: step 17230, loss = 0.84 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:49:22.878614: step 17240, loss = 0.77 (1025.6 examples/sec; 0.125 sec/batch)
2017-05-07 20:49:24.174416: step 17250, loss = 1.35 (987.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:49:25.435099: step 17260, loss = 0.72 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:49:26.704027: step 17270, loss = 0.90 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:49:27.992723: step 17280, loss = 1.10 (993.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:49:29.270373: step 17290, loss = 1.21 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:49:30.608622: step 17300, loss = 1.06 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:49:31.771674: step 17310, loss = 0.87 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-07 20:49:33.089063: step 17320, loss = 0.90 (971.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:49:34.349535: step 17330, loss = 0.91 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:49:35.635903: step 17340, loss = 0.80 (995.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:49:36.893749: step 17350, loss = 0.75 (1017.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:49:38.173759: step 17360, loss = 0.87 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:49:39.433904: step 17370, loss = 0.74 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:49:40.693998: step 17380, loss = 0.92 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:49:41.963156: step 17390, loss = 0.84 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:49:43.299620: step 17400, loss = 0.80 (957.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:49:44.451319: step 17410, loss = 0.88 (1111.4 examples/sec; 0.115 sec/batch)
2017-05-07 20:49:45.740510: step 17420, loss = 1.23 (992.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:49:47.007621: step 17430, loss = 0.73 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:49:48.291826: step 17440, loss = 0.88 (996.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:49:49.566379: step 17450, loss = 0.95 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:49:50.848225: step 17460, loss = 0.92 (998.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:49:52.114204: step 17470, loss = 0.95 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:49:53.396521: step 17480, loss = 1.22 (998.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:49:54.656319: step 17490, loss = 0.89 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:49:56.044737: step 17500, loss = 0.95 (921.9 examples/sec; 0.139 sec/batch)
2017-05-07 20:49:57.219685: step 17510, loss = 0.85 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-07 20:49:58.472717: step 17520, loss = 0.86 (1021.5 examples/sec; 0.125 sec/batch)
2017-05-07 20:49:59.744792: step 17530, loss = 0.79 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:50:01.026139: step 17540, loss = 0.84 (999.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:50:02.290400: step 17550, loss = 0.80 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:50:03.549146: step 17560, loss = 0.92 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:50:04.803403: step 17570, loss = 0.85 (1020.5 examples/sec; 0.125 sec/batch)
2017-05-07 20:50:06.080197: step 17580, loss = 0.91 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:50:07.368162: step 17590, loss = 0.86 (993.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:50:08.768664: step 17600, loss = 1.09 (914.0 examples/sec; 0.140 sec/batch)
2017-05-07 20:50:09.960996: step 17610, loss = 0.90 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-07 20:50:11.272385: step 17620, loss = 0.82 (976.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:50:12.543093: step 17630, loss = 0.86 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:50:13.818714: step 17640, loss = 0.93 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:50:15.093327: step 17650, loss = 0.87 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:50:16.369590: step 17660, loss = 1.06 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:50:17.655088: step 17670, loss = 0.90 (995.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:50:18.921612: step 17680, loss = 0.94 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:50:20.203236: step 17690, loss = 0.96 (998.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:50:21.599175: step 17700, loss = 0.87 (916.9 examples/sec; 0.140 sec/batch)
2017-05-07 20:50:22.765895: step 17710, loss = 0.87 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-07 20:50:24.018110: step 17720, loss = 0.82 (1022.2 examples/sec; 0.125 sec/batch)
2017-05-07 20:50:25.301369: step 17730, loss = 0.86 (997.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:50:26.549436: step 17740, loss = 0.98 (1025.6 examples/sec; 0.125 sec/batch)
201E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 366 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
7-05-07 20:50:27.811590: step 17750, loss = 0.88 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:50:29.079280: step 17760, loss = 0.93 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:50:30.343905: step 17770, loss = 0.91 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:50:31.602078: step 17780, loss = 0.98 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:50:32.878661: step 17790, loss = 0.88 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:50:34.252351: step 17800, loss = 0.93 (931.8 examples/sec; 0.137 sec/batch)
2017-05-07 20:50:35.480071: step 17810, loss = 0.79 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-07 20:50:36.758112: step 17820, loss = 1.16 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:50:38.042959: step 17830, loss = 0.95 (996.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:50:39.330562: step 17840, loss = 0.91 (994.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:50:40.630880: step 17850, loss = 0.89 (984.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:50:41.927811: step 17860, loss = 0.87 (986.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:50:43.227548: step 17870, loss = 0.78 (984.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:50:44.506932: step 17880, loss = 0.76 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:50:45.768502: step 17890, loss = 1.05 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:50:47.120274: step 17900, loss = 0.78 (946.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:50:48.311555: step 17910, loss = 0.78 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-07 20:50:49.572399: step 17920, loss = 0.98 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:50:50.822996: step 17930, loss = 0.88 (1023.5 examples/sec; 0.125 sec/batch)
2017-05-07 20:50:52.088378: step 17940, loss = 0.95 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:50:53.369421: step 17950, loss = 1.03 (999.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:50:54.661471: step 17960, loss = 0.97 (990.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:50:55.952863: step 17970, loss = 0.79 (991.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:50:57.232234: step 17980, loss = 0.90 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:50:58.505506: step 17990, loss = 0.85 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:50:59.887123: step 18000, loss = 0.96 (926.5 examples/sec; 0.138 sec/batch)
2017-05-07 20:51:01.047921: step 18010, loss = 1.05 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-07 20:51:02.302036: step 18020, loss = 0.85 (1020.6 examples/sec; 0.125 sec/batch)
2017-05-07 20:51:03.583759: step 18030, loss = 0.86 (998.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:51:04.869521: step 18040, loss = 0.84 (995.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:51:06.147622: step 18050, loss = 1.14 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:51:07.429129: step 18060, loss = 1.01 (998.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:51:08.690673: step 18070, loss = 1.00 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:51:09.959088: step 18080, loss = 1.06 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:51:11.256594: step 18090, loss = 0.95 (986.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:51:12.628590: step 18100, loss = 0.83 (932.9 examples/sec; 0.137 sec/batch)
2017-05-07 20:51:13.793101: step 18110, loss = 0.90 (1099.2 examples/sec; 0.116 sec/batch)
2017-05-07 20:51:15.074007: step 18120, loss = 0.83 (999.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:51:16.355625: step 18130, loss = 0.82 (998.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:51:17.653390: step 18140, loss = 1.15 (986.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:51:18.916751: step 18150, loss = 0.90 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:51:20.186207: step 18160, loss = 0.90 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:51:21.447335: step 18170, loss = 0.89 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:51:22.720756: step 18180, loss = 0.91 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:51:24.019572: step 18190, loss = 0.83 (985.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:51:25.400901: step 18200, loss = 0.87 (926.6 examples/sec; 0.138 sec/batch)
2017-05-07 20:51:26.631710: step 18210, loss = 0.88 (1040.0 examples/sec; 0.123 sec/batch)
2017-05-07 20:51:27.912553: step 18220, loss = 1.01 (999.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:51:29.207139: step 18230, loss = 0.80 (988.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:51:30.492089: step 18240, loss = 1.02 (996.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:51:31.756094: step 18250, loss = 0.88 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:51:33.055222: step 18260, loss = 0.84 (985.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:51:34.332697: step 18270, loss = 0.94 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:51:35.612204: step 18280, loss = 1.04 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:51:36.890460: step 18290, loss = 0.73 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:51:38.252671: step 18300, loss = 1.05 (939.6 examples/sec; 0.136 sec/batch)
2017-05-07 20:51:39.441923: step 18310, loss = 0.84 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-07 20:51:40.744176: step 18320, loss = 0.75 (982.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:51:42.008868: step 18330, loss = 0.88 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:51:43.269053: step 18340, loss = 0.89 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:51:44.543564: step 18350, loss = 0.76 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:51:45.808535: step 18360, loss = 0.92 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:51:47.092753: step 18370, loss = 0.77 (996.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:51:48.393375: step 18380, loss = 1.22 (984.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:51:49.660820: step 18390, loss = 0.72 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:51:51.044480: step 18400, loss = 0.72 (925.1 examples/sec; 0.138 sec/batch)
2017-05-07 20:51:52.235979: step 18410, loss = 0.92 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-07 20:51:53.516423: step 18420, loss = 0.89 (999.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:51:54.810825: step 18430, loss = 0.92 (988.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:51:56.080036: step 18440, loss = 0.70 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:51:57.365622: step 18450, loss = 0.96 (995.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:51:58.619679: step 18460, loss = 0.94 (1020.7 examples/sec; 0.125 sec/batch)
2017-05-07 20:51:59.890735: step 18470, loss = 0.86 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:52:01.166902: step 18480, loss = 0.97 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:52:02.477495: step 18490, loss = 0.84 (976.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:52:03.849423: step 18500, loss = 0.93 (933.0 examples/sec; 0.137 sec/batch)
2017-05-07 20:52:05.022230: step 18510, loss = 0.95 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-07 20:52:06.279662: step 18520, loss = 0.82 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:52:07.542641: step 18530, loss = 0.71 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:52:08.813811: step 18540, loss = 0.90 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:52:10.077445: step 18550, loss = 0.73 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:52:11.344846: step 18560, loss = 0.87 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:52:12.601604: step 18570, loss = 1.00 (1018.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:52:13.849147: step 18580, loss = 0.76 (1026.0 examples/sec; 0.125 sec/batch)
2017-05-07 20:52:15.122729: step 18590, loss = 1.05 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:52:16.504553: step 18600, loss = 0.87 (926.3 examples/sec; 0.138 sec/batch)
2017-05-07 20:52:17.707104: step 18610, loss = 1.08 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-07 20:52:18.996802: step 18620, loss = 0.95 (992.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:52:20.269617: step 18630, loss = 0.96 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:52:21.587033: step 18640, loss = 0.89 (971.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:52:22.857420: step 18650, losE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 387 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
s = 0.83 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:52:24.181118: step 18660, loss = 1.00 (967.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:52:25.466720: step 18670, loss = 0.83 (995.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:52:26.761383: step 18680, loss = 0.76 (988.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:52:28.028485: step 18690, loss = 0.90 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:52:29.390326: step 18700, loss = 0.92 (939.9 examples/sec; 0.136 sec/batch)
2017-05-07 20:52:30.576009: step 18710, loss = 0.93 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-07 20:52:31.870336: step 18720, loss = 0.82 (988.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:52:33.136622: step 18730, loss = 1.08 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:52:34.390951: step 18740, loss = 0.86 (1020.5 examples/sec; 0.125 sec/batch)
2017-05-07 20:52:35.643680: step 18750, loss = 0.93 (1021.8 examples/sec; 0.125 sec/batch)
2017-05-07 20:52:36.898337: step 18760, loss = 0.94 (1020.2 examples/sec; 0.125 sec/batch)
2017-05-07 20:52:38.216714: step 18770, loss = 1.16 (970.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:52:39.486191: step 18780, loss = 0.87 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:52:40.763143: step 18790, loss = 0.83 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:52:42.116156: step 18800, loss = 0.78 (946.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:52:43.284757: step 18810, loss = 1.02 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-07 20:52:44.549543: step 18820, loss = 0.94 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:52:45.797835: step 18830, loss = 0.86 (1025.4 examples/sec; 0.125 sec/batch)
2017-05-07 20:52:47.064947: step 18840, loss = 0.84 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:52:48.331472: step 18850, loss = 0.89 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:52:49.611874: step 18860, loss = 0.82 (999.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:52:50.893891: step 18870, loss = 1.04 (998.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:52:52.179648: step 18880, loss = 0.93 (995.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:52:53.493386: step 18890, loss = 0.83 (974.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:52:54.860332: step 18900, loss = 0.90 (936.4 examples/sec; 0.137 sec/batch)
2017-05-07 20:52:56.046244: step 18910, loss = 0.96 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-07 20:52:57.350539: step 18920, loss = 0.99 (981.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:52:58.621494: step 18930, loss = 0.89 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:52:59.905239: step 18940, loss = 1.00 (997.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:53:01.206079: step 18950, loss = 0.92 (984.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:53:02.481914: step 18960, loss = 1.03 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:53:03.775641: step 18970, loss = 0.82 (989.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:53:05.059369: step 18980, loss = 0.84 (997.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:53:06.320515: step 18990, loss = 0.77 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:53:07.720048: step 19000, loss = 0.96 (914.6 examples/sec; 0.140 sec/batch)
2017-05-07 20:53:08.914438: step 19010, loss = 0.97 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-07 20:53:10.179420: step 19020, loss = 0.86 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:53:11.455703: step 19030, loss = 0.90 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:53:12.725902: step 19040, loss = 0.91 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:53:14.000523: step 19050, loss = 0.85 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:53:15.268446: step 19060, loss = 0.89 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:53:16.572365: step 19070, loss = 0.92 (981.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:53:17.864956: step 19080, loss = 0.89 (990.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:53:19.128299: step 19090, loss = 0.91 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:53:20.534521: step 19100, loss = 0.84 (910.2 examples/sec; 0.141 sec/batch)
2017-05-07 20:53:21.700199: step 19110, loss = 0.80 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-07 20:53:22.990134: step 19120, loss = 0.88 (992.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:53:24.279086: step 19130, loss = 1.03 (993.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:53:25.543851: step 19140, loss = 0.91 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:53:26.813183: step 19150, loss = 0.77 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:53:28.132133: step 19160, loss = 0.82 (970.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:53:29.405664: step 19170, loss = 0.99 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:53:30.693932: step 19180, loss = 0.78 (993.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:53:31.986131: step 19190, loss = 0.92 (990.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:53:33.375046: step 19200, loss = 0.78 (921.6 examples/sec; 0.139 sec/batch)
2017-05-07 20:53:34.571286: step 19210, loss = 0.85 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-07 20:53:35.857356: step 19220, loss = 1.09 (995.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:53:37.129227: step 19230, loss = 0.82 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:53:38.411950: step 19240, loss = 0.92 (997.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:53:39.692123: step 19250, loss = 1.15 (999.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:53:40.986545: step 19260, loss = 0.96 (988.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:53:42.279422: step 19270, loss = 0.81 (990.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:53:43.580302: step 19280, loss = 0.86 (983.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:53:44.862338: step 19290, loss = 0.90 (998.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:53:46.234711: step 19300, loss = 0.79 (932.7 examples/sec; 0.137 sec/batch)
2017-05-07 20:53:47.404156: step 19310, loss = 0.82 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-07 20:53:48.671757: step 19320, loss = 0.97 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:53:49.970527: step 19330, loss = 0.84 (985.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:53:51.232887: step 19340, loss = 0.85 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:53:52.482951: step 19350, loss = 1.01 (1023.9 examples/sec; 0.125 sec/batch)
2017-05-07 20:53:53.750495: step 19360, loss = 0.88 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:53:55.045435: step 19370, loss = 0.77 (988.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:53:56.365216: step 19380, loss = 0.81 (969.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:53:57.631466: step 19390, loss = 0.86 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:53:59.012678: step 19400, loss = 0.92 (926.7 examples/sec; 0.138 sec/batch)
2017-05-07 20:54:00.205014: step 19410, loss = 0.97 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-07 20:54:01.451762: step 19420, loss = 0.99 (1026.7 examples/sec; 0.125 sec/batch)
2017-05-07 20:54:02.733069: step 19430, loss = 0.97 (999.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:54:04.030918: step 19440, loss = 0.97 (986.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:54:05.323862: step 19450, loss = 1.01 (990.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:54:06.566926: step 19460, loss = 0.89 (1029.7 examples/sec; 0.124 sec/batch)
2017-05-07 20:54:07.853202: step 19470, loss = 0.90 (995.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:54:09.121505: step 19480, loss = 0.87 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:54:10.378244: step 19490, loss = 0.87 (1018.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:54:11.731333: step 19500, loss = 0.93 (946.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:54:12.921260: step 19510, loss = 0.93 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-07 20:54:14.172934: step 19520, loss = 0.88 (1022.6 examples/sec; 0.125 sec/batch)
2017-05-07 20:54:15.447083: step 19530, loss = 0.98 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:54:16.736897: step 19540, loss = 1.06 (992.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:54:18.005901: step 19550, loss = 0.89 (1008.6 examples/sec; 0.127 sec/batch)E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 407 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU

2017-05-07 20:54:19.259645: step 19560, loss = 1.13 (1020.9 examples/sec; 0.125 sec/batch)
2017-05-07 20:54:20.533000: step 19570, loss = 1.26 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:54:21.803859: step 19580, loss = 0.87 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:54:23.095564: step 19590, loss = 0.98 (990.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:54:24.465304: step 19600, loss = 0.91 (934.5 examples/sec; 0.137 sec/batch)
2017-05-07 20:54:25.639757: step 19610, loss = 0.92 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-07 20:54:26.932897: step 19620, loss = 0.71 (989.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:54:28.211546: step 19630, loss = 0.87 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:54:29.500796: step 19640, loss = 1.24 (992.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:54:30.766037: step 19650, loss = 0.92 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:54:32.041436: step 19660, loss = 0.87 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:54:33.319147: step 19670, loss = 0.87 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:54:34.575665: step 19680, loss = 0.86 (1018.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:54:35.829242: step 19690, loss = 0.97 (1021.1 examples/sec; 0.125 sec/batch)
2017-05-07 20:54:37.191570: step 19700, loss = 0.95 (939.6 examples/sec; 0.136 sec/batch)
2017-05-07 20:54:38.374912: step 19710, loss = 0.95 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-07 20:54:39.651325: step 19720, loss = 0.91 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:54:40.903930: step 19730, loss = 0.73 (1021.9 examples/sec; 0.125 sec/batch)
2017-05-07 20:54:42.152647: step 19740, loss = 1.15 (1025.1 examples/sec; 0.125 sec/batch)
2017-05-07 20:54:43.408876: step 19750, loss = 0.74 (1018.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:54:44.671263: step 19760, loss = 0.90 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:54:45.977458: step 19770, loss = 1.07 (979.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:54:47.279283: step 19780, loss = 0.96 (983.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:54:48.551135: step 19790, loss = 0.85 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:54:49.897488: step 19800, loss = 0.84 (950.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:54:51.072547: step 19810, loss = 0.78 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-07 20:54:52.332738: step 19820, loss = 0.73 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:54:53.587214: step 19830, loss = 0.82 (1020.3 examples/sec; 0.125 sec/batch)
2017-05-07 20:54:54.855548: step 19840, loss = 0.95 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:54:56.154127: step 19850, loss = 0.77 (985.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:54:57.423592: step 19860, loss = 0.83 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:54:58.681540: step 19870, loss = 0.84 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:54:59.947082: step 19880, loss = 0.78 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:55:01.214861: step 19890, loss = 0.98 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:55:02.622409: step 19900, loss = 0.70 (909.4 examples/sec; 0.141 sec/batch)
2017-05-07 20:55:03.795865: step 19910, loss = 0.80 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-07 20:55:05.064990: step 19920, loss = 0.83 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:55:06.348257: step 19930, loss = 0.87 (997.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:55:07.621662: step 19940, loss = 0.83 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:55:08.896098: step 19950, loss = 0.81 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:55:10.162850: step 19960, loss = 0.83 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:55:11.471708: step 19970, loss = 1.07 (978.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:55:12.774717: step 19980, loss = 0.78 (982.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:55:14.064289: step 19990, loss = 0.90 (992.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:55:15.413464: step 20000, loss = 0.92 (948.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:55:16.599300: step 20010, loss = 0.92 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-07 20:55:17.850224: step 20020, loss = 1.04 (1023.2 examples/sec; 0.125 sec/batch)
2017-05-07 20:55:19.110575: step 20030, loss = 0.84 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:55:20.371602: step 20040, loss = 0.78 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:55:21.683704: step 20050, loss = 0.97 (975.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:55:22.952636: step 20060, loss = 0.82 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:55:24.228758: step 20070, loss = 0.92 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:55:25.509845: step 20080, loss = 0.89 (999.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:55:26.773179: step 20090, loss = 0.90 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:55:28.148162: step 20100, loss = 0.84 (930.9 examples/sec; 0.137 sec/batch)
2017-05-07 20:55:29.322194: step 20110, loss = 0.94 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-07 20:55:30.591992: step 20120, loss = 1.03 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:55:31.890052: step 20130, loss = 0.96 (986.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:55:33.197943: step 20140, loss = 0.98 (978.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:55:34.485098: step 20150, loss = 1.16 (994.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:55:35.755197: step 20160, loss = 0.77 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:55:37.006058: step 20170, loss = 0.85 (1023.3 examples/sec; 0.125 sec/batch)
2017-05-07 20:55:38.261086: step 20180, loss = 0.80 (1019.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:55:39.523101: step 20190, loss = 0.81 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:55:40.886171: step 20200, loss = 0.91 (939.1 examples/sec; 0.136 sec/batch)
2017-05-07 20:55:42.036156: step 20210, loss = 0.82 (1113.1 examples/sec; 0.115 sec/batch)
2017-05-07 20:55:43.314360: step 20220, loss = 0.91 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:55:44.586482: step 20230, loss = 0.84 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:55:45.881046: step 20240, loss = 0.85 (988.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:55:47.156602: step 20250, loss = 0.97 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:55:48.439900: step 20260, loss = 0.90 (997.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:55:49.733862: step 20270, loss = 0.93 (989.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:55:51.017806: step 20280, loss = 0.88 (996.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:55:52.286760: step 20290, loss = 0.99 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:55:53.678364: step 20300, loss = 0.87 (919.8 examples/sec; 0.139 sec/batch)
2017-05-07 20:55:54.863523: step 20310, loss = 0.77 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-07 20:55:56.135590: step 20320, loss = 0.81 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:55:57.401000: step 20330, loss = 0.74 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:55:58.697669: step 20340, loss = 0.88 (987.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:55:59.960691: step 20350, loss = 0.87 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:56:01.237783: step 20360, loss = 0.91 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:56:02.501342: step 20370, loss = 0.80 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:56:03.761323: step 20380, loss = 0.89 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:56:05.050491: step 20390, loss = 0.96 (992.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:56:06.415134: step 20400, loss = 0.84 (938.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:56:07.619779: step 20410, loss = 1.04 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-07 20:56:08.901877: step 20420, loss = 0.99 (998.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:56:10.199016: step 20430, loss = 0.86 (986.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:56:11.481705: step 20440, loss = 0.84 (997.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:56:12.786632: step 20450, loss = 0.76 (980.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:56:14.069369: step 20E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 427 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
460, loss = 0.94 (997.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:56:15.340994: step 20470, loss = 0.97 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:56:16.644332: step 20480, loss = 0.81 (982.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:56:17.925679: step 20490, loss = 0.95 (998.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:56:19.300930: step 20500, loss = 0.97 (930.7 examples/sec; 0.138 sec/batch)
2017-05-07 20:56:20.491096: step 20510, loss = 0.97 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-07 20:56:21.770386: step 20520, loss = 0.86 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:56:23.053861: step 20530, loss = 0.82 (997.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:56:24.341641: step 20540, loss = 0.68 (994.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:56:25.610451: step 20550, loss = 0.89 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:56:26.901127: step 20560, loss = 1.01 (991.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:56:28.198447: step 20570, loss = 0.80 (986.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:56:29.464791: step 20580, loss = 0.87 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:56:30.725761: step 20590, loss = 0.89 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:56:32.082114: step 20600, loss = 0.79 (943.7 examples/sec; 0.136 sec/batch)
2017-05-07 20:56:33.263228: step 20610, loss = 0.82 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-07 20:56:34.530694: step 20620, loss = 0.98 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:56:35.806294: step 20630, loss = 0.75 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:56:37.061235: step 20640, loss = 0.93 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-07 20:56:38.302939: step 20650, loss = 0.78 (1030.8 examples/sec; 0.124 sec/batch)
2017-05-07 20:56:39.590121: step 20660, loss = 0.87 (994.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:56:40.874884: step 20670, loss = 1.12 (996.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:56:42.142520: step 20680, loss = 0.98 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:56:43.432450: step 20690, loss = 0.93 (992.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:56:44.817086: step 20700, loss = 0.76 (924.4 examples/sec; 0.138 sec/batch)
2017-05-07 20:56:46.000503: step 20710, loss = 0.88 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-07 20:56:47.291680: step 20720, loss = 0.96 (991.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:56:48.560721: step 20730, loss = 0.79 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:56:49.863339: step 20740, loss = 0.71 (982.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:56:51.145900: step 20750, loss = 0.94 (998.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:56:52.449234: step 20760, loss = 0.99 (982.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:56:53.706854: step 20770, loss = 0.92 (1017.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:56:54.984047: step 20780, loss = 0.78 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:56:56.248811: step 20790, loss = 0.70 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:56:57.603698: step 20800, loss = 0.83 (944.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:56:58.781829: step 20810, loss = 0.88 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-07 20:57:00.072084: step 20820, loss = 0.70 (992.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:57:01.377183: step 20830, loss = 1.03 (980.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:57:02.651012: step 20840, loss = 0.84 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:57:03.952803: step 20850, loss = 0.92 (983.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:57:05.248025: step 20860, loss = 0.79 (988.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:57:06.512209: step 20870, loss = 0.97 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:57:07.776816: step 20880, loss = 0.79 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:57:09.084938: step 20890, loss = 0.88 (978.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:57:10.467817: step 20900, loss = 0.87 (925.6 examples/sec; 0.138 sec/batch)
2017-05-07 20:57:11.621160: step 20910, loss = 0.77 (1109.8 examples/sec; 0.115 sec/batch)
2017-05-07 20:57:12.922702: step 20920, loss = 0.96 (983.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:57:14.217051: step 20930, loss = 0.98 (988.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:57:15.475511: step 20940, loss = 0.95 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:57:16.765448: step 20950, loss = 1.04 (992.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:57:18.041907: step 20960, loss = 0.89 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:57:19.310119: step 20970, loss = 0.80 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:57:20.573927: step 20980, loss = 0.82 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:57:21.824262: step 20990, loss = 0.85 (1023.7 examples/sec; 0.125 sec/batch)
2017-05-07 20:57:23.174821: step 21000, loss = 1.10 (947.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:57:24.367829: step 21010, loss = 0.86 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-07 20:57:25.624170: step 21020, loss = 0.73 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:57:26.903951: step 21030, loss = 0.95 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:57:28.195795: step 21040, loss = 0.89 (990.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:57:29.478735: step 21050, loss = 0.81 (997.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:57:30.772798: step 21060, loss = 0.84 (989.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:57:32.073820: step 21070, loss = 0.98 (983.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:57:33.343139: step 21080, loss = 0.92 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:57:34.635309: step 21090, loss = 0.85 (990.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:57:36.005465: step 21100, loss = 0.95 (934.2 examples/sec; 0.137 sec/batch)
2017-05-07 20:57:37.212369: step 21110, loss = 0.96 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-07 20:57:38.502513: step 21120, loss = 0.96 (992.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:57:39.784014: step 21130, loss = 1.02 (998.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:57:41.070629: step 21140, loss = 0.84 (994.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:57:42.324763: step 21150, loss = 0.80 (1020.6 examples/sec; 0.125 sec/batch)
2017-05-07 20:57:43.602835: step 21160, loss = 0.80 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:57:44.913740: step 21170, loss = 0.87 (976.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:57:46.200714: step 21180, loss = 0.83 (994.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:57:47.454468: step 21190, loss = 0.81 (1020.9 examples/sec; 0.125 sec/batch)
2017-05-07 20:57:48.803658: step 21200, loss = 0.83 (948.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:57:49.974891: step 21210, loss = 0.97 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-07 20:57:51.245120: step 21220, loss = 0.94 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:57:52.537925: step 21230, loss = 1.04 (990.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:57:53.858540: step 21240, loss = 0.94 (969.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:57:55.186450: step 21250, loss = 0.93 (963.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:57:56.502587: step 21260, loss = 0.93 (972.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:57:57.775162: step 21270, loss = 0.84 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:57:59.058289: step 21280, loss = 1.08 (997.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:58:00.332766: step 21290, loss = 0.74 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:58:01.738859: step 21300, loss = 0.86 (910.3 examples/sec; 0.141 sec/batch)
2017-05-07 20:58:02.932025: step 21310, loss = 0.92 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-07 20:58:04.214312: step 21320, loss = 0.89 (998.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:58:05.525635: step 21330, loss = 0.98 (976.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:58:06.810256: step 21340, loss = 0.88 (996.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:58:08.064572: step 21350, loss = 0.94 (1020.5 examples/sec; 0.125 sec/batch)
2017-05-07 20:58:09.342826: step 21360, loss = 0.79 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:58:10.620732: step 21370, loss = 0.85 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:58:11.897877: step 21380, loss = 0.77 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:58:13.176113: step 21390, loss = 1.04 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:58:14.536080: step 21400, loss = 0.83 (941.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:58:15.744854: step 21410, loss = 0.81 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-07 20:58:17.013610: step 21420, loss = 0.78 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:58:18.278556: step 21430, loss = 0.91 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:58:19.542504: step 21440, loss = 0.82 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:58:20.852179: step 21450, loss = 1.21 (977.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:58:22.147514: step 21460, loss = 1.01 (988.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:58:23.447332: step 21470, loss = 1.02 (984.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:58:24.717075: step 21480, loss = 1.02 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:58:25.988717: step 21490, loss = 0.89 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:58:27.348348: step 21500, loss = 0.74 (941.4 examples/sec; 0.136 sec/batch)
2017-05-07 20:58:28.527753: step 21510, loss = 0.79 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-07 20:58:29.786611: step 21520, loss = 0.78 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:58:31.061060: step 21530, loss = 0.86 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:58:32.327380: step 21540, loss = 0.85 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:58:33.613587: step 21550, loss = 0.94 (995.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:58:34.903660: step 21560, loss = 0.91 (992.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:58:36.228950: step 21570, loss = 1.05 (965.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:58:37.507718: step 21580, loss = 0.81 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:58:38.761608: step 21590, loss = 0.88 (1020.8 examples/sec; 0.125 sec/batch)
2017-05-07 20:58:40.154744: step 21600, loss = 0.76 (918.8 examples/sec; 0.139 sec/batch)
2017-05-07 20:58:41.342325: step 21610, loss = 0.84 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-07 20:58:42.630544: step 21620, loss = 0.74 (993.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:58:43.893950: step 21630, loss = 0.90 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:58:45.162635: step 21640, loss = 0.90 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:58:46.430419: step 21650, loss = 0.82 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:58:47.698167: step 21660, loss = 0.85 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:58:48.988071: step 21670, loss = 0.87 (992.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:58:50.268664: step 21680, loss = 0.86 (999.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:58:51.554514: step 21690, loss = 0.81 (995.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:58:52.938109: step 21700, loss = 0.98 (925.1 examples/sec; 0.138 sec/batch)
2017-05-07 20:58:54.108615: step 21710, loss = 1.04 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-07 20:58:55.411414: step 21720, loss = 0.81 (982.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:58:56.694360: step 21730, loss = 0.96 (997.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:58:57.971662: step 21740, loss = 0.76 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:58:59.271855: step 21750, loss = 0.88 (984.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:59:00.555486: step 21760, loss = 0.74 (997.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:59:01.859500: step 21770, loss = 0.80 (981.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:59:03.116994: step 21780, loss = 1.09 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:59:04.397756: step 21790, loss = 0.83 (999.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:59:05.774120: step 21800, loss = 0.78 (930.0 examples/sec; 0.138 sec/batch)
2017-05-07 20:59:06.933446: step 21810, loss = 0.86 (1104.1 examples/sec; 0.116 sec/batch)
2017-05-07 20:59:E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 447 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
08.228569: step 21820, loss = 0.97 (988.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:59:09.527957: step 21830, loss = 0.74 (985.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:59:10.797366: step 21840, loss = 0.86 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:59:12.078792: step 21850, loss = 0.98 (998.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:59:13.341514: step 21860, loss = 0.74 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:59:14.627763: step 21870, loss = 0.67 (995.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:59:15.940101: step 21880, loss = 0.83 (975.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:59:17.235447: step 21890, loss = 0.86 (988.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:59:18.612743: step 21900, loss = 0.83 (929.4 examples/sec; 0.138 sec/batch)
2017-05-07 20:59:19.809391: step 21910, loss = 0.78 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-07 20:59:21.118002: step 21920, loss = 0.87 (978.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:59:22.409514: step 21930, loss = 0.89 (991.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:59:23.675791: step 21940, loss = 0.77 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:59:24.973518: step 21950, loss = 0.84 (986.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:59:26.231178: step 21960, loss = 0.74 (1017.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:59:27.517572: step 21970, loss = 0.89 (995.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:59:28.776416: step 21980, loss = 0.77 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:59:30.028426: step 21990, loss = 0.71 (1022.4 examples/sec; 0.125 sec/batch)
2017-05-07 20:59:31.402750: step 22000, loss = 0.92 (931.4 examples/sec; 0.137 sec/batch)
2017-05-07 20:59:32.567837: step 22010, loss = 0.80 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-07 20:59:33.840196: step 22020, loss = 0.87 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:59:35.115716: step 22030, loss = 1.01 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:59:36.391570: step 22040, loss = 0.90 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:59:37.695396: step 22050, loss = 0.96 (981.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:59:38.963580: step 22060, loss = 1.02 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:59:40.279455: step 22070, loss = 0.89 (972.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:59:41.568443: step 22080, loss = 1.11 (993.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:59:42.855647: step 22090, loss = 0.85 (994.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:59:44.229491: step 22100, loss = 0.71 (931.7 examples/sec; 0.137 sec/batch)
2017-05-07 20:59:45.383442: step 22110, loss = 0.83 (1109.2 examples/sec; 0.115 sec/batch)
2017-05-07 20:59:46.631743: step 22120, loss = 0.89 (1025.4 examples/sec; 0.125 sec/batch)
2017-05-07 20:59:47.902417: step 22130, loss = 0.84 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:59:49.141444: step 22140, loss = 0.84 (1033.1 examples/sec; 0.124 sec/batch)
2017-05-07 20:59:50.428728: step 22150, loss = 0.98 (994.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:59:51.695771: step 22160, loss = 0.89 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:59:52.934079: step 22170, loss = 0.82 (1033.7 examples/sec; 0.124 sec/batch)
2017-05-07 20:59:54.191590: step 22180, loss = 0.77 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:59:55.456614: step 22190, loss = 1.03 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:59:56.815841: step 22200, loss = 1.04 (941.7 examples/sec; 0.136 sec/batch)
2017-05-07 20:59:58.009793: step 22210, loss = 0.88 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-07 20:59:59.330837: step 22220, loss = 0.85 (968.9 examples/sec; 0.132 sec/batch)
2017-05-07 21:00:00.593706: step 22230, loss = 0.89 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:00:01.861239: step 22240, loss = 0.94 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:00:03.149075: step 22250, loss = 0.97 (993.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:00:04.434476: step 22260, loss = 0.81 (995.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:00:05.735309: step 22270, loss = 0.90 (984.0 examples/sec; 0.130 sec/batch)
2017-05-07 21:00:07.006545: step 22280, loss = 0.89 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:00:08.332773: step 22290, loss = 0.94 (965.1 examples/sec; 0.133 sec/batch)
2017-05-07 21:00:09.715474: step 22300, loss = 0.70 (925.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:00:10.893072: step 22310, loss = 0.84 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-07 21:00:12.193651: step 22320, loss = 0.79 (984.2 examples/sec; 0.130 sec/batch)
2017-05-07 21:00:13.441823: step 22330, loss = 0.85 (1025.5 examples/sec; 0.125 sec/batch)
2017-05-07 21:00:14.710426: step 22340, loss = 0.84 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:00:15.983777: step 22350, loss = 0.93 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:00:17.268879: step 22360, loss = 1.01 (996.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:00:18.572591: step 22370, loss = 0.86 (981.8 examples/sec; 0.130 sec/batch)
2017-05-07 21:00:19.877657: step 22380, loss = 0.97 (980.8 examples/sec; 0.131 sec/batch)
2017-05-07 21:00:21.190234: step 22390, loss = 0.99 (975.2 examples/sec; 0.131 sec/batch)
2017-05-07 21:00:22.540629: step 22400, loss = 0.83 (947.9 examples/sec; 0.135 sec/batch)
2017-05-07 21:00:23.748289: step 22410, loss = 0.74 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-07 21:00:25.018845: step 22420, loss = 0.87 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:00:26.295952: step 22430, loss = 1.00 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:00:27.565356: step 22440, loss = 0.79 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:00:28.818286: step 22450, loss = 0.87 (1021.6 examples/sec; 0.125 sec/batch)
2017-05-07 21:00:30.068583: step 22460, loss = 1.01 (1023.8 examples/sec; 0.125 sec/batch)
2017-05-07 21:00:31.359825: step 22470, loss = 0.92 (991.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:00:32.626659: step 22480, loss = 0.99 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:00:33.924612: step 22490, loss = 0.74 (986.2 examples/sec; 0.130 sec/batch)
2017-05-07 21:00:35.321281: step 22500, loss = 0.90 (916.5 examples/sec; 0.140 sec/batch)
2017-05-07 21:00:36.512142: step 22510, loss = 0.85 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-07 21:00:37.783451: step 22520, loss = 0.96 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:00:39.074188: step 22530, loss = 0.89 (991.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:00:40.343732: step 22540, loss = 0.74 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:00:41.628575: step 22550, loss = 0.75 (996.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:00:42.907004: step 22560, loss = 1.09 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:00:44.208023: step 22570, loss = 1.11 (983.9 examples/sec; 0.130 sec/batch)
2017-05-07 21:00:45.480485: step 22580, loss = 0.82 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:00:46.726944: step 22590, loss = 0.81 (1026.9 examples/sec; 0.125 sec/batch)
2017-05-07 21:00:48.101075: step 22600, loss = 0.91 (931.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:00:49.314110: step 22610, loss = 0.80 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-07 21:00:50.621560: step 22620, loss = 0.81 (979.0 examples/sec; 0.131 sec/batch)
2017-05-07 21:00:51.911413: step 22630, loss = 0.80 (992.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:00:53.193583: step 22640, loss = 0.94 (998.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:00:54.460938: step 22650, loss = 0.90 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:00:55.788952: step 22660, loss = 0.78 (963.8 examples/sec; 0.133 sec/batch)
2017-05-07 21:00:57.055288: step 22670, loss = 0.80 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:00:58.335653: step 22680, loss = 0.87 (999.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:00:59.606283: step 22690, loss = 0.77 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:01:00.984325: step 22700, loss = 0.87 (928.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:01:02.143263: step 22710, loss = 0.84 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-07 21:01:03.411557: step 22720, loss = 0.90 (1009.2 exaE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 467 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
mples/sec; 0.127 sec/batch)
2017-05-07 21:01:04.695302: step 22730, loss = 0.87 (997.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:01:05.941738: step 22740, loss = 0.80 (1026.9 examples/sec; 0.125 sec/batch)
2017-05-07 21:01:07.227252: step 22750, loss = 0.94 (995.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:01:08.516416: step 22760, loss = 0.96 (992.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:01:09.795299: step 22770, loss = 0.65 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:01:11.074331: step 22780, loss = 0.96 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:01:12.345788: step 22790, loss = 0.89 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:01:13.716127: step 22800, loss = 1.00 (934.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:01:14.882047: step 22810, loss = 0.70 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-07 21:01:16.163791: step 22820, loss = 0.89 (998.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:01:17.464216: step 22830, loss = 0.81 (984.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:01:18.754300: step 22840, loss = 0.84 (992.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:01:20.036572: step 22850, loss = 0.85 (998.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:01:21.345485: step 22860, loss = 0.84 (977.9 examples/sec; 0.131 sec/batch)
2017-05-07 21:01:22.597619: step 22870, loss = 0.90 (1022.3 examples/sec; 0.125 sec/batch)
2017-05-07 21:01:23.884538: step 22880, loss = 0.69 (994.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:01:25.138757: step 22890, loss = 0.87 (1020.6 examples/sec; 0.125 sec/batch)
2017-05-07 21:01:26.498891: step 22900, loss = 0.82 (941.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:01:27.681841: step 22910, loss = 0.76 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-07 21:01:28.947658: step 22920, loss = 0.76 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:01:30.217035: step 22930, loss = 0.71 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:01:31.479266: step 22940, loss = 0.96 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:01:32.789929: step 22950, loss = 0.99 (976.6 examples/sec; 0.131 sec/batch)
2017-05-07 21:01:34.047352: step 22960, loss = 0.95 (1018.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:01:35.327024: step 22970, loss = 0.82 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:01:36.599074: step 22980, loss = 0.85 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:01:37.876944: step 22990, loss = 0.76 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:01:39.263680: step 23000, loss = 0.86 (923.0 examples/sec; 0.139 sec/batch)
2017-05-07 21:01:40.439205: step 23010, loss = 0.85 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-07 21:01:41.730463: step 23020, loss = 0.97 (991.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:01:43.022334: step 23030, loss = 0.88 (990.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:01:44.277924: step 23040, loss = 0.95 (1019.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:01:45.527437: step 23050, loss = 0.81 (1024.4 examples/sec; 0.125 sec/batch)
2017-05-07 21:01:46.778614: step 23060, loss = 0.71 (1023.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:01:48.049483: step 23070, loss = 0.80 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:01:49.318157: step 23080, loss = 0.84 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:01:50.596824: step 23090, loss = 1.03 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:01:51.950426: step 23100, loss = 0.75 (945.6 examples/sec; 0.135 sec/batch)
2017-05-07 21:01:53.127792: step 23110, loss = 0.84 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-07 21:01:54.429814: step 23120, loss = 0.83 (983.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:01:55.718902: step 23130, loss = 0.88 (993.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:01:57.014161: step 23140, loss = 0.86 (988.2 examples/sec; 0.130 sec/batch)
2017-05-07 21:01:58.274665: step 23150, loss = 0.71 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:01:59.562325: step 23160, loss = 1.16 (994.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:02:00.832164: step 23170, loss = 0.86 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:02:02.124481: step 23180, loss = 0.64 (990.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:02:03.415676: step 23190, loss = 0.85 (991.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:02:04.789511: step 23200, loss = 0.73 (931.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:02:05.964575: step 23210, loss = 0.82 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-07 21:02:07.244043: step 23220, loss = 0.78 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:02:08.515586: step 23230, loss = 0.68 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:02:09.763169: step 23240, loss = 0.78 (1026.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:02:11.018556: step 23250, loss = 0.90 (1019.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:02:12.276314: step 23260, loss = 0.78 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:02:13.544798: step 23270, loss = 0.87 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:02:14.792923: step 23280, loss = 0.76 (1025.5 examples/sec; 0.125 sec/batch)
2017-05-07 21:02:16.069816: step 23290, loss = 0.69 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:02:17.473724: step 23300, loss = 0.88 (911.7 examples/sec; 0.140 sec/batch)
2017-05-07 21:02:18.638433: step 23310, loss = 0.89 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-07 21:02:19.916838: step 23320, loss = 0.86 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:02:21.181288: step 23330, loss = 0.87 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:02:22.456307: step 23340, loss = 0.88 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:02:23.763944: step 23350, loss = 1.09 (978.9 examples/sec; 0.131 sec/batch)
2017-05-07 21:02:25.070648: step 23360, loss = 0.98 (979.6 examples/sec; 0.131 sec/batch)
2017-05-07 21:02:26.348366: step 23370, loss = 0.84 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:02:27.608631: step 23380, loss = 1.03 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:02:28.919189: step 23390, loss = 0.91 (976.7 examples/sec; 0.131 sec/batch)
2017-05-07 21:02:30.284725: step 23400, loss = 1.04 (937.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:02:31.482947: step 23410, loss = 0.80 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-07 21:02:32.749050: step 23420, loss = 0.86 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:02:34.036752: step 23430, loss = 0.88 (994.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:02:35.335824: step 23440, loss = 0.85 (985.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:02:36.627358: step 23450, loss = 0.93 (991.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:02:37.942301: step 23460, loss = 0.84 (973.4 examples/sec; 0.131 sec/batch)
2017-05-07 21:02:39.213612: step 23470, loss = 0.72 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:02:40.473889: step 23480, loss = 0.84 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:02:41.741696: step 23490, loss = 0.79 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:02:43.119632: step 23500, loss = 0.69 (928.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:02:44.303275: step 23510, loss = 0.95 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-07 21:02:45.563201: step 23520, loss = 1.02 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:02:46.863055: step 23530, loss = 0.81 (984.7 examples/sec; 0.130 sec/batch)
2017-05-07 21:02:48.143720: step 23540, loss = 0.99 (999.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:02:49.426174: step 23550, loss = 0.94 (998.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:02:50.691914: step 23560, loss = 0.66 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:02:51.983844: step 23570, loss = 0.88 (990.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:02:53.263642: step 23580, loss = 0.79 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:02:54.547830: step 23590, loss = 0.76 (996.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:02:55.925852: step 23600, loss = 1.02 (928.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:02:57.126209: step 23610, loss = 0.67 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-07 21:02:58.387017: step 23620, loss = 0.79 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-07 21E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 488 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
:02:59.674686: step 23630, loss = 0.73 (994.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:03:00.975883: step 23640, loss = 1.32 (983.7 examples/sec; 0.130 sec/batch)
2017-05-07 21:03:02.249070: step 23650, loss = 0.84 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:03:03.544439: step 23660, loss = 0.86 (988.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:03:04.865693: step 23670, loss = 0.80 (968.8 examples/sec; 0.132 sec/batch)
2017-05-07 21:03:06.128186: step 23680, loss = 0.88 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:03:07.414291: step 23690, loss = 0.75 (995.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:03:08.789054: step 23700, loss = 0.77 (931.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:03:09.995599: step 23710, loss = 0.90 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-07 21:03:11.293729: step 23720, loss = 1.18 (986.0 examples/sec; 0.130 sec/batch)
2017-05-07 21:03:12.560190: step 23730, loss = 0.77 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:03:13.826444: step 23740, loss = 0.80 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:03:15.096281: step 23750, loss = 0.77 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:03:16.371266: step 23760, loss = 0.83 (1003.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:03:17.633544: step 23770, loss = 1.01 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:03:18.904559: step 23780, loss = 0.69 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:03:20.186034: step 23790, loss = 0.76 (998.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:03:21.547601: step 23800, loss = 1.03 (940.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:03:22.751855: step 23810, loss = 1.00 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-07 21:03:24.030240: step 23820, loss = 0.83 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:03:25.298845: step 23830, loss = 0.81 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:03:26.573368: step 23840, loss = 1.02 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:03:27.857374: step 23850, loss = 0.75 (996.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:03:29.165925: step 23860, loss = 0.85 (978.2 examples/sec; 0.131 sec/batch)
2017-05-07 21:03:30.418128: step 23870, loss = 0.87 (1022.2 examples/sec; 0.125 sec/batch)
2017-05-07 21:03:31.693673: step 23880, loss = 0.81 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:03:32.951271: step 23890, loss = 1.00 (1017.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:03:34.285223: step 23900, loss = 0.82 (959.6 examples/sec; 0.133 sec/batch)
2017-05-07 21:03:35.485908: step 23910, loss = 1.03 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-07 21:03:36.753980: step 23920, loss = 0.67 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:03:38.007338: step 23930, loss = 0.82 (1021.2 examples/sec; 0.125 sec/batch)
2017-05-07 21:03:39.276708: step 23940, loss = 0.71 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:03:40.547758: step 23950, loss = 0.78 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:03:41.797672: step 23960, loss = 0.82 (1024.1 examples/sec; 0.125 sec/batch)
2017-05-07 21:03:43.055789: step 23970, loss = 1.09 (1017.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:03:44.350118: step 23980, loss = 0.98 (988.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:03:45.667141: step 23990, loss = 0.90 (971.9 examples/sec; 0.132 sec/batch)
2017-05-07 21:03:47.024880: step 24000, loss = 0.85 (942.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:03:48.205075: step 24010, loss = 0.75 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-07 21:03:49.490941: step 24020, loss = 0.78 (995.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:03:50.798542: step 24030, loss = 0.99 (978.9 examples/sec; 0.131 sec/batch)
2017-05-07 21:03:52.079121: step 24040, loss = 0.76 (999.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:03:53.329896: step 24050, loss = 0.78 (1023.4 examples/sec; 0.125 sec/batch)
2017-05-07 21:03:54.597387: step 24060, loss = 0.94 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:03:55.871082: step 24070, loss = 0.96 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:03:57.158326: step 24080, loss = 0.80 (994.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:03:58.443790: step 24090, loss = 0.98 (995.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:03:59.812084: step 24100, loss = 0.86 (935.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:04:01.001472: step 24110, loss = 0.88 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-07 21:04:02.304533: step 24120, loss = 0.95 (982.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:04:03.556319: step 24130, loss = 0.80 (1022.5 examples/sec; 0.125 sec/batch)
2017-05-07 21:04:04.835988: step 24140, loss = 0.80 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:04:06.130550: step 24150, loss = 0.91 (988.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:04:07.425088: step 24160, loss = 0.87 (988.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:04:08.705538: step 24170, loss = 1.02 (999.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:04:09.972161: step 24180, loss = 0.95 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:04:11.280909: step 24190, loss = 1.01 (978.0 examples/sec; 0.131 sec/batch)
2017-05-07 21:04:12.651354: step 24200, loss = 0.84 (934.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:04:13.827956: step 24210, loss = 0.89 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-07 21:04:15.100225: step 24220, loss = 0.92 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:04:16.385470: step 24230, loss = 0.91 (995.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:04:17.670229: step 24240, loss = 0.95 (996.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:04:18.941400: step 24250, loss = 0.83 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:04:20.222650: step 24260, loss = 1.01 (999.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:04:21.500570: step 24270, loss = 0.69 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:04:22.810252: step 24280, loss = 1.12 (977.3 examples/sec; 0.131 sec/batch)
2017-05-07 21:04:24.078491: step 24290, loss = 0.79 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:04:25.453300: step 24300, loss = 0.76 (931.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:04:26.621332: step 24310, loss = 1.13 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-07 21:04:27.909692: step 24320, loss = 0.84 (993.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:04:29.182953: step 24330, loss = 0.99 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:04:30.470110: step 24340, loss = 0.89 (994.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:04:31.752611: step 24350, loss = 0.93 (998.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:04:33.046338: step 24360, loss = 0.87 (989.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:04:34.321323: step 24370, loss = 0.99 (1003.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:04:35.612701: step 24380, loss = 0.96 (991.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:04:36.911622: step 24390, loss = 0.92 (985.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:04:38.307728: step 24400, loss = 0.72 (916.8 examples/sec; 0.140 sec/batch)
2017-05-07 21:04:39.480543: step 24410, loss = 1.06 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-07 21:04:40.762221: step 24420, loss = 1.07 (998.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:04:42.040749: step 24430, loss = 0.94 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:04:43.325686: step 24440, loss = 0.82 (996.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:04:44.603115: step 24450, loss = 0.88 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:04:45.908744: step 24460, loss = 0.90 (980.4 examples/sec; 0.131 sec/batch)
2017-05-07 21:04:47.180026: step 24470, loss = 0.90 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:04:48.441397: step 24480, loss = 0.96 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:04:49.699121: step 24490, loss = 0.64 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:04:51.077472: step 24500, loss = 0.79 (928.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:04:52.241460: step 24510, loss = 0.80 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-07 21:04:53.532744: step 24520, loss = 0.90 (991.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:04:54.805159: step 24530, loss = 0.86 (1006.0E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 508 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
 examples/sec; 0.127 sec/batch)
2017-05-07 21:04:56.101875: step 24540, loss = 0.94 (987.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:04:57.359836: step 24550, loss = 0.79 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:04:58.622726: step 24560, loss = 0.82 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:04:59.885085: step 24570, loss = 0.93 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:05:01.160674: step 24580, loss = 0.75 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:05:02.428444: step 24590, loss = 0.82 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:05:03.784773: step 24600, loss = 0.84 (943.7 examples/sec; 0.136 sec/batch)
2017-05-07 21:05:04.943585: step 24610, loss = 0.77 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-07 21:05:06.210638: step 24620, loss = 0.85 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:05:07.475572: step 24630, loss = 0.91 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:05:08.752992: step 24640, loss = 0.83 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:05:10.026604: step 24650, loss = 1.03 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:05:11.333866: step 24660, loss = 0.76 (979.1 examples/sec; 0.131 sec/batch)
2017-05-07 21:05:12.621296: step 24670, loss = 0.87 (994.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:05:13.877061: step 24680, loss = 0.84 (1019.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:05:15.164342: step 24690, loss = 0.94 (994.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:05:16.532467: step 24700, loss = 0.85 (935.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:05:17.717329: step 24710, loss = 0.88 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-07 21:05:18.970492: step 24720, loss = 0.87 (1021.4 examples/sec; 0.125 sec/batch)
2017-05-07 21:05:20.236371: step 24730, loss = 0.83 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:05:21.513797: step 24740, loss = 0.83 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:05:22.776435: step 24750, loss = 0.73 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:05:24.057345: step 24760, loss = 0.81 (999.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:05:25.328594: step 24770, loss = 0.75 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:05:26.596747: step 24780, loss = 0.81 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:05:27.866855: step 24790, loss = 0.89 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:05:29.195101: step 24800, loss = 1.02 (963.7 examples/sec; 0.133 sec/batch)
2017-05-07 21:05:30.376379: step 24810, loss = 0.89 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-07 21:05:31.660496: step 24820, loss = 0.85 (996.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:05:32.963151: step 24830, loss = 0.89 (982.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:05:34.243343: step 24840, loss = 0.92 (999.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:05:35.548204: step 24850, loss = 0.95 (980.9 examples/sec; 0.130 sec/batch)
2017-05-07 21:05:36.816164: step 24860, loss = 0.85 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:05:38.087627: step 24870, loss = 0.91 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:05:39.342590: step 24880, loss = 0.85 (1019.9 examples/sec; 0.125 sec/batch)
2017-05-07 21:05:40.633597: step 24890, loss = 0.76 (991.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:05:42.008806: step 24900, loss = 0.81 (930.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:05:43.188104: step 24910, loss = 0.76 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-07 21:05:44.448620: step 24920, loss = 0.89 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:05:45.727854: step 24930, loss = 0.89 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:05:47.015788: step 24940, loss = 0.98 (993.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:05:48.292497: step 24950, loss = 0.74 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:05:49.568144: step 24960, loss = 0.85 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:05:50.890090: step 24970, loss = 1.03 (968.3 examples/sec; 0.132 sec/batch)
2017-05-07 21:05:52.178523: step 24980, loss = 0.82 (993.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:05:53.443043: step 24990, loss = 0.95 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:05:54.799648: step 25000, loss = 1.10 (943.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:05:55.983811: step 25010, loss = 0.72 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-07 21:05:57.249664: step 25020, loss = 0.92 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:05:58.502564: step 25030, loss = 0.74 (1021.6 examples/sec; 0.125 sec/batch)
2017-05-07 21:05:59.802589: step 25040, loss = 0.89 (984.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:06:01.069342: step 25050, loss = 0.99 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:06:02.370628: step 25060, loss = 0.86 (983.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:06:03.652486: step 25070, loss = 0.77 (998.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:06:04.928302: step 25080, loss = 0.93 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:06:06.193329: step 25090, loss = 0.89 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:06:07.564053: step 25100, loss = 0.86 (933.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:06:08.778161: step 25110, loss = 0.71 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-07 21:06:10.044721: step 25120, loss = 0.76 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:06:11.301131: step 25130, loss = 0.79 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:06:12.584922: step 25140, loss = 0.86 (997.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:06:13.857288: step 25150, loss = 0.86 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:06:15.132938: step 25160, loss = 0.83 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:06:16.428741: step 25170, loss = 0.74 (987.8 examples/sec; 0.130 sec/batch)
2017-05-07 21:06:17.723899: step 25180, loss = 0.75 (988.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:06:18.988326: step 25190, loss = 0.83 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:06:20.365022: step 25200, loss = 1.00 (929.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:06:21.557649: step 25210, loss = 0.81 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-07 21:06:22.843595: step 25220, loss = 1.00 (995.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:06:24.153887: step 25230, loss = 0.69 (976.9 examples/sec; 0.131 sec/batch)
2017-05-07 21:06:25.408134: step 25240, loss = 0.98 (1020.5 examples/sec; 0.125 sec/batch)
2017-05-07 21:06:26.683028: step 25250, loss = 0.77 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:06:27.952804: step 25260, loss = 0.82 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:06:29.221600: step 25270, loss = 0.86 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:06:30.492700: step 25280, loss = 0.74 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:06:31.786614: step 25290, loss = 0.79 (989.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:06:33.178499: step 25300, loss = 0.98 (919.6 examples/sec; 0.139 sec/batch)
2017-05-07 21:06:34.336522: step 25310, loss = 0.79 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-07 21:06:35.603456: step 25320, loss = 0.83 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:06:36.903339: step 25330, loss = 1.03 (984.7 examples/sec; 0.130 sec/batch)
2017-05-07 21:06:38.194232: step 25340, loss = 0.76 (991.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:06:39.470726: step 25350, loss = 0.93 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:06:40.763770: step 25360, loss = 0.78 (989.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:06:42.064123: step 25370, loss = 0.87 (984.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:06:43.358455: step 25380, loss = 0.78 (988.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:06:44.641691: step 25390, loss = 1.01 (997.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:06:46.028723: step 25400, loss = 1.22 (922.8 examples/sec; 0.139 sec/batch)
2017-05-07 21:06:47.261742: step 25410, loss = 0.77 (1038.1 examples/sec; 0.123 sec/batch)
2017-05-07 21:06:48.529060: step 25420, loss = 0.84 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:06:49.816805: step 25430, loss = 0.77 (994.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:06:51.087854: step 25440, loss = 0.72 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:06:52.365354: step 25450, loss = 0.82 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:06:53.626494: step 25460, loss = 1.10 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:06:54.913042: step 25470, loss = 0.84 (994.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:06:56.183493: step 25480, loss = 0.66 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:06:57.468492: step 25490, loss = 0.77 (996.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:06:58.852912: step 25500, loss = 0.94 (924.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:07:00.053413: step 25510, loss = 0.84 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-07 21:07:01.332859: step 25520, loss = 0.98 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:07:02.623244: step 25530, loss = 1.04 (991.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:07:03.890318: step 25540, loss = 0.77 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:07:05.177372: step 25550, loss = 0.82 (994.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:07:06.463574: step 25560, loss = 0.93 (995.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:07:07.769252: step 25570, loss = 0.86 (980.3 examples/sec; 0.131 sec/batch)
2017-05-07 21:07:09.054204: step 25580, loss = 0.80 (996.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:07:10.345763: step 25590, loss = 0.77 (991.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:07:11.720261: step 25600, loss = 0.92 (931.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:07:12.921881: step 25610, loss = 0.95 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-07 21:07:14.165786: step 25620, loss = 0.79 (1029.0 examples/sec; 0.124 sec/batch)
2017-05-07 21:07:15.443259: step 25630, loss = 0.82 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:07:16.707864: step 25640, loss = 0.88 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:07:17.962294: step 25650, loss = 0.86 (1020.4 examples/sec; 0.125 sec/batch)
2017-05-07 21:07:19.227469: step 25660, loss = 0.81 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:07:20.507340: step 25670, loss = 0.86 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:07:21.749194: step 25680, loss = 0.92 (1030.7 examples/sec; 0.124 sec/batch)
2017-05-07 21:07:23.068821: step 25690, loss = 1.13 (970.0 examples/sec; 0.132 sec/batch)
2017-05-07 21:07:24.466842: step 25700, loss = 1.17 (915.6 examples/sec; 0.140 sec/batch)
2017-05-07 21:07:25.642080: step 25710, loss = 0.87 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-07 21:07:26.935888: step 25720, loss = 0.97 (989.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:07:28.222244: step 25730, loss = 0.74 (995.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:07:29.495309: step 25740, loss = 0.90 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:07:30.750632: step 25750, loss = 0.78 (1019.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:07:32.027495: step 25760, loss = 0.91 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:07:33.301128: step 25770, loss = 0.89 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:07:34.582670: step 25780, loss = 0.80 (998.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:07:35.869644: step 25790, loss = 0.91 (994.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:07:37.239300: step 25800, loss = 0.71 (934.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:07:38.404415: step 25810, loss = 0.95 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-07 21:07:39.664240: step 25820, loss = 0.76 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:07:40.938089: step 25830, loss = 0.77 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:07:42.215876: step 25840, loss = 0.76 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:07:43.486865: step 25850, loss = 0.74 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:07:44.756777: step 25860, loss = 0.71 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:07:46.016096: step 25870, loss = 0.84 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:07:47.338706: step 25880, loss = 1.00 (967.8 examples/sec; 0.132 sec/batch)
2017-05-07 21:07:48.599789:E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 528 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
 step 25890, loss = 0.83 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:07:49.962490: step 25900, loss = 0.83 (939.3 examples/sec; 0.136 sec/batch)
2017-05-07 21:07:51.142749: step 25910, loss = 0.79 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-07 21:07:52.445237: step 25920, loss = 0.81 (982.8 examples/sec; 0.130 sec/batch)
2017-05-07 21:07:53.737111: step 25930, loss = 0.86 (990.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:07:55.001351: step 25940, loss = 1.05 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:07:56.284608: step 25950, loss = 0.87 (997.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:07:57.588910: step 25960, loss = 0.81 (981.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:07:58.872068: step 25970, loss = 0.80 (997.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:08:00.145338: step 25980, loss = 0.97 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:08:01.436652: step 25990, loss = 0.84 (991.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:08:02.791456: step 26000, loss = 0.82 (944.8 examples/sec; 0.135 sec/batch)
2017-05-07 21:08:03.999935: step 26010, loss = 0.82 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-07 21:08:05.293356: step 26020, loss = 1.16 (989.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:08:06.578905: step 26030, loss = 0.87 (995.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:08:07.847599: step 26040, loss = 0.92 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:08:09.121839: step 26050, loss = 0.58 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:08:10.410479: step 26060, loss = 0.81 (993.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:08:11.681567: step 26070, loss = 1.00 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:08:12.948640: step 26080, loss = 0.66 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:08:14.200850: step 26090, loss = 0.87 (1022.2 examples/sec; 0.125 sec/batch)
2017-05-07 21:08:15.562066: step 26100, loss = 0.92 (940.3 examples/sec; 0.136 sec/batch)
2017-05-07 21:08:16.729083: step 26110, loss = 0.81 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-07 21:08:17.997597: step 26120, loss = 0.63 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:08:19.301436: step 26130, loss = 0.80 (981.7 examples/sec; 0.130 sec/batch)
2017-05-07 21:08:20.582152: step 26140, loss = 0.77 (999.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:08:21.864408: step 26150, loss = 0.79 (998.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:08:23.136798: step 26160, loss = 0.85 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:08:24.394187: step 26170, loss = 0.96 (1018.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:08:25.662419: step 26180, loss = 0.77 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:08:26.928770: step 26190, loss = 0.96 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:08:28.328674: step 26200, loss = 0.77 (914.4 examples/sec; 0.140 sec/batch)
2017-05-07 21:08:29.534901: step 26210, loss = 0.84 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-07 21:08:30.807312: step 26220, loss = 0.77 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:08:32.095288: step 26230, loss = 0.76 (993.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:08:33.350227: step 26240, loss = 1.04 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:08:34.619076: step 26250, loss = 0.87 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:08:35.871601: step 26260, loss = 0.86 (1021.9 examples/sec; 0.125 sec/batch)
2017-05-07 21:08:37.144905: step 26270, loss = 0.80 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:08:38.419479: step 26280, loss = 0.88 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:08:39.723806: step 26290, loss = 0.91 (981.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:08:41.114418: step 26300, loss = 0.81 (920.5 examples/sec; 0.139 sec/batch)
2017-05-07 21:08:42.315463: step 26310, loss = 0.70 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-07 21:08:43.588145: step 26320, loss = 0.81 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:08:44.869736: step 26330, loss = 0.68 (998.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:08:46.128655: step 26340, loss = 1.01 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:08:47.380419: step 26350, loss = 0.81 (1022.6 examples/sec; 0.125 sec/batch)
2017-05-07 21:08:48.636527: step 26360, loss = 0.92 (1019.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:08:49.925681: step 26370, loss = 0.98 (992.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:08:51.196319: step 26380, loss = 0.81 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:08:52.495229: step 26390, loss = 0.90 (985.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:08:53.885557: step 26400, loss = 1.23 (920.6 examples/sec; 0.139 sec/batch)
2017-05-07 21:08:55.060574: step 26410, loss = 0.80 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-07 21:08:56.323755: step 26420, loss = 0.79 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:08:57.599631: step 26430, loss = 0.99 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:08:58.908958: step 26440, loss = 0.72 (977.6 examples/sec; 0.131 sec/batch)
2017-05-07 21:09:00.202941: step 26450, loss = 0.97 (989.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:09:01.491403: step 26460, loss = 0.93 (993.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:09:02.802761: step 26470, loss = 0.77 (976.1 examples/sec; 0.131 sec/batch)
2017-05-07 21:09:04.068371: step 26480, loss = 0.81 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:09:05.326944: step 26490, loss = 0.84 (1017.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:09:06.697707: step 26500, loss = 0.82 (933.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:09:07.866208: step 26510, loss = 0.85 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-07 21:09:09.140633: step 26520, loss = 0.82 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:09:10.401155: step 26530, loss = 0.75 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:09:11.669538: step 26540, loss = 1.13 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:09:12.973611: step 26550, loss = 0.82 (981.5 examples/sec; 0.130 sec/batch)
2017-05-07 21:09:14.267931: step 26560, loss = 1.02 (988.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:09:15.528555: step 26570, loss = 1.02 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:09:16.813489: step 26580, loss = 0.74 (996.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:09:18.085217: step 26590, loss = 0.91 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:09:19.446575: step 26600, loss = 0.93 (940.2 examples/sec; 0.136 sec/batch)
2017-05-07 21:09:20.654214: step 26610, loss = 0.81 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-07 21:09:21.914258: step 26620, loss = 1.05 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:09:23.197733: step 26630, loss = 0.73 (997.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:09:24.492087: step 26640, loss = 0.85 (988.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:09:25.767192: step 26650, loss = 0.92 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:09:27.062633: step 26660, loss = 0.83 (988.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:09:28.361665: step 26670, loss = 0.92 (985.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:09:29.616518: step 26680, loss = 0.87 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:09:30.884085: step 26690, loss = 0.68 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:09:32.273472: step 26700, loss = 0.87 (921.3 examples/sec; 0.139 sec/batch)
2017-05-07 21:09:33.453486: step 26710, loss = 0.70 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-07 21:09:34.740884: step 26720, loss = 0.78 (994.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:09:36.029321: step 26730, loss = 0.76 (993.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:09:37.351087: step 26740, loss = 1.15 (968.4 examples/sec; 0.132 sec/batch)
2017-05-07 21:09:38.610838: step 26750, loss = 0.90 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:09:39.912547: step 26760, loss = 0.85 (983.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:09:41.206556: step 26770, loss = 0.78 (989.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:09:42.495579: step 26780, loss = 0.90 (993.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:09:43.800480: step 26790, loss = 0.95 (980.9 examples/sec;E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 548 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
 0.130 sec/batch)
2017-05-07 21:09:45.183413: step 26800, loss = 0.84 (925.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:09:46.359348: step 26810, loss = 0.73 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-07 21:09:47.646559: step 26820, loss = 0.87 (994.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:09:48.938108: step 26830, loss = 0.82 (991.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:09:50.205937: step 26840, loss = 1.12 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:09:51.531461: step 26850, loss = 0.84 (965.7 examples/sec; 0.133 sec/batch)
2017-05-07 21:09:52.827357: step 26860, loss = 0.86 (987.7 examples/sec; 0.130 sec/batch)
2017-05-07 21:09:54.101572: step 26870, loss = 0.95 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:09:55.385545: step 26880, loss = 0.98 (996.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:09:56.644368: step 26890, loss = 0.80 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:09:58.024959: step 26900, loss = 0.90 (927.1 examples/sec; 0.138 sec/batch)
2017-05-07 21:09:59.219297: step 26910, loss = 0.96 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-07 21:10:00.499331: step 26920, loss = 0.73 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:10:01.772499: step 26930, loss = 0.91 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:10:03.089268: step 26940, loss = 1.00 (972.1 examples/sec; 0.132 sec/batch)
2017-05-07 21:10:04.388847: step 26950, loss = 0.87 (984.9 examples/sec; 0.130 sec/batch)
2017-05-07 21:10:05.689731: step 26960, loss = 0.87 (983.9 examples/sec; 0.130 sec/batch)
2017-05-07 21:10:06.980958: step 26970, loss = 0.81 (991.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:10:08.251817: step 26980, loss = 0.82 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:10:09.534676: step 26990, loss = 0.98 (997.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:10:10.919004: step 27000, loss = 0.86 (924.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:10:12.092122: step 27010, loss = 0.92 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-07 21:10:13.360799: step 27020, loss = 0.83 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:10:14.629435: step 27030, loss = 1.00 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:10:15.914230: step 27040, loss = 0.86 (996.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:10:17.188310: step 27050, loss = 0.82 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:10:18.498365: step 27060, loss = 0.75 (977.1 examples/sec; 0.131 sec/batch)
2017-05-07 21:10:19.778583: step 27070, loss = 0.83 (999.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:10:21.085729: step 27080, loss = 0.77 (979.2 examples/sec; 0.131 sec/batch)
2017-05-07 21:10:22.496141: step 27090, loss = 0.96 (907.5 examples/sec; 0.141 sec/batch)
2017-05-07 21:10:23.918190: step 27100, loss = 0.75 (900.1 examples/sec; 0.142 sec/batch)
2017-05-07 21:10:25.145579: step 27110, loss = 0.81 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-07 21:10:26.447052: step 27120, loss = 0.74 (983.5 examples/sec; 0.130 sec/batch)
2017-05-07 21:10:27.799059: step 27130, loss = 0.93 (946.7 examples/sec; 0.135 sec/batch)
2017-05-07 21:10:29.129008: step 27140, loss = 0.96 (962.4 examples/sec; 0.133 sec/batch)
2017-05-07 21:10:30.442555: step 27150, loss = 0.81 (974.5 examples/sec; 0.131 sec/batch)
2017-05-07 21:10:31.834452: step 27160, loss = 0.90 (919.6 examples/sec; 0.139 sec/batch)
2017-05-07 21:10:33.193385: step 27170, loss = 0.86 (941.9 examples/sec; 0.136 sec/batch)
2017-05-07 21:10:34.540498: step 27180, loss = 0.90 (950.2 examples/sec; 0.135 sec/batch)
2017-05-07 21:10:35.911438: step 27190, loss = 0.91 (933.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:10:37.381243: step 27200, loss = 1.10 (870.9 examples/sec; 0.147 sec/batch)
2017-05-07 21:10:38.699935: step 27210, loss = 1.03 (970.7 examples/sec; 0.132 sec/batch)
2017-05-07 21:10:40.087689: step 27220, loss = 0.84 (922.4 examples/sec; 0.139 sec/batch)
2017-05-07 21:10:41.443458: step 27230, loss = 0.87 (944.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:10:42.819927: step 27240, loss = 0.87 (929.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:10:44.181614: step 27250, loss = 0.87 (940.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:10:45.556207: step 27260, loss = 0.89 (931.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:10:46.944012: step 27270, loss = 0.75 (922.3 examples/sec; 0.139 sec/batch)
2017-05-07 21:10:48.338022: step 27280, loss = 0.82 (918.2 examples/sec; 0.139 sec/batch)
2017-05-07 21:10:49.694133: step 27290, loss = 0.77 (943.9 examples/sec; 0.136 sec/batch)
2017-05-07 21:10:51.168879: step 27300, loss = 0.90 (867.9 examples/sec; 0.147 sec/batch)
2017-05-07 21:10:52.438812: step 27310, loss = 0.90 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:10:53.814612: step 27320, loss = 0.73 (930.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:10:55.206025: step 27330, loss = 0.79 (920.0 examples/sec; 0.139 sec/batch)
2017-05-07 21:10:56.588175: step 27340, loss = 0.75 (926.1 examples/sec; 0.138 sec/batch)
2017-05-07 21:10:57.948831: step 27350, loss = 0.77 (940.7 examples/sec; 0.136 sec/batch)
2017-05-07 21:10:59.330551: step 27360, loss = 0.82 (926.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:11:00.739142: step 27370, loss = 1.04 (908.7 examples/sec; 0.141 sec/batch)
2017-05-07 21:11:02.107664: step 27380, loss = 0.82 (935.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:11:03.488361: step 27390, loss = 0.75 (927.1 examples/sec; 0.138 sec/batch)
2017-05-07 21:11:04.966011: step 27400, loss = 0.88 (866.2 examples/sec; 0.148 sec/batch)
2017-05-07 21:11:06.232020: step 27410, loss = 0.96 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:11:07.610003: step 27420, loss = 0.87 (928.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:11:08.988065: step 27430, loss = 1.06 (928.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:11:10.375405: step 27440, loss = 0.83 (922.6 examples/sec; 0.139 sec/batch)
2017-05-07 21:11:11.719844: step 27450, loss = 0.70 (952.1 examples/sec; 0.134 sec/batch)
2017-05-07 21:11:13.107684: step 27460, loss = 0.74 (922.3 examples/sec; 0.139 sec/batch)
2017-05-07 21:11:14.491151: step 27470, loss = 0.86 (925.2 examples/sec; 0.138 sec/batch)
2017-05-07 21:11:15.873805: step 27480, loss = 0.75 (925.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:11:17.244074: step 27490, loss = 0.88 (934.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:11:18.706736: step 27500, loss = 0.82 (875.1 examples/sec; 0.146 sec/batch)
2017-05-07 21:11:19.998658: step 27510, loss = 0.88 (990.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:11:21.408673: step 27520, loss = 0.93 (907.8 examples/sec; 0.141 sec/batch)
2017-05-07 21:11:22.780827: step 27530, loss = 0.89 (932.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:11:24.158064: step 27540, loss = 0.83 (929.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:11:25.527760: step 27550, loss = 0.77 (934.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:11:26.915091: step 27560, loss = 0.90 (922.6 examples/sec; 0.139 sec/batch)
2017-05-07 21:11:28.302596: step 27570, loss = 0.88 (922.5 examples/sec; 0.139 sec/batch)
2017-05-07 21:11:29.674196: step 27580, loss = 0.81 (933.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:11:31.040026: step 27590, loss = 0.79 (937.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:11:32.509156: step 27600, loss = 0.91 (871.3 examples/sec; 0.147 sec/batch)
2017-05-07 21:11:33.775957: step 27610, loss = 0.91 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:11:35.164074: step 27620, loss = 0.98 (922.1 examples/sec; 0.139 sec/batch)
2017-05-07 21:11:36.537655: step 27630, loss = 0.75 (931.9 examples/sec; 0.137 sec/batch)
2017-05-07 21:11:37.923120: step 27640, loss = 1.01 (923.9 examples/sec; 0.139 sec/batch)
2017-05-07 21:11:39.291235: step 27650, loss = 0.89 (935.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:11:40.688305: step 27660, loss = 0.70 (916.2 examples/sec; 0.140 sec/batch)
2017-05-07 21:11:42.065353: step 27670, loss = 0.90 (929.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:11:43.453261: step 27680, loss = 0.90 (922.2 examples/sec; 0.139 sec/batch)
2017-05-07 21:11:44.783148: step 27690, loss = 0.77 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 21:11:46.249207: step 27700, loss = 0.88 (873.1E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 566 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
 examples/sec; 0.147 sec/batch)
2017-05-07 21:11:47.515930: step 27710, loss = 0.86 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:11:48.922496: step 27720, loss = 0.77 (910.0 examples/sec; 0.141 sec/batch)
2017-05-07 21:11:50.302068: step 27730, loss = 0.77 (927.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:11:51.725407: step 27740, loss = 0.88 (899.3 examples/sec; 0.142 sec/batch)
2017-05-07 21:11:53.093574: step 27750, loss = 0.88 (935.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:11:54.458382: step 27760, loss = 0.87 (937.9 examples/sec; 0.136 sec/batch)
2017-05-07 21:11:55.828716: step 27770, loss = 0.92 (934.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:11:57.224655: step 27780, loss = 0.99 (916.9 examples/sec; 0.140 sec/batch)
2017-05-07 21:11:58.603526: step 27790, loss = 0.81 (928.3 examples/sec; 0.138 sec/batch)
2017-05-07 21:12:00.071137: step 27800, loss = 0.84 (872.2 examples/sec; 0.147 sec/batch)
2017-05-07 21:12:01.358439: step 27810, loss = 0.81 (994.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:12:02.718549: step 27820, loss = 0.85 (941.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:12:04.115223: step 27830, loss = 0.91 (916.5 examples/sec; 0.140 sec/batch)
2017-05-07 21:12:05.498754: step 27840, loss = 0.91 (925.2 examples/sec; 0.138 sec/batch)
2017-05-07 21:12:06.886344: step 27850, loss = 0.92 (922.5 examples/sec; 0.139 sec/batch)
2017-05-07 21:12:08.272598: step 27860, loss = 0.77 (923.4 examples/sec; 0.139 sec/batch)
2017-05-07 21:12:09.610195: step 27870, loss = 0.87 (956.9 examples/sec; 0.134 sec/batch)
2017-05-07 21:12:11.002707: step 27880, loss = 0.73 (919.2 examples/sec; 0.139 sec/batch)
2017-05-07 21:12:12.412747: step 27890, loss = 0.88 (907.8 examples/sec; 0.141 sec/batch)
2017-05-07 21:12:13.911138: step 27900, loss = 0.73 (854.3 examples/sec; 0.150 sec/batch)
2017-05-07 21:12:15.172621: step 27910, loss = 0.91 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:12:16.522659: step 27920, loss = 0.82 (948.1 examples/sec; 0.135 sec/batch)
2017-05-07 21:12:17.888999: step 27930, loss = 0.81 (936.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:12:19.261721: step 27940, loss = 0.81 (932.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:12:20.648787: step 27950, loss = 0.86 (922.8 examples/sec; 0.139 sec/batch)
2017-05-07 21:12:22.015355: step 27960, loss = 0.84 (936.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:12:23.392312: step 27970, loss = 1.08 (929.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:12:24.765401: step 27980, loss = 0.73 (932.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:12:26.169904: step 27990, loss = 0.89 (911.3 examples/sec; 0.140 sec/batch)
2017-05-07 21:12:27.655693: step 28000, loss = 0.78 (861.5 examples/sec; 0.149 sec/batch)
2017-05-07 21:12:28.951735: step 28010, loss = 0.85 (987.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:12:30.335119: step 28020, loss = 0.94 (925.3 examples/sec; 0.138 sec/batch)
2017-05-07 21:12:31.723550: step 28030, loss = 0.73 (921.9 examples/sec; 0.139 sec/batch)
2017-05-07 21:12:33.106047: step 28040, loss = 0.96 (925.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:12:34.485828: step 28050, loss = 0.74 (927.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:12:35.873622: step 28060, loss = 0.88 (922.3 examples/sec; 0.139 sec/batch)
2017-05-07 21:12:37.226068: step 28070, loss = 0.87 (946.4 examples/sec; 0.135 sec/batch)
2017-05-07 21:12:38.612617: step 28080, loss = 0.76 (923.2 examples/sec; 0.139 sec/batch)
2017-05-07 21:12:39.996580: step 28090, loss = 0.97 (924.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:12:41.489806: step 28100, loss = 0.86 (857.2 examples/sec; 0.149 sec/batch)
2017-05-07 21:12:42.791447: step 28110, loss = 0.79 (983.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:12:44.160684: step 28120, loss = 0.67 (934.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:12:45.546062: step 28130, loss = 1.05 (923.9 examples/sec; 0.139 sec/batch)
2017-05-07 21:12:46.903479: step 28140, loss = 0.90 (943.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:12:48.294081: step 28150, loss = 0.89 (920.5 examples/sec; 0.139 sec/batch)
2017-05-07 21:12:49.660793: step 28160, loss = 0.80 (936.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:12:51.054169: step 28170, loss = 0.83 (918.6 examples/sec; 0.139 sec/batch)
2017-05-07 21:12:52.447464: step 28180, loss = 1.05 (918.7 examples/sec; 0.139 sec/batch)
2017-05-07 21:12:53.834003: step 28190, loss = 0.89 (923.2 examples/sec; 0.139 sec/batch)
2017-05-07 21:12:55.299406: step 28200, loss = 0.85 (873.5 examples/sec; 0.147 sec/batch)
2017-05-07 21:12:56.570010: step 28210, loss = 0.75 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:12:57.948334: step 28220, loss = 0.69 (928.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:12:59.319682: step 28230, loss = 0.82 (933.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:13:00.700435: step 28240, loss = 0.93 (927.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:13:02.075725: step 28250, loss = 0.81 (930.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:13:03.409904: step 28260, loss = 0.70 (959.4 examples/sec; 0.133 sec/batch)
2017-05-07 21:13:04.799085: step 28270, loss = 0.94 (921.4 examples/sec; 0.139 sec/batch)
2017-05-07 21:13:06.165178: step 28280, loss = 0.90 (937.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:13:07.542719: step 28290, loss = 0.87 (929.2 examples/sec; 0.138 sec/batch)
2017-05-07 21:13:09.017412: step 28300, loss = 0.89 (868.0 examples/sec; 0.147 sec/batch)
2017-05-07 21:13:10.292228: step 28310, loss = 0.81 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:13:11.691957: step 28320, loss = 0.90 (914.5 examples/sec; 0.140 sec/batch)
2017-05-07 21:13:13.089190: step 28330, loss = 0.96 (916.1 examples/sec; 0.140 sec/batch)
2017-05-07 21:13:14.452510: step 28340, loss = 0.79 (938.9 examples/sec; 0.136 sec/batch)
2017-05-07 21:13:15.847230: step 28350, loss = 1.00 (917.8 examples/sec; 0.139 sec/batch)
2017-05-07 21:13:17.219464: step 28360, loss = 0.79 (932.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:13:18.592570: step 28370, loss = 0.88 (932.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:13:20.007623: step 28380, loss = 1.10 (904.6 examples/sec; 0.142 sec/batch)
2017-05-07 21:13:21.365700: step 28390, loss = 0.71 (942.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:13:22.816533: step 28400, loss = 0.81 (882.3 examples/sec; 0.145 sec/batch)
2017-05-07 21:13:24.101163: step 28410, loss = 0.83 (996.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:13:25.471934: step 28420, loss = 0.74 (933.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:13:26.846472: step 28430, loss = 0.89 (931.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:13:28.230339: step 28440, loss = 0.93 (924.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:13:29.649164: step 28450, loss = 0.90 (902.2 examples/sec; 0.142 sec/batch)
2017-05-07 21:13:31.009381: step 28460, loss = 0.80 (941.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:13:32.386833: step 28470, loss = 0.75 (929.3 examples/sec; 0.138 sec/batch)
2017-05-07 21:13:33.774832: step 28480, loss = 0.95 (922.2 examples/sec; 0.139 sec/batch)
2017-05-07 21:13:35.184936: step 28490, loss = 0.88 (907.7 examples/sec; 0.141 sec/batch)
2017-05-07 21:13:36.649007: step 28500, loss = 0.81 (874.3 examples/sec; 0.146 sec/batch)
2017-05-07 21:13:37.895390: step 28510, loss = 0.97 (1027.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:13:39.284672: step 28520, loss = 1.02 (921.3 examples/sec; 0.139 sec/batch)
2017-05-07 21:13:40.652711: step 28530, loss = 0.71 (935.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:13:42.058541: step 28540, loss = 0.73 (910.5 examples/sec; 0.141 sec/batch)
2017-05-07 21:13:43.452905: step 28550, loss = 0.90 (918.0 examples/sec; 0.139 sec/batch)
2017-05-07 21:13:44.839654: step 28560, loss = 0.83 (923.0 examples/sec; 0.139 sec/batch)
2017-05-07 21:13:46.195434: step 28570, loss = 0.75 (944.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:13:47.593636: step 28580, loss = 1.04 (915.5 examples/sec; 0.140 sec/batch)
2017-05-07 21:13:48.956988: step 28590, loss = 0.90 (938.9 examples/sec; 0.136 sec/batch)
2017-05-07 21:13:50.441572: step 28600, loss = 0.88 (862.2 examples/sec; 0.148 sec/batch)
2017-05-07 21:13:51.683173: step 28610, loss = 0.83 (10E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 585 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
30.9 examples/sec; 0.124 sec/batch)
2017-05-07 21:13:53.049592: step 28620, loss = 0.71 (936.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:13:54.442123: step 28630, loss = 0.82 (919.2 examples/sec; 0.139 sec/batch)
2017-05-07 21:13:55.811259: step 28640, loss = 0.78 (934.9 examples/sec; 0.137 sec/batch)
2017-05-07 21:13:57.205913: step 28650, loss = 0.63 (917.8 examples/sec; 0.139 sec/batch)
2017-05-07 21:13:58.578807: step 28660, loss = 0.86 (932.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:13:59.986214: step 28670, loss = 0.87 (909.5 examples/sec; 0.141 sec/batch)
2017-05-07 21:14:01.365601: step 28680, loss = 0.92 (928.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:14:02.710305: step 28690, loss = 1.03 (951.9 examples/sec; 0.134 sec/batch)
2017-05-07 21:14:04.200856: step 28700, loss = 0.83 (858.7 examples/sec; 0.149 sec/batch)
2017-05-07 21:14:05.491270: step 28710, loss = 1.14 (991.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:14:06.907943: step 28720, loss = 1.03 (903.5 examples/sec; 0.142 sec/batch)
2017-05-07 21:14:08.299002: step 28730, loss = 0.84 (920.2 examples/sec; 0.139 sec/batch)
2017-05-07 21:14:09.647947: step 28740, loss = 0.90 (948.9 examples/sec; 0.135 sec/batch)
2017-05-07 21:14:11.014525: step 28750, loss = 0.68 (936.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:14:12.390300: step 28760, loss = 0.83 (930.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:14:13.736232: step 28770, loss = 0.80 (951.0 examples/sec; 0.135 sec/batch)
2017-05-07 21:14:15.122435: step 28780, loss = 0.88 (923.4 examples/sec; 0.139 sec/batch)
2017-05-07 21:14:16.509065: step 28790, loss = 0.91 (923.1 examples/sec; 0.139 sec/batch)
2017-05-07 21:14:17.961855: step 28800, loss = 0.97 (881.1 examples/sec; 0.145 sec/batch)
2017-05-07 21:14:19.238536: step 28810, loss = 0.74 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:14:20.615918: step 28820, loss = 0.76 (929.3 examples/sec; 0.138 sec/batch)
2017-05-07 21:14:21.999741: step 28830, loss = 0.90 (925.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:14:23.366123: step 28840, loss = 0.93 (936.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:14:24.725672: step 28850, loss = 0.81 (941.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:14:26.101520: step 28860, loss = 0.83 (930.3 examples/sec; 0.138 sec/batch)
2017-05-07 21:14:27.484869: step 28870, loss = 0.81 (925.3 examples/sec; 0.138 sec/batch)
2017-05-07 21:14:28.859239: step 28880, loss = 0.89 (931.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:14:30.204511: step 28890, loss = 0.82 (951.5 examples/sec; 0.135 sec/batch)
2017-05-07 21:14:31.684062: step 28900, loss = 0.71 (865.1 examples/sec; 0.148 sec/batch)
2017-05-07 21:14:32.940899: step 28910, loss = 0.76 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:14:34.327404: step 28920, loss = 0.75 (923.2 examples/sec; 0.139 sec/batch)
2017-05-07 21:14:35.696945: step 28930, loss = 0.88 (934.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:14:37.093413: step 28940, loss = 0.98 (916.6 examples/sec; 0.140 sec/batch)
2017-05-07 21:14:38.460341: step 28950, loss = 0.96 (936.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:14:39.831570: step 28960, loss = 0.89 (933.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:14:41.208299: step 28970, loss = 0.77 (929.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:14:42.582498: step 28980, loss = 0.84 (931.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:14:43.988497: step 28990, loss = 1.08 (910.4 examples/sec; 0.141 sec/batch)
2017-05-07 21:14:45.460811: step 29000, loss = 1.03 (869.4 examples/sec; 0.147 sec/batch)
2017-05-07 21:14:46.750439: step 29010, loss = 1.01 (992.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:14:48.153532: step 29020, loss = 0.89 (912.3 examples/sec; 0.140 sec/batch)
2017-05-07 21:14:49.535141: step 29030, loss = 0.76 (926.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:14:50.901814: step 29040, loss = 0.81 (936.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:14:52.283401: step 29050, loss = 0.86 (926.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:14:53.644131: step 29060, loss = 0.76 (940.7 examples/sec; 0.136 sec/batch)
2017-05-07 21:14:55.021557: step 29070, loss = 0.79 (929.3 examples/sec; 0.138 sec/batch)
2017-05-07 21:14:56.374494: step 29080, loss = 0.79 (946.1 examples/sec; 0.135 sec/batch)
2017-05-07 21:14:57.738301: step 29090, loss = 0.86 (938.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:14:59.194002: step 29100, loss = 0.79 (879.3 examples/sec; 0.146 sec/batch)
2017-05-07 21:15:00.473654: step 29110, loss = 0.91 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:15:01.860338: step 29120, loss = 0.88 (923.1 examples/sec; 0.139 sec/batch)
2017-05-07 21:15:03.262508: step 29130, loss = 0.78 (912.9 examples/sec; 0.140 sec/batch)
2017-05-07 21:15:04.628182: step 29140, loss = 0.80 (937.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:15:06.003640: step 29150, loss = 0.74 (930.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:15:07.391453: step 29160, loss = 0.73 (922.3 examples/sec; 0.139 sec/batch)
2017-05-07 21:15:08.757938: step 29170, loss = 0.95 (936.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:15:10.126756: step 29180, loss = 0.67 (935.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:15:11.503090: step 29190, loss = 0.86 (930.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:15:12.977073: step 29200, loss = 0.82 (868.4 examples/sec; 0.147 sec/batch)
2017-05-07 21:15:14.248132: step 29210, loss = 0.90 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:15:15.647606: step 29220, loss = 0.98 (914.6 examples/sec; 0.140 sec/batch)
2017-05-07 21:15:17.022385: step 29230, loss = 0.85 (931.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:15:18.404711: step 29240, loss = 0.76 (926.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:15:19.754111: step 29250, loss = 0.93 (948.6 examples/sec; 0.135 sec/batch)
2017-05-07 21:15:21.111145: step 29260, loss = 0.67 (943.2 examples/sec; 0.136 sec/batch)
2017-05-07 21:15:22.503897: step 29270, loss = 0.89 (919.0 examples/sec; 0.139 sec/batch)
2017-05-07 21:15:23.889450: step 29280, loss = 0.96 (923.8 examples/sec; 0.139 sec/batch)
2017-05-07 21:15:25.254084: step 29290, loss = 1.03 (938.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:15:26.690052: step 29300, loss = 0.81 (891.4 examples/sec; 0.144 sec/batch)
2017-05-07 21:15:27.948766: step 29310, loss = 0.91 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:15:29.321464: step 29320, loss = 0.94 (932.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:15:30.720438: step 29330, loss = 0.75 (915.0 examples/sec; 0.140 sec/batch)
2017-05-07 21:15:32.109018: step 29340, loss = 1.06 (921.8 examples/sec; 0.139 sec/batch)
2017-05-07 21:15:33.505079: step 29350, loss = 0.91 (916.9 examples/sec; 0.140 sec/batch)
2017-05-07 21:15:34.875051: step 29360, loss = 0.62 (934.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:15:36.241381: step 29370, loss = 0.77 (936.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:15:37.619839: step 29380, loss = 0.91 (928.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:15:38.978274: step 29390, loss = 0.90 (942.3 examples/sec; 0.136 sec/batch)
2017-05-07 21:15:40.419984: step 29400, loss = 0.97 (887.8 examples/sec; 0.144 sec/batch)
2017-05-07 21:15:41.690630: step 29410, loss = 0.81 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:15:43.080215: step 29420, loss = 0.99 (921.1 examples/sec; 0.139 sec/batch)
2017-05-07 21:15:44.434033: step 29430, loss = 0.80 (945.5 examples/sec; 0.135 sec/batch)
2017-05-07 21:15:45.809477: step 29440, loss = 0.88 (930.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:15:47.193325: step 29450, loss = 0.79 (925.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:15:48.571283: step 29460, loss = 0.90 (928.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:15:49.921756: step 29470, loss = 1.07 (947.8 examples/sec; 0.135 sec/batch)
2017-05-07 21:15:51.297525: step 29480, loss = 0.64 (930.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:15:52.654144: step 29490, loss = 0.91 (943.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:15:54.101913: step 29500, loss = 0.84 (884.1 examples/sec; 0.145 sec/batch)
2017-05-07 21:15:55.390277: step 29510, loss = 0.82 (993.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:15:56.763823: step 29520, loss = 0.8E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 603 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
0 (931.9 examples/sec; 0.137 sec/batch)
2017-05-07 21:15:58.140679: step 29530, loss = 0.88 (929.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:15:59.536668: step 29540, loss = 0.97 (916.9 examples/sec; 0.140 sec/batch)
2017-05-07 21:16:00.941505: step 29550, loss = 0.83 (911.1 examples/sec; 0.140 sec/batch)
2017-05-07 21:16:02.320680: step 29560, loss = 0.81 (928.1 examples/sec; 0.138 sec/batch)
2017-05-07 21:16:03.714790: step 29570, loss = 0.74 (918.2 examples/sec; 0.139 sec/batch)
2017-05-07 21:16:05.081825: step 29580, loss = 0.86 (936.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:16:06.447704: step 29590, loss = 0.91 (937.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:16:07.925467: step 29600, loss = 0.79 (866.2 examples/sec; 0.148 sec/batch)
2017-05-07 21:16:09.207190: step 29610, loss = 1.01 (998.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:16:10.558318: step 29620, loss = 0.71 (947.4 examples/sec; 0.135 sec/batch)
2017-05-07 21:16:11.965065: step 29630, loss = 0.93 (909.9 examples/sec; 0.141 sec/batch)
2017-05-07 21:16:13.345779: step 29640, loss = 0.93 (927.1 examples/sec; 0.138 sec/batch)
2017-05-07 21:16:14.742330: step 29650, loss = 0.82 (916.5 examples/sec; 0.140 sec/batch)
2017-05-07 21:16:16.144738: step 29660, loss = 1.03 (912.7 examples/sec; 0.140 sec/batch)
2017-05-07 21:16:17.498869: step 29670, loss = 0.99 (945.3 examples/sec; 0.135 sec/batch)
2017-05-07 21:16:18.895580: step 29680, loss = 0.90 (916.4 examples/sec; 0.140 sec/batch)
2017-05-07 21:16:20.252755: step 29690, loss = 0.82 (943.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:16:21.717911: step 29700, loss = 0.86 (873.6 examples/sec; 0.147 sec/batch)
2017-05-07 21:16:23.043717: step 29710, loss = 0.78 (965.5 examples/sec; 0.133 sec/batch)
2017-05-07 21:16:24.411442: step 29720, loss = 0.88 (935.9 examples/sec; 0.137 sec/batch)
2017-05-07 21:16:25.771075: step 29730, loss = 0.74 (941.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:16:27.138505: step 29740, loss = 0.81 (936.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:16:28.488281: step 29750, loss = 0.83 (948.3 examples/sec; 0.135 sec/batch)
2017-05-07 21:16:29.909133: step 29760, loss = 0.77 (900.9 examples/sec; 0.142 sec/batch)
2017-05-07 21:16:31.284490: step 29770, loss = 0.71 (930.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:16:32.631641: step 29780, loss = 0.74 (950.2 examples/sec; 0.135 sec/batch)
2017-05-07 21:16:33.973372: step 29790, loss = 0.82 (954.0 examples/sec; 0.134 sec/batch)
2017-05-07 21:16:35.440281: step 29800, loss = 0.83 (872.6 examples/sec; 0.147 sec/batch)
2017-05-07 21:16:36.763029: step 29810, loss = 0.90 (967.7 examples/sec; 0.132 sec/batch)
2017-05-07 21:16:38.166196: step 29820, loss = 0.81 (912.2 examples/sec; 0.140 sec/batch)
2017-05-07 21:16:39.527309: step 29830, loss = 0.71 (940.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:16:40.891850: step 29840, loss = 0.69 (938.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:16:42.278744: step 29850, loss = 0.78 (922.9 examples/sec; 0.139 sec/batch)
2017-05-07 21:16:43.641431: step 29860, loss = 1.02 (939.3 examples/sec; 0.136 sec/batch)
2017-05-07 21:16:45.040805: step 29870, loss = 0.66 (914.7 examples/sec; 0.140 sec/batch)
2017-05-07 21:16:46.435579: step 29880, loss = 0.77 (917.7 examples/sec; 0.139 sec/batch)
2017-05-07 21:16:47.770507: step 29890, loss = 1.02 (958.9 examples/sec; 0.133 sec/batch)
2017-05-07 21:16:49.254790: step 29900, loss = 0.82 (862.4 examples/sec; 0.148 sec/batch)
2017-05-07 21:16:50.540840: step 29910, loss = 0.91 (995.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:16:51.940440: step 29920, loss = 0.93 (914.5 examples/sec; 0.140 sec/batch)
2017-05-07 21:16:53.358486: step 29930, loss = 0.90 (902.6 examples/sec; 0.142 sec/batch)
2017-05-07 21:16:54.714265: step 29940, loss = 0.96 (944.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:16:56.110419: step 29950, loss = 0.86 (916.8 examples/sec; 0.140 sec/batch)
2017-05-07 21:16:57.480929: step 29960, loss = 0.68 (934.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:16:58.863087: step 29970, loss = 0.79 (926.1 examples/sec; 0.138 sec/batch)
2017-05-07 21:17:00.217608: step 29980, loss = 0.62 (945.0 examples/sec; 0.135 sec/batch)
2017-05-07 21:17:01.591692: step 29990, loss = 1.06 (931.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:17:03.041594: step 30000, loss = 0.71 (882.8 examples/sec; 0.145 sec/batch)
2017-05-07 21:17:04.368403: step 30010, loss = 0.92 (964.7 examples/sec; 0.133 sec/batch)
2017-05-07 21:17:05.744824: step 30020, loss = 0.93 (929.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:17:07.134411: step 30030, loss = 1.00 (921.1 examples/sec; 0.139 sec/batch)
2017-05-07 21:17:08.500079: step 30040, loss = 0.94 (937.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:17:09.917170: step 30050, loss = 0.77 (903.2 examples/sec; 0.142 sec/batch)
2017-05-07 21:17:11.289781: step 30060, loss = 0.75 (932.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:17:12.651017: step 30070, loss = 0.89 (940.3 examples/sec; 0.136 sec/batch)
2017-05-07 21:17:14.012629: step 30080, loss = 0.75 (940.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:17:15.405644: step 30090, loss = 0.89 (918.9 examples/sec; 0.139 sec/batch)
2017-05-07 21:17:16.897098: step 30100, loss = 0.81 (858.2 examples/sec; 0.149 sec/batch)
2017-05-07 21:17:18.204145: step 30110, loss = 0.84 (979.3 examples/sec; 0.131 sec/batch)
2017-05-07 21:17:19.605346: step 30120, loss = 0.75 (913.5 examples/sec; 0.140 sec/batch)
2017-05-07 21:17:20.975382: step 30130, loss = 0.74 (934.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:17:22.360389: step 30140, loss = 0.85 (924.2 examples/sec; 0.139 sec/batch)
2017-05-07 21:17:23.707952: step 30150, loss = 0.91 (949.9 examples/sec; 0.135 sec/batch)
2017-05-07 21:17:25.066712: step 30160, loss = 0.82 (942.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:17:26.436426: step 30170, loss = 0.78 (934.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:17:27.789207: step 30180, loss = 0.71 (946.2 examples/sec; 0.135 sec/batch)
2017-05-07 21:17:29.177237: step 30190, loss = 1.06 (922.2 examples/sec; 0.139 sec/batch)
2017-05-07 21:17:30.665842: step 30200, loss = 0.87 (859.9 examples/sec; 0.149 sec/batch)
2017-05-07 21:17:31.935549: step 30210, loss = 0.83 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:17:33.298039: step 30220, loss = 0.76 (939.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:17:34.662362: step 30230, loss = 0.71 (938.2 examples/sec; 0.136 sec/batch)
2017-05-07 21:17:36.041897: step 30240, loss = 0.79 (927.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:17:37.399211: step 30250, loss = 0.67 (943.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:17:38.762368: step 30260, loss = 0.69 (939.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:17:40.151869: step 30270, loss = 0.77 (921.2 examples/sec; 0.139 sec/batch)
2017-05-07 21:17:41.532332: step 30280, loss = 0.87 (927.2 examples/sec; 0.138 sec/batch)
2017-05-07 21:17:42.923863: step 30290, loss = 0.82 (919.9 examples/sec; 0.139 sec/batch)
2017-05-07 21:17:44.404097: step 30300, loss = 1.00 (864.7 examples/sec; 0.148 sec/batch)
2017-05-07 21:17:45.712597: step 30310, loss = 0.96 (978.2 examples/sec; 0.131 sec/batch)
2017-05-07 21:17:47.087785: step 30320, loss = 0.83 (930.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:17:48.496853: step 30330, loss = 0.86 (908.4 examples/sec; 0.141 sec/batch)
2017-05-07 21:17:49.848610: step 30340, loss = 0.66 (946.9 examples/sec; 0.135 sec/batch)
2017-05-07 21:17:51.254487: step 30350, loss = 0.83 (910.5 examples/sec; 0.141 sec/batch)
2017-05-07 21:17:52.651072: step 30360, loss = 0.88 (916.5 examples/sec; 0.140 sec/batch)
2017-05-07 21:17:53.999574: step 30370, loss = 0.92 (949.2 examples/sec; 0.135 sec/batch)
2017-05-07 21:17:55.378783: step 30380, loss = 0.95 (928.1 examples/sec; 0.138 sec/batch)
2017-05-07 21:17:56.733670: step 30390, loss = 0.81 (944.7 examples/sec; 0.135 sec/batch)
2017-05-07 21:17:58.185573: step 30400, loss = 0.75 (881.6 examples/sec; 0.145 sec/batch)
2017-05-07 21:17:59.472622: step 30410, loss = 0.66 (994.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:18:00.839415: step 30420, loss = 0.96 (936.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:18:02.208362: step 30430, loss = 0.79E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 621 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
 (935.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:18:03.569520: step 30440, loss = 1.14 (940.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:18:04.964184: step 30450, loss = 0.83 (917.8 examples/sec; 0.139 sec/batch)
2017-05-07 21:18:06.334540: step 30460, loss = 0.87 (934.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:18:07.751600: step 30470, loss = 0.95 (903.3 examples/sec; 0.142 sec/batch)
2017-05-07 21:18:09.124806: step 30480, loss = 0.85 (932.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:18:10.509293: step 30490, loss = 0.71 (924.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:18:11.975527: step 30500, loss = 0.89 (873.0 examples/sec; 0.147 sec/batch)
2017-05-07 21:18:13.294043: step 30510, loss = 0.88 (970.8 examples/sec; 0.132 sec/batch)
2017-05-07 21:18:14.672182: step 30520, loss = 0.68 (928.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:18:16.052791: step 30530, loss = 0.74 (927.1 examples/sec; 0.138 sec/batch)
2017-05-07 21:18:17.445381: step 30540, loss = 0.79 (919.1 examples/sec; 0.139 sec/batch)
2017-05-07 21:18:18.805156: step 30550, loss = 0.86 (941.3 examples/sec; 0.136 sec/batch)
2017-05-07 21:18:20.176799: step 30560, loss = 0.78 (933.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:18:21.561908: step 30570, loss = 0.69 (924.1 examples/sec; 0.139 sec/batch)
2017-05-07 21:18:22.930538: step 30580, loss = 0.83 (935.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:18:24.297090: step 30590, loss = 0.82 (936.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:18:25.765860: step 30600, loss = 0.58 (871.5 examples/sec; 0.147 sec/batch)
2017-05-07 21:18:27.081036: step 30610, loss = 0.76 (973.3 examples/sec; 0.132 sec/batch)
2017-05-07 21:18:28.460472: step 30620, loss = 0.87 (927.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:18:29.843720: step 30630, loss = 0.69 (925.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:18:31.226890: step 30640, loss = 0.93 (925.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:18:32.637924: step 30650, loss = 0.72 (907.1 examples/sec; 0.141 sec/batch)
2017-05-07 21:18:33.991454: step 30660, loss = 0.97 (945.7 examples/sec; 0.135 sec/batch)
2017-05-07 21:18:35.361286: step 30670, loss = 0.98 (934.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:18:36.741730: step 30680, loss = 1.05 (927.2 examples/sec; 0.138 sec/batch)
2017-05-07 21:18:38.154678: step 30690, loss = 0.94 (905.9 examples/sec; 0.141 sec/batch)
2017-05-07 21:18:39.640159: step 30700, loss = 0.87 (861.7 examples/sec; 0.149 sec/batch)
2017-05-07 21:18:40.920226: step 30710, loss = 0.78 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:18:42.303940: step 30720, loss = 0.79 (925.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:18:43.697204: step 30730, loss = 0.90 (918.7 examples/sec; 0.139 sec/batch)
2017-05-07 21:18:45.052333: step 30740, loss = 0.85 (944.6 examples/sec; 0.136 sec/batch)
2017-05-07 21:18:46.453305: step 30750, loss = 0.76 (913.7 examples/sec; 0.140 sec/batch)
2017-05-07 21:18:47.846303: step 30760, loss = 0.69 (918.9 examples/sec; 0.139 sec/batch)
2017-05-07 21:18:49.265695: step 30770, loss = 0.83 (901.8 examples/sec; 0.142 sec/batch)
2017-05-07 21:18:50.648105: step 30780, loss = 0.78 (925.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:18:52.027139: step 30790, loss = 0.87 (928.2 examples/sec; 0.138 sec/batch)
2017-05-07 21:18:53.493136: step 30800, loss = 0.95 (873.1 examples/sec; 0.147 sec/batch)
2017-05-07 21:18:54.802153: step 30810, loss = 0.80 (977.8 examples/sec; 0.131 sec/batch)
2017-05-07 21:18:56.145185: step 30820, loss = 0.87 (953.1 examples/sec; 0.134 sec/batch)
2017-05-07 21:18:57.505482: step 30830, loss = 0.78 (941.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:18:58.874782: step 30840, loss = 0.84 (934.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:19:00.241233: step 30850, loss = 0.77 (936.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:19:01.596448: step 30860, loss = 0.80 (944.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:19:02.974802: step 30870, loss = 0.94 (928.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:19:04.343498: step 30880, loss = 0.81 (935.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:19:05.694852: step 30890, loss = 0.71 (947.2 examples/sec; 0.135 sec/batch)
2017-05-07 21:19:07.159021: step 30900, loss = 0.95 (874.2 examples/sec; 0.146 sec/batch)
2017-05-07 21:19:08.428867: step 30910, loss = 0.92 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:19:09.800184: step 30920, loss = 0.84 (933.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:19:11.196719: step 30930, loss = 1.05 (916.6 examples/sec; 0.140 sec/batch)
2017-05-07 21:19:12.555996: step 30940, loss = 0.78 (941.7 examples/sec; 0.136 sec/batch)
2017-05-07 21:19:13.957411: step 30950, loss = 1.09 (913.3 examples/sec; 0.140 sec/batch)
2017-05-07 21:19:15.355427: step 30960, loss = 0.82 (915.6 examples/sec; 0.140 sec/batch)
2017-05-07 21:19:16.740525: step 30970, loss = 0.85 (924.1 examples/sec; 0.139 sec/batch)
2017-05-07 21:19:18.116188: step 30980, loss = 0.88 (930.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:19:19.478611: step 30990, loss = 0.76 (939.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:19:20.963461: step 31000, loss = 0.92 (862.0 examples/sec; 0.148 sec/batch)
2017-05-07 21:19:22.236353: step 31010, loss = 0.85 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:19:23.606300: step 31020, loss = 0.95 (934.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:19:25.033845: step 31030, loss = 0.87 (896.6 examples/sec; 0.143 sec/batch)
2017-05-07 21:19:26.413331: step 31040, loss = 0.62 (927.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:19:27.786988: step 31050, loss = 0.81 (931.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:19:29.169518: step 31060, loss = 0.71 (925.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:19:30.545498: step 31070, loss = 0.86 (930.2 examples/sec; 0.138 sec/batch)
2017-05-07 21:19:31.898612: step 31080, loss = 0.75 (946.0 examples/sec; 0.135 sec/batch)
2017-05-07 21:19:33.259669: step 31090, loss = 0.69 (940.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:19:34.759045: step 31100, loss = 0.81 (853.7 examples/sec; 0.150 sec/batch)
2017-05-07 21:19:36.007786: step 31110, loss = 0.81 (1025.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:19:37.357619: step 31120, loss = 0.78 (948.3 examples/sec; 0.135 sec/batch)
2017-05-07 21:19:38.736755: step 31130, loss = 0.96 (928.1 examples/sec; 0.138 sec/batch)
2017-05-07 21:19:40.138625: step 31140, loss = 0.85 (913.1 examples/sec; 0.140 sec/batch)
2017-05-07 21:19:41.520928: step 31150, loss = 0.75 (926.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:19:42.895230: step 31160, loss = 1.27 (931.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:19:44.295668: step 31170, loss = 1.06 (914.0 examples/sec; 0.140 sec/batch)
2017-05-07 21:19:45.655278: step 31180, loss = 0.81 (941.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:19:47.010811: step 31190, loss = 0.87 (944.3 examples/sec; 0.136 sec/batch)
2017-05-07 21:19:48.492402: step 31200, loss = 0.87 (863.9 examples/sec; 0.148 sec/batch)
2017-05-07 21:19:49.783009: step 31210, loss = 0.68 (991.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:19:51.165547: step 31220, loss = 0.75 (925.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:19:52.556389: step 31230, loss = 0.76 (920.3 examples/sec; 0.139 sec/batch)
2017-05-07 21:19:53.936459: step 31240, loss = 0.97 (927.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:19:55.302701: step 31250, loss = 0.82 (936.9 examples/sec; 0.137 sec/batch)
2017-05-07 21:19:56.671627: step 31260, loss = 0.88 (935.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:19:58.026665: step 31270, loss = 0.79 (944.6 examples/sec; 0.136 sec/batch)
2017-05-07 21:19:59.367308: step 31280, loss = 0.76 (954.8 examples/sec; 0.134 sec/batch)
2017-05-07 21:20:00.725909: step 31290, loss = 0.69 (942.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:20:02.190134: step 31300, loss = 0.85 (874.2 examples/sec; 0.146 sec/batch)
2017-05-07 21:20:03.458056: step 31310, loss = 0.65 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:20:04.816132: step 31320, loss = 0.71 (942.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:20:06.174266: step 31330, loss = 0.74 (942.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:20:07.601528: step 31340, loss = 0E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 639 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
.95 (896.8 examples/sec; 0.143 sec/batch)
2017-05-07 21:20:08.966192: step 31350, loss = 0.76 (938.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:20:10.346396: step 31360, loss = 1.07 (927.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:20:11.723509: step 31370, loss = 0.81 (929.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:20:13.086323: step 31380, loss = 0.80 (939.2 examples/sec; 0.136 sec/batch)
2017-05-07 21:20:14.448408: step 31390, loss = 0.83 (939.7 examples/sec; 0.136 sec/batch)
2017-05-07 21:20:15.897746: step 31400, loss = 0.87 (883.2 examples/sec; 0.145 sec/batch)
2017-05-07 21:20:17.187512: step 31410, loss = 0.86 (992.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:20:18.587027: step 31420, loss = 0.73 (914.6 examples/sec; 0.140 sec/batch)
2017-05-07 21:20:19.954076: step 31430, loss = 0.82 (936.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:20:21.349029: step 31440, loss = 0.90 (917.6 examples/sec; 0.139 sec/batch)
2017-05-07 21:20:22.717722: step 31450, loss = 0.79 (935.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:20:24.112813: step 31460, loss = 1.06 (917.5 examples/sec; 0.140 sec/batch)
2017-05-07 21:20:25.481907: step 31470, loss = 0.77 (934.9 examples/sec; 0.137 sec/batch)
2017-05-07 21:20:26.916567: step 31480, loss = 0.86 (892.2 examples/sec; 0.143 sec/batch)
2017-05-07 21:20:28.304218: step 31490, loss = 0.86 (922.4 examples/sec; 0.139 sec/batch)
2017-05-07 21:20:29.772627: step 31500, loss = 0.94 (871.7 examples/sec; 0.147 sec/batch)
2017-05-07 21:20:31.067493: step 31510, loss = 0.88 (988.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:20:32.416126: step 31520, loss = 0.72 (949.1 examples/sec; 0.135 sec/batch)
2017-05-07 21:20:33.776696: step 31530, loss = 0.99 (940.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:20:35.169392: step 31540, loss = 0.85 (919.1 examples/sec; 0.139 sec/batch)
2017-05-07 21:20:36.541488: step 31550, loss = 0.86 (932.9 examples/sec; 0.137 sec/batch)
2017-05-07 21:20:37.934938: step 31560, loss = 0.80 (918.6 examples/sec; 0.139 sec/batch)
2017-05-07 21:20:39.328804: step 31570, loss = 0.79 (918.3 examples/sec; 0.139 sec/batch)
2017-05-07 21:20:40.693915: step 31580, loss = 0.81 (937.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:20:42.079274: step 31590, loss = 0.88 (923.9 examples/sec; 0.139 sec/batch)
2017-05-07 21:20:43.548110: step 31600, loss = 0.87 (871.4 examples/sec; 0.147 sec/batch)
2017-05-07 21:20:44.860425: step 31610, loss = 0.71 (975.4 examples/sec; 0.131 sec/batch)
2017-05-07 21:20:46.242191: step 31620, loss = 0.81 (926.3 examples/sec; 0.138 sec/batch)
2017-05-07 21:20:47.623226: step 31630, loss = 0.81 (926.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:20:48.974323: step 31640, loss = 1.10 (947.4 examples/sec; 0.135 sec/batch)
2017-05-07 21:20:50.379851: step 31650, loss = 0.93 (910.7 examples/sec; 0.141 sec/batch)
2017-05-07 21:20:51.781216: step 31660, loss = 0.84 (913.4 examples/sec; 0.140 sec/batch)
2017-05-07 21:20:53.150196: step 31670, loss = 1.03 (935.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:20:54.511873: step 31680, loss = 0.90 (940.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:20:55.929518: step 31690, loss = 0.84 (902.9 examples/sec; 0.142 sec/batch)
2017-05-07 21:20:57.415919: step 31700, loss = 0.75 (861.1 examples/sec; 0.149 sec/batch)
2017-05-07 21:20:58.706301: step 31710, loss = 0.80 (991.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:21:00.101803: step 31720, loss = 0.78 (917.2 examples/sec; 0.140 sec/batch)
2017-05-07 21:21:01.456833: step 31730, loss = 0.85 (944.6 examples/sec; 0.136 sec/batch)
2017-05-07 21:21:02.813435: step 31740, loss = 0.79 (943.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:21:04.194856: step 31750, loss = 0.96 (926.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:21:05.576736: step 31760, loss = 0.86 (926.3 examples/sec; 0.138 sec/batch)
2017-05-07 21:21:06.943304: step 31770, loss = 0.81 (936.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:21:08.305052: step 31780, loss = 0.79 (940.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:21:09.709199: step 31790, loss = 0.75 (911.6 examples/sec; 0.140 sec/batch)
2017-05-07 21:21:11.178472: step 31800, loss = 0.88 (871.2 examples/sec; 0.147 sec/batch)
2017-05-07 21:21:12.461164: step 31810, loss = 0.82 (997.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:21:13.825690: step 31820, loss = 0.81 (938.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:21:15.203633: step 31830, loss = 0.76 (928.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:21:16.556199: step 31840, loss = 0.81 (946.4 examples/sec; 0.135 sec/batch)
2017-05-07 21:21:17.934519: step 31850, loss = 0.86 (928.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:21:19.335639: step 31860, loss = 0.81 (913.6 examples/sec; 0.140 sec/batch)
2017-05-07 21:21:20.723048: step 31870, loss = 0.81 (922.6 examples/sec; 0.139 sec/batch)
2017-05-07 21:21:22.083076: step 31880, loss = 0.78 (941.2 examples/sec; 0.136 sec/batch)
2017-05-07 21:21:23.495399: step 31890, loss = 0.73 (906.4 examples/sec; 0.141 sec/batch)
2017-05-07 21:21:24.983148: step 31900, loss = 0.89 (860.3 examples/sec; 0.149 sec/batch)
2017-05-07 21:21:26.245704: step 31910, loss = 0.80 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:21:27.611211: step 31920, loss = 0.83 (937.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:21:28.989453: step 31930, loss = 0.81 (928.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:21:30.354109: step 31940, loss = 0.83 (938.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:21:31.729003: step 31950, loss = 0.92 (931.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:21:33.123326: step 31960, loss = 0.93 (918.0 examples/sec; 0.139 sec/batch)
2017-05-07 21:21:34.502879: step 31970, loss = 0.75 (927.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:21:35.883301: step 31980, loss = 1.09 (927.2 examples/sec; 0.138 sec/batch)
2017-05-07 21:21:37.254951: step 31990, loss = 0.76 (933.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:21:38.710645: step 32000, loss = 0.81 (879.3 examples/sec; 0.146 sec/batch)
2017-05-07 21:21:39.980815: step 32010, loss = 0.75 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:21:41.366132: step 32020, loss = 0.75 (924.0 examples/sec; 0.139 sec/batch)
2017-05-07 21:21:42.723386: step 32030, loss = 0.72 (943.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:21:44.096865: step 32040, loss = 0.81 (931.9 examples/sec; 0.137 sec/batch)
2017-05-07 21:21:45.459494: step 32050, loss = 0.85 (939.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:21:46.825279: step 32060, loss = 0.81 (937.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:21:48.201215: step 32070, loss = 0.73 (930.3 examples/sec; 0.138 sec/batch)
2017-05-07 21:21:49.566596: step 32080, loss = 0.83 (937.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:21:50.941417: step 32090, loss = 0.77 (931.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:21:52.431073: step 32100, loss = 1.07 (859.3 examples/sec; 0.149 sec/batch)
2017-05-07 21:21:53.709925: step 32110, loss = 0.92 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:21:55.099131: step 32120, loss = 0.88 (921.4 examples/sec; 0.139 sec/batch)
2017-05-07 21:21:56.507584: step 32130, loss = 0.64 (908.8 examples/sec; 0.141 sec/batch)
2017-05-07 21:21:57.881901: step 32140, loss = 0.88 (931.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:21:59.257629: step 32150, loss = 0.89 (930.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:22:00.622529: step 32160, loss = 0.80 (937.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:22:02.003387: step 32170, loss = 0.88 (927.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:22:03.410582: step 32180, loss = 0.94 (909.6 examples/sec; 0.141 sec/batch)
2017-05-07 21:22:04.804854: step 32190, loss = 0.87 (918.0 examples/sec; 0.139 sec/batch)
2017-05-07 21:22:06.283311: step 32200, loss = 0.89 (865.8 examples/sec; 0.148 sec/batch)
2017-05-07 21:22:07.559456: step 32210, loss = 0.77 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:22:08.941748: step 32220, loss = 0.95 (926.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:22:10.344503: step 32230, loss = 0.93 (912.5 examples/sec; 0.140 sec/batch)
2017-05-07 21:22:11.736306: step 32240, loss = 0.82 (919.7 examples/sec; 0.139 sec/batch)
2017-05-07 21:22:13.074407: step 32250, loss =E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 657 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
 1.01 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 21:22:14.456791: step 32260, loss = 0.82 (925.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:22:15.830063: step 32270, loss = 0.83 (932.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:22:17.203637: step 32280, loss = 0.79 (931.9 examples/sec; 0.137 sec/batch)
2017-05-07 21:22:18.585459: step 32290, loss = 0.82 (926.3 examples/sec; 0.138 sec/batch)
2017-05-07 21:22:20.041163: step 32300, loss = 0.76 (879.3 examples/sec; 0.146 sec/batch)
2017-05-07 21:22:21.362956: step 32310, loss = 0.93 (968.4 examples/sec; 0.132 sec/batch)
2017-05-07 21:22:22.705470: step 32320, loss = 0.63 (953.4 examples/sec; 0.134 sec/batch)
2017-05-07 21:22:24.097699: step 32330, loss = 0.99 (919.4 examples/sec; 0.139 sec/batch)
2017-05-07 21:22:25.458913: step 32340, loss = 0.79 (940.3 examples/sec; 0.136 sec/batch)
2017-05-07 21:22:26.817776: step 32350, loss = 0.89 (942.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:22:28.178344: step 32360, loss = 1.07 (940.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:22:29.541447: step 32370, loss = 0.81 (939.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:22:30.923096: step 32380, loss = 0.91 (926.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:22:32.343708: step 32390, loss = 0.99 (901.0 examples/sec; 0.142 sec/batch)
2017-05-07 21:22:33.798048: step 32400, loss = 0.93 (880.1 examples/sec; 0.145 sec/batch)
2017-05-07 21:22:35.082810: step 32410, loss = 0.78 (996.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:22:36.477118: step 32420, loss = 0.74 (918.0 examples/sec; 0.139 sec/batch)
2017-05-07 21:22:37.960596: step 32430, loss = 0.88 (862.8 examples/sec; 0.148 sec/batch)
2017-05-07 21:22:39.295063: step 32440, loss = 1.00 (959.2 examples/sec; 0.133 sec/batch)
2017-05-07 21:22:40.720542: step 32450, loss = 0.81 (897.9 examples/sec; 0.143 sec/batch)
2017-05-07 21:22:42.103454: step 32460, loss = 0.79 (925.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:22:43.457945: step 32470, loss = 1.04 (945.0 examples/sec; 0.135 sec/batch)
2017-05-07 21:22:44.826417: step 32480, loss = 0.71 (935.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:22:46.202845: step 32490, loss = 0.83 (929.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:22:47.699189: step 32500, loss = 0.74 (855.4 examples/sec; 0.150 sec/batch)
2017-05-07 21:22:49.033057: step 32510, loss = 0.73 (959.6 examples/sec; 0.133 sec/batch)
2017-05-07 21:22:50.413713: step 32520, loss = 0.97 (927.1 examples/sec; 0.138 sec/batch)
2017-05-07 21:22:51.806427: step 32530, loss = 1.00 (919.1 examples/sec; 0.139 sec/batch)
2017-05-07 21:22:53.153799: step 32540, loss = 0.72 (950.0 examples/sec; 0.135 sec/batch)
2017-05-07 21:22:54.512716: step 32550, loss = 0.72 (941.9 examples/sec; 0.136 sec/batch)
2017-05-07 21:22:55.868266: step 32560, loss = 0.83 (944.3 examples/sec; 0.136 sec/batch)
2017-05-07 21:22:57.227441: step 32570, loss = 0.94 (941.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:22:58.599005: step 32580, loss = 0.87 (933.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:22:59.976437: step 32590, loss = 0.81 (929.2 examples/sec; 0.138 sec/batch)
2017-05-07 21:23:01.465135: step 32600, loss = 0.89 (859.8 examples/sec; 0.149 sec/batch)
2017-05-07 21:23:02.728142: step 32610, loss = 0.74 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:23:04.082832: step 32620, loss = 1.04 (944.7 examples/sec; 0.135 sec/batch)
2017-05-07 21:23:05.501590: step 32630, loss = 0.96 (902.2 examples/sec; 0.142 sec/batch)
2017-05-07 21:23:06.887278: step 32640, loss = 0.74 (923.7 examples/sec; 0.139 sec/batch)
2017-05-07 21:23:08.268163: step 32650, loss = 0.85 (926.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:23:09.628245: step 32660, loss = 0.91 (941.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:23:10.979697: step 32670, loss = 0.70 (947.1 examples/sec; 0.135 sec/batch)
2017-05-07 21:23:12.373738: step 32680, loss = 0.78 (918.2 examples/sec; 0.139 sec/batch)
2017-05-07 21:23:13.768942: step 32690, loss = 0.78 (917.4 examples/sec; 0.140 sec/batch)
2017-05-07 21:23:15.210172: step 32700, loss = 0.81 (888.1 examples/sec; 0.144 sec/batch)
2017-05-07 21:23:16.524523: step 32710, loss = 0.74 (973.9 examples/sec; 0.131 sec/batch)
2017-05-07 21:23:17.896289: step 32720, loss = 1.01 (933.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:23:19.286400: step 32730, loss = 0.81 (920.8 examples/sec; 0.139 sec/batch)
2017-05-07 21:23:20.653519: step 32740, loss = 0.67 (936.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:23:22.029862: step 32750, loss = 0.82 (930.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:23:23.416606: step 32760, loss = 0.85 (923.0 examples/sec; 0.139 sec/batch)
2017-05-07 21:23:24.813316: step 32770, loss = 0.91 (916.4 examples/sec; 0.140 sec/batch)
2017-05-07 21:23:26.184147: step 32780, loss = 1.00 (933.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:23:27.550129: step 32790, loss = 1.12 (937.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:23:29.037336: step 32800, loss = 0.89 (860.7 examples/sec; 0.149 sec/batch)
2017-05-07 21:23:30.343506: step 32810, loss = 0.76 (980.0 examples/sec; 0.131 sec/batch)
2017-05-07 21:23:31.715698: step 32820, loss = 0.80 (932.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:23:33.119576: step 32830, loss = 0.74 (911.8 examples/sec; 0.140 sec/batch)
2017-05-07 21:23:34.478830: step 32840, loss = 0.82 (941.7 examples/sec; 0.136 sec/batch)
2017-05-07 21:23:35.851152: step 32850, loss = 0.84 (932.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:23:37.197361: step 32860, loss = 0.85 (950.8 examples/sec; 0.135 sec/batch)
2017-05-07 21:23:38.557927: step 32870, loss = 0.76 (940.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:23:39.948159: step 32880, loss = 0.75 (920.7 examples/sec; 0.139 sec/batch)
2017-05-07 21:23:41.322739: step 32890, loss = 0.78 (931.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:23:42.783048: step 32900, loss = 0.91 (876.5 examples/sec; 0.146 sec/batch)
2017-05-07 21:23:44.052374: step 32910, loss = 0.89 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:23:45.401017: step 32920, loss = 0.87 (949.1 examples/sec; 0.135 sec/batch)
2017-05-07 21:23:46.782622: step 32930, loss = 0.74 (926.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:23:48.183380: step 32940, loss = 0.79 (913.8 examples/sec; 0.140 sec/batch)
2017-05-07 21:23:49.570590: step 32950, loss = 0.72 (922.7 examples/sec; 0.139 sec/batch)
2017-05-07 21:23:50.962285: step 32960, loss = 0.67 (919.7 examples/sec; 0.139 sec/batch)
2017-05-07 21:23:52.317649: step 32970, loss = 0.71 (944.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:23:53.729542: step 32980, loss = 0.95 (906.6 examples/sec; 0.141 sec/batch)
2017-05-07 21:23:55.104864: step 32990, loss = 0.79 (930.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:23:56.581767: step 33000, loss = 1.00 (866.7 examples/sec; 0.148 sec/batch)
2017-05-07 21:23:57.866418: step 33010, loss = 0.76 (996.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:23:59.218887: step 33020, loss = 0.87 (946.4 examples/sec; 0.135 sec/batch)
2017-05-07 21:24:00.581100: step 33030, loss = 0.78 (939.6 examples/sec; 0.136 sec/batch)
2017-05-07 21:24:01.974234: step 33040, loss = 0.83 (918.8 examples/sec; 0.139 sec/batch)
2017-05-07 21:24:03.321256: step 33050, loss = 0.87 (950.2 examples/sec; 0.135 sec/batch)
2017-05-07 21:24:04.675994: step 33060, loss = 0.79 (944.9 examples/sec; 0.135 sec/batch)
2017-05-07 21:24:06.041698: step 33070, loss = 0.95 (937.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:24:07.400496: step 33080, loss = 0.88 (942.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:24:08.803761: step 33090, loss = 0.75 (912.2 examples/sec; 0.140 sec/batch)
2017-05-07 21:24:10.327519: step 33100, loss = 0.83 (840.0 examples/sec; 0.152 sec/batch)
2017-05-07 21:24:11.633884: step 33110, loss = 0.69 (979.8 examples/sec; 0.131 sec/batch)
2017-05-07 21:24:13.021812: step 33120, loss = 0.79 (922.2 examples/sec; 0.139 sec/batch)
2017-05-07 21:24:14.408155: step 33130, loss = 0.70 (923.3 examples/sec; 0.139 sec/batch)
2017-05-07 21:24:15.781344: step 33140, loss = 0.78 (932.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:24:17.156111: step 33150, loss = 0.73 (931.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:24:18.531578: step 33160, loss =E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 676 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
 0.68 (930.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:24:19.953691: step 33170, loss = 0.99 (900.1 examples/sec; 0.142 sec/batch)
2017-05-07 21:24:21.323318: step 33180, loss = 0.84 (934.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:24:22.686373: step 33190, loss = 0.76 (939.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:24:24.141824: step 33200, loss = 0.76 (879.5 examples/sec; 0.146 sec/batch)
2017-05-07 21:24:25.436652: step 33210, loss = 1.04 (988.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:24:26.831549: step 33220, loss = 0.71 (917.6 examples/sec; 0.139 sec/batch)
2017-05-07 21:24:28.186451: step 33230, loss = 0.86 (944.7 examples/sec; 0.135 sec/batch)
2017-05-07 21:24:29.541434: step 33240, loss = 0.79 (944.7 examples/sec; 0.135 sec/batch)
2017-05-07 21:24:30.924454: step 33250, loss = 0.68 (925.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:24:32.294906: step 33260, loss = 1.03 (934.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:24:33.673075: step 33270, loss = 0.78 (928.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:24:35.071159: step 33280, loss = 0.78 (915.5 examples/sec; 0.140 sec/batch)
2017-05-07 21:24:36.452116: step 33290, loss = 1.10 (926.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:24:37.939728: step 33300, loss = 0.92 (860.4 examples/sec; 0.149 sec/batch)
2017-05-07 21:24:39.215140: step 33310, loss = 0.71 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:24:40.614827: step 33320, loss = 0.93 (914.5 examples/sec; 0.140 sec/batch)
2017-05-07 21:24:41.984094: step 33330, loss = 0.91 (934.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:24:43.362850: step 33340, loss = 0.93 (928.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:24:44.763803: step 33350, loss = 0.94 (913.7 examples/sec; 0.140 sec/batch)
2017-05-07 21:24:46.133488: step 33360, loss = 0.69 (934.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:24:47.487825: step 33370, loss = 0.84 (945.1 examples/sec; 0.135 sec/batch)
2017-05-07 21:24:48.860673: step 33380, loss = 0.81 (932.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:24:50.240737: step 33390, loss = 1.04 (927.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:24:51.737608: step 33400, loss = 0.96 (855.1 examples/sec; 0.150 sec/batch)
2017-05-07 21:24:53.013525: step 33410, loss = 0.96 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:24:54.409481: step 33420, loss = 0.88 (916.9 examples/sec; 0.140 sec/batch)
2017-05-07 21:24:55.804656: step 33430, loss = 0.77 (917.4 examples/sec; 0.140 sec/batch)
2017-05-07 21:24:57.167719: step 33440, loss = 0.88 (939.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:24:58.527789: step 33450, loss = 0.95 (941.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:24:59.895522: step 33460, loss = 0.77 (935.9 examples/sec; 0.137 sec/batch)
2017-05-07 21:25:01.244953: step 33470, loss = 0.81 (948.5 examples/sec; 0.135 sec/batch)
2017-05-07 21:25:02.622843: step 33480, loss = 0.79 (929.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:25:04.029561: step 33490, loss = 0.74 (909.9 examples/sec; 0.141 sec/batch)
2017-05-07 21:25:05.528187: step 33500, loss = 0.95 (854.1 examples/sec; 0.150 sec/batch)
2017-05-07 21:25:06.829933: step 33510, loss = 0.96 (983.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:25:08.185870: step 33520, loss = 0.80 (944.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:25:09.531110: step 33530, loss = 0.71 (951.5 examples/sec; 0.135 sec/batch)
2017-05-07 21:25:10.883670: step 33540, loss = 0.72 (946.4 examples/sec; 0.135 sec/batch)
2017-05-07 21:25:12.258413: step 33550, loss = 1.08 (931.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:25:13.638421: step 33560, loss = 0.94 (927.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:25:15.047812: step 33570, loss = 0.88 (908.2 examples/sec; 0.141 sec/batch)
2017-05-07 21:25:16.427736: step 33580, loss = 0.77 (927.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:25:17.801935: step 33590, loss = 0.71 (931.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:25:19.237294: step 33600, loss = 0.84 (891.8 examples/sec; 0.144 sec/batch)
2017-05-07 21:25:20.539416: step 33610, loss = 0.65 (983.0 examples/sec; 0.130 sec/batch)
2017-05-07 21:25:21.934835: step 33620, loss = 0.81 (917.3 examples/sec; 0.140 sec/batch)
2017-05-07 21:25:23.320296: step 33630, loss = 0.93 (923.9 examples/sec; 0.139 sec/batch)
2017-05-07 21:25:24.695332: step 33640, loss = 0.93 (930.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:25:26.051779: step 33650, loss = 0.67 (943.6 examples/sec; 0.136 sec/batch)
2017-05-07 21:25:27.412238: step 33660, loss = 0.78 (940.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:25:28.790162: step 33670, loss = 0.99 (928.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:25:30.153984: step 33680, loss = 0.76 (938.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:25:31.533272: step 33690, loss = 0.88 (928.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:25:32.978293: step 33700, loss = 0.68 (885.8 examples/sec; 0.145 sec/batch)
2017-05-07 21:25:34.261586: step 33710, loss = 0.83 (997.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:25:35.629374: step 33720, loss = 0.81 (935.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:25:37.050382: step 33730, loss = 0.77 (900.8 examples/sec; 0.142 sec/batch)
2017-05-07 21:25:38.427253: step 33740, loss = 0.75 (929.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:25:39.813171: step 33750, loss = 0.90 (923.6 examples/sec; 0.139 sec/batch)
2017-05-07 21:25:41.188379: step 33760, loss = 1.01 (930.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:25:42.554545: step 33770, loss = 0.83 (936.9 examples/sec; 0.137 sec/batch)
2017-05-07 21:25:43.934923: step 33780, loss = 0.95 (927.3 examples/sec; 0.138 sec/batch)
2017-05-07 21:25:45.310071: step 33790, loss = 0.93 (930.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:25:46.743255: step 33800, loss = 0.79 (893.1 examples/sec; 0.143 sec/batch)
2017-05-07 21:25:48.013317: step 33810, loss = 0.62 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:25:49.377631: step 33820, loss = 0.82 (938.2 examples/sec; 0.136 sec/batch)
2017-05-07 21:25:50.757513: step 33830, loss = 0.79 (927.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:25:52.132679: step 33840, loss = 0.92 (930.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:25:53.549458: step 33850, loss = 0.72 (903.5 examples/sec; 0.142 sec/batch)
2017-05-07 21:25:54.923992: step 33860, loss = 0.84 (931.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:25:56.322118: step 33870, loss = 0.85 (915.5 examples/sec; 0.140 sec/batch)
2017-05-07 21:25:57.697573: step 33880, loss = 0.92 (930.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:25:59.114544: step 33890, loss = 0.92 (903.3 examples/sec; 0.142 sec/batch)
2017-05-07 21:26:00.568612: step 33900, loss = 0.86 (880.3 examples/sec; 0.145 sec/batch)
2017-05-07 21:26:01.862367: step 33910, loss = 0.95 (989.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:26:03.271776: step 33920, loss = 1.15 (908.2 examples/sec; 0.141 sec/batch)
2017-05-07 21:26:04.633458: step 33930, loss = 0.85 (940.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:26:06.019065: step 33940, loss = 0.86 (923.8 examples/sec; 0.139 sec/batch)
2017-05-07 21:26:07.398707: step 33950, loss = 0.71 (927.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:26:08.769205: step 33960, loss = 0.98 (934.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:26:10.144818: step 33970, loss = 0.72 (930.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:26:11.534096: step 33980, loss = 0.77 (921.3 examples/sec; 0.139 sec/batch)
2017-05-07 21:26:12.888462: step 33990, loss = 0.71 (945.1 examples/sec; 0.135 sec/batch)
2017-05-07 21:26:14.372859: step 34000, loss = 0.70 (862.3 examples/sec; 0.148 sec/batch)
2017-05-07 21:26:15.663665: step 34010, loss = 0.91 (991.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:26:17.047060: step 34020, loss = 0.81 (925.3 examples/sec; 0.138 sec/batch)
2017-05-07 21:26:18.443672: step 34030, loss = 0.74 (916.5 examples/sec; 0.140 sec/batch)
2017-05-07 21:26:19.814498: step 34040, loss = 0.84 (933.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:26:21.196789: step 34050, loss = 0.77 (926.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:26:22.551286: step 34060, loss = 0.75 (945.0 examples/sec; 0.135 sec/batch)
2017-05-07 21:26:23.924028: step 34070, loss E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 694 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
= 0.66 (932.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:26:25.325031: step 34080, loss = 0.79 (913.6 examples/sec; 0.140 sec/batch)
2017-05-07 21:26:26.691311: step 34090, loss = 0.76 (936.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:26:28.127910: step 34100, loss = 1.01 (891.0 examples/sec; 0.144 sec/batch)
2017-05-07 21:26:29.443058: step 34110, loss = 0.71 (973.3 examples/sec; 0.132 sec/batch)
2017-05-07 21:26:30.798594: step 34120, loss = 0.89 (944.3 examples/sec; 0.136 sec/batch)
2017-05-07 21:26:32.176928: step 34130, loss = 0.78 (928.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:26:33.558410: step 34140, loss = 0.88 (926.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:26:34.920107: step 34150, loss = 0.82 (940.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:26:36.303831: step 34160, loss = 0.91 (925.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:26:37.688192: step 34170, loss = 0.73 (924.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:26:39.060776: step 34180, loss = 0.88 (932.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:26:40.433429: step 34190, loss = 0.86 (932.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:26:41.918902: step 34200, loss = 0.99 (861.7 examples/sec; 0.149 sec/batch)
2017-05-07 21:26:43.190139: step 34210, loss = 0.80 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:26:44.572730: step 34220, loss = 0.66 (925.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:26:45.935931: step 34230, loss = 0.82 (939.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:26:47.274242: step 34240, loss = 0.85 (956.4 examples/sec; 0.134 sec/batch)
2017-05-07 21:26:48.645064: step 34250, loss = 0.82 (933.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:26:50.006002: step 34260, loss = 1.04 (940.6 examples/sec; 0.136 sec/batch)
2017-05-07 21:26:51.411529: step 34270, loss = 0.94 (910.7 examples/sec; 0.141 sec/batch)
2017-05-07 21:26:52.808471: step 34280, loss = 0.75 (916.3 examples/sec; 0.140 sec/batch)
2017-05-07 21:26:54.191731: step 34290, loss = 0.82 (925.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:26:55.654178: step 34300, loss = 0.92 (875.2 examples/sec; 0.146 sec/batch)
2017-05-07 21:26:56.903193: step 34310, loss = 0.82 (1024.8 examples/sec; 0.125 sec/batch)
2017-05-07 21:26:58.294011: step 34320, loss = 0.97 (920.3 examples/sec; 0.139 sec/batch)
2017-05-07 21:26:59.675215: step 34330, loss = 0.99 (926.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:27:01.040946: step 34340, loss = 0.90 (937.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:27:02.401882: step 34350, loss = 0.75 (940.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:27:03.757782: step 34360, loss = 0.69 (944.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:27:05.118798: step 34370, loss = 0.87 (940.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:27:06.507652: step 34380, loss = 0.61 (921.6 examples/sec; 0.139 sec/batch)
2017-05-07 21:27:07.895796: step 34390, loss = 0.85 (922.1 examples/sec; 0.139 sec/batch)
2017-05-07 21:27:09.381182: step 34400, loss = 0.84 (861.7 examples/sec; 0.149 sec/batch)
2017-05-07 21:27:10.682127: step 34410, loss = 0.84 (983.9 examples/sec; 0.130 sec/batch)
2017-05-07 21:27:12.036353: step 34420, loss = 0.87 (945.2 examples/sec; 0.135 sec/batch)
2017-05-07 21:27:13.392524: step 34430, loss = 0.83 (943.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:27:14.776333: step 34440, loss = 0.78 (925.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:27:16.118360: step 34450, loss = 0.75 (953.8 examples/sec; 0.134 sec/batch)
2017-05-07 21:27:17.499830: step 34460, loss = 0.70 (926.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:27:18.911608: step 34470, loss = 0.84 (906.7 examples/sec; 0.141 sec/batch)
2017-05-07 21:27:20.279218: step 34480, loss = 0.75 (935.9 examples/sec; 0.137 sec/batch)
2017-05-07 21:27:21.638194: step 34490, loss = 0.92 (941.9 examples/sec; 0.136 sec/batch)
2017-05-07 21:27:23.115537: step 34500, loss = 0.76 (866.4 examples/sec; 0.148 sec/batch)
2017-05-07 21:27:24.391227: step 34510, loss = 0.89 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:27:25.749753: step 34520, loss = 0.83 (942.2 examples/sec; 0.136 sec/batch)
2017-05-07 21:27:27.151566: step 34530, loss = 0.82 (913.1 examples/sec; 0.140 sec/batch)
2017-05-07 21:27:28.512216: step 34540, loss = 0.90 (940.7 examples/sec; 0.136 sec/batch)
2017-05-07 21:27:29.873469: step 34550, loss = 0.63 (940.3 examples/sec; 0.136 sec/batch)
2017-05-07 21:27:31.301607: step 34560, loss = 0.76 (896.3 examples/sec; 0.143 sec/batch)
2017-05-07 21:27:32.682327: step 34570, loss = 0.84 (927.1 examples/sec; 0.138 sec/batch)
2017-05-07 21:27:34.045546: step 34580, loss = 0.74 (938.9 examples/sec; 0.136 sec/batch)
2017-05-07 21:27:35.429501: step 34590, loss = 0.99 (924.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:27:36.907140: step 34600, loss = 0.73 (866.2 examples/sec; 0.148 sec/batch)
2017-05-07 21:27:38.190922: step 34610, loss = 0.72 (997.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:27:39.573512: step 34620, loss = 0.75 (925.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:27:40.978630: step 34630, loss = 0.73 (911.0 examples/sec; 0.141 sec/batch)
2017-05-07 21:27:42.364411: step 34640, loss = 0.82 (923.7 examples/sec; 0.139 sec/batch)
2017-05-07 21:27:43.753782: step 34650, loss = 0.86 (921.3 examples/sec; 0.139 sec/batch)
2017-05-07 21:27:45.157670: step 34660, loss = 0.71 (911.7 examples/sec; 0.140 sec/batch)
2017-05-07 21:27:46.533648: step 34670, loss = 0.79 (930.2 examples/sec; 0.138 sec/batch)
2017-05-07 21:27:47.919548: step 34680, loss = 1.11 (923.6 examples/sec; 0.139 sec/batch)
2017-05-07 21:27:49.320706: step 34690, loss = 0.85 (913.5 examples/sec; 0.140 sec/batch)
2017-05-07 21:27:50.781759: step 34700, loss = 0.62 (876.1 examples/sec; 0.146 sec/batch)
2017-05-07 21:27:52.056603: step 34710, loss = 0.84 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:27:53.425195: step 34720, loss = 0.87 (935.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:27:54.787812: step 34730, loss = 0.86 (939.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:27:56.170860: step 34740, loss = 0.91 (925.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:27:57.564718: step 34750, loss = 0.81 (918.3 examples/sec; 0.139 sec/batch)
2017-05-07 21:27:58.975776: step 34760, loss = 0.91 (907.1 examples/sec; 0.141 sec/batch)
2017-05-07 21:28:00.346799: step 34770, loss = 0.79 (933.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:28:01.763526: step 34780, loss = 0.88 (903.5 examples/sec; 0.142 sec/batch)
2017-05-07 21:28:03.172920: step 34790, loss = 0.75 (908.2 examples/sec; 0.141 sec/batch)
2017-05-07 21:28:04.643303: step 34800, loss = 0.88 (870.5 examples/sec; 0.147 sec/batch)
2017-05-07 21:28:05.900197: step 34810, loss = 0.68 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:28:07.254121: step 34820, loss = 0.78 (945.4 examples/sec; 0.135 sec/batch)
2017-05-07 21:28:08.632552: step 34830, loss = 0.94 (928.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:28:09.986181: step 34840, loss = 0.69 (945.6 examples/sec; 0.135 sec/batch)
2017-05-07 21:28:11.391491: step 34850, loss = 0.79 (910.8 examples/sec; 0.141 sec/batch)
2017-05-07 21:28:12.758888: step 34860, loss = 0.85 (936.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:28:14.120942: step 34870, loss = 0.77 (939.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:28:15.509363: step 34880, loss = 0.91 (921.9 examples/sec; 0.139 sec/batch)
2017-05-07 21:28:16.879231: step 34890, loss = 1.17 (934.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:28:18.324308: step 34900, loss = 0.84 (885.8 examples/sec; 0.145 sec/batch)
2017-05-07 21:28:19.595592: step 34910, loss = 0.71 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:28:20.964881: step 34920, loss = 0.65 (934.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:28:22.331204: step 34930, loss = 0.81 (936.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:28:23.731251: step 34940, loss = 0.89 (914.3 examples/sec; 0.140 sec/batch)
2017-05-07 21:28:25.118106: step 34950, loss = 0.82 (923.0 examples/sec; 0.139 sec/batch)
2017-05-07 21:28:26.472547: step 34960, loss = 0.61 (945.0 examples/sec; 0.135 sec/batch)
2017-05-07 21:28:27.853605: step 34970, loss = 0.74 (926.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:28:29.196858: step 34980, lE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 712 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
oss = 0.86 (952.9 examples/sec; 0.134 sec/batch)
2017-05-07 21:28:30.588860: step 34990, loss = 0.90 (919.5 examples/sec; 0.139 sec/batch)
2017-05-07 21:28:32.036797: step 35000, loss = 0.95 (884.0 examples/sec; 0.145 sec/batch)
2017-05-07 21:28:33.307274: step 35010, loss = 0.81 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:28:34.666634: step 35020, loss = 0.88 (941.6 examples/sec; 0.136 sec/batch)
2017-05-07 21:28:36.023034: step 35030, loss = 0.86 (943.7 examples/sec; 0.136 sec/batch)
2017-05-07 21:28:37.377549: step 35040, loss = 0.98 (945.0 examples/sec; 0.135 sec/batch)
2017-05-07 21:28:38.790453: step 35050, loss = 0.98 (905.9 examples/sec; 0.141 sec/batch)
2017-05-07 21:28:40.165429: step 35060, loss = 1.00 (930.9 examples/sec; 0.137 sec/batch)
2017-05-07 21:28:41.538478: step 35070, loss = 1.00 (932.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:28:42.894073: step 35080, loss = 0.74 (944.2 examples/sec; 0.136 sec/batch)
2017-05-07 21:28:44.262608: step 35090, loss = 0.70 (935.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:28:45.736331: step 35100, loss = 0.72 (868.5 examples/sec; 0.147 sec/batch)
2017-05-07 21:28:47.009939: step 35110, loss = 0.79 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:28:48.374605: step 35120, loss = 0.76 (938.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:28:49.742056: step 35130, loss = 0.65 (936.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:28:51.121385: step 35140, loss = 0.70 (928.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:28:52.470910: step 35150, loss = 0.68 (948.5 examples/sec; 0.135 sec/batch)
2017-05-07 21:28:53.839441: step 35160, loss = 0.74 (935.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:28:55.216541: step 35170, loss = 0.90 (929.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:28:56.609829: step 35180, loss = 0.95 (918.7 examples/sec; 0.139 sec/batch)
2017-05-07 21:28:57.971532: step 35190, loss = 0.79 (940.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:28:59.426524: step 35200, loss = 0.76 (879.7 examples/sec; 0.145 sec/batch)
2017-05-07 21:29:00.721392: step 35210, loss = 0.90 (988.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:29:02.071770: step 35220, loss = 0.80 (947.9 examples/sec; 0.135 sec/batch)
2017-05-07 21:29:03.433276: step 35230, loss = 0.77 (940.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:29:04.830516: step 35240, loss = 0.87 (916.1 examples/sec; 0.140 sec/batch)
2017-05-07 21:29:06.207524: step 35250, loss = 1.00 (929.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:29:07.610730: step 35260, loss = 0.99 (912.2 examples/sec; 0.140 sec/batch)
2017-05-07 21:29:08.995256: step 35270, loss = 0.69 (924.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:29:10.363894: step 35280, loss = 0.75 (935.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:29:11.746243: step 35290, loss = 0.75 (926.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:29:13.168049: step 35300, loss = 0.88 (900.3 examples/sec; 0.142 sec/batch)
2017-05-07 21:29:14.478480: step 35310, loss = 0.84 (976.8 examples/sec; 0.131 sec/batch)
2017-05-07 21:29:15.846803: step 35320, loss = 0.79 (935.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:29:17.219555: step 35330, loss = 1.04 (932.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:29:18.587566: step 35340, loss = 0.70 (935.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:29:19.961746: step 35350, loss = 0.91 (931.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:29:21.336642: step 35360, loss = 0.74 (931.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:29:22.680603: step 35370, loss = 0.93 (952.4 examples/sec; 0.134 sec/batch)
2017-05-07 21:29:24.057094: step 35380, loss = 0.88 (929.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:29:25.445599: step 35390, loss = 0.67 (921.9 examples/sec; 0.139 sec/batch)
2017-05-07 21:29:26.918320: step 35400, loss = 0.63 (869.1 examples/sec; 0.147 sec/batch)
2017-05-07 21:29:28.214313: step 35410, loss = 0.73 (987.7 examples/sec; 0.130 sec/batch)
2017-05-07 21:29:29.557422: step 35420, loss = 0.74 (953.0 examples/sec; 0.134 sec/batch)
2017-05-07 21:29:30.950010: step 35430, loss = 0.69 (919.2 examples/sec; 0.139 sec/batch)
2017-05-07 21:29:32.361179: step 35440, loss = 0.99 (907.1 examples/sec; 0.141 sec/batch)
2017-05-07 21:29:33.738028: step 35450, loss = 0.92 (929.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:29:35.097400: step 35460, loss = 0.88 (941.6 examples/sec; 0.136 sec/batch)
2017-05-07 21:29:36.475422: step 35470, loss = 0.85 (928.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:29:37.850220: step 35480, loss = 1.00 (931.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:29:39.212435: step 35490, loss = 0.81 (939.7 examples/sec; 0.136 sec/batch)
2017-05-07 21:29:40.715096: step 35500, loss = 0.87 (851.8 examples/sec; 0.150 sec/batch)
2017-05-07 21:29:42.028291: step 35510, loss = 0.74 (974.7 examples/sec; 0.131 sec/batch)
2017-05-07 21:29:43.389945: step 35520, loss = 0.76 (940.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:29:44.746794: step 35530, loss = 0.74 (943.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:29:46.128222: step 35540, loss = 0.83 (926.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:29:47.485179: step 35550, loss = 0.78 (943.3 examples/sec; 0.136 sec/batch)
2017-05-07 21:29:48.837456: step 35560, loss = 0.91 (946.6 examples/sec; 0.135 sec/batch)
2017-05-07 21:29:50.206648: step 35570, loss = 0.78 (934.9 examples/sec; 0.137 sec/batch)
2017-05-07 21:29:51.555493: step 35580, loss = 0.78 (949.0 examples/sec; 0.135 sec/batch)
2017-05-07 21:29:52.920158: step 35590, loss = 0.76 (938.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:29:54.382058: step 35600, loss = 0.80 (875.6 examples/sec; 0.146 sec/batch)
2017-05-07 21:29:55.624156: step 35610, loss = 1.15 (1030.5 examples/sec; 0.124 sec/batch)
2017-05-07 21:29:56.996803: step 35620, loss = 1.23 (932.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:29:58.382017: step 35630, loss = 1.05 (924.0 examples/sec; 0.139 sec/batch)
2017-05-07 21:29:59.753378: step 35640, loss = 0.77 (933.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:30:01.168074: step 35650, loss = 0.75 (904.8 examples/sec; 0.141 sec/batch)
2017-05-07 21:30:02.526109: step 35660, loss = 0.83 (942.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:30:03.911429: step 35670, loss = 0.80 (924.0 examples/sec; 0.139 sec/batch)
2017-05-07 21:30:05.287083: step 35680, loss = 0.67 (930.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:30:06.641669: step 35690, loss = 0.69 (944.9 examples/sec; 0.135 sec/batch)
2017-05-07 21:30:08.111482: step 35700, loss = 0.72 (870.9 examples/sec; 0.147 sec/batch)
2017-05-07 21:30:09.386243: step 35710, loss = 0.69 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:30:10.770477: step 35720, loss = 0.69 (924.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:30:12.150179: step 35730, loss = 0.88 (927.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:30:13.547050: step 35740, loss = 0.82 (916.3 examples/sec; 0.140 sec/batch)
2017-05-07 21:30:14.925411: step 35750, loss = 0.82 (928.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:30:16.303740: step 35760, loss = 0.88 (928.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:30:17.660808: step 35770, loss = 0.60 (943.2 examples/sec; 0.136 sec/batch)
2017-05-07 21:30:19.041952: step 35780, loss = 0.75 (926.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:30:20.402862: step 35790, loss = 0.78 (940.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:30:21.862054: step 35800, loss = 0.99 (877.2 examples/sec; 0.146 sec/batch)
2017-05-07 21:30:23.145760: step 35810, loss = 0.77 (997.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:30:24.533356: step 35820, loss = 0.85 (922.5 examples/sec; 0.139 sec/batch)
2017-05-07 21:30:25.880891: step 35830, loss = 0.85 (949.9 examples/sec; 0.135 sec/batch)
2017-05-07 21:30:27.282206: step 35840, loss = 0.89 (913.4 examples/sec; 0.140 sec/batch)
2017-05-07 21:30:28.630684: step 35850, loss = 1.00 (949.2 examples/sec; 0.135 sec/batch)
2017-05-07 21:30:30.066941: step 35860, loss = 0.78 (891.2 examples/sec; 0.144 sec/batch)
2017-05-07 21:30:31.377407: step 35870, loss = 0.87 (976.7 examples/sec; 0.131 sec/batch)
2017-05-07 21:30:32.635822: step 35880, loss = 0.76 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:30:33.930684: step 35890E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 730 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
, loss = 0.82 (988.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:30:35.287368: step 35900, loss = 0.70 (943.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:30:36.495304: step 35910, loss = 0.90 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-07 21:30:37.792687: step 35920, loss = 1.01 (986.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:30:39.089049: step 35930, loss = 0.95 (987.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:30:40.400003: step 35940, loss = 0.84 (976.4 examples/sec; 0.131 sec/batch)
2017-05-07 21:30:41.653586: step 35950, loss = 0.86 (1021.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:30:42.958110: step 35960, loss = 0.82 (981.2 examples/sec; 0.130 sec/batch)
2017-05-07 21:30:44.243125: step 35970, loss = 0.91 (996.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:30:45.541442: step 35980, loss = 0.86 (985.9 examples/sec; 0.130 sec/batch)
2017-05-07 21:30:46.810825: step 35990, loss = 0.80 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:30:48.171170: step 36000, loss = 0.81 (940.9 examples/sec; 0.136 sec/batch)
2017-05-07 21:30:49.355095: step 36010, loss = 0.81 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-07 21:30:50.622589: step 36020, loss = 0.70 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:30:51.886373: step 36030, loss = 0.79 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:30:53.133130: step 36040, loss = 1.06 (1026.7 examples/sec; 0.125 sec/batch)
2017-05-07 21:30:54.388108: step 36050, loss = 0.91 (1019.9 examples/sec; 0.125 sec/batch)
2017-05-07 21:30:55.681274: step 36060, loss = 0.82 (989.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:30:56.978552: step 36070, loss = 0.82 (986.7 examples/sec; 0.130 sec/batch)
2017-05-07 21:30:58.254299: step 36080, loss = 0.83 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:30:59.521436: step 36090, loss = 0.82 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:31:00.879889: step 36100, loss = 1.01 (942.2 examples/sec; 0.136 sec/batch)
2017-05-07 21:31:02.033109: step 36110, loss = 0.77 (1109.9 examples/sec; 0.115 sec/batch)
2017-05-07 21:31:03.342785: step 36120, loss = 0.76 (977.3 examples/sec; 0.131 sec/batch)
2017-05-07 21:31:04.629043: step 36130, loss = 0.68 (995.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:31:05.922369: step 36140, loss = 0.80 (989.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:31:07.186727: step 36150, loss = 0.81 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:31:08.464327: step 36160, loss = 0.94 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:31:09.754281: step 36170, loss = 0.78 (992.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:31:11.047513: step 36180, loss = 0.90 (989.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:31:12.310665: step 36190, loss = 0.85 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:31:13.692091: step 36200, loss = 0.88 (926.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:31:14.854521: step 36210, loss = 0.80 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-07 21:31:16.156689: step 36220, loss = 0.77 (983.0 examples/sec; 0.130 sec/batch)
2017-05-07 21:31:17.416144: step 36230, loss = 0.74 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:31:18.691195: step 36240, loss = 0.70 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:31:19.971233: step 36250, loss = 0.78 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:31:21.281483: step 36260, loss = 0.94 (976.9 examples/sec; 0.131 sec/batch)
2017-05-07 21:31:22.564634: step 36270, loss = 0.80 (997.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:31:23.860847: step 36280, loss = 0.89 (987.5 examples/sec; 0.130 sec/batch)
2017-05-07 21:31:25.159421: step 36290, loss = 0.69 (985.7 examples/sec; 0.130 sec/batch)
2017-05-07 21:31:26.552168: step 36300, loss = 0.78 (919.0 examples/sec; 0.139 sec/batch)
2017-05-07 21:31:27.727289: step 36310, loss = 0.74 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-07 21:31:29.000475: step 36320, loss = 0.80 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:31:30.255396: step 36330, loss = 0.86 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:31:31.527913: step 36340, loss = 0.90 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:31:32.779551: step 36350, loss = 0.85 (1022.7 examples/sec; 0.125 sec/batch)
2017-05-07 21:31:34.032443: step 36360, loss = 0.86 (1021.6 examples/sec; 0.125 sec/batch)
2017-05-07 21:31:35.315315: step 36370, loss = 0.59 (997.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:31:36.585675: step 36380, loss = 0.68 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:31:37.832917: step 36390, loss = 0.76 (1026.3 examples/sec; 0.125 sec/batch)
2017-05-07 21:31:39.169500: step 36400, loss = 0.75 (957.7 examples/sec; 0.134 sec/batch)
2017-05-07 21:31:40.342042: step 36410, loss = 0.90 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-07 21:31:41.593691: step 36420, loss = 0.93 (1022.7 examples/sec; 0.125 sec/batch)
2017-05-07 21:31:42.881920: step 36430, loss = 0.97 (993.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:31:44.167472: step 36440, loss = 0.78 (995.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:31:45.419876: step 36450, loss = 0.81 (1022.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:31:46.675475: step 36460, loss = 0.91 (1019.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:31:47.951655: step 36470, loss = 0.71 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:31:49.206622: step 36480, loss = 0.75 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:31:50.473568: step 36490, loss = 0.92 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:31:51.826911: step 36500, loss = 0.75 (945.8 examples/sec; 0.135 sec/batch)
2017-05-07 21:31:53.020043: step 36510, loss = 0.87 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-07 21:31:54.275040: step 36520, loss = 0.97 (1019.9 examples/sec; 0.125 sec/batch)
2017-05-07 21:31:55.547204: step 36530, loss = 0.69 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:31:56.825563: step 36540, loss = 0.72 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:31:58.090950: step 36550, loss = 0.97 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:31:59.352516: step 36560, loss = 0.71 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:32:00.609510: step 36570, loss = 0.92 (1018.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:32:01.876516: step 36580, loss = 0.84 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:32:03.137535: step 36590, loss = 1.03 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:32:04.505310: step 36600, loss = 0.83 (935.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:32:05.699877: step 36610, loss = 0.79 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-07 21:32:06.965957: step 36620, loss = 0.86 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:32:08.266394: step 36630, loss = 0.94 (984.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:32:09.525580: step 36640, loss = 0.95 (1016.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:32:10.793738: step 36650, loss = 0.78 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:32:12.065319: step 36660, loss = 0.75 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:32:13.327344: step 36670, loss = 0.81 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:32:14.581039: step 36680, loss = 0.83 (1021.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:32:15.848883: step 36690, loss = 0.89 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:32:17.226149: step 36700, loss = 0.93 (929.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:32:18.414941: step 36710, loss = 0.79 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-07 21:32:19.707272: step 36720, loss = 0.73 (990.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:32:20.986173: step 36730, loss = 0.98 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:32:22.237435: step 36740, loss = 0.89 (1023.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:32:23.507231: step 36750, loss = 0.75 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:32:24.783744: step 36760, loss = 0.67 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:32:26.070421: step 36770, loss = 1.07 (994.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:32:27.384152: step 36780, loss = 0.80 (974.3 examples/sec; 0.131 sec/batch)
2017-05-07 21:32:28.659061: step 36790, loss = 0.64 (1004.0 examples/sec; 0E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 750 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
.127 sec/batch)
2017-05-07 21:32:30.006078: step 36800, loss = 0.84 (950.2 examples/sec; 0.135 sec/batch)
2017-05-07 21:32:31.196731: step 36810, loss = 0.62 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-07 21:32:32.461413: step 36820, loss = 0.73 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:32:33.715399: step 36830, loss = 0.80 (1020.8 examples/sec; 0.125 sec/batch)
2017-05-07 21:32:34.966134: step 36840, loss = 0.89 (1023.4 examples/sec; 0.125 sec/batch)
2017-05-07 21:32:36.213539: step 36850, loss = 0.81 (1026.1 examples/sec; 0.125 sec/batch)
2017-05-07 21:32:37.490609: step 36860, loss = 0.79 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:32:38.790688: step 36870, loss = 0.76 (984.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:32:40.052125: step 36880, loss = 0.90 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:32:41.333082: step 36890, loss = 0.88 (999.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:32:42.714418: step 36900, loss = 0.83 (926.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:32:43.879268: step 36910, loss = 0.73 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-07 21:32:45.132742: step 36920, loss = 0.77 (1021.2 examples/sec; 0.125 sec/batch)
2017-05-07 21:32:46.384923: step 36930, loss = 0.74 (1022.2 examples/sec; 0.125 sec/batch)
2017-05-07 21:32:47.645356: step 36940, loss = 0.92 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:32:48.927287: step 36950, loss = 0.74 (998.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:32:50.212931: step 36960, loss = 0.81 (995.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:32:51.465020: step 36970, loss = 0.66 (1022.3 examples/sec; 0.125 sec/batch)
2017-05-07 21:32:52.761625: step 36980, loss = 0.76 (987.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:32:54.044511: step 36990, loss = 0.72 (997.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:32:55.412321: step 37000, loss = 1.01 (935.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:32:56.618100: step 37010, loss = 0.87 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-07 21:32:57.847057: step 37020, loss = 0.79 (1041.5 examples/sec; 0.123 sec/batch)
2017-05-07 21:32:59.111959: step 37030, loss = 0.69 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:33:00.387562: step 37040, loss = 0.70 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:33:01.633706: step 37050, loss = 0.90 (1027.2 examples/sec; 0.125 sec/batch)
2017-05-07 21:33:02.897123: step 37060, loss = 0.80 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:33:04.182391: step 37070, loss = 0.85 (995.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:33:05.449353: step 37080, loss = 0.81 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:33:06.691765: step 37090, loss = 0.73 (1030.3 examples/sec; 0.124 sec/batch)
2017-05-07 21:33:08.056008: step 37100, loss = 0.89 (938.3 examples/sec; 0.136 sec/batch)
2017-05-07 21:33:09.238717: step 37110, loss = 0.81 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-07 21:33:10.493546: step 37120, loss = 0.88 (1020.1 examples/sec; 0.125 sec/batch)
2017-05-07 21:33:11.760022: step 37130, loss = 0.85 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:33:13.040907: step 37140, loss = 0.98 (999.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:33:14.299173: step 37150, loss = 0.86 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:33:15.608499: step 37160, loss = 0.84 (977.6 examples/sec; 0.131 sec/batch)
2017-05-07 21:33:16.910208: step 37170, loss = 0.77 (983.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:33:18.174807: step 37180, loss = 0.80 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:33:19.436896: step 37190, loss = 1.05 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:33:20.782199: step 37200, loss = 0.90 (951.5 examples/sec; 0.135 sec/batch)
2017-05-07 21:33:22.002648: step 37210, loss = 0.77 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-07 21:33:23.265519: step 37220, loss = 0.79 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:33:24.541759: step 37230, loss = 0.81 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:33:25.837918: step 37240, loss = 0.93 (987.5 examples/sec; 0.130 sec/batch)
2017-05-07 21:33:27.141532: step 37250, loss = 0.99 (981.9 examples/sec; 0.130 sec/batch)
2017-05-07 21:33:28.435351: step 37260, loss = 1.01 (989.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:33:29.683925: step 37270, loss = 0.81 (1025.2 examples/sec; 0.125 sec/batch)
2017-05-07 21:33:30.966718: step 37280, loss = 0.71 (997.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:33:32.262472: step 37290, loss = 0.77 (987.8 examples/sec; 0.130 sec/batch)
2017-05-07 21:33:33.636442: step 37300, loss = 0.80 (931.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:33:34.822876: step 37310, loss = 0.79 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-07 21:33:36.093093: step 37320, loss = 0.70 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:33:37.361284: step 37330, loss = 0.73 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:33:38.621266: step 37340, loss = 0.75 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:33:39.896694: step 37350, loss = 0.76 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:33:41.177166: step 37360, loss = 0.97 (999.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:33:42.427560: step 37370, loss = 0.75 (1023.7 examples/sec; 0.125 sec/batch)
2017-05-07 21:33:43.708143: step 37380, loss = 0.99 (999.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:33:44.993756: step 37390, loss = 0.72 (995.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:33:46.347107: step 37400, loss = 0.74 (945.8 examples/sec; 0.135 sec/batch)
2017-05-07 21:33:47.526287: step 37410, loss = 0.96 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-07 21:33:48.788564: step 37420, loss = 0.92 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:33:50.090299: step 37430, loss = 0.73 (983.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:33:51.389452: step 37440, loss = 0.94 (985.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:33:52.634925: step 37450, loss = 0.79 (1027.7 examples/sec; 0.125 sec/batch)
2017-05-07 21:33:53.901310: step 37460, loss = 0.79 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:33:55.182157: step 37470, loss = 0.80 (999.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:33:56.491842: step 37480, loss = 0.77 (977.3 examples/sec; 0.131 sec/batch)
2017-05-07 21:33:57.785384: step 37490, loss = 0.72 (989.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:33:59.143179: step 37500, loss = 0.92 (942.7 examples/sec; 0.136 sec/batch)
2017-05-07 21:34:00.366610: step 37510, loss = 0.85 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-07 21:34:01.629196: step 37520, loss = 0.89 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:34:02.904131: step 37530, loss = 0.89 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:34:04.174382: step 37540, loss = 0.97 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:34:05.470545: step 37550, loss = 0.75 (987.5 examples/sec; 0.130 sec/batch)
2017-05-07 21:34:06.729021: step 37560, loss = 0.76 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:34:07.999201: step 37570, loss = 0.85 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:34:09.278045: step 37580, loss = 0.83 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:34:10.568787: step 37590, loss = 0.75 (991.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:34:11.922256: step 37600, loss = 0.84 (945.7 examples/sec; 0.135 sec/batch)
2017-05-07 21:34:13.123469: step 37610, loss = 0.79 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-07 21:34:14.393409: step 37620, loss = 0.74 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:34:15.682348: step 37630, loss = 0.77 (993.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:34:16.936208: step 37640, loss = 0.69 (1020.8 examples/sec; 0.125 sec/batch)
2017-05-07 21:34:18.175307: step 37650, loss = 0.76 (1033.0 examples/sec; 0.124 sec/batch)
2017-05-07 21:34:19.469731: step 37660, loss = 0.81 (988.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:34:20.749338: step 37670, loss = 0.74 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:34:22.017629: step 37680, loss = 0.82 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:34:23.277038: step 37690, loss = 0.78 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:34:24.66E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 771 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
7529: step 37700, loss = 0.78 (920.5 examples/sec; 0.139 sec/batch)
2017-05-07 21:34:25.849886: step 37710, loss = 0.88 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-07 21:34:27.097088: step 37720, loss = 0.75 (1026.3 examples/sec; 0.125 sec/batch)
2017-05-07 21:34:28.373359: step 37730, loss = 0.78 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:34:29.657032: step 37740, loss = 0.68 (997.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:34:30.915278: step 37750, loss = 0.81 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:34:32.205146: step 37760, loss = 0.82 (992.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:34:33.454649: step 37770, loss = 0.76 (1024.4 examples/sec; 0.125 sec/batch)
2017-05-07 21:34:34.729221: step 37780, loss = 0.82 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:34:36.015725: step 37790, loss = 0.79 (994.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:34:37.412721: step 37800, loss = 0.80 (916.3 examples/sec; 0.140 sec/batch)
2017-05-07 21:34:38.600172: step 37810, loss = 0.93 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-07 21:34:39.891592: step 37820, loss = 0.79 (991.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:34:41.172661: step 37830, loss = 0.89 (999.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:34:42.450277: step 37840, loss = 0.91 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:34:43.712347: step 37850, loss = 0.85 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:34:44.994385: step 37860, loss = 0.88 (998.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:34:46.278055: step 37870, loss = 0.99 (997.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:34:47.562957: step 37880, loss = 0.62 (996.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:34:48.858314: step 37890, loss = 0.89 (988.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:34:50.228637: step 37900, loss = 0.86 (934.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:34:51.437459: step 37910, loss = 0.76 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-07 21:34:52.742166: step 37920, loss = 0.78 (981.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:34:54.036532: step 37930, loss = 1.05 (988.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:34:55.301144: step 37940, loss = 0.91 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:34:56.568444: step 37950, loss = 0.86 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:34:57.867533: step 37960, loss = 0.96 (985.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:34:59.131451: step 37970, loss = 0.81 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:35:00.406900: step 37980, loss = 0.77 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:35:01.652016: step 37990, loss = 0.78 (1028.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:35:03.010216: step 38000, loss = 0.70 (942.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:35:04.207561: step 38010, loss = 1.08 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-07 21:35:05.477995: step 38020, loss = 0.83 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:35:06.770293: step 38030, loss = 0.79 (990.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:35:08.066838: step 38040, loss = 0.79 (987.2 examples/sec; 0.130 sec/batch)
2017-05-07 21:35:09.329439: step 38050, loss = 0.88 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:35:10.597627: step 38060, loss = 0.73 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:35:11.872991: step 38070, loss = 0.82 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:35:13.140184: step 38080, loss = 0.80 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:35:14.408458: step 38090, loss = 0.73 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:35:15.774493: step 38100, loss = 0.75 (937.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:35:16.982800: step 38110, loss = 0.90 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-07 21:35:18.258019: step 38120, loss = 0.73 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:35:19.526410: step 38130, loss = 0.79 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:35:20.804771: step 38140, loss = 0.89 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:35:22.089038: step 38150, loss = 0.85 (996.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:35:23.401660: step 38160, loss = 0.88 (975.1 examples/sec; 0.131 sec/batch)
2017-05-07 21:35:24.664403: step 38170, loss = 0.81 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:35:25.939531: step 38180, loss = 0.82 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:35:27.208713: step 38190, loss = 0.84 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:35:28.568961: step 38200, loss = 0.86 (941.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:35:29.737110: step 38210, loss = 0.77 (1095.8 examples/sec; 0.117 sec/batch)
2017-05-07 21:35:31.008392: step 38220, loss = 0.85 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:35:32.275780: step 38230, loss = 0.70 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:35:33.560716: step 38240, loss = 0.84 (996.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:35:34.867188: step 38250, loss = 0.86 (979.7 examples/sec; 0.131 sec/batch)
2017-05-07 21:35:36.174238: step 38260, loss = 0.81 (979.3 examples/sec; 0.131 sec/batch)
2017-05-07 21:35:37.457993: step 38270, loss = 0.86 (997.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:35:38.731733: step 38280, loss = 0.76 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:35:40.004736: step 38290, loss = 0.80 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:35:41.369247: step 38300, loss = 0.69 (938.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:35:42.494638: step 38310, loss = 0.78 (1137.4 examples/sec; 0.113 sec/batch)
2017-05-07 21:35:43.800572: step 38320, loss = 1.00 (980.1 examples/sec; 0.131 sec/batch)
2017-05-07 21:35:45.094692: step 38330, loss = 0.87 (989.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:35:46.377588: step 38340, loss = 0.89 (997.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:35:47.677480: step 38350, loss = 0.83 (984.7 examples/sec; 0.130 sec/batch)
2017-05-07 21:35:48.975250: step 38360, loss = 0.97 (986.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:35:50.246785: step 38370, loss = 0.76 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:35:51.513332: step 38380, loss = 0.76 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:35:52.771366: step 38390, loss = 0.83 (1017.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:35:54.129863: step 38400, loss = 0.75 (942.2 examples/sec; 0.136 sec/batch)
2017-05-07 21:35:55.312470: step 38410, loss = 0.82 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-07 21:35:56.589253: step 38420, loss = 0.72 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:35:57.869378: step 38430, loss = 1.34 (999.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:35:59.158934: step 38440, loss = 0.76 (992.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:36:00.423846: step 38450, loss = 0.90 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:36:01.701539: step 38460, loss = 0.86 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:36:02.976054: step 38470, loss = 0.79 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:36:04.264104: step 38480, loss = 0.88 (993.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:36:05.536150: step 38490, loss = 0.89 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:36:06.908352: step 38500, loss = 0.54 (932.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:36:08.107765: step 38510, loss = 0.83 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-07 21:36:09.382362: step 38520, loss = 0.77 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:36:10.663739: step 38530, loss = 0.87 (998.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:36:11.968081: step 38540, loss = 0.85 (981.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:36:13.278938: step 38550, loss = 0.84 (976.5 examples/sec; 0.131 sec/batch)
2017-05-07 21:36:14.542933: step 38560, loss = 0.88 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:36:15.851269: step 38570, loss = 0.86 (978.3 examples/sec; 0.131 sec/batch)
2017-05-07 21:36:17.112029: step 38580, loss = 0.67 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:36:18.372015: step 38590, loss = 0.85 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:36:19.762295: step 38600, loss = 0.93 (920.7 examplE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 791 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
es/sec; 0.139 sec/batch)
2017-05-07 21:36:20.930307: step 38610, loss = 0.76 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-07 21:36:22.191038: step 38620, loss = 0.87 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:36:23.467742: step 38630, loss = 0.67 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:36:24.761546: step 38640, loss = 0.86 (989.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:36:26.026728: step 38650, loss = 0.87 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:36:27.311011: step 38660, loss = 0.88 (996.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:36:28.617898: step 38670, loss = 0.61 (979.4 examples/sec; 0.131 sec/batch)
2017-05-07 21:36:29.895271: step 38680, loss = 0.84 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:36:31.170633: step 38690, loss = 0.90 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:36:32.524534: step 38700, loss = 0.70 (945.4 examples/sec; 0.135 sec/batch)
2017-05-07 21:36:33.703473: step 38710, loss = 0.85 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-07 21:36:34.984681: step 38720, loss = 0.77 (999.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:36:36.249527: step 38730, loss = 0.82 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:36:37.514501: step 38740, loss = 0.85 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:36:38.784005: step 38750, loss = 0.84 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:36:40.066105: step 38760, loss = 0.73 (998.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:36:41.326237: step 38770, loss = 0.85 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:36:42.599591: step 38780, loss = 0.86 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:36:43.859270: step 38790, loss = 0.72 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:36:45.229411: step 38800, loss = 0.89 (934.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:36:46.388344: step 38810, loss = 0.79 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-07 21:36:47.647372: step 38820, loss = 0.83 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:36:48.930646: step 38830, loss = 0.77 (997.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:36:50.184905: step 38840, loss = 0.84 (1020.5 examples/sec; 0.125 sec/batch)
2017-05-07 21:36:51.449136: step 38850, loss = 0.76 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:36:52.703382: step 38860, loss = 0.78 (1020.5 examples/sec; 0.125 sec/batch)
2017-05-07 21:36:53.957606: step 38870, loss = 0.65 (1020.9 examples/sec; 0.125 sec/batch)
2017-05-07 21:36:55.230593: step 38880, loss = 0.78 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:36:56.498356: step 38890, loss = 0.92 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:36:57.851311: step 38900, loss = 0.80 (946.1 examples/sec; 0.135 sec/batch)
2017-05-07 21:36:59.026443: step 38910, loss = 0.87 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-07 21:37:00.312055: step 38920, loss = 0.89 (995.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:37:01.601591: step 38930, loss = 0.74 (992.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:37:02.865530: step 38940, loss = 0.91 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:37:04.139790: step 38950, loss = 0.82 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:37:05.423147: step 38960, loss = 0.89 (997.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:37:06.689430: step 38970, loss = 0.66 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:37:07.960308: step 38980, loss = 0.82 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:37:09.227265: step 38990, loss = 0.87 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:37:10.586761: step 39000, loss = 0.84 (941.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:37:11.768122: step 39010, loss = 1.17 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-07 21:37:13.064650: step 39020, loss = 0.75 (987.2 examples/sec; 0.130 sec/batch)
2017-05-07 21:37:14.317860: step 39030, loss = 0.76 (1021.4 examples/sec; 0.125 sec/batch)
2017-05-07 21:37:15.605579: step 39040, loss = 0.85 (994.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:37:16.891133: step 39050, loss = 0.82 (995.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:37:18.151289: step 39060, loss = 0.87 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:37:19.422066: step 39070, loss = 0.89 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:37:20.701765: step 39080, loss = 0.99 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:37:21.949918: step 39090, loss = 0.84 (1025.5 examples/sec; 0.125 sec/batch)
2017-05-07 21:37:23.320931: step 39100, loss = 0.86 (933.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:37:24.519010: step 39110, loss = 0.65 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-07 21:37:25.784704: step 39120, loss = 0.83 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:37:27.078208: step 39130, loss = 0.81 (989.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:37:28.381410: step 39140, loss = 0.99 (982.2 examples/sec; 0.130 sec/batch)
2017-05-07 21:37:29.673152: step 39150, loss = 0.78 (990.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:37:30.975515: step 39160, loss = 0.80 (982.8 examples/sec; 0.130 sec/batch)
2017-05-07 21:37:32.264172: step 39170, loss = 0.93 (993.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:37:33.541882: step 39180, loss = 0.82 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:37:34.832644: step 39190, loss = 0.81 (991.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:37:36.216908: step 39200, loss = 0.90 (924.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:37:37.386466: step 39210, loss = 0.77 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-07 21:37:38.670372: step 39220, loss = 0.76 (996.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:37:39.949022: step 39230, loss = 0.82 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:37:41.203889: step 39240, loss = 0.67 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:37:42.446294: step 39250, loss = 0.69 (1030.3 examples/sec; 0.124 sec/batch)
2017-05-07 21:37:43.719248: step 39260, loss = 0.73 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:37:44.975552: step 39270, loss = 0.86 (1018.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:37:46.279595: step 39280, loss = 0.86 (981.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:37:47.554636: step 39290, loss = 0.88 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:37:48.912082: step 39300, loss = 0.89 (942.9 examples/sec; 0.136 sec/batch)
2017-05-07 21:37:50.083306: step 39310, loss = 0.89 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-07 21:37:51.349515: step 39320, loss = 0.89 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:37:52.610094: step 39330, loss = 0.83 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:37:53.867626: step 39340, loss = 0.67 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:37:55.154309: step 39350, loss = 0.86 (994.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:37:56.410078: step 39360, loss = 0.72 (1019.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:37:57.706912: step 39370, loss = 0.78 (987.0 examples/sec; 0.130 sec/batch)
2017-05-07 21:37:59.007426: step 39380, loss = 0.88 (984.2 examples/sec; 0.130 sec/batch)
2017-05-07 21:38:00.284156: step 39390, loss = 0.75 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:38:01.635828: step 39400, loss = 0.84 (947.0 examples/sec; 0.135 sec/batch)
2017-05-07 21:38:02.804314: step 39410, loss = 0.82 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-07 21:38:04.084051: step 39420, loss = 0.73 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:38:05.338832: step 39430, loss = 0.75 (1020.1 examples/sec; 0.125 sec/batch)
2017-05-07 21:38:06.602146: step 39440, loss = 0.88 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:38:07.881072: step 39450, loss = 0.89 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:38:09.159467: step 39460, loss = 0.81 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:38:10.428226: step 39470, loss = 0.76 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:38:11.698159: step 39480, loss = 0.84 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:38:12.953342: step 39490, loss = 0.77 (1019.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:38:14.307308: step 39500, loss = 1.04 (945.4 examples/sec; 0.135 sec/batch)
2017-05-0E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 811 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
7 21:38:15.487108: step 39510, loss = 0.71 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-07 21:38:16.751810: step 39520, loss = 0.67 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:38:18.032235: step 39530, loss = 0.74 (999.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:38:19.297138: step 39540, loss = 0.73 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:38:20.619290: step 39550, loss = 0.77 (968.1 examples/sec; 0.132 sec/batch)
2017-05-07 21:38:21.894300: step 39560, loss = 0.69 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:38:23.192891: step 39570, loss = 0.79 (985.7 examples/sec; 0.130 sec/batch)
2017-05-07 21:38:24.457061: step 39580, loss = 0.82 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:38:25.741570: step 39590, loss = 0.79 (996.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:38:27.083208: step 39600, loss = 0.74 (954.1 examples/sec; 0.134 sec/batch)
2017-05-07 21:38:28.273798: step 39610, loss = 0.79 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-07 21:38:29.555620: step 39620, loss = 0.91 (998.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:38:30.826812: step 39630, loss = 0.84 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:38:32.093762: step 39640, loss = 0.77 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:38:33.381546: step 39650, loss = 0.85 (994.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:38:34.642537: step 39660, loss = 0.78 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:38:35.941508: step 39670, loss = 0.91 (985.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:38:37.191708: step 39680, loss = 0.90 (1023.8 examples/sec; 0.125 sec/batch)
2017-05-07 21:38:38.445217: step 39690, loss = 0.76 (1021.1 examples/sec; 0.125 sec/batch)
2017-05-07 21:38:39.790175: step 39700, loss = 0.92 (951.7 examples/sec; 0.134 sec/batch)
2017-05-07 21:38:40.966288: step 39710, loss = 1.08 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-07 21:38:42.229255: step 39720, loss = 0.93 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:38:43.501466: step 39730, loss = 0.92 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:38:44.801673: step 39740, loss = 0.72 (984.5 examples/sec; 0.130 sec/batch)
2017-05-07 21:38:46.086615: step 39750, loss = 0.79 (996.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:38:47.362205: step 39760, loss = 0.92 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:38:48.602857: step 39770, loss = 0.70 (1031.7 examples/sec; 0.124 sec/batch)
2017-05-07 21:38:49.880585: step 39780, loss = 0.80 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:38:51.190731: step 39790, loss = 0.85 (977.0 examples/sec; 0.131 sec/batch)
2017-05-07 21:38:52.556563: step 39800, loss = 0.75 (937.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:38:53.749395: step 39810, loss = 0.96 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-07 21:38:55.016687: step 39820, loss = 0.78 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:38:56.314526: step 39830, loss = 0.72 (986.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:38:57.597406: step 39840, loss = 0.76 (997.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:38:58.874282: step 39850, loss = 0.67 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:39:00.136495: step 39860, loss = 0.83 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:39:01.427469: step 39870, loss = 0.75 (991.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:39:02.688597: step 39880, loss = 0.85 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:39:03.961463: step 39890, loss = 0.72 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:39:05.326480: step 39900, loss = 0.79 (937.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:39:06.515797: step 39910, loss = 0.71 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-07 21:39:07.763113: step 39920, loss = 0.80 (1026.2 examples/sec; 0.125 sec/batch)
2017-05-07 21:39:09.063290: step 39930, loss = 0.89 (984.5 examples/sec; 0.130 sec/batch)
2017-05-07 21:39:10.342104: step 39940, loss = 1.07 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:39:11.641946: step 39950, loss = 0.80 (984.7 examples/sec; 0.130 sec/batch)
2017-05-07 21:39:12.932614: step 39960, loss = 0.77 (991.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:39:14.211288: step 39970, loss = 0.75 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:39:15.505313: step 39980, loss = 0.84 (989.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:39:16.788718: step 39990, loss = 0.87 (997.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:39:18.152238: step 40000, loss = 0.98 (938.7 examples/sec; 0.136 sec/batch)
2017-05-07 21:39:19.366794: step 40010, loss = 0.89 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-07 21:39:20.669012: step 40020, loss = 0.80 (982.9 examples/sec; 0.130 sec/batch)
2017-05-07 21:39:21.935184: step 40030, loss = 0.78 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:39:23.192223: step 40040, loss = 0.83 (1018.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:39:24.453492: step 40050, loss = 0.81 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:39:25.724415: step 40060, loss = 0.84 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:39:27.000321: step 40070, loss = 0.80 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:39:28.259815: step 40080, loss = 0.85 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:39:29.516610: step 40090, loss = 0.75 (1018.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:39:30.880155: step 40100, loss = 0.86 (938.7 examples/sec; 0.136 sec/batch)
2017-05-07 21:39:32.075156: step 40110, loss = 0.89 (1071.1 examples/sec; 0.119 sec/batch)
2017-05-07 21:39:33.348344: step 40120, loss = 0.80 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:39:34.644317: step 40130, loss = 0.98 (987.7 examples/sec; 0.130 sec/batch)
2017-05-07 21:39:35.933543: step 40140, loss = 0.92 (992.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:39:37.225482: step 40150, loss = 0.78 (990.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:39:38.493381: step 40160, loss = 0.82 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:39:39.766863: step 40170, loss = 0.56 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:39:41.021356: step 40180, loss = 1.03 (1020.3 examples/sec; 0.125 sec/batch)
2017-05-07 21:39:42.278178: step 40190, loss = 0.84 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:39:43.643641: step 40200, loss = 0.94 (937.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:39:44.840699: step 40210, loss = 0.71 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-07 21:39:46.124736: step 40220, loss = 0.73 (996.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:39:47.421375: step 40230, loss = 0.91 (987.2 examples/sec; 0.130 sec/batch)
2017-05-07 21:39:48.700021: step 40240, loss = 0.73 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:39:49.973237: step 40250, loss = 0.71 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:39:51.234736: step 40260, loss = 0.92 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:39:52.518285: step 40270, loss = 0.81 (997.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:39:53.764571: step 40280, loss = 0.81 (1027.1 examples/sec; 0.125 sec/batch)
2017-05-07 21:39:55.038772: step 40290, loss = 0.78 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:39:56.428344: step 40300, loss = 0.67 (921.1 examples/sec; 0.139 sec/batch)
2017-05-07 21:39:57.624462: step 40310, loss = 0.70 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-07 21:39:58.905729: step 40320, loss = 0.92 (999.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:40:00.191583: step 40330, loss = 0.94 (995.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:40:01.493840: step 40340, loss = 0.97 (982.9 examples/sec; 0.130 sec/batch)
2017-05-07 21:40:02.783494: step 40350, loss = 0.90 (992.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:40:04.077571: step 40360, loss = 0.89 (989.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:40:05.359455: step 40370, loss = 1.00 (998.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:40:06.624588: step 40380, loss = 0.97 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:40:07.911363: step 40390, loss = 0.87 (994.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:40:09.294514: step 40400, loss = 0.80 (925.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:40:10.484431: step 40410, loss = 0.79 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-07 21:40:11.771556: step 40420, loss = 0.79 (994.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:40:13.038510: step 40430, loss = 0.81 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:40:14.281188: step 40440, loss = 0.63 (1030.0 examples/sec; 0.124 sec/batch)
2017-05-07 21:40:15.530098: step 40450, loss = 0.73 (1024.9 examples/sec; 0.125 sec/batch)
2017-05-07 21:40:16.806742: step 40460, loss = 0.72 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:40:18.063680: step 40470, loss = 0.75 (1018.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:40:19.314713: step 40480, loss = 0.88 (1023.1 examples/sec; 0.125 sec/batch)
2017-05-07 21:40:20.579516: step 40490, loss = 0.70 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:40:21.955059: step 40500, loss = 0.87 (930.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:40:23.157368: step 40510, loss = 0.69 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-07 21:40:24.436320: step 40520, loss = 0.73 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:40:25.735072: step 40530, loss = 0.99 (985.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:40:27.002031: step 40540, loss = 0.92 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:40:28.290338: step 40550, loss = 0.84 (993.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:40:29.553322: step 40560, loss = 0.83 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:40:30.849638: step 40570, loss = 0.92 (987.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:40:32.110323: step 40580, loss = 0.81 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:40:33.401961: step 40590, loss = 0.64 (991.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:40:34.787166: step 40600, loss = 0.61 (924.1 examples/sec; 0.139 sec/batch)
2017-05-07 21:40:35.995743: step 40610, loss = 0.84 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-07 21:40:37.289603: step 40620, loss = 0.93 (989.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:40:38.558991: step 40630, loss = 0.92 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:40:39.825316: step 40640, loss = 0.89 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:40:41.096899: step 40650, loss = 1.02 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:40:42.375151: step 40660, loss = 0.82 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:40:43.684309: step 40670, loss = 0.72 (977.7 examples/sec; 0.131 sec/batch)
2017-05-07 21:40:44.980840: step 40680, loss = 0.77 (987.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:40:46.257214: step 40690, loss = 0.65 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:40:47.623186: step 40700, loss = 0.81 (937.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:40:48.811266: step 40710, loss = 0.81 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-07 21:40:50.082958: step 40720, loss = 0.69 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:40:51.340149: step 40730, loss = 0.83 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:40:52.588162: step 40740, loss = 0.86 (1025.6 examples/sec; 0.125 sec/batch)
2017-05-07 21:40:53.851942: step 40750, loss = 0.81 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:40:55.123959: step 40760, loss = 0.66 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:40:56.391164: step 40770, loss = 0.68 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:40:57.634533: step 40780, loss = 0.77 (1029.5 examples/sec; 0.124 sec/batch)
2017-05-07 21:40:58.912863: step 40790, loss = 1.01 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:41:00.280536: step 40800, loss = 1.00 (935.9 examples/sec; 0.137 sec/batch)
2017-05-07 21:41:01.482593: step 40810, loss = 0.69 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-07 21:41:02.773882: step 40820, loss = 0.82 (991.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:41:04.064259: step 40830, loss = 0.82 (992.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:41:05.357554: step 40840, loss = 0.86 (989.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:41:06.637199: step 40850, loss = 0.77 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:41:07.925360: step 40860, loss = 0.85 (993.7 examples/E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 831 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
sec; 0.129 sec/batch)
2017-05-07 21:41:09.198403: step 40870, loss = 0.83 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:41:10.467727: step 40880, loss = 0.67 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:41:11.751601: step 40890, loss = 0.85 (997.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:41:13.125931: step 40900, loss = 0.93 (931.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:41:14.284274: step 40910, loss = 0.87 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-07 21:41:15.540695: step 40920, loss = 0.66 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:41:16.801394: step 40930, loss = 0.91 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:41:18.055060: step 40940, loss = 0.98 (1021.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:41:19.339003: step 40950, loss = 1.04 (996.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:41:20.655706: step 40960, loss = 0.85 (972.1 examples/sec; 0.132 sec/batch)
2017-05-07 21:41:21.924072: step 40970, loss = 0.78 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:41:23.187524: step 40980, loss = 0.92 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:41:24.466717: step 40990, loss = 0.97 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:41:25.841627: step 41000, loss = 0.80 (931.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:41:27.027917: step 41010, loss = 0.78 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-07 21:41:28.295381: step 41020, loss = 0.71 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:41:29.523005: step 41030, loss = 0.78 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-07 21:41:30.790575: step 41040, loss = 0.89 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:41:32.100931: step 41050, loss = 0.86 (976.8 examples/sec; 0.131 sec/batch)
2017-05-07 21:41:33.378614: step 41060, loss = 0.90 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:41:34.657848: step 41070, loss = 0.92 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:41:35.963099: step 41080, loss = 0.70 (980.6 examples/sec; 0.131 sec/batch)
2017-05-07 21:41:37.256801: step 41090, loss = 0.81 (989.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:41:38.621051: step 41100, loss = 0.83 (938.2 examples/sec; 0.136 sec/batch)
2017-05-07 21:41:39.830442: step 41110, loss = 0.79 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-07 21:41:41.094429: step 41120, loss = 0.65 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:41:42.380899: step 41130, loss = 0.88 (995.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:41:43.673244: step 41140, loss = 0.83 (990.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:41:44.976158: step 41150, loss = 0.71 (982.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:41:46.246457: step 41160, loss = 0.78 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:41:47.510904: step 41170, loss = 0.78 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:41:48.772941: step 41180, loss = 0.85 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:41:50.069376: step 41190, loss = 0.90 (987.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:41:51.448670: step 41200, loss = 0.77 (928.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:41:52.653787: step 41210, loss = 1.11 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-07 21:41:53.929609: step 41220, loss = 1.00 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:41:55.207281: step 41230, loss = 0.77 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:41:56.518366: step 41240, loss = 0.81 (976.3 examples/sec; 0.131 sec/batch)
2017-05-07 21:41:57.775876: step 41250, loss = 0.84 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:41:59.073417: step 41260, loss = 0.72 (986.5 examples/sec; 0.130 sec/batch)
2017-05-07 21:42:00.355616: step 41270, loss = 0.84 (998.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:42:01.654802: step 41280, loss = 0.75 (985.2 examples/sec; 0.130 sec/batch)
2017-05-07 21:42:02.943698: step 41290, loss = 0.92 (993.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:42:04.340105: step 41300, loss = 0.70 (916.6 examples/sec; 0.140 sec/batch)
2017-05-07 21:42:05.498361: step 41310, loss = 0.89 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-07 21:42:06.758922: step 41320, loss = 0.85 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:42:08.051500: step 41330, loss = 0.86 (990.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:42:09.328176: step 41340, loss = 0.68 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:42:10.592849: step 41350, loss = 0.70 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:42:11.862921: step 41360, loss = 0.76 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:42:13.136246: step 41370, loss = 0.68 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:42:14.407197: step 41380, loss = 0.91 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:42:15.677117: step 41390, loss = 0.71 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:42:17.043401: step 41400, loss = 1.01 (936.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:42:18.203856: step 41410, loss = 0.75 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-07 21:42:19.491912: step 41420, loss = 0.79 (993.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:42:20.784880: step 41430, loss = 0.90 (990.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:42:22.047133: step 41440, loss = 0.75 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:42:23.315316: step 41450, loss = 1.02 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:42:24.584958: step 41460, loss = 0.90 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:42:25.848638: step 41470, loss = 0.85 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:42:27.158099: step 41480, loss = 0.99 (977.5 examples/sec; 0.131 sec/batch)
2017-05-07 21:42:28.442583: step 41490, loss = 0.80 (996.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:42:29.807617: step 41500, loss = 0.94 (937.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:42:31.000654: step 41510, loss = 0.91 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-07 21:42:32.302663: step 41520, loss = 0.81 (983.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:42:33.586938: step 41530, loss = 0.76 (996.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:42:34.855579: step 41540, loss = 0.91 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:42:36.160805: step 41550, loss = 0.99 (980.7 examples/sec; 0.131 sec/batch)
2017-05-07 21:42:37.434185: step 41560, loss = 0.75 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:42:38.723289: step 41570, loss = 0.77 (992.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:42:39.997144: step 41580, loss = 0.83 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:42:41.283606: step 41590, loss = 0.80 (995.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:42:42.619638: step 41600, loss = 0.80 (958.1 examples/sec; 0.134 sec/batch)
2017-05-07 21:42:43.798158: step 41610, loss = 1.15 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-07 21:42:45.081762: step 41620, loss = 0.91 (997.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:42:46.355235: step 41630, loss = 0.84 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:42:47.639078: step 41640, loss = 0.99 (997.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:42:48.921234: step 41650, loss = 0.81 (998.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:42:50.197238: step 41660, loss = 0.80 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:42:51.453329: step 41670, loss = 0.72 (1019.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:42:52.705705: step 41680, loss = 0.69 (1022.1 examples/sec; 0.125 sec/batch)
2017-05-07 21:42:53.981206: step 41690, loss = 0.82 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:42:55.387160: step 41700, loss = 0.78 (910.4 examples/sec; 0.141 sec/batch)
2017-05-07 21:42:56.586674: step 41710, loss = 0.84 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-07 21:42:57.891162: step 41720, loss = 0.79 (981.2 examples/sec; 0.130 sec/batch)
2017-05-07 21:42:59.174290: step 41730, loss = 0.72 (997.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:43:00.455239: step 41740, loss = 0.68 (999.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:43:01.729327: step 41750, loss = 0.75 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:43:02.999357: step 41760, loss = 0.74 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:43:04.E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 852 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
287839: step 41770, loss = 0.71 (993.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:43:05.549616: step 41780, loss = 0.71 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:43:06.832042: step 41790, loss = 0.82 (998.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:43:08.182394: step 41800, loss = 0.81 (947.9 examples/sec; 0.135 sec/batch)
2017-05-07 21:43:09.348159: step 41810, loss = 0.72 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-07 21:43:10.658549: step 41820, loss = 0.67 (976.8 examples/sec; 0.131 sec/batch)
2017-05-07 21:43:11.956688: step 41830, loss = 0.70 (986.0 examples/sec; 0.130 sec/batch)
2017-05-07 21:43:13.241587: step 41840, loss = 0.80 (996.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:43:14.510192: step 41850, loss = 0.83 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:43:15.810639: step 41860, loss = 0.81 (984.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:43:17.109173: step 41870, loss = 0.79 (985.7 examples/sec; 0.130 sec/batch)
2017-05-07 21:43:18.399222: step 41880, loss = 0.69 (992.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:43:19.674464: step 41890, loss = 0.83 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:43:21.052934: step 41900, loss = 0.93 (928.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:43:22.230254: step 41910, loss = 1.08 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-07 21:43:23.506735: step 41920, loss = 0.96 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:43:24.756958: step 41930, loss = 0.86 (1023.8 examples/sec; 0.125 sec/batch)
2017-05-07 21:43:26.019826: step 41940, loss = 0.73 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:43:27.287468: step 41950, loss = 0.77 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:43:28.554362: step 41960, loss = 0.80 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:43:29.821904: step 41970, loss = 0.78 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:43:31.091122: step 41980, loss = 0.68 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:43:32.365445: step 41990, loss = 0.80 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:43:33.754673: step 42000, loss = 0.80 (921.4 examples/sec; 0.139 sec/batch)
2017-05-07 21:43:34.945227: step 42010, loss = 0.95 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-07 21:43:36.258735: step 42020, loss = 0.78 (974.5 examples/sec; 0.131 sec/batch)
2017-05-07 21:43:37.540555: step 42030, loss = 0.84 (998.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:43:38.799041: step 42040, loss = 0.73 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:43:40.094758: step 42050, loss = 0.66 (987.9 examples/sec; 0.130 sec/batch)
2017-05-07 21:43:41.383938: step 42060, loss = 0.73 (992.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:43:42.638692: step 42070, loss = 0.84 (1020.1 examples/sec; 0.125 sec/batch)
2017-05-07 21:43:43.904375: step 42080, loss = 0.84 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:43:45.156286: step 42090, loss = 0.88 (1022.4 examples/sec; 0.125 sec/batch)
2017-05-07 21:43:46.527561: step 42100, loss = 0.78 (933.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:43:47.694145: step 42110, loss = 0.73 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-07 21:43:48.952741: step 42120, loss = 0.85 (1017.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:43:50.249192: step 42130, loss = 0.92 (987.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:43:51.540185: step 42140, loss = 0.78 (991.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:43:52.823418: step 42150, loss = 0.79 (997.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:43:54.135958: step 42160, loss = 0.98 (975.2 examples/sec; 0.131 sec/batch)
2017-05-07 21:43:55.420494: step 42170, loss = 0.70 (996.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:43:56.717302: step 42180, loss = 0.97 (987.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:43:57.999741: step 42190, loss = 0.88 (998.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:43:59.398266: step 42200, loss = 0.75 (915.2 examples/sec; 0.140 sec/batch)
2017-05-07 21:44:00.590067: step 42210, loss = 0.83 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-07 21:44:01.865297: step 42220, loss = 0.81 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:44:03.131792: step 42230, loss = 0.86 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:44:04.395596: step 42240, loss = 0.80 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:44:05.664358: step 42250, loss = 0.79 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:44:06.952184: step 42260, loss = 0.78 (993.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:44:08.247771: step 42270, loss = 0.84 (988.0 examples/sec; 0.130 sec/batch)
2017-05-07 21:44:09.548792: step 42280, loss = 0.89 (983.8 examples/sec; 0.130 sec/batch)
2017-05-07 21:44:10.827102: step 42290, loss = 0.63 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:44:12.195057: step 42300, loss = 0.91 (935.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:44:13.376702: step 42310, loss = 0.75 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-07 21:44:14.649565: step 42320, loss = 0.80 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:44:15.957491: step 42330, loss = 1.10 (978.7 examples/sec; 0.131 sec/batch)
2017-05-07 21:44:17.245476: step 42340, loss = 0.79 (993.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:44:18.511483: step 42350, loss = 0.85 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:44:19.794253: step 42360, loss = 0.80 (997.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:44:21.064889: step 42370, loss = 0.87 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:44:22.353497: step 42380, loss = 0.85 (993.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:44:23.622043: step 42390, loss = 0.77 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:44:24.992684: step 42400, loss = 0.95 (933.9 examples/sec; 0.137 sec/batch)
2017-05-07 21:44:26.143205: step 42410, loss = 0.80 (1112.5 examples/sec; 0.115 sec/batch)
2017-05-07 21:44:27.416145: step 42420, loss = 0.77 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:44:28.660521: step 42430, loss = 0.79 (1028.6 examples/sec; 0.124 sec/batch)
2017-05-07 21:44:29.911406: step 42440, loss = 0.81 (1023.3 examples/sec; 0.125 sec/batch)
2017-05-07 21:44:31.191185: step 42450, loss = 0.80 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:44:32.440022: step 42460, loss = 1.05 (1025.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:44:33.711440: step 42470, loss = 0.92 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:44:34.981044: step 42480, loss = 0.80 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:44:36.253362: step 42490, loss = 0.84 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:44:37.629995: step 42500, loss = 0.81 (929.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:44:38.816755: step 42510, loss = 0.71 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-07 21:44:40.072786: step 42520, loss = 0.88 (1019.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:44:41.341620: step 42530, loss = 0.77 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:44:42.610595: step 42540, loss = 0.88 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:44:43.857924: step 42550, loss = 0.93 (1026.2 examples/sec; 0.125 sec/batch)
2017-05-07 21:44:45.116868: step 42560, loss = 0.80 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:44:46.393370: step 42570, loss = 0.80 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:44:47.688445: step 42580, loss = 0.74 (988.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:44:48.990252: step 42590, loss = 0.83 (983.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:44:50.371178: step 42600, loss = 0.73 (926.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:44:51.558337: step 42610, loss = 0.87 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-07 21:44:52.819489: step 42620, loss = 0.96 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:44:54.071582: step 42630, loss = 0.89 (1022.3 examples/sec; 0.125 sec/batch)
2017-05-07 21:44:55.357183: step 42640, loss = 0.82 (995.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:44:56.653429: step 42650, loss = 0.83 (987.5 examples/sec; 0.130 sec/batch)
2017-05-07 21:44:57.948527: step 42660, loss = 0.71 (988.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:44:59.237574: step 42670, loss = 0.69 (993.0 exaE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 872 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
mples/sec; 0.129 sec/batch)
2017-05-07 21:45:00.507168: step 42680, loss = 0.93 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:45:01.814354: step 42690, loss = 0.91 (979.2 examples/sec; 0.131 sec/batch)
2017-05-07 21:45:03.180792: step 42700, loss = 0.76 (936.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:45:04.376904: step 42710, loss = 0.69 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-07 21:45:05.670832: step 42720, loss = 0.89 (989.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:45:06.944881: step 42730, loss = 0.78 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:45:08.220750: step 42740, loss = 0.98 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:45:09.481826: step 42750, loss = 0.93 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:45:10.739296: step 42760, loss = 0.78 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:45:12.040369: step 42770, loss = 0.90 (983.8 examples/sec; 0.130 sec/batch)
2017-05-07 21:45:13.333679: step 42780, loss = 0.66 (989.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:45:14.578132: step 42790, loss = 0.81 (1028.6 examples/sec; 0.124 sec/batch)
2017-05-07 21:45:15.930054: step 42800, loss = 0.76 (946.8 examples/sec; 0.135 sec/batch)
2017-05-07 21:45:17.129617: step 42810, loss = 0.80 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-07 21:45:18.436315: step 42820, loss = 0.94 (979.6 examples/sec; 0.131 sec/batch)
2017-05-07 21:45:19.729149: step 42830, loss = 0.92 (990.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:45:20.984843: step 42840, loss = 0.81 (1019.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:45:22.237903: step 42850, loss = 0.78 (1021.5 examples/sec; 0.125 sec/batch)
2017-05-07 21:45:23.498454: step 42860, loss = 0.99 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:45:24.777424: step 42870, loss = 0.80 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:45:26.062250: step 42880, loss = 1.06 (996.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:45:27.348284: step 42890, loss = 0.87 (995.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:45:28.723040: step 42900, loss = 0.81 (931.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:45:29.911558: step 42910, loss = 0.98 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-07 21:45:31.178921: step 42920, loss = 0.71 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:45:32.466933: step 42930, loss = 0.77 (993.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:45:33.734694: step 42940, loss = 0.91 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:45:35.044515: step 42950, loss = 0.78 (977.2 examples/sec; 0.131 sec/batch)
2017-05-07 21:45:36.333711: step 42960, loss = 0.97 (992.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:45:37.615359: step 42970, loss = 0.86 (998.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:45:38.888938: step 42980, loss = 0.89 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:45:40.166274: step 42990, loss = 0.86 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:45:41.545534: step 43000, loss = 1.21 (928.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:45:42.725187: step 43010, loss = 0.70 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-07 21:45:43.977270: step 43020, loss = 0.71 (1022.3 examples/sec; 0.125 sec/batch)
2017-05-07 21:45:45.234515: step 43030, loss = 0.78 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:45:46.507837: step 43040, loss = 0.84 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:45:47.783393: step 43050, loss = 0.81 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:45:49.039386: step 43060, loss = 0.86 (1019.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:45:50.301507: step 43070, loss = 0.84 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:45:51.572573: step 43080, loss = 0.84 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:45:52.851868: step 43090, loss = 0.71 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:45:54.202183: step 43100, loss = 0.71 (947.9 examples/sec; 0.135 sec/batch)
2017-05-07 21:45:55.362307: step 43110, loss = 0.83 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-07 21:45:56.676528: step 43120, loss = 0.83 (973.9 examples/sec; 0.131 sec/batch)
2017-05-07 21:45:57.939553: step 43130, loss = 1.16 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:45:59.231739: step 43140, loss = 0.85 (990.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:46:00.523260: step 43150, loss = 0.88 (991.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:46:01.788134: step 43160, loss = 0.93 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:46:03.064164: step 43170, loss = 0.78 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:46:04.368474: step 43180, loss = 0.94 (981.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:46:05.660909: step 43190, loss = 0.93 (990.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:46:07.001608: step 43200, loss = 0.76 (954.7 examples/sec; 0.134 sec/batch)
2017-05-07 21:46:08.172910: step 43210, loss = 0.76 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-07 21:46:09.451392: step 43220, loss = 0.69 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:46:10.717298: step 43230, loss = 0.94 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:46:12.021118: step 43240, loss = 0.84 (981.7 examples/sec; 0.130 sec/batch)
2017-05-07 21:46:13.298261: step 43250, loss = 0.67 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:46:14.551426: step 43260, loss = 0.79 (1021.4 examples/sec; 0.125 sec/batch)
2017-05-07 21:46:15.831123: step 43270, loss = 0.83 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:46:17.104674: step 43280, loss = 0.84 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:46:18.390461: step 43290, loss = 0.92 (995.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:46:19.741327: step 43300, loss = 0.76 (947.5 examples/sec; 0.135 sec/batch)
2017-05-07 21:46:20.906318: step 43310, loss = 0.73 (1098.7 examples/sec; 0.116 sec/batch)
2017-05-07 21:46:22.175957: step 43320, loss = 0.78 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:46:23.463667: step 43330, loss = 0.92 (994.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:46:24.762770: step 43340, loss = 0.86 (985.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:46:26.045379: step 43350, loss = 0.92 (998.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:46:27.342489: step 43360, loss = 1.00 (986.8 examples/sec; 0.130 sec/batch)
2017-05-07 21:46:28.612086: step 43370, loss = 0.77 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:46:29.911550: step 43380, loss = 0.79 (985.0 examples/sec; 0.130 sec/batch)
2017-05-07 21:46:31.213916: step 43390, loss = 0.75 (982.8 examples/sec; 0.130 sec/batch)
2017-05-07 21:46:32.599694: step 43400, loss = 0.82 (923.7 examples/sec; 0.139 sec/batch)
2017-05-07 21:46:33.758379: step 43410, loss = 0.83 (1104.7 examples/sec; 0.116 sec/batch)
2017-05-07 21:46:35.029502: step 43420, loss = 0.87 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:46:36.313635: step 43430, loss = 0.71 (996.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:46:37.604866: step 43440, loss = 0.75 (991.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:46:38.867058: step 43450, loss = 0.90 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:46:40.143691: step 43460, loss = 0.76 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:46:41.396925: step 43470, loss = 0.77 (1021.4 examples/sec; 0.125 sec/batch)
2017-05-07 21:46:42.687825: step 43480, loss = 0.99 (991.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:46:43.981201: step 43490, loss = 0.82 (989.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:46:45.364064: step 43500, loss = 0.85 (925.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:46:46.545428: step 43510, loss = 0.90 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-07 21:46:47.829493: step 43520, loss = 0.88 (996.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:46:49.092454: step 43530, loss = 0.75 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:46:50.387960: step 43540, loss = 0.74 (988.0 examples/sec; 0.130 sec/batch)
2017-05-07 21:46:51.652882: step 43550, loss = 0.61 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:46:52.919271: step 43560, loss = 0.82 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:46:54.186087: step 43570, loss = 0.79 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 892 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
46:55.481773: step 43580, loss = 0.75 (987.9 examples/sec; 0.130 sec/batch)
2017-05-07 21:46:56.759885: step 43590, loss = 0.84 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:46:58.133948: step 43600, loss = 0.86 (931.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:46:59.310260: step 43610, loss = 0.68 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-07 21:47:00.571124: step 43620, loss = 0.88 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:47:01.877568: step 43630, loss = 0.75 (979.8 examples/sec; 0.131 sec/batch)
2017-05-07 21:47:03.170553: step 43640, loss = 0.62 (990.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:47:04.461866: step 43650, loss = 0.75 (991.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:47:05.746152: step 43660, loss = 0.92 (996.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:47:06.997808: step 43670, loss = 0.86 (1022.6 examples/sec; 0.125 sec/batch)
2017-05-07 21:47:08.289804: step 43680, loss = 0.83 (990.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:47:09.595736: step 43690, loss = 0.71 (980.1 examples/sec; 0.131 sec/batch)
2017-05-07 21:47:10.952547: step 43700, loss = 0.82 (943.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:47:12.131767: step 43710, loss = 0.86 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-07 21:47:13.405907: step 43720, loss = 1.15 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:47:14.695514: step 43730, loss = 0.91 (992.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:47:16.002701: step 43740, loss = 0.67 (979.2 examples/sec; 0.131 sec/batch)
2017-05-07 21:47:17.257381: step 43750, loss = 0.95 (1020.2 examples/sec; 0.125 sec/batch)
2017-05-07 21:47:18.552077: step 43760, loss = 0.87 (988.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:47:19.842403: step 43770, loss = 0.84 (992.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:47:21.116060: step 43780, loss = 0.80 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:47:22.384506: step 43790, loss = 0.69 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:47:23.774536: step 43800, loss = 0.88 (920.8 examples/sec; 0.139 sec/batch)
2017-05-07 21:47:24.941869: step 43810, loss = 0.82 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-07 21:47:26.219735: step 43820, loss = 0.75 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:47:27.488421: step 43830, loss = 1.00 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:47:28.805427: step 43840, loss = 0.74 (971.9 examples/sec; 0.132 sec/batch)
2017-05-07 21:47:30.069794: step 43850, loss = 0.80 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:47:31.320423: step 43860, loss = 0.85 (1023.5 examples/sec; 0.125 sec/batch)
2017-05-07 21:47:32.584359: step 43870, loss = 0.74 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:47:33.866395: step 43880, loss = 0.88 (998.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:47:35.166409: step 43890, loss = 0.70 (984.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:47:36.544237: step 43900, loss = 0.98 (929.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:47:37.740676: step 43910, loss = 0.73 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-07 21:47:39.018176: step 43920, loss = 0.89 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:47:40.291125: step 43930, loss = 0.66 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:47:41.542509: step 43940, loss = 1.00 (1022.9 examples/sec; 0.125 sec/batch)
2017-05-07 21:47:42.822143: step 43950, loss = 0.64 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:47:44.084520: step 43960, loss = 0.75 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:47:45.373763: step 43970, loss = 0.80 (992.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:47:46.691929: step 43980, loss = 0.82 (971.0 examples/sec; 0.132 sec/batch)
2017-05-07 21:47:47.951364: step 43990, loss = 0.78 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:47:49.318787: step 44000, loss = 0.92 (936.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:47:50.514856: step 44010, loss = 0.84 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-07 21:47:51.799015: step 44020, loss = 0.99 (996.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:47:53.081696: step 44030, loss = 0.71 (997.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:47:54.375797: step 44040, loss = 0.82 (989.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:47:55.645527: step 44050, loss = 0.83 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:47:56.924236: step 44060, loss = 0.79 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:47:58.216176: step 44070, loss = 0.81 (990.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:47:59.479351: step 44080, loss = 0.78 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:48:00.753143: step 44090, loss = 0.84 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:48:02.106471: step 44100, loss = 0.96 (945.8 examples/sec; 0.135 sec/batch)
2017-05-07 21:48:03.298742: step 44110, loss = 0.88 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-07 21:48:04.565799: step 44120, loss = 0.91 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:48:05.833432: step 44130, loss = 0.87 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:48:07.151179: step 44140, loss = 0.78 (971.4 examples/sec; 0.132 sec/batch)
2017-05-07 21:48:08.424204: step 44150, loss = 0.91 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:48:09.695101: step 44160, loss = 1.01 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:48:10.979999: step 44170, loss = 0.73 (996.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:48:12.258600: step 44180, loss = 0.78 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:48:13.530621: step 44190, loss = 0.89 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:48:14.906121: step 44200, loss = 0.76 (930.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:48:16.071292: step 44210, loss = 0.78 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-07 21:48:17.354914: step 44220, loss = 0.89 (997.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:48:18.679686: step 44230, loss = 0.90 (966.2 examples/sec; 0.132 sec/batch)
2017-05-07 21:48:19.946195: step 44240, loss = 0.65 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:48:21.234813: step 44250, loss = 0.69 (993.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:48:22.536900: step 44260, loss = 0.79 (983.0 examples/sec; 0.130 sec/batch)
2017-05-07 21:48:23.853813: step 44270, loss = 0.99 (972.0 examples/sec; 0.132 sec/batch)
2017-05-07 21:48:25.149439: step 44280, loss = 0.64 (987.9 examples/sec; 0.130 sec/batch)
2017-05-07 21:48:26.416655: step 44290, loss = 0.71 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:48:27.775871: step 44300, loss = 0.91 (941.7 examples/sec; 0.136 sec/batch)
2017-05-07 21:48:28.947092: step 44310, loss = 0.64 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-07 21:48:30.225280: step 44320, loss = 0.73 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:48:31.546066: step 44330, loss = 0.99 (969.1 examples/sec; 0.132 sec/batch)
2017-05-07 21:48:32.830505: step 44340, loss = 0.87 (996.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:48:34.097029: step 44350, loss = 0.77 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:48:35.352934: step 44360, loss = 0.82 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:48:36.630036: step 44370, loss = 0.93 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:48:37.890811: step 44380, loss = 0.71 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:48:39.165236: step 44390, loss = 0.72 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:48:40.534283: step 44400, loss = 0.70 (935.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:48:41.705846: step 44410, loss = 0.78 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-07 21:48:42.952138: step 44420, loss = 0.67 (1027.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:48:44.238103: step 44430, loss = 0.76 (995.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:48:45.520028: step 44440, loss = 0.58 (998.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:48:46.820290: step 44450, loss = 0.81 (984.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:48:48.089029: step 44460, loss = 0.74 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:48:49.402539: step 44470, loss = 0.88 (974.5 examples/sec; 0.131 sec/batch)
2017-05-07 21:48:50.681719: step 44480, loss = 0.66 (1000.6E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 912 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
 examples/sec; 0.128 sec/batch)
2017-05-07 21:48:51.957992: step 44490, loss = 0.77 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:48:53.338710: step 44500, loss = 0.79 (927.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:48:54.524761: step 44510, loss = 0.95 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-07 21:48:55.837309: step 44520, loss = 0.69 (975.2 examples/sec; 0.131 sec/batch)
2017-05-07 21:48:57.107951: step 44530, loss = 0.79 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:48:58.392949: step 44540, loss = 0.89 (996.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:48:59.678568: step 44550, loss = 0.84 (995.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:49:00.934477: step 44560, loss = 0.67 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:49:02.191860: step 44570, loss = 0.88 (1018.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:49:03.466434: step 44580, loss = 0.83 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:49:04.754450: step 44590, loss = 0.80 (993.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:49:06.134909: step 44600, loss = 1.01 (927.2 examples/sec; 0.138 sec/batch)
2017-05-07 21:49:07.301541: step 44610, loss = 0.90 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-07 21:49:08.589647: step 44620, loss = 0.71 (993.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:49:09.873324: step 44630, loss = 0.80 (997.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:49:11.159315: step 44640, loss = 0.80 (995.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:49:12.416193: step 44650, loss = 0.81 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:49:13.679174: step 44660, loss = 0.74 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:49:14.945702: step 44670, loss = 0.82 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:49:16.245466: step 44680, loss = 0.76 (984.8 examples/sec; 0.130 sec/batch)
2017-05-07 21:49:17.526310: step 44690, loss = 0.67 (999.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:49:18.949065: step 44700, loss = 0.86 (899.7 examples/sec; 0.142 sec/batch)
2017-05-07 21:49:20.143946: step 44710, loss = 0.89 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-07 21:49:21.410767: step 44720, loss = 0.73 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:49:22.672161: step 44730, loss = 0.74 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:49:23.979338: step 44740, loss = 0.85 (979.2 examples/sec; 0.131 sec/batch)
2017-05-07 21:49:25.266598: step 44750, loss = 1.00 (994.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:49:26.529525: step 44760, loss = 0.80 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:49:27.789045: step 44770, loss = 0.73 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:49:29.094846: step 44780, loss = 0.77 (980.2 examples/sec; 0.131 sec/batch)
2017-05-07 21:49:30.354149: step 44790, loss = 0.78 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:49:31.738876: step 44800, loss = 0.63 (924.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:49:32.921897: step 44810, loss = 0.89 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-07 21:49:34.171497: step 44820, loss = 0.69 (1024.3 examples/sec; 0.125 sec/batch)
2017-05-07 21:49:35.433102: step 44830, loss = 1.11 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:49:36.712265: step 44840, loss = 0.64 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:49:37.975419: step 44850, loss = 0.81 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:49:39.239482: step 44860, loss = 0.81 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:49:40.519961: step 44870, loss = 0.91 (999.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:49:41.784451: step 44880, loss = 0.79 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:49:43.059572: step 44890, loss = 0.78 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:49:44.429723: step 44900, loss = 0.87 (934.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:49:45.604124: step 44910, loss = 0.88 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-07 21:49:46.905514: step 44920, loss = 0.78 (983.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:49:48.197250: step 44930, loss = 0.77 (990.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:49:49.491747: step 44940, loss = 0.65 (988.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:49:50.779664: step 44950, loss = 0.86 (993.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:49:52.067287: step 44960, loss = 0.79 (994.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:49:53.363373: step 44970, loss = 0.84 (987.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:49:54.634186: step 44980, loss = 0.73 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:49:55.908720: step 44990, loss = 0.81 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:49:57.290420: step 45000, loss = 0.84 (926.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:49:58.471823: step 45010, loss = 0.81 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-07 21:49:59.767664: step 45020, loss = 0.81 (987.8 examples/sec; 0.130 sec/batch)
2017-05-07 21:50:01.043006: step 45030, loss = 0.94 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:50:02.326839: step 45040, loss = 0.77 (997.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:50:03.605710: step 45050, loss = 0.72 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:50:04.897727: step 45060, loss = 0.77 (990.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:50:06.186184: step 45070, loss = 0.78 (993.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:50:07.447719: step 45080, loss = 0.87 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:50:08.720395: step 45090, loss = 0.78 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:50:10.090550: step 45100, loss = 0.82 (934.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:50:11.274900: step 45110, loss = 0.65 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-07 21:50:12.533323: step 45120, loss = 0.81 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:50:13.804511: step 45130, loss = 0.78 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:50:15.062923: step 45140, loss = 0.90 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:50:16.343900: step 45150, loss = 0.77 (999.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:50:17.610579: step 45160, loss = 0.91 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:50:18.904837: step 45170, loss = 0.78 (989.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:50:20.195534: step 45180, loss = 0.89 (991.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:50:21.454289: step 45190, loss = 0.79 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:50:22.824649: step 45200, loss = 0.95 (934.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:50:24.016070: step 45210, loss = 0.77 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-07 21:50:25.309307: step 45220, loss = 0.80 (989.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:50:26.598612: step 45230, loss = 0.88 (992.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:50:27.882529: step 45240, loss = 0.90 (996.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:50:29.165327: step 45250, loss = 0.79 (997.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:50:30.416709: step 45260, loss = 0.62 (1022.9 examples/sec; 0.125 sec/batch)
2017-05-07 21:50:31.679347: step 45270, loss = 0.66 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:50:32.960817: step 45280, loss = 0.72 (998.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:50:34.246910: step 45290, loss = 0.84 (995.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:50:35.635539: step 45300, loss = 0.66 (921.8 examples/sec; 0.139 sec/batch)
2017-05-07 21:50:36.850185: step 45310, loss = 0.86 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-07 21:50:38.142771: step 45320, loss = 0.91 (990.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:50:39.435897: step 45330, loss = 0.79 (989.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:50:40.697330: step 45340, loss = 0.65 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:50:41.956156: step 45350, loss = 1.06 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:50:43.261180: step 45360, loss = 0.86 (980.8 examples/sec; 0.131 sec/batch)
2017-05-07 21:50:44.548083: step 45370, loss = 0.78 (994.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:50:45.791762: step 45380, loss = 0.98 (1029.2 examples/sec; 0.124 sec/batch)
2017-05-07 21:50:47.063936: step 45390, loss = 0.98 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:50:48.427093: step 45400, loss = 1.04 (939.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:50:49.624331: step 45410, loss = 0.62 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-07 21:50:50.897225: step 45420, loss = 0.86 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:50:52.183920: step 45430, loss = 0.65 (994.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:50:53.472746: step 45440, loss = 0.79 (993.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:50:54.773521: step 45450, loss = 0.66 (984.0 examples/sec; 0.130 sec/batch)
2017-05-07 21:50:56.044446: step 45460, loss = 0.81 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:50:57.316264: step 45470, loss = 0.81 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:50:58.572185: step 45480, loss = 0.78 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:50:59.825720: step 45490, loss = 0.87 (1021.1 examples/sec; 0.125 sec/batch)
2017-05-07 21:51:01.210708: step 45500, loss = 0.90 (924.2 examples/sec; 0.138 sec/batch)
2017-05-07 21:51:02.392080: step 45510, loss = 0.77 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-07 21:51:03.702352: step 45520, loss = 0.75 (976.9 examples/sec; 0.131 sec/batch)
2017-05-07 21:51:04.988445: step 45530, loss = 1.01 (995.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:51:06.240761: step 45540, loss = 0.76 (1022.1 examples/sec; 0.125 sec/batch)
2017-05-07 21:51:07.546767: step 45550, loss = 0.86 (980.1 examples/sec; 0.131 sec/batch)
2017-05-07 21:51:08.818877: step 45560, loss = 0.99 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:51:10.130901: step 45570, loss = 0.87 (975.6 examples/sec; 0.131 sec/batch)
2017-05-07 21:51:11.423455: step 45580, loss = 1.01 (990.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:51:12.710639: step 45590, loss = 0.90 (994.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:51:14.073633: step 45600, loss = 0.83 (939.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:51:15.263028: step 45610, loss = 0.77 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-07 21:51:16.536336: step 45620, loss = 1.02 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:51:17.822335: step 45630, loss = 0.88 (995.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:51:19.128389: step 45640, loss = 0.76 (980.0 examples/sec; 0.131 sec/batch)
2017-05-07 21:51:20.421012: step 45650, loss = 0.82 (990.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:51:21.712633: step 45660, loss = 0.83 (991.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:51:23.016434: step 45670, loss = 0.67 (981.7 examples/sec; 0.130 sec/batch)
2017-05-07 21:51:24.281900: step 45680, loss = 0.77 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:51:25.548872: step 45690, loss = 0.70 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:51:26.906192: step 45700, loss = 0.85 (943.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:51:28.079117: step 45710, loss = 0.72 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-07 21:51:29.345228: step 45720, loss = 0.83 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:51:30.603143: step 45730, loss = 0.80 (1017.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:51:31.879839: step 45740, loss = 0.85 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:51:33.150225: step 45750, loss = 0.77 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:51:34.400363: step 45760, loss = 0.80 (1023.9 examples/sec; 0.125 sec/batch)
2017-05-07 21:51:35.688211: step 45770, loss = 0.97 (993.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:51:36.949211: step 45780, loss = 0.71 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:51:38.223073: step 45790, loss = 0.75 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:51:39.593405: step 45800, loss = 0.84 (934.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:51:40.770115: step 45810, loss = 0.81 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-07 21:51:42.037260: step 45820, loss = 0.72 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:51:43.289359: step 45830, loss = 0.75 (1022.3 examples/sec; 0.125 sec/batch)
2017-05-07 21:51:44.555875: step 458E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 932 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
40, loss = 0.83 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:51:45.818666: step 45850, loss = 0.73 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:51:47.098640: step 45860, loss = 0.85 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:51:48.356728: step 45870, loss = 0.70 (1017.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:51:49.618617: step 45880, loss = 0.72 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:51:50.896014: step 45890, loss = 1.09 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:51:52.258483: step 45900, loss = 0.76 (939.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:51:53.459401: step 45910, loss = 1.05 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-07 21:51:54.765407: step 45920, loss = 0.91 (980.1 examples/sec; 0.131 sec/batch)
2017-05-07 21:51:56.045816: step 45930, loss = 0.85 (999.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:51:57.330270: step 45940, loss = 0.80 (996.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:51:58.611341: step 45950, loss = 0.67 (999.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:51:59.890484: step 45960, loss = 0.77 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:52:01.147352: step 45970, loss = 0.82 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:52:02.397738: step 45980, loss = 0.71 (1023.7 examples/sec; 0.125 sec/batch)
2017-05-07 21:52:03.668721: step 45990, loss = 0.74 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:52:05.028320: step 46000, loss = 0.75 (941.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:52:06.195130: step 46010, loss = 0.75 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-07 21:52:07.478832: step 46020, loss = 0.78 (997.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:52:08.801309: step 46030, loss = 0.72 (967.9 examples/sec; 0.132 sec/batch)
2017-05-07 21:52:10.082550: step 46040, loss = 0.90 (999.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:52:11.374248: step 46050, loss = 0.73 (990.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:52:12.667866: step 46060, loss = 0.73 (989.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:52:13.941988: step 46070, loss = 0.73 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:52:15.245942: step 46080, loss = 0.72 (981.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:52:16.521646: step 46090, loss = 0.64 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:52:17.930884: step 46100, loss = 0.65 (908.3 examples/sec; 0.141 sec/batch)
2017-05-07 21:52:19.092810: step 46110, loss = 0.80 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-07 21:52:20.398957: step 46120, loss = 0.72 (980.0 examples/sec; 0.131 sec/batch)
2017-05-07 21:52:21.640472: step 46130, loss = 0.71 (1031.0 examples/sec; 0.124 sec/batch)
2017-05-07 21:52:22.889405: step 46140, loss = 0.98 (1024.9 examples/sec; 0.125 sec/batch)
2017-05-07 21:52:24.168372: step 46150, loss = 0.81 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:52:25.463720: step 46160, loss = 0.93 (988.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:52:26.738309: step 46170, loss = 0.82 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:52:28.029611: step 46180, loss = 0.78 (991.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:52:29.295280: step 46190, loss = 0.72 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:52:30.688630: step 46200, loss = 0.94 (918.7 examples/sec; 0.139 sec/batch)
2017-05-07 21:52:31.892752: step 46210, loss = 0.72 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-07 21:52:33.177045: step 46220, loss = 0.76 (996.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:52:34.455923: step 46230, loss = 1.06 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:52:35.771921: step 46240, loss = 0.84 (972.6 examples/sec; 0.132 sec/batch)
2017-05-07 21:52:37.067296: step 46250, loss = 0.95 (988.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:52:38.389735: step 46260, loss = 0.92 (967.9 examples/sec; 0.132 sec/batch)
2017-05-07 21:52:39.647748: step 46270, loss = 0.85 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:52:40.934004: step 46280, loss = 0.88 (995.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:52:42.188386: step 46290, loss = 0.98 (1020.4 examples/sec; 0.125 sec/batch)
2017-05-07 21:52:43.563111: step 46300, loss = 0.74 (931.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:52:44.724161: step 46310, loss = 0.90 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-07 21:52:45.991280: step 46320, loss = 0.73 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:52:47.267193: step 46330, loss = 0.86 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:52:48.501537: step 46340, loss = 0.67 (1037.0 examples/sec; 0.123 sec/batch)
2017-05-07 21:52:49.759020: step 46350, loss = 0.69 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:52:51.056596: step 46360, loss = 0.87 (986.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:52:52.344867: step 46370, loss = 0.89 (993.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:52:53.609911: step 46380, loss = 0.88 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:52:54.870743: step 46390, loss = 0.66 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:52:56.232530: step 46400, loss = 0.72 (939.9 examples/sec; 0.136 sec/batch)
2017-05-07 21:52:57.417888: step 46410, loss = 0.67 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-07 21:52:58.680038: step 46420, loss = 0.72 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:52:59.953776: step 46430, loss = 0.75 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:53:01.264366: step 46440, loss = 0.76 (976.7 examples/sec; 0.131 sec/batch)
2017-05-07 21:53:02.514148: step 46450, loss = 0.78 (1024.2 examples/sec; 0.125 sec/batch)
2017-05-07 21:53:03.913832: step 46460, loss = 0.90 (914.5 examples/sec; 0.140 sec/batch)
2017-05-07 21:53:05.615908: step 46470, loss = 0.99 (752.0 examples/sec; 0.170 sec/batch)
2017-05-07 21:53:07.567290: step 46480, loss = 0.69 (655.9 examples/sec; 0.195 sec/batch)
2017-05-07 21:53:09.459341: step 46490, loss = 0.69 (676.5 examples/sec; 0.189 sec/batch)
2017-05-07 21:53:11.469762: step 46500, loss = 0.77 (636.7 examples/sec; 0.201 sec/batch)
2017-05-07 21:53:13.312070: step 46510, loss = 0.83 (694.8 examples/sec; 0.184 sec/batch)
2017-05-07 21:53:15.242822: step 46520, loss = 0.75 (663.0 examples/sec; 0.193 sec/batch)
2017-05-07 21:53:16.694621: step 46530, loss = 0.97 (881.7 examples/sec; 0.145 sec/batch)
2017-05-07 21:53:17.967396: step 46540, loss = 0.72 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:53:19.260905: step 46550, loss = 0.68 (989.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:53:20.559708: step 46560, loss = 0.92 (985.5 examples/sec; 0.130 sec/batch)
2017-05-07 21:53:21.853070: step 46570, loss = 1.09 (989.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:53:23.117911: step 46580, loss = 0.75 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:53:24.388961: step 46590, loss = 0.85 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:53:25.738487: step 46600, loss = 0.81 (948.5 examples/sec; 0.135 sec/batch)
2017-05-07 21:53:26.917878: step 46610, loss = 0.66 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-07 21:53:28.219441: step 46620, loss = 0.94 (983.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:53:29.507015: step 46630, loss = 0.74 (994.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:53:30.751914: step 46640, loss = 0.81 (1028.2 examples/sec; 0.124 sec/batch)
2017-05-07 21:53:32.034624: step 46650, loss = 0.85 (997.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:53:33.321420: step 46660, loss = 0.87 (994.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:53:34.585247: step 46670, loss = 0.92 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:53:35.880222: step 46680, loss = 0.87 (988.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:53:37.166810: step 46690, loss = 0.93 (994.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:53:38.552533: step 46700, loss = 0.70 (923.7 examples/sec; 0.139 sec/batch)
2017-05-07 21:53:39.712881: step 46710, loss = 0.79 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-07 21:53:41.013640: step 46720, loss = 0.74 (984.0 examples/sec; 0.130 sec/batch)
2017-05-07 21:53:42.280636: step 46730, loss = 0.89 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:53:43.549616: step 46740, loss = 0.62 (1008.7 examples/sec; 0.127 sec/bE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 953 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
atch)
2017-05-07 21:53:44.817423: step 46750, loss = 0.73 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:53:46.080430: step 46760, loss = 0.87 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:53:47.366330: step 46770, loss = 0.70 (995.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:53:48.655220: step 46780, loss = 0.73 (993.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:53:49.956578: step 46790, loss = 0.94 (983.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:53:51.327283: step 46800, loss = 0.70 (933.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:53:52.500558: step 46810, loss = 0.82 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-07 21:53:53.796597: step 46820, loss = 0.93 (987.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:53:55.071284: step 46830, loss = 0.64 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:53:56.376044: step 46840, loss = 1.00 (981.0 examples/sec; 0.130 sec/batch)
2017-05-07 21:53:57.669023: step 46850, loss = 0.82 (990.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:53:58.962287: step 46860, loss = 0.77 (989.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:54:00.240678: step 46870, loss = 0.70 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:54:01.505307: step 46880, loss = 0.73 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:54:02.778735: step 46890, loss = 0.71 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:54:04.142608: step 46900, loss = 0.83 (938.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:54:05.322097: step 46910, loss = 0.71 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-07 21:54:06.581559: step 46920, loss = 0.73 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:54:07.877162: step 46930, loss = 0.85 (987.9 examples/sec; 0.130 sec/batch)
2017-05-07 21:54:09.152737: step 46940, loss = 0.92 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:54:10.411054: step 46950, loss = 0.87 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:54:11.699264: step 46960, loss = 0.83 (993.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:54:12.983245: step 46970, loss = 0.80 (996.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:54:14.268194: step 46980, loss = 1.06 (996.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:54:15.555435: step 46990, loss = 0.84 (994.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:54:16.939447: step 47000, loss = 0.75 (924.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:54:18.112851: step 47010, loss = 0.92 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-07 21:54:19.381444: step 47020, loss = 0.73 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:54:20.645679: step 47030, loss = 0.99 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:54:21.913993: step 47040, loss = 0.83 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:54:23.175312: step 47050, loss = 0.77 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:54:24.450600: step 47060, loss = 0.76 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:54:25.722200: step 47070, loss = 0.64 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:54:26.982709: step 47080, loss = 0.74 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:54:28.260306: step 47090, loss = 0.82 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:54:29.612550: step 47100, loss = 0.76 (946.6 examples/sec; 0.135 sec/batch)
2017-05-07 21:54:30.770315: step 47110, loss = 0.85 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-07 21:54:32.017585: step 47120, loss = 0.88 (1026.2 examples/sec; 0.125 sec/batch)
2017-05-07 21:54:33.316517: step 47130, loss = 0.91 (985.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:54:34.589412: step 47140, loss = 0.80 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:54:35.854456: step 47150, loss = 0.76 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:54:37.118729: step 47160, loss = 0.59 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:54:38.381118: step 47170, loss = 0.83 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:54:39.645811: step 47180, loss = 0.80 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:54:40.909472: step 47190, loss = 0.74 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:54:42.256713: step 47200, loss = 0.93 (950.1 examples/sec; 0.135 sec/batch)
2017-05-07 21:54:43.411315: step 47210, loss = 0.74 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-07 21:54:44.676138: step 47220, loss = 0.63 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:54:45.934914: step 47230, loss = 0.70 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:54:47.210701: step 47240, loss = 0.88 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:54:48.478150: step 47250, loss = 0.83 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:54:49.746012: step 47260, loss = 0.66 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:54:51.002088: step 47270, loss = 0.73 (1019.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:54:52.273362: step 47280, loss = 0.70 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:54:53.540401: step 47290, loss = 0.73 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:54:54.906870: step 47300, loss = 0.73 (936.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:54:56.072897: step 47310, loss = 0.78 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-07 21:54:57.364606: step 47320, loss = 0.92 (990.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:54:58.630743: step 47330, loss = 1.05 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:54:59.894093: step 47340, loss = 0.72 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:55:01.180259: step 47350, loss = 0.79 (995.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:55:02.481813: step 47360, loss = 0.70 (983.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:55:03.757579: step 47370, loss = 0.98 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:55:05.066145: step 47380, loss = 0.75 (978.2 examples/sec; 0.131 sec/batch)
2017-05-07 21:55:06.346364: step 47390, loss = 0.94 (999.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:55:07.705474: step 47400, loss = 0.90 (941.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:55:08.887784: step 47410, loss = 0.79 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-07 21:55:10.151734: step 47420, loss = 0.87 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:55:11.419110: step 47430, loss = 1.17 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:55:12.697075: step 47440, loss = 0.84 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:55:13.995784: step 47450, loss = 0.93 (985.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:55:15.270993: step 47460, loss = 0.81 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:55:16.523862: step 47470, loss = 0.77 (1021.6 examples/sec; 0.125 sec/batch)
2017-05-07 21:55:17.786518: step 47480, loss = 0.88 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:55:19.050929: step 47490, loss = 0.85 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:55:20.431273: step 47500, loss = 0.80 (927.3 examples/sec; 0.138 sec/batch)
2017-05-07 21:55:21.600295: step 47510, loss = 0.87 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-07 21:55:22.879267: step 47520, loss = 1.02 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:55:24.171041: step 47530, loss = 0.73 (990.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:55:25.458049: step 47540, loss = 1.05 (994.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:55:26.723488: step 47550, loss = 0.79 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:55:28.015994: step 47560, loss = 0.98 (990.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:55:29.302017: step 47570, loss = 0.80 (995.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:55:30.565022: step 47580, loss = 0.72 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:55:31.832005: step 47590, loss = 0.96 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:55:33.211897: step 47600, loss = 0.79 (927.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:55:34.409485: step 47610, loss = 0.98 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-07 21:55:35.712149: step 47620, loss = 0.68 (982.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:55:37.006779: step 47630, loss = 0.76 (988.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:55:38.256542: step 47640, loss = 0.81 (1024.2 examples/sec; 0.125 sec/batch)
2017-05-07 21:55:39.509870: stE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 973 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
ep 47650, loss = 1.09 (1021.3 examples/sec; 0.125 sec/batch)
2017-05-07 21:55:40.816907: step 47660, loss = 0.90 (979.3 examples/sec; 0.131 sec/batch)
2017-05-07 21:55:42.091161: step 47670, loss = 0.75 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:55:43.345610: step 47680, loss = 0.86 (1020.4 examples/sec; 0.125 sec/batch)
2017-05-07 21:55:44.616503: step 47690, loss = 0.70 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:55:45.960883: step 47700, loss = 0.74 (952.1 examples/sec; 0.134 sec/batch)
2017-05-07 21:55:47.157405: step 47710, loss = 0.83 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-07 21:55:48.424343: step 47720, loss = 0.81 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:55:49.698055: step 47730, loss = 0.62 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:55:50.957429: step 47740, loss = 0.72 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:55:52.234554: step 47750, loss = 0.62 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:55:53.500798: step 47760, loss = 0.95 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:55:54.753928: step 47770, loss = 0.77 (1021.4 examples/sec; 0.125 sec/batch)
2017-05-07 21:55:56.027869: step 47780, loss = 0.61 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:55:57.286393: step 47790, loss = 0.68 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:55:58.645788: step 47800, loss = 0.86 (941.6 examples/sec; 0.136 sec/batch)
2017-05-07 21:55:59.876476: step 47810, loss = 0.72 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-07 21:56:01.158307: step 47820, loss = 0.81 (998.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:56:02.397788: step 47830, loss = 0.62 (1032.7 examples/sec; 0.124 sec/batch)
2017-05-07 21:56:03.661356: step 47840, loss = 0.81 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:56:04.946864: step 47850, loss = 0.87 (995.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:56:06.226092: step 47860, loss = 0.81 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:56:07.497319: step 47870, loss = 0.83 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:56:08.748250: step 47880, loss = 0.72 (1023.2 examples/sec; 0.125 sec/batch)
2017-05-07 21:56:10.010799: step 47890, loss = 0.77 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:56:11.379122: step 47900, loss = 0.70 (935.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:56:12.607447: step 47910, loss = 0.80 (1042.1 examples/sec; 0.123 sec/batch)
2017-05-07 21:56:13.869157: step 47920, loss = 0.81 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:56:15.148198: step 47930, loss = 1.00 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:56:16.410415: step 47940, loss = 0.96 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:56:17.686734: step 47950, loss = 0.83 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:56:18.962474: step 47960, loss = 0.68 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:56:20.245314: step 47970, loss = 0.68 (997.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:56:21.503626: step 47980, loss = 0.81 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:56:22.748289: step 47990, loss = 0.81 (1028.4 examples/sec; 0.124 sec/batch)
2017-05-07 21:56:24.122914: step 48000, loss = 0.76 (931.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:56:25.296870: step 48010, loss = 0.70 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-07 21:56:26.541372: step 48020, loss = 0.73 (1028.5 examples/sec; 0.124 sec/batch)
2017-05-07 21:56:27.829496: step 48030, loss = 0.69 (993.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:56:29.116923: step 48040, loss = 0.70 (994.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:56:30.383585: step 48050, loss = 0.71 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:56:31.636000: step 48060, loss = 0.68 (1022.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:56:32.896109: step 48070, loss = 0.75 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:56:34.148009: step 48080, loss = 0.68 (1022.4 examples/sec; 0.125 sec/batch)
2017-05-07 21:56:35.423723: step 48090, loss = 0.77 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:56:36.788556: step 48100, loss = 0.81 (937.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:56:37.949215: step 48110, loss = 0.89 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-07 21:56:39.237182: step 48120, loss = 0.82 (993.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:56:40.506185: step 48130, loss = 0.88 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:56:41.813724: step 48140, loss = 0.96 (978.9 examples/sec; 0.131 sec/batch)
2017-05-07 21:56:43.109348: step 48150, loss = 0.75 (987.9 examples/sec; 0.130 sec/batch)
2017-05-07 21:56:44.386269: step 48160, loss = 0.83 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:56:45.659736: step 48170, loss = 0.80 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:56:46.947745: step 48180, loss = 0.71 (993.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:56:48.223511: step 48190, loss = 0.81 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:56:49.561773: step 48200, loss = 0.89 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 21:56:50.719592: step 48210, loss = 1.00 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-07 21:56:51.994470: step 48220, loss = 0.80 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:56:53.266591: step 48230, loss = 0.65 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:56:54.529777: step 48240, loss = 0.83 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:56:55.823672: step 48250, loss = 0.78 (989.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:56:57.122176: step 48260, loss = 0.84 (985.8 examples/sec; 0.130 sec/batch)
2017-05-07 21:56:58.397756: step 48270, loss = 0.89 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:56:59.694499: step 48280, loss = 0.88 (987.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:57:00.986903: step 48290, loss = 0.66 (990.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:57:02.330250: step 48300, loss = 0.84 (952.8 examples/sec; 0.134 sec/batch)
2017-05-07 21:57:03.504342: step 48310, loss = 0.85 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-07 21:57:04.813289: step 48320, loss = 0.60 (977.9 examples/sec; 0.131 sec/batch)
2017-05-07 21:57:06.096173: step 48330, loss = 0.79 (997.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:57:07.378478: step 48340, loss = 0.71 (998.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:57:08.638319: step 48350, loss = 0.85 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:57:09.935137: step 48360, loss = 0.75 (987.0 examples/sec; 0.130 sec/batch)
2017-05-07 21:57:11.237822: step 48370, loss = 0.89 (982.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:57:12.505646: step 48380, loss = 1.01 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:57:13.762765: step 48390, loss = 0.65 (1018.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:57:15.144407: step 48400, loss = 0.89 (926.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:57:16.367289: step 48410, loss = 0.80 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-07 21:57:17.652761: step 48420, loss = 0.76 (995.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:57:18.949698: step 48430, loss = 0.86 (986.9 examples/sec; 0.130 sec/batch)
2017-05-07 21:57:20.231445: step 48440, loss = 0.91 (998.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:57:21.497997: step 48450, loss = 0.62 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:57:22.778893: step 48460, loss = 0.77 (999.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:57:24.038037: step 48470, loss = 0.62 (1016.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:57:25.335281: step 48480, loss = 0.79 (986.7 examples/sec; 0.130 sec/batch)
2017-05-07 21:57:26.593746: step 48490, loss = 0.89 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:57:27.945691: step 48500, loss = 0.84 (946.8 examples/sec; 0.135 sec/batch)
2017-05-07 21:57:29.121881: step 48510, loss = 0.74 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-07 21:57:30.400207: step 48520, loss = 0.64 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:57:31.677852: step 48530, loss = 0.98 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:57:32.961925: step 48540, loss = 0.78 (996.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:57:34.256828: step 48550, loss = 0.72 (988.5 exampleE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 993 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
s/sec; 0.129 sec/batch)
2017-05-07 21:57:35.563605: step 48560, loss = 0.87 (979.5 examples/sec; 0.131 sec/batch)
2017-05-07 21:57:36.838005: step 48570, loss = 0.91 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:57:38.121747: step 48580, loss = 0.96 (997.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:57:39.408455: step 48590, loss = 0.80 (994.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:57:40.795447: step 48600, loss = 0.97 (922.9 examples/sec; 0.139 sec/batch)
2017-05-07 21:57:41.985340: step 48610, loss = 0.87 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-07 21:57:43.271445: step 48620, loss = 0.79 (995.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:57:44.527542: step 48630, loss = 0.89 (1019.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:57:45.822165: step 48640, loss = 0.72 (988.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:57:47.116488: step 48650, loss = 0.67 (988.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:57:48.401513: step 48660, loss = 0.84 (996.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:57:49.691687: step 48670, loss = 1.04 (992.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:57:50.973057: step 48680, loss = 0.89 (998.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:57:52.283886: step 48690, loss = 0.92 (976.5 examples/sec; 0.131 sec/batch)
2017-05-07 21:57:53.677705: step 48700, loss = 0.72 (918.3 examples/sec; 0.139 sec/batch)
2017-05-07 21:57:54.824792: step 48710, loss = 0.73 (1115.9 examples/sec; 0.115 sec/batch)
2017-05-07 21:57:56.123174: step 48720, loss = 1.00 (985.8 examples/sec; 0.130 sec/batch)
2017-05-07 21:57:57.399022: step 48730, loss = 0.85 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:57:58.688288: step 48740, loss = 0.81 (992.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:57:59.979345: step 48750, loss = 0.66 (991.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:58:01.265758: step 48760, loss = 0.90 (995.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:58:02.577244: step 48770, loss = 0.85 (976.0 examples/sec; 0.131 sec/batch)
2017-05-07 21:58:03.882229: step 48780, loss = 0.74 (980.9 examples/sec; 0.130 sec/batch)
2017-05-07 21:58:05.181549: step 48790, loss = 0.87 (985.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:58:06.526221: step 48800, loss = 0.69 (951.9 examples/sec; 0.134 sec/batch)
2017-05-07 21:58:07.749388: step 48810, loss = 0.80 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-07 21:58:08.999673: step 48820, loss = 0.85 (1023.8 examples/sec; 0.125 sec/batch)
2017-05-07 21:58:10.284841: step 48830, loss = 0.83 (996.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:58:11.580133: step 48840, loss = 0.62 (988.2 examples/sec; 0.130 sec/batch)
2017-05-07 21:58:12.849256: step 48850, loss = 0.89 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:58:14.144754: step 48860, loss = 0.80 (988.0 examples/sec; 0.130 sec/batch)
2017-05-07 21:58:15.424118: step 48870, loss = 0.87 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:58:16.693701: step 48880, loss = 0.92 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:58:17.950314: step 48890, loss = 0.77 (1018.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:58:19.311901: step 48900, loss = 0.84 (940.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:58:20.501204: step 48910, loss = 0.72 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-07 21:58:21.793186: step 48920, loss = 0.91 (990.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:58:23.082182: step 48930, loss = 0.92 (993.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:58:24.415602: step 48940, loss = 0.76 (959.9 examples/sec; 0.133 sec/batch)
2017-05-07 21:58:25.690993: step 48950, loss = 0.84 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:58:26.977699: step 48960, loss = 0.95 (994.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:58:28.258285: step 48970, loss = 0.73 (999.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:58:29.551444: step 48980, loss = 0.79 (989.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:58:30.848744: step 48990, loss = 0.73 (986.7 examples/sec; 0.130 sec/batch)
2017-05-07 21:58:32.217608: step 49000, loss = 1.01 (935.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:58:33.416448: step 49010, loss = 0.82 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-07 21:58:34.710865: step 49020, loss = 0.71 (988.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:58:35.986338: step 49030, loss = 0.83 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:58:37.256682: step 49040, loss = 0.93 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:58:38.521420: step 49050, loss = 0.86 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:58:39.800697: step 49060, loss = 0.86 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:58:41.113249: step 49070, loss = 0.83 (975.2 examples/sec; 0.131 sec/batch)
2017-05-07 21:58:42.389127: step 49080, loss = 0.82 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:58:43.684671: step 49090, loss = 0.75 (988.0 examples/sec; 0.130 sec/batch)
2017-05-07 21:58:45.061634: step 49100, loss = 0.72 (929.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:58:46.255328: step 49110, loss = 0.76 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-07 21:58:47.515636: step 49120, loss = 0.83 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:58:48.823432: step 49130, loss = 0.62 (978.7 examples/sec; 0.131 sec/batch)
2017-05-07 21:58:50.091002: step 49140, loss = 0.81 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:58:51.354439: step 49150, loss = 0.80 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:58:52.626558: step 49160, loss = 0.85 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:58:53.900671: step 49170, loss = 0.76 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:58:55.207698: step 49180, loss = 0.77 (979.3 examples/sec; 0.131 sec/batch)
2017-05-07 21:58:56.488019: step 49190, loss = 0.71 (999.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:58:57.842171: step 49200, loss = 0.77 (945.2 examples/sec; 0.135 sec/batch)
2017-05-07 21:58:59.034753: step 49210, loss = 0.75 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-07 21:59:00.321274: step 49220, loss = 0.74 (994.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:59:01.578405: step 49230, loss = 0.58 (1018.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:59:02.840924: step 49240, loss = 0.68 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:59:04.103363: step 49250, loss = 0.73 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:59:05.402027: step 49260, loss = 0.81 (985.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:59:06.692840: step 49270, loss = 0.87 (991.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:59:07.971136: step 49280, loss = 0.80 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:59:09.256872: step 49290, loss = 1.04 (995.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:59:10.623878: step 49300, loss = 0.89 (936.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:59:11.817519: step 49310, loss = 0.84 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-07 21:59:13.130330: step 49320, loss = 0.68 (975.0 examples/sec; 0.131 sec/batch)
2017-05-07 21:59:14.395178: step 49330, loss = 0.84 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:59:15.701602: step 49340, loss = 0.87 (979.8 examples/sec; 0.131 sec/batch)
2017-05-07 21:59:17.006783: step 49350, loss = 0.87 (980.7 examples/sec; 0.131 sec/batch)
2017-05-07 21:59:18.269923: step 49360, loss = 0.66 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:59:19.531757: step 49370, loss = 0.82 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:59:20.835336: step 49380, loss = 0.84 (981.9 examples/sec; 0.130 sec/batch)
2017-05-07 21:59:22.107796: step 49390, loss = 0.87 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:59:23.466710: step 49400, loss = 0.84 (941.9 examples/sec; 0.136 sec/batch)
2017-05-07 21:59:24.677745: step 49410, loss = 0.72 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-07 21:59:25.956501: step 49420, loss = 0.75 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:59:27.242823: step 49430, loss = 0.66 (995.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:59:28.546118: step 49440, loss = 0.74 (982.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:59:29.797503: step 49450, loss = 0.75 (1022.9 examples/sec; 0.125 sec/batch)
2017-05-07 21:59:31.069178: stE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1013 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
ep 49460, loss = 0.87 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:59:32.384596: step 49470, loss = 0.87 (973.1 examples/sec; 0.132 sec/batch)
2017-05-07 21:59:33.679350: step 49480, loss = 0.96 (988.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:59:34.965492: step 49490, loss = 0.89 (995.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:59:36.323875: step 49500, loss = 0.70 (942.3 examples/sec; 0.136 sec/batch)
2017-05-07 21:59:37.495553: step 49510, loss = 0.79 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-07 21:59:38.801108: step 49520, loss = 0.78 (980.4 examples/sec; 0.131 sec/batch)
2017-05-07 21:59:40.109003: step 49530, loss = 0.68 (978.7 examples/sec; 0.131 sec/batch)
2017-05-07 21:59:41.365049: step 49540, loss = 0.72 (1019.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:59:42.639833: step 49550, loss = 0.73 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:59:43.941873: step 49560, loss = 0.91 (983.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:59:45.248348: step 49570, loss = 0.94 (979.7 examples/sec; 0.131 sec/batch)
2017-05-07 21:59:46.552599: step 49580, loss = 0.80 (981.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:59:47.837942: step 49590, loss = 0.86 (995.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:59:49.221543: step 49600, loss = 0.85 (925.1 examples/sec; 0.138 sec/batch)
2017-05-07 21:59:50.413893: step 49610, loss = 0.79 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-07 21:59:51.683015: step 49620, loss = 0.72 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:59:52.982779: step 49630, loss = 1.33 (984.8 examples/sec; 0.130 sec/batch)
2017-05-07 21:59:54.281607: step 49640, loss = 0.78 (985.5 examples/sec; 0.130 sec/batch)
2017-05-07 21:59:55.555749: step 49650, loss = 0.80 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:59:56.844913: step 49660, loss = 0.74 (992.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:59:58.124144: step 49670, loss = 0.97 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:59:59.410943: step 49680, loss = 1.03 (994.7 examples/sec; 0.129 sec/batch)
2017-05-07 22:00:00.706283: step 49690, loss = 0.81 (988.2 examples/sec; 0.130 sec/batch)
2017-05-07 22:00:02.068473: step 49700, loss = 0.73 (939.7 examples/sec; 0.136 sec/batch)
2017-05-07 22:00:03.269664: step 49710, loss = 0.68 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-07 22:00:04.540521: step 49720, loss = 0.75 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-07 22:00:05.806479: step 49730, loss = 0.80 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-07 22:00:07.079413: step 49740, loss = 0.78 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-07 22:00:08.351146: step 49750, loss = 0.85 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-07 22:00:09.598504: step 49760, loss = 0.88 (1026.2 examples/sec; 0.125 sec/batch)
2017-05-07 22:00:10.889250: step 49770, loss = 0.79 (991.7 examples/sec; 0.129 sec/batch)
2017-05-07 22:00:12.191063: step 49780, loss = 0.76 (983.2 examples/sec; 0.130 sec/batch)
2017-05-07 22:00:13.485120: step 49790, loss = 0.85 (989.1 examples/sec; 0.129 sec/batch)
2017-05-07 22:00:14.845539: step 49800, loss = 0.76 (940.9 examples/sec; 0.136 sec/batch)
2017-05-07 22:00:16.029211: step 49810, loss = 0.65 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-07 22:00:17.297564: step 49820, loss = 0.82 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-07 22:00:18.569457: step 49830, loss = 0.68 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-07 22:00:19.858406: step 49840, loss = 0.66 (993.1 examples/sec; 0.129 sec/batch)
2017-05-07 22:00:21.137459: step 49850, loss = 0.73 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-07 22:00:22.380945: step 49860, loss = 0.67 (1029.4 examples/sec; 0.124 sec/batch)
2017-05-07 22:00:23.679395: step 49870, loss = 0.80 (985.8 examples/sec; 0.130 sec/batch)
2017-05-07 22:00:24.973120: step 49880, loss = 0.73 (989.4 examples/sec; 0.129 sec/batch)
2017-05-07 22:00:26.240291: step 49890, loss = 0.95 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-07 22:00:27.603321: step 49900, loss = 0.85 (939.1 examples/sec; 0.136 sec/batch)
2017-05-07 22:00:28.767135: step 49910, loss = 0.75 (109E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1016 events to /tmp/cifar10_train/events.out.tfevents.1494202341.GHC33.GHC.ANDREW.CMU.EDU
9.8 examples/sec; 0.116 sec/batch)
2017-05-07 22:00:30.041876: step 49920, loss = 0.65 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-07 22:00:31.337379: step 49930, loss = 0.77 (988.0 examples/sec; 0.130 sec/batch)
2017-05-07 22:00:32.647937: step 49940, loss = 0.82 (976.7 examples/sec; 0.131 sec/batch)
2017-05-07 22:00:33.935138: step 49950, loss = 0.83 (994.4 examples/sec; 0.129 sec/batch)
2017-05-07 22:00:35.212314: step 49960, loss = 0.75 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-07 22:00:36.477455: step 49970, loss = 0.85 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-07 22:00:37.743651: step 49980, loss = 0.80 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-07 22:00:39.030141: step 49990, loss = 0.77 (995.0 examples/sec; 0.129 sec/batch)
2017-05-07 22:00:40.393871: step 50000, loss = 0.81 (938.6 examples/sec; 0.136 sec/batch)
--- 6499.00299001 seconds ---
