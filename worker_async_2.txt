Connecting to port  12362
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Received variable values from ps
2017-05-07 20:12:42.851133: step 0, loss = 4.68 (75.3 examples/sec; 1.699 sec/batch)
2017-05-07 20:12:43.895775: step 10, loss = 4.55 (1225.3 examples/sec; 0.104 sec/batch)
2017-05-07 20:12:45.159798: step 20, loss = 4.24 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:12:46.433598: step 30, loss = 4.27 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:12:47.709824: step 40, loss = 4.18 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:12:49.010456: step 50, loss = 4.13 (984.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:12:50.300918: step 60, loss = 4.28 (991.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:12:51.572350: step 70, loss = 4.10 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:12:52.881072: step 80, loss = 4.06 (978.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:12:54.175819: step 90, loss = 3.98 (988.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:12:55.564379: step 100, loss = 3.83 (921.8 examples/sec; 0.139 sec/batch)
2017-05-07 20:12:56.698747: step 110, loss = 3.86 (1128.4 examples/sec; 0.113 sec/batch)
2017-05-07 20:12:57.972616: step 120, loss = 3.77 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:12:59.242567: step 130, loss = 3.81 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:13:00.506305: step 140, loss = 3.72 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:13:01.758830: step 150, loss = 3.65 (1021.9 examples/sec; 0.125 sec/batch)
2017-05-07 20:13:03.060632: step 160, loss = 3.66 (983.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:13:04.362021: step 170, loss = 3.48 (983.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:13:05.656860: step 180, loss = 3.79 (988.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:13:06.937380: step 190, loss = 3.70 (999.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:13:08.321114: step 200, loss = 3.67 (925.0 examples/sec; 0.138 sec/batch)
2017-05-07 20:13:09.463265: step 210, loss = 3.53 (1120.7 examples/sec; 0.114 sec/batch)
2017-05-07 20:13:10.725282: step 220, loss = 3.39 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:13:12.006129: step 230, loss = 3.37 (999.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:13:13.285257: step 240, loss = 3.24 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:13:14.537742: step 250, loss = 3.37 (1022.0 examples/sec; 0.125 sec/batch)
2017-05-07 20:13:15.818369: step 260, loss = 3.21 (999.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:13:17.081774: step 270, loss = 3.13 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:13:18.397942: step 280, loss = 3.30 (972.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:13:19.684360: step 290, loss = 3.24 (995.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:13:21.091090: step 300, loss = 3.23 (909.9 examples/sec; 0.141 sec/batch)
2017-05-07 20:13:22.242076: step 310, loss = 3.18 (1112.1 examples/sec; 0.115 sec/batch)
2017-05-07 20:13:23.553518: step 320, loss = 3.25 (976.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:13:24.855795: step 330, loss = 3.16 (982.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:13:26.132351: step 340, loss = 3.18 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:13:27.394448: step 350, loss = 2.95 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:13:28.683242: step 360, loss = 3.09 (993.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:13:29.978191: step 370, loss = 2.94 (988.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:13:31.261574: step 380, loss = 3.03 (997.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:13:32.540228: step 390, loss = 2.87 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:13:33.905158: step 400, loss = 3.13 (937.8 examples/sec; 0.136 sec/batch)
2017-05-07 20:13:35.068443: step 410, loss = 2.76 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-07 20:13:36.357604: step 420, loss = 2.89 (992.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:13:37.609883: step 430, loss = 2.98 (1022.1 examples/sec; 0.125 sec/batch)
2017-05-07 20:13:38.904040: step 440, loss = 3.04 (989.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:13:40.181464: step 450, loss = 2.74 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:13:41.478382: step 460, loss = 2.98 (987.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:13:42.780804: step 470, loss = 2.64 (982.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:13:44.071248: step 480, loss = 2.80 (991.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:13:45.331496: step 490, loss = 2.77 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:13:46.689765: step 500, loss = 2.79 (942.4 examples/sec; 0.136 sec/batch)
2017-05-07 20:13:47.881700: step 510, loss = 2.59 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-07 20:13:49.177248: step 520, loss = 2.78 (988.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:13:50.460783: step 530, loss = 2.70 (997.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:13:51.764161: step 540, loss = 2.63 (982.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:13:53.020300: step 550, loss = 2.54 (1019.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:13:54.286416: step 560, loss = 2.56 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:13:55.592375: step 570, loss = 2.86 (980.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:13:56.882581: step 580, loss = 2.49 (992.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:13:58.160064: step 590, loss = 2.39 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:13:59.540157: step 600, loss = 2.67 (927.5 examples/sec; 0.138 sec/batch)
2017-05-07 20:14:00.708877: step 610, loss = 2.73 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-07 20:14:01.945142: step 620, loss = 2.44 (1035.4 examples/sec; 0.124 sec/batch)
2017-05-07 20:14:03.256410: step 630, loss = 2.43 (976.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:14:04.548814: step 640, loss = 2.63 (990.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:14:05.811217: step 650, loss = 2.22 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:14:07.133949: step 660, loss = 2.28 (967.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:14:08.417331: step 670, loss = 2.59 (997.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:14:09.686664: step 680, loss = 2.34 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:14:10.949430: step 690, loss = 2.32 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:14:12.326897: step 700, loss = 2.39 (929.2 examples/sec; 0.138 sec/batch)
2017-05-07 20:14:13.495691: step 710, loss = 2.35 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-07 20:14:14.757227: step 720, loss = 2.39 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:14:16.038523: step 730, loss = 2.32 (999.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:14:17.336184: step 740, loss = 2.47 (986.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:14:18.647618: step 750, loss = 2.31 (976.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:14:19.917141: step 760, loss = 2.21 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:14:21.257772: step 770, loss = 2.04 (954.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:14:22.538466: step 780, loss = 2.24 (999.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:14:23.806620: step 790, loss = 2.24 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:14:25.177007: step 800, loss = 2.17 (934.0 examples/sec; 0.137 sec/batch)
2017-05-07 20:14:26.363612: step 810, loss = 2.19 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-07 20:14:27.634211: step 820, loss = 2.12 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:14:28.911120: step 830, loss = 2.30 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:14:30.179121: step 840, loss = 1.91 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:14:31.474182: step 850, loss = 1.99 (988.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:14:32.735703: step 860, loss = 1.85 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:14:34.007902: step 870, loss = 2.07 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:14:35.296258: step 880, loss = 2.04 (993.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:14:36.569247: step 890, loss = 2.06 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:14:37.955448: step 900, loss = 2.30 (923.4 examples/sec; 0.139 sec/batch)
2017-05-07 20:14:39.150694: step 910, loss = 2.05 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-07 20:14:40.440438: step 920, loss = 1.84 (992.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:14:41.729290: step 930, loss = 1.84 (993.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:14:43.033870: step 940, loss = 2.26 (981.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:14:44.331432: step 950, loss = 1.96 (986.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:14:45.605254: step 960, loss = 2.05 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:14:46.878680: step 970, loss = 1.99 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:14:48.177415: step 980, loss = 1.91 (985.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:14:49.468705: step 990, loss = 1.89 (991.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:14:50.841285: step 1000, loss = 1.95 (932.5 examples/sec; 0.137 sec/batch)
2017-05-07 20:14:52.043009: step 1010, loss = 1.83 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-07 20:14:53.357466: step 1020, loss = 1.91 (973.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:14:54.615659: step 1030, loss = 1.69 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:14:55.894494: step 1040, loss = 2.00 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:14:57.167886: step 1050, loss = 1.97 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:14:58.433893: step 1060, loss = 1.76 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:14:59.713012: step 1070, loss = 2.01 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:15:01.018468: step 1080, loss = 1.93 (980.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:15:02.291940: step 1090, loss = 1.73 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:15:03.658857: step 1100, loss = 1.75 (936.4 examples/sec; 0.137 sec/batch)
2017-05-07 20:15:04.842511: step 1110, loss = 1.93 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-07 20:15:06.159668: step 1120, loss = 2.03 (971.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:15:07.443903: step 1130, loss = 1.98 (996.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:15:08.777398: step 1140, loss = 1.86 (959.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:15:10.035834: step 1150, loss = 1.84 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:15:11.283926: step 1160, loss = 1.70 (1025.5 examples/sec; 0.125 sec/batch)
2017-05-07 20:15:12.575534: step 1170, loss = 1.54 (991.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:15:13.853780: step 1180, loss = 1.62 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:15:15.155582: step 1190, loss = 1.67 (983.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:15:16.549371: step 1200, loss = 1.69 (918.4 examples/sec; 0.139 sec/batch)
2017-05-07 20:15:17.763449: step 1210, loss = 1.94 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-07 20:15:19.058809: step 1220, loss = 1.87 (988.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:15:20.362796: step 1230, loss = 1.65 (981.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:15:21.666757: step 1240, loss = 1.77 (981.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:15:22.966054: step 1250, loss = 1.88 (985.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:15:24.286775: step 1260, loss = 1.60 (969.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:15:25.567926: step 1270, loss = 1.84 (999.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:15:26.861007: step 1280, loss = 1.57 (989.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:15:28.143595: step 1290, loss = 1.77 (998.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:15:29.525703: step 1300, loss = 1.53 (926.1 examples/sec; 0.138 sec/batch)
2017-05-07 20:15:30.723055: step 1310, loss = 1.69 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-07 20:15:32.007354: step 1320, loss = 1.73 (996.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:15:33.283519: step 1330, loss = 1.79 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:15:34.549902: step 1340, loss = 1.57 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:15:35.819418: step 1350, loss = 1.74 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:15:37.096059: step 1360, loss = 1.71 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:15:38.367327: step 1370, loss = 1.57 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:15:39.667105: step 1380, loss = 1.67 (984.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:15:40.967408: step 1390, loss = 1.81 (984.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:15:42.367621: step 1400, loss = 1.61 (914.2 examples/sec; 0.140 sec/batch)
2017-05-07 20:15:43.552068: step 1410, loss = 1.53 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-07 20:15:44.835303: step 1420, loss = 2.10 (997.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:15:46.137932: step 1430, loss = 1.71 (982.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:15:47.409227: step 1440, loss = 1.55 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:15:48.690704: step 1450, loss = 1.62 (998.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:15:49.954544: step 1460, loss = 1.55 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:15:51.241809: step 1470, loss = 1.49 (994.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:15:52.534296: step 1480, loss = 1.48 (990.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:15:53.804248: step 1490, loss = 1.29 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:15:55.198479: step 1500, loss = 1.49 (918.1 examples/sec; 0.139 sec/batch)
2017-05-07 20:15:56.432960: step 1510, loss = 1.71 (1036.9 examples/sec; 0.123 sec/batch)
2017-05-07 20:15:57.705415: step 1520, loss = 1.39 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:15:58.984574: step 1530, loss = 1.47 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:16:00.267579: step 1540, loss = 1.39 (997.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:16:01.541520: step 1550, loss = 1.23 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:16:02.846644: step 1560, loss = 1.38 (980.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:16:04.163277: step 1570, loss = 1.36 (972.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:16:05.467319: step 1580, loss = 2.00 (981.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:16:06.767020: step 1590, loss = 1.60 (984.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:16:08.168355: step 1600, loss = 1.36 (913.4 examples/sec; 0.140 sec/batch)
2017-05-07 20:16:09.366297: step 1610, loss = 1.52 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-07 20:16:10.673716: step 1620, loss = 1.28 (979.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:16:11.942815: step 1630, loss = 1.46 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:16:13.237542: step 1640, loss = 1.41 (988.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:16:14.507023: step 1650, loss = 1.31 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:16:15.798897: step 1660, loss = 1.44 (990.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:16:17.064798: step 1670, loss = 1.46 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:16:18.344192: step 1680, loss = 1.27 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:16:19.642072: step 1690, loss = 1.61 (986.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:16:21.030405: step 1700, loss = 1.54 (922.0 examples/sec; 0.139 sec/batch)
2017-05-07 20:16:22.231707: step 1710, loss = 1.33 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-07 20:16:23.536184: step 1720, loss = 1.42 (981.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:16:24.813479: step 1730, loss = 1.41 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:16:26.083755: step 1740, loss = 1.53 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:16:27.401709: step 1750, loss = 1.34 (971.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:16:28.705924: step 1760, loss = 1.34 (981.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:16:29.958740: step 1770, loss = 1.47 (1021.7 examples/sec; 0.125 sec/batch)
2017-05-07 20:16:31.259117: step 1780, loss = 1.37 (984.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:16:32.561438: step 1790, loss = 1.49 (982.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:16:33.947426: step 1800, loss = 1.36 (923.5 examples/sec; 0.139 sec/batch)
2017-05-07 20:16:35.144606: step 1810, loss = 1.37 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-07 20:16:36.414578: step 1820, loss = 1.31 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:16:37.682513: step 1830, loss = 1.43 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:16:38.957928: step 1840, loss = 1.32 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:16:40.240501: step 1850, loss = 1.63 (998.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:16:41.525363: step 1860, loss = 1.40 (996.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:16:42.805284: step 1870, loss = 1.48 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:16:44.085997: step 1880, loss = 1.16 (999.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:16:45.373538: step 1890, loss = 1.28 (994.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:16:46.738919: step 1900, loss = 1.18 (937.5 examples/sec; 0.137 sec/batch)
2017-05-07 20:16:47.951501: step 1910, loss = 1.58 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-07 20:16:49.227667: step 1920, loss = 1.67 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:16:50.515107: step 1930, loss = 1.33 (994.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:16:51.772320: step 1940, loss = 1.27 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:16:53.058511: step 1950, loss = 1.50 (995.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:16:54.354479: step 1960, loss = 1.43 (987.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:16:55.625403: step 1970, loss = 1.30 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:16:56.890612: step 1980, loss = 1.21 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:16:58.163435: step 1990, loss = 1.32 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:16:59.550467: step 2000, loss = 1.27 (922.8 examples/sec; 0.139 sec/batch)
2017-05-07 20:17:00.731065: step 2010, loss = 1.38 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-07 20:17:01.994610: step 2020, loss = 1.44 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:17:03.304599: step 2030, loss = 1.27 (977.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:17:04.568718: step 2040, loss = 1.62 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:17:05.841557: step 2050, loss = 1.32 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:17:07.120272: step 2060, loss = 1.18 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:17:08.398825: step 2070, loss = 1.10 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:17:09.681221: step 2080, loss = 1.38 (998.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:17:10.948862: step 2090, loss = 1.08 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:17:12.305806: step 2100, loss = 1.25 (943.3 examples/sec; 0.136 sec/batch)
2017-05-07 20:17:13.499842: step 2110, loss = 1.54 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-07 20:17:14.778500: step 2120, loss = 1.29 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:17:16.062812: step 2130, loss = 1.49 (996.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:17:17.370600: step 2140, loss = 1.17 (978.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:17:18.661779: step 2150, loss = 1.25 (991.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:17:19.930675: step 2160, loss = 1.09 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:17:21.211120: step 2170, loss = 1.28 (999.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:17:22.481690: step 2180, loss = 1.25 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:17:23.762940: step 2190, loss = 1.24 (999.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:17:25.169657: step 2200, loss = 1.36 (909.9 examples/sec; 0.141 sec/batch)
2017-05-07 20:17:26.326196: step 2210, loss = 1.39 (1106.7 examples/sec; 0.116 sec/batch)
2017-05-07 20:17:27.621876: step 2220, loss = 1.41 (987.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:17:28.912700: step 2230, loss = 1.24 (991.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:17:30.202098: step 2240, loss = 1.54 (992.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:17:31.474411: step 2250, loss = 1.14 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:17:32.770522: step 2260, loss = 1.22 (987.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:17:34.067776: step 2270, loss = 1.11 (986.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:17:35.365531: step 2280, loss = 1.32 (986.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:17:36.682545: step 2290, loss = 1.09 (971.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:17:38.070733: step 2300, loss = 1.44 (922.1 examples/sec; 0.139 sec/batch)
2017-05-07 20:17:39.303958: step 2310, loss = 1.40 (1037.9 examples/sec; 0.123 sec/batch)
2017-05-07 20:17:40.620535: step 2320, loss = 1.29 (972.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:17:41.886533: step 2330, loss = 1.22 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:17:43.184765: step 2340, loss = 1.14 (986.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:17:44.462230: step 2350, loss = 1.19 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:17:45.758868: step 2360, loss = 1.26 (987.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:17:47.049825: step 2370, loss = 1.39 (991.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:17:48.321397: step 2380, loss = 1.44 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:17:49.603760: step 2390, loss = 1.32 (998.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:17:50.997174: step 2400, loss = 1.10 (918.6 examples/sec; 0.139 sec/batch)
2017-05-07 20:17:52.171162: step 2410, loss = 1.48 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-07 20:17:53.454690: step 2420, loss = 2.18 (997.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:17:54.742658: step 2430, loss = 1.46 (993.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:17:56.031117: step 2440, loss = 1.27 (993.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:17:57.303185: step 2450, loss = 1.32 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:17:58.623949: step 2460, loss = 1.17 (969.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:17:59.905089: step 2470, loss = 1.25 (999.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:18:01.223047: step 2480, loss = 1.26 (971.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:18:02.487825: step 2490, loss = 1.42 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:18:03.838635: step 2500, loss = 1.05 (947.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:18:05.024728: step 2510, loss = 1.28 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-07 20:18:06.308708: step 2520, loss = 1.37 (996.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:18:07.579765: step 2530, loss = 1.34 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:18:08.867772: step 2540, loss = 1.02 (993.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:18:10.146906: step 2550, loss = 1.13 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:18:11.404036: step 2560, loss = 1.00 (1018.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:18:12.670174: step 2570, loss = 1.10 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:18:13.928316: step 2580, loss = 1.07 (1017.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:18:15.218307: step 2590, loss = 1.16 (992.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:18:16.604037: step 2600, loss = 1.23 (923.7 examples/sec; 0.139 sec/batch)
2017-05-07 20:18:17.776443: step 2610, loss = 1.45 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-07 20:18:19.047157: step 2620, loss = 1.27 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:18:20.315652: step 2630, loss = 1.27 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:18:21.620864: step 2640, loss = 1.37 (980.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:18:22.895532: step 2650, loss = 1.33 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:18:24.186781: step 2660, loss = 1.36 (991.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:18:25.433229: step 2670, loss = 1.36 (1026.9 examples/sec; 0.125 sec/batch)
2017-05-07 20:18:26.725391: step 2680, loss = 1.45 (990.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:18:27.990405: step 2690, loss = 1.30 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:18:29.373461: step 2700, loss = 1.18 (925.5 examples/sec; 0.138 sec/batch)
2017-05-07 20:18:30.551726: step 2710, loss = 1.27 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-07 20:18:31.821725: step 2720, loss = 1.05 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:18:33.097615: step 2730, loss = 1.17 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:18:34.348941: step 2740, loss = 1.47 (1022.9 examples/sec; 0.125 sec/batch)
2017-05-07 20:18:35.633159: step 2750, loss = 1.10 (996.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:18:36.912636: step 2760, loss = 1.03 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:18:38.181110: step 2770, loss = 0.93 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:18:39.485852: step 2780, loss = 1.13 (981.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:18:40.780948: step 2790, loss = 1.47 (988.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:18:42.157149: step 2800, loss = 1.36 (930.1 examples/sec; 0.138 sec/batch)
2017-05-07 20:18:43.354230: step 2810, loss = 1.09 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-07 20:18:44.622309: step 2820, loss = 1.45 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:18:45.905519: step 2830, loss = 1.32 (997.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:18:47.212941: step 2840, loss = 1.18 (979.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:18:48.506208: step 2850, loss = 1.19 (989.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:18:49.813501: step 2860, loss = 1.17 (979.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:18:51.107526: step 2870, loss = 1.17 (989.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:18:52.392538: step 2880, loss = 1.09 (996.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:18:53.677034: step 2890, loss = 1.23 (996.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:18:55.049145: step 2900, loss = 1.22 (932.9 examples/sec; 0.137 sec/batch)
2017-05-07 20:18:56.246346: step 2910, loss = 1.23 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-07 20:18:57.528677: step 2920, loss = 1.33 (998.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:18:58.809035: step 2930, loss = 1.22 (999.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:19:00.086939: step 2940, loss = 1.38 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:19:01.367221: step 2950, loss = 1.04 (999.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:19:02.687362: step 2960, loss = 0.98 (969.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:19:03.969764: step 2970, loss = 1.13 (998.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:19:05.270886: step 2980, loss = 1.31 (983.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:19:06.574952: step 2990, loss = 1.43 (981.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:19:07.970918: step 3000, loss = 1.06 (916.9 examples/sec; 0.140 sec/batch)
2017-05-07 20:19:09.172459: step 3010, loss = 1.14 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-07 20:19:10.445558: step 3020, loss = 1.12 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:19:11.729897: step 3030, loss = 1.52 (996.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:19:13.019894: step 3040, loss = 1.12 (992.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:19:14.299700: step 3050, loss = 1.16 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:19:15.596691: step 3060, loss = 1.16 (986.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:19:16.870591: step 3070, loss = 1.23 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:19:18.141671: step 3080, loss = 1.01 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:19:19.433015: step 3090, loss = 1.34 (991.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:19:20.795502: step 3100, loss = 1.30 (939.5 examples/sec; 0.136 sec/batch)
2017-05-07 20:19:21.999176: step 3110, loss = 1.26 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-07 20:19:23.321778: step 3120, loss = 1.18 (967.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:19:24.634626: step 3130, loss = 1.32 (975.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:19:25.919712: step 3140, loss = 1.00 (996.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:19:27.223303: step 3150, loss = 1.02 (981.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:19:28.516680: step 3160, loss = 1.15 (989.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:19:29.832372: step 3170, loss = 1.10 (972.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:19:31.149592: step 3180, loss = 1.14 (971.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:19:32.421922: step 3190, loss = 1.16 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:19:33.818428: step 3200, loss = 1.21 (916.6 examples/sec; 0.140 sec/batch)
2017-05-07 20:19:35.024950: step 3210, loss = 1.14 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-07 20:19:36.313821: step 3220, loss = 1.03 (993.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:19:37.588248: step 3230, loss = 0.99 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:19:38.881876: step 3240, loss = 1.12 (989.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:19:40.154630: step 3250, loss = 1.13 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:19:41.450268: step 3260, loss = 1.15 (987.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:19:42.757159: step 3270, loss = 1.04 (979.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:19:44.043323: step 3280, loss = 1.24 (995.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:19:45.336637: step 3290, loss = 1.20 (989.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:19:46.707737: step 3300, loss = 1.20 (933.6 examples/sec; 0.137 sec/batch)
2017-05-07 20:19:47.902904: step 3310, loss = 1.03 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-07 20:19:49.190550: step 3320, loss = 0.92 (994.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:19:50.469070: step 3330, loss = 1.31 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:19:51.782160: step 3340, loss = 1.32 (974.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:19:53.100013: step 3350, loss = 1.06 (971.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:19:54.389337: step 3360, loss = 1.20 (992.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:19:55.682161: step 3370, loss = 1.23 (990.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:19:56.948858: step 3380, loss = 1.26 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:19:58.230167: step 3390, loss = 1.08 (999.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:19:59.620030: step 3400, loss = 1.14 (920.9 examples/sec; 0.139 sec/batch)
2017-05-07 20:20:00.839205: step 3410, loss = 1.28 (1049.9 examples/sec; 0.122 sec/batch)
2017-05-07 20:20:02.142361: step 3420, loss = 1.24 (982.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:20:03.437758: step 3430, loss = 1.25 (988.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:20:04.735124: step 3440, loss = 1.12 (986.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:20:06.026792: step 3450, loss = 0.97 (991.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:20:07.327108: step 3460, loss = 1.01 (984.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:20:08.644757: step 3470, loss = 1.28 (971.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:20:09.921434: step 3480, loss = 1.33 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:20:11.209802: step 3490, loss = 1.12 (993.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:20:12.605205: step 3500, loss = 1.10 (917.3 examples/sec; 0.140 sec/batch)
2017-05-07 20:20:13.774159: step 3510, loss = 0.97 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-07 20:20:15.037338: step 3520, loss = 1.15 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:20:16.337366: step 3530, loss = 1.06 (984.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:20:17.597391: step 3540, loss = 1.15 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:20:18.889525: step 3550, loss = 1.27 (990.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:20:20.179388: step 3560, loss = 1.19 (992.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:20:21.504784: step 3570, loss = 1.16 (965.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:20:22.775493: step 3580, loss = 1.23 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:20:24.071847: step 3590, loss = 1.09 (987.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:20:25.460062: step 3600, loss = 1.00 (922.0 examples/sec; 0.139 sec/batch)
2017-05-07 20:20:26.642270: step 3610, loss = 1.11 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-07 20:20:27.924832: step 3620, loss = 0.91 (998.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:20:29.203492: step 3630, loss = 0.85 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:20:30.459210: step 3640, loss = 1.19 (1019.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:20:31.734611: step 3650, loss = 1.00 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:20:33.036037: step 3660, loss = 1.53 (983.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:20:34.326282: step 3670, loss = 1.29 (992.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:20:35.619794: step 3680, loss = 1.01 (989.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:20:36.909411: step 3690, loss = 1.21 (992.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:20:38.271103: step 3700, loss = 1.06 (940.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:20:39.459855: step 3710, loss = 1.14 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-07 20:20:40.747710: step 3720, loss = 1.10 (993.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:20:42.003335: step 3730, loss = 1.09 (1019.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:20:43.273749: step 3740, loss = 1.47 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:20:44.572491: step 3750, loss = 0.98 (985.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:20:45.842741: step 3760, loss = 0.99 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:20:47.172482: step 3770, loss = 1.37 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:20:48.471102: step 3780, loss = 1.16 (985.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:20:49.776359: step 3790, loss = 1.13 (980.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:20:51.159041: step 3800, loss = 0.89 (925.7 examples/sec; 0.138 sec/batch)
2017-05-07 20:20:52.341085: step 3810, loss = 1.02 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-07 20:20:53.614533: step 3820, loss = 1.05 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:20:54.900694: step 3830, loss = 0.99 (995.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:20:56.175120: step 3840, loss = 1.15 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:20:57.472007: step 3850, loss = 0.87 (987.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:20:58.741157: step 3860, loss = 0.99 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:21:00.034775: step 3870, loss = 1.20 (989.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:21:01.306518: step 3880, loss = 0.85 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:21:02.586167: step 3890, loss = 1.25 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:21:03.958500: step 3900, loss = 1.10 (932.7 examples/sec; 0.137 sec/batch)
2017-05-07 20:21:05.165761: step 3910, loss = 1.04 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-07 20:21:06.492542: step 3920, loss = 1.06 (964.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:21:07.780050: step 3930, loss = 1.04 (994.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:21:09.054293: step 3940, loss = 1.12 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:21:10.332258: step 3950, loss = 1.04 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:21:11.640117: step 3960, loss = 0.99 (978.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:21:12.906335: step 3970, loss = 1.13 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:21:14.171932: step 3980, loss = 1.24 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:21:15.454031: step 3990, loss = 1.06 (998.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:21:16.813056: step 4000, loss = 0.93 (941.8 examples/sec; 0.136 sec/batch)
2017-05-07 20:21:17.983466: step 4010, loss = 1.33 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-07 20:21:19.276648: step 4020, loss = 1.07 (989.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:21:20.586913: step 4030, loss = 1.12 (976.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:21:21.898290: step 4040, loss = 1.14 (976.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:21:23.181515: step 4050, loss = 1.27 (997.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:21:24.480887: step 4060, loss = 1.21 (985.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:21:25.775536: step 4070, loss = 1.30 (988.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:21:27.074875: step 4080, loss = 1.28 (985.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:21:28.364776: step 4090, loss = 1.08 (992.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:21:29.756901: step 4100, loss = 0.91 (919.5 examples/sec; 0.139 sec/batch)
2017-05-07 20:21:30.986667: step 4110, loss = 1.38 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-07 20:21:32.280490: step 4120, loss = 1.16 (989.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:21:33.590899: step 4130, loss = 1.16 (976.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:21:34.885878: step 4140, loss = 1.11 (988.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:21:36.178449: step 4150, loss = 1.21 (990.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:21:37.470208: step 4160, loss = 1.06 (990.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:21:38.783682: step 4170, loss = 1.17 (974.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:21:40.079662: step 4180, loss = 1.18 (987.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:21:41.376352: step 4190, loss = 1.08 (987.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:21:42.767309: step 4200, loss = 1.36 (920.2 examples/sec; 0.139 sec/batch)
2017-05-07 20:21:43.971394: step 4210, loss = 1.34 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-07 20:21:45.263520: step 4220, loss = 1.21 (990.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:21:46.545533: step 4230, loss = 1.24 (998.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:21:47.838351: step 4240, loss = 0.97 (990.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:21:49.114610: step 4250, loss = 1.08 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:21:50.412025: step 4260, loss = 1.27 (986.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:21:51.707487: step 4270, loss = 1.36 (988.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:21:52.994135: step 4280, loss = 0.90 (994.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:21:54.277887: step 4290, loss = 0.92 (997.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:21:55.671447: step 4300, loss = 0.98 (918.5 examples/sec; 0.139 sec/batch)
2017-05-07 20:21:56.858207: step 4310, loss = 1.08 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-07 20:21:58.142034: step 4320, loss = 1.10 (997.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:21:59.435553: step 4330, loss = 1.08 (989.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:22:00.737754: step 4340, loss = 1.20 (982.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:22:02.040495: step 4350, loss = 1.14 (982.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:22:03.352602: step 4360, loss = 1.09 (975.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:22:04.625475: step 4370, loss = 1.14 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:22:05.905468: step 4380, loss = 0.99 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:22:07.180203: step 4390, loss = 1.00 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:22:08.536528: step 4400, loss = 0.97 (943.7 examples/sec; 0.136 sec/batch)
2017-05-07 20:22:09.729166: step 4410, loss = 1.01 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-07 20:22:11.001096: step 4420, loss = 1.08 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:22:12.293683: step 4430, loss = 0.93 (990.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:22:13.582495: step 4440, loss = 1.41 (993.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:22:14.853255: step 4450, loss = 1.33 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:22:16.162203: step 4460, loss = 1.10 (977.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:22:17.438289: step 4470, loss = 1.12 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:22:18.729825: step 4480, loss = 1.38 (991.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:22:20.038818: step 4490, loss = 1.20 (977.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:22:21.405485: step 4500, loss = 1.14 (936.6 examples/sec; 0.137 sec/batch)
2017-05-07 20:22:22.599107: step 4510, loss = 1.18 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-07 20:22:23.901193: step 4520, loss = 0.91 (983.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:22:25.200693: step 4530, loss = 0.93 (985.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:22:26.500856: step 4540, loss = 0.91 (984.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:22:27.787819: step 4550, loss = 1.10 (994.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:22:29.089274: step 4560, loss = 1.09 (983.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:22:30.364903: step 4570, loss = 1.02 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:22:31.665164: step 4580, loss = 1.08 (984.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:22:32.943373: step 4590, loss = 1.11 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:22:34.299471: step 4600, loss = 1.05 (943.9 examples/sec; 0.136 sec/batch)
2017-05-07 20:22:35.485004: step 4610, loss = 0.99 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-07 20:22:36.749549: step 4620, loss = 0.97 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:22:38.015775: step 4630, loss = 0.87 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:22:39.310430: step 4640, loss = 1.20 (988.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:22:40.578727: step 4650, loss = 0.99 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:22:41.842097: step 4660, loss = 1.00 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:22:43.156241: step 4670, loss = 1.17 (974.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:22:44.440125: step 4680, loss = 1.00 (997.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:22:45.717202: step 4690, loss = 1.10 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:22:47.075723: step 4700, loss = 1.13 (942.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:22:48.250450: step 4710, loss = 1.08 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-07 20:22:49.494854: step 4720, loss = 1.29 (1028.6 examples/sec; 0.124 sec/batch)
2017-05-07 20:22:50.794400: step 4730, loss = 1.12 (985.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:22:52.053045: step 4740, loss = 0.92 (1017.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:22:53.331039: step 4750, loss = 1.25 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:22:54.602468: step 4760, loss = 1.06 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:22:55.901412: step 4770, loss = 1.20 (985.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:22:57.214637: step 4780, loss = 1.07 (974.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:22:58.513308: step 4790, loss = 0.96 (985.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:22:59.906935: step 4800, loss = 0.98 (918.5 examples/sec; 0.139 sec/batch)
2017-05-07 20:23:01.092528: step 4810, loss = 1.16 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-07 20:23:02.393896: step 4820, loss = 1.11 (983.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:23:03.690350: step 4830, loss = 1.03 (987.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:23:04.979886: step 4840, loss = 1.17 (992.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:23:06.249911: step 4850, loss = 0.98 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:23:07.530470: step 4860, loss = 1.02 (999.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:23:08.809406: step 4870, loss = 0.87 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:23:10.070776: step 4880, loss = 1.17 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:23:11.366473: step 4890, loss = 1.24 (987.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:23:12.743820: step 4900, loss = 0.90 (929.3 examples/sec; 0.138 sec/batch)
2017-05-07 20:23:13.914804: step 4910, loss = 1.04 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-07 20:23:15.205951: step 4920, loss = 1.21 (991.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:23:16.490480: step 4930, loss = 1.27 (996.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:23:17.777253: step 4940, loss = 1.13 (994.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:23:19.068261: step 4950, loss = 0.81 (991.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:23:20.389270: step 4960, loss = 1.38 (969.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:23:21.656702: step 4970, loss = 1.13 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:23:22.932848: step 4980, loss = 1.02 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:23:24.204106: step 4990, loss = 1.24 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:23:25.600061: step 5000, loss = 1.01 (916.9 examples/sec; 0.140 sec/batch)
2017-05-07 20:23:26.805232: step 5010, loss = 1.08 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-07 20:23:28.113626: step 5020, loss = 0.97 (978.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:23:29.388940: step 5030, loss = 0.94 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:23:30.647230: step 5040, loss = 1.17 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:23:31.943742: step 5050, loss = 0.97 (987.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:23:33.231768: step 5060, loss = 1.01 (993.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:23:34.525179: step 5070, loss = 0.99 (989.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:23:35.827077: step 5080, loss = 1.17 (983.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:23:37.124466: step 5090, loss = 1.21 (986.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:23:38.498786: step 5100, loss = 1.27 (931.4 examples/sec; 0.137 sec/batch)
2017-05-07 20:23:39.740320: step 5110, loss = 0.97 (1031.0 examples/sec; 0.124 sec/batch)
2017-05-07 20:23:41.033341: step 5120, loss = 1.02 (989.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:23:42.346355: step 5130, loss = 1.06 (974.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:23:43.644731: step 5140, loss = 1.09 (985.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:23:44.948535: step 5150, loss = 0.90 (981.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:23:46.253352: step 5160, loss = 1.03 (981.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:23:47.576334: step 5170, loss = 0.99 (967.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:23:48.872261: step 5180, loss = 1.12 (987.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:23:50.164500: step 5190, loss = 1.41 (990.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:23:51.567770: step 5200, loss = 1.09 (912.2 examples/sec; 0.140 sec/batch)
2017-05-07 20:23:52.777521: step 5210, loss = 1.21 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-07 20:23:54.041312: step 5220, loss = 1.03 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:23:55.306727: step 5230, loss = 0.94 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:23:56.565597: step 5240, loss = 0.97 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:23:57.828094: step 5250, loss = 1.18 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:23:59.141088: step 5260, loss = 1.08 (974.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:24:00.439395: step 5270, loss = 1.07 (985.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:24:01.720501: step 5280, loss = 1.16 (999.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:24:03.000326: step 5290, loss = 0.96 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:24:04.366659: step 5300, loss = 0.92 (936.8 examples/sec; 0.137 sec/batch)
2017-05-07 20:24:05.539518: step 5310, loss = 1.13 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-07 20:24:06.834656: step 5320, loss = 0.91 (988.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:24:08.119193: step 5330, loss = 1.17 (996.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:24:09.435788: step 5340, loss = 1.02 (972.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:24:10.727396: step 5350, loss = 0.91 (991.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:24:12.012882: step 5360, loss = 1.05 (995.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:24:13.280774: step 5370, loss = 0.88 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:24:14.578085: step 5380, loss = 1.41 (986.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:24:15.871293: step 5390, loss = 1.07 (989.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:24:17.278532: step 5400, loss = 0.98 (909.6 examples/sec; 0.141 sec/batch)
2017-05-07 20:24:18.472959: step 5410, loss = 1.14 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-07 20:24:19.746405: step 5420, loss = 0.91 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:24:21.051811: step 5430, loss = 1.07 (980.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:24:22.322557: step 5440, loss = 0.96 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:24:23.597480: step 5450, loss = 0.97 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:24:24.911726: step 5460, loss = 1.02 (973.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:24:26.186625: step 5470, loss = 0.94 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:24:27.460401: step 5480, loss = 1.13 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:24:28.758322: step 5490, loss = 1.06 (986.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:24:30.151700: step 5500, loss = 1.00 (918.6 examples/sec; 0.139 sec/batch)
2017-05-07 20:24:31.365412: step 5510, loss = 0.95 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-07 20:24:32.652714: step 5520, loss = 0.94 (994.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:24:33.973864: step 5530, loss = 1.09 (968.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:24:35.233214: step 5540, loss = 0.99 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:24:36.507754: step 5550, loss = 1.09 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:24:37.782551: step 5560, loss = 1.10 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:24:39.077583: step 5570, loss = 0.96 (988.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:24:40.356647: step 5580, loss = 0.97 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:24:41.626667: step 5590, loss = 0.92 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:24:43.028730: step 5600, loss = 1.00 (912.9 examples/sec; 0.140 sec/batch)
2017-05-07 20:24:44.230355: step 5610, loss = 1.07 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-07 20:24:45.519373: step 5620, loss = 1.19 (993.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:24:46.784316: step 5630, loss = 0.87 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:24:48.058407: step 5640, loss = 1.14 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:24:49.338601: step 5650, loss = 1.10 (999.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:24:50.636745: step 5660, loss = 0.98 (986.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:24:51.932031: step 5670, loss = 1.05 (988.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:24:53.232689: step 5680, loss = 1.25 (984.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:24:54.505929: step 5690, loss = 1.05 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:24:55.932838: step 5700, loss = 0.90 (897.0 examples/sec; 0.143 sec/batch)
2017-05-07 20:24:57.137601: step 5710, loss = 1.03 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-07 20:24:58.404909: step 5720, loss = 1.20 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:24:59.696611: step 5730, loss = 0.96 (990.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:25:01.005037: step 5740, loss = 1.19 (978.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:25:02.317647: step 5750, loss = 0.89 (975.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:25:03.640126: step 5760, loss = 1.04 (967.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:25:04.913153: step 5770, loss = 0.93 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:25:06.193667: step 5780, loss = 0.87 (999.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:25:07.481138: step 5790, loss = 1.00 (994.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:25:08.832240: step 5800, loss = 0.94 (947.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:25:10.031419: step 5810, loss = 1.09 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-07 20:25:11.309741: step 5820, loss = 0.92 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:25:12.624628: step 5830, loss = 1.04 (973.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:25:13.910296: step 5840, loss = 1.23 (995.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:25:15.230890: step 5850, loss = 0.99 (969.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:25:16.543635: step 5860, loss = 1.13 (975.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:25:17.841729: step 5870, loss = 1.21 (986.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:25:19.126424: step 5880, loss = 1.04 (996.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:25:20.396341: step 5890, loss = 1.04 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:25:21.766595: step 5900, loss = 0.97 (934.1 examples/sec; 0.137 sec/batch)
2017-05-07 20:25:22.975386: step 5910, loss = 1.00 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-07 20:25:24.270551: step 5920, loss = 0.97 (988.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:25:25.558309: step 5930, loss = 1.05 (994.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:25:26.890959: step 5940, loss = 1.22 (960.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:25:28.170037: step 5950, loss = 0.97 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:25:29.463442: step 5960, loss = 1.15 (989.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:25:30.750855: step 5970, loss = 0.92 (994.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:25:32.056378: step 5980, loss = 0.97 (980.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:25:33.371228: step 5990, loss = 0.99 (973.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:25:34.779457: step 6000, loss = 0.99 (908.9 examples/sec; 0.141 sec/batch)
2017-05-07 20:25:35.976248: step 6010, loss = 0.88 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-07 20:25:37.240328: step 6020, loss = 1.06 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:25:38.518732: step 6030, loss = 1.08 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:25:39.809799: step 6040, loss = 0.99 (991.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:25:41.086429: step 6050, loss = 1.08 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:25:42.360321: step 6060, loss = 1.04 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:25:43.632155: step 6070, loss = 1.09 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:25:44.903113: step 6080, loss = 1.07 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:25:46.168851: step 6090, loss = 0.98 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:25:47.548492: step 6100, loss = 1.08 (927.8 examples/sec; 0.138 sec/batch)
2017-05-07 20:25:48.766449: step 6110, loss = 1.09 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-07 20:25:50.030559: step 6120, loss = 1.18 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:25:51.326452: step 6130, loss = 1.08 (987.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:25:52.598806: step 6140, loss = 1.18 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:25:53.881109: step 6150, loss = 1.24 (998.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:25:55.169908: step 6160, loss = 0.91 (993.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:25:56.463007: step 6170, loss = 1.03 (989.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:25:57.735375: step 6180, loss = 0.93 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:25:59.023442: step 6190, loss = 1.15 (993.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:26:00.426414: step 6200, loss = 0.94 (912.4 examples/sec; 0.140 sec/batch)
2017-05-07 20:26:01.606068: step 6210, loss = 1.36 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-07 20:26:02.899412: step 6220, loss = 1.99 (989.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:26:04.167014: step 6230, loss = 1.17 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:26:05.454683: step 6240, loss = 0.85 (994.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:26:06.707448: step 6250, loss = 1.16 (1021.8 examples/sec; 0.125 sec/batch)
2017-05-07 20:26:07.979531: step 6260, loss = 1.02 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:26:09.286418: step 6270, loss = 1.02 (979.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:26:10.603470: step 6280, loss = 1.02 (971.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:26:11.872121: step 6290, loss = 0.95 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:26:13.271612: step 6300, loss = 1.05 (914.6 examples/sec; 0.140 sec/batch)
2017-05-07 20:26:14.472079: step 6310, loss = 1.14 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-07 20:26:15.771694: step 6320, loss = 1.10 (984.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:26:17.054802: step 6330, loss = 0.96 (997.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:26:18.374115: step 6340, loss = 0.85 (970.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:26:19.665703: step 6350, loss = 1.14 (991.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:26:20.968995: step 6360, loss = 0.88 (982.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:26:22.282261: step 6370, loss = 1.14 (974.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:26:23.587206: step 6380, loss = 0.98 (980.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:26:24.897670: step 6390, loss = 0.99 (976.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:26:26.289541: step 6400, loss = 1.01 (919.6 examples/sec; 0.139 sec/batch)
2017-05-07 20:26:27.498216: step 6410, loss = 1.09 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-07 20:26:28.784220: step 6420, loss = 1.08 (995.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:26:30.073382: step 6430, loss = 0.90 (992.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:26:31.339359: step 6440, loss = 1.07 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:26:32.612352: step 6450, loss = 1.02 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:26:33.885704: step 6460, loss = 0.91 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:26:35.168293: step 6470, loss = 0.97 (998.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:26:36.463500: step 6480, loss = 1.19 (988.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:26:37.754500: step 6490, loss = 0.84 (991.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:26:39.151183: step 6500, loss = 0.87 (916.5 examples/sec; 0.140 sec/batch)
2017-05-07 20:26:40.356075: step 6510, loss = 1.01 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-07 20:26:41.644914: step 6520, loss = 0.91 (993.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:26:42.918807: step 6530, loss = 1.08 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:26:44.235373: step 6540, loss = 0.98 (972.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:26:45.528113: step 6550, loss = 1.11 (990.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:26:46.820639: step 6560, loss = 1.03 (990.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:26:48.092464: step 6570, loss = 1.16 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:26:49.371238: step 6580, loss = 1.06 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:26:50.642947: step 6590, loss = 0.97 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:26:52.020186: step 6600, loss = 0.78 (929.4 examples/sec; 0.138 sec/batch)
2017-05-07 20:26:53.198818: step 6610, loss = 0.97 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-07 20:26:54.485912: step 6620, loss = 1.30 (994.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:26:55.781142: step 6630, loss = 1.14 (988.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:26:57.050922: step 6640, loss = 0.90 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:26:58.342486: step 6650, loss = 1.14 (991.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:26:59.649669: step 6660, loss = 0.89 (979.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:27:00.910992: step 6670, loss = 0.92 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:27:02.193738: step 6680, loss = 1.07 (997.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:27:03.530003: step 6690, loss = 1.03 (957.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:27:04.892469: step 6700, loss = 0.90 (939.5 examples/sec; 0.136 sec/batch)
2017-05-07 20:27:06.085412: step 6710, loss = 0.92 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-07 20:27:07.384832: step 6720, loss = 1.33 (985.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:27:08.689370: step 6730, loss = 0.90 (981.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:27:09.969848: step 6740, loss = 0.95 (999.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:27:11.249444: step 6750, loss = 1.00 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:27:12.551359: step 6760, loss = 1.00 (983.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:27:13.855745: step 6770, loss = 1.11 (981.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:27:15.161793: step 6780, loss = 1.17 (980.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:27:16.458081: step 6790, loss = 0.98 (987.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:27:17.844218: step 6800, loss = 0.86 (923.4 examples/sec; 0.139 sec/batch)
2017-05-07 20:27:19.060567: step 6810, loss = 1.16 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-07 20:27:20.351004: step 6820, loss = 0.86 (991.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:27:21.601512: step 6830, loss = 0.84 (1023.6 examples/sec; 0.125 sec/batch)
2017-05-07 20:27:22.894841: step 6840, loss = 0.93 (989.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:27:24.154179: step 6850, loss = 0.90 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:27:25.437742: step 6860, loss = 0.92 (997.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:27:26.720886: step 6870, loss = 0.98 (997.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:27:28.035654: step 6880, loss = 0.96 (973.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:27:29.316608: step 6890, loss = 1.11 (999.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:27:30.694193: step 6900, loss = 0.95 (929.2 examples/sec; 0.138 sec/batch)
2017-05-07 20:27:31.891750: step 6910, loss = 1.10 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-07 20:27:33.187071: step 6920, loss = 0.97 (988.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:27:34.498592: step 6930, loss = 1.08 (976.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:27:35.800038: step 6940, loss = 1.15 (983.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:27:37.091110: step 6950, loss = 0.93 (991.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:27:38.379709: step 6960, loss = 0.97 (993.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:27:39.660273: step 6970, loss = 1.11 (999.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:27:40.943761: step 6980, loss = 1.13 (997.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:27:42.229273: step 6990, loss = 1.10 (995.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:27:43.602375: step 7000, loss = 0.86 (932.2 examples/sec; 0.137 sec/batch)
2017-05-07 20:27:44.773763: step 7010, loss = 1.02 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-07 20:27:46.050019: step 7020, loss = 0.95 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:27:47.362200: step 7030, loss = 0.92 (975.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:27:48.630115: step 7040, loss = 1.00 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:27:49.923805: step 7050, loss = 1.16 (989.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:27:51.247657: step 7060, loss = 1.18 (966.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:27:52.532402: step 7070, loss = 1.02 (996.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:27:53.805059: step 7080, loss = 1.00 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:27:55.090512: step 7090, loss = 1.09 (995.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:27:56.454997: step 7100, loss = 0.98 (938.1 examples/sec; 0.136 sec/batch)
2017-05-07 20:27:57.651040: step 7110, loss = 1.04 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-07 20:27:58.941829: step 7120, loss = 0.85 (991.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:28:00.229571: step 7130, loss = 1.14 (994.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:28:01.539727: step 7140, loss = 1.09 (977.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:28:02.835285: step 7150, loss = 0.90 (988.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:28:04.127884: step 7160, loss = 0.95 (990.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:28:05.404004: step 7170, loss = 1.11 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:28:06.717699: step 7180, loss = 1.35 (974.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:28:08.030617: step 7190, loss = 0.97 (974.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:28:09.392762: step 7200, loss = 1.02 (939.7 examples/sec; 0.136 sec/batch)
2017-05-07 20:28:10.602725: step 7210, loss = 1.03 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-07 20:28:11.918891: step 7220, loss = 0.81 (972.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:28:13.201756: step 7230, loss = 0.90 (997.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:28:14.490705: step 7240, loss = 1.04 (993.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:28:15.789276: step 7250, loss = 1.11 (985.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:28:17.107940: step 7260, loss = 1.00 (970.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:28:18.406304: step 7270, loss = 0.89 (985.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:28:19.721062: step 7280, loss = 1.11 (973.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:28:21.026696: step 7290, loss = 0.99 (980.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:28:22.405186: step 7300, loss = 0.95 (928.6 examples/sec; 0.138 sec/batch)
2017-05-07 20:28:23.590303: step 7310, loss = 1.01 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-07 20:28:24.868353: step 7320, loss = 0.88 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:28:26.137059: step 7330, loss = 0.83 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:28:27.409859: step 7340, loss = 1.07 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:28:28.705793: step 7350, loss = 1.10 (987.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:28:30.015451: step 7360, loss = 0.98 (977.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:28:31.295819: step 7370, loss = 0.92 (999.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:28:32.590291: step 7380, loss = 0.89 (988.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:28:33.873019: step 7390, loss = 0.98 (997.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:28:35.243115: step 7400, loss = 1.00 (934.2 examples/sec; 0.137 sec/batch)
2017-05-07 20:28:36.466004: step 7410, loss = 0.89 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-07 20:28:37.754935: step 7420, loss = 1.22 (993.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:28:39.033706: step 7430, loss = 1.23 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:28:40.299396: step 7440, loss = 1.07 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:28:41.599079: step 7450, loss = 1.08 (984.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:28:42.876511: step 7460, loss = 0.92 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:28:44.143335: step 7470, loss = 0.84 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:28:45.453850: step 7480, loss = 1.01 (976.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:28:46.743001: step 7490, loss = 0.95 (992.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:28:48.110407: step 7500, loss = 0.76 (936.1 examples/sec; 0.137 sec/batch)
2017-05-07 20:28:49.305427: step 7510, loss = 0.98 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-07 20:28:50.578000: step 7520, loss = 1.00 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:28:51.897665: step 7530, loss = 0.95 (969.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:28:53.194577: step 7540, loss = 1.04 (987.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:28:54.453248: step 7550, loss = 0.96 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:28:55.729121: step 7560, loss = 1.02 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:28:57.050654: step 7570, loss = 0.89 (968.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:28:58.341154: step 7580, loss = 1.04 (991.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:28:59.624752: step 7590, loss = 0.78 (997.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:29:01.018235: step 7600, loss = 1.07 (918.6 examples/sec; 0.139 sec/batch)
2017-05-07 20:29:02.240502: step 7610, loss = 1.14 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-07 20:29:03.532086: step 7620, loss = 1.00 (991.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:29:04.818385: step 7630, loss = 1.03 (995.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:29:06.089844: step 7640, loss = 0.83 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:29:07.365210: step 7650, loss = 1.11 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:29:08.645547: step 7660, loss = 0.93 (999.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:29:09.926511: step 7670, loss = 1.02 (999.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:29:11.228364: step 7680, loss = 1.08 (983.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:29:12.524660: step 7690, loss = 1.00 (987.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:29:13.896410: step 7700, loss = 0.92 (933.1 examples/sec; 0.137 sec/batch)
2017-05-07 20:29:15.100564: step 7710, loss = 1.02 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-07 20:29:16.408373: step 7720, loss = 1.09 (978.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:29:17.678177: step 7730, loss = 0.85 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:29:18.953617: step 7740, loss = 1.12 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:29:20.288142: step 7750, loss = 1.11 (959.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:29:21.575558: step 7760, loss = 1.00 (994.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:29:22.845845: step 7770, loss = 1.18 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:29:24.159721: step 7780, loss = 1.05 (974.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:29:25.429071: step 7790, loss = 0.89 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:29:26.805911: step 7800, loss = 0.89 (929.7 examples/sec; 0.138 sec/batch)
2017-05-07 20:29:28.019556: step 7810, loss = 0.99 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-07 20:29:29.298706: step 7820, loss = 1.02 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:29:30.595958: step 7830, loss = 1.05 (986.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:29:31.868888: step 7840, loss = 1.05 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:29:33.164797: step 7850, loss = 1.13 (987.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:29:34.458399: step 7860, loss = 1.02 (989.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:29:35.755553: step 7870, loss = 1.02 (986.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:29:37.033924: step 7880, loss = 0.89 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:29:38.333838: step 7890, loss = 0.92 (984.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:29:39.694687: step 7900, loss = 0.89 (940.6 examples/sec; 0.136 sec/batch)
2017-05-07 20:29:40.884221: step 7910, loss = 0.86 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-07 20:29:42.181239: step 7920, loss = 0.96 (986.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:29:43.502474: step 7930, loss = 1.05 (968.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:29:44.811237: step 7940, loss = 1.10 (978.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:29:46.082448: step 7950, loss = 1.04 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:29:47.395523: step 7960, loss = 1.11 (974.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:29:48.709284: step 7970, loss = 1.00 (974.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:29:49.995190: step 7980, loss = 1.11 (995.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:29:51.289958: step 7990, loss = 1.03 (988.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:29:52.679733: step 8000, loss = 1.15 (921.0 examples/sec; 0.139 sec/batch)
2017-05-07 20:29:53.892439: step 8010, loss = 0.91 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-07 20:29:55.172006: step 8020, loss = 1.01 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:29:56.450606: step 8030, loss = 0.98 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:29:57.746003: step 8040, loss = 0.94 (988.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:29:59.063424: step 8050, loss = 0.94 (971.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:30:00.358662: step 8060, loss = 0.91 (988.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:30:01.623443: step 8070, loss = 0.72 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:30:02.903758: step 8080, loss = 1.04 (999.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:30:04.194632: step 8090, loss = 1.03 (991.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:30:05.565792: step 8100, loss = 1.15 (933.5 examples/sec; 0.137 sec/batch)
2017-05-07 20:30:06.764399: step 8110, loss = 1.21 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-07 20:30:08.076580: step 8120, loss = 1.13 (975.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:30:09.362523: step 8130, loss = 1.00 (995.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:30:10.653719: step 8140, loss = 0.89 (991.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:30:11.970887: step 8150, loss = 1.11 (971.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:30:13.258062: step 8160, loss = 0.78 (994.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:30:14.527374: step 8170, loss = 0.84 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:30:15.846579: step 8180, loss = 0.93 (970.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:30:17.167316: step 8190, loss = 0.81 (969.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:30:18.543421: step 8200, loss = 0.88 (930.2 examples/sec; 0.138 sec/batch)
2017-05-07 20:30:19.712345: step 8210, loss = 0.98 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-07 20:30:21.000911: step 8220, loss = 1.02 (993.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:30:22.273406: step 8230, loss = 1.01 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:30:23.554298: step 8240, loss = 0.97 (999.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:30:24.844479: step 8250, loss = 0.95 (992.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:30:26.144490: step 8260, loss = 1.25 (984.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:30:27.445497: step 8270, loss = 0.81 (983.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:30:28.725310: step 8280, loss = 0.81 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:30:30.016931: step 8290, loss = 1.24 (991.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:30:31.418502: step 8300, loss = 1.04 (913.3 examples/sec; 0.140 sec/batch)
2017-05-07 20:30:32.620694: step 8310, loss = 0.97 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-07 20:30:33.928172: step 8320, loss = 0.84 (979.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:30:35.221840: step 8330, loss = 0.89 (989.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:30:36.497421: step 8340, loss = 0.95 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:30:37.776990: step 8350, loss = 1.03 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:30:39.080503: step 8360, loss = 1.11 (981.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:30:40.376499: step 8370, loss = 0.93 (987.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:30:41.651279: step 8380, loss = 0.91 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:30:42.939236: step 8390, loss = 0.98 (994.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:30:44.328401: step 8400, loss = 1.01 (921.2 examples/sec; 0.139 sec/batch)
2017-05-07 20:30:45.522814: step 8410, loss = 1.09 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-07 20:30:46.837264: step 8420, loss = 0.97 (973.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:30:48.150605: step 8430, loss = 1.26 (974.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:30:49.436577: step 8440, loss = 0.88 (995.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:30:50.750433: step 8450, loss = 0.86 (974.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:30:52.054948: step 8460, loss = 1.12 (981.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:30:53.349930: step 8470, loss = 0.79 (988.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:30:54.610231: step 8480, loss = 1.05 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:30:55.916525: step 8490, loss = 0.93 (979.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:30:57.293630: step 8500, loss = 1.02 (929.5 examples/sec; 0.138 sec/batch)
2017-05-07 20:30:58.485174: step 8510, loss = 0.83 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-07 20:30:59.766650: step 8520, loss = 0.77 (998.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:31:01.057774: step 8530, loss = 1.05 (991.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:31:02.378460: step 8540, loss = 0.97 (969.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:31:03.671347: step 8550, loss = 1.03 (990.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:31:04.969438: step 8560, loss = 0.86 (986.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:31:06.283462: step 8570, loss = 1.00 (974.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:31:07.577898: step 8580, loss = 0.97 (988.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:31:08.856291: step 8590, loss = 1.11 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:31:10.208287: step 8600, loss = 1.09 (946.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:31:11.400231: step 8610, loss = 1.00 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-07 20:31:12.656412: step 8620, loss = 0.83 (1019.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:31:13.936016: step 8630, loss = 1.08 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:31:15.214252: step 8640, loss = 0.89 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:31:16.495000: step 8650, loss = 1.02 (999.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:31:17.758339: step 8660, loss = 0.79 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:31:19.052426: step 8670, loss = 0.85 (989.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:31:20.334685: step 8680, loss = 0.85 (998.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:31:21.631236: step 8690, loss = 1.03 (987.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:31:23.014479: step 8700, loss = 0.96 (925.4 examples/sec; 0.138 sec/batch)
2017-05-07 20:31:24.224239: step 8710, loss = 1.02 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-07 20:31:25.527604: step 8720, loss = 1.02 (982.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:31:26.804468: step 8730, loss = 1.05 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:31:28.068632: step 8740, loss = 0.87 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:31:29.356205: step 8750, loss = 0.88 (994.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:31:30.630667: step 8760, loss = 0.93 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:31:31.916676: step 8770, loss = 0.80 (995.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:31:33.230031: step 8780, loss = 1.29 (974.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:31:34.532939: step 8790, loss = 1.02 (982.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:31:35.916139: step 8800, loss = 1.03 (925.4 examples/sec; 0.138 sec/batch)
2017-05-07 20:31:37.129919: step 8810, loss = 0.93 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-07 20:31:38.428335: step 8820, loss = 1.19 (985.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:31:39.738268: step 8830, loss = 1.07 (977.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:31:41.052630: step 8840, loss = 1.09 (973.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:31:42.324208: step 8850, loss = 0.99 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:31:43.595458: step 8860, loss = 1.05 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:31:44.873106: step 8870, loss = 0.87 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:31:46.150513: step 8880, loss = 0.88 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:31:47.445215: step 8890, loss = 1.28 (988.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:31:48.820809: step 8900, loss = 0.84 (930.5 examples/sec; 0.138 sec/batch)
2017-05-07 20:31:49.985645: step 8910, loss = 0.95 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-07 20:31:51.285105: step 8920, loss = 0.90 (985.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:31:52.579917: step 8930, loss = 1.02 (988.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:31:53.842697: step 8940, loss = 1.08 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:31:55.130506: step 8950, loss = 1.02 (993.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:31:56.411309: step 8960, loss = 0.91 (999.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:31:57.699143: step 8970, loss = 0.92 (993.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:31:58.972188: step 8980, loss = 0.97 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:32:00.291180: step 8990, loss = 1.06 (970.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:32:01.667634: step 9000, loss = 1.21 (929.9 examples/sec; 0.138 sec/batch)
2017-05-07 20:32:02.865279: step 9010, loss = 1.02 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-07 20:32:04.153094: step 9020, loss = 0.85 (993.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:32:05.433376: step 9030, loss = 0.87 (999.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:32:06.714463: step 9040, loss = 1.01 (999.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:32:08.026509: step 9050, loss = 0.97 (975.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:32:09.311935: step 9060, loss = 1.02 (995.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:32:10.600421: step 9070, loss = 0.90 (993.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:32:11.919493: step 9080, loss = 1.03 (970.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:32:13.231490: step 9090, loss = 1.03 (975.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:32:14.604421: step 9100, loss = 0.82 (932.3 examples/sec; 0.137 sec/batch)
2017-05-07 20:32:15.801995: step 9110, loss = 0.86 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-07 20:32:17.085489: step 9120, loss = 0.96 (997.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:32:18.376234: step 9130, loss = 0.83 (991.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:32:19.668515: step 9140, loss = 1.01 (990.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:32:20.974058: step 9150, loss = 0.98 (980.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:32:22.270481: step 9160, loss = 1.00 (987.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:32:23.564917: step 9170, loss = 0.92 (988.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:32:24.854795: step 9180, loss = 1.20 (992.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:32:26.145199: step 9190, loss = 1.02 (991.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:32:27.531263: step 9200, loss = 0.93 (923.5 examples/sec; 0.139 sec/batch)
2017-05-07 20:32:28.752870: step 9210, loss = 1.08 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-07 20:32:30.011615: step 9220, loss = 0.98 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:32:31.293178: step 9230, loss = 0.88 (998.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:32:32.574102: step 9240, loss = 0.95 (999.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:32:33.845427: step 9250, loss = 0.85 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:32:35.129512: step 9260, loss = 0.77 (996.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:32:36.434708: step 9270, loss = 0.99 (980.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:32:37.743435: step 9280, loss = 0.90 (978.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:32:39.026026: step 9290, loss = 0.80 (998.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:32:40.442191: step 9300, loss = 0.96 (903.8 examples/sec; 0.142 sec/batch)
2017-05-07 20:32:41.651629: step 9310, loss = 1.01 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-07 20:32:43.019272: step 9320, loss = 1.00 (935.9 examples/sec; 0.137 sec/batch)
2017-05-07 20:32:44.285654: step 9330, loss = 1.24 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:32:45.568722: step 9340, loss = 0.97 (997.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:32:46.826739: step 9350, loss = 0.93 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:32:48.100412: step 9360, loss = 1.12 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:32:49.382258: step 9370, loss = 1.21 (998.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:32:50.678426: step 9380, loss = 1.03 (987.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:32:51.970675: step 9390, loss = 0.89 (990.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:32:53.345842: step 9400, loss = 0.83 (930.8 examples/sec; 0.138 sec/batch)
2017-05-07 20:32:54.556423: step 9410, loss = 1.11 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-07 20:32:55.869868: step 9420, loss = 1.14 (974.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:32:57.137705: step 9430, loss = 1.08 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:32:58.418319: step 9440, loss = 0.98 (999.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:32:59.707357: step 9450, loss = 0.85 (993.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:33:00.981548: step 9460, loss = 0.97 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:33:02.259907: step 9470, loss = 0.88 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:33:03.543186: step 9480, loss = 0.85 (997.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:33:04.827239: step 9490, loss = 1.09 (996.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:33:06.209356: step 9500, loss = 0.91 (926.1 examples/sec; 0.138 sec/batch)
2017-05-07 20:33:07.395705: step 9510, loss = 0.96 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-07 20:33:08.650095: step 9520, loss = 0.80 (1020.4 examples/sec; 0.125 sec/batch)
2017-05-07 20:33:09.910192: step 9530, loss = 0.87 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:33:11.176685: step 9540, loss = 0.95 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:33:12.461752: step 9550, loss = 0.88 (996.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:33:13.720266: step 9560, loss = 0.87 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:33:14.993887: step 9570, loss = 1.02 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:33:16.280951: step 9580, loss = 0.98 (994.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:33:17.556886: step 9590, loss = 1.02 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:33:18.979496: step 9600, loss = 0.81 (899.8 examples/sec; 0.142 sec/batch)
2017-05-07 20:33:20.187882: step 9610, loss = 1.09 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-07 20:33:21.472078: step 9620, loss = 0.84 (996.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:33:22.726200: step 9630, loss = 1.25 (1020.6 examples/sec; 0.125 sec/batch)
2017-05-07 20:33:23.996572: step 9640, loss = 0.89 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:33:25.298394: step 9650, loss = 1.06 (983.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:33:26.592647: step 9660, loss = 0.84 (989.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:33:27.869528: step 9670, loss = 0.85 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:33:29.182054: step 9680, loss = 0.92 (975.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:33:30.479288: step 9690, loss = 1.13 (986.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:33:31.859137: step 9700, loss = 0.88 (927.6 examples/sec; 0.138 sec/batch)
2017-05-07 20:33:33.047041: step 9710, loss = 1.07 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-07 20:33:34.335436: step 9720, loss = 0.94 (993.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:33:35.613819: step 9730, loss = 1.06 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:33:36.886474: step 9740, loss = 0.83 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:33:38.193540: step 9750, loss = 1.12 (979.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:33:39.525055: step 9760, loss = 1.00 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:33:40.810863: step 9770, loss = 0.99 (995.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:33:42.100276: step 9780, loss = 1.16 (992.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:33:43.380655: step 9790, loss = 1.00 (999.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:33:44.760874: step 9800, loss = 0.86 (927.4 examples/sec; 0.138 sec/batch)
2017-05-07 20:33:45.966091: step 9810, loss = 0.97 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-07 20:33:47.243631: step 9820, loss = 0.98 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:33:48.528005: step 9830, loss = 1.04 (996.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:33:49.797006: step 9840, loss = 0.80 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:33:51.108902: step 9850, loss = 0.79 (975.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:33:52.406367: step 9860, loss = 0.93 (986.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:33:53.694719: step 9870, loss = 0.85 (993.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:33:54.963892: step 9880, loss = 0.95 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:33:56.237680: step 9890, loss = 0.81 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:33:57.619275: step 9900, loss = 0.97 (926.5 examples/sec; 0.138 sec/batch)
2017-05-07 20:33:58.837504: step 9910, loss = 0.98 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-07 20:34:00.142074: step 9920, loss = 1.12 (981.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:34:01.428553: step 9930, loss = 1.01 (995.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:34:02.705040: step 9940, loss = 0.77 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:34:03.985637: step 9950, loss = 0.96 (999.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:34:05.273447: step 9960, loss = 0.98 (993.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:34:06.551084: step 9970, loss = 1.03 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:34:07.836999: step 9980, loss = 0.90 (995.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:34:09.126013: step 9990, loss = 0.94 (993.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:34:10.513276: step 10000, loss = 1.13 (922.7 examples/sec; 0.139 sec/batch)
2017-05-07 20:34:11.721406: step 10010, loss = 1.10 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-07 20:34:12.993835: step 10020, loss = 0.97 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:34:14.266166: step 10030, loss = 0.94 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:34:15.557767: step 10040, loss = 1.02 (991.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:34:16.864042: step 10050, loss = 0.90 (979.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:34:18.149168: step 10060, loss = 0.88 (996.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:34:19.418216: step 10070, loss = 0.93 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:34:20.721400: step 10080, loss = 0.91 (982.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:34:21.984150: step 10090, loss = 0.93 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:34:23.392320: step 10100, loss = 0.99 (909.0 examples/sec; 0.141 sec/batch)
2017-05-07 20:34:24.585333: step 10110, loss = 0.95 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-07 20:34:25.891355: step 10120, loss = 1.01 (980.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:34:27.174449: step 10130, loss = 1.10 (997.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:34:28.481442: step 10140, loss = 0.87 (979.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:34:29.750620: step 10150, loss = 0.92 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:34:31.041137: step 10160, loss = 1.18 (991.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:34:32.324231: step 10170, loss = 0.87 (997.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:34:33.596436: step 10180, loss = 1.03 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:34:34.875822: step 10190, loss = 0.92 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:34:36.261822: step 10200, loss = 0.83 (923.5 examples/sec; 0.139 sec/batch)
2017-05-07 20:34:37.489929: step 10210, loss = 0.96 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-07 20:34:38.796396: step 10220, loss = 1.31 (979.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:34:40.089371: step 10230, loss = 0.93 (990.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:34:41.385881: step 10240, loss = 0.85 (987.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:34:42.652316: step 10250, loss = 0.96 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:34:43.940281: step 10260, loss = 0.87 (993.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:34:45.242167: step 10270, loss = 1.04 (983.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:34:46.571708: step 10280, loss = 1.12 (962.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:34:47.863025: step 10290, loss = 0.97 (991.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:34:49.231012: step 10300, loss = 1.00 (935.7 examples/sec; 0.137 sec/batch)
2017-05-07 20:34:50.414213: step 10310, loss = 0.91 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-07 20:34:51.682587: step 10320, loss = 1.10 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:34:52.956813: step 10330, loss = 1.07 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:34:54.224637: step 10340, loss = 0.78 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:34:55.505494: step 10350, loss = 0.96 (999.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:34:56.826795: step 10360, loss = 1.07 (968.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:34:58.117801: step 10370, loss = 0.83 (991.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:34:59.412805: step 10380, loss = 1.09 (988.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:35:00.686407: step 10390, loss = 0.83 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:35:02.074799: step 10400, loss = 0.86 (921.9 examples/sec; 0.139 sec/batch)
2017-05-07 20:35:03.278581: step 10410, loss = 0.86 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-07 20:35:04.548681: step 10420, loss = 0.82 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:35:05.824259: step 10430, loss = 1.20 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:35:07.103480: step 10440, loss = 1.02 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:35:08.424151: step 10450, loss = 1.08 (969.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:35:09.707272: step 10460, loss = 0.97 (997.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:35:10.984118: step 10470, loss = 1.11 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:35:12.263215: step 10480, loss = 1.06 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:35:13.555937: step 10490, loss = 0.93 (990.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:35:14.930396: step 10500, loss = 0.83 (931.3 examples/sec; 0.137 sec/batch)
2017-05-07 20:35:16.107369: step 10510, loss = 1.08 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-07 20:35:17.386627: step 10520, loss = 0.91 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:35:18.670910: step 10530, loss = 0.90 (996.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:35:19.973701: step 10540, loss = 1.06 (982.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:35:21.257860: step 10550, loss = 1.03 (996.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:35:22.519594: step 10560, loss = 0.88 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:35:23.788257: step 10570, loss = 0.89 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:35:25.086329: step 10580, loss = 0.93 (986.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:35:26.394089: step 10590, loss = 0.95 (978.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:35:27.788547: step 10600, loss = 0.91 (917.9 examples/sec; 0.139 sec/batch)
2017-05-07 20:35:28.969729: step 10610, loss = 0.98 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-07 20:35:30.245402: step 10620, loss = 0.94 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:35:31.536258: step 10630, loss = 0.99 (991.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:35:32.844424: step 10640, loss = 0.88 (978.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:35:34.146273: step 10650, loss = 1.01 (983.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:35:35.417581: step 10660, loss = 0.98 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:35:36.689481: step 10670, loss = 0.95 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:35:37.969229: step 10680, loss = 0.90 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:35:39.258934: step 10690, loss = 1.25 (992.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:35:40.660425: step 10700, loss = 1.02 (913.3 examples/sec; 0.140 sec/batch)
2017-05-07 20:35:41.835312: step 10710, loss = 0.68 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-07 20:35:43.119509: step 10720, loss = 0.83 (996.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:35:44.397224: step 10730, loss = 0.93 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:35:45.677379: step 10740, loss = 1.16 (999.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:35:46.954187: step 10750, loss = 0.95 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:35:48.227162: step 10760, loss = 1.11 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:35:49.492127: step 10770, loss = 0.86 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:35:50.786006: step 10780, loss = 0.82 (989.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:35:52.107187: step 10790, loss = 0.93 (968.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:35:53.492423: step 10800, loss = 0.97 (924.0 examples/sec; 0.139 sec/batch)
2017-05-07 20:35:54.691747: step 10810, loss = 0.82 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-07 20:35:55.975856: step 10820, loss = 0.93 (996.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:35:57.263674: step 10830, loss = 0.81 (993.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:35:58.553437: step 10840, loss = 1.00 (992.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:35:59.814926: step 10850, loss = 0.85 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:36:01.111152: step 10860, loss = 0.81 (987.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:36:02.393957: step 10870, loss = 0.85 (997.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:36:03.701447: step 10880, loss = 0.93 (979.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:36:04.992914: step 10890, loss = 1.27 (991.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:36:06.358717: step 10900, loss = 0.89 (937.2 examples/sec; 0.137 sec/batch)
2017-05-07 20:36:07.561990: step 10910, loss = 1.06 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-07 20:36:08.844033: step 10920, loss = 1.06 (998.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:36:10.152283: step 10930, loss = 1.17 (978.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:36:11.453912: step 10940, loss = 0.98 (983.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:36:12.741280: step 10950, loss = 0.95 (994.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:36:14.012161: step 10960, loss = 0.87 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:36:15.266096: step 10970, loss = 0.95 (1020.8 examples/sec; 0.125 sec/batch)
2017-05-07 20:36:16.538884: step 10980, loss = 0.89 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:36:17.826270: step 10990, loss = 0.93 (994.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:36:19.203934: step 11000, loss = 0.91 (929.1 examples/sec; 0.138 sec/batch)
2017-05-07 20:36:20.378554: step 11010, loss = 0.84 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-07 20:36:21.664542: step 11020, loss = 1.05 (995.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:36:22.942279: step 11030, loss = 1.01 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:36:24.222769: step 11040, loss = 0.91 (999.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:36:25.504097: step 11050, loss = 0.86 (999.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:36:26.791260: step 11060, loss = 0.98 (994.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:36:28.100747: step 11070, loss = 1.30 (977.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:36:29.416444: step 11080, loss = 1.27 (972.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:36:30.713770: step 11090, loss = 0.97 (986.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:36:32.107482: step 11100, loss = 0.95 (918.4 examples/sec; 0.139 sec/batch)
2017-05-07 20:36:33.286639: step 11110, loss = 0.90 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-07 20:36:34.567528: step 11120, loss = 1.01 (999.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:36:35.847379: step 11130, loss = 1.14 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:36:37.127535: step 11140, loss = 0.90 (999.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:36:38.435770: step 11150, loss = 0.92 (978.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:36:39.717430: step 11160, loss = 1.04 (998.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:36:40.989645: step 11170, loss = 0.90 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:36:42.262392: step 11180, loss = 0.95 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:36:43.529635: step 11190, loss = 0.92 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:36:44.905137: step 11200, loss = 0.91 (930.6 examples/sec; 0.138 sec/batch)
2017-05-07 20:36:46.084527: step 11210, loss = 0.96 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-07 20:36:47.361016: step 11220, loss = 0.80 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:36:48.636074: step 11230, loss = 0.97 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:36:49.912655: step 11240, loss = 0.91 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:36:51.221868: step 11250, loss = 0.94 (977.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:36:52.516498: step 11260, loss = 0.87 (988.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:36:53.824282: step 11270, loss = 1.05 (978.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:36:55.118640: step 11280, loss = 1.01 (988.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:36:56.399822: step 11290, loss = 0.92 (999.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:36:57.804642: step 11300, loss = 0.96 (911.2 examples/sec; 0.140 sec/batch)
2017-05-07 20:36:58.992895: step 11310, loss = 0.77 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-07 20:37:00.314562: step 11320, loss = 1.01 (968.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:37:01.592006: step 11330, loss = 0.86 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:37:02.869456: step 11340, loss = 0.98 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:37:04.139535: step 11350, loss = 0.98 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:37:05.417603: step 11360, loss = 0.92 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:37:06.709282: step 11370, loss = 0.86 (991.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:37:07.972825: step 11380, loss = 1.01 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:37:09.241300: step 11390, loss = 1.10 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:37:10.618803: step 11400, loss = 0.98 (929.2 examples/sec; 0.138 sec/batch)
2017-05-07 20:37:11.802796: step 11410, loss = 0.91 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-07 20:37:13.074571: step 11420, loss = 0.89 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:37:14.377476: step 11430, loss = 0.93 (982.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:37:15.678393: step 11440, loss = 1.18 (983.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:37:16.995118: step 11450, loss = 0.94 (972.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:37:18.305950: step 11460, loss = 1.03 (976.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:37:19.619821: step 11470, loss = 1.14 (974.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:37:20.927124: step 11480, loss = 1.01 (979.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:37:22.208836: step 11490, loss = 0.84 (998.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:37:23.597219: step 11500, loss = 0.85 (921.9 examples/sec; 0.139 sec/batch)
2017-05-07 20:37:24.796087: step 11510, loss = 0.89 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-07 20:37:26.070221: step 11520, loss = 0.92 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:37:27.369548: step 11530, loss = 0.89 (985.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:37:28.672139: step 11540, loss = 1.15 (982.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:37:29.969694: step 11550, loss = 0.90 (986.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:37:31.265516: step 11560, loss = 1.23 (987.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:37:32.544440: step 11570, loss = 0.95 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:37:33.818954: step 11580, loss = 0.80 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:37:35.109214: step 11590, loss = 1.00 (992.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:37:36.506605: step 11600, loss = 1.02 (916.0 examples/sec; 0.140 sec/batch)
2017-05-07 20:37:37.702449: step 11610, loss = 1.07 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-07 20:37:38.998397: step 11620, loss = 1.03 (987.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:37:40.294123: step 11630, loss = 0.87 (987.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:37:41.574803: step 11640, loss = 1.06 (999.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:37:42.860454: step 11650, loss = 0.95 (995.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:37:44.159170: step 11660, loss = 0.98 (985.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:37:45.437844: step 11670, loss = 0.87 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:37:46.707343: step 11680, loss = 0.89 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:37:48.011885: step 11690, loss = 1.08 (981.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:37:49.386671: step 11700, loss = 0.96 (931.1 examples/sec; 0.137 sec/batch)
2017-05-07 20:37:50.558057: step 11710, loss = 0.75 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-07 20:37:51.819700: step 11720, loss = 0.94 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:37:53.103070: step 11730, loss = 0.94 (997.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:37:54.406300: step 11740, loss = 1.37 (982.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:37:55.702387: step 11750, loss = 1.24 (987.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:37:56.982273: step 11760, loss = 1.04 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:37:58.248931: step 11770, loss = 0.86 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:37:59.532887: step 11780, loss = 0.82 (996.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:38:00.812518: step 11790, loss = 0.90 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:38:02.171523: step 11800, loss = 1.06 (941.9 examples/sec; 0.136 sec/batch)
2017-05-07 20:38:03.389637: step 11810, loss = 0.91 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-07 20:38:04.686040: step 11820, loss = 0.71 (987.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:38:05.987702: step 11830, loss = 0.87 (983.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:38:07.258037: step 11840, loss = 0.98 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:38:08.540790: step 11850, loss = 0.84 (997.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:38:09.843199: step 11860, loss = 1.04 (982.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:38:11.182429: step 11870, loss = 0.89 (955.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:38:12.459382: step 11880, loss = 0.94 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:38:13.746365: step 11890, loss = 1.06 (994.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:38:15.110314: step 11900, loss = 0.95 (938.4 examples/sec; 0.136 sec/batch)
2017-05-07 20:38:16.314894: step 11910, loss = 1.05 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-07 20:38:17.575546: step 11920, loss = 1.09 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:38:18.855572: step 11930, loss = 1.07 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:38:20.129262: step 11940, loss = 0.93 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:38:21.412612: step 11950, loss = 0.95 (997.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:38:22.700862: step 11960, loss = 1.10 (993.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:38:24.008030: step 11970, loss = 1.07 (979.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:38:25.336947: step 11980, loss = 1.02 (963.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:38:26.623778: step 11990, loss = 0.82 (994.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:38:27.992923: step 12000, loss = 1.13 (934.9 examples/sec; 0.137 sec/batch)
2017-05-07 20:38:29.185201: step 12010, loss = 0.90 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-07 20:38:30.458892: step 12020, loss = 0.98 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:38:31.790504: step 12030, loss = 0.95 (961.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:38:33.086226: step 12040, loss = 1.08 (987.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:38:34.372947: step 12050, loss = 0.92 (994.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:38:35.678274: step 12060, loss = 0.96 (980.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:38:36.963608: step 12070, loss = 1.05 (995.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:38:38.236952: step 12080, loss = 0.93 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:38:39.521044: step 12090, loss = 1.00 (996.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:38:40.912044: step 12100, loss = 0.69 (920.2 examples/sec; 0.139 sec/batch)
2017-05-07 20:38:42.132534: step 12110, loss = 0.80 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-07 20:38:43.460081: step 12120, loss = 0.86 (964.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:38:44.722778: step 12130, loss = 1.11 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:38:46.003341: step 12140, loss = 0.79 (999.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:38:47.275210: step 12150, loss = 0.77 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:38:48.556783: step 12160, loss = 0.86 (998.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:38:49.826125: step 12170, loss = 0.95 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:38:51.111022: step 12180, loss = 0.93 (996.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:38:52.400612: step 12190, loss = 1.07 (992.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:38:53.784817: step 12200, loss = 0.89 (924.7 examples/sec; 0.138 sec/batch)
2017-05-07 20:38:54.998553: step 12210, loss = 0.92 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-07 20:38:56.273603: step 12220, loss = 0.94 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:38:57.554381: step 12230, loss = 1.05 (999.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:38:58.834614: step 12240, loss = 0.90 (999.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:39:00.113137: step 12250, loss = 1.01 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:39:01.400544: step 12260, loss = 0.99 (994.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:39:02.709522: step 12270, loss = 1.15 (977.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:39:03.995321: step 12280, loss = 0.77 (995.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:39:05.268615: step 12290, loss = 0.87 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:39:06.631587: step 12300, loss = 1.02 (939.1 examples/sec; 0.136 sec/batch)
2017-05-07 20:39:07.827262: step 12310, loss = 0.87 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-07 20:39:09.130187: step 12320, loss = 0.89 (982.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:39:10.435490: step 12330, loss = 1.02 (980.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:39:11.723189: step 12340, loss = 0.82 (994.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:39:13.009537: step 12350, loss = 0.81 (995.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:39:14.323425: step 12360, loss = 0.98 (974.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:39:15.622041: step 12370, loss = 0.97 (985.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:39:16.936668: step 12380, loss = 0.93 (973.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:39:18.214058: step 12390, loss = 0.80 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:39:19.589022: step 12400, loss = 0.92 (930.9 examples/sec; 0.137 sec/batch)
2017-05-07 20:39:20.810707: step 12410, loss = 0.81 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-07 20:39:22.074117: step 12420, loss = 0.93 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:39:23.343762: step 12430, loss = 0.92 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:39:24.618942: step 12440, loss = 1.13 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:39:25.903269: step 12450, loss = 0.81 (996.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:39:27.197829: step 12460, loss = 1.00 (988.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:39:28.483831: step 12470, loss = 1.06 (995.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:39:29.788272: step 12480, loss = 1.11 (981.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:39:31.062604: step 12490, loss = 0.75 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:39:32.419863: step 12500, loss = 1.04 (943.1 examples/sec; 0.136 sec/batch)
2017-05-07 20:39:33.585999: step 12510, loss = 0.95 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-07 20:39:34.876393: step 12520, loss = 0.98 (991.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:39:36.156003: step 12530, loss = 0.92 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:39:37.432397: step 12540, loss = 0.94 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:39:38.713773: step 12550, loss = 0.97 (998.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:39:39.982454: step 12560, loss = 1.00 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:39:41.268815: step 12570, loss = 0.84 (995.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:39:42.534434: step 12580, loss = 0.90 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:39:43.833828: step 12590, loss = 1.01 (985.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:39:45.207668: step 12600, loss = 0.88 (931.7 examples/sec; 0.137 sec/batch)
2017-05-07 20:39:46.371339: step 12610, loss = 1.04 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-07 20:39:47.651143: step 12620, loss = 0.86 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:39:48.957617: step 12630, loss = 0.84 (979.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:39:50.233585: step 12640, loss = 1.10 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:39:51.498466: step 12650, loss = 0.96 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:39:52.800356: step 12660, loss = 0.91 (983.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:39:54.054643: step 12670, loss = 0.91 (1020.5 examples/sec; 0.125 sec/batch)
2017-05-07 20:39:55.330786: step 12680, loss = 0.77 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:39:56.635803: step 12690, loss = 0.96 (980.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:39:58.019587: step 12700, loss = 1.05 (925.0 examples/sec; 0.138 sec/batch)
2017-05-07 20:39:59.240101: step 12710, loss = 1.10 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-07 20:40:00.537871: step 12720, loss = 0.87 (986.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:40:01.843700: step 12730, loss = 1.20 (980.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:40:03.132595: step 12740, loss = 1.12 (993.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:40:04.424793: step 12750, loss = 0.75 (990.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:40:05.687291: step 12760, loss = 0.96 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:40:06.995875: step 12770, loss = 0.94 (978.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:40:08.289175: step 12780, loss = 1.06 (989.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:40:09.562839: step 12790, loss = 0.87 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:40:10.950203: step 12800, loss = 0.93 (922.6 examples/sec; 0.139 sec/batch)
2017-05-07 20:40:12.132247: step 12810, loss = 0.78 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-07 20:40:13.415522: step 12820, loss = 0.97 (997.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:40:14.706122: step 12830, loss = 0.86 (991.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:40:16.030558: step 12840, loss = 1.00 (966.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:40:17.328944: step 12850, loss = 0.85 (985.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:40:18.600766: step 12860, loss = 0.92 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:40:19.898935: step 12870, loss = 1.13 (986.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:40:21.201109: step 12880, loss = 0.79 (983.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:40:22.480500: step 12890, loss = 0.94 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:40:23.876356: step 12900, loss = 0.99 (917.0 examples/sec; 0.140 sec/batch)
2017-05-07 20:40:25.085953: step 12910, loss = 1.04 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-07 20:40:26.395763: step 12920, loss = 0.84 (977.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:40:27.693546: step 12930, loss = 0.96 (986.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:40:28.961708: step 12940, loss = 1.02 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:40:30.243316: step 12950, loss = 0.85 (998.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:40:31.521763: step 12960, loss = 0.94 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:40:32.810029: step 12970, loss = 1.00 (993.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:40:34.054640: step 12980, loss = 0.79 (1028.4 examples/sec; 0.124 sec/batch)
2017-05-07 20:40:35.325487: step 12990, loss = 0.97 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:40:36.710768: step 13000, loss = 0.83 (924.0 examples/sec; 0.139 sec/batch)
2017-05-07 20:40:37.901434: step 13010, loss = 0.86 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-07 20:40:39.177233: step 13020, loss = 0.87 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:40:40.466656: step 13030, loss = 0.78 (992.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:40:41.728725: step 13040, loss = 1.05 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:40:42.992193: step 13050, loss = 1.10 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:40:44.264391: step 13060, loss = 0.74 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:40:45.524055: step 13070, loss = 1.07 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:40:46.799507: step 13080, loss = 0.82 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:40:48.072749: step 13090, loss = 0.79 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:40:49.458192: step 13100, loss = 0.93 (923.9 examples/sec; 0.139 sec/batch)
2017-05-07 20:40:50.684309: step 13110, loss = 0.92 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-07 20:40:51.965309: step 13120, loss = 0.90 (999.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:40:53.269378: step 13130, loss = 0.98 (981.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:40:54.571317: step 13140, loss = 0.97 (983.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:40:55.867580: step 13150, loss = 0.93 (987.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:40:57.163983: step 13160, loss = 1.05 (987.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:40:58.434539: step 13170, loss = 0.91 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:40:59.714812: step 13180, loss = 1.02 (999.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:41:00.998838: step 13190, loss = 1.02 (996.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:41:02.401541: step 13200, loss = 0.81 (912.5 examples/sec; 0.140 sec/batch)
2017-05-07 20:41:03.607094: step 13210, loss = 0.90 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-07 20:41:04.918794: step 13220, loss = 0.94 (975.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:41:06.208819: step 13230, loss = 0.97 (992.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:41:07.501363: step 13240, loss = 1.27 (990.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:41:08.784090: step 13250, loss = 0.88 (997.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:41:10.033923: step 13260, loss = 1.00 (1024.1 examples/sec; 0.125 sec/batch)
2017-05-07 20:41:11.313081: step 13270, loss = 0.87 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:41:12.600258: step 13280, loss = 0.89 (994.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:41:13.855841: step 13290, loss = 0.95 (1019.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:41:15.217213: step 13300, loss = 0.76 (940.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:41:16.414854: step 13310, loss = 0.95 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-07 20:41:17.685756: step 13320, loss = 0.91 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:41:18.992827: step 13330, loss = 0.96 (979.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:41:20.288783: step 13340, loss = 1.11 (987.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:41:21.557480: step 13350, loss = 0.90 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:41:22.847993: step 13360, loss = 0.96 (991.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:41:24.146612: step 13370, loss = 0.90 (985.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:41:25.432338: step 13380, loss = 1.07 (995.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:41:26.735450: step 13390, loss = 1.11 (982.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:41:28.106251: step 13400, loss = 0.98 (933.8 examples/sec; 0.137 sec/batch)
2017-05-07 20:41:29.308234: step 13410, loss = 0.86 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-07 20:41:30.583272: step 13420, loss = 0.72 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:41:31.849899: step 13430, loss = 0.84 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:41:33.146464: step 13440, loss = 0.89 (987.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:41:34.461938: step 13450, loss = 0.89 (973.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:41:35.732885: step 13460, loss = 1.01 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:41:37.035724: step 13470, loss = 1.00 (982.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:41:38.312439: step 13480, loss = 0.94 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:41:39.600812: step 13490, loss = 0.95 (993.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:41:40.986008: step 13500, loss = 0.92 (924.1 examples/sec; 0.139 sec/batch)
2017-05-07 20:41:42.162457: step 13510, loss = 1.08 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-07 20:41:43.441010: step 13520, loss = 0.98 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:41:44.729780: step 13530, loss = 1.41 (993.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:41:46.029098: step 13540, loss = 1.03 (985.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:41:47.335636: step 13550, loss = 0.82 (979.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:41:48.645530: step 13560, loss = 0.78 (977.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:41:49.942561: step 13570, loss = 1.07 (986.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:41:51.212576: step 13580, loss = 0.91 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:41:52.474097: step 13590, loss = 0.94 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:41:53.864409: step 13600, loss = 0.91 (920.7 examples/sec; 0.139 sec/batch)
2017-05-07 20:41:55.048251: step 13610, loss = 0.99 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-07 20:41:56.346188: step 13620, loss = 0.80 (986.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:41:57.653571: step 13630, loss = 0.82 (979.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:41:58.946486: step 13640, loss = 0.87 (990.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:42:00.239183: step 13650, loss = 0.90 (990.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:42:01.507463: step 13660, loss = 0.88 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:42:02.782661: step 13670, loss = 0.83 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:42:04.084767: step 13680, loss = 0.84 (983.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:42:05.388719: step 13690, loss = 0.84 (981.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:42:06.767339: step 13700, loss = 0.90 (928.5 examples/sec; 0.138 sec/batch)
2017-05-07 20:42:07.987376: step 13710, loss = 0.93 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-07 20:42:09.257581: step 13720, loss = 0.72 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:42:10.534574: step 13730, loss = 1.03 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:42:11.814207: step 13740, loss = 0.77 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:42:13.143057: step 13750, loss = 0.95 (963.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:42:14.422622: step 13760, loss = 0.90 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:42:15.688310: step 13770, loss = 0.94 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:42:16.947985: step 13780, loss = 0.85 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:42:18.233612: step 13790, loss = 0.87 (995.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:42:19.618376: step 13800, loss = 0.84 (924.3 examples/sec; 0.138 sec/batch)
2017-05-07 20:42:20.823817: step 13810, loss = 0.86 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-07 20:42:22.089728: step 13820, loss = 0.83 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:42:23.371175: step 13830, loss = 1.02 (998.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:42:24.667876: step 13840, loss = 0.97 (987.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:42:25.971012: step 13850, loss = 1.11 (982.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:42:27.266667: step 13860, loss = 0.81 (987.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:42:28.536365: step 13870, loss = 1.08 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:42:29.808083: step 13880, loss = 0.82 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:42:31.071625: step 13890, loss = 0.97 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:42:32.455354: step 13900, loss = 0.90 (925.0 examples/sec; 0.138 sec/batch)
2017-05-07 20:42:33.616355: step 13910, loss = 0.90 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-07 20:42:34.906192: step 13920, loss = 0.85 (992.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:42:36.188614: step 13930, loss = 0.85 (998.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:42:37.480578: step 13940, loss = 1.10 (990.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:42:38.779777: step 13950, loss = 0.82 (985.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:42:40.087767: step 13960, loss = 1.12 (978.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:42:41.359584: step 13970, loss = 1.09 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:42:42.632888: step 13980, loss = 0.92 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:42:43.926208: step 13990, loss = 1.06 (989.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:42:45.294565: step 14000, loss = 0.87 (935.4 examples/sec; 0.137 sec/batch)
2017-05-07 20:42:46.459278: step 14010, loss = 0.96 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-07 20:42:47.774687: step 14020, loss = 0.95 (973.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:42:49.050329: step 14030, loss = 0.93 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:42:50.324834: step 14040, loss = 0.87 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:42:51.590946: step 14050, loss = 1.08 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:42:52.861000: step 14060, loss = 0.94 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:42:54.136122: step 14070, loss = 0.79 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:42:55.405854: step 14080, loss = 0.96 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:42:56.689581: step 14090, loss = 0.88 (997.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:42:58.035404: step 14100, loss = 0.79 (951.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:42:59.232911: step 14110, loss = 0.91 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-07 20:43:00.524033: step 14120, loss = 0.72 (991.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:43:01.788552: step 14130, loss = 0.90 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:43:03.094596: step 14140, loss = 0.91 (980.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:43:04.395934: step 14150, loss = 1.05 (983.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:43:05.694877: step 14160, loss = 1.10 (985.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:43:06.973473: step 14170, loss = 0.94 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:43:08.246332: step 14180, loss = 1.07 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:43:09.560200: step 14190, loss = 0.98 (974.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:43:10.937550: step 14200, loss = 1.21 (929.3 examples/sec; 0.138 sec/batch)
2017-05-07 20:43:12.127045: step 14210, loss = 0.97 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-07 20:43:13.417532: step 14220, loss = 0.96 (991.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:43:14.697371: step 14230, loss = 0.92 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:43:16.007682: step 14240, loss = 0.77 (976.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:43:17.308809: step 14250, loss = 0.69 (983.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:43:18.588595: step 14260, loss = 0.85 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:43:19.900846: step 14270, loss = 0.88 (975.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:43:21.198798: step 14280, loss = 0.92 (986.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:43:22.482773: step 14290, loss = 0.80 (996.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:43:23.869223: step 14300, loss = 0.80 (923.2 examples/sec; 0.139 sec/batch)
2017-05-07 20:43:25.056036: step 14310, loss = 0.87 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-07 20:43:26.334882: step 14320, loss = 0.99 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:43:27.646638: step 14330, loss = 0.88 (975.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:43:28.924044: step 14340, loss = 0.97 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:43:30.208661: step 14350, loss = 0.97 (996.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:43:31.485491: step 14360, loss = 0.75 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:43:32.776691: step 14370, loss = 0.83 (991.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:43:34.032263: step 14380, loss = 1.03 (1019.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:43:35.326610: step 14390, loss = 1.17 (988.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:43:36.718482: step 14400, loss = 0.90 (919.6 examples/sec; 0.139 sec/batch)
2017-05-07 20:43:37.890682: step 14410, loss = 0.93 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-07 20:43:39.203643: step 14420, loss = 1.04 (974.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:43:40.488353: step 14430, loss = 0.84 (996.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:43:41.734346: step 14440, loss = 0.79 (1027.3 examples/sec; 0.125 sec/batch)
2017-05-07 20:43:42.989418: step 14450, loss = 1.01 (1019.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:43:44.264151: step 14460, loss = 0.86 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:43:45.565582: step 14470, loss = 1.00 (983.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:43:46.857189: step 14480, loss = 1.00 (991.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:43:48.138225: step 14490, loss = 0.97 (999.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:43:49.539301: step 14500, loss = 0.85 (913.6 examples/sec; 0.140 sec/batch)
2017-05-07 20:43:50.739208: step 14510, loss = 0.80 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-07 20:43:52.020678: step 14520, loss = 0.97 (998.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:43:53.305156: step 14530, loss = 0.81 (996.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:43:54.579379: step 14540, loss = 0.99 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:43:55.849271: step 14550, loss = 0.93 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:43:57.142279: step 14560, loss = 0.83 (989.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:43:58.446379: step 14570, loss = 0.78 (981.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:43:59.708584: step 14580, loss = 0.87 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:44:01.012008: step 14590, loss = 0.98 (982.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:44:02.392776: step 14600, loss = 1.01 (927.0 examples/sec; 0.138 sec/batch)
2017-05-07 20:44:03.582826: step 14610, loss = 0.97 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-07 20:44:04.876575: step 14620, loss = 0.87 (989.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:44:06.143770: step 14630, loss = 0.99 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:44:07.429619: step 14640, loss = 0.78 (995.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:44:08.716113: step 14650, loss = 0.85 (994.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:44:10.008676: step 14660, loss = 0.77 (990.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:44:11.303429: step 14670, loss = 1.02 (988.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:44:12.572744: step 14680, loss = 1.01 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:44:13.857444: step 14690, loss = 1.13 (996.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:44:15.239440: step 14700, loss = 0.90 (926.2 examples/sec; 0.138 sec/batch)
2017-05-07 20:44:16.430798: step 14710, loss = 0.89 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-07 20:44:17.754344: step 14720, loss = 1.38 (967.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:44:19.026197: step 14730, loss = 0.92 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:44:20.325619: step 14740, loss = 0.85 (985.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:44:21.608917: step 14750, loss = 0.78 (997.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:44:22.926572: step 14760, loss = 0.89 (971.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:44:24.229892: step 14770, loss = 0.96 (982.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:44:25.540816: step 14780, loss = 0.79 (976.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:44:26.811342: step 14790, loss = 0.80 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:44:28.183596: step 14800, loss = 0.78 (932.8 examples/sec; 0.137 sec/batch)
2017-05-07 20:44:29.352905: step 14810, loss = 1.15 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-07 20:44:30.623125: step 14820, loss = 0.89 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:44:31.913725: step 14830, loss = 1.16 (991.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:44:33.182762: step 14840, loss = 0.78 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:44:34.463196: step 14850, loss = 0.93 (999.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:44:35.732640: step 14860, loss = 0.87 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:44:36.999377: step 14870, loss = 0.75 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:44:38.287396: step 14880, loss = 0.73 (993.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:44:39.562058: step 14890, loss = 0.86 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:44:40.918177: step 14900, loss = 0.84 (943.9 examples/sec; 0.136 sec/batch)
2017-05-07 20:44:42.098672: step 14910, loss = 0.74 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-07 20:44:43.377018: step 14920, loss = 0.98 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:44:44.684564: step 14930, loss = 0.99 (979.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:44:45.968858: step 14940, loss = 1.02 (996.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:44:47.287084: step 14950, loss = 0.95 (971.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:44:48.552905: step 14960, loss = 1.07 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:44:49.834318: step 14970, loss = 1.13 (998.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:44:51.159743: step 14980, loss = 0.98 (965.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:44:52.444633: step 14990, loss = 0.97 (996.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:44:53.789760: step 15000, loss = 0.84 (951.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:44:54.981201: step 15010, loss = 0.85 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-07 20:44:56.282366: step 15020, loss = 0.86 (983.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:44:57.572261: step 15030, loss = 0.89 (992.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:44:58.852016: step 15040, loss = 0.99 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:45:00.122251: step 15050, loss = 0.95 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:45:01.380587: step 15060, loss = 0.86 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:45:02.665066: step 15070, loss = 0.82 (996.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:45:03.960915: step 15080, loss = 1.06 (987.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:45:05.269566: step 15090, loss = 1.01 (978.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:45:06.652281: step 15100, loss = 0.88 (925.7 examples/sec; 0.138 sec/batch)
2017-05-07 20:45:07.830390: step 15110, loss = 0.78 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-07 20:45:09.099898: step 15120, loss = 0.78 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:45:10.371618: step 15130, loss = 0.81 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:45:11.665444: step 15140, loss = 0.88 (989.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:45:12.962639: step 15150, loss = 0.92 (986.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:45:14.255521: step 15160, loss = 1.06 (990.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:45:15.557573: step 15170, loss = 0.96 (983.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:45:16.836988: step 15180, loss = 0.71 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:45:18.117285: step 15190, loss = 0.84 (999.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:45:19.518044: step 15200, loss = 0.99 (913.8 examples/sec; 0.140 sec/batch)
2017-05-07 20:45:20.702691: step 15210, loss = 0.80 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-07 20:45:21.980184: step 15220, loss = 0.83 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:45:23.249713: step 15230, loss = 0.91 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:45:24.549256: step 15240, loss = 0.86 (985.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:45:25.811426: step 15250, loss = 1.05 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:45:27.099660: step 15260, loss = 1.03 (993.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:45:28.390525: step 15270, loss = 0.79 (991.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:45:29.651339: step 15280, loss = 0.97 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:45:30.938856: step 15290, loss = 0.87 (994.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:45:32.306484: step 15300, loss = 0.77 (935.9 examples/sec; 0.137 sec/batch)
2017-05-07 20:45:33.500480: step 15310, loss = 0.97 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-07 20:45:34.786083: step 15320, loss = 0.92 (995.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:45:36.083623: step 15330, loss = 0.87 (986.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:45:37.377196: step 15340, loss = 0.81 (989.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:45:38.643198: step 15350, loss = 0.82 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:45:39.966635: step 15360, loss = 0.96 (967.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:45:41.242191: step 15370, loss = 0.94 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:45:42.510408: step 15380, loss = 0.85 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:45:43.772466: step 15390, loss = 0.87 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:45:45.140685: step 15400, loss = 0.91 (935.5 examples/sec; 0.137 sec/batch)
2017-05-07 20:45:46.323488: step 15410, loss = 0.81 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-07 20:45:47.594207: step 15420, loss = 0.91 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:45:48.895946: step 15430, loss = 0.90 (983.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:45:50.192871: step 15440, loss = 0.86 (987.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:45:51.480023: step 15450, loss = 1.00 (994.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:45:52.750033: step 15460, loss = 0.98 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:45:54.017410: step 15470, loss = 1.01 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:45:55.321937: step 15480, loss = 0.89 (981.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:45:56.578903: step 15490, loss = 0.90 (1018.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:45:57.953603: step 15500, loss = 0.99 (931.1 examples/sec; 0.137 sec/batch)
2017-05-07 20:45:59.124553: step 15510, loss = 0.84 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-07 20:46:00.421825: step 15520, loss = 1.02 (986.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:46:01.695682: step 15530, loss = 1.04 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:46:02.995248: step 15540, loss = 0.81 (985.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:46:04.291562: step 15550, loss = 0.99 (987.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:46:05.565211: step 15560, loss = 1.03 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:46:06.852610: step 15570, loss = 1.11 (994.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:46:08.111957: step 15580, loss = 0.86 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:46:09.384348: step 15590, loss = 0.92 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:46:10.764395: step 15600, loss = 1.08 (927.5 examples/sec; 0.138 sec/batch)
2017-05-07 20:46:11.938621: step 15610, loss = 1.11 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-07 20:46:13.196924: step 15620, loss = 0.92 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:46:14.467941: step 15630, loss = 0.85 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:46:15.750600: step 15640, loss = 0.94 (997.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:46:17.039585: step 15650, loss = 0.96 (993.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:46:18.336344: step 15660, loss = 0.99 (987.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:46:19.618607: step 15670, loss = 0.84 (998.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:46:20.895603: step 15680, loss = 0.79 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:46:22.153257: step 15690, loss = 0.93 (1017.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:46:23.545957: step 15700, loss = 0.75 (919.1 examples/sec; 0.139 sec/batch)
2017-05-07 20:46:24.728729: step 15710, loss = 0.90 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-07 20:46:26.021208: step 15720, loss = 0.96 (990.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:46:27.307134: step 15730, loss = 0.80 (995.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:46:28.616700: step 15740, loss = 1.24 (977.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:46:29.900811: step 15750, loss = 0.99 (996.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:46:31.197777: step 15760, loss = 0.90 (986.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:46:32.485394: step 15770, loss = 0.93 (994.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:46:33.736909: step 15780, loss = 0.92 (1022.8 examples/sec; 0.125 sec/batch)
2017-05-07 20:46:35.028085: step 15790, loss = 0.78 (991.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:46:36.416853: step 15800, loss = 0.87 (921.7 examples/sec; 0.139 sec/batch)
2017-05-07 20:46:37.576816: step 15810, loss = 0.96 (1103.5 examples/sec; 0.116 sec/batch)
2017-05-07 20:46:38.871357: step 15820, loss = 1.08 (988.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:46:40.175166: step 15830, loss = 0.98 (981.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:46:41.462310: step 15840, loss = 0.71 (994.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:46:42.761075: step 15850, loss = 1.13 (985.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:46:44.071750: step 15860, loss = 0.97 (976.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:46:45.367425: step 15870, loss = 0.85 (987.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:46:46.642432: step 15880, loss = 1.06 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:46:47.932662: step 15890, loss = 0.99 (992.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:46:49.312123: step 15900, loss = 0.86 (927.9 examples/sec; 0.138 sec/batch)
2017-05-07 20:46:50.508501: step 15910, loss = 1.20 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-07 20:46:51.776429: step 15920, loss = 1.06 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:46:53.097601: step 15930, loss = 0.95 (968.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:46:54.411816: step 15940, loss = 0.74 (974.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:46:55.715346: step 15950, loss = 1.13 (982.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:46:57.020572: step 15960, loss = 0.89 (980.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:46:58.319541: step 15970, loss = 0.76 (985.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:46:59.631393: step 15980, loss = 0.92 (975.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:47:00.923338: step 15990, loss = 0.89 (990.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:47:02.302303: step 16000, loss = 0.84 (928.2 examples/sec; 0.138 sec/batch)
2017-05-07 20:47:03.497097: step 16010, loss = 0.88 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-07 20:47:04.793064: step 16020, loss = 0.84 (987.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:47:06.070799: step 16030, loss = 0.84 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:47:07.328508: step 16040, loss = 0.80 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:47:08.606839: step 16050, loss = 0.90 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:47:09.871073: step 16060, loss = 0.83 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:47:11.164658: step 16070, loss = 1.36 (989.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:47:12.470805: step 16080, loss = 0.80 (980.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:47:13.780512: step 16090, loss = 0.77 (977.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:47:15.180517: step 16100, loss = 1.04 (914.3 examples/sec; 0.140 sec/batch)
2017-05-07 20:47:16.356585: step 16110, loss = 0.98 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-07 20:47:17.625106: step 16120, loss = 0.77 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:47:18.912200: step 16130, loss = 0.87 (994.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:47:20.223834: step 16140, loss = 0.84 (975.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:47:21.482532: step 16150, loss = 0.91 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:47:22.804303: step 16160, loss = 0.81 (968.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:47:24.111267: step 16170, loss = 0.91 (979.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:47:25.396555: step 16180, loss = 0.75 (995.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:47:26.648725: step 16190, loss = 0.76 (1022.2 examples/sec; 0.125 sec/batch)
2017-05-07 20:47:28.017235: step 16200, loss = 0.86 (935.3 examples/sec; 0.137 sec/batch)
2017-05-07 20:47:29.172928: step 16210, loss = 1.02 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-07 20:47:30.434784: step 16220, loss = 0.83 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:47:31.711121: step 16230, loss = 1.00 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:47:33.020924: step 16240, loss = 1.02 (977.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:47:34.292700: step 16250, loss = 0.92 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:47:35.589244: step 16260, loss = 0.86 (987.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:47:36.881208: step 16270, loss = 0.84 (990.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:47:38.157279: step 16280, loss = 0.83 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:47:39.419755: step 16290, loss = 1.00 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:47:40.797816: step 16300, loss = 1.01 (928.8 examples/sec; 0.138 sec/batch)
2017-05-07 20:47:41.975685: step 16310, loss = 1.09 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-07 20:47:43.242244: step 16320, loss = 0.80 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:47:44.537165: step 16330, loss = 0.96 (988.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:47:45.796555: step 16340, loss = 1.14 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:47:47.116006: step 16350, loss = 1.04 (970.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:47:48.383338: step 16360, loss = 0.83 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:47:49.661575: step 16370, loss = 0.78 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:47:50.968925: step 16380, loss = 0.90 (979.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:47:52.265843: step 16390, loss = 0.84 (987.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:47:53.614854: step 16400, loss = 0.95 (948.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:47:54.799363: step 16410, loss = 0.92 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-07 20:47:56.070664: step 16420, loss = 0.90 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:47:57.350481: step 16430, loss = 1.10 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:47:58.653286: step 16440, loss = 1.03 (982.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:47:59.914738: step 16450, loss = 0.88 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:48:01.216160: step 16460, loss = 0.99 (983.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:48:02.484163: step 16470, loss = 0.88 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:48:03.751039: step 16480, loss = 0.90 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:48:05.036208: step 16490, loss = 0.82 (996.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:48:06.390328: step 16500, loss = 0.91 (945.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:48:07.560268: step 16510, loss = 0.76 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-07 20:48:08.848906: step 16520, loss = 0.80 (993.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:48:10.119882: step 16530, loss = 1.10 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:48:11.397736: step 16540, loss = 0.94 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:48:12.669237: step 16550, loss = 1.03 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:48:13.960787: step 16560, loss = 0.90 (991.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:48:15.233525: step 16570, loss = 0.87 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:48:16.548480: step 16580, loss = 0.76 (973.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:48:17.844360: step 16590, loss = 0.90 (987.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:48:19.192299: step 16600, loss = 0.78 (949.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:48:20.409079: step 16610, loss = 0.76 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-07 20:48:21.684077: step 16620, loss = 0.91 (1003.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:48:22.944274: step 16630, loss = 0.92 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:48:24.244388: step 16640, loss = 0.85 (984.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:48:25.516686: step 16650, loss = 0.98 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:48:26.782479: step 16660, loss = 0.79 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:48:28.048016: step 16670, loss = 1.01 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:48:29.308468: step 16680, loss = 0.81 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:48:30.585636: step 16690, loss = 0.74 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:48:31.970545: step 16700, loss = 0.80 (924.2 examples/sec; 0.138 sec/batch)
2017-05-07 20:48:33.143483: step 16710, loss = 0.95 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-07 20:48:34.385825: step 16720, loss = 0.91 (1030.3 examples/sec; 0.124 sec/batch)
2017-05-07 20:48:35.663644: step 16730, loss = 0.76 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:48:36.934536: step 16740, loss = 1.19 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:48:38.221600: step 16750, loss = 1.07 (994.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:48:39.486219: step 16760, loss = 0.88 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:48:40.807062: step 16770, loss = 1.13 (969.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:48:42.065339: step 16780, loss = 0.93 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:48:43.327660: step 16790, loss = 0.75 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:48:44.678840: step 16800, loss = 0.78 (947.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:48:45.851100: step 16810, loss = 1.11 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-07 20:48:47.153167: step 16820, loss = 1.10 (983.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:48:48.414164: step 16830, loss = 0.76 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:48:49.681145: step 16840, loss = 0.88 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:48:50.938568: step 16850, loss = 0.88 (1018.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:48:52.240107: step 16860, loss = 0.99 (983.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:48:53.499558: step 16870, loss = 0.78 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:48:54.781738: step 16880, loss = 0.97 (998.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:48:56.058078: step 16890, loss = 0.92 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:48:57.429518: step 16900, loss = 0.78 (933.3 examples/sec; 0.137 sec/batch)
2017-05-07 20:48:58.598839: step 16910, loss = 1.13 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-07 20:48:59.886058: step 16920, loss = 0.88 (994.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:49:01.170271: step 16930, loss = 0.88 (996.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:49:02.456146: step 16940, loss = 0.72 (995.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:49:03.738196: step 16950, loss = 0.83 (998.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:49:05.013017: step 16960, loss = 0.83 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:49:06.298589: step 16970, loss = 0.97 (995.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:49:07.578151: step 16980, loss = 0.82 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:49:08.902917: step 16990, loss = 1.11 (966.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:49:10.247831: step 17000, loss = 0.74 (951.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:49:11.451472: step 17010, loss = 1.02 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-07 20:49:12.707263: step 17020, loss = 0.87 (1019.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:49:13.975222: step 17030, loss = 0.94 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:49:15.237264: step 17040, loss = 0.93 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:49:16.519821: step 17050, loss = 0.69 (998.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:49:17.781943: step 17060, loss = 0.88 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:49:19.057912: step 17070, loss = 1.08 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:49:20.342267: step 17080, loss = 0.83 (996.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:49:21.584305: step 17090, loss = 0.94 (1030.6 examples/sec; 0.124 sec/batch)
2017-05-07 20:49:22.964018: step 17100, loss = 0.91 (927.7 examples/sec; 0.138 sec/batch)
2017-05-07 20:49:24.143132: step 17110, loss = 1.11 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-07 20:49:25.406494: step 17120, loss = 0.92 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:49:26.673488: step 17130, loss = 0.97 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:49:27.966375: step 17140, loss = 1.05 (990.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:49:29.240776: step 17150, loss = 1.23 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:49:30.524807: step 17160, loss = 1.12 (996.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:49:31.814261: step 17170, loss = 1.07 (992.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:49:33.071927: step 17180, loss = 0.89 (1017.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:49:34.367741: step 17190, loss = 0.86 (987.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:49:35.743204: step 17200, loss = 0.92 (930.6 examples/sec; 0.138 sec/batch)
2017-05-07 20:49:36.942195: step 17210, loss = 0.86 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-07 20:49:38.211532: step 17220, loss = 0.79 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:49:39.464208: step 17230, loss = 0.69 (1021.8 examples/sec; 0.125 sec/batch)
2017-05-07 20:49:40.743439: step 17240, loss = 0.96 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:49:42.006617: step 17250, loss = 0.81 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:49:43.277189: step 17260, loss = 0.82 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:49:44.561755: step 17270, loss = 1.03 (996.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:49:45.837728: step 17280, loss = 1.15 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:49:47.108706: step 17290, loss = 1.08 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:49:48.483473: step 17300, loss = 0.94 (931.1 examples/sec; 0.137 sec/batch)
2017-05-07 20:49:49.672259: step 17310, loss = 0.86 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-07 20:49:50.952692: step 17320, loss = 0.84 (999.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:49:52.225350: step 17330, loss = 1.03 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:49:53.532855: step 17340, loss = 0.98 (979.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:49:54.801642: step 17350, loss = 0.97 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:49:56.113492: step 17360, loss = 1.05 (975.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:49:57.376608: step 17370, loss = 0.72 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:49:58.646445: step 17380, loss = 0.87 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:49:59.932582: step 17390, loss = 0.79 (995.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:50:01.304361: step 17400, loss = 0.73 (933.1 examples/sec; 0.137 sec/batch)
2017-05-07 20:50:02.495417: step 17410, loss = 0.90 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-07 20:50:03.766946: step 17420, loss = 0.84 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:50:05.068467: step 17430, loss = 1.02 (983.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:50:06.355240: step 17440, loss = 0.84 (994.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:50:07.642236: step 17450, loss = 1.00 (994.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:50:08.926143: step 17460, loss = 0.86 (997.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:50:10.238200: step 17470, loss = 0.90 (975.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:50:11.540914: step 17480, loss = 0.92 (982.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:50:12.819891: step 17490, loss = 0.87 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:50:14.192235: step 17500, loss = 0.94 (932.7 examples/sec; 0.137 sec/batch)
2017-05-07 20:50:15.367611: step 17510, loss = 0.85 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-07 20:50:16.649302: step 17520, loss = 1.02 (998.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:50:17.933470: step 17530, loss = 0.95 (996.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:50:19.202048: step 17540, loss = 1.04 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:50:20.470903: step 17550, loss = 0.90 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:50:21.763906: step 17560, loss = 0.92 (989.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:50:23.061291: step 17570, loss = 0.90 (986.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:50:24.341252: step 17580, loss = 0.96 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:50:25.613658: step 17590, loss = 0.83 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:50:26.981041: step 17600, loss = 0.72 (936.1 examples/sec; 0.137 sec/batch)
2017-05-07 20:50:28.168105: step 17610, loss = 0.87 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-07 20:50:29.475188: step 17620, loss = 0.82 (979.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:50:30.775143: step 17630, loss = 0.79 (984.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:50:32.031434: step 17640, loss = 0.91 (1018.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:50:33.283686: step 17650, loss = 0.82 (1022.2 examples/sec; 0.125 sec/batch)
2017-05-07 20:50:34.550994: step 17660, loss = 0.92 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:50:35.837095: step 17670, loss = 0.88 (995.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:50:37.144685: step 17680, loss = 0.95 (978.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:50:38.436930: step 17690, loss = 0.97 (990.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:50:39.842650: step 17700, loss = 1.03 (910.6 examples/sec; 0.141 sec/batch)
2017-05-07 20:50:41.042209: step 17710, loss = 1.01 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-07 20:50:42.298340: step 17720, loss = 0.90 (1019.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:50:43.568708: step 17730, loss = 0.86 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:50:44.862635: step 17740, loss = 0.70 (989.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:50:46.108621: step 17750, loss = 1.00 (1027.3 examples/sec; 0.125 sec/batch)
2017-05-07 20:50:47.366818: step 17760, loss = 0.89 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:50:48.639703: step 17770, loss = 0.92 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:50:49.897477: step 17780, loss = 0.93 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:50:51.169738: step 17790, loss = 0.84 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:50:52.547005: step 17800, loss = 1.02 (929.4 examples/sec; 0.138 sec/batch)
2017-05-07 20:50:53.737440: step 17810, loss = 1.06 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-07 20:50:55.022665: step 17820, loss = 0.96 (995.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:50:56.340213: step 17830, loss = 1.06 (971.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:50:57.642259: step 17840, loss = 0.88 (983.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:50:58.919201: step 17850, loss = 0.82 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:51:00.193117: step 17860, loss = 0.82 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:51:01.464100: step 17870, loss = 0.93 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:51:02.721007: step 17880, loss = 0.87 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:51:03.996061: step 17890, loss = 0.92 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:51:05.362647: step 17900, loss = 0.80 (936.6 examples/sec; 0.137 sec/batch)
2017-05-07 20:51:06.550236: step 17910, loss = 1.02 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-07 20:51:07.834391: step 17920, loss = 0.96 (996.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:51:09.095447: step 17930, loss = 0.84 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:51:10.360566: step 17940, loss = 0.89 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:51:11.617792: step 17950, loss = 0.80 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:51:12.892646: step 17960, loss = 0.91 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:51:14.157601: step 17970, loss = 0.95 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:51:15.463836: step 17980, loss = 0.81 (979.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:51:16.753728: step 17990, loss = 0.92 (992.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:51:18.106073: step 18000, loss = 0.99 (946.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:51:19.297345: step 18010, loss = 0.85 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-07 20:51:20.594938: step 18020, loss = 0.73 (986.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:51:21.864471: step 18030, loss = 0.88 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:51:23.123666: step 18040, loss = 0.91 (1016.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:51:24.409860: step 18050, loss = 0.90 (995.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:51:25.704396: step 18060, loss = 0.82 (988.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:51:26.999310: step 18070, loss = 0.68 (988.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:51:28.302479: step 18080, loss = 0.93 (982.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:51:29.592648: step 18090, loss = 1.01 (992.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:51:30.969328: step 18100, loss = 0.80 (929.8 examples/sec; 0.138 sec/batch)
2017-05-07 20:51:32.154560: step 18110, loss = 0.80 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-07 20:51:33.425086: step 18120, loss = 0.96 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:51:34.739029: step 18130, loss = 1.08 (974.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:51:36.006803: step 18140, loss = 0.98 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:51:37.289741: step 18150, loss = 1.00 (997.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:51:38.578856: step 18160, loss = 0.85 (992.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:51:39.860629: step 18170, loss = 0.79 (998.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:51:41.136118: step 18180, loss = 0.90 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:51:42.417358: step 18190, loss = 1.12 (999.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:51:43.773474: step 18200, loss = 0.85 (943.9 examples/sec; 0.136 sec/batch)
2017-05-07 20:51:44.945885: step 18210, loss = 0.95 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-07 20:51:46.223065: step 18220, loss = 0.99 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:51:47.495659: step 18230, loss = 0.94 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:51:48.795871: step 18240, loss = 1.02 (984.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:51:50.064948: step 18250, loss = 0.95 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:51:51.351527: step 18260, loss = 0.85 (994.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:51:52.638685: step 18270, loss = 0.92 (994.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:51:53.897924: step 18280, loss = 0.74 (1016.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:51:55.154482: step 18290, loss = 0.85 (1018.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:51:56.542050: step 18300, loss = 0.85 (922.5 examples/sec; 0.139 sec/batch)
2017-05-07 20:51:57.732894: step 18310, loss = 1.12 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-07 20:51:59.024271: step 18320, loss = 1.10 (991.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:52:00.295454: step 18330, loss = 0.95 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:52:01.574926: step 18340, loss = 0.81 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:52:02.836693: step 18350, loss = 0.88 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:52:04.122105: step 18360, loss = 0.76 (995.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:52:05.439323: step 18370, loss = 0.81 (971.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:52:06.707041: step 18380, loss = 0.83 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:52:07.962920: step 18390, loss = 0.89 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:52:09.343427: step 18400, loss = 1.04 (927.2 examples/sec; 0.138 sec/batch)
2017-05-07 20:52:10.508376: step 18410, loss = 0.90 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-07 20:52:11.779014: step 18420, loss = 0.85 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:52:13.072563: step 18430, loss = 0.85 (989.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:52:14.326671: step 18440, loss = 1.01 (1020.6 examples/sec; 0.125 sec/batch)
2017-05-07 20:52:15.652519: step 18450, loss = 1.17 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:52:16.924901: step 18460, loss = 1.18 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:52:18.221112: step 18470, loss = 0.96 (987.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:52:19.517208: step 18480, loss = 0.90 (987.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:52:20.833072: step 18490, loss = 0.96 (972.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:52:22.190299: step 18500, loss = 0.88 (943.1 examples/sec; 0.136 sec/batch)
2017-05-07 20:52:23.394915: step 18510, loss = 0.88 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-07 20:52:24.673152: step 18520, loss = 0.77 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:52:25.972758: step 18530, loss = 0.96 (984.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:52:27.290644: step 18540, loss = 0.94 (971.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:52:28.550748: step 18550, loss = 0.89 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:52:29.789055: step 18560, loss = 0.84 (1033.7 examples/sec; 0.124 sec/batch)
2017-05-07 20:52:31.047581: step 18570, loss = 1.11 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:52:32.320353: step 18580, loss = 0.97 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:52:33.580354: step 18590, loss = 0.85 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:52:34.937139: step 18600, loss = 1.01 (943.4 examples/sec; 0.136 sec/batch)
2017-05-07 20:52:36.110578: step 18610, loss = 0.77 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-07 20:52:37.385234: step 18620, loss = 0.96 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:52:38.637560: step 18630, loss = 0.81 (1022.1 examples/sec; 0.125 sec/batch)
2017-05-07 20:52:39.892113: step 18640, loss = 0.76 (1020.3 examples/sec; 0.125 sec/batch)
2017-05-07 20:52:41.162741: step 18650, loss = 0.98 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:52:42.442884: step 18660, loss = 0.75 (999.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:52:43.739858: step 18670, loss = 0.85 (986.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:52:44.999800: step 18680, loss = 0.98 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:52:46.263393: step 18690, loss = 0.74 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:52:47.637870: step 18700, loss = 1.05 (931.3 examples/sec; 0.137 sec/batch)
2017-05-07 20:52:48.856865: step 18710, loss = 1.11 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-07 20:52:50.144949: step 18720, loss = 0.96 (993.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:52:51.438549: step 18730, loss = 1.00 (989.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:52:52.718533: step 18740, loss = 0.99 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:52:53.981739: step 18750, loss = 0.97 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:52:55.286122: step 18760, loss = 0.86 (981.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:52:56.579643: step 18770, loss = 1.00 (989.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:52:57.863488: step 18780, loss = 0.95 (997.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:52:59.151277: step 18790, loss = 0.90 (993.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:53:00.515652: step 18800, loss = 0.88 (938.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:53:01.706515: step 18810, loss = 0.81 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-07 20:53:03.007489: step 18820, loss = 0.80 (983.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:53:04.292699: step 18830, loss = 0.85 (995.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:53:05.584621: step 18840, loss = 0.70 (990.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:53:06.849182: step 18850, loss = 0.86 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:53:08.124685: step 18860, loss = 0.94 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:53:09.439000: step 18870, loss = 0.99 (973.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:53:10.720867: step 18880, loss = 0.86 (998.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:53:11.997651: step 18890, loss = 0.76 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:53:13.354739: step 18900, loss = 0.60 (943.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:53:14.523024: step 18910, loss = 1.05 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-07 20:53:15.812561: step 18920, loss = 0.93 (992.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:53:17.089616: step 18930, loss = 1.06 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:53:18.336249: step 18940, loss = 0.91 (1026.8 examples/sec; 0.125 sec/batch)
2017-05-07 20:53:19.646206: step 18950, loss = 1.08 (977.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:53:20.922207: step 18960, loss = 0.91 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:53:22.215394: step 18970, loss = 0.78 (989.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:53:23.490756: step 18980, loss = 0.97 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:53:24.753165: step 18990, loss = 1.04 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:53:26.135463: step 19000, loss = 0.80 (926.0 examples/sec; 0.138 sec/batch)
2017-05-07 20:53:27.332795: step 19010, loss = 0.97 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-07 20:53:28.631280: step 19020, loss = 0.69 (985.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:53:29.906199: step 19030, loss = 1.10 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:53:31.214710: step 19040, loss = 0.81 (978.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:53:32.515313: step 19050, loss = 0.81 (984.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:53:33.786590: step 19060, loss = 0.82 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:53:35.089329: step 19070, loss = 1.04 (982.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:53:36.340068: step 19080, loss = 0.87 (1023.4 examples/sec; 0.125 sec/batch)
2017-05-07 20:53:37.634649: step 19090, loss = 0.85 (988.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:53:39.009133: step 19100, loss = 0.91 (931.3 examples/sec; 0.137 sec/batch)
2017-05-07 20:53:40.198498: step 19110, loss = 0.90 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-07 20:53:41.500759: step 19120, loss = 0.79 (982.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:53:42.796119: step 19130, loss = 0.74 (988.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:53:44.081783: step 19140, loss = 0.72 (995.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:53:45.361192: step 19150, loss = 0.84 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:53:46.616554: step 19160, loss = 1.01 (1019.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:53:47.892644: step 19170, loss = 1.01 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:53:49.176931: step 19180, loss = 0.87 (996.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:53:50.427730: step 19190, loss = 0.94 (1023.3 examples/sec; 0.125 sec/batch)
2017-05-07 20:53:51.776875: step 19200, loss = 0.93 (948.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:53:52.966481: step 19210, loss = 1.02 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-07 20:53:54.265140: step 19220, loss = 0.78 (985.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:53:55.580271: step 19230, loss = 0.91 (973.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:53:56.859176: step 19240, loss = 0.88 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:53:58.151655: step 19250, loss = 0.92 (990.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:53:59.416599: step 19260, loss = 1.09 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:54:00.681281: step 19270, loss = 1.02 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:54:01.972703: step 19280, loss = 0.86 (991.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:54:03.272782: step 19290, loss = 0.98 (984.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:54:04.628682: step 19300, loss = 0.93 (944.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:54:05.792400: step 19310, loss = 1.03 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-07 20:54:07.047811: step 19320, loss = 0.74 (1019.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:54:08.322753: step 19330, loss = 1.07 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:54:09.577170: step 19340, loss = 0.96 (1020.4 examples/sec; 0.125 sec/batch)
2017-05-07 20:54:10.847867: step 19350, loss = 0.82 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:54:12.114566: step 19360, loss = 0.84 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:54:13.369651: step 19370, loss = 0.87 (1019.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:54:14.645471: step 19380, loss = 0.75 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:54:15.940442: step 19390, loss = 0.82 (988.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:54:17.306078: step 19400, loss = 1.02 (937.3 examples/sec; 0.137 sec/batch)
2017-05-07 20:54:18.480115: step 19410, loss = 0.83 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-07 20:54:19.752923: step 19420, loss = 0.81 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:54:21.024710: step 19430, loss = 0.84 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:54:22.328418: step 19440, loss = 0.90 (981.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:54:23.595129: step 19450, loss = 0.81 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:54:24.881654: step 19460, loss = 0.95 (994.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:54:26.165379: step 19470, loss = 0.89 (997.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:54:27.453619: step 19480, loss = 0.82 (993.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:54:28.718697: step 19490, loss = 1.12 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:54:30.064038: step 19500, loss = 0.91 (951.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:54:31.273516: step 19510, loss = 0.81 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-07 20:54:32.584250: step 19520, loss = 0.92 (976.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:54:33.861082: step 19530, loss = 0.83 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:54:35.131279: step 19540, loss = 0.88 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:54:36.417626: step 19550, loss = 0.80 (995.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:54:37.691935: step 19560, loss = 0.81 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:54:38.959004: step 19570, loss = 0.76 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:54:40.232611: step 19580, loss = 0.80 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-07 20:54:41.485315: step 19590, loss = 0.91 (1021.8 examples/sec; 0.125 sec/batch)
2017-05-07 20:54:42.847783: step 19600, loss = 0.75 (939.5 examples/sec; 0.136 sec/batch)
2017-05-07 20:54:44.036720: step 19610, loss = 0.95 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-07 20:54:45.308997: step 19620, loss = 0.94 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:54:46.601916: step 19630, loss = 0.85 (990.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:54:47.846367: step 19640, loss = 0.92 (1028.6 examples/sec; 0.124 sec/batch)
2017-05-07 20:54:49.116692: step 19650, loss = 0.84 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:54:50.398281: step 19660, loss = 0.78 (998.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:54:51.658856: step 19670, loss = 0.81 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:54:52.939474: step 19680, loss = 0.89 (999.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:54:54.198611: step 19690, loss = 1.19 (1016.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:54:55.574160: step 19700, loss = 1.02 (930.5 examples/sec; 0.138 sec/batch)
2017-05-07 20:54:56.735577: step 19710, loss = 0.99 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-07 20:54:58.003383: step 19720, loss = 0.78 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:54:59.261384: step 19730, loss = 0.78 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:55:00.557076: step 19740, loss = 0.87 (987.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:55:01.821183: step 19750, loss = 1.11 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:55:03.117060: step 19760, loss = 0.82 (987.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:55:04.372158: step 19770, loss = 0.94 (1019.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:55:05.642988: step 19780, loss = 0.78 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:55:06.912424: step 19790, loss = 0.83 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:55:08.271409: step 19800, loss = 0.86 (941.9 examples/sec; 0.136 sec/batch)
2017-05-07 20:55:09.424885: step 19810, loss = 0.78 (1109.7 examples/sec; 0.115 sec/batch)
2017-05-07 20:55:10.700044: step 19820, loss = 0.84 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:55:11.978307: step 19830, loss = 1.17 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:55:13.298574: step 19840, loss = 0.95 (969.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:55:14.589465: step 19850, loss = 0.84 (991.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:55:15.871184: step 19860, loss = 0.89 (998.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:55:17.137961: step 19870, loss = 0.90 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:55:18.392325: step 19880, loss = 0.92 (1020.4 examples/sec; 0.125 sec/batch)
2017-05-07 20:55:19.651335: step 19890, loss = 0.81 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:55:21.011918: step 19900, loss = 0.84 (940.8 examples/sec; 0.136 sec/batch)
2017-05-07 20:55:22.170171: step 19910, loss = 0.83 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-07 20:55:23.422514: step 19920, loss = 0.80 (1022.1 examples/sec; 0.125 sec/batch)
2017-05-07 20:55:24.685536: step 19930, loss = 0.86 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:55:25.926199: step 19940, loss = 0.72 (1031.7 examples/sec; 0.124 sec/batch)
2017-05-07 20:55:27.193269: step 19950, loss = 0.83 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:55:28.464051: step 19960, loss = 0.84 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:55:29.717668: step 19970, loss = 0.99 (1021.0 examples/sec; 0.125 sec/batch)
2017-05-07 20:55:30.990853: step 19980, loss = 0.83 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:55:32.279129: step 19990, loss = 1.00 (993.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:55:33.669619: step 20000, loss = 0.93 (920.5 examples/sec; 0.139 sec/batch)
2017-05-07 20:55:34.824596: step 20010, loss = 0.89 (1108.3 examples/sec; 0.115 sec/batch)
2017-05-07 20:55:36.087306: step 20020, loss = 0.78 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:55:37.348779: step 20030, loss = 0.84 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:55:38.606048: step 20040, loss = 0.90 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:55:39.871060: step 20050, loss = 0.92 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:55:41.126321: step 20060, loss = 0.84 (1019.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:55:42.404937: step 20070, loss = 0.78 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:55:43.694676: step 20080, loss = 0.80 (992.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:55:44.992251: step 20090, loss = 1.06 (986.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:55:46.367863: step 20100, loss = 0.99 (930.5 examples/sec; 0.138 sec/batch)
2017-05-07 20:55:47.558877: step 20110, loss = 0.98 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-07 20:55:48.839499: step 20120, loss = 0.81 (999.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:55:50.092247: step 20130, loss = 1.05 (1021.8 examples/sec; 0.125 sec/batch)
2017-05-07 20:55:51.388099: step 20140, loss = 0.82 (987.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:55:52.687104: step 20150, loss = 0.79 (985.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:55:53.928152: step 20160, loss = 0.95 (1031.4 examples/sec; 0.124 sec/batch)
2017-05-07 20:55:55.208326: step 20170, loss = 0.86 (999.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:55:56.470034: step 20180, loss = 0.92 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:55:57.727708: step 20190, loss = 0.98 (1017.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:55:59.084594: step 20200, loss = 0.75 (943.3 examples/sec; 0.136 sec/batch)
2017-05-07 20:56:00.271014: step 20210, loss = 0.79 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-07 20:56:01.513400: step 20220, loss = 0.76 (1030.3 examples/sec; 0.124 sec/batch)
2017-05-07 20:56:02.808163: step 20230, loss = 0.96 (988.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:56:04.059627: step 20240, loss = 0.81 (1022.8 examples/sec; 0.125 sec/batch)
2017-05-07 20:56:05.333460: step 20250, loss = 1.03 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:56:06.577653: step 20260, loss = 0.92 (1028.8 examples/sec; 0.124 sec/batch)
2017-05-07 20:56:07.878062: step 20270, loss = 1.20 (984.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:56:09.177315: step 20280, loss = 0.85 (985.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:56:10.470495: step 20290, loss = 0.89 (989.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:56:11.862690: step 20300, loss = 0.84 (919.4 examples/sec; 0.139 sec/batch)
2017-05-07 20:56:13.029683: step 20310, loss = 0.79 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-07 20:56:14.318365: step 20320, loss = 0.92 (993.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:56:15.616696: step 20330, loss = 0.78 (985.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:56:16.880804: step 20340, loss = 0.78 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:56:18.192594: step 20350, loss = 0.82 (975.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:56:19.451713: step 20360, loss = 0.87 (1016.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:56:20.737288: step 20370, loss = 0.89 (995.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:56:21.995201: step 20380, loss = 0.80 (1017.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:56:23.297523: step 20390, loss = 0.96 (982.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:56:24.663874: step 20400, loss = 0.93 (936.8 examples/sec; 0.137 sec/batch)
2017-05-07 20:56:25.847294: step 20410, loss = 0.92 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-07 20:56:27.156424: step 20420, loss = 0.94 (977.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:56:28.433432: step 20430, loss = 0.89 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:56:29.689919: step 20440, loss = 0.95 (1018.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:56:30.966262: step 20450, loss = 0.83 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:56:32.203149: step 20460, loss = 0.91 (1034.9 examples/sec; 0.124 sec/batch)
2017-05-07 20:56:33.460367: step 20470, loss = 0.80 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:56:34.702584: step 20480, loss = 0.78 (1030.4 examples/sec; 0.124 sec/batch)
2017-05-07 20:56:35.972153: step 20490, loss = 0.79 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:56:37.352639: step 20500, loss = 0.99 (927.2 examples/sec; 0.138 sec/batch)
2017-05-07 20:56:38.542111: step 20510, loss = 0.76 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-07 20:56:39.829451: step 20520, loss = 0.84 (994.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:56:41.108054: step 20530, loss = 1.01 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:56:42.383627: step 20540, loss = 1.17 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:56:43.687429: step 20550, loss = 0.82 (981.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:56:44.972097: step 20560, loss = 0.87 (996.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:56:46.276959: step 20570, loss = 0.94 (980.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:56:47.576110: step 20580, loss = 0.88 (985.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:56:48.837728: step 20590, loss = 0.74 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:56:50.196906: step 20600, loss = 0.92 (941.7 examples/sec; 0.136 sec/batch)
2017-05-07 20:56:51.408304: step 20610, loss = 0.98 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-07 20:56:52.648884: step 20620, loss = 0.88 (1031.8 examples/sec; 0.124 sec/batch)
2017-05-07 20:56:53.923315: step 20630, loss = 0.76 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:56:55.197044: step 20640, loss = 0.80 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:56:56.454705: step 20650, loss = 0.65 (1017.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:56:57.717642: step 20660, loss = 0.88 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:56:58.976436: step 20670, loss = 0.83 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:57:00.238875: step 20680, loss = 0.84 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:57:01.490153: step 20690, loss = 1.12 (1023.0 examples/sec; 0.125 sec/batch)
2017-05-07 20:57:02.873040: step 20700, loss = 0.76 (925.6 examples/sec; 0.138 sec/batch)
2017-05-07 20:57:04.079223: step 20710, loss = 0.90 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-07 20:57:05.355512: step 20720, loss = 0.85 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:57:06.611951: step 20730, loss = 0.75 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:57:07.918810: step 20740, loss = 1.08 (979.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:57:09.220730: step 20750, loss = 0.78 (983.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:57:10.473945: step 20760, loss = 0.72 (1021.4 examples/sec; 0.125 sec/batch)
2017-05-07 20:57:11.787697: step 20770, loss = 0.82 (974.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:57:13.033798: step 20780, loss = 0.86 (1027.2 examples/sec; 0.125 sec/batch)
2017-05-07 20:57:14.316780: step 20790, loss = 0.82 (997.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:57:15.679222: step 20800, loss = 1.00 (939.5 examples/sec; 0.136 sec/batch)
2017-05-07 20:57:16.845921: step 20810, loss = 0.75 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-07 20:57:18.128660: step 20820, loss = 0.98 (997.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:57:19.391735: step 20830, loss = 0.77 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:57:20.665432: step 20840, loss = 0.73 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:57:21.911959: step 20850, loss = 0.85 (1026.9 examples/sec; 0.125 sec/batch)
2017-05-07 20:57:23.152357: step 20860, loss = 0.73 (1031.9 examples/sec; 0.124 sec/batch)
2017-05-07 20:57:24.418556: step 20870, loss = 0.84 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:57:25.671450: step 20880, loss = 0.93 (1021.6 examples/sec; 0.125 sec/batch)
2017-05-07 20:57:26.925642: step 20890, loss = 0.93 (1020.6 examples/sec; 0.125 sec/batch)
2017-05-07 20:57:28.275093: step 20900, loss = 0.91 (948.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:57:29.479538: step 20910, loss = 0.96 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-07 20:57:30.781498: step 20920, loss = 0.77 (983.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:57:32.047697: step 20930, loss = 0.89 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:57:33.341921: step 20940, loss = 0.97 (989.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:57:34.641623: step 20950, loss = 0.95 (984.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:57:35.924599: step 20960, loss = 0.77 (997.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:57:37.225435: step 20970, loss = 0.98 (984.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:57:38.475418: step 20980, loss = 0.85 (1024.0 examples/sec; 0.125 sec/batch)
2017-05-07 20:57:39.783412: step 20990, loss = 0.92 (978.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:57:41.131887: step 21000, loss = 0.80 (949.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:57:42.293853: step 21010, loss = 0.93 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-07 20:57:43.599892: step 21020, loss = 0.90 (980.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:57:44.900303: step 21030, loss = 0.80 (984.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:57:46.163910: step 21040, loss = 0.90 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:57:47.415462: step 21050, loss = 0.85 (1022.7 examples/sec; 0.125 sec/batch)
2017-05-07 20:57:48.683128: step 21060, loss = 0.90 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:57:49.947224: step 21070, loss = 0.81 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:57:51.242685: step 21080, loss = 0.92 (988.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:57:52.535935: step 21090, loss = 0.67 (989.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:57:53.932045: step 21100, loss = 0.89 (916.8 examples/sec; 0.140 sec/batch)
2017-05-07 20:57:55.173659: step 21110, loss = 0.88 (1030.9 examples/sec; 0.124 sec/batch)
2017-05-07 20:57:56.482943: step 21120, loss = 1.01 (977.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:57:57.759817: step 21130, loss = 0.84 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:57:59.034043: step 21140, loss = 0.84 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:58:00.321064: step 21150, loss = 0.86 (994.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:58:01.613196: step 21160, loss = 0.79 (990.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:58:02.906606: step 21170, loss = 0.83 (989.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:58:04.215525: step 21180, loss = 0.86 (977.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:58:05.483303: step 21190, loss = 0.98 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:58:06.849529: step 21200, loss = 0.91 (936.9 examples/sec; 0.137 sec/batch)
2017-05-07 20:58:08.033906: step 21210, loss = 0.82 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-07 20:58:09.297677: step 21220, loss = 0.94 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:58:10.571206: step 21230, loss = 0.80 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:58:11.832321: step 21240, loss = 0.75 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:58:13.096432: step 21250, loss = 0.73 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:58:14.360754: step 21260, loss = 0.75 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:58:15.659852: step 21270, loss = 0.75 (985.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:58:16.932884: step 21280, loss = 0.81 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:58:18.203721: step 21290, loss = 0.84 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:58:19.570474: step 21300, loss = 0.78 (936.5 examples/sec; 0.137 sec/batch)
2017-05-07 20:58:20.746433: step 21310, loss = 0.78 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-07 20:58:22.013683: step 21320, loss = 1.02 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:58:23.314496: step 21330, loss = 0.90 (984.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:58:24.609454: step 21340, loss = 0.97 (988.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:58:25.898197: step 21350, loss = 0.98 (993.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:58:27.185623: step 21360, loss = 0.88 (994.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:58:28.473383: step 21370, loss = 0.74 (994.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:58:29.735103: step 21380, loss = 0.78 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:58:30.986934: step 21390, loss = 0.62 (1022.5 examples/sec; 0.125 sec/batch)
2017-05-07 20:58:32.375616: step 21400, loss = 0.92 (921.7 examples/sec; 0.139 sec/batch)
2017-05-07 20:58:33.546346: step 21410, loss = 0.72 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-07 20:58:34.879588: step 21420, loss = 0.70 (960.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:58:36.203406: step 21430, loss = 1.03 (966.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:58:37.477134: step 21440, loss = 0.75 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:58:38.781294: step 21450, loss = 0.82 (981.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:58:40.044891: step 21460, loss = 0.77 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:58:41.325618: step 21470, loss = 1.01 (999.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:58:42.577460: step 21480, loss = 0.82 (1022.5 examples/sec; 0.125 sec/batch)
2017-05-07 20:58:43.841230: step 21490, loss = 1.00 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-07 20:58:45.205717: step 21500, loss = 0.78 (938.1 examples/sec; 0.136 sec/batch)
2017-05-07 20:58:46.355665: step 21510, loss = 0.96 (1113.1 examples/sec; 0.115 sec/batch)
2017-05-07 20:58:47.618507: step 21520, loss = 1.01 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:58:48.871525: step 21530, loss = 0.92 (1021.5 examples/sec; 0.125 sec/batch)
2017-05-07 20:58:50.171271: step 21540, loss = 0.96 (984.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:58:51.441442: step 21550, loss = 0.73 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:58:52.722403: step 21560, loss = 0.92 (999.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:58:53.988748: step 21570, loss = 0.96 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:58:55.265598: step 21580, loss = 0.92 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:58:56.563716: step 21590, loss = 0.90 (986.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:58:57.952508: step 21600, loss = 1.06 (921.7 examples/sec; 0.139 sec/batch)
2017-05-07 20:58:59.129620: step 21610, loss = 0.79 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-07 20:59:00.443149: step 21620, loss = 0.81 (974.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:59:01.702047: step 21630, loss = 0.76 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-07 20:59:03.001460: step 21640, loss = 0.86 (985.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:59:04.292113: step 21650, loss = 0.96 (991.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:59:05.563432: step 21660, loss = 0.84 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-07 20:59:06.836278: step 21670, loss = 0.91 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:59:08.121865: step 21680, loss = 0.86 (995.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:59:09.393753: step 21690, loss = 0.86 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:59:10.756958: step 21700, loss = 0.90 (939.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:59:11.963386: step 21710, loss = 0.92 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-07 20:59:13.239092: step 21720, loss = 0.76 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:59:14.486834: step 21730, loss = 0.90 (1025.9 examples/sec; 0.125 sec/batch)
2017-05-07 20:59:15.806742: step 21740, loss = 0.79 (969.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:59:17.104913: step 21750, loss = 0.84 (986.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:59:18.384855: step 21760, loss = 0.97 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:59:19.680020: step 21770, loss = 0.66 (988.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:59:20.969218: step 21780, loss = 1.00 (992.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:59:22.254672: step 21790, loss = 0.71 (995.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:59:23.594639: step 21800, loss = 1.00 (955.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:59:24.773595: step 21810, loss = 0.78 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-07 20:59:26.024301: step 21820, loss = 0.95 (1023.4 examples/sec; 0.125 sec/batch)
2017-05-07 20:59:27.286345: step 21830, loss = 0.93 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:59:28.561570: step 21840, loss = 0.80 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:59:29.813088: step 21850, loss = 0.90 (1022.8 examples/sec; 0.125 sec/batch)
2017-05-07 20:59:31.088809: step 21860, loss = 0.80 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:59:32.368073: step 21870, loss = 0.82 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:59:33.625319: step 21880, loss = 1.02 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:59:34.887494: step 21890, loss = 0.72 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:59:36.243285: step 21900, loss = 0.75 (944.1 examples/sec; 0.136 sec/batch)
2017-05-07 20:59:37.425248: step 21910, loss = 0.95 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-07 20:59:38.687498: step 21920, loss = 0.91 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:59:40.017204: step 21930, loss = 0.93 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:59:41.307912: step 21940, loss = 0.88 (991.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:59:42.560312: step 21950, loss = 0.85 (1022.0 examples/sec; 0.125 sec/batch)
2017-05-07 20:59:43.818089: step 21960, loss = 0.95 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:59:45.090624: step 21970, loss = 1.02 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:59:46.344587: step 21980, loss = 0.77 (1020.8 examples/sec; 0.125 sec/batch)
2017-05-07 20:59:47.606740: step 21990, loss = 0.92 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:59:48.978656: step 22000, loss = 0.87 (933.0 examples/sec; 0.137 sec/batch)
2017-05-07 20:59:50.154278: step 22010, loss = 1.14 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-07 20:59:51.460185: step 22020, loss = 1.06 (980.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:59:52.724558: step 22030, loss = 0.78 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:59:53.986755: step 22040, loss = 0.81 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:59:55.228666: step 22050, loss = 0.99 (1030.7 examples/sec; 0.124 sec/batch)
2017-05-07 20:59:56.500184: step 22060, loss = 1.03 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:59:57.777554: step 22070, loss = 0.73 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:59:59.057170: step 22080, loss = 0.96 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:00:00.314581: step 22090, loss = 0.86 (1018.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:00:01.683039: step 22100, loss = 0.82 (935.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:00:02.879238: step 22110, loss = 1.00 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-07 21:00:04.178424: step 22120, loss = 1.00 (985.2 examples/sec; 0.130 sec/batch)
2017-05-07 21:00:05.444295: step 22130, loss = 0.84 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:00:06.724113: step 22140, loss = 0.76 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:00:08.041100: step 22150, loss = 0.71 (971.9 examples/sec; 0.132 sec/batch)
2017-05-07 21:00:09.332059: step 22160, loss = 0.71 (991.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:00:10.607767: step 22170, loss = 0.83 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:00:11.902007: step 22180, loss = 0.94 (989.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:00:13.158747: step 22190, loss = 0.83 (1018.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:00:14.521671: step 22200, loss = 0.80 (939.2 examples/sec; 0.136 sec/batch)
2017-05-07 21:00:15.719824: step 22210, loss = 0.90 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-07 21:00:17.020057: step 22220, loss = 0.95 (984.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:00:18.301216: step 22230, loss = 0.82 (999.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:00:19.621890: step 22240, loss = 0.71 (969.2 examples/sec; 0.132 sec/batch)
2017-05-07 21:00:20.929590: step 22250, loss = 0.74 (978.8 examples/sec; 0.131 sec/batch)
2017-05-07 21:00:22.237091: step 22260, loss = 0.88 (979.0 examples/sec; 0.131 sec/batch)
2017-05-07 21:00:23.483135: step 22270, loss = 0.86 (1027.3 examples/sec; 0.125 sec/batch)
2017-05-07 21:00:24.742369: step 22280, loss = 0.87 (1016.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:00:26.003737: step 22290, loss = 1.00 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:00:27.358365: step 22300, loss = 1.06 (944.9 examples/sec; 0.135 sec/batch)
2017-05-07 21:00:28.530304: step 22310, loss = 0.91 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-07 21:00:29.803729: step 22320, loss = 0.85 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:00:31.097357: step 22330, loss = 0.81 (989.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:00:32.387282: step 22340, loss = 1.17 (992.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:00:33.659564: step 22350, loss = 1.00 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:00:34.964481: step 22360, loss = 0.93 (980.9 examples/sec; 0.130 sec/batch)
2017-05-07 21:00:36.228352: step 22370, loss = 0.82 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:00:37.482122: step 22380, loss = 1.02 (1020.9 examples/sec; 0.125 sec/batch)
2017-05-07 21:00:38.739601: step 22390, loss = 0.85 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:00:40.099563: step 22400, loss = 0.78 (941.2 examples/sec; 0.136 sec/batch)
2017-05-07 21:00:41.275520: step 22410, loss = 0.86 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-07 21:00:42.542852: step 22420, loss = 0.95 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:00:43.811506: step 22430, loss = 0.93 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:00:45.087495: step 22440, loss = 0.78 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:00:46.329956: step 22450, loss = 0.84 (1030.2 examples/sec; 0.124 sec/batch)
2017-05-07 21:00:47.627615: step 22460, loss = 0.92 (986.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:00:48.908647: step 22470, loss = 0.82 (999.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:00:50.230241: step 22480, loss = 0.91 (968.5 examples/sec; 0.132 sec/batch)
2017-05-07 21:00:51.516245: step 22490, loss = 0.89 (995.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:00:52.906952: step 22500, loss = 0.95 (920.4 examples/sec; 0.139 sec/batch)
2017-05-07 21:00:54.120953: step 22510, loss = 0.93 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-07 21:00:55.381546: step 22520, loss = 0.72 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:00:56.654571: step 22530, loss = 0.74 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:00:57.902928: step 22540, loss = 0.87 (1025.3 examples/sec; 0.125 sec/batch)
2017-05-07 21:00:59.177386: step 22550, loss = 0.79 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:01:00.452152: step 22560, loss = 0.84 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:01:01.709607: step 22570, loss = 0.89 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:01:03.004700: step 22580, loss = 0.82 (988.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:01:04.292699: step 22590, loss = 0.90 (993.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:01:05.635871: step 22600, loss = 0.89 (953.0 examples/sec; 0.134 sec/batch)
2017-05-07 21:01:06.841369: step 22610, loss = 0.81 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-07 21:01:08.127459: step 22620, loss = 1.11 (995.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:01:09.397466: step 22630, loss = 0.75 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:01:10.645850: step 22640, loss = 0.90 (1025.3 examples/sec; 0.125 sec/batch)
2017-05-07 21:01:11.914821: step 22650, loss = 0.83 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:01:13.181148: step 22660, loss = 1.00 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:01:14.431708: step 22670, loss = 0.82 (1023.5 examples/sec; 0.125 sec/batch)
2017-05-07 21:01:15.709149: step 22680, loss = 0.83 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:01:16.971402: step 22690, loss = 0.80 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:01:18.329595: step 22700, loss = 0.87 (942.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:01:19.531875: step 22710, loss = 0.82 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-07 21:01:20.831407: step 22720, loss = 0.70 (985.0 examples/sec; 0.130 sec/batch)
2017-05-07 21:01:22.071253: step 22730, loss = 0.91 (1032.4 examples/sec; 0.124 sec/batch)
2017-05-07 21:01:23.335124: step 22740, loss = 0.72 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:01:24.596111: step 22750, loss = 0.81 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:01:25.855538: step 22760, loss = 0.98 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:01:27.098416: step 22770, loss = 0.90 (1029.9 examples/sec; 0.124 sec/batch)
2017-05-07 21:01:28.377009: step 22780, loss = 0.92 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:01:29.634570: step 22790, loss = 0.92 (1017.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:01:30.974213: step 22800, loss = 1.02 (955.5 examples/sec; 0.134 sec/batch)
2017-05-07 21:01:32.128346: step 22810, loss = 0.97 (1109.1 examples/sec; 0.115 sec/batch)
2017-05-07 21:01:33.380268: step 22820, loss = 0.74 (1022.4 examples/sec; 0.125 sec/batch)
2017-05-07 21:01:34.623655: step 22830, loss = 0.80 (1029.5 examples/sec; 0.124 sec/batch)
2017-05-07 21:01:35.907660: step 22840, loss = 0.75 (996.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:01:37.167891: step 22850, loss = 0.78 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:01:38.422821: step 22860, loss = 0.71 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:01:39.700647: step 22870, loss = 1.13 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:01:40.967073: step 22880, loss = 1.27 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:01:42.235726: step 22890, loss = 1.06 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:01:43.591453: step 22900, loss = 1.05 (944.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:01:44.754555: step 22910, loss = 0.91 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-07 21:01:46.018871: step 22920, loss = 0.79 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:01:47.310764: step 22930, loss = 0.81 (990.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:01:48.584265: step 22940, loss = 0.92 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:01:49.866208: step 22950, loss = 0.82 (998.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:01:51.133013: step 22960, loss = 0.98 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:01:52.389892: step 22970, loss = 0.83 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:01:53.649515: step 22980, loss = 0.83 (1016.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:01:54.951056: step 22990, loss = 0.92 (983.5 examples/sec; 0.130 sec/batch)
2017-05-07 21:01:56.344644: step 23000, loss = 0.97 (918.5 examples/sec; 0.139 sec/batch)
2017-05-07 21:01:57.520173: step 23010, loss = 0.74 (1088.9 examples/sec; 0.118 sec/batch)
2017-05-07 21:01:58.811958: step 23020, loss = 0.84 (990.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:02:00.072453: step 23030, loss = 0.99 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:02:01.362617: step 23040, loss = 0.90 (992.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:02:02.623270: step 23050, loss = 1.06 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:02:03.910408: step 23060, loss = 1.00 (994.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:02:05.175925: step 23070, loss = 0.79 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:02:06.437601: step 23080, loss = 1.10 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:02:07.697202: step 23090, loss = 0.75 (1016.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:02:09.045088: step 23100, loss = 0.76 (949.6 examples/sec; 0.135 sec/batch)
2017-05-07 21:02:10.209480: step 23110, loss = 1.05 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-07 21:02:11.475739: step 23120, loss = 0.93 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:02:12.733128: step 23130, loss = 0.78 (1018.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:02:13.994947: step 23140, loss = 0.89 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:02:15.261522: step 23150, loss = 0.96 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:02:16.541383: step 23160, loss = 0.74 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:02:17.806903: step 23170, loss = 0.84 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:02:19.072913: step 23180, loss = 0.78 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:02:20.332123: step 23190, loss = 0.94 (1016.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:02:21.674706: step 23200, loss = 0.92 (953.4 examples/sec; 0.134 sec/batch)
2017-05-07 21:02:22.864335: step 23210, loss = 1.15 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-07 21:02:24.161203: step 23220, loss = 0.87 (987.0 examples/sec; 0.130 sec/batch)
2017-05-07 21:02:25.432235: step 23230, loss = 0.83 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:02:26.708742: step 23240, loss = 0.83 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:02:27.998425: step 23250, loss = 0.85 (992.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:02:29.308615: step 23260, loss = 0.92 (977.0 examples/sec; 0.131 sec/batch)
2017-05-07 21:02:30.578569: step 23270, loss = 0.90 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:02:31.882921: step 23280, loss = 0.81 (981.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:02:33.158110: step 23290, loss = 0.79 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:02:34.516822: step 23300, loss = 0.76 (942.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:02:35.695706: step 23310, loss = 0.93 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-07 21:02:36.998713: step 23320, loss = 0.84 (982.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:02:38.268145: step 23330, loss = 0.86 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:02:39.523540: step 23340, loss = 0.88 (1019.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:02:40.818342: step 23350, loss = 1.25 (988.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:02:42.082033: step 23360, loss = 0.86 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:02:43.381025: step 23370, loss = 0.73 (985.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:02:44.654813: step 23380, loss = 0.82 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:02:45.937381: step 23390, loss = 0.90 (998.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:02:47.313245: step 23400, loss = 0.95 (930.3 examples/sec; 0.138 sec/batch)
2017-05-07 21:02:48.527521: step 23410, loss = 1.00 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-07 21:02:49.777547: step 23420, loss = 0.82 (1024.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:02:51.038324: step 23430, loss = 0.79 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:02:52.300693: step 23440, loss = 0.87 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:02:53.555016: step 23450, loss = 0.79 (1020.5 examples/sec; 0.125 sec/batch)
2017-05-07 21:02:54.835935: step 23460, loss = 0.80 (999.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:02:56.106728: step 23470, loss = 0.99 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:02:57.398778: step 23480, loss = 0.82 (990.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:02:58.662736: step 23490, loss = 0.83 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:03:00.035599: step 23500, loss = 0.83 (932.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:03:01.242761: step 23510, loss = 0.93 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-07 21:03:02.516261: step 23520, loss = 1.02 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:03:03.809296: step 23530, loss = 0.74 (989.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:03:05.088742: step 23540, loss = 0.77 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:03:06.369805: step 23550, loss = 0.87 (999.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:03:07.650166: step 23560, loss = 0.78 (999.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:03:08.941365: step 23570, loss = 0.65 (991.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:03:10.245839: step 23580, loss = 1.00 (981.2 examples/sec; 0.130 sec/batch)
2017-05-07 21:03:11.513073: step 23590, loss = 0.90 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:03:12.857319: step 23600, loss = 0.74 (952.2 examples/sec; 0.134 sec/batch)
2017-05-07 21:03:14.022527: step 23610, loss = 0.82 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-07 21:03:15.287427: step 23620, loss = 0.73 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:03:16.559734: step 23630, loss = 0.87 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:03:17.811861: step 23640, loss = 0.64 (1022.3 examples/sec; 0.125 sec/batch)
2017-05-07 21:03:19.087505: step 23650, loss = 0.79 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:03:20.342379: step 23660, loss = 0.87 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:03:21.604379: step 23670, loss = 1.10 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:03:22.858930: step 23680, loss = 0.83 (1020.3 examples/sec; 0.125 sec/batch)
2017-05-07 21:03:24.122183: step 23690, loss = 0.82 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:03:25.483161: step 23700, loss = 0.69 (940.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:03:26.684639: step 23710, loss = 0.87 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-07 21:03:27.986302: step 23720, loss = 0.94 (983.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:03:29.254931: step 23730, loss = 0.74 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:03:30.527169: step 23740, loss = 0.77 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:03:31.785803: step 23750, loss = 0.89 (1017.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:03:33.042254: step 23760, loss = 1.03 (1018.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:03:34.288646: step 23770, loss = 0.82 (1027.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:03:35.567905: step 23780, loss = 0.78 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:03:36.834437: step 23790, loss = 0.93 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:03:38.199750: step 23800, loss = 0.83 (937.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:03:39.355801: step 23810, loss = 0.78 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-07 21:03:40.623828: step 23820, loss = 0.82 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:03:41.893123: step 23830, loss = 0.96 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:03:43.194719: step 23840, loss = 1.01 (983.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:03:44.486116: step 23850, loss = 1.15 (991.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:03:45.768868: step 23860, loss = 0.88 (997.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:03:47.029052: step 23870, loss = 0.86 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:03:48.322873: step 23880, loss = 0.81 (989.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:03:49.623740: step 23890, loss = 0.85 (984.0 examples/sec; 0.130 sec/batch)
2017-05-07 21:03:50.999790: step 23900, loss = 0.92 (930.2 examples/sec; 0.138 sec/batch)
2017-05-07 21:03:52.168091: step 23910, loss = 0.83 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-07 21:03:53.437269: step 23920, loss = 0.86 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:03:54.726506: step 23930, loss = 0.96 (992.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:03:56.028402: step 23940, loss = 0.80 (983.2 examples/sec; 0.130 sec/batch)
2017-05-07 21:03:57.306158: step 23950, loss = 0.83 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:03:58.579211: step 23960, loss = 0.87 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:03:59.847347: step 23970, loss = 1.07 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:04:01.147753: step 23980, loss = 0.84 (984.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:04:02.412210: step 23990, loss = 1.06 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:04:03.796843: step 24000, loss = 0.79 (924.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:04:04.977879: step 24010, loss = 0.96 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-07 21:04:06.268613: step 24020, loss = 0.89 (991.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:04:07.539590: step 24030, loss = 0.79 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:04:08.855452: step 24040, loss = 0.93 (972.8 examples/sec; 0.132 sec/batch)
2017-05-07 21:04:10.119250: step 24050, loss = 0.80 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:04:11.397332: step 24060, loss = 1.00 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:04:12.675418: step 24070, loss = 0.88 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:04:13.971502: step 24080, loss = 0.90 (987.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:04:15.255697: step 24090, loss = 0.91 (996.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:04:16.622312: step 24100, loss = 0.75 (936.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:04:17.808640: step 24110, loss = 0.91 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-07 21:04:19.094528: step 24120, loss = 0.79 (995.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:04:20.356627: step 24130, loss = 0.89 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:04:21.642710: step 24140, loss = 0.80 (995.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:04:22.926345: step 24150, loss = 0.86 (997.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:04:24.220895: step 24160, loss = 0.88 (988.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:04:25.500658: step 24170, loss = 0.94 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:04:26.770151: step 24180, loss = 0.72 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:04:28.045745: step 24190, loss = 0.85 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:04:29.428558: step 24200, loss = 0.68 (925.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:04:30.600762: step 24210, loss = 0.72 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-07 21:04:31.902659: step 24220, loss = 1.04 (983.2 examples/sec; 0.130 sec/batch)
2017-05-07 21:04:33.180747: step 24230, loss = 0.73 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:04:34.472770: step 24240, loss = 0.88 (990.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:04:35.757346: step 24250, loss = 1.03 (996.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:04:37.048248: step 24260, loss = 0.86 (991.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:04:38.333517: step 24270, loss = 0.88 (995.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:04:39.617076: step 24280, loss = 0.93 (997.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:04:40.911137: step 24290, loss = 0.86 (989.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:04:42.275889: step 24300, loss = 0.92 (937.9 examples/sec; 0.136 sec/batch)
2017-05-07 21:04:43.477801: step 24310, loss = 0.79 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-07 21:04:44.746459: step 24320, loss = 0.91 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:04:46.031750: step 24330, loss = 0.73 (995.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:04:47.351320: step 24340, loss = 0.92 (970.0 examples/sec; 0.132 sec/batch)
2017-05-07 21:04:48.627410: step 24350, loss = 0.72 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:04:49.914115: step 24360, loss = 0.94 (994.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:04:51.208952: step 24370, loss = 0.73 (988.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:04:52.489239: step 24380, loss = 1.03 (999.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:04:53.755655: step 24390, loss = 1.00 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:04:55.152416: step 24400, loss = 0.86 (916.4 examples/sec; 0.140 sec/batch)
2017-05-07 21:04:56.307428: step 24410, loss = 0.77 (1108.2 examples/sec; 0.116 sec/batch)
2017-05-07 21:04:57.577265: step 24420, loss = 0.91 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:04:58.816828: step 24430, loss = 0.87 (1032.6 examples/sec; 0.124 sec/batch)
2017-05-07 21:05:00.077097: step 24440, loss = 1.07 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:05:01.356095: step 24450, loss = 1.03 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:05:02.633311: step 24460, loss = 0.89 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:05:03.893201: step 24470, loss = 0.83 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:05:05.159455: step 24480, loss = 0.78 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:05:06.439129: step 24490, loss = 1.02 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:05:07.792495: step 24500, loss = 0.82 (945.8 examples/sec; 0.135 sec/batch)
2017-05-07 21:05:08.990588: step 24510, loss = 0.86 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-07 21:05:10.282305: step 24520, loss = 0.90 (990.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:05:11.561821: step 24530, loss = 0.85 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:05:12.837427: step 24540, loss = 0.78 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:05:14.103872: step 24550, loss = 0.79 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:05:15.364784: step 24560, loss = 0.75 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:05:16.628051: step 24570, loss = 0.79 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:05:17.884712: step 24580, loss = 0.80 (1018.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:05:19.174957: step 24590, loss = 1.07 (992.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:05:20.536070: step 24600, loss = 0.71 (940.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:05:21.707454: step 24610, loss = 0.88 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-07 21:05:22.972248: step 24620, loss = 0.80 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:05:24.248103: step 24630, loss = 0.68 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:05:25.518712: step 24640, loss = 0.89 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:05:26.782919: step 24650, loss = 0.79 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:05:28.064204: step 24660, loss = 0.97 (999.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:05:29.345333: step 24670, loss = 0.94 (999.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:05:30.622200: step 24680, loss = 0.80 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:05:31.924179: step 24690, loss = 0.66 (983.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:05:33.313755: step 24700, loss = 0.96 (921.1 examples/sec; 0.139 sec/batch)
2017-05-07 21:05:34.502242: step 24710, loss = 0.77 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-07 21:05:35.760543: step 24720, loss = 0.96 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:05:37.045848: step 24730, loss = 0.86 (995.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:05:38.290707: step 24740, loss = 0.94 (1028.2 examples/sec; 0.124 sec/batch)
2017-05-07 21:05:39.596196: step 24750, loss = 0.87 (980.5 examples/sec; 0.131 sec/batch)
2017-05-07 21:05:40.845549: step 24760, loss = 0.80 (1024.5 examples/sec; 0.125 sec/batch)
2017-05-07 21:05:42.107610: step 24770, loss = 0.90 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:05:43.369404: step 24780, loss = 0.87 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:05:44.649036: step 24790, loss = 0.79 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:05:45.984846: step 24800, loss = 0.88 (958.2 examples/sec; 0.134 sec/batch)
2017-05-07 21:05:47.153470: step 24810, loss = 0.97 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-07 21:05:48.448584: step 24820, loss = 0.89 (988.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:05:49.723454: step 24830, loss = 0.83 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:05:51.001775: step 24840, loss = 0.78 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:05:52.266394: step 24850, loss = 0.87 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:05:53.536352: step 24860, loss = 0.92 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:05:54.789714: step 24870, loss = 0.88 (1021.3 examples/sec; 0.125 sec/batch)
2017-05-07 21:05:56.062820: step 24880, loss = 0.84 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:05:57.323087: step 24890, loss = 0.80 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:05:58.675680: step 24900, loss = 0.81 (946.3 examples/sec; 0.135 sec/batch)
2017-05-07 21:05:59.843172: step 24910, loss = 0.78 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-07 21:06:01.108208: step 24920, loss = 0.80 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:06:02.369424: step 24930, loss = 1.00 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:06:03.629163: step 24940, loss = 0.80 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:06:04.911709: step 24950, loss = 1.00 (998.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:06:06.173394: step 24960, loss = 0.82 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:06:07.446741: step 24970, loss = 0.91 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:06:08.701873: step 24980, loss = 0.83 (1019.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:06:09.969435: step 24990, loss = 0.75 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:06:11.332452: step 25000, loss = 0.82 (939.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:06:12.504530: step 25010, loss = 0.79 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-07 21:06:13.766943: step 25020, loss = 0.80 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:06:15.026410: step 25030, loss = 1.13 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:06:16.304804: step 25040, loss = 1.04 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:06:17.564823: step 25050, loss = 0.83 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:06:18.821041: step 25060, loss = 0.79 (1018.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:06:20.078189: step 25070, loss = 0.77 (1018.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:06:21.334136: step 25080, loss = 0.79 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:06:22.605116: step 25090, loss = 1.12 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:06:23.962098: step 25100, loss = 0.88 (943.3 examples/sec; 0.136 sec/batch)
2017-05-07 21:06:25.144934: step 25110, loss = 0.99 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-07 21:06:26.420101: step 25120, loss = 1.07 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:06:27.667286: step 25130, loss = 0.81 (1026.3 examples/sec; 0.125 sec/batch)
2017-05-07 21:06:28.951272: step 25140, loss = 0.80 (996.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:06:30.241506: step 25150, loss = 1.15 (992.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:06:31.517142: step 25160, loss = 0.89 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:06:32.810385: step 25170, loss = 0.83 (989.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:06:34.104822: step 25180, loss = 0.91 (988.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:06:35.375916: step 25190, loss = 0.69 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:06:36.739309: step 25200, loss = 0.87 (938.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:06:37.933493: step 25210, loss = 0.89 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-07 21:06:39.232856: step 25220, loss = 0.95 (985.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:06:40.523408: step 25230, loss = 0.91 (991.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:06:41.815081: step 25240, loss = 0.77 (991.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:06:43.096780: step 25250, loss = 0.88 (998.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:06:44.396734: step 25260, loss = 0.98 (984.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:06:45.689140: step 25270, loss = 1.12 (990.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:06:46.993880: step 25280, loss = 0.93 (981.0 examples/sec; 0.130 sec/batch)
2017-05-07 21:06:48.270669: step 25290, loss = 0.89 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:06:49.649027: step 25300, loss = 0.74 (928.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:06:50.854263: step 25310, loss = 0.89 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-07 21:06:52.128159: step 25320, loss = 0.97 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:06:53.408943: step 25330, loss = 0.91 (999.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:06:54.686193: step 25340, loss = 1.04 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:06:55.958825: step 25350, loss = 0.94 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:06:57.232364: step 25360, loss = 0.97 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:06:58.499083: step 25370, loss = 0.99 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:06:59.778872: step 25380, loss = 1.06 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:07:01.066352: step 25390, loss = 0.99 (994.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:07:02.426526: step 25400, loss = 0.83 (941.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:07:03.614226: step 25410, loss = 0.91 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-07 21:07:04.928571: step 25420, loss = 0.80 (973.9 examples/sec; 0.131 sec/batch)
2017-05-07 21:07:06.236950: step 25430, loss = 0.88 (978.3 examples/sec; 0.131 sec/batch)
2017-05-07 21:07:07.497373: step 25440, loss = 0.76 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:07:08.789866: step 25450, loss = 0.80 (990.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:07:10.068867: step 25460, loss = 0.89 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:07:11.372867: step 25470, loss = 0.72 (981.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:07:12.620100: step 25480, loss = 0.89 (1026.3 examples/sec; 0.125 sec/batch)
2017-05-07 21:07:13.878083: step 25490, loss = 0.91 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:07:15.244792: step 25500, loss = 1.10 (936.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:07:16.412043: step 25510, loss = 0.91 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-07 21:07:17.679999: step 25520, loss = 0.88 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:07:18.950026: step 25530, loss = 0.95 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:07:20.215906: step 25540, loss = 0.85 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:07:21.491507: step 25550, loss = 0.80 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:07:22.806085: step 25560, loss = 0.86 (973.7 examples/sec; 0.131 sec/batch)
2017-05-07 21:07:24.105082: step 25570, loss = 0.93 (985.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:07:25.366329: step 25580, loss = 0.86 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:07:26.634743: step 25590, loss = 0.83 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:07:27.996719: step 25600, loss = 0.73 (939.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:07:29.171494: step 25610, loss = 0.87 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-07 21:07:30.441828: step 25620, loss = 0.89 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:07:31.703130: step 25630, loss = 0.85 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:07:32.975941: step 25640, loss = 0.79 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:07:34.243517: step 25650, loss = 0.81 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:07:35.522207: step 25660, loss = 0.75 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:07:36.805308: step 25670, loss = 0.76 (997.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:07:38.074229: step 25680, loss = 0.85 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:07:39.352127: step 25690, loss = 0.86 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:07:40.716236: step 25700, loss = 0.73 (938.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:07:41.881102: step 25710, loss = 0.89 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-07 21:07:43.156748: step 25720, loss = 0.80 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:07:44.420137: step 25730, loss = 0.84 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:07:45.676426: step 25740, loss = 0.90 (1018.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:07:46.965384: step 25750, loss = 0.91 (993.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:07:48.257951: step 25760, loss = 1.00 (990.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:07:49.521953: step 25770, loss = 0.87 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:07:50.786079: step 25780, loss = 0.89 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:07:52.068729: step 25790, loss = 0.79 (997.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:07:53.451413: step 25800, loss = 0.85 (925.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:07:54.646126: step 25810, loss = 0.90 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-07 21:07:55.923527: step 25820, loss = 0.73 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:07:57.193488: step 25830, loss = 1.03 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:07:58.477882: step 25840, loss = 0.79 (996.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:07:59.765896: step 25850, loss = 1.12 (993.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:08:01.038210: step 25860, loss = 0.88 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:08:02.308173: step 25870, loss = 0.83 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:08:03.603534: step 25880, loss = 0.91 (988.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:08:04.886227: step 25890, loss = 1.20 (997.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:08:06.241865: step 25900, loss = 0.91 (944.2 examples/sec; 0.136 sec/batch)
2017-05-07 21:08:07.410559: step 25910, loss = 0.73 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-07 21:08:08.675878: step 25920, loss = 0.71 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:08:09.933142: step 25930, loss = 0.84 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:08:11.215519: step 25940, loss = 0.76 (998.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:08:12.470133: step 25950, loss = 0.81 (1020.2 examples/sec; 0.125 sec/batch)
2017-05-07 21:08:13.735489: step 25960, loss = 0.85 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:08:14.992859: step 25970, loss = 0.76 (1018.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:08:16.273669: step 25980, loss = 0.75 (999.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:08:17.537459: step 25990, loss = 0.72 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:08:18.865630: step 26000, loss = 0.82 (963.7 examples/sec; 0.133 sec/batch)
2017-05-07 21:08:20.051397: step 26010, loss = 0.83 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-07 21:08:21.329868: step 26020, loss = 0.90 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:08:22.596177: step 26030, loss = 0.93 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:08:23.859176: step 26040, loss = 0.92 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:08:25.138882: step 26050, loss = 0.91 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:08:26.407094: step 26060, loss = 1.05 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:08:27.703870: step 26070, loss = 0.83 (987.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:08:29.012308: step 26080, loss = 0.84 (978.3 examples/sec; 0.131 sec/batch)
2017-05-07 21:08:30.291958: step 26090, loss = 0.68 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:08:31.641350: step 26100, loss = 0.88 (948.6 examples/sec; 0.135 sec/batch)
2017-05-07 21:08:32.807526: step 26110, loss = 0.98 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-07 21:08:34.070809: step 26120, loss = 1.06 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:08:35.338780: step 26130, loss = 0.89 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:08:36.618102: step 26140, loss = 0.95 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:08:37.895840: step 26150, loss = 0.78 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:08:39.189747: step 26160, loss = 0.75 (989.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:08:40.505033: step 26170, loss = 0.74 (973.2 examples/sec; 0.132 sec/batch)
2017-05-07 21:08:41.779644: step 26180, loss = 0.70 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:08:43.056369: step 26190, loss = 0.95 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:08:44.401750: step 26200, loss = 0.87 (951.4 examples/sec; 0.135 sec/batch)
2017-05-07 21:08:45.565035: step 26210, loss = 0.70 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-07 21:08:46.855451: step 26220, loss = 0.74 (991.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:08:48.102846: step 26230, loss = 0.87 (1026.1 examples/sec; 0.125 sec/batch)
2017-05-07 21:08:49.397015: step 26240, loss = 0.78 (989.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:08:50.690842: step 26250, loss = 1.23 (989.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:08:51.989711: step 26260, loss = 0.78 (985.5 examples/sec; 0.130 sec/batch)
2017-05-07 21:08:53.293040: step 26270, loss = 1.02 (982.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:08:54.581585: step 26280, loss = 1.00 (993.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:08:55.846045: step 26290, loss = 0.80 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:08:57.200057: step 26300, loss = 0.85 (945.3 examples/sec; 0.135 sec/batch)
2017-05-07 21:08:58.388223: step 26310, loss = 0.85 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-07 21:08:59.700189: step 26320, loss = 0.82 (975.6 examples/sec; 0.131 sec/batch)
2017-05-07 21:09:00.978816: step 26330, loss = 0.91 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:09:02.274845: step 26340, loss = 0.78 (987.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:09:03.540453: step 26350, loss = 0.84 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:09:04.841666: step 26360, loss = 0.99 (983.7 examples/sec; 0.130 sec/batch)
2017-05-07 21:09:06.113120: step 26370, loss = 0.85 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:09:07.406753: step 26380, loss = 0.80 (989.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:09:08.672613: step 26390, loss = 0.88 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:09:10.018615: step 26400, loss = 0.82 (951.0 examples/sec; 0.135 sec/batch)
2017-05-07 21:09:11.180186: step 26410, loss = 0.84 (1102.0 examples/sec; 0.116 sec/batch)
2017-05-07 21:09:12.460231: step 26420, loss = 0.93 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:09:13.740158: step 26430, loss = 0.85 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:09:15.020773: step 26440, loss = 1.06 (999.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:09:16.317170: step 26450, loss = 0.82 (987.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:09:17.590841: step 26460, loss = 0.86 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:09:18.881534: step 26470, loss = 1.06 (991.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:09:20.160695: step 26480, loss = 0.93 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:09:21.421246: step 26490, loss = 0.87 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:09:22.796551: step 26500, loss = 0.87 (930.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:09:23.981797: step 26510, loss = 0.85 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-07 21:09:25.245707: step 26520, loss = 0.89 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:09:26.547822: step 26530, loss = 0.90 (983.0 examples/sec; 0.130 sec/batch)
2017-05-07 21:09:27.814891: step 26540, loss = 0.91 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:09:29.095930: step 26550, loss = 0.82 (999.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:09:30.337797: step 26560, loss = 0.91 (1030.7 examples/sec; 0.124 sec/batch)
2017-05-07 21:09:31.610829: step 26570, loss = 0.78 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:09:32.883823: step 26580, loss = 0.96 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:09:34.140714: step 26590, loss = 0.92 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:09:35.512074: step 26600, loss = 0.73 (933.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:09:36.695764: step 26610, loss = 1.05 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-07 21:09:37.978847: step 26620, loss = 0.75 (997.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:09:39.266701: step 26630, loss = 0.82 (993.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:09:40.561522: step 26640, loss = 0.86 (988.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:09:41.848631: step 26650, loss = 0.87 (994.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:09:43.135827: step 26660, loss = 0.98 (994.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:09:44.429606: step 26670, loss = 0.86 (989.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:09:45.700041: step 26680, loss = 0.74 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:09:47.043497: step 26690, loss = 0.87 (952.8 examples/sec; 0.134 sec/batch)
2017-05-07 21:09:48.379812: step 26700, loss = 0.87 (957.9 examples/sec; 0.134 sec/batch)
2017-05-07 21:09:49.597289: step 26710, loss = 0.95 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-07 21:09:50.862529: step 26720, loss = 0.85 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:09:52.174778: step 26730, loss = 0.89 (975.4 examples/sec; 0.131 sec/batch)
2017-05-07 21:09:53.439036: step 26740, loss = 0.87 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:09:54.732481: step 26750, loss = 0.78 (989.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:09:56.028724: step 26760, loss = 0.99 (987.5 examples/sec; 0.130 sec/batch)
2017-05-07 21:09:57.308552: step 26770, loss = 0.90 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:09:58.571943: step 26780, loss = 0.74 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:09:59.855257: step 26790, loss = 1.04 (997.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:10:01.229267: step 26800, loss = 0.75 (931.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:10:02.444619: step 26810, loss = 0.80 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-07 21:10:03.743801: step 26820, loss = 0.85 (985.2 examples/sec; 0.130 sec/batch)
2017-05-07 21:10:05.058652: step 26830, loss = 0.77 (973.5 examples/sec; 0.131 sec/batch)
2017-05-07 21:10:06.338030: step 26840, loss = 1.06 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:10:07.603768: step 26850, loss = 0.62 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:10:08.881284: step 26860, loss = 0.84 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:10:10.162781: step 26870, loss = 0.85 (998.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:10:11.439866: step 26880, loss = 1.03 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:10:12.694772: step 26890, loss = 0.77 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:10:14.073175: step 26900, loss = 0.88 (928.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:10:15.267125: step 26910, loss = 0.85 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-07 21:10:16.569593: step 26920, loss = 0.67 (982.8 examples/sec; 0.130 sec/batch)
2017-05-07 21:10:17.848310: step 26930, loss = 0.75 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:10:19.142907: step 26940, loss = 0.85 (988.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:10:20.447655: step 26950, loss = 0.83 (981.0 examples/sec; 0.130 sec/batch)
2017-05-07 21:10:21.759806: step 26960, loss = 0.75 (975.5 examples/sec; 0.131 sec/batch)
2017-05-07 21:10:23.138364: step 26970, loss = 0.84 (928.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:10:24.449226: step 26980, loss = 0.83 (976.5 examples/sec; 0.131 sec/batch)
2017-05-07 21:10:25.756388: step 26990, loss = 0.76 (979.2 examples/sec; 0.131 sec/batch)
2017-05-07 21:10:27.214607: step 27000, loss = 0.92 (877.8 examples/sec; 0.146 sec/batch)
2017-05-07 21:10:28.448177: step 27010, loss = 1.16 (1037.6 examples/sec; 0.123 sec/batch)
2017-05-07 21:10:29.737680: step 27020, loss = 0.83 (992.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:10:31.108820: step 27030, loss = 0.92 (933.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:10:32.471895: step 27040, loss = 0.95 (939.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:10:33.822301: step 27050, loss = 0.88 (947.9 examples/sec; 0.135 sec/batch)
2017-05-07 21:10:35.235143: step 27060, loss = 0.78 (906.0 examples/sec; 0.141 sec/batch)
2017-05-07 21:10:36.604023: step 27070, loss = 1.13 (935.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:10:37.983334: step 27080, loss = 0.91 (928.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:10:39.331244: step 27090, loss = 0.76 (949.6 examples/sec; 0.135 sec/batch)
2017-05-07 21:10:40.804589: step 27100, loss = 0.79 (868.8 examples/sec; 0.147 sec/batch)
2017-05-07 21:10:42.059158: step 27110, loss = 0.70 (1020.3 examples/sec; 0.125 sec/batch)
2017-05-07 21:10:43.425867: step 27120, loss = 0.78 (936.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:10:44.808503: step 27130, loss = 0.73 (925.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:10:46.174506: step 27140, loss = 0.82 (937.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:10:47.506117: step 27150, loss = 0.80 (961.2 examples/sec; 0.133 sec/batch)
2017-05-07 21:10:48.875350: step 27160, loss = 1.02 (934.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:10:50.230745: step 27170, loss = 1.05 (944.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:10:51.567646: step 27180, loss = 0.76 (957.4 examples/sec; 0.134 sec/batch)
2017-05-07 21:10:52.923442: step 27190, loss = 0.91 (944.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:10:54.382100: step 27200, loss = 0.94 (877.5 examples/sec; 0.146 sec/batch)
2017-05-07 21:10:55.662254: step 27210, loss = 1.08 (999.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:10:57.032868: step 27220, loss = 0.86 (933.9 examples/sec; 0.137 sec/batch)
2017-05-07 21:10:58.408632: step 27230, loss = 0.72 (930.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:10:59.774019: step 27240, loss = 0.94 (937.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:11:01.125738: step 27250, loss = 0.95 (946.9 examples/sec; 0.135 sec/batch)
2017-05-07 21:11:02.506616: step 27260, loss = 0.76 (926.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:11:03.887912: step 27270, loss = 0.92 (926.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:11:05.263316: step 27280, loss = 0.88 (930.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:11:06.637641: step 27290, loss = 0.86 (931.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:11:08.120253: step 27300, loss = 0.84 (863.3 examples/sec; 0.148 sec/batch)
2017-05-07 21:11:09.386713: step 27310, loss = 0.74 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:11:10.748836: step 27320, loss = 0.83 (939.7 examples/sec; 0.136 sec/batch)
2017-05-07 21:11:12.117089: step 27330, loss = 0.70 (935.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:11:13.523079: step 27340, loss = 0.79 (910.4 examples/sec; 0.141 sec/batch)
2017-05-07 21:11:14.891374: step 27350, loss = 0.82 (935.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:11:16.265345: step 27360, loss = 0.90 (931.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:11:17.669545: step 27370, loss = 0.87 (911.6 examples/sec; 0.140 sec/batch)
2017-05-07 21:11:19.054284: step 27380, loss = 0.72 (924.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:11:20.446397: step 27390, loss = 0.87 (919.5 examples/sec; 0.139 sec/batch)
2017-05-07 21:11:21.935102: step 27400, loss = 1.00 (859.8 examples/sec; 0.149 sec/batch)
2017-05-07 21:11:23.237629: step 27410, loss = 0.79 (982.7 examples/sec; 0.130 sec/batch)
2017-05-07 21:11:24.607814: step 27420, loss = 0.88 (934.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:11:25.975270: step 27430, loss = 0.83 (936.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:11:27.316928: step 27440, loss = 0.85 (954.0 examples/sec; 0.134 sec/batch)
2017-05-07 21:11:28.674789: step 27450, loss = 0.81 (942.7 examples/sec; 0.136 sec/batch)
2017-05-07 21:11:30.046019: step 27460, loss = 0.87 (933.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:11:31.437391: step 27470, loss = 0.73 (920.0 examples/sec; 0.139 sec/batch)
2017-05-07 21:11:32.779209: step 27480, loss = 0.84 (953.9 examples/sec; 0.134 sec/batch)
2017-05-07 21:11:34.135914: step 27490, loss = 0.78 (943.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:11:35.615324: step 27500, loss = 0.86 (865.2 examples/sec; 0.148 sec/batch)
2017-05-07 21:11:36.902406: step 27510, loss = 0.82 (994.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:11:38.265744: step 27520, loss = 0.76 (938.9 examples/sec; 0.136 sec/batch)
2017-05-07 21:11:39.643071: step 27530, loss = 0.71 (929.3 examples/sec; 0.138 sec/batch)
2017-05-07 21:11:40.985039: step 27540, loss = 0.86 (953.8 examples/sec; 0.134 sec/batch)
2017-05-07 21:11:42.321109: step 27550, loss = 0.96 (958.0 examples/sec; 0.134 sec/batch)
2017-05-07 21:11:43.742236: step 27560, loss = 1.02 (900.7 examples/sec; 0.142 sec/batch)
2017-05-07 21:11:45.114773: step 27570, loss = 1.02 (932.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:11:46.483825: step 27580, loss = 0.81 (935.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:11:47.859147: step 27590, loss = 0.89 (930.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:11:49.324191: step 27600, loss = 1.09 (873.7 examples/sec; 0.147 sec/batch)
2017-05-07 21:11:50.586493: step 27610, loss = 0.97 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:11:51.947070: step 27620, loss = 0.80 (940.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:11:53.344175: step 27630, loss = 0.87 (916.2 examples/sec; 0.140 sec/batch)
2017-05-07 21:11:54.723090: step 27640, loss = 0.92 (928.3 examples/sec; 0.138 sec/batch)
2017-05-07 21:11:56.120638: step 27650, loss = 0.90 (915.9 examples/sec; 0.140 sec/batch)
2017-05-07 21:11:57.507185: step 27660, loss = 0.89 (923.2 examples/sec; 0.139 sec/batch)
2017-05-07 21:11:58.850440: step 27670, loss = 0.88 (952.9 examples/sec; 0.134 sec/batch)
2017-05-07 21:12:00.263722: step 27680, loss = 0.88 (905.7 examples/sec; 0.141 sec/batch)
2017-05-07 21:12:01.606310: step 27690, loss = 1.03 (953.4 examples/sec; 0.134 sec/batch)
2017-05-07 21:12:03.095297: step 27700, loss = 0.86 (859.6 examples/sec; 0.149 sec/batch)
2017-05-07 21:12:04.395668: step 27710, loss = 0.96 (984.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:12:05.787362: step 27720, loss = 0.68 (919.7 examples/sec; 0.139 sec/batch)
2017-05-07 21:12:07.150304: step 27730, loss = 0.76 (939.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:12:08.535522: step 27740, loss = 0.78 (924.0 examples/sec; 0.139 sec/batch)
2017-05-07 21:12:09.911393: step 27750, loss = 0.76 (930.3 examples/sec; 0.138 sec/batch)
2017-05-07 21:12:11.284689: step 27760, loss = 0.92 (932.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:12:12.646185: step 27770, loss = 0.93 (940.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:12:14.011480: step 27780, loss = 0.77 (937.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:12:15.399100: step 27790, loss = 0.83 (922.4 examples/sec; 0.139 sec/batch)
2017-05-07 21:12:16.858999: step 27800, loss = 0.91 (876.8 examples/sec; 0.146 sec/batch)
2017-05-07 21:12:18.151599: step 27810, loss = 0.82 (990.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:12:19.523491: step 27820, loss = 0.97 (933.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:12:20.901735: step 27830, loss = 0.92 (928.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:12:22.260781: step 27840, loss = 0.69 (941.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:12:23.657584: step 27850, loss = 0.80 (916.4 examples/sec; 0.140 sec/batch)
2017-05-07 21:12:25.065123: step 27860, loss = 0.92 (909.4 examples/sec; 0.141 sec/batch)
2017-05-07 21:12:26.432400: step 27870, loss = 0.84 (936.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:12:27.816273: step 27880, loss = 0.92 (924.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:12:29.250173: step 27890, loss = 0.73 (892.7 examples/sec; 0.143 sec/batch)
2017-05-07 21:12:30.696360: step 27900, loss = 0.80 (885.1 examples/sec; 0.145 sec/batch)
2017-05-07 21:12:31.956336: step 27910, loss = 0.89 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:12:33.352442: step 27920, loss = 0.81 (916.8 examples/sec; 0.140 sec/batch)
2017-05-07 21:12:34.762639: step 27930, loss = 0.76 (907.7 examples/sec; 0.141 sec/batch)
2017-05-07 21:12:36.164892: step 27940, loss = 0.93 (912.8 examples/sec; 0.140 sec/batch)
2017-05-07 21:12:37.547631: step 27950, loss = 0.80 (925.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:12:38.935922: step 27960, loss = 0.86 (922.0 examples/sec; 0.139 sec/batch)
2017-05-07 21:12:40.310183: step 27970, loss = 0.89 (931.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:12:41.680775: step 27980, loss = 0.96 (933.9 examples/sec; 0.137 sec/batch)
2017-05-07 21:12:43.053101: step 27990, loss = 0.81 (932.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:12:44.546022: step 28000, loss = 0.76 (857.4 examples/sec; 0.149 sec/batch)
2017-05-07 21:12:45.794533: step 28010, loss = 1.01 (1025.2 examples/sec; 0.125 sec/batch)
2017-05-07 21:12:47.163112: step 28020, loss = 0.78 (935.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:12:48.542377: step 28030, loss = 0.79 (928.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:12:49.888734: step 28040, loss = 0.80 (950.7 examples/sec; 0.135 sec/batch)
2017-05-07 21:12:51.261851: step 28050, loss = 0.83 (932.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:12:52.634018: step 28060, loss = 0.80 (932.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:12:54.004382: step 28070, loss = 0.90 (934.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:12:55.381158: step 28080, loss = 0.71 (929.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:12:56.760249: step 28090, loss = 0.78 (928.1 examples/sec; 0.138 sec/batch)
2017-05-07 21:12:58.214634: step 28100, loss = 0.85 (880.1 examples/sec; 0.145 sec/batch)
2017-05-07 21:12:59.517566: step 28110, loss = 0.64 (982.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:13:00.923294: step 28120, loss = 0.85 (910.6 examples/sec; 0.141 sec/batch)
2017-05-07 21:13:02.274798: step 28130, loss = 0.83 (947.1 examples/sec; 0.135 sec/batch)
2017-05-07 21:13:03.628802: step 28140, loss = 0.92 (945.3 examples/sec; 0.135 sec/batch)
2017-05-07 21:13:04.985533: step 28150, loss = 1.01 (943.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:13:06.361911: step 28160, loss = 0.69 (930.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:13:07.710207: step 28170, loss = 0.93 (949.3 examples/sec; 0.135 sec/batch)
2017-05-07 21:13:09.075937: step 28180, loss = 0.90 (937.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:13:10.444939: step 28190, loss = 0.85 (935.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:13:11.921597: step 28200, loss = 0.77 (866.8 examples/sec; 0.148 sec/batch)
2017-05-07 21:13:13.205504: step 28210, loss = 0.98 (997.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:13:14.596175: step 28220, loss = 0.93 (920.4 examples/sec; 0.139 sec/batch)
2017-05-07 21:13:15.971567: step 28230, loss = 0.90 (930.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:13:17.359145: step 28240, loss = 1.09 (922.5 examples/sec; 0.139 sec/batch)
2017-05-07 21:13:18.751896: step 28250, loss = 0.84 (919.0 examples/sec; 0.139 sec/batch)
2017-05-07 21:13:20.119631: step 28260, loss = 0.87 (935.9 examples/sec; 0.137 sec/batch)
2017-05-07 21:13:21.459505: step 28270, loss = 0.73 (955.3 examples/sec; 0.134 sec/batch)
2017-05-07 21:13:22.825034: step 28280, loss = 0.80 (937.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:13:24.187635: step 28290, loss = 0.88 (939.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:13:25.651581: step 28300, loss = 0.93 (874.3 examples/sec; 0.146 sec/batch)
2017-05-07 21:13:26.989868: step 28310, loss = 1.01 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 21:13:28.382021: step 28320, loss = 0.85 (919.5 examples/sec; 0.139 sec/batch)
2017-05-07 21:13:29.745580: step 28330, loss = 0.81 (938.7 examples/sec; 0.136 sec/batch)
2017-05-07 21:13:31.127239: step 28340, loss = 0.89 (926.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:13:32.499320: step 28350, loss = 0.97 (932.9 examples/sec; 0.137 sec/batch)
2017-05-07 21:13:33.883295: step 28360, loss = 0.83 (924.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:13:35.256867: step 28370, loss = 0.74 (931.9 examples/sec; 0.137 sec/batch)
2017-05-07 21:13:36.622539: step 28380, loss = 0.72 (937.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:13:38.034416: step 28390, loss = 0.82 (906.6 examples/sec; 0.141 sec/batch)
2017-05-07 21:13:39.493604: step 28400, loss = 0.72 (877.2 examples/sec; 0.146 sec/batch)
2017-05-07 21:13:40.803686: step 28410, loss = 0.85 (977.1 examples/sec; 0.131 sec/batch)
2017-05-07 21:13:42.203145: step 28420, loss = 0.92 (914.6 examples/sec; 0.140 sec/batch)
2017-05-07 21:13:43.569490: step 28430, loss = 0.89 (936.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:13:44.952951: step 28440, loss = 0.89 (925.2 examples/sec; 0.138 sec/batch)
2017-05-07 21:13:46.342156: step 28450, loss = 0.92 (921.4 examples/sec; 0.139 sec/batch)
2017-05-07 21:13:47.733207: step 28460, loss = 0.94 (920.2 examples/sec; 0.139 sec/batch)
2017-05-07 21:13:49.129822: step 28470, loss = 0.87 (916.5 examples/sec; 0.140 sec/batch)
2017-05-07 21:13:50.519285: step 28480, loss = 0.89 (921.2 examples/sec; 0.139 sec/batch)
2017-05-07 21:13:51.885567: step 28490, loss = 0.78 (936.9 examples/sec; 0.137 sec/batch)
2017-05-07 21:13:53.341811: step 28500, loss = 0.92 (879.0 examples/sec; 0.146 sec/batch)
2017-05-07 21:13:54.611102: step 28510, loss = 0.97 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:13:55.993906: step 28520, loss = 0.80 (925.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:13:57.375377: step 28530, loss = 0.72 (926.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:13:58.725304: step 28540, loss = 0.85 (948.2 examples/sec; 0.135 sec/batch)
2017-05-07 21:14:00.119008: step 28550, loss = 0.85 (918.4 examples/sec; 0.139 sec/batch)
2017-05-07 21:14:01.468253: step 28560, loss = 0.73 (948.7 examples/sec; 0.135 sec/batch)
2017-05-07 21:14:02.874793: step 28570, loss = 1.01 (910.0 examples/sec; 0.141 sec/batch)
2017-05-07 21:14:04.228864: step 28580, loss = 0.97 (945.3 examples/sec; 0.135 sec/batch)
2017-05-07 21:14:05.649328: step 28590, loss = 0.83 (901.1 examples/sec; 0.142 sec/batch)
2017-05-07 21:14:07.124686: step 28600, loss = 0.80 (867.6 examples/sec; 0.148 sec/batch)
2017-05-07 21:14:08.384475: step 28610, loss = 0.94 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:14:09.761191: step 28620, loss = 0.90 (929.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:14:11.132116: step 28630, loss = 0.67 (933.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:14:12.487477: step 28640, loss = 1.05 (944.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:14:13.870707: step 28650, loss = 1.01 (925.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:14:15.228816: step 28660, loss = 0.80 (942.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:14:16.606832: step 28670, loss = 0.79 (928.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:14:17.950704: step 28680, loss = 0.79 (952.5 examples/sec; 0.134 sec/batch)
2017-05-07 21:14:19.318719: step 28690, loss = 0.90 (935.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:14:20.772833: step 28700, loss = 1.00 (880.3 examples/sec; 0.145 sec/batch)
2017-05-07 21:14:22.046926: step 28710, loss = 0.80 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:14:23.428339: step 28720, loss = 0.84 (926.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:14:24.802230: step 28730, loss = 0.53 (931.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:14:26.163315: step 28740, loss = 1.00 (940.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:14:27.537195: step 28750, loss = 0.74 (931.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:14:28.892634: step 28760, loss = 0.91 (944.3 examples/sec; 0.136 sec/batch)
2017-05-07 21:14:30.280836: step 28770, loss = 0.82 (922.1 examples/sec; 0.139 sec/batch)
2017-05-07 21:14:31.666860: step 28780, loss = 0.86 (923.5 examples/sec; 0.139 sec/batch)
2017-05-07 21:14:33.056870: step 28790, loss = 0.66 (920.9 examples/sec; 0.139 sec/batch)
2017-05-07 21:14:34.499563: step 28800, loss = 0.87 (887.2 examples/sec; 0.144 sec/batch)
2017-05-07 21:14:35.787505: step 28810, loss = 0.80 (993.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:14:37.144211: step 28820, loss = 0.81 (943.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:14:38.509342: step 28830, loss = 1.00 (937.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:14:39.867202: step 28840, loss = 0.90 (942.7 examples/sec; 0.136 sec/batch)
2017-05-07 21:14:41.257166: step 28850, loss = 0.72 (920.9 examples/sec; 0.139 sec/batch)
2017-05-07 21:14:42.597999: step 28860, loss = 0.94 (954.6 examples/sec; 0.134 sec/batch)
2017-05-07 21:14:43.971795: step 28870, loss = 1.02 (931.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:14:45.361844: step 28880, loss = 0.89 (920.8 examples/sec; 0.139 sec/batch)
2017-05-07 21:14:46.747523: step 28890, loss = 0.90 (923.7 examples/sec; 0.139 sec/batch)
2017-05-07 21:14:48.191210: step 28900, loss = 0.87 (886.6 examples/sec; 0.144 sec/batch)
2017-05-07 21:14:49.453087: step 28910, loss = 1.00 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:14:50.817542: step 28920, loss = 0.67 (938.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:14:52.184795: step 28930, loss = 0.84 (936.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:14:53.550516: step 28940, loss = 0.84 (937.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:14:54.921186: step 28950, loss = 0.88 (933.9 examples/sec; 0.137 sec/batch)
2017-05-07 21:14:56.298915: step 28960, loss = 0.88 (929.1 examples/sec; 0.138 sec/batch)
2017-05-07 21:14:57.663515: step 28970, loss = 0.90 (938.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:14:59.048602: step 28980, loss = 0.69 (924.1 examples/sec; 0.139 sec/batch)
2017-05-07 21:15:00.422192: step 28990, loss = 1.01 (931.9 examples/sec; 0.137 sec/batch)
2017-05-07 21:15:01.884597: step 29000, loss = 0.63 (875.3 examples/sec; 0.146 sec/batch)
2017-05-07 21:15:03.183373: step 29010, loss = 0.79 (985.5 examples/sec; 0.130 sec/batch)
2017-05-07 21:15:04.565710: step 29020, loss = 0.78 (926.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:15:05.916315: step 29030, loss = 0.82 (947.7 examples/sec; 0.135 sec/batch)
2017-05-07 21:15:07.293893: step 29040, loss = 0.77 (929.2 examples/sec; 0.138 sec/batch)
2017-05-07 21:15:08.644392: step 29050, loss = 0.72 (947.8 examples/sec; 0.135 sec/batch)
2017-05-07 21:15:10.025641: step 29060, loss = 0.77 (926.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:15:11.381555: step 29070, loss = 0.82 (944.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:15:12.758479: step 29080, loss = 0.98 (929.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:15:14.147332: step 29090, loss = 0.92 (921.6 examples/sec; 0.139 sec/batch)
2017-05-07 21:15:15.587043: step 29100, loss = 0.88 (889.1 examples/sec; 0.144 sec/batch)
2017-05-07 21:15:16.849958: step 29110, loss = 0.96 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:15:18.210561: step 29120, loss = 0.78 (940.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:15:19.572096: step 29130, loss = 0.82 (940.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:15:20.979714: step 29140, loss = 0.80 (909.3 examples/sec; 0.141 sec/batch)
2017-05-07 21:15:22.354341: step 29150, loss = 0.92 (931.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:15:23.706080: step 29160, loss = 1.05 (946.9 examples/sec; 0.135 sec/batch)
2017-05-07 21:15:25.051323: step 29170, loss = 0.71 (951.5 examples/sec; 0.135 sec/batch)
2017-05-07 21:15:26.418678: step 29180, loss = 0.78 (936.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:15:27.829939: step 29190, loss = 1.14 (907.0 examples/sec; 0.141 sec/batch)
2017-05-07 21:15:29.322182: step 29200, loss = 0.82 (857.8 examples/sec; 0.149 sec/batch)
2017-05-07 21:15:30.585503: step 29210, loss = 0.86 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:15:31.966680: step 29220, loss = 0.77 (926.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:15:33.329461: step 29230, loss = 0.79 (939.3 examples/sec; 0.136 sec/batch)
2017-05-07 21:15:34.688843: step 29240, loss = 0.77 (941.6 examples/sec; 0.136 sec/batch)
2017-05-07 21:15:36.036168: step 29250, loss = 0.81 (950.0 examples/sec; 0.135 sec/batch)
2017-05-07 21:15:37.412881: step 29260, loss = 0.76 (929.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:15:38.781118: step 29270, loss = 0.85 (935.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:15:40.152432: step 29280, loss = 0.88 (933.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:15:41.501164: step 29290, loss = 0.76 (949.0 examples/sec; 0.135 sec/batch)
2017-05-07 21:15:42.963866: step 29300, loss = 0.87 (875.1 examples/sec; 0.146 sec/batch)
2017-05-07 21:15:44.242705: step 29310, loss = 0.68 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:15:45.605636: step 29320, loss = 0.87 (939.2 examples/sec; 0.136 sec/batch)
2017-05-07 21:15:46.970913: step 29330, loss = 0.89 (937.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:15:48.325117: step 29340, loss = 0.69 (945.2 examples/sec; 0.135 sec/batch)
2017-05-07 21:15:49.674937: step 29350, loss = 0.92 (948.3 examples/sec; 0.135 sec/batch)
2017-05-07 21:15:51.060307: step 29360, loss = 0.86 (923.9 examples/sec; 0.139 sec/batch)
2017-05-07 21:15:52.410718: step 29370, loss = 1.02 (947.9 examples/sec; 0.135 sec/batch)
2017-05-07 21:15:53.775154: step 29380, loss = 0.92 (938.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:15:55.133645: step 29390, loss = 0.97 (942.2 examples/sec; 0.136 sec/batch)
2017-05-07 21:15:56.597856: step 29400, loss = 0.82 (874.2 examples/sec; 0.146 sec/batch)
2017-05-07 21:15:57.901407: step 29410, loss = 0.76 (981.9 examples/sec; 0.130 sec/batch)
2017-05-07 21:15:59.255726: step 29420, loss = 0.81 (945.1 examples/sec; 0.135 sec/batch)
2017-05-07 21:16:00.635766: step 29430, loss = 0.84 (927.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:16:02.036208: step 29440, loss = 0.78 (914.0 examples/sec; 0.140 sec/batch)
2017-05-07 21:16:03.436913: step 29450, loss = 0.76 (913.8 examples/sec; 0.140 sec/batch)
2017-05-07 21:16:04.830490: step 29460, loss = 0.91 (918.5 examples/sec; 0.139 sec/batch)
2017-05-07 21:16:06.196534: step 29470, loss = 0.75 (937.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:16:07.591197: step 29480, loss = 0.94 (917.8 examples/sec; 0.139 sec/batch)
2017-05-07 21:16:08.989315: step 29490, loss = 0.90 (915.5 examples/sec; 0.140 sec/batch)
2017-05-07 21:16:10.449509: step 29500, loss = 0.72 (876.6 examples/sec; 0.146 sec/batch)
2017-05-07 21:16:11.755562: step 29510, loss = 0.71 (980.0 examples/sec; 0.131 sec/batch)
2017-05-07 21:16:13.128438: step 29520, loss = 0.74 (932.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:16:14.465290: step 29530, loss = 0.70 (957.5 examples/sec; 0.134 sec/batch)
2017-05-07 21:16:15.843415: step 29540, loss = 1.01 (928.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:16:17.204582: step 29550, loss = 0.90 (940.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:16:18.614846: step 29560, loss = 0.86 (907.6 examples/sec; 0.141 sec/batch)
2017-05-07 21:16:19.993780: step 29570, loss = 0.94 (928.3 examples/sec; 0.138 sec/batch)
2017-05-07 21:16:21.360705: step 29580, loss = 0.73 (936.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:16:22.738108: step 29590, loss = 0.78 (929.3 examples/sec; 0.138 sec/batch)
2017-05-07 21:16:24.184872: step 29600, loss = 1.01 (884.7 examples/sec; 0.145 sec/batch)
2017-05-07 21:16:25.462769: step 29610, loss = 0.80 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:16:26.819108: step 29620, loss = 0.85 (943.7 examples/sec; 0.136 sec/batch)
2017-05-07 21:16:28.195159: step 29630, loss = 0.83 (930.2 examples/sec; 0.138 sec/batch)
2017-05-07 21:16:29.592716: step 29640, loss = 0.98 (915.9 examples/sec; 0.140 sec/batch)
2017-05-07 21:16:30.943866: step 29650, loss = 0.83 (947.3 examples/sec; 0.135 sec/batch)
2017-05-07 21:16:32.310423: step 29660, loss = 0.84 (936.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:16:33.679667: step 29670, loss = 0.74 (934.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:16:35.063496: step 29680, loss = 0.75 (925.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:16:36.470811: step 29690, loss = 0.99 (909.5 examples/sec; 0.141 sec/batch)
2017-05-07 21:16:37.956651: step 29700, loss = 0.92 (861.5 examples/sec; 0.149 sec/batch)
2017-05-07 21:16:39.222570: step 29710, loss = 0.88 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:16:40.557664: step 29720, loss = 0.71 (958.7 examples/sec; 0.134 sec/batch)
2017-05-07 21:16:41.913016: step 29730, loss = 0.83 (944.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:16:43.299377: step 29740, loss = 1.01 (923.3 examples/sec; 0.139 sec/batch)
2017-05-07 21:16:44.660400: step 29750, loss = 0.80 (940.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:16:46.021480: step 29760, loss = 0.70 (940.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:16:47.445627: step 29770, loss = 1.05 (898.8 examples/sec; 0.142 sec/batch)
2017-05-07 21:16:48.808085: step 29780, loss = 0.87 (939.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:16:50.170610: step 29790, loss = 0.82 (939.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:16:51.610420: step 29800, loss = 0.59 (889.0 examples/sec; 0.144 sec/batch)
2017-05-07 21:16:52.925410: step 29810, loss = 0.78 (973.4 examples/sec; 0.131 sec/batch)
2017-05-07 21:16:54.285932: step 29820, loss = 1.20 (940.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:16:55.671309: step 29830, loss = 0.91 (923.9 examples/sec; 0.139 sec/batch)
2017-05-07 21:16:57.047007: step 29840, loss = 0.65 (930.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:16:58.427009: step 29850, loss = 0.80 (927.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:16:59.783934: step 29860, loss = 0.93 (943.3 examples/sec; 0.136 sec/batch)
2017-05-07 21:17:01.168426: step 29870, loss = 1.02 (924.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:17:02.555920: step 29880, loss = 0.91 (922.5 examples/sec; 0.139 sec/batch)
2017-05-07 21:17:03.930657: step 29890, loss = 0.94 (931.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:17:05.393325: step 29900, loss = 1.04 (875.1 examples/sec; 0.146 sec/batch)
2017-05-07 21:17:06.681585: step 29910, loss = 0.87 (993.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:17:08.111827: step 29920, loss = 0.73 (895.0 examples/sec; 0.143 sec/batch)
2017-05-07 21:17:09.460754: step 29930, loss = 0.85 (948.9 examples/sec; 0.135 sec/batch)
2017-05-07 21:17:10.845661: step 29940, loss = 0.88 (924.2 examples/sec; 0.138 sec/batch)
2017-05-07 21:17:12.240644: step 29950, loss = 0.85 (917.6 examples/sec; 0.139 sec/batch)
2017-05-07 21:17:13.614837: step 29960, loss = 0.86 (931.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:17:14.997002: step 29970, loss = 1.02 (926.1 examples/sec; 0.138 sec/batch)
2017-05-07 21:17:16.380614: step 29980, loss = 0.83 (925.1 examples/sec; 0.138 sec/batch)
2017-05-07 21:17:17.754491: step 29990, loss = 0.82 (931.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:17:19.217223: step 30000, loss = 0.76 (875.1 examples/sec; 0.146 sec/batch)
2017-05-07 21:17:20.481703: step 30010, loss = 0.73 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:17:21.860022: step 30020, loss = 0.87 (928.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:17:23.231069: step 30030, loss = 0.83 (933.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:17:24.617886: step 30040, loss = 0.69 (923.0 examples/sec; 0.139 sec/batch)
2017-05-07 21:17:25.982851: step 30050, loss = 0.88 (937.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:17:27.341688: step 30060, loss = 0.83 (942.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:17:28.721176: step 30070, loss = 0.99 (927.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:17:30.077928: step 30080, loss = 0.74 (943.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:17:31.467078: step 30090, loss = 0.87 (921.4 examples/sec; 0.139 sec/batch)
2017-05-07 21:17:32.930931: step 30100, loss = 0.84 (874.4 examples/sec; 0.146 sec/batch)
2017-05-07 21:17:34.189398: step 30110, loss = 0.91 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:17:35.554770: step 30120, loss = 0.86 (937.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:17:36.929769: step 30130, loss = 0.89 (930.9 examples/sec; 0.137 sec/batch)
2017-05-07 21:17:38.302326: step 30140, loss = 0.80 (932.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:17:39.666358: step 30150, loss = 0.94 (938.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:17:41.003502: step 30160, loss = 0.82 (957.3 examples/sec; 0.134 sec/batch)
2017-05-07 21:17:42.384804: step 30170, loss = 0.92 (926.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:17:43.767470: step 30180, loss = 0.94 (925.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:17:45.151910: step 30190, loss = 0.82 (924.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:17:46.629795: step 30200, loss = 0.91 (866.1 examples/sec; 0.148 sec/batch)
2017-05-07 21:17:47.939027: step 30210, loss = 1.01 (977.7 examples/sec; 0.131 sec/batch)
2017-05-07 21:17:49.321408: step 30220, loss = 0.91 (925.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:17:50.721495: step 30230, loss = 0.83 (914.2 examples/sec; 0.140 sec/batch)
2017-05-07 21:17:52.107699: step 30240, loss = 0.87 (923.4 examples/sec; 0.139 sec/batch)
2017-05-07 21:17:53.440840: step 30250, loss = 0.95 (960.1 examples/sec; 0.133 sec/batch)
2017-05-07 21:17:54.802695: step 30260, loss = 0.94 (939.9 examples/sec; 0.136 sec/batch)
2017-05-07 21:17:56.166203: step 30270, loss = 0.78 (938.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:17:57.543403: step 30280, loss = 0.69 (929.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:17:58.944679: step 30290, loss = 0.91 (913.4 examples/sec; 0.140 sec/batch)
2017-05-07 21:18:00.415365: step 30300, loss = 0.68 (870.3 examples/sec; 0.147 sec/batch)
2017-05-07 21:18:01.701925: step 30310, loss = 0.88 (994.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:18:03.040774: step 30320, loss = 0.99 (956.0 examples/sec; 0.134 sec/batch)
2017-05-07 21:18:04.421316: step 30330, loss = 0.82 (927.2 examples/sec; 0.138 sec/batch)
2017-05-07 21:18:05.794876: step 30340, loss = 0.89 (931.9 examples/sec; 0.137 sec/batch)
2017-05-07 21:18:07.168261: step 30350, loss = 0.71 (932.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:18:08.556202: step 30360, loss = 0.68 (922.2 examples/sec; 0.139 sec/batch)
2017-05-07 21:18:09.936394: step 30370, loss = 0.85 (927.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:18:11.301399: step 30380, loss = 0.88 (937.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:18:12.697042: step 30390, loss = 0.93 (917.1 examples/sec; 0.140 sec/batch)
2017-05-07 21:18:14.150608: step 30400, loss = 0.76 (880.6 examples/sec; 0.145 sec/batch)
2017-05-07 21:18:15.444738: step 30410, loss = 0.89 (989.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:18:16.808338: step 30420, loss = 0.76 (938.7 examples/sec; 0.136 sec/batch)
2017-05-07 21:18:18.169339: step 30430, loss = 0.93 (940.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:18:19.565805: step 30440, loss = 0.79 (916.6 examples/sec; 0.140 sec/batch)
2017-05-07 21:18:20.938715: step 30450, loss = 0.87 (932.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:18:22.289117: step 30460, loss = 0.80 (947.9 examples/sec; 0.135 sec/batch)
2017-05-07 21:18:23.664574: step 30470, loss = 0.73 (930.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:18:25.025825: step 30480, loss = 0.84 (940.3 examples/sec; 0.136 sec/batch)
2017-05-07 21:18:26.378942: step 30490, loss = 0.79 (946.0 examples/sec; 0.135 sec/batch)
2017-05-07 21:18:27.851040: step 30500, loss = 0.69 (869.5 examples/sec; 0.147 sec/batch)
2017-05-07 21:18:29.141752: step 30510, loss = 0.82 (991.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:18:30.532695: step 30520, loss = 0.84 (920.2 examples/sec; 0.139 sec/batch)
2017-05-07 21:18:31.880436: step 30530, loss = 0.97 (949.7 examples/sec; 0.135 sec/batch)
2017-05-07 21:18:33.227568: step 30540, loss = 0.83 (950.2 examples/sec; 0.135 sec/batch)
2017-05-07 21:18:34.591529: step 30550, loss = 0.84 (938.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:18:35.944073: step 30560, loss = 0.77 (946.4 examples/sec; 0.135 sec/batch)
2017-05-07 21:18:37.324772: step 30570, loss = 0.92 (927.1 examples/sec; 0.138 sec/batch)
2017-05-07 21:18:38.722449: step 30580, loss = 0.85 (915.8 examples/sec; 0.140 sec/batch)
2017-05-07 21:18:40.106549: step 30590, loss = 0.74 (924.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:18:41.586334: step 30600, loss = 0.89 (865.0 examples/sec; 0.148 sec/batch)
2017-05-07 21:18:42.844288: step 30610, loss = 0.80 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:18:44.226876: step 30620, loss = 0.98 (925.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:18:45.633500: step 30630, loss = 0.80 (910.0 examples/sec; 0.141 sec/batch)
2017-05-07 21:18:47.022421: step 30640, loss = 0.71 (921.6 examples/sec; 0.139 sec/batch)
2017-05-07 21:18:48.391016: step 30650, loss = 0.94 (935.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:18:49.757572: step 30660, loss = 0.80 (936.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:18:51.112352: step 30670, loss = 0.72 (944.8 examples/sec; 0.135 sec/batch)
2017-05-07 21:18:52.494647: step 30680, loss = 0.79 (926.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:18:53.857508: step 30690, loss = 0.74 (939.2 examples/sec; 0.136 sec/batch)
2017-05-07 21:18:55.322993: step 30700, loss = 0.82 (873.5 examples/sec; 0.147 sec/batch)
2017-05-07 21:18:56.615722: step 30710, loss = 0.79 (990.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:18:57.981524: step 30720, loss = 0.77 (937.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:18:59.348392: step 30730, loss = 0.65 (936.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:19:00.721803: step 30740, loss = 0.83 (932.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:19:02.077111: step 30750, loss = 0.81 (944.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:19:03.466388: step 30760, loss = 0.98 (921.3 examples/sec; 0.139 sec/batch)
2017-05-07 21:19:04.837554: step 30770, loss = 0.81 (933.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:19:06.194998: step 30780, loss = 0.83 (942.9 examples/sec; 0.136 sec/batch)
2017-05-07 21:19:07.560227: step 30790, loss = 0.79 (937.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:19:09.042599: step 30800, loss = 0.80 (863.5 examples/sec; 0.148 sec/batch)
2017-05-07 21:19:10.301224: step 30810, loss = 0.97 (1017.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:19:11.655473: step 30820, loss = 0.82 (945.2 examples/sec; 0.135 sec/batch)
2017-05-07 21:19:12.990631: step 30830, loss = 0.71 (958.7 examples/sec; 0.134 sec/batch)
2017-05-07 21:19:14.373255: step 30840, loss = 0.94 (925.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:19:15.766893: step 30850, loss = 0.73 (918.5 examples/sec; 0.139 sec/batch)
2017-05-07 21:19:17.117375: step 30860, loss = 0.68 (947.8 examples/sec; 0.135 sec/batch)
2017-05-07 21:19:18.484779: step 30870, loss = 0.81 (936.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:19:19.833465: step 30880, loss = 0.98 (949.1 examples/sec; 0.135 sec/batch)
2017-05-07 21:19:21.211210: step 30890, loss = 0.85 (929.1 examples/sec; 0.138 sec/batch)
2017-05-07 21:19:22.643907: step 30900, loss = 0.64 (893.4 examples/sec; 0.143 sec/batch)
2017-05-07 21:19:23.891736: step 30910, loss = 0.95 (1025.8 examples/sec; 0.125 sec/batch)
2017-05-07 21:19:25.270751: step 30920, loss = 0.93 (928.2 examples/sec; 0.138 sec/batch)
2017-05-07 21:19:26.624282: step 30930, loss = 0.62 (945.7 examples/sec; 0.135 sec/batch)
2017-05-07 21:19:28.014320: step 30940, loss = 0.72 (920.8 examples/sec; 0.139 sec/batch)
2017-05-07 21:19:29.381329: step 30950, loss = 1.14 (936.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:19:30.740761: step 30960, loss = 0.75 (941.6 examples/sec; 0.136 sec/batch)
2017-05-07 21:19:32.122037: step 30970, loss = 0.63 (926.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:19:33.491787: step 30980, loss = 0.90 (934.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:19:34.874365: step 30990, loss = 0.87 (925.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:19:36.329102: step 31000, loss = 1.01 (879.9 examples/sec; 0.145 sec/batch)
2017-05-07 21:19:37.611077: step 31010, loss = 0.78 (998.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:19:39.003188: step 31020, loss = 0.79 (919.5 examples/sec; 0.139 sec/batch)
2017-05-07 21:19:40.418674: step 31030, loss = 0.86 (904.3 examples/sec; 0.142 sec/batch)
2017-05-07 21:19:41.785918: step 31040, loss = 0.83 (936.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:19:43.161362: step 31050, loss = 0.96 (930.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:19:44.542262: step 31060, loss = 0.95 (926.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:19:45.893659: step 31070, loss = 0.86 (947.2 examples/sec; 0.135 sec/batch)
2017-05-07 21:19:47.262424: step 31080, loss = 0.85 (935.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:19:48.627899: step 31090, loss = 0.80 (937.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:19:50.092483: step 31100, loss = 0.71 (874.0 examples/sec; 0.146 sec/batch)
2017-05-07 21:19:51.363745: step 31110, loss = 0.84 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:19:52.712153: step 31120, loss = 0.68 (949.3 examples/sec; 0.135 sec/batch)
2017-05-07 21:19:54.110660: step 31130, loss = 1.02 (915.3 examples/sec; 0.140 sec/batch)
2017-05-07 21:19:55.487190: step 31140, loss = 0.97 (929.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:19:56.847094: step 31150, loss = 0.75 (941.2 examples/sec; 0.136 sec/batch)
2017-05-07 21:19:58.197523: step 31160, loss = 0.77 (947.8 examples/sec; 0.135 sec/batch)
2017-05-07 21:19:59.564388: step 31170, loss = 0.87 (936.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:20:00.942362: step 31180, loss = 0.77 (928.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:20:02.343155: step 31190, loss = 0.86 (913.8 examples/sec; 0.140 sec/batch)
2017-05-07 21:20:03.823940: step 31200, loss = 0.92 (864.4 examples/sec; 0.148 sec/batch)
2017-05-07 21:20:05.134265: step 31210, loss = 0.72 (976.8 examples/sec; 0.131 sec/batch)
2017-05-07 21:20:06.492586: step 31220, loss = 0.89 (942.3 examples/sec; 0.136 sec/batch)
2017-05-07 21:20:07.855368: step 31230, loss = 0.79 (939.3 examples/sec; 0.136 sec/batch)
2017-05-07 21:20:09.269247: step 31240, loss = 0.89 (905.3 examples/sec; 0.141 sec/batch)
2017-05-07 21:20:10.659741: step 31250, loss = 0.83 (920.5 examples/sec; 0.139 sec/batch)
2017-05-07 21:20:12.022043: step 31260, loss = 0.88 (939.6 examples/sec; 0.136 sec/batch)
2017-05-07 21:20:13.380420: step 31270, loss = 0.80 (942.3 examples/sec; 0.136 sec/batch)
2017-05-07 21:20:14.749994: step 31280, loss = 0.82 (934.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:20:16.121493: step 31290, loss = 0.85 (933.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:20:17.565940: step 31300, loss = 0.80 (886.2 examples/sec; 0.144 sec/batch)
2017-05-07 21:20:18.825785: step 31310, loss = 0.83 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:20:20.237115: step 31320, loss = 0.75 (907.0 examples/sec; 0.141 sec/batch)
2017-05-07 21:20:21.608676: step 31330, loss = 0.83 (933.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:20:23.026654: step 31340, loss = 0.82 (902.7 examples/sec; 0.142 sec/batch)
2017-05-07 21:20:24.404065: step 31350, loss = 0.79 (929.3 examples/sec; 0.138 sec/batch)
2017-05-07 21:20:25.779138: step 31360, loss = 0.93 (930.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:20:27.121174: step 31370, loss = 0.74 (953.8 examples/sec; 0.134 sec/batch)
2017-05-07 21:20:28.490673: step 31380, loss = 0.89 (934.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:20:29.870482: step 31390, loss = 0.96 (927.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:20:31.361135: step 31400, loss = 0.79 (858.7 examples/sec; 0.149 sec/batch)
2017-05-07 21:20:32.681924: step 31410, loss = 0.79 (969.1 examples/sec; 0.132 sec/batch)
2017-05-07 21:20:34.068032: step 31420, loss = 0.74 (923.5 examples/sec; 0.139 sec/batch)
2017-05-07 21:20:35.421883: step 31430, loss = 1.21 (945.5 examples/sec; 0.135 sec/batch)
2017-05-07 21:20:36.835686: step 31440, loss = 0.69 (905.4 examples/sec; 0.141 sec/batch)
2017-05-07 21:20:38.196744: step 31450, loss = 0.84 (940.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:20:39.591372: step 31460, loss = 0.79 (917.8 examples/sec; 0.139 sec/batch)
2017-05-07 21:20:40.955642: step 31470, loss = 0.75 (938.2 examples/sec; 0.136 sec/batch)
2017-05-07 21:20:42.340235: step 31480, loss = 0.91 (924.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:20:43.756185: step 31490, loss = 0.77 (904.0 examples/sec; 0.142 sec/batch)
2017-05-07 21:20:45.212662: step 31500, loss = 0.73 (878.8 examples/sec; 0.146 sec/batch)
2017-05-07 21:20:46.502033: step 31510, loss = 0.87 (992.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:20:47.868431: step 31520, loss = 0.71 (936.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:20:49.277457: step 31530, loss = 0.82 (908.4 examples/sec; 0.141 sec/batch)
2017-05-07 21:20:50.639304: step 31540, loss = 0.75 (939.9 examples/sec; 0.136 sec/batch)
2017-05-07 21:20:52.025426: step 31550, loss = 1.09 (923.4 examples/sec; 0.139 sec/batch)
2017-05-07 21:20:53.393019: step 31560, loss = 0.99 (936.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:20:54.787580: step 31570, loss = 0.64 (917.8 examples/sec; 0.139 sec/batch)
2017-05-07 21:20:56.186866: step 31580, loss = 0.97 (914.8 examples/sec; 0.140 sec/batch)
2017-05-07 21:20:57.558376: step 31590, loss = 0.86 (933.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:20:59.040725: step 31600, loss = 0.91 (863.5 examples/sec; 0.148 sec/batch)
2017-05-07 21:21:00.325643: step 31610, loss = 0.86 (996.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:21:01.684395: step 31620, loss = 0.92 (942.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:21:03.091597: step 31630, loss = 0.97 (909.6 examples/sec; 0.141 sec/batch)
2017-05-07 21:21:04.471782: step 31640, loss = 0.80 (927.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:21:05.876768: step 31650, loss = 0.91 (911.0 examples/sec; 0.140 sec/batch)
2017-05-07 21:21:07.243703: step 31660, loss = 0.78 (936.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:21:08.619515: step 31670, loss = 0.78 (930.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:21:09.993504: step 31680, loss = 0.79 (931.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:21:11.403563: step 31690, loss = 0.82 (907.8 examples/sec; 0.141 sec/batch)
2017-05-07 21:21:12.862430: step 31700, loss = 0.95 (877.4 examples/sec; 0.146 sec/batch)
2017-05-07 21:21:14.139111: step 31710, loss = 0.70 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:21:15.491275: step 31720, loss = 0.75 (946.6 examples/sec; 0.135 sec/batch)
2017-05-07 21:21:16.888601: step 31730, loss = 0.76 (916.0 examples/sec; 0.140 sec/batch)
2017-05-07 21:21:18.249499: step 31740, loss = 0.79 (940.6 examples/sec; 0.136 sec/batch)
2017-05-07 21:21:19.610612: step 31750, loss = 0.87 (940.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:21:20.958859: step 31760, loss = 1.09 (949.4 examples/sec; 0.135 sec/batch)
2017-05-07 21:21:22.324441: step 31770, loss = 0.90 (937.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:21:23.687806: step 31780, loss = 0.81 (938.9 examples/sec; 0.136 sec/batch)
2017-05-07 21:21:25.063814: step 31790, loss = 0.84 (930.2 examples/sec; 0.138 sec/batch)
2017-05-07 21:21:26.541131: step 31800, loss = 0.80 (866.4 examples/sec; 0.148 sec/batch)
2017-05-07 21:21:27.817420: step 31810, loss = 0.94 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:21:29.188997: step 31820, loss = 0.76 (933.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:21:30.529300: step 31830, loss = 0.95 (955.0 examples/sec; 0.134 sec/batch)
2017-05-07 21:21:31.907276: step 31840, loss = 0.91 (928.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:21:33.290773: step 31850, loss = 0.86 (925.2 examples/sec; 0.138 sec/batch)
2017-05-07 21:21:34.635611: step 31860, loss = 0.87 (951.8 examples/sec; 0.134 sec/batch)
2017-05-07 21:21:35.970695: step 31870, loss = 0.89 (958.7 examples/sec; 0.134 sec/batch)
2017-05-07 21:21:37.312901: step 31880, loss = 0.82 (953.7 examples/sec; 0.134 sec/batch)
2017-05-07 21:21:38.669079: step 31890, loss = 0.66 (943.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:21:40.118139: step 31900, loss = 0.74 (883.3 examples/sec; 0.145 sec/batch)
2017-05-07 21:21:41.402262: step 31910, loss = 0.90 (996.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:21:42.788669: step 31920, loss = 0.89 (923.3 examples/sec; 0.139 sec/batch)
2017-05-07 21:21:44.166128: step 31930, loss = 0.74 (929.2 examples/sec; 0.138 sec/batch)
2017-05-07 21:21:45.539274: step 31940, loss = 0.73 (932.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:21:46.915777: step 31950, loss = 0.80 (929.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:21:48.303388: step 31960, loss = 0.92 (922.4 examples/sec; 0.139 sec/batch)
2017-05-07 21:21:49.691319: step 31970, loss = 0.62 (922.2 examples/sec; 0.139 sec/batch)
2017-05-07 21:21:51.102501: step 31980, loss = 0.95 (907.0 examples/sec; 0.141 sec/batch)
2017-05-07 21:21:52.456134: step 31990, loss = 1.06 (945.6 examples/sec; 0.135 sec/batch)
2017-05-07 21:21:53.921563: step 32000, loss = 0.76 (873.5 examples/sec; 0.147 sec/batch)
2017-05-07 21:21:55.215813: step 32010, loss = 1.03 (989.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:21:56.578018: step 32020, loss = 0.83 (939.7 examples/sec; 0.136 sec/batch)
2017-05-07 21:21:57.946159: step 32030, loss = 0.74 (935.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:21:59.326526: step 32040, loss = 0.91 (927.3 examples/sec; 0.138 sec/batch)
2017-05-07 21:22:00.673894: step 32050, loss = 0.93 (950.0 examples/sec; 0.135 sec/batch)
2017-05-07 21:22:02.030277: step 32060, loss = 0.75 (943.7 examples/sec; 0.136 sec/batch)
2017-05-07 21:22:03.386828: step 32070, loss = 0.87 (943.6 examples/sec; 0.136 sec/batch)
2017-05-07 21:22:04.789347: step 32080, loss = 1.17 (912.6 examples/sec; 0.140 sec/batch)
2017-05-07 21:22:06.194661: step 32090, loss = 0.83 (910.8 examples/sec; 0.141 sec/batch)
2017-05-07 21:22:07.673091: step 32100, loss = 0.65 (865.8 examples/sec; 0.148 sec/batch)
2017-05-07 21:22:08.968435: step 32110, loss = 0.91 (988.2 examples/sec; 0.130 sec/batch)
2017-05-07 21:22:10.347284: step 32120, loss = 0.87 (928.3 examples/sec; 0.138 sec/batch)
2017-05-07 21:22:11.700182: step 32130, loss = 0.83 (946.1 examples/sec; 0.135 sec/batch)
2017-05-07 21:22:13.056829: step 32140, loss = 0.90 (943.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:22:14.431265: step 32150, loss = 0.87 (931.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:22:15.781172: step 32160, loss = 0.83 (948.2 examples/sec; 0.135 sec/batch)
2017-05-07 21:22:17.135004: step 32170, loss = 0.78 (945.5 examples/sec; 0.135 sec/batch)
2017-05-07 21:22:18.488136: step 32180, loss = 0.79 (946.0 examples/sec; 0.135 sec/batch)
2017-05-07 21:22:19.875147: step 32190, loss = 0.98 (922.8 examples/sec; 0.139 sec/batch)
2017-05-07 21:22:21.329379: step 32200, loss = 0.91 (880.2 examples/sec; 0.145 sec/batch)
2017-05-07 21:22:22.596823: step 32210, loss = 0.75 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:22:23.968396: step 32220, loss = 0.89 (933.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:22:25.365873: step 32230, loss = 0.75 (915.9 examples/sec; 0.140 sec/batch)
2017-05-07 21:22:26.722346: step 32240, loss = 0.91 (943.6 examples/sec; 0.136 sec/batch)
2017-05-07 21:22:28.100367: step 32250, loss = 0.74 (928.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:22:29.456454: step 32260, loss = 0.78 (943.9 examples/sec; 0.136 sec/batch)
2017-05-07 21:22:30.818692: step 32270, loss = 0.81 (939.6 examples/sec; 0.136 sec/batch)
2017-05-07 21:22:32.203567: step 32280, loss = 0.93 (924.3 examples/sec; 0.138 sec/batch)
2017-05-07 21:22:33.598269: step 32290, loss = 0.93 (917.8 examples/sec; 0.139 sec/batch)
2017-05-07 21:22:35.045352: step 32300, loss = 0.87 (884.5 examples/sec; 0.145 sec/batch)
2017-05-07 21:22:36.363835: step 32310, loss = 0.82 (970.8 examples/sec; 0.132 sec/batch)
2017-05-07 21:22:37.734503: step 32320, loss = 0.99 (933.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:22:39.050814: step 32330, loss = 0.74 (972.4 examples/sec; 0.132 sec/batch)
2017-05-07 21:22:40.415004: step 32340, loss = 0.99 (938.3 examples/sec; 0.136 sec/batch)
2017-05-07 21:22:41.800821: step 32350, loss = 0.99 (923.6 examples/sec; 0.139 sec/batch)
2017-05-07 21:22:43.183647: step 32360, loss = 0.75 (925.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:22:44.581492: step 32370, loss = 0.89 (915.7 examples/sec; 0.140 sec/batch)
2017-05-07 21:22:45.939572: step 32380, loss = 0.89 (942.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:22:47.329635: step 32390, loss = 0.83 (920.8 examples/sec; 0.139 sec/batch)
2017-05-07 21:22:48.772546: step 32400, loss = 0.71 (887.1 examples/sec; 0.144 sec/batch)
2017-05-07 21:22:50.025005: step 32410, loss = 0.70 (1022.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:22:51.418444: step 32420, loss = 0.83 (918.6 examples/sec; 0.139 sec/batch)
2017-05-07 21:22:52.788210: step 32430, loss = 0.79 (934.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:22:54.136074: step 32440, loss = 0.93 (949.7 examples/sec; 0.135 sec/batch)
2017-05-07 21:22:55.516230: step 32450, loss = 0.89 (927.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:22:56.902011: step 32460, loss = 0.73 (923.6 examples/sec; 0.139 sec/batch)
2017-05-07 21:22:58.246726: step 32470, loss = 0.72 (951.9 examples/sec; 0.134 sec/batch)
2017-05-07 21:22:59.606770: step 32480, loss = 0.88 (941.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:23:00.960709: step 32490, loss = 0.97 (945.4 examples/sec; 0.135 sec/batch)
2017-05-07 21:23:02.402077: step 32500, loss = 0.91 (888.0 examples/sec; 0.144 sec/batch)
2017-05-07 21:23:03.705188: step 32510, loss = 0.76 (982.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:23:05.070565: step 32520, loss = 0.89 (937.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:23:06.460332: step 32530, loss = 0.83 (921.0 examples/sec; 0.139 sec/batch)
2017-05-07 21:23:07.815489: step 32540, loss = 0.88 (944.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:23:09.186716: step 32550, loss = 0.77 (933.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:23:10.567614: step 32560, loss = 0.88 (926.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:23:11.946451: step 32570, loss = 0.91 (928.3 examples/sec; 0.138 sec/batch)
2017-05-07 21:23:13.327373: step 32580, loss = 0.70 (926.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:23:14.728208: step 32590, loss = 0.74 (913.7 examples/sec; 0.140 sec/batch)
2017-05-07 21:23:16.192600: step 32600, loss = 0.79 (874.1 examples/sec; 0.146 sec/batch)
2017-05-07 21:23:17.475579: step 32610, loss = 0.91 (997.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:23:18.894494: step 32620, loss = 0.87 (902.1 examples/sec; 0.142 sec/batch)
2017-05-07 21:23:20.270164: step 32630, loss = 0.86 (930.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:23:21.633519: step 32640, loss = 0.91 (938.9 examples/sec; 0.136 sec/batch)
2017-05-07 21:23:23.020515: step 32650, loss = 0.75 (922.9 examples/sec; 0.139 sec/batch)
2017-05-07 21:23:24.401390: step 32660, loss = 1.01 (926.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:23:25.795918: step 32670, loss = 0.90 (917.9 examples/sec; 0.139 sec/batch)
2017-05-07 21:23:27.170527: step 32680, loss = 0.77 (931.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:23:28.541544: step 32690, loss = 0.93 (933.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:23:30.009252: step 32700, loss = 0.86 (872.1 examples/sec; 0.147 sec/batch)
2017-05-07 21:23:31.264424: step 32710, loss = 0.71 (1019.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:23:32.662749: step 32720, loss = 0.75 (915.4 examples/sec; 0.140 sec/batch)
2017-05-07 21:23:34.010155: step 32730, loss = 0.84 (950.0 examples/sec; 0.135 sec/batch)
2017-05-07 21:23:35.371619: step 32740, loss = 0.92 (940.2 examples/sec; 0.136 sec/batch)
2017-05-07 21:23:36.739755: step 32750, loss = 0.80 (935.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:23:38.108043: step 32760, loss = 0.88 (935.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:23:39.473213: step 32770, loss = 0.79 (937.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:23:40.808665: step 32780, loss = 0.72 (958.5 examples/sec; 0.134 sec/batch)
2017-05-07 21:23:42.160142: step 32790, loss = 1.03 (947.1 examples/sec; 0.135 sec/batch)
2017-05-07 21:23:43.633754: step 32800, loss = 0.78 (868.6 examples/sec; 0.147 sec/batch)
2017-05-07 21:23:44.932845: step 32810, loss = 0.78 (985.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:23:46.285280: step 32820, loss = 0.90 (946.4 examples/sec; 0.135 sec/batch)
2017-05-07 21:23:47.655098: step 32830, loss = 0.91 (934.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:23:49.035311: step 32840, loss = 0.87 (927.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:23:50.441523: step 32850, loss = 0.84 (910.3 examples/sec; 0.141 sec/batch)
2017-05-07 21:23:51.796247: step 32860, loss = 0.92 (944.8 examples/sec; 0.135 sec/batch)
2017-05-07 21:23:53.165702: step 32870, loss = 0.89 (934.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:23:54.525913: step 32880, loss = 0.87 (941.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:23:55.927729: step 32890, loss = 0.81 (913.1 examples/sec; 0.140 sec/batch)
2017-05-07 21:23:57.384253: step 32900, loss = 0.71 (878.8 examples/sec; 0.146 sec/batch)
2017-05-07 21:23:58.644715: step 32910, loss = 0.89 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:23:59.987481: step 32920, loss = 0.88 (953.3 examples/sec; 0.134 sec/batch)
2017-05-07 21:24:01.352061: step 32930, loss = 0.90 (938.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:24:02.721586: step 32940, loss = 0.79 (934.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:24:04.112598: step 32950, loss = 0.79 (920.2 examples/sec; 0.139 sec/batch)
2017-05-07 21:24:05.500436: step 32960, loss = 0.95 (922.3 examples/sec; 0.139 sec/batch)
2017-05-07 21:24:06.910576: step 32970, loss = 0.80 (907.7 examples/sec; 0.141 sec/batch)
2017-05-07 21:24:08.276730: step 32980, loss = 0.87 (936.9 examples/sec; 0.137 sec/batch)
2017-05-07 21:24:09.676514: step 32990, loss = 0.89 (914.4 examples/sec; 0.140 sec/batch)
2017-05-07 21:24:11.156457: step 33000, loss = 0.71 (864.9 examples/sec; 0.148 sec/batch)
2017-05-07 21:24:12.395825: step 33010, loss = 0.91 (1032.8 examples/sec; 0.124 sec/batch)
2017-05-07 21:24:13.759385: step 33020, loss = 0.75 (938.7 examples/sec; 0.136 sec/batch)
2017-05-07 21:24:15.135097: step 33030, loss = 0.86 (930.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:24:16.522636: step 33040, loss = 0.79 (922.5 examples/sec; 0.139 sec/batch)
2017-05-07 21:24:17.898490: step 33050, loss = 0.88 (930.3 examples/sec; 0.138 sec/batch)
2017-05-07 21:24:19.256295: step 33060, loss = 0.99 (942.7 examples/sec; 0.136 sec/batch)
2017-05-07 21:24:20.612445: step 33070, loss = 0.91 (943.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:24:21.958468: step 33080, loss = 0.80 (950.9 examples/sec; 0.135 sec/batch)
2017-05-07 21:24:23.312516: step 33090, loss = 0.73 (945.3 examples/sec; 0.135 sec/batch)
2017-05-07 21:24:24.768846: step 33100, loss = 0.97 (878.9 examples/sec; 0.146 sec/batch)
2017-05-07 21:24:26.030943: step 33110, loss = 0.85 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:24:27.409073: step 33120, loss = 0.80 (928.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:24:28.815485: step 33130, loss = 0.87 (910.1 examples/sec; 0.141 sec/batch)
2017-05-07 21:24:30.189891: step 33140, loss = 0.74 (931.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:24:31.547222: step 33150, loss = 1.10 (943.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:24:32.919642: step 33160, loss = 0.77 (932.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:24:34.296729: step 33170, loss = 0.67 (929.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:24:35.661796: step 33180, loss = 0.83 (937.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:24:37.028218: step 33190, loss = 0.78 (936.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:24:38.499542: step 33200, loss = 1.01 (870.0 examples/sec; 0.147 sec/batch)
2017-05-07 21:24:39.779831: step 33210, loss = 0.84 (999.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:24:41.172909: step 33220, loss = 0.97 (918.8 examples/sec; 0.139 sec/batch)
2017-05-07 21:24:42.556083: step 33230, loss = 0.91 (925.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:24:43.925072: step 33240, loss = 1.02 (935.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:24:45.284651: step 33250, loss = 0.75 (941.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:24:46.682960: step 33260, loss = 0.79 (915.4 examples/sec; 0.140 sec/batch)
2017-05-07 21:24:48.049062: step 33270, loss = 0.67 (937.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:24:49.421852: step 33280, loss = 0.81 (932.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:24:50.809100: step 33290, loss = 0.89 (922.7 examples/sec; 0.139 sec/batch)
2017-05-07 21:24:52.275381: step 33300, loss = 0.86 (873.0 examples/sec; 0.147 sec/batch)
2017-05-07 21:24:53.569404: step 33310, loss = 0.83 (989.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:24:54.940524: step 33320, loss = 0.97 (933.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:24:56.319167: step 33330, loss = 1.02 (928.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:24:57.657304: step 33340, loss = 0.79 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 21:24:59.029368: step 33350, loss = 0.88 (932.9 examples/sec; 0.137 sec/batch)
2017-05-07 21:25:00.391136: step 33360, loss = 0.81 (940.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:25:01.844701: step 33370, loss = 0.88 (880.6 examples/sec; 0.145 sec/batch)
2017-05-07 21:25:03.179175: step 33380, loss = 0.93 (959.2 examples/sec; 0.133 sec/batch)
2017-05-07 21:25:04.525445: step 33390, loss = 0.81 (950.8 examples/sec; 0.135 sec/batch)
2017-05-07 21:25:05.991193: step 33400, loss = 0.84 (873.3 examples/sec; 0.147 sec/batch)
2017-05-07 21:25:07.292468: step 33410, loss = 0.85 (983.7 examples/sec; 0.130 sec/batch)
2017-05-07 21:25:08.669587: step 33420, loss = 0.72 (929.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:25:10.037076: step 33430, loss = 0.94 (936.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:25:11.423709: step 33440, loss = 0.94 (923.1 examples/sec; 0.139 sec/batch)
2017-05-07 21:25:12.814382: step 33450, loss = 0.95 (920.4 examples/sec; 0.139 sec/batch)
2017-05-07 21:25:14.171332: step 33460, loss = 0.86 (943.3 examples/sec; 0.136 sec/batch)
2017-05-07 21:25:15.560741: step 33470, loss = 0.77 (921.3 examples/sec; 0.139 sec/batch)
2017-05-07 21:25:16.936860: step 33480, loss = 1.01 (930.2 examples/sec; 0.138 sec/batch)
2017-05-07 21:25:18.307362: step 33490, loss = 0.80 (934.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:25:19.777004: step 33500, loss = 0.78 (871.0 examples/sec; 0.147 sec/batch)
2017-05-07 21:25:21.046695: step 33510, loss = 0.85 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:25:22.419603: step 33520, loss = 0.76 (932.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:25:23.786633: step 33530, loss = 0.73 (936.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:25:25.177458: step 33540, loss = 0.87 (920.3 examples/sec; 0.139 sec/batch)
2017-05-07 21:25:26.530157: step 33550, loss = 0.65 (946.2 examples/sec; 0.135 sec/batch)
2017-05-07 21:25:27.884612: step 33560, loss = 0.93 (945.0 examples/sec; 0.135 sec/batch)
2017-05-07 21:25:29.256124: step 33570, loss = 0.67 (933.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:25:30.607826: step 33580, loss = 0.95 (947.0 examples/sec; 0.135 sec/batch)
2017-05-07 21:25:31.986191: step 33590, loss = 0.79 (928.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:25:33.459371: step 33600, loss = 0.77 (868.9 examples/sec; 0.147 sec/batch)
2017-05-07 21:25:34.724085: step 33610, loss = 0.72 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:25:36.075840: step 33620, loss = 0.85 (946.9 examples/sec; 0.135 sec/batch)
2017-05-07 21:25:37.424938: step 33630, loss = 0.74 (948.8 examples/sec; 0.135 sec/batch)
2017-05-07 21:25:38.790086: step 33640, loss = 0.83 (937.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:25:40.135479: step 33650, loss = 0.90 (951.4 examples/sec; 0.135 sec/batch)
2017-05-07 21:25:41.503129: step 33660, loss = 0.97 (935.9 examples/sec; 0.137 sec/batch)
2017-05-07 21:25:42.864682: step 33670, loss = 0.95 (940.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:25:44.230820: step 33680, loss = 0.91 (937.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:25:45.607329: step 33690, loss = 0.93 (929.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:25:47.058836: step 33700, loss = 0.80 (881.8 examples/sec; 0.145 sec/batch)
2017-05-07 21:25:48.338625: step 33710, loss = 0.73 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:25:49.714486: step 33720, loss = 0.79 (930.3 examples/sec; 0.138 sec/batch)
2017-05-07 21:25:51.057241: step 33730, loss = 0.85 (953.3 examples/sec; 0.134 sec/batch)
2017-05-07 21:25:52.438369: step 33740, loss = 0.95 (926.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:25:53.828661: step 33750, loss = 0.77 (920.7 examples/sec; 0.139 sec/batch)
2017-05-07 21:25:55.189890: step 33760, loss = 0.89 (940.3 examples/sec; 0.136 sec/batch)
2017-05-07 21:25:56.606613: step 33770, loss = 0.83 (903.5 examples/sec; 0.142 sec/batch)
2017-05-07 21:25:57.997177: step 33780, loss = 0.91 (920.5 examples/sec; 0.139 sec/batch)
2017-05-07 21:25:59.361589: step 33790, loss = 0.86 (938.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:26:00.829362: step 33800, loss = 0.71 (872.1 examples/sec; 0.147 sec/batch)
2017-05-07 21:26:02.142649: step 33810, loss = 0.97 (974.7 examples/sec; 0.131 sec/batch)
2017-05-07 21:26:03.506699: step 33820, loss = 0.97 (938.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:26:04.876623: step 33830, loss = 0.94 (934.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:26:06.238346: step 33840, loss = 0.87 (940.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:26:07.608094: step 33850, loss = 0.67 (934.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:26:08.968072: step 33860, loss = 0.77 (941.2 examples/sec; 0.136 sec/batch)
2017-05-07 21:26:10.308121: step 33870, loss = 0.80 (955.2 examples/sec; 0.134 sec/batch)
2017-05-07 21:26:11.695151: step 33880, loss = 0.80 (922.8 examples/sec; 0.139 sec/batch)
2017-05-07 21:26:13.067422: step 33890, loss = 0.87 (932.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:26:14.529705: step 33900, loss = 0.56 (875.3 examples/sec; 0.146 sec/batch)
2017-05-07 21:26:15.789540: step 33910, loss = 0.82 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:26:17.152688: step 33920, loss = 0.77 (939.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:26:18.497953: step 33930, loss = 0.75 (951.5 examples/sec; 0.135 sec/batch)
2017-05-07 21:26:19.891012: step 33940, loss = 0.77 (918.8 examples/sec; 0.139 sec/batch)
2017-05-07 21:26:21.239414: step 33950, loss = 0.82 (949.3 examples/sec; 0.135 sec/batch)
2017-05-07 21:26:22.622098: step 33960, loss = 0.66 (925.7 examples/sec; 0.138 sec/batch)
2017-05-07 21:26:23.999738: step 33970, loss = 0.97 (929.1 examples/sec; 0.138 sec/batch)
2017-05-07 21:26:25.384754: step 33980, loss = 0.76 (924.2 examples/sec; 0.139 sec/batch)
2017-05-07 21:26:26.784857: step 33990, loss = 0.85 (914.2 examples/sec; 0.140 sec/batch)
2017-05-07 21:26:28.281389: step 34000, loss = 0.81 (855.3 examples/sec; 0.150 sec/batch)
2017-05-07 21:26:29.539096: step 34010, loss = 0.97 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:26:30.898862: step 34020, loss = 0.77 (941.3 examples/sec; 0.136 sec/batch)
2017-05-07 21:26:32.260843: step 34030, loss = 0.89 (939.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:26:33.623777: step 34040, loss = 0.84 (939.2 examples/sec; 0.136 sec/batch)
2017-05-07 21:26:35.005477: step 34050, loss = 0.76 (926.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:26:36.383345: step 34060, loss = 0.92 (929.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:26:37.749091: step 34070, loss = 0.78 (937.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:26:39.139441: step 34080, loss = 0.83 (920.6 examples/sec; 0.139 sec/batch)
2017-05-07 21:26:40.518796: step 34090, loss = 0.79 (928.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:26:41.988796: step 34100, loss = 0.88 (870.7 examples/sec; 0.147 sec/batch)
2017-05-07 21:26:43.286854: step 34110, loss = 1.00 (986.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:26:44.662532: step 34120, loss = 0.82 (930.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:26:46.006737: step 34130, loss = 0.74 (952.2 examples/sec; 0.134 sec/batch)
2017-05-07 21:26:47.388892: step 34140, loss = 0.82 (926.1 examples/sec; 0.138 sec/batch)
2017-05-07 21:26:48.775180: step 34150, loss = 0.83 (923.3 examples/sec; 0.139 sec/batch)
2017-05-07 21:26:50.166233: step 34160, loss = 0.92 (920.2 examples/sec; 0.139 sec/batch)
2017-05-07 21:26:51.537453: step 34170, loss = 0.87 (933.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:26:52.904389: step 34180, loss = 0.83 (936.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:26:54.273995: step 34190, loss = 0.75 (934.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:26:55.743064: step 34200, loss = 0.89 (871.3 examples/sec; 0.147 sec/batch)
2017-05-07 21:26:57.025478: step 34210, loss = 0.79 (998.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:26:58.367828: step 34220, loss = 0.77 (953.6 examples/sec; 0.134 sec/batch)
2017-05-07 21:26:59.731162: step 34230, loss = 0.73 (938.9 examples/sec; 0.136 sec/batch)
2017-05-07 21:27:01.084176: step 34240, loss = 0.93 (946.0 examples/sec; 0.135 sec/batch)
2017-05-07 21:27:02.464737: step 34250, loss = 0.84 (927.2 examples/sec; 0.138 sec/batch)
2017-05-07 21:27:03.848852: step 34260, loss = 0.83 (924.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:27:05.192537: step 34270, loss = 0.89 (952.6 examples/sec; 0.134 sec/batch)
2017-05-07 21:27:06.521395: step 34280, loss = 0.80 (963.2 examples/sec; 0.133 sec/batch)
2017-05-07 21:27:07.884815: step 34290, loss = 0.73 (938.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:27:09.334246: step 34300, loss = 0.87 (883.1 examples/sec; 0.145 sec/batch)
2017-05-07 21:27:10.600676: step 34310, loss = 0.76 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:27:11.986658: step 34320, loss = 0.87 (923.5 examples/sec; 0.139 sec/batch)
2017-05-07 21:27:13.390307: step 34330, loss = 0.87 (911.9 examples/sec; 0.140 sec/batch)
2017-05-07 21:27:14.783533: step 34340, loss = 0.68 (918.7 examples/sec; 0.139 sec/batch)
2017-05-07 21:27:16.162915: step 34350, loss = 0.75 (928.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:27:17.517853: step 34360, loss = 0.79 (944.7 examples/sec; 0.135 sec/batch)
2017-05-07 21:27:18.877855: step 34370, loss = 1.16 (941.2 examples/sec; 0.136 sec/batch)
2017-05-07 21:27:20.252242: step 34380, loss = 0.82 (931.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:27:21.639358: step 34390, loss = 0.81 (922.8 examples/sec; 0.139 sec/batch)
2017-05-07 21:27:23.101852: step 34400, loss = 0.75 (875.2 examples/sec; 0.146 sec/batch)
2017-05-07 21:27:24.368771: step 34410, loss = 0.87 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:27:25.731254: step 34420, loss = 0.86 (939.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:27:27.077674: step 34430, loss = 0.85 (950.7 examples/sec; 0.135 sec/batch)
2017-05-07 21:27:28.435886: step 34440, loss = 0.88 (942.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:27:29.785471: step 34450, loss = 0.95 (948.4 examples/sec; 0.135 sec/batch)
2017-05-07 21:27:31.138041: step 34460, loss = 0.63 (946.3 examples/sec; 0.135 sec/batch)
2017-05-07 21:27:32.523026: step 34470, loss = 0.96 (924.2 examples/sec; 0.138 sec/batch)
2017-05-07 21:27:33.865453: step 34480, loss = 0.67 (953.5 examples/sec; 0.134 sec/batch)
2017-05-07 21:27:35.242633: step 34490, loss = 0.97 (929.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:27:36.715844: step 34500, loss = 0.62 (868.8 examples/sec; 0.147 sec/batch)
2017-05-07 21:27:37.986777: step 34510, loss = 0.81 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:27:39.327091: step 34520, loss = 0.80 (955.0 examples/sec; 0.134 sec/batch)
2017-05-07 21:27:40.682716: step 34530, loss = 1.00 (944.2 examples/sec; 0.136 sec/batch)
2017-05-07 21:27:42.080922: step 34540, loss = 0.95 (915.5 examples/sec; 0.140 sec/batch)
2017-05-07 21:27:43.468608: step 34550, loss = 0.72 (922.4 examples/sec; 0.139 sec/batch)
2017-05-07 21:27:44.870201: step 34560, loss = 1.01 (913.2 examples/sec; 0.140 sec/batch)
2017-05-07 21:27:46.258195: step 34570, loss = 0.91 (922.2 examples/sec; 0.139 sec/batch)
2017-05-07 21:27:47.666560: step 34580, loss = 1.07 (908.9 examples/sec; 0.141 sec/batch)
2017-05-07 21:27:49.015874: step 34590, loss = 0.88 (948.6 examples/sec; 0.135 sec/batch)
2017-05-07 21:27:50.476570: step 34600, loss = 0.70 (876.3 examples/sec; 0.146 sec/batch)
2017-05-07 21:27:51.745536: step 34610, loss = 0.85 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:27:53.103407: step 34620, loss = 0.71 (942.6 examples/sec; 0.136 sec/batch)
2017-05-07 21:27:54.465686: step 34630, loss = 0.88 (939.6 examples/sec; 0.136 sec/batch)
2017-05-07 21:27:55.839028: step 34640, loss = 0.76 (932.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:27:57.202455: step 34650, loss = 0.84 (938.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:27:58.572116: step 34660, loss = 0.73 (934.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:27:59.971155: step 34670, loss = 0.81 (914.9 examples/sec; 0.140 sec/batch)
2017-05-07 21:28:01.316067: step 34680, loss = 1.05 (951.7 examples/sec; 0.134 sec/batch)
2017-05-07 21:28:02.692585: step 34690, loss = 1.01 (929.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:28:04.165132: step 34700, loss = 0.90 (869.2 examples/sec; 0.147 sec/batch)
2017-05-07 21:28:05.432700: step 34710, loss = 0.73 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:28:06.809803: step 34720, loss = 0.71 (929.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:28:08.193595: step 34730, loss = 0.90 (925.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:28:09.535125: step 34740, loss = 0.72 (954.1 examples/sec; 0.134 sec/batch)
2017-05-07 21:28:10.934954: step 34750, loss = 0.75 (914.4 examples/sec; 0.140 sec/batch)
2017-05-07 21:28:12.326349: step 34760, loss = 0.94 (919.9 examples/sec; 0.139 sec/batch)
2017-05-07 21:28:13.686941: step 34770, loss = 0.70 (940.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:28:15.065858: step 34780, loss = 0.74 (928.3 examples/sec; 0.138 sec/batch)
2017-05-07 21:28:16.455297: step 34790, loss = 0.90 (921.2 examples/sec; 0.139 sec/batch)
2017-05-07 21:28:17.959039: step 34800, loss = 0.94 (851.2 examples/sec; 0.150 sec/batch)
2017-05-07 21:28:19.238516: step 34810, loss = 0.80 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:28:20.630848: step 34820, loss = 0.70 (919.3 examples/sec; 0.139 sec/batch)
2017-05-07 21:28:22.016168: step 34830, loss = 0.89 (924.0 examples/sec; 0.139 sec/batch)
2017-05-07 21:28:23.432181: step 34840, loss = 0.72 (903.9 examples/sec; 0.142 sec/batch)
2017-05-07 21:28:24.813928: step 34850, loss = 0.93 (926.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:28:26.163148: step 34860, loss = 0.77 (948.7 examples/sec; 0.135 sec/batch)
2017-05-07 21:28:27.529170: step 34870, loss = 0.77 (937.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:28:28.905687: step 34880, loss = 0.70 (929.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:28:30.280903: step 34890, loss = 0.87 (930.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:28:31.755621: step 34900, loss = 0.87 (868.0 examples/sec; 0.147 sec/batch)
2017-05-07 21:28:33.046435: step 34910, loss = 0.82 (991.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:28:34.433556: step 34920, loss = 0.83 (922.8 examples/sec; 0.139 sec/batch)
2017-05-07 21:28:35.821081: step 34930, loss = 1.03 (922.5 examples/sec; 0.139 sec/batch)
2017-05-07 21:28:37.255991: step 34940, loss = 0.90 (892.1 examples/sec; 0.143 sec/batch)
2017-05-07 21:28:38.645768: step 34950, loss = 0.91 (921.0 examples/sec; 0.139 sec/batch)
2017-05-07 21:28:40.039936: step 34960, loss = 0.77 (918.1 examples/sec; 0.139 sec/batch)
2017-05-07 21:28:41.421675: step 34970, loss = 0.86 (926.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:28:42.831418: step 34980, loss = 0.86 (908.0 examples/sec; 0.141 sec/batch)
2017-05-07 21:28:44.219456: step 34990, loss = 0.78 (922.2 examples/sec; 0.139 sec/batch)
2017-05-07 21:28:45.672166: step 35000, loss = 0.82 (881.1 examples/sec; 0.145 sec/batch)
2017-05-07 21:28:46.946190: step 35010, loss = 0.78 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:28:48.316475: step 35020, loss = 0.88 (934.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:28:49.699817: step 35030, loss = 0.79 (925.3 examples/sec; 0.138 sec/batch)
2017-05-07 21:28:51.073214: step 35040, loss = 0.82 (932.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:28:52.428158: step 35050, loss = 0.84 (944.7 examples/sec; 0.135 sec/batch)
2017-05-07 21:28:53.777217: step 35060, loss = 0.68 (948.8 examples/sec; 0.135 sec/batch)
2017-05-07 21:28:55.168702: step 35070, loss = 0.84 (919.9 examples/sec; 0.139 sec/batch)
2017-05-07 21:28:56.513037: step 35080, loss = 0.74 (952.1 examples/sec; 0.134 sec/batch)
2017-05-07 21:28:57.900573: step 35090, loss = 0.87 (922.5 examples/sec; 0.139 sec/batch)
2017-05-07 21:28:59.360705: step 35100, loss = 1.03 (876.6 examples/sec; 0.146 sec/batch)
2017-05-07 21:29:00.643278: step 35110, loss = 0.81 (998.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:29:02.031149: step 35120, loss = 0.83 (922.3 examples/sec; 0.139 sec/batch)
2017-05-07 21:29:03.428340: step 35130, loss = 0.78 (916.1 examples/sec; 0.140 sec/batch)
2017-05-07 21:29:04.830465: step 35140, loss = 0.88 (912.9 examples/sec; 0.140 sec/batch)
2017-05-07 21:29:06.218994: step 35150, loss = 0.88 (921.9 examples/sec; 0.139 sec/batch)
2017-05-07 21:29:07.594393: step 35160, loss = 0.88 (930.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:29:08.954691: step 35170, loss = 0.78 (941.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:29:10.325060: step 35180, loss = 0.75 (934.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:29:11.706457: step 35190, loss = 0.83 (926.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:29:13.159836: step 35200, loss = 1.06 (880.7 examples/sec; 0.145 sec/batch)
2017-05-07 21:29:14.457799: step 35210, loss = 0.90 (986.2 examples/sec; 0.130 sec/batch)
2017-05-07 21:29:15.808677: step 35220, loss = 0.82 (947.5 examples/sec; 0.135 sec/batch)
2017-05-07 21:29:17.194918: step 35230, loss = 0.80 (923.4 examples/sec; 0.139 sec/batch)
2017-05-07 21:29:18.542494: step 35240, loss = 0.81 (949.9 examples/sec; 0.135 sec/batch)
2017-05-07 21:29:19.924177: step 35250, loss = 0.75 (926.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:29:21.276328: step 35260, loss = 0.85 (946.6 examples/sec; 0.135 sec/batch)
2017-05-07 21:29:22.652003: step 35270, loss = 0.79 (930.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:29:24.009825: step 35280, loss = 0.74 (942.7 examples/sec; 0.136 sec/batch)
2017-05-07 21:29:25.374152: step 35290, loss = 0.79 (938.2 examples/sec; 0.136 sec/batch)
2017-05-07 21:29:26.823501: step 35300, loss = 0.92 (883.2 examples/sec; 0.145 sec/batch)
2017-05-07 21:29:28.092486: step 35310, loss = 0.89 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:29:29.484153: step 35320, loss = 1.06 (919.8 examples/sec; 0.139 sec/batch)
2017-05-07 21:29:30.827511: step 35330, loss = 0.91 (952.8 examples/sec; 0.134 sec/batch)
2017-05-07 21:29:32.215579: step 35340, loss = 0.86 (922.1 examples/sec; 0.139 sec/batch)
2017-05-07 21:29:33.569553: step 35350, loss = 0.72 (945.4 examples/sec; 0.135 sec/batch)
2017-05-07 21:29:34.929629: step 35360, loss = 0.79 (941.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:29:36.315164: step 35370, loss = 0.86 (923.8 examples/sec; 0.139 sec/batch)
2017-05-07 21:29:37.696518: step 35380, loss = 0.91 (926.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:29:39.089178: step 35390, loss = 0.80 (919.1 examples/sec; 0.139 sec/batch)
2017-05-07 21:29:40.581735: step 35400, loss = 1.02 (857.6 examples/sec; 0.149 sec/batch)
2017-05-07 21:29:41.859253: step 35410, loss = 0.79 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:29:43.238762: step 35420, loss = 1.05 (927.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:29:44.651045: step 35430, loss = 0.79 (906.3 examples/sec; 0.141 sec/batch)
2017-05-07 21:29:46.002779: step 35440, loss = 0.66 (946.9 examples/sec; 0.135 sec/batch)
2017-05-07 21:29:47.393243: step 35450, loss = 0.76 (920.6 examples/sec; 0.139 sec/batch)
2017-05-07 21:29:48.757521: step 35460, loss = 0.87 (938.2 examples/sec; 0.136 sec/batch)
2017-05-07 21:29:50.143216: step 35470, loss = 0.76 (923.7 examples/sec; 0.139 sec/batch)
2017-05-07 21:29:51.506589: step 35480, loss = 0.81 (938.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:29:52.866162: step 35490, loss = 0.72 (941.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:29:54.318663: step 35500, loss = 0.81 (881.2 examples/sec; 0.145 sec/batch)
2017-05-07 21:29:55.606662: step 35510, loss = 0.92 (993.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:29:57.004767: step 35520, loss = 1.11 (915.5 examples/sec; 0.140 sec/batch)
2017-05-07 21:29:58.394898: step 35530, loss = 0.72 (920.8 examples/sec; 0.139 sec/batch)
2017-05-07 21:29:59.784934: step 35540, loss = 0.84 (920.8 examples/sec; 0.139 sec/batch)
2017-05-07 21:30:01.132266: step 35550, loss = 0.75 (950.0 examples/sec; 0.135 sec/batch)
2017-05-07 21:30:02.494189: step 35560, loss = 0.76 (939.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:30:03.873732: step 35570, loss = 0.90 (927.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:30:05.233275: step 35580, loss = 0.74 (941.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:30:06.654998: step 35590, loss = 0.74 (900.3 examples/sec; 0.142 sec/batch)
2017-05-07 21:30:08.158872: step 35600, loss = 0.77 (851.1 examples/sec; 0.150 sec/batch)
2017-05-07 21:30:09.450802: step 35610, loss = 0.73 (990.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:30:10.840654: step 35620, loss = 0.83 (921.0 examples/sec; 0.139 sec/batch)
2017-05-07 21:30:12.232507: step 35630, loss = 0.86 (919.6 examples/sec; 0.139 sec/batch)
2017-05-07 21:30:13.612078: step 35640, loss = 0.81 (927.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:30:14.949740: step 35650, loss = 0.95 (956.9 examples/sec; 0.134 sec/batch)
2017-05-07 21:30:16.327589: step 35660, loss = 0.87 (929.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:30:17.698070: step 35670, loss = 0.76 (934.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:30:19.069884: step 35680, loss = 0.82 (933.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:30:20.436616: step 35690, loss = 0.88 (936.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:30:21.895450: step 35700, loss = 0.71 (877.4 examples/sec; 0.146 sec/batch)
2017-05-07 21:30:23.188438: step 35710, loss = 0.88 (990.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:30:24.536049: step 35720, loss = 0.86 (949.8 examples/sec; 0.135 sec/batch)
2017-05-07 21:30:25.924492: step 35730, loss = 0.80 (921.9 examples/sec; 0.139 sec/batch)
2017-05-07 21:30:27.279843: step 35740, loss = 0.90 (944.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:30:28.668642: step 35750, loss = 0.75 (921.7 examples/sec; 0.139 sec/batch)
2017-05-07 21:30:30.080678: step 35760, loss = 0.91 (906.5 examples/sec; 0.141 sec/batch)
2017-05-07 21:30:31.373265: step 35770, loss = 0.84 (990.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:30:32.653275: step 35780, loss = 0.88 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:30:33.933360: step 35790, loss = 0.80 (999.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:30:35.311083: step 35800, loss = 0.88 (929.1 examples/sec; 0.138 sec/batch)
2017-05-07 21:30:36.514327: step 35810, loss = 0.89 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-07 21:30:37.795494: step 35820, loss = 0.85 (999.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:30:39.085462: step 35830, loss = 0.93 (992.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:30:40.374664: step 35840, loss = 0.80 (992.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:30:41.640815: step 35850, loss = 0.89 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:30:42.955678: step 35860, loss = 0.79 (973.5 examples/sec; 0.131 sec/batch)
2017-05-07 21:30:44.246175: step 35870, loss = 0.73 (991.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:30:45.529687: step 35880, loss = 0.84 (997.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:30:46.782888: step 35890, loss = 0.74 (1021.4 examples/sec; 0.125 sec/batch)
2017-05-07 21:30:48.154230: step 35900, loss = 0.64 (933.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:30:49.329651: step 35910, loss = 0.80 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-07 21:30:50.581771: step 35920, loss = 0.81 (1022.3 examples/sec; 0.125 sec/batch)
2017-05-07 21:30:51.833521: step 35930, loss = 0.75 (1022.6 examples/sec; 0.125 sec/batch)
2017-05-07 21:30:53.103381: step 35940, loss = 0.77 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:30:54.411833: step 35950, loss = 0.95 (978.3 examples/sec; 0.131 sec/batch)
2017-05-07 21:30:55.717033: step 35960, loss = 0.90 (980.7 examples/sec; 0.131 sec/batch)
2017-05-07 21:30:56.979837: step 35970, loss = 0.79 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:30:58.276882: step 35980, loss = 0.87 (986.9 examples/sec; 0.130 sec/batch)
2017-05-07 21:30:59.544608: step 35990, loss = 0.83 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:31:00.923080: step 36000, loss = 0.92 (928.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:31:02.068567: step 36010, loss = 0.70 (1117.4 examples/sec; 0.115 sec/batch)
2017-05-07 21:31:03.344830: step 36020, loss = 0.88 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:31:04.633410: step 36030, loss = 0.72 (993.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:31:05.896224: step 36040, loss = 0.95 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:31:07.176882: step 36050, loss = 0.78 (999.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:31:08.484156: step 36060, loss = 0.97 (979.1 examples/sec; 0.131 sec/batch)
2017-05-07 21:31:09.754768: step 36070, loss = 0.81 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:31:11.058891: step 36080, loss = 0.84 (981.5 examples/sec; 0.130 sec/batch)
2017-05-07 21:31:12.339788: step 36090, loss = 0.70 (999.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:31:13.712531: step 36100, loss = 0.80 (932.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:31:14.886569: step 36110, loss = 0.82 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-07 21:31:16.162147: step 36120, loss = 0.84 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:31:17.434710: step 36130, loss = 0.73 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:31:18.715242: step 36140, loss = 0.88 (999.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:31:19.992510: step 36150, loss = 0.85 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:31:21.291206: step 36160, loss = 0.66 (985.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:31:22.572184: step 36170, loss = 1.04 (999.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:31:23.859906: step 36180, loss = 0.79 (994.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:31:25.161628: step 36190, loss = 0.87 (983.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:31:26.520605: step 36200, loss = 0.88 (941.9 examples/sec; 0.136 sec/batch)
2017-05-07 21:31:27.693820: step 36210, loss = 0.96 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-07 21:31:28.951019: step 36220, loss = 0.83 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:31:30.201808: step 36230, loss = 0.82 (1023.4 examples/sec; 0.125 sec/batch)
2017-05-07 21:31:31.467504: step 36240, loss = 0.91 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:31:32.735343: step 36250, loss = 0.94 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:31:34.000162: step 36260, loss = 1.03 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:31:35.272153: step 36270, loss = 0.85 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:31:36.530675: step 36280, loss = 0.65 (1017.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:31:37.789999: step 36290, loss = 0.77 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:31:39.135448: step 36300, loss = 0.79 (951.4 examples/sec; 0.135 sec/batch)
2017-05-07 21:31:40.304523: step 36310, loss = 0.80 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-07 21:31:41.558510: step 36320, loss = 0.76 (1020.7 examples/sec; 0.125 sec/batch)
2017-05-07 21:31:42.814574: step 36330, loss = 0.92 (1019.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:31:44.085578: step 36340, loss = 0.91 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:31:45.367230: step 36350, loss = 0.73 (998.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:31:46.642187: step 36360, loss = 0.96 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:31:47.905459: step 36370, loss = 0.80 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:31:49.173864: step 36380, loss = 0.79 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:31:50.449111: step 36390, loss = 0.76 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:31:51.824694: step 36400, loss = 0.95 (930.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:31:52.999392: step 36410, loss = 0.95 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-07 21:31:54.238064: step 36420, loss = 0.95 (1033.4 examples/sec; 0.124 sec/batch)
2017-05-07 21:31:55.542295: step 36430, loss = 0.82 (981.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:31:56.785141: step 36440, loss = 0.74 (1029.9 examples/sec; 0.124 sec/batch)
2017-05-07 21:31:58.045524: step 36450, loss = 0.89 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:31:59.333188: step 36460, loss = 0.71 (994.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:32:00.584439: step 36470, loss = 0.90 (1023.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:32:01.837575: step 36480, loss = 0.87 (1021.4 examples/sec; 0.125 sec/batch)
2017-05-07 21:32:03.135278: step 36490, loss = 0.96 (986.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:32:04.518950: step 36500, loss = 0.88 (925.1 examples/sec; 0.138 sec/batch)
2017-05-07 21:32:05.719201: step 36510, loss = 0.84 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-07 21:32:06.994539: step 36520, loss = 0.87 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:32:08.243246: step 36530, loss = 0.89 (1025.1 examples/sec; 0.125 sec/batch)
2017-05-07 21:32:09.498867: step 36540, loss = 0.81 (1019.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:32:10.761430: step 36550, loss = 0.80 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:32:12.026451: step 36560, loss = 0.93 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:32:13.287610: step 36570, loss = 0.90 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:32:14.532134: step 36580, loss = 0.91 (1028.5 examples/sec; 0.124 sec/batch)
2017-05-07 21:32:15.809017: step 36590, loss = 0.79 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:32:17.163529: step 36600, loss = 0.72 (945.0 examples/sec; 0.135 sec/batch)
2017-05-07 21:32:18.334387: step 36610, loss = 0.98 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-07 21:32:19.596331: step 36620, loss = 0.93 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:32:20.884551: step 36630, loss = 0.96 (993.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:32:22.148433: step 36640, loss = 0.85 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:32:23.431598: step 36650, loss = 0.80 (997.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:32:24.695175: step 36660, loss = 0.73 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:32:25.952703: step 36670, loss = 1.14 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:32:27.229216: step 36680, loss = 0.81 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:32:28.500840: step 36690, loss = 0.71 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:32:29.862858: step 36700, loss = 0.96 (939.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:32:31.033668: step 36710, loss = 0.84 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-07 21:32:32.285639: step 36720, loss = 0.76 (1022.4 examples/sec; 0.125 sec/batch)
2017-05-07 21:32:33.541204: step 36730, loss = 0.85 (1019.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:32:34.809307: step 36740, loss = 0.85 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:32:36.099620: step 36750, loss = 0.76 (992.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:32:37.394751: step 36760, loss = 0.80 (988.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:32:38.638505: step 36770, loss = 0.87 (1029.1 examples/sec; 0.124 sec/batch)
2017-05-07 21:32:39.908507: step 36780, loss = 0.93 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:32:41.184561: step 36790, loss = 0.81 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:32:42.517436: step 36800, loss = 0.91 (960.3 examples/sec; 0.133 sec/batch)
2017-05-07 21:32:43.721731: step 36810, loss = 0.91 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-07 21:32:44.970470: step 36820, loss = 0.80 (1025.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:32:46.225328: step 36830, loss = 0.83 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:32:47.497029: step 36840, loss = 0.73 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:32:48.798798: step 36850, loss = 0.85 (983.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:32:50.050438: step 36860, loss = 0.81 (1022.7 examples/sec; 0.125 sec/batch)
2017-05-07 21:32:51.293162: step 36870, loss = 0.86 (1030.0 examples/sec; 0.124 sec/batch)
2017-05-07 21:32:52.567660: step 36880, loss = 0.91 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:32:53.808376: step 36890, loss = 0.76 (1031.7 examples/sec; 0.124 sec/batch)
2017-05-07 21:32:55.164149: step 36900, loss = 0.86 (944.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:32:56.357750: step 36910, loss = 0.80 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-07 21:32:57.657496: step 36920, loss = 0.88 (984.8 examples/sec; 0.130 sec/batch)
2017-05-07 21:32:58.945367: step 36930, loss = 0.92 (993.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:33:00.213302: step 36940, loss = 0.86 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:33:01.483250: step 36950, loss = 0.82 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:33:02.729417: step 36960, loss = 0.86 (1027.2 examples/sec; 0.125 sec/batch)
2017-05-07 21:33:04.007537: step 36970, loss = 0.95 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:33:05.286084: step 36980, loss = 1.00 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:33:06.523740: step 36990, loss = 0.70 (1034.2 examples/sec; 0.124 sec/batch)
2017-05-07 21:33:07.887033: step 37000, loss = 0.92 (938.9 examples/sec; 0.136 sec/batch)
2017-05-07 21:33:09.095690: step 37010, loss = 0.79 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-07 21:33:10.337525: step 37020, loss = 0.79 (1030.7 examples/sec; 0.124 sec/batch)
2017-05-07 21:33:11.617349: step 37030, loss = 0.90 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:33:12.888597: step 37040, loss = 0.79 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:33:14.185061: step 37050, loss = 0.71 (987.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:33:15.465880: step 37060, loss = 0.95 (999.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:33:16.745581: step 37070, loss = 0.79 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:33:18.007434: step 37080, loss = 0.95 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:33:19.285958: step 37090, loss = 0.71 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:33:20.638356: step 37100, loss = 0.88 (946.5 examples/sec; 0.135 sec/batch)
2017-05-07 21:33:21.813616: step 37110, loss = 0.77 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-07 21:33:23.097075: step 37120, loss = 0.74 (997.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:33:24.345479: step 37130, loss = 0.68 (1025.3 examples/sec; 0.125 sec/batch)
2017-05-07 21:33:25.602850: step 37140, loss = 0.71 (1018.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:33:26.881010: step 37150, loss = 0.90 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:33:28.173751: step 37160, loss = 0.99 (990.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:33:29.469734: step 37170, loss = 0.85 (987.7 examples/sec; 0.130 sec/batch)
2017-05-07 21:33:30.737198: step 37180, loss = 0.89 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:33:31.999645: step 37190, loss = 0.78 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:33:33.353183: step 37200, loss = 1.04 (945.7 examples/sec; 0.135 sec/batch)
2017-05-07 21:33:34.541429: step 37210, loss = 0.94 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-07 21:33:35.793772: step 37220, loss = 0.82 (1022.1 examples/sec; 0.125 sec/batch)
2017-05-07 21:33:37.067242: step 37230, loss = 0.78 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:33:38.325687: step 37240, loss = 0.69 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:33:39.599826: step 37250, loss = 0.95 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:33:40.872391: step 37260, loss = 0.84 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:33:42.131478: step 37270, loss = 1.00 (1016.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:33:43.407602: step 37280, loss = 0.73 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:33:44.672623: step 37290, loss = 0.87 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:33:46.031130: step 37300, loss = 0.83 (942.2 examples/sec; 0.136 sec/batch)
2017-05-07 21:33:47.180490: step 37310, loss = 0.78 (1113.7 examples/sec; 0.115 sec/batch)
2017-05-07 21:33:48.447315: step 37320, loss = 0.86 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:33:49.705048: step 37330, loss = 0.78 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:33:50.994751: step 37340, loss = 1.09 (992.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:33:52.290063: step 37350, loss = 0.86 (988.2 examples/sec; 0.130 sec/batch)
2017-05-07 21:33:53.557271: step 37360, loss = 0.98 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:33:54.815516: step 37370, loss = 0.89 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:33:56.104478: step 37380, loss = 0.71 (993.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:33:57.381512: step 37390, loss = 0.84 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:33:58.762163: step 37400, loss = 1.06 (927.1 examples/sec; 0.138 sec/batch)
2017-05-07 21:33:59.972608: step 37410, loss = 0.90 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-07 21:34:01.271711: step 37420, loss = 0.92 (985.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:34:02.550689: step 37430, loss = 1.01 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:34:03.826782: step 37440, loss = 0.84 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:34:05.082183: step 37450, loss = 0.81 (1019.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:34:06.389894: step 37460, loss = 0.87 (978.8 examples/sec; 0.131 sec/batch)
2017-05-07 21:34:07.642245: step 37470, loss = 0.91 (1022.1 examples/sec; 0.125 sec/batch)
2017-05-07 21:34:08.928615: step 37480, loss = 0.83 (995.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:34:10.196007: step 37490, loss = 0.80 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:34:11.536068: step 37500, loss = 0.82 (955.2 examples/sec; 0.134 sec/batch)
2017-05-07 21:34:12.741947: step 37510, loss = 0.76 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-07 21:34:14.024483: step 37520, loss = 0.86 (998.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:34:15.268533: step 37530, loss = 0.73 (1028.9 examples/sec; 0.124 sec/batch)
2017-05-07 21:34:16.524733: step 37540, loss = 0.75 (1018.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:34:17.809164: step 37550, loss = 0.93 (996.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:34:19.089932: step 37560, loss = 0.87 (999.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:34:20.388255: step 37570, loss = 0.82 (985.9 examples/sec; 0.130 sec/batch)
2017-05-07 21:34:21.679727: step 37580, loss = 0.90 (991.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:34:22.938578: step 37590, loss = 0.79 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:34:24.303925: step 37600, loss = 0.99 (937.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:34:25.455552: step 37610, loss = 0.75 (1111.5 examples/sec; 0.115 sec/batch)
2017-05-07 21:34:26.753753: step 37620, loss = 0.81 (986.0 examples/sec; 0.130 sec/batch)
2017-05-07 21:34:28.022711: step 37630, loss = 0.91 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:34:29.301416: step 37640, loss = 0.83 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:34:30.574216: step 37650, loss = 0.81 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:34:31.843655: step 37660, loss = 0.78 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:34:33.133662: step 37670, loss = 0.88 (992.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:34:34.383704: step 37680, loss = 0.74 (1024.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:34:35.642758: step 37690, loss = 0.76 (1016.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:34:37.037572: step 37700, loss = 0.77 (917.7 examples/sec; 0.139 sec/batch)
2017-05-07 21:34:38.211702: step 37710, loss = 0.86 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-07 21:34:39.510945: step 37720, loss = 0.82 (985.2 examples/sec; 0.130 sec/batch)
2017-05-07 21:34:40.807683: step 37730, loss = 0.90 (987.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:34:42.086577: step 37740, loss = 0.77 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:34:43.352046: step 37750, loss = 0.77 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:34:44.633321: step 37760, loss = 0.76 (999.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:34:45.902790: step 37770, loss = 0.89 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:34:47.196233: step 37780, loss = 0.81 (989.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:34:48.462303: step 37790, loss = 0.71 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:34:49.861077: step 37800, loss = 0.94 (915.1 examples/sec; 0.140 sec/batch)
2017-05-07 21:34:51.047175: step 37810, loss = 0.68 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-07 21:34:52.352749: step 37820, loss = 0.80 (980.4 examples/sec; 0.131 sec/batch)
2017-05-07 21:34:53.638694: step 37830, loss = 0.96 (995.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:34:54.902824: step 37840, loss = 0.72 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:34:56.196340: step 37850, loss = 0.72 (989.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:34:57.484512: step 37860, loss = 0.78 (993.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:34:58.767042: step 37870, loss = 0.97 (998.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:35:00.042740: step 37880, loss = 0.81 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:35:01.308832: step 37890, loss = 0.78 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:35:02.655200: step 37900, loss = 0.84 (950.7 examples/sec; 0.135 sec/batch)
2017-05-07 21:35:03.858083: step 37910, loss = 0.89 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-07 21:35:05.124329: step 37920, loss = 0.89 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:35:06.397611: step 37930, loss = 0.83 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:35:07.703674: step 37940, loss = 0.76 (980.1 examples/sec; 0.131 sec/batch)
2017-05-07 21:35:08.988599: step 37950, loss = 0.86 (996.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:35:10.258480: step 37960, loss = 0.71 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:35:11.511047: step 37970, loss = 0.90 (1021.9 examples/sec; 0.125 sec/batch)
2017-05-07 21:35:12.796538: step 37980, loss = 0.84 (995.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:35:14.061274: step 37990, loss = 0.69 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:35:15.428682: step 38000, loss = 0.76 (936.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:35:16.610475: step 38010, loss = 0.77 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-07 21:35:17.890318: step 38020, loss = 0.82 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:35:19.175537: step 38030, loss = 0.87 (995.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:35:20.456378: step 38040, loss = 0.86 (999.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:35:21.727780: step 38050, loss = 0.96 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:35:23.012504: step 38060, loss = 1.03 (996.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:35:24.317455: step 38070, loss = 0.82 (980.9 examples/sec; 0.130 sec/batch)
2017-05-07 21:35:25.572359: step 38080, loss = 0.95 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:35:26.856712: step 38090, loss = 0.90 (996.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:35:28.213475: step 38100, loss = 0.85 (943.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:35:29.398279: step 38110, loss = 0.73 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-07 21:35:30.657025: step 38120, loss = 0.89 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:35:31.919641: step 38130, loss = 0.89 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:35:33.186414: step 38140, loss = 0.92 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:35:34.481015: step 38150, loss = 0.74 (988.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:35:35.755229: step 38160, loss = 0.76 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:35:37.016682: step 38170, loss = 0.94 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:35:38.270602: step 38180, loss = 0.84 (1020.8 examples/sec; 0.125 sec/batch)
2017-05-07 21:35:39.540226: step 38190, loss = 0.93 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:35:40.894688: step 38200, loss = 0.86 (945.0 examples/sec; 0.135 sec/batch)
2017-05-07 21:35:42.042529: step 38210, loss = 0.84 (1115.1 examples/sec; 0.115 sec/batch)
2017-05-07 21:35:43.288043: step 38220, loss = 0.76 (1027.7 examples/sec; 0.125 sec/batch)
2017-05-07 21:35:44.563702: step 38230, loss = 0.84 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:35:45.860168: step 38240, loss = 0.77 (987.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:35:47.154806: step 38250, loss = 1.01 (988.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:35:48.459203: step 38260, loss = 0.86 (981.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:35:49.766167: step 38270, loss = 0.83 (979.4 examples/sec; 0.131 sec/batch)
2017-05-07 21:35:51.050205: step 38280, loss = 0.77 (996.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:35:52.303821: step 38290, loss = 0.87 (1021.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:35:53.675255: step 38300, loss = 0.91 (933.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:35:54.844365: step 38310, loss = 0.88 (1094.9 examples/sec; 0.117 sec/batch)
2017-05-07 21:35:56.113756: step 38320, loss = 1.09 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:35:57.365881: step 38330, loss = 0.81 (1022.3 examples/sec; 0.125 sec/batch)
2017-05-07 21:35:58.619672: step 38340, loss = 1.06 (1020.9 examples/sec; 0.125 sec/batch)
2017-05-07 21:35:59.896822: step 38350, loss = 0.93 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:36:01.166170: step 38360, loss = 1.00 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:36:02.466311: step 38370, loss = 0.77 (984.5 examples/sec; 0.130 sec/batch)
2017-05-07 21:36:03.717417: step 38380, loss = 0.87 (1023.1 examples/sec; 0.125 sec/batch)
2017-05-07 21:36:04.973707: step 38390, loss = 1.02 (1018.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:36:06.331601: step 38400, loss = 0.79 (942.6 examples/sec; 0.136 sec/batch)
2017-05-07 21:36:07.487945: step 38410, loss = 0.89 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-07 21:36:08.787565: step 38420, loss = 0.80 (984.9 examples/sec; 0.130 sec/batch)
2017-05-07 21:36:10.042107: step 38430, loss = 0.92 (1020.3 examples/sec; 0.125 sec/batch)
2017-05-07 21:36:11.318535: step 38440, loss = 0.81 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:36:12.620694: step 38450, loss = 0.81 (983.0 examples/sec; 0.130 sec/batch)
2017-05-07 21:36:13.885061: step 38460, loss = 0.83 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:36:15.188421: step 38470, loss = 0.79 (982.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:36:16.461257: step 38480, loss = 0.85 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:36:17.759943: step 38490, loss = 0.84 (985.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:36:19.119781: step 38500, loss = 0.80 (941.3 examples/sec; 0.136 sec/batch)
2017-05-07 21:36:20.314296: step 38510, loss = 0.82 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-07 21:36:21.607848: step 38520, loss = 0.82 (989.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:36:22.863021: step 38530, loss = 0.61 (1019.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:36:24.120737: step 38540, loss = 0.93 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:36:25.382422: step 38550, loss = 0.69 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:36:26.677705: step 38560, loss = 0.97 (988.2 examples/sec; 0.130 sec/batch)
2017-05-07 21:36:27.975073: step 38570, loss = 0.82 (986.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:36:29.250579: step 38580, loss = 0.70 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:36:30.564715: step 38590, loss = 0.67 (974.0 examples/sec; 0.131 sec/batch)
2017-05-07 21:36:31.938266: step 38600, loss = 0.78 (931.9 examples/sec; 0.137 sec/batch)
2017-05-07 21:36:33.119029: step 38610, loss = 0.86 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-07 21:36:34.394934: step 38620, loss = 0.71 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:36:35.674930: step 38630, loss = 0.78 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:36:36.951329: step 38640, loss = 0.81 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:36:38.207792: step 38650, loss = 0.97 (1018.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:36:39.477342: step 38660, loss = 0.73 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:36:40.722583: step 38670, loss = 0.89 (1027.9 examples/sec; 0.125 sec/batch)
2017-05-07 21:36:41.983325: step 38680, loss = 0.81 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:36:43.269867: step 38690, loss = 0.85 (994.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:36:44.614117: step 38700, loss = 0.79 (952.2 examples/sec; 0.134 sec/batch)
2017-05-07 21:36:45.790228: step 38710, loss = 0.82 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-07 21:36:47.064644: step 38720, loss = 0.96 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:36:48.319848: step 38730, loss = 0.66 (1019.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:36:49.584231: step 38740, loss = 1.02 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:36:50.874520: step 38750, loss = 0.91 (992.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:36:52.148724: step 38760, loss = 0.86 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:36:53.436426: step 38770, loss = 0.83 (994.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:36:54.692786: step 38780, loss = 0.94 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:36:55.969889: step 38790, loss = 1.01 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:36:57.319056: step 38800, loss = 0.92 (948.7 examples/sec; 0.135 sec/batch)
2017-05-07 21:36:58.495547: step 38810, loss = 0.91 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-07 21:36:59.799499: step 38820, loss = 0.75 (981.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:37:01.083792: step 38830, loss = 0.79 (996.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:37:02.340396: step 38840, loss = 1.01 (1018.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:37:03.615073: step 38850, loss = 0.84 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:37:04.897090: step 38860, loss = 0.74 (998.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:37:06.149013: step 38870, loss = 0.71 (1022.4 examples/sec; 0.125 sec/batch)
2017-05-07 21:37:07.430185: step 38880, loss = 0.68 (999.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:37:08.681307: step 38890, loss = 0.79 (1023.1 examples/sec; 0.125 sec/batch)
2017-05-07 21:37:10.036602: step 38900, loss = 0.72 (944.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:37:11.243376: step 38910, loss = 0.78 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-07 21:37:12.542035: step 38920, loss = 0.95 (985.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:37:13.775816: step 38930, loss = 0.73 (1037.5 examples/sec; 0.123 sec/batch)
2017-05-07 21:37:15.065363: step 38940, loss = 0.76 (992.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:37:16.359862: step 38950, loss = 0.80 (988.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:37:17.632606: step 38960, loss = 0.88 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:37:18.933105: step 38970, loss = 0.75 (984.2 examples/sec; 0.130 sec/batch)
2017-05-07 21:37:20.210070: step 38980, loss = 1.04 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:37:21.474220: step 38990, loss = 0.65 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:37:22.834990: step 39000, loss = 0.88 (940.7 examples/sec; 0.136 sec/batch)
2017-05-07 21:37:24.004341: step 39010, loss = 0.83 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-07 21:37:25.284426: step 39020, loss = 0.79 (999.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:37:26.565567: step 39030, loss = 0.78 (999.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:37:27.862522: step 39040, loss = 0.78 (986.9 examples/sec; 0.130 sec/batch)
2017-05-07 21:37:29.158919: step 39050, loss = 0.87 (987.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:37:30.454578: step 39060, loss = 0.82 (987.9 examples/sec; 0.130 sec/batch)
2017-05-07 21:37:31.744070: step 39070, loss = 0.92 (992.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:37:33.014055: step 39080, loss = 0.90 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:37:34.297616: step 39090, loss = 0.73 (997.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:37:35.671439: step 39100, loss = 0.89 (931.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:37:36.858630: step 39110, loss = 0.80 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-07 21:37:38.134317: step 39120, loss = 0.79 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:37:39.396718: step 39130, loss = 0.80 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:37:40.657655: step 39140, loss = 0.70 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:37:41.906220: step 39150, loss = 0.75 (1025.2 examples/sec; 0.125 sec/batch)
2017-05-07 21:37:43.198377: step 39160, loss = 0.68 (990.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:37:44.463823: step 39170, loss = 0.94 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:37:45.754015: step 39180, loss = 0.65 (992.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:37:47.012209: step 39190, loss = 0.76 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:37:48.365391: step 39200, loss = 0.74 (945.9 examples/sec; 0.135 sec/batch)
2017-05-07 21:37:49.536408: step 39210, loss = 0.72 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-07 21:37:50.811993: step 39220, loss = 0.67 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:37:52.075106: step 39230, loss = 0.69 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:37:53.344264: step 39240, loss = 0.80 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:37:54.596470: step 39250, loss = 0.79 (1022.2 examples/sec; 0.125 sec/batch)
2017-05-07 21:37:55.885699: step 39260, loss = 0.95 (992.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:37:57.175642: step 39270, loss = 0.97 (992.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:37:58.471890: step 39280, loss = 0.97 (987.5 examples/sec; 0.130 sec/batch)
2017-05-07 21:37:59.735460: step 39290, loss = 0.84 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:38:01.093421: step 39300, loss = 0.80 (942.6 examples/sec; 0.136 sec/batch)
2017-05-07 21:38:02.260526: step 39310, loss = 0.64 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-07 21:38:03.538274: step 39320, loss = 0.81 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:38:04.793710: step 39330, loss = 0.78 (1019.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:38:06.064769: step 39340, loss = 0.70 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:38:07.349718: step 39350, loss = 0.80 (996.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:38:08.631111: step 39360, loss = 0.81 (998.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:38:09.948335: step 39370, loss = 0.86 (971.7 examples/sec; 0.132 sec/batch)
2017-05-07 21:38:11.218013: step 39380, loss = 0.97 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:38:12.492322: step 39390, loss = 0.85 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:38:13.839627: step 39400, loss = 0.89 (950.0 examples/sec; 0.135 sec/batch)
2017-05-07 21:38:15.017537: step 39410, loss = 0.73 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-07 21:38:16.274809: step 39420, loss = 0.83 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:38:17.551761: step 39430, loss = 0.95 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:38:18.828440: step 39440, loss = 0.86 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:38:20.086641: step 39450, loss = 0.95 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:38:21.393688: step 39460, loss = 0.73 (979.5 examples/sec; 0.131 sec/batch)
2017-05-07 21:38:22.668240: step 39470, loss = 0.69 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:38:23.915114: step 39480, loss = 1.06 (1026.6 examples/sec; 0.125 sec/batch)
2017-05-07 21:38:25.210580: step 39490, loss = 1.01 (988.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:38:26.559929: step 39500, loss = 0.91 (948.6 examples/sec; 0.135 sec/batch)
2017-05-07 21:38:27.745844: step 39510, loss = 0.74 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-07 21:38:29.027007: step 39520, loss = 0.81 (999.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:38:30.287018: step 39530, loss = 1.10 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:38:31.565710: step 39540, loss = 0.84 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:38:32.843337: step 39550, loss = 0.70 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:38:34.116034: step 39560, loss = 0.69 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:38:35.407398: step 39570, loss = 0.84 (991.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:38:36.720080: step 39580, loss = 0.88 (975.1 examples/sec; 0.131 sec/batch)
2017-05-07 21:38:37.980548: step 39590, loss = 0.87 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:38:39.360160: step 39600, loss = 0.83 (927.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:38:40.537246: step 39610, loss = 0.69 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-07 21:38:41.823828: step 39620, loss = 1.01 (994.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:38:43.115898: step 39630, loss = 0.96 (990.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:38:44.392466: step 39640, loss = 0.91 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:38:45.661614: step 39650, loss = 0.86 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:38:46.931636: step 39660, loss = 0.91 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:38:48.203849: step 39670, loss = 0.74 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:38:49.483186: step 39680, loss = 0.91 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:38:50.797676: step 39690, loss = 0.81 (973.8 examples/sec; 0.131 sec/batch)
2017-05-07 21:38:52.182374: step 39700, loss = 0.79 (924.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:38:53.364428: step 39710, loss = 0.81 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-07 21:38:54.646991: step 39720, loss = 0.75 (998.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:38:55.915049: step 39730, loss = 0.78 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:38:57.197844: step 39740, loss = 0.65 (997.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:38:58.452891: step 39750, loss = 0.88 (1019.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:38:59.718439: step 39760, loss = 0.97 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:39:01.011959: step 39770, loss = 0.95 (989.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:39:02.283543: step 39780, loss = 0.56 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:39:03.543111: step 39790, loss = 0.85 (1016.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:39:04.945567: step 39800, loss = 0.98 (912.7 examples/sec; 0.140 sec/batch)
2017-05-07 21:39:06.112075: step 39810, loss = 0.78 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-07 21:39:07.364675: step 39820, loss = 0.77 (1021.9 examples/sec; 0.125 sec/batch)
2017-05-07 21:39:08.668009: step 39830, loss = 0.89 (982.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:39:09.964413: step 39840, loss = 0.96 (987.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:39:11.252784: step 39850, loss = 0.89 (993.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:39:12.537783: step 39860, loss = 0.89 (996.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:39:13.826235: step 39870, loss = 0.89 (993.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:39:15.115266: step 39880, loss = 0.82 (993.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:39:16.417276: step 39890, loss = 0.74 (983.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:39:17.804801: step 39900, loss = 0.90 (922.5 examples/sec; 0.139 sec/batch)
2017-05-07 21:39:18.964553: step 39910, loss = 0.91 (1103.7 examples/sec; 0.116 sec/batch)
2017-05-07 21:39:20.280084: step 39920, loss = 0.94 (973.0 examples/sec; 0.132 sec/batch)
2017-05-07 21:39:21.575069: step 39930, loss = 0.80 (988.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:39:22.859602: step 39940, loss = 0.85 (996.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:39:24.124877: step 39950, loss = 0.82 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:39:25.364700: step 39960, loss = 0.94 (1032.4 examples/sec; 0.124 sec/batch)
2017-05-07 21:39:26.650647: step 39970, loss = 0.83 (995.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:39:27.939165: step 39980, loss = 0.71 (993.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:39:29.233312: step 39990, loss = 0.90 (989.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:39:30.573468: step 40000, loss = 0.73 (955.1 examples/sec; 0.134 sec/batch)
2017-05-07 21:39:31.756837: step 40010, loss = 0.93 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-07 21:39:33.002724: step 40020, loss = 0.77 (1027.4 examples/sec; 0.125 sec/batch)
2017-05-07 21:39:34.280454: step 40030, loss = 1.11 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:39:35.553845: step 40040, loss = 0.88 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:39:36.825660: step 40050, loss = 0.79 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:39:38.077384: step 40060, loss = 0.82 (1022.6 examples/sec; 0.125 sec/batch)
2017-05-07 21:39:39.340727: step 40070, loss = 0.76 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:39:40.616545: step 40080, loss = 0.69 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:39:41.877047: step 40090, loss = 0.71 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:39:43.270238: step 40100, loss = 0.92 (918.8 examples/sec; 0.139 sec/batch)
2017-05-07 21:39:44.460125: step 40110, loss = 0.96 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-07 21:39:45.722935: step 40120, loss = 0.79 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:39:47.023181: step 40130, loss = 1.01 (984.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:39:48.291783: step 40140, loss = 0.74 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:39:49.562490: step 40150, loss = 0.78 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:39:50.823116: step 40160, loss = 0.81 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:39:52.097074: step 40170, loss = 0.75 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:39:53.408512: step 40180, loss = 1.04 (976.0 examples/sec; 0.131 sec/batch)
2017-05-07 21:39:54.684961: step 40190, loss = 0.80 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:39:56.068056: step 40200, loss = 0.87 (925.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:39:57.239513: step 40210, loss = 0.83 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-07 21:39:58.536736: step 40220, loss = 0.84 (986.7 examples/sec; 0.130 sec/batch)
2017-05-07 21:39:59.821539: step 40230, loss = 1.12 (996.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:40:01.109958: step 40240, loss = 0.82 (993.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:40:02.401663: step 40250, loss = 0.75 (990.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:40:03.676070: step 40260, loss = 0.77 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:40:04.981500: step 40270, loss = 0.83 (980.5 examples/sec; 0.131 sec/batch)
2017-05-07 21:40:06.225669: step 40280, loss = 0.73 (1028.8 examples/sec; 0.124 sec/batch)
2017-05-07 21:40:07.510132: step 40290, loss = 0.85 (996.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:40:08.902035: step 40300, loss = 0.74 (919.6 examples/sec; 0.139 sec/batch)
2017-05-07 21:40:10.092737: step 40310, loss = 0.97 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-07 21:40:11.367665: step 40320, loss = 0.78 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:40:12.672025: step 40330, loss = 0.72 (981.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:40:13.946118: step 40340, loss = 0.87 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:40:15.233890: step 40350, loss = 0.87 (994.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:40:16.493323: step 40360, loss = 0.72 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:40:17.771139: step 40370, loss = 0.86 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:40:19.029451: step 40380, loss = 0.84 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:40:20.300258: step 40390, loss = 0.82 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:40:21.698086: step 40400, loss = 0.79 (915.7 examples/sec; 0.140 sec/batch)
2017-05-07 21:40:22.913420: step 40410, loss = 0.86 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-07 21:40:24.207188: step 40420, loss = 0.71 (989.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:40:25.462090: step 40430, loss = 0.88 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:40:26.719468: step 40440, loss = 0.80 (1018.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:40:28.008259: step 40450, loss = 0.89 (993.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:40:29.296062: step 40460, loss = 0.98 (993.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:40:30.563739: step 40470, loss = 0.80 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:40:31.869232: step 40480, loss = 0.79 (980.5 examples/sec; 0.131 sec/batch)
2017-05-07 21:40:33.167681: step 40490, loss = 0.68 (985.8 examples/sec; 0.130 sec/batch)
2017-05-07 21:40:34.543850: step 40500, loss = 0.90 (930.1 examples/sec; 0.138 sec/batch)
2017-05-07 21:40:35.733334: step 40510, loss = 0.89 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-07 21:40:37.009580: step 40520, loss = 1.01 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:40:38.277560: step 40530, loss = 1.03 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:40:39.523819: step 40540, loss = 0.75 (1027.1 examples/sec; 0.125 sec/batch)
2017-05-07 21:40:40.821297: step 40550, loss = 0.63 (986.5 examples/sec; 0.130 sec/batch)
2017-05-07 21:40:42.104153: step 40560, loss = 0.97 (997.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:40:43.418770: step 40570, loss = 0.85 (973.7 examples/sec; 0.131 sec/batch)
2017-05-07 21:40:44.717169: step 40580, loss = 1.08 (985.8 examples/sec; 0.130 sec/batch)
2017-05-07 21:40:45.981471: step 40590, loss = 0.79 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:40:47.354079: step 40600, loss = 1.25 (932.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:40:48.525221: step 40610, loss = 1.12 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-07 21:40:49.775883: step 40620, loss = 0.79 (1023.5 examples/sec; 0.125 sec/batch)
2017-05-07 21:40:51.034707: step 40630, loss = 0.84 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:40:52.311104: step 40640, loss = 0.88 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:40:53.560381: step 40650, loss = 0.73 (1024.6 examples/sec; 0.125 sec/batch)
2017-05-07 21:40:54.823430: step 40660, loss = 0.85 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:40:56.103365: step 40670, loss = 0.80 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:40:57.370944: step 40680, loss = 0.97 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:40:58.672890: step 40690, loss = 0.81 (983.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:41:00.055736: step 40700, loss = 1.04 (925.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:41:01.226918: step 40710, loss = 0.73 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-07 21:41:02.522751: step 40720, loss = 0.60 (987.8 examples/sec; 0.130 sec/batch)
2017-05-07 21:41:03.809041: step 40730, loss = 0.93 (995.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:41:05.092217: step 40740, loss = 0.67 (997.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:41:06.364775: step 40750, loss = 0.77 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:41:07.632493: step 40760, loss = 0.69 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:41:08.894891: step 40770, loss = 0.70 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:41:10.161215: step 40780, loss = 0.77 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:41:11.437504: step 40790, loss = 0.78 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:41:12.781092: step 40800, loss = 0.86 (952.7 examples/sec; 0.134 sec/batch)
2017-05-07 21:41:13.944467: step 40810, loss = 1.11 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-07 21:41:15.199568: step 40820, loss = 0.79 (1019.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:41:16.451242: step 40830, loss = 0.83 (1022.6 examples/sec; 0.125 sec/batch)
2017-05-07 21:41:17.705140: step 40840, loss = 0.81 (1020.8 examples/sec; 0.125 sec/batch)
2017-05-07 21:41:18.974062: step 40850, loss = 0.77 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:41:20.269098: step 40860, loss = 0.94 (988.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:41:21.572199: step 40870, loss = 0.69 (982.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:41:22.826141: step 40880, loss = 0.86 (1020.8 examples/sec; 0.125 sec/batch)
2017-05-07 21:41:24.095878: step 40890, loss = 0.79 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:41:25.456116: step 40900, loss = 0.89 (941.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:41:26.611723: step 40910, loss = 0.90 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-07 21:41:27.874514: step 40920, loss = 0.88 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:41:29.129859: step 40930, loss = 0.88 (1019.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:41:30.411138: step 40940, loss = 0.80 (999.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:41:31.710487: step 40950, loss = 0.93 (985.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:41:32.980872: step 40960, loss = 0.89 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:41:34.285121: step 40970, loss = 0.77 (981.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:41:35.579162: step 40980, loss = 0.91 (989.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:41:36.873911: step 40990, loss = 0.76 (988.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:41:38.262240: step 41000, loss = 1.00 (922.0 examples/sec; 0.139 sec/batch)
2017-05-07 21:41:39.422504: step 41010, loss = 0.80 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-07 21:41:40.725473: step 41020, loss = 0.88 (982.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:41:42.015275: step 41030, loss = 1.09 (992.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:41:43.283285: step 41040, loss = 0.64 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:41:44.587407: step 41050, loss = 0.82 (981.5 examples/sec; 0.130 sec/batch)
2017-05-07 21:41:45.836043: step 41060, loss = 0.65 (1025.1 examples/sec; 0.125 sec/batch)
2017-05-07 21:41:47.094182: step 41070, loss = 0.65 (1017.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:41:48.390040: step 41080, loss = 0.78 (987.8 examples/sec; 0.130 sec/batch)
2017-05-07 21:41:49.679866: step 41090, loss = 1.28 (992.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:41:51.074886: step 41100, loss = 0.67 (917.6 examples/sec; 0.140 sec/batch)
2017-05-07 21:41:52.278020: step 41110, loss = 0.93 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-07 21:41:53.530820: step 41120, loss = 0.70 (1021.7 examples/sec; 0.125 sec/batch)
2017-05-07 21:41:54.824761: step 41130, loss = 0.78 (989.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:41:56.090955: step 41140, loss = 0.66 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:41:57.395623: step 41150, loss = 0.71 (981.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:41:58.685651: step 41160, loss = 0.70 (992.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:41:59.965911: step 41170, loss = 0.83 (999.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:42:01.270575: step 41180, loss = 0.85 (981.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:42:02.546431: step 41190, loss = 0.78 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:42:03.915270: step 41200, loss = 0.82 (935.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:42:05.084755: step 41210, loss = 0.74 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-07 21:42:06.357580: step 41220, loss = 0.81 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:42:07.659595: step 41230, loss = 0.81 (983.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:42:08.916380: step 41240, loss = 0.82 (1018.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:42:10.187805: step 41250, loss = 0.88 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:42:11.441505: step 41260, loss = 0.79 (1021.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:42:12.702785: step 41270, loss = 0.87 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:42:13.974307: step 41280, loss = 0.74 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:42:15.251538: step 41290, loss = 0.83 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:42:16.634182: step 41300, loss = 0.83 (925.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:42:17.787307: step 41310, loss = 0.74 (1110.0 examples/sec; 0.115 sec/batch)
2017-05-07 21:42:19.100869: step 41320, loss = 0.84 (974.5 examples/sec; 0.131 sec/batch)
2017-05-07 21:42:20.393588: step 41330, loss = 0.83 (990.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:42:21.646424: step 41340, loss = 0.81 (1021.7 examples/sec; 0.125 sec/batch)
2017-05-07 21:42:22.902145: step 41350, loss = 0.85 (1019.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:42:24.190112: step 41360, loss = 0.94 (993.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:42:25.454903: step 41370, loss = 0.84 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:42:26.763144: step 41380, loss = 0.97 (978.4 examples/sec; 0.131 sec/batch)
2017-05-07 21:42:28.040834: step 41390, loss = 0.74 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:42:29.432512: step 41400, loss = 0.87 (919.7 examples/sec; 0.139 sec/batch)
2017-05-07 21:42:30.634834: step 41410, loss = 0.93 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-07 21:42:31.901043: step 41420, loss = 0.86 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:42:33.174991: step 41430, loss = 1.04 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:42:34.467670: step 41440, loss = 0.84 (990.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:42:35.741918: step 41450, loss = 0.80 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:42:37.024651: step 41460, loss = 0.75 (997.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:42:38.281865: step 41470, loss = 0.78 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:42:39.551251: step 41480, loss = 0.77 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:42:40.813603: step 41490, loss = 0.66 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:42:42.149997: step 41500, loss = 0.72 (957.8 examples/sec; 0.134 sec/batch)
2017-05-07 21:42:43.333160: step 41510, loss = 0.81 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-07 21:42:44.612436: step 41520, loss = 0.89 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:42:45.855473: step 41530, loss = 0.96 (1029.7 examples/sec; 0.124 sec/batch)
2017-05-07 21:42:47.129319: step 41540, loss = 0.79 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:42:48.391327: step 41550, loss = 0.80 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:42:49.655749: step 41560, loss = 0.89 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:42:50.916623: step 41570, loss = 0.85 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:42:52.182714: step 41580, loss = 0.90 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:42:53.501919: step 41590, loss = 0.85 (970.3 examples/sec; 0.132 sec/batch)
2017-05-07 21:42:54.870658: step 41600, loss = 0.73 (935.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:42:56.073193: step 41610, loss = 0.91 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-07 21:42:57.382959: step 41620, loss = 0.76 (977.3 examples/sec; 0.131 sec/batch)
2017-05-07 21:42:58.641007: step 41630, loss = 0.80 (1017.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:42:59.923230: step 41640, loss = 0.86 (998.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:43:01.200110: step 41650, loss = 0.93 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:43:02.449573: step 41660, loss = 0.81 (1024.4 examples/sec; 0.125 sec/batch)
2017-05-07 21:43:03.724857: step 41670, loss = 0.76 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:43:04.963488: step 41680, loss = 0.53 (1033.4 examples/sec; 0.124 sec/batch)
2017-05-07 21:43:06.224819: step 41690, loss = 0.66 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:43:07.586669: step 41700, loss = 0.91 (939.9 examples/sec; 0.136 sec/batch)
2017-05-07 21:43:08.757786: step 41710, loss = 1.03 (1093.0 examples/sec; 0.117 sec/batch)
2017-05-07 21:43:10.002062: step 41720, loss = 0.77 (1028.7 examples/sec; 0.124 sec/batch)
2017-05-07 21:43:11.295724: step 41730, loss = 0.82 (989.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:43:12.623241: step 41740, loss = 0.74 (964.2 examples/sec; 0.133 sec/batch)
2017-05-07 21:43:13.891915: step 41750, loss = 0.97 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:43:15.175034: step 41760, loss = 0.90 (997.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:43:16.445028: step 41770, loss = 0.90 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:43:17.745213: step 41780, loss = 0.80 (984.5 examples/sec; 0.130 sec/batch)
2017-05-07 21:43:19.024796: step 41790, loss = 1.00 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:43:20.421632: step 41800, loss = 0.75 (916.4 examples/sec; 0.140 sec/batch)
2017-05-07 21:43:21.621469: step 41810, loss = 0.91 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-07 21:43:22.897123: step 41820, loss = 0.79 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:43:24.174085: step 41830, loss = 0.86 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:43:25.430060: step 41840, loss = 0.85 (1019.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:43:26.702557: step 41850, loss = 0.76 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:43:27.972454: step 41860, loss = 0.93 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:43:29.239912: step 41870, loss = 0.66 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:43:30.502020: step 41880, loss = 0.83 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:43:31.762094: step 41890, loss = 0.75 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:43:33.122939: step 41900, loss = 0.85 (940.6 examples/sec; 0.136 sec/batch)
2017-05-07 21:43:34.316883: step 41910, loss = 0.60 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-07 21:43:35.604490: step 41920, loss = 0.91 (994.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:43:36.905267: step 41930, loss = 0.83 (984.0 examples/sec; 0.130 sec/batch)
2017-05-07 21:43:38.200873: step 41940, loss = 0.75 (988.0 examples/sec; 0.130 sec/batch)
2017-05-07 21:43:39.465673: step 41950, loss = 0.76 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:43:40.751445: step 41960, loss = 0.99 (995.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:43:42.034346: step 41970, loss = 0.83 (997.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:43:43.304544: step 41980, loss = 1.12 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:43:44.584026: step 41990, loss = 0.81 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:43:45.915456: step 42000, loss = 0.72 (961.4 examples/sec; 0.133 sec/batch)
2017-05-07 21:43:47.112925: step 42010, loss = 0.87 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-07 21:43:48.368765: step 42020, loss = 0.81 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:43:49.620856: step 42030, loss = 0.85 (1022.3 examples/sec; 0.125 sec/batch)
2017-05-07 21:43:50.904405: step 42040, loss = 1.01 (997.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:43:52.201708: step 42050, loss = 0.98 (986.7 examples/sec; 0.130 sec/batch)
2017-05-07 21:43:53.489750: step 42060, loss = 0.87 (993.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:43:54.778215: step 42070, loss = 0.77 (993.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:43:56.065885: step 42080, loss = 0.67 (994.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:43:57.376633: step 42090, loss = 0.95 (976.5 examples/sec; 0.131 sec/batch)
2017-05-07 21:43:58.738446: step 42100, loss = 0.84 (939.9 examples/sec; 0.136 sec/batch)
2017-05-07 21:43:59.950762: step 42110, loss = 0.93 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-07 21:44:01.242770: step 42120, loss = 0.81 (990.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:44:02.539347: step 42130, loss = 1.02 (987.2 examples/sec; 0.130 sec/batch)
2017-05-07 21:44:03.792690: step 42140, loss = 0.62 (1021.3 examples/sec; 0.125 sec/batch)
2017-05-07 21:44:05.053187: step 42150, loss = 0.82 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:44:06.324251: step 42160, loss = 0.88 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:44:07.602882: step 42170, loss = 0.80 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:44:08.893733: step 42180, loss = 0.71 (991.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:44:10.164546: step 42190, loss = 0.75 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:44:11.538996: step 42200, loss = 0.84 (931.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:44:12.709635: step 42210, loss = 0.89 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-07 21:44:14.005318: step 42220, loss = 0.86 (987.9 examples/sec; 0.130 sec/batch)
2017-05-07 21:44:15.310662: step 42230, loss = 0.77 (980.6 examples/sec; 0.131 sec/batch)
2017-05-07 21:44:16.579559: step 42240, loss = 1.05 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:44:17.847071: step 42250, loss = 0.86 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:44:19.120222: step 42260, loss = 0.97 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:44:20.398647: step 42270, loss = 0.78 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:44:21.679350: step 42280, loss = 0.87 (999.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:44:22.937747: step 42290, loss = 0.68 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:44:24.293691: step 42300, loss = 0.72 (944.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:44:25.470660: step 42310, loss = 1.00 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-07 21:44:26.741077: step 42320, loss = 0.69 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:44:27.999693: step 42330, loss = 0.92 (1017.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:44:29.269298: step 42340, loss = 0.83 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:44:30.525598: step 42350, loss = 0.82 (1018.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:44:31.795247: step 42360, loss = 0.96 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:44:33.051132: step 42370, loss = 0.84 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:44:34.315344: step 42380, loss = 0.71 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:44:35.597439: step 42390, loss = 0.74 (998.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:44:36.956098: step 42400, loss = 0.84 (942.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:44:38.116762: step 42410, loss = 0.64 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-07 21:44:39.387427: step 42420, loss = 0.86 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:44:40.660168: step 42430, loss = 0.77 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:44:41.929419: step 42440, loss = 0.86 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:44:43.196587: step 42450, loss = 0.69 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:44:44.487833: step 42460, loss = 0.86 (991.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:44:45.758009: step 42470, loss = 0.73 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:44:47.042354: step 42480, loss = 0.85 (996.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:44:48.337005: step 42490, loss = 0.86 (988.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:44:49.696322: step 42500, loss = 0.85 (941.6 examples/sec; 0.136 sec/batch)
2017-05-07 21:44:50.874142: step 42510, loss = 0.73 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-07 21:44:52.155416: step 42520, loss = 0.82 (999.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:44:53.418016: step 42530, loss = 0.79 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:44:54.715729: step 42540, loss = 0.78 (986.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:44:56.012767: step 42550, loss = 0.80 (986.9 examples/sec; 0.130 sec/batch)
2017-05-07 21:44:57.307921: step 42560, loss = 0.74 (988.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:44:58.577274: step 42570, loss = 0.80 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:44:59.869569: step 42580, loss = 0.85 (990.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:45:01.170796: step 42590, loss = 0.95 (983.7 examples/sec; 0.130 sec/batch)
2017-05-07 21:45:02.563439: step 42600, loss = 0.82 (919.1 examples/sec; 0.139 sec/batch)
2017-05-07 21:45:03.734630: step 42610, loss = 0.77 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-07 21:45:05.030957: step 42620, loss = 0.77 (987.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:45:06.285890: step 42630, loss = 0.78 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:45:07.559939: step 42640, loss = 0.64 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:45:08.827795: step 42650, loss = 1.03 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:45:10.085575: step 42660, loss = 0.67 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:45:11.369112: step 42670, loss = 0.88 (997.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:45:12.664331: step 42680, loss = 0.82 (988.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:45:13.931621: step 42690, loss = 0.60 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:45:15.330248: step 42700, loss = 0.86 (915.2 examples/sec; 0.140 sec/batch)
2017-05-07 21:45:16.527178: step 42710, loss = 0.74 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-07 21:45:17.792399: step 42720, loss = 0.87 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:45:19.067622: step 42730, loss = 1.03 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:45:20.325834: step 42740, loss = 0.90 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:45:21.581694: step 42750, loss = 0.65 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:45:22.830276: step 42760, loss = 0.68 (1025.2 examples/sec; 0.125 sec/batch)
2017-05-07 21:45:24.111781: step 42770, loss = 0.83 (998.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:45:25.401273: step 42780, loss = 1.21 (992.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:45:26.683414: step 42790, loss = 0.90 (998.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:45:28.039632: step 42800, loss = 0.83 (943.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:45:29.224342: step 42810, loss = 0.94 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-07 21:45:30.487204: step 42820, loss = 0.76 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:45:31.747605: step 42830, loss = 0.91 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:45:33.013660: step 42840, loss = 0.88 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:45:34.267831: step 42850, loss = 0.69 (1020.6 examples/sec; 0.125 sec/batch)
2017-05-07 21:45:35.557333: step 42860, loss = 0.96 (992.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:45:36.849348: step 42870, loss = 0.78 (990.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:45:38.129374: step 42880, loss = 0.73 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:45:39.426756: step 42890, loss = 0.85 (986.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:45:40.811285: step 42900, loss = 0.81 (924.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:45:41.986401: step 42910, loss = 0.93 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-07 21:45:43.276085: step 42920, loss = 0.70 (992.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:45:44.540990: step 42930, loss = 0.88 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:45:45.804045: step 42940, loss = 0.99 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:45:47.085214: step 42950, loss = 0.76 (999.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:45:48.334219: step 42960, loss = 0.79 (1024.8 examples/sec; 0.125 sec/batch)
2017-05-07 21:45:49.598101: step 42970, loss = 0.75 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:45:50.860082: step 42980, loss = 1.24 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:45:52.129419: step 42990, loss = 0.76 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:45:53.484749: step 43000, loss = 0.82 (944.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:45:54.661251: step 43010, loss = 0.65 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-07 21:45:55.902816: step 43020, loss = 0.93 (1030.9 examples/sec; 0.124 sec/batch)
2017-05-07 21:45:57.169110: step 43030, loss = 0.96 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:45:58.440717: step 43040, loss = 0.83 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:45:59.737554: step 43050, loss = 0.87 (987.0 examples/sec; 0.130 sec/batch)
2017-05-07 21:46:01.000808: step 43060, loss = 0.86 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:46:02.281249: step 43070, loss = 0.64 (999.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:46:03.586031: step 43080, loss = 0.87 (981.0 examples/sec; 0.130 sec/batch)
2017-05-07 21:46:04.877328: step 43090, loss = 1.05 (991.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:46:06.212332: step 43100, loss = 0.73 (958.8 examples/sec; 0.134 sec/batch)
2017-05-07 21:46:07.390911: step 43110, loss = 0.70 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-07 21:46:08.650869: step 43120, loss = 0.79 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:46:09.929480: step 43130, loss = 0.83 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:46:11.210202: step 43140, loss = 0.77 (999.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:46:12.484589: step 43150, loss = 0.65 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:46:13.704691: step 43160, loss = 0.96 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-07 21:46:14.983470: step 43170, loss = 0.71 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:46:16.255276: step 43180, loss = 0.78 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:46:17.517437: step 43190, loss = 0.82 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:46:18.901491: step 43200, loss = 0.85 (924.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:46:20.062822: step 43210, loss = 0.78 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-07 21:46:21.326871: step 43220, loss = 0.95 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:46:22.585804: step 43230, loss = 0.86 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:46:23.862313: step 43240, loss = 0.88 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:46:25.173447: step 43250, loss = 0.82 (976.3 examples/sec; 0.131 sec/batch)
2017-05-07 21:46:26.445757: step 43260, loss = 0.97 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:46:27.739266: step 43270, loss = 0.85 (989.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:46:29.012758: step 43280, loss = 0.66 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:46:30.306598: step 43290, loss = 0.91 (989.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:46:31.689850: step 43300, loss = 0.67 (925.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:46:32.857233: step 43310, loss = 0.74 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-07 21:46:34.119039: step 43320, loss = 0.86 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:46:35.392405: step 43330, loss = 0.72 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:46:36.699951: step 43340, loss = 0.77 (978.9 examples/sec; 0.131 sec/batch)
2017-05-07 21:46:37.955939: step 43350, loss = 0.69 (1019.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:46:39.215746: step 43360, loss = 0.66 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:46:40.473854: step 43370, loss = 0.99 (1017.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:46:41.766890: step 43380, loss = 0.81 (989.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:46:43.073782: step 43390, loss = 0.88 (979.4 examples/sec; 0.131 sec/batch)
2017-05-07 21:46:44.457704: step 43400, loss = 0.76 (924.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:46:45.663054: step 43410, loss = 0.91 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-07 21:46:46.951372: step 43420, loss = 0.94 (993.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:46:48.233561: step 43430, loss = 0.82 (998.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:46:49.487587: step 43440, loss = 0.66 (1020.7 examples/sec; 0.125 sec/batch)
2017-05-07 21:46:50.788869: step 43450, loss = 0.70 (983.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:46:52.070256: step 43460, loss = 0.70 (998.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:46:53.349299: step 43470, loss = 0.73 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:46:54.596883: step 43480, loss = 0.67 (1026.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:46:55.891659: step 43490, loss = 0.98 (988.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:46:57.249356: step 43500, loss = 0.88 (942.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:46:58.439546: step 43510, loss = 0.80 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-07 21:46:59.718223: step 43520, loss = 0.82 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:47:00.981633: step 43530, loss = 0.75 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:47:02.267742: step 43540, loss = 0.71 (995.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:47:03.540079: step 43550, loss = 0.81 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:47:04.811832: step 43560, loss = 0.64 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:47:06.070502: step 43570, loss = 0.83 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:47:07.312562: step 43580, loss = 0.75 (1030.5 examples/sec; 0.124 sec/batch)
2017-05-07 21:47:08.602067: step 43590, loss = 1.02 (992.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:47:10.001817: step 43600, loss = 0.72 (914.4 examples/sec; 0.140 sec/batch)
2017-05-07 21:47:11.167487: step 43610, loss = 0.72 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-07 21:47:12.404106: step 43620, loss = 0.75 (1035.1 examples/sec; 0.124 sec/batch)
2017-05-07 21:47:13.680098: step 43630, loss = 0.87 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:47:14.933142: step 43640, loss = 0.73 (1021.5 examples/sec; 0.125 sec/batch)
2017-05-07 21:47:16.228417: step 43650, loss = 0.78 (988.2 examples/sec; 0.130 sec/batch)
2017-05-07 21:47:17.522056: step 43660, loss = 0.99 (989.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:47:18.810575: step 43670, loss = 0.81 (993.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:47:20.062571: step 43680, loss = 0.86 (1022.4 examples/sec; 0.125 sec/batch)
2017-05-07 21:47:21.316419: step 43690, loss = 0.82 (1020.9 examples/sec; 0.125 sec/batch)
2017-05-07 21:47:22.651301: step 43700, loss = 0.77 (958.9 examples/sec; 0.133 sec/batch)
2017-05-07 21:47:23.819127: step 43710, loss = 0.70 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-07 21:47:25.100826: step 43720, loss = 0.67 (998.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:47:26.364849: step 43730, loss = 0.75 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:47:27.636447: step 43740, loss = 0.98 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:47:28.912670: step 43750, loss = 0.85 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:47:30.157422: step 43760, loss = 0.90 (1028.3 examples/sec; 0.124 sec/batch)
2017-05-07 21:47:31.459477: step 43770, loss = 0.89 (983.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:47:32.742841: step 43780, loss = 0.83 (997.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:47:33.986183: step 43790, loss = 0.66 (1029.5 examples/sec; 0.124 sec/batch)
2017-05-07 21:47:35.378124: step 43800, loss = 0.72 (919.6 examples/sec; 0.139 sec/batch)
2017-05-07 21:47:36.534226: step 43810, loss = 0.84 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-07 21:47:37.803213: step 43820, loss = 0.83 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:47:39.081519: step 43830, loss = 0.79 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:47:40.330852: step 43840, loss = 0.72 (1024.5 examples/sec; 0.125 sec/batch)
2017-05-07 21:47:41.595741: step 43850, loss = 0.78 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:47:42.852837: step 43860, loss = 0.85 (1018.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:47:44.119543: step 43870, loss = 0.77 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:47:45.386802: step 43880, loss = 0.75 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:47:46.659229: step 43890, loss = 0.83 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:47:48.056091: step 43900, loss = 0.84 (916.3 examples/sec; 0.140 sec/batch)
2017-05-07 21:47:49.239090: step 43910, loss = 0.73 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-07 21:47:50.523223: step 43920, loss = 0.80 (996.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:47:51.814398: step 43930, loss = 0.77 (991.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:47:53.094049: step 43940, loss = 0.83 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:47:54.338621: step 43950, loss = 0.96 (1028.5 examples/sec; 0.124 sec/batch)
2017-05-07 21:47:55.614233: step 43960, loss = 0.72 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:47:56.903906: step 43970, loss = 0.77 (992.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:47:58.193077: step 43980, loss = 0.76 (992.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:47:59.449434: step 43990, loss = 0.96 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:48:00.812012: step 44000, loss = 1.10 (939.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:48:02.023686: step 44010, loss = 0.70 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-07 21:48:03.279032: step 44020, loss = 1.04 (1019.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:48:04.576049: step 44030, loss = 0.83 (986.9 examples/sec; 0.130 sec/batch)
2017-05-07 21:48:05.857416: step 44040, loss = 0.70 (998.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:48:07.144480: step 44050, loss = 0.73 (994.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:48:08.423134: step 44060, loss = 1.01 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:48:09.697218: step 44070, loss = 1.03 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:48:10.960105: step 44080, loss = 0.80 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:48:12.233214: step 44090, loss = 0.94 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:48:13.596249: step 44100, loss = 1.12 (939.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:48:14.789383: step 44110, loss = 0.70 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-07 21:48:16.065686: step 44120, loss = 0.73 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:48:17.373435: step 44130, loss = 0.82 (978.8 examples/sec; 0.131 sec/batch)
2017-05-07 21:48:18.649619: step 44140, loss = 0.80 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:48:19.938167: step 44150, loss = 0.78 (993.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:48:21.238697: step 44160, loss = 1.07 (984.2 examples/sec; 0.130 sec/batch)
2017-05-07 21:48:22.548780: step 44170, loss = 0.86 (977.0 examples/sec; 0.131 sec/batch)
2017-05-07 21:48:23.851546: step 44180, loss = 0.94 (982.5 examples/sec; 0.130 sec/batch)
2017-05-07 21:48:25.141690: step 44190, loss = 0.81 (992.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:48:26.513191: step 44200, loss = 0.84 (933.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:48:27.704052: step 44210, loss = 0.90 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-07 21:48:28.971401: step 44220, loss = 0.80 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:48:30.242217: step 44230, loss = 0.94 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:48:31.524604: step 44240, loss = 1.13 (998.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:48:32.808142: step 44250, loss = 0.88 (997.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:48:34.054771: step 44260, loss = 0.74 (1026.8 examples/sec; 0.125 sec/batch)
2017-05-07 21:48:35.332012: step 44270, loss = 0.82 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:48:36.605056: step 44280, loss = 0.90 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:48:37.828202: step 44290, loss = 0.70 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-07 21:48:39.177939: step 44300, loss = 0.61 (948.3 examples/sec; 0.135 sec/batch)
2017-05-07 21:48:40.367694: step 44310, loss = 0.74 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-07 21:48:41.611582: step 44320, loss = 0.75 (1029.0 examples/sec; 0.124 sec/batch)
2017-05-07 21:48:42.879182: step 44330, loss = 0.65 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:48:44.163793: step 44340, loss = 0.79 (996.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:48:45.418034: step 44350, loss = 0.92 (1020.5 examples/sec; 0.125 sec/batch)
2017-05-07 21:48:46.688268: step 44360, loss = 0.96 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:48:47.989427: step 44370, loss = 0.71 (983.7 examples/sec; 0.130 sec/batch)
2017-05-07 21:48:49.271449: step 44380, loss = 0.68 (998.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:48:50.551650: step 44390, loss = 0.89 (999.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:48:51.931639: step 44400, loss = 0.71 (927.5 examples/sec; 0.138 sec/batch)
2017-05-07 21:48:53.135017: step 44410, loss = 0.92 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-07 21:48:54.406895: step 44420, loss = 0.75 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:48:55.685201: step 44430, loss = 0.88 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:48:56.957247: step 44440, loss = 0.96 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:48:58.233440: step 44450, loss = 0.82 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:48:59.503533: step 44460, loss = 0.69 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:49:00.780032: step 44470, loss = 0.88 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:49:02.044561: step 44480, loss = 0.80 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:49:03.325112: step 44490, loss = 0.89 (999.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:49:04.711660: step 44500, loss = 0.90 (923.1 examples/sec; 0.139 sec/batch)
2017-05-07 21:49:05.884078: step 44510, loss = 0.90 (1091.8 examples/sec; 0.117 sec/batch)
2017-05-07 21:49:07.163406: step 44520, loss = 0.81 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:49:08.471257: step 44530, loss = 0.82 (978.7 examples/sec; 0.131 sec/batch)
2017-05-07 21:49:09.766857: step 44540, loss = 0.71 (988.0 examples/sec; 0.130 sec/batch)
2017-05-07 21:49:11.044831: step 44550, loss = 0.93 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:49:12.351209: step 44560, loss = 0.82 (979.8 examples/sec; 0.131 sec/batch)
2017-05-07 21:49:13.622558: step 44570, loss = 0.72 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:49:14.887422: step 44580, loss = 0.82 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:49:16.152215: step 44590, loss = 0.83 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:49:17.511020: step 44600, loss = 0.97 (942.0 examples/sec; 0.136 sec/batch)
2017-05-07 21:49:18.726369: step 44610, loss = 0.74 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-07 21:49:20.033378: step 44620, loss = 0.97 (979.3 examples/sec; 0.131 sec/batch)
2017-05-07 21:49:21.301622: step 44630, loss = 0.76 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:49:22.565450: step 44640, loss = 1.00 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:49:23.849553: step 44650, loss = 0.83 (996.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:49:25.112200: step 44660, loss = 0.67 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:49:26.381607: step 44670, loss = 0.74 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:49:27.645544: step 44680, loss = 0.95 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:49:28.955786: step 44690, loss = 0.81 (976.9 examples/sec; 0.131 sec/batch)
2017-05-07 21:49:30.319650: step 44700, loss = 0.69 (938.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:49:31.500510: step 44710, loss = 0.77 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-07 21:49:32.759331: step 44720, loss = 0.87 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:49:34.012133: step 44730, loss = 0.69 (1021.7 examples/sec; 0.125 sec/batch)
2017-05-07 21:49:35.275517: step 44740, loss = 0.76 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:49:36.570066: step 44750, loss = 0.94 (988.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:49:37.869643: step 44760, loss = 0.72 (984.9 examples/sec; 0.130 sec/batch)
2017-05-07 21:49:39.154917: step 44770, loss = 0.76 (995.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:49:40.414487: step 44780, loss = 0.86 (1016.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:49:41.677308: step 44790, loss = 0.89 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:49:43.041635: step 44800, loss = 0.97 (938.2 examples/sec; 0.136 sec/batch)
2017-05-07 21:49:44.235648: step 44810, loss = 0.69 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-07 21:49:45.502592: step 44820, loss = 0.63 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:49:46.774724: step 44830, loss = 0.92 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:49:48.078722: step 44840, loss = 0.76 (981.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:49:49.354142: step 44850, loss = 0.81 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:49:50.656179: step 44860, loss = 0.83 (983.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:49:51.948406: step 44870, loss = 0.81 (990.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:49:53.225658: step 44880, loss = 0.99 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:49:54.496159: step 44890, loss = 0.82 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:49:55.863618: step 44900, loss = 0.88 (936.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:49:57.054149: step 44910, loss = 0.73 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-07 21:49:58.340946: step 44920, loss = 0.89 (994.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:49:59.620284: step 44930, loss = 0.85 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:50:00.886065: step 44940, loss = 0.85 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:50:02.189363: step 44950, loss = 0.76 (982.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:50:03.470391: step 44960, loss = 0.71 (999.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:50:04.770628: step 44970, loss = 0.78 (984.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:50:06.031229: step 44980, loss = 0.75 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:50:07.294849: step 44990, loss = 0.92 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:50:08.657184: step 45000, loss = 0.62 (939.6 examples/sec; 0.136 sec/batch)
2017-05-07 21:50:09.823765: step 45010, loss = 0.81 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-07 21:50:11.094256: step 45020, loss = 0.66 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:50:12.364110: step 45030, loss = 0.78 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:50:13.636614: step 45040, loss = 0.89 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:50:14.918415: step 45050, loss = 0.76 (998.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:50:16.184831: step 45060, loss = 0.71 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:50:17.491067: step 45070, loss = 0.81 (979.9 examples/sec; 0.131 sec/batch)
2017-05-07 21:50:18.774212: step 45080, loss = 0.69 (997.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:50:20.082339: step 45090, loss = 0.73 (978.5 examples/sec; 0.131 sec/batch)
2017-05-07 21:50:21.457184: step 45100, loss = 0.81 (931.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:50:22.640871: step 45110, loss = 0.78 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-07 21:50:23.906571: step 45120, loss = 0.89 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:50:25.179286: step 45130, loss = 0.65 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:50:26.459152: step 45140, loss = 0.83 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:50:27.778057: step 45150, loss = 0.90 (970.5 examples/sec; 0.132 sec/batch)
2017-05-07 21:50:29.048866: step 45160, loss = 0.83 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:50:30.332218: step 45170, loss = 0.86 (997.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:50:31.603378: step 45180, loss = 0.67 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:50:32.858705: step 45190, loss = 0.75 (1019.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:50:34.222650: step 45200, loss = 0.65 (938.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:50:35.427994: step 45210, loss = 0.75 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-07 21:50:36.718711: step 45220, loss = 0.88 (991.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:50:38.025131: step 45230, loss = 0.79 (979.8 examples/sec; 0.131 sec/batch)
2017-05-07 21:50:39.308428: step 45240, loss = 0.80 (997.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:50:40.613342: step 45250, loss = 0.71 (980.9 examples/sec; 0.130 sec/batch)
2017-05-07 21:50:41.866194: step 45260, loss = 0.68 (1021.7 examples/sec; 0.125 sec/batch)
2017-05-07 21:50:43.131056: step 45270, loss = 0.98 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:50:44.389850: step 45280, loss = 0.78 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:50:45.651533: step 45290, loss = 0.87 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:50:47.045403: step 45300, loss = 0.83 (918.3 examples/sec; 0.139 sec/batch)
2017-05-07 21:50:48.235674: step 45310, loss = 0.98 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-07 21:50:49.483270: step 45320, loss = 1.14 (1026.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:50:50.781815: step 45330, loss = 0.88 (985.7 examples/sec; 0.130 sec/batch)
2017-05-07 21:50:52.074491: step 45340, loss = 0.65 (990.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:50:53.365820: step 45350, loss = 0.78 (991.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:50:54.637407: step 45360, loss = 0.95 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:50:55.942439: step 45370, loss = 0.81 (980.8 examples/sec; 0.131 sec/batch)
2017-05-07 21:50:57.220995: step 45380, loss = 0.93 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:50:58.468714: step 45390, loss = 0.83 (1025.8 examples/sec; 0.125 sec/batch)
2017-05-07 21:50:59.825167: step 45400, loss = 0.78 (943.6 examples/sec; 0.136 sec/batch)
2017-05-07 21:51:01.008849: step 45410, loss = 0.89 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-07 21:51:02.282827: step 45420, loss = 0.78 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:51:03.565289: step 45430, loss = 0.86 (998.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:51:04.868457: step 45440, loss = 0.86 (982.2 examples/sec; 0.130 sec/batch)
2017-05-07 21:51:06.139912: step 45450, loss = 0.75 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:51:07.420794: step 45460, loss = 0.76 (999.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:51:08.705140: step 45470, loss = 0.79 (996.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:51:09.984933: step 45480, loss = 0.68 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:51:11.282230: step 45490, loss = 0.98 (986.7 examples/sec; 0.130 sec/batch)
2017-05-07 21:51:12.635010: step 45500, loss = 0.77 (946.2 examples/sec; 0.135 sec/batch)
2017-05-07 21:51:13.799112: step 45510, loss = 0.74 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-07 21:51:15.063006: step 45520, loss = 0.82 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:51:16.313186: step 45530, loss = 0.93 (1023.8 examples/sec; 0.125 sec/batch)
2017-05-07 21:51:17.582959: step 45540, loss = 0.73 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:51:18.864644: step 45550, loss = 0.88 (998.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:51:20.167452: step 45560, loss = 0.84 (982.3 examples/sec; 0.130 sec/batch)
2017-05-07 21:51:21.470438: step 45570, loss = 0.88 (982.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:51:22.753496: step 45580, loss = 0.87 (997.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:51:23.995030: step 45590, loss = 0.85 (1031.0 examples/sec; 0.124 sec/batch)
2017-05-07 21:51:25.361819: step 45600, loss = 0.58 (936.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:51:26.532666: step 45610, loss = 0.73 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-07 21:51:27.809198: step 45620, loss = 0.74 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:51:29.062148: step 45630, loss = 0.89 (1021.6 examples/sec; 0.125 sec/batch)
2017-05-07 21:51:30.313932: step 45640, loss = 0.80 (1022.5 examples/sec; 0.125 sec/batch)
2017-05-07 21:51:31.581503: step 45650, loss = 0.70 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:51:32.851355: step 45660, loss = 0.77 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:51:34.121683: step 45670, loss = 0.83 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:51:35.407762: step 45680, loss = 1.04 (995.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:51:36.657503: step 45690, loss = 0.68 (1024.2 examples/sec; 0.125 sec/batch)
2017-05-07 21:51:38.002929: step 45700, loss = 0.82 (951.4 examples/sec; 0.135 sec/batch)
2017-05-07 21:51:39.181958: step 45710, loss = 0.93 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-07 21:51:40.442636: step 45720, loss = 0.65 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:51:41.693670: step 45730, loss = 0.86 (1023.2 examples/sec; 0.125 sec/batch)
2017-05-07 21:51:42.956093: step 45740, loss = 0.87 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:51:44.229080: step 45750, loss = 0.81 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:51:45.495133: step 45760, loss = 0.69 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:51:46.761314: step 45770, loss = 0.86 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:51:48.019411: step 45780, loss = 0.93 (1017.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:51:49.302908: step 45790, loss = 0.65 (997.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:51:50.665073: step 45800, loss = 0.77 (939.7 examples/sec; 0.136 sec/batch)
2017-05-07 21:51:51.838078: step 45810, loss = 0.73 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-07 21:51:53.102325: step 45820, loss = 0.72 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:51:54.380883: step 45830, loss = 0.84 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:51:55.672273: step 45840, loss = 0.90 (991.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:51:56.964871: step 45850, loss = 0.95 (990.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:51:58.244237: step 45860, loss = 0.87 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:51:59.521820: step 45870, loss = 0.81 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:52:00.793070: step 45880, loss = 0.83 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:52:02.062151: step 45890, loss = 0.87 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:52:03.437542: step 45900, loss = 0.79 (930.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:52:04.609727: step 45910, loss = 0.66 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-07 21:52:05.871727: step 45920, loss = 0.77 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:52:07.136076: step 45930, loss = 0.81 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:52:08.412129: step 45940, loss = 0.76 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:52:09.698188: step 45950, loss = 0.77 (995.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:52:10.977343: step 45960, loss = 0.94 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:52:12.278854: step 45970, loss = 0.67 (983.5 examples/sec; 0.130 sec/batch)
2017-05-07 21:52:13.534677: step 45980, loss = 0.76 (1019.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:52:14.834657: step 45990, loss = 1.11 (984.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:52:16.221714: step 46000, loss = 0.78 (922.8 examples/sec; 0.139 sec/batch)
2017-05-07 21:52:17.414991: step 46010, loss = 0.67 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-07 21:52:18.701657: step 46020, loss = 0.81 (994.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:52:19.966750: step 46030, loss = 0.77 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:52:21.225745: step 46040, loss = 0.75 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:52:22.479278: step 46050, loss = 0.71 (1021.1 examples/sec; 0.125 sec/batch)
2017-05-07 21:52:23.743744: step 46060, loss = 0.77 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:52:25.021018: step 46070, loss = 0.86 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:52:26.280544: step 46080, loss = 0.77 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:52:27.527676: step 46090, loss = 0.97 (1026.4 examples/sec; 0.125 sec/batch)
2017-05-07 21:52:28.922080: step 46100, loss = 0.87 (918.0 examples/sec; 0.139 sec/batch)
2017-05-07 21:52:30.085201: step 46110, loss = 0.82 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-07 21:52:31.370317: step 46120, loss = 0.85 (996.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:52:32.674164: step 46130, loss = 0.94 (981.7 examples/sec; 0.130 sec/batch)
2017-05-07 21:52:33.955741: step 46140, loss = 0.80 (998.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:52:35.249318: step 46150, loss = 0.69 (989.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:52:36.545675: step 46160, loss = 0.81 (987.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:52:37.841533: step 46170, loss = 0.79 (987.8 examples/sec; 0.130 sec/batch)
2017-05-07 21:52:39.102061: step 46180, loss = 0.90 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:52:40.389541: step 46190, loss = 0.73 (994.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:52:41.740850: step 46200, loss = 0.98 (947.2 examples/sec; 0.135 sec/batch)
2017-05-07 21:52:42.918230: step 46210, loss = 0.71 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-07 21:52:44.245326: step 46220, loss = 0.85 (964.5 examples/sec; 0.133 sec/batch)
2017-05-07 21:52:45.522914: step 46230, loss = 0.98 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:52:46.781902: step 46240, loss = 0.78 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:52:48.049047: step 46250, loss = 0.91 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:52:49.302814: step 46260, loss = 0.60 (1020.9 examples/sec; 0.125 sec/batch)
2017-05-07 21:52:50.551746: step 46270, loss = 0.68 (1024.9 examples/sec; 0.125 sec/batch)
2017-05-07 21:52:51.808540: step 46280, loss = 1.06 (1018.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:52:53.078027: step 46290, loss = 0.94 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:52:54.428562: step 46300, loss = 0.92 (947.8 examples/sec; 0.135 sec/batch)
2017-05-07 21:52:55.613622: step 46310, loss = 0.70 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-07 21:52:56.866162: step 46320, loss = 0.75 (1021.9 examples/sec; 0.125 sec/batch)
2017-05-07 21:52:58.133505: step 46330, loss = 0.71 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:52:59.391629: step 46340, loss = 0.76 (1017.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:53:00.687411: step 46350, loss = 0.76 (987.8 examples/sec; 0.130 sec/batch)
2017-05-07 21:53:01.963491: step 46360, loss = 0.79 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:53:03.347531: step 46370, loss = 0.79 (924.8 examples/sec; 0.138 sec/batch)
2017-05-07 21:53:04.763797: step 46380, loss = 0.88 (903.8 examples/sec; 0.142 sec/batch)
2017-05-07 21:53:06.689179: step 46390, loss = 0.89 (664.8 examples/sec; 0.193 sec/batch)
2017-05-07 21:53:08.661504: step 46400, loss = 0.70 (649.0 examples/sec; 0.197 sec/batch)
2017-05-07 21:53:10.474669: step 46410, loss = 0.82 (705.9 examples/sec; 0.181 sec/batch)
2017-05-07 21:53:12.365818: step 46420, loss = 0.80 (676.8 examples/sec; 0.189 sec/batch)
2017-05-07 21:53:14.265612: step 46430, loss = 0.66 (673.8 examples/sec; 0.190 sec/batch)
2017-05-07 21:53:16.023015: step 46440, loss = 1.04 (728.3 examples/sec; 0.176 sec/batch)
2017-05-07 21:53:17.311179: step 46450, loss = 0.67 (993.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:53:18.592158: step 46460, loss = 0.70 (999.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:53:19.914423: step 46470, loss = 0.66 (968.0 examples/sec; 0.132 sec/batch)
2017-05-07 21:53:21.189645: step 46480, loss = 0.95 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:53:22.437462: step 46490, loss = 0.80 (1025.8 examples/sec; 0.125 sec/batch)
2017-05-07 21:53:23.792243: step 46500, loss = 0.69 (944.8 examples/sec; 0.135 sec/batch)
2017-05-07 21:53:24.990061: step 46510, loss = 0.82 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-07 21:53:26.266503: step 46520, loss = 0.82 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:53:27.570558: step 46530, loss = 0.92 (981.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:53:28.832682: step 46540, loss = 0.80 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:53:30.095056: step 46550, loss = 0.71 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:53:31.385446: step 46560, loss = 0.71 (991.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:53:32.676823: step 46570, loss = 0.78 (991.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:53:33.932145: step 46580, loss = 0.74 (1019.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:53:35.225915: step 46590, loss = 0.84 (989.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:53:36.607707: step 46600, loss = 0.78 (926.3 examples/sec; 0.138 sec/batch)
2017-05-07 21:53:37.798849: step 46610, loss = 0.63 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-07 21:53:39.065809: step 46620, loss = 0.83 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:53:40.340223: step 46630, loss = 0.86 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:53:41.594350: step 46640, loss = 0.83 (1020.6 examples/sec; 0.125 sec/batch)
2017-05-07 21:53:42.854927: step 46650, loss = 0.78 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:53:44.120739: step 46660, loss = 0.70 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:53:45.389269: step 46670, loss = 0.88 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:53:46.652623: step 46680, loss = 0.86 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:53:47.925335: step 46690, loss = 0.82 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:53:49.265854: step 46700, loss = 0.86 (954.9 examples/sec; 0.134 sec/batch)
2017-05-07 21:53:50.431034: step 46710, loss = 0.85 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-07 21:53:51.716399: step 46720, loss = 0.88 (995.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:53:53.025471: step 46730, loss = 0.96 (977.8 examples/sec; 0.131 sec/batch)
2017-05-07 21:53:54.286242: step 46740, loss = 0.96 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:53:55.596320: step 46750, loss = 0.86 (977.0 examples/sec; 0.131 sec/batch)
2017-05-07 21:53:56.879361: step 46760, loss = 0.90 (997.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:53:58.159184: step 46770, loss = 0.85 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:53:59.443309: step 46780, loss = 0.73 (996.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:54:00.714004: step 46790, loss = 0.71 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:54:02.060732: step 46800, loss = 0.74 (950.4 examples/sec; 0.135 sec/batch)
2017-05-07 21:54:03.226826: step 46810, loss = 0.83 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-07 21:54:04.486154: step 46820, loss = 0.77 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:54:05.739351: step 46830, loss = 0.73 (1021.4 examples/sec; 0.125 sec/batch)
2017-05-07 21:54:07.010047: step 46840, loss = 0.82 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:54:08.284363: step 46850, loss = 0.84 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:54:09.559557: step 46860, loss = 1.09 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:54:10.811009: step 46870, loss = 0.82 (1022.8 examples/sec; 0.125 sec/batch)
2017-05-07 21:54:12.110183: step 46880, loss = 0.70 (985.2 examples/sec; 0.130 sec/batch)
2017-05-07 21:54:13.367331: step 46890, loss = 0.82 (1018.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:54:14.748077: step 46900, loss = 0.72 (927.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:54:15.914337: step 46910, loss = 0.97 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-07 21:54:17.163773: step 46920, loss = 0.77 (1024.5 examples/sec; 0.125 sec/batch)
2017-05-07 21:54:18.402636: step 46930, loss = 0.99 (1033.2 examples/sec; 0.124 sec/batch)
2017-05-07 21:54:19.680887: step 46940, loss = 0.71 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:54:20.965724: step 46950, loss = 0.80 (996.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:54:22.228801: step 46960, loss = 0.76 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:54:23.517931: step 46970, loss = 1.04 (992.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:54:24.799390: step 46980, loss = 0.83 (998.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:54:26.054693: step 46990, loss = 0.78 (1019.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:54:27.416229: step 47000, loss = 0.85 (940.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:54:28.596147: step 47010, loss = 0.68 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-07 21:54:29.846064: step 47020, loss = 0.77 (1024.1 examples/sec; 0.125 sec/batch)
2017-05-07 21:54:31.112743: step 47030, loss = 0.78 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:54:32.395301: step 47040, loss = 0.93 (998.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:54:33.662281: step 47050, loss = 0.78 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:54:34.924298: step 47060, loss = 0.82 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:54:36.185127: step 47070, loss = 0.87 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:54:37.437612: step 47080, loss = 0.70 (1022.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:54:38.713161: step 47090, loss = 0.85 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:54:40.072346: step 47100, loss = 0.73 (941.7 examples/sec; 0.136 sec/batch)
2017-05-07 21:54:41.242309: step 47110, loss = 0.80 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-07 21:54:42.483691: step 47120, loss = 0.83 (1031.1 examples/sec; 0.124 sec/batch)
2017-05-07 21:54:43.742164: step 47130, loss = 0.64 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:54:45.013406: step 47140, loss = 0.73 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:54:46.292055: step 47150, loss = 0.98 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:54:47.548699: step 47160, loss = 0.75 (1018.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:54:48.805347: step 47170, loss = 0.71 (1018.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:54:50.071293: step 47180, loss = 0.78 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:54:51.343352: step 47190, loss = 0.83 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-07 21:54:52.704377: step 47200, loss = 0.88 (940.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:54:53.881991: step 47210, loss = 0.78 (1087.0 examples/sec; 0.118 sec/batch)
2017-05-07 21:54:55.127728: step 47220, loss = 0.70 (1027.5 examples/sec; 0.125 sec/batch)
2017-05-07 21:54:56.392553: step 47230, loss = 0.71 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:54:57.651351: step 47240, loss = 0.84 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:54:58.923756: step 47250, loss = 0.81 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:55:00.179199: step 47260, loss = 0.71 (1019.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:55:01.449371: step 47270, loss = 0.83 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:55:02.761765: step 47280, loss = 0.84 (975.3 examples/sec; 0.131 sec/batch)
2017-05-07 21:55:04.035553: step 47290, loss = 0.70 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:55:05.410254: step 47300, loss = 0.76 (931.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:55:06.586336: step 47310, loss = 1.03 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-07 21:55:07.837484: step 47320, loss = 0.75 (1023.1 examples/sec; 0.125 sec/batch)
2017-05-07 21:55:09.108041: step 47330, loss = 0.84 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:55:10.363738: step 47340, loss = 0.88 (1019.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:55:11.656696: step 47350, loss = 1.05 (990.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:55:12.970030: step 47360, loss = 0.78 (974.6 examples/sec; 0.131 sec/batch)
2017-05-07 21:55:14.256024: step 47370, loss = 0.92 (995.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:55:15.546787: step 47380, loss = 0.82 (991.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:55:16.814563: step 47390, loss = 0.80 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:55:18.155662: step 47400, loss = 0.72 (954.4 examples/sec; 0.134 sec/batch)
2017-05-07 21:55:19.342757: step 47410, loss = 0.85 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-07 21:55:20.593329: step 47420, loss = 0.80 (1023.5 examples/sec; 0.125 sec/batch)
2017-05-07 21:55:21.877615: step 47430, loss = 1.01 (996.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:55:23.165531: step 47440, loss = 0.90 (993.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:55:24.445414: step 47450, loss = 0.81 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:55:25.734739: step 47460, loss = 0.80 (992.8 examples/sec; 0.129 sec/batch)
2017-05-07 21:55:27.003718: step 47470, loss = 0.81 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:55:28.267406: step 47480, loss = 0.93 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:55:29.514094: step 47490, loss = 0.85 (1026.7 examples/sec; 0.125 sec/batch)
2017-05-07 21:55:30.884512: step 47500, loss = 0.73 (934.0 examples/sec; 0.137 sec/batch)
2017-05-07 21:55:32.130400: step 47510, loss = 0.78 (1027.4 examples/sec; 0.125 sec/batch)
2017-05-07 21:55:33.364365: step 47520, loss = 0.77 (1037.3 examples/sec; 0.123 sec/batch)
2017-05-07 21:55:34.668504: step 47530, loss = 0.92 (981.5 examples/sec; 0.130 sec/batch)
2017-05-07 21:55:35.953817: step 47540, loss = 0.67 (995.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:55:37.219798: step 47550, loss = 0.86 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-07 21:55:38.490382: step 47560, loss = 0.71 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:55:39.771434: step 47570, loss = 0.85 (999.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:55:41.035882: step 47580, loss = 0.71 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:55:42.292557: step 47590, loss = 0.77 (1018.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:55:43.637470: step 47600, loss = 0.63 (951.7 examples/sec; 0.134 sec/batch)
2017-05-07 21:55:44.814214: step 47610, loss = 0.73 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-07 21:55:46.077496: step 47620, loss = 0.83 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:55:47.347691: step 47630, loss = 0.73 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:55:48.623129: step 47640, loss = 0.78 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:55:49.878848: step 47650, loss = 0.89 (1019.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:55:51.157029: step 47660, loss = 0.71 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:55:52.421538: step 47670, loss = 0.64 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:55:53.700321: step 47680, loss = 0.66 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:55:54.959772: step 47690, loss = 0.70 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:55:56.317815: step 47700, loss = 0.72 (942.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:55:57.498806: step 47710, loss = 0.76 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-07 21:55:58.815939: step 47720, loss = 1.02 (971.8 examples/sec; 0.132 sec/batch)
2017-05-07 21:56:00.105580: step 47730, loss = 0.73 (992.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:56:01.362010: step 47740, loss = 0.81 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:56:02.622252: step 47750, loss = 0.73 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:56:03.905585: step 47760, loss = 0.78 (997.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:56:05.190139: step 47770, loss = 0.73 (996.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:56:06.450296: step 47780, loss = 0.92 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:56:07.727635: step 47790, loss = 0.70 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:56:09.069922: step 47800, loss = 0.71 (953.6 examples/sec; 0.134 sec/batch)
2017-05-07 21:56:10.234733: step 47810, loss = 0.82 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-07 21:56:11.533521: step 47820, loss = 0.59 (985.5 examples/sec; 0.130 sec/batch)
2017-05-07 21:56:12.817470: step 47830, loss = 0.84 (996.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:56:14.074822: step 47840, loss = 0.72 (1018.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:56:15.374934: step 47850, loss = 0.80 (984.5 examples/sec; 0.130 sec/batch)
2017-05-07 21:56:16.661423: step 47860, loss = 0.84 (995.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:56:17.929058: step 47870, loss = 0.91 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:56:19.198556: step 47880, loss = 0.73 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:56:20.473555: step 47890, loss = 0.77 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:56:21.818121: step 47900, loss = 0.92 (952.0 examples/sec; 0.134 sec/batch)
2017-05-07 21:56:22.991001: step 47910, loss = 0.92 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-07 21:56:24.243871: step 47920, loss = 0.67 (1021.7 examples/sec; 0.125 sec/batch)
2017-05-07 21:56:25.499906: step 47930, loss = 0.88 (1019.1 examples/sec; 0.126 sec/batch)
2017-05-07 21:56:26.778601: step 47940, loss = 0.85 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:56:28.065225: step 47950, loss = 0.79 (994.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:56:29.347659: step 47960, loss = 0.79 (998.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:56:30.618430: step 47970, loss = 0.82 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:56:31.857888: step 47980, loss = 0.94 (1032.7 examples/sec; 0.124 sec/batch)
2017-05-07 21:56:33.125179: step 47990, loss = 0.89 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:56:34.474275: step 48000, loss = 0.71 (948.8 examples/sec; 0.135 sec/batch)
2017-05-07 21:56:35.654215: step 48010, loss = 0.75 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-07 21:56:36.908197: step 48020, loss = 0.72 (1020.7 examples/sec; 0.125 sec/batch)
2017-05-07 21:56:38.190019: step 48030, loss = 0.75 (998.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:56:39.487198: step 48040, loss = 0.77 (986.8 examples/sec; 0.130 sec/batch)
2017-05-07 21:56:40.784745: step 48050, loss = 0.82 (986.5 examples/sec; 0.130 sec/batch)
2017-05-07 21:56:42.065781: step 48060, loss = 0.97 (999.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:56:43.345945: step 48070, loss = 0.86 (999.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:56:44.623307: step 48080, loss = 0.87 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:56:45.879245: step 48090, loss = 0.88 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:56:47.270750: step 48100, loss = 0.81 (919.9 examples/sec; 0.139 sec/batch)
2017-05-07 21:56:48.447011: step 48110, loss = 0.84 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-07 21:56:49.721937: step 48120, loss = 0.76 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:56:51.008538: step 48130, loss = 0.86 (994.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:56:52.290074: step 48140, loss = 0.74 (998.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:56:53.544502: step 48150, loss = 0.68 (1020.4 examples/sec; 0.125 sec/batch)
2017-05-07 21:56:54.801356: step 48160, loss = 0.77 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-07 21:56:56.087369: step 48170, loss = 0.67 (995.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:56:57.359810: step 48180, loss = 0.94 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:56:58.653855: step 48190, loss = 0.88 (989.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:57:00.022639: step 48200, loss = 0.86 (935.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:57:01.205191: step 48210, loss = 0.77 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-07 21:57:02.478302: step 48220, loss = 0.75 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-07 21:57:03.784060: step 48230, loss = 0.85 (980.3 examples/sec; 0.131 sec/batch)
2017-05-07 21:57:05.068877: step 48240, loss = 0.79 (996.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:57:06.331149: step 48250, loss = 0.92 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:57:07.607839: step 48260, loss = 0.77 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:57:08.919103: step 48270, loss = 0.77 (976.2 examples/sec; 0.131 sec/batch)
2017-05-07 21:57:10.205244: step 48280, loss = 0.81 (995.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:57:11.497200: step 48290, loss = 0.83 (990.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:57:12.870726: step 48300, loss = 0.90 (931.9 examples/sec; 0.137 sec/batch)
2017-05-07 21:57:14.034404: step 48310, loss = 0.90 (1100.0 examples/sec; 0.116 sec/batch)
2017-05-07 21:57:15.319440: step 48320, loss = 1.07 (996.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:57:16.619723: step 48330, loss = 0.90 (984.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:57:17.901765: step 48340, loss = 0.82 (998.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:57:19.180059: step 48350, loss = 0.79 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:57:20.457843: step 48360, loss = 0.67 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:57:21.705429: step 48370, loss = 0.94 (1026.0 examples/sec; 0.125 sec/batch)
2017-05-07 21:57:23.003019: step 48380, loss = 0.70 (986.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:57:24.296567: step 48390, loss = 0.70 (989.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:57:25.667625: step 48400, loss = 0.87 (933.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:57:26.877236: step 48410, loss = 0.86 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-07 21:57:28.149601: step 48420, loss = 0.84 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:57:29.432129: step 48430, loss = 0.76 (998.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:57:30.690438: step 48440, loss = 0.63 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:57:31.967878: step 48450, loss = 0.81 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:57:33.240551: step 48460, loss = 0.88 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:57:34.525351: step 48470, loss = 0.85 (996.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:57:35.799543: step 48480, loss = 0.70 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:57:37.099757: step 48490, loss = 0.94 (984.5 examples/sec; 0.130 sec/batch)
2017-05-07 21:57:38.452061: step 48500, loss = 0.87 (946.5 examples/sec; 0.135 sec/batch)
2017-05-07 21:57:39.648956: step 48510, loss = 0.88 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-07 21:57:40.930774: step 48520, loss = 0.94 (998.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:57:42.232173: step 48530, loss = 0.74 (983.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:57:43.526817: step 48540, loss = 0.84 (988.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:57:44.803037: step 48550, loss = 0.79 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:57:46.081536: step 48560, loss = 0.84 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:57:47.391939: step 48570, loss = 0.78 (976.8 examples/sec; 0.131 sec/batch)
2017-05-07 21:57:48.675053: step 48580, loss = 0.86 (997.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:57:49.930799: step 48590, loss = 0.71 (1019.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:57:51.318743: step 48600, loss = 0.82 (922.2 examples/sec; 0.139 sec/batch)
2017-05-07 21:57:52.527631: step 48610, loss = 0.66 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-07 21:57:53.783852: step 48620, loss = 0.87 (1018.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:57:55.078844: step 48630, loss = 0.72 (988.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:57:56.360894: step 48640, loss = 0.80 (998.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:57:57.639800: step 48650, loss = 0.78 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:57:58.931300: step 48660, loss = 0.78 (991.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:58:00.224054: step 48670, loss = 0.72 (990.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:58:01.511616: step 48680, loss = 0.71 (994.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:58:02.847768: step 48690, loss = 0.97 (958.0 examples/sec; 0.134 sec/batch)
2017-05-07 21:58:04.249772: step 48700, loss = 0.74 (913.0 examples/sec; 0.140 sec/batch)
2017-05-07 21:58:05.425144: step 48710, loss = 0.77 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-07 21:58:06.715068: step 48720, loss = 0.98 (992.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:58:07.974068: step 48730, loss = 0.96 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:58:09.275297: step 48740, loss = 0.80 (983.7 examples/sec; 0.130 sec/batch)
2017-05-07 21:58:10.536909: step 48750, loss = 0.91 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-07 21:58:11.819122: step 48760, loss = 0.76 (998.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:58:13.119302: step 48770, loss = 0.97 (984.5 examples/sec; 0.130 sec/batch)
2017-05-07 21:58:14.385759: step 48780, loss = 0.87 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:58:15.657536: step 48790, loss = 0.94 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-07 21:58:17.007233: step 48800, loss = 0.94 (948.4 examples/sec; 0.135 sec/batch)
2017-05-07 21:58:18.199655: step 48810, loss = 0.80 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-07 21:58:19.506693: step 48820, loss = 0.86 (979.3 examples/sec; 0.131 sec/batch)
2017-05-07 21:58:20.752424: step 48830, loss = 0.70 (1027.5 examples/sec; 0.125 sec/batch)
2017-05-07 21:58:22.058094: step 48840, loss = 0.82 (980.3 examples/sec; 0.131 sec/batch)
2017-05-07 21:58:23.369637: step 48850, loss = 0.83 (975.9 examples/sec; 0.131 sec/batch)
2017-05-07 21:58:24.654085: step 48860, loss = 0.75 (996.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:58:25.950094: step 48870, loss = 0.98 (987.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:58:27.222745: step 48880, loss = 0.84 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:58:28.528168: step 48890, loss = 0.84 (980.5 examples/sec; 0.131 sec/batch)
2017-05-07 21:58:29.897412: step 48900, loss = 0.68 (934.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:58:31.106581: step 48910, loss = 0.75 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-07 21:58:32.387991: step 48920, loss = 0.79 (998.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:58:33.653334: step 48930, loss = 0.72 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:58:34.936923: step 48940, loss = 0.77 (997.2 examples/sec; 0.128 sec/batch)
2017-05-07 21:58:36.207159: step 48950, loss = 0.88 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:58:37.526020: step 48960, loss = 0.77 (970.5 examples/sec; 0.132 sec/batch)
2017-05-07 21:58:38.809765: step 48970, loss = 0.80 (997.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:58:40.080940: step 48980, loss = 0.84 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:58:41.353351: step 48990, loss = 0.90 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:58:42.755119: step 49000, loss = 0.75 (913.1 examples/sec; 0.140 sec/batch)
2017-05-07 21:58:43.936131: step 49010, loss = 0.85 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-07 21:58:45.250112: step 49020, loss = 0.72 (974.1 examples/sec; 0.131 sec/batch)
2017-05-07 21:58:46.539000: step 49030, loss = 0.74 (993.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:58:47.792712: step 49040, loss = 1.00 (1020.9 examples/sec; 0.125 sec/batch)
2017-05-07 21:58:49.098014: step 49050, loss = 0.68 (980.6 examples/sec; 0.131 sec/batch)
2017-05-07 21:58:50.376158: step 49060, loss = 0.65 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-07 21:58:51.653855: step 49070, loss = 0.76 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:58:52.916532: step 49080, loss = 0.85 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-07 21:58:54.179761: step 49090, loss = 0.90 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-07 21:58:55.550766: step 49100, loss = 0.66 (933.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:58:56.698500: step 49110, loss = 0.80 (1115.2 examples/sec; 0.115 sec/batch)
2017-05-07 21:58:57.967137: step 49120, loss = 0.78 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-07 21:58:59.224671: step 49130, loss = 0.76 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-07 21:59:00.506583: step 49140, loss = 0.78 (998.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:59:01.774128: step 49150, loss = 0.68 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:59:03.028319: step 49160, loss = 0.67 (1020.6 examples/sec; 0.125 sec/batch)
2017-05-07 21:59:04.278506: step 49170, loss = 0.76 (1023.8 examples/sec; 0.125 sec/batch)
2017-05-07 21:59:05.541497: step 49180, loss = 0.69 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-07 21:59:06.825593: step 49190, loss = 0.78 (996.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:59:08.210630: step 49200, loss = 0.74 (924.2 examples/sec; 0.139 sec/batch)
2017-05-07 21:59:09.413432: step 49210, loss = 0.82 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-07 21:59:10.672333: step 49220, loss = 0.80 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:59:11.971508: step 49230, loss = 0.63 (985.2 examples/sec; 0.130 sec/batch)
2017-05-07 21:59:13.261558: step 49240, loss = 0.67 (992.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:59:14.538163: step 49250, loss = 0.89 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-07 21:59:15.829484: step 49260, loss = 0.74 (991.2 examples/sec; 0.129 sec/batch)
2017-05-07 21:59:17.137375: step 49270, loss = 0.97 (978.7 examples/sec; 0.131 sec/batch)
2017-05-07 21:59:18.415740: step 49280, loss = 0.81 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:59:19.673053: step 49290, loss = 0.70 (1018.0 examples/sec; 0.126 sec/batch)
2017-05-07 21:59:21.029839: step 49300, loss = 0.94 (943.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:59:22.223272: step 49310, loss = 0.89 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-07 21:59:23.505153: step 49320, loss = 1.06 (998.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:59:24.789270: step 49330, loss = 0.88 (996.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:59:26.087364: step 49340, loss = 0.73 (986.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:59:27.385935: step 49350, loss = 0.86 (985.7 examples/sec; 0.130 sec/batch)
2017-05-07 21:59:28.692874: step 49360, loss = 0.74 (979.4 examples/sec; 0.131 sec/batch)
2017-05-07 21:59:29.957388: step 49370, loss = 0.75 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-07 21:59:31.225122: step 49380, loss = 0.80 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-07 21:59:32.514511: step 49390, loss = 0.96 (992.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:59:33.891463: step 49400, loss = 0.85 (929.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:59:35.061174: step 49410, loss = 0.69 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-07 21:59:36.352145: step 49420, loss = 0.76 (991.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:59:37.645265: step 49430, loss = 0.81 (989.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:59:38.938550: step 49440, loss = 0.82 (989.7 examples/sec; 0.129 sec/batch)
2017-05-07 21:59:40.253247: step 49450, loss = 1.02 (973.6 examples/sec; 0.131 sec/batch)
2017-05-07 21:59:41.542250: step 49460, loss = 0.92 (993.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:59:42.814910: step 49470, loss = 0.66 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:59:44.091952: step 49480, loss = 0.73 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:59:45.398526: step 49490, loss = 0.70 (979.7 examples/sec; 0.131 sec/batch)
2017-05-07 21:59:46.768181: step 49500, loss = 0.85 (934.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:59:47.966757: step 49510, loss = 0.80 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-07 21:59:49.248155: step 49520, loss = 0.90 (998.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:59:50.524956: step 49530, loss = 0.82 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-07 21:59:51.833442: step 49540, loss = 0.96 (978.2 examples/sec; 0.131 sec/batch)
2017-05-07 21:59:53.116986: step 49550, loss = 1.00 (997.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:59:54.388609: step 49560, loss = 0.70 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:59:55.674571: step 49570, loss = 0.83 (995.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:59:56.953683: step 49580, loss = 0.78 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:59:58.253866: step 49590, loss = 0.93 (984.5 examples/sec; 0.130 sec/batch)
2017-05-07 21:59:59.626310: step 49600, loss = 0.75 (932.6 examples/sec; 0.137 sec/batch)
2017-05-07 22:00:00.809872: step 49610, loss = 1.01 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-07 22:00:02.090860: step 49620, loss = 0.78 (999.2 examples/sec; 0.128 sec/batch)
2017-05-07 22:00:03.365171: step 49630, loss = 0.56 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-07 22:00:04.632723: step 49640, loss = 0.73 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-07 22:00:05.907637: step 49650, loss = 0.85 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-07 22:00:07.157257: step 49660, loss = 0.79 (1024.3 examples/sec; 0.125 sec/batch)
2017-05-07 22:00:08.443113: step 49670, loss = 0.69 (995.5 examples/sec; 0.129 sec/batch)
2017-05-07 22:00:09.744717: step 49680, loss = 0.74 (983.4 examples/sec; 0.130 sec/batch)
2017-05-07 22:00:11.030341: step 49690, loss = 0.89 (995.6 examples/sec; 0.129 sec/batch)
2017-05-07 22:00:12.388409: step 49700, loss = 0.77 (942.5 examples/sec; 0.136 sec/batch)
2017-05-07 22:00:13.593071: step 49710, loss = 0.83 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-07 22:00:14.868379: step 49720, loss = 0.78 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-07 22:00:16.132864: step 49730, loss = 0.67 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-07 22:00:17.386670: step 49740, loss = 0.66 (1020.9 examples/sec; 0.125 sec/batch)
2017-05-07 22:00:18.639457: step 49750, loss = 0.78 (1021.7 examples/sec; 0.125 sec/batch)
2017-05-07 22:00:19.912931: step 49760, loss = 0.74 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-07 22:00:21.170508: step 49770, loss = 0.64 (1017.8 examples/sec; 0.126 sec/batch)
2017-05-07 22:00:22.449266: step 49780, loss = 0.67 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-07 22:00:23.717915: step 49790, loss = 0.85 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-07 22:00:25.085324: step 49800, loss = 0.80 (936.1 examples/sec; 0.137 sec/batch)
2017-05-07 22:00:26.265244: step 49810, loss = 0.86 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-07 22:00:27.539484: step 49820, loss = 0.88 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-07 22:00:28.828109: step 49830, loss = 0.86 (993.3 examples/sec; 0.129 sec/batch)
2017-05-07 22:00:30.077508: step 49840, loss = 0.90 (1024.5 examples/sec; 0.125 sec/batch)
2017-05-07 22:00:31.339143: step 49850, loss = 0.89 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-07 22:00:32.645249: step 49860, loss = 0.75 (980.0 examples/sec; 0.131 sec/batch)
2017-05-07 22:00:33.910099: step 49870, loss = 0.70 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-07 22:00:35.183105: step 49880, loss = 0.82 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-07 22:00:36.446487: step 49890, loss = 0.90 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-07 22:00:37.844242: step 49900, loss = 0.77 (915.8 examples/sec; 0.140 sec/batch)
2017-05-07 22:00:39.000464: step 49910, loss = 0.77 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-07 22:00:40.248102: step 49920, loss = 0.84 (1025.9 examples/sec; 0.125 sec/batch)
2017-05-07 22:00:41.249651: step 49930, loss = 0.57 (1278.0 examples/sec; 0.100 sec/batch)
2017-05-07 22:00:42.079563: step 49940, loss = 0.73 (1542.3 examples/sec; 0.083 sec/batch)
2017-05-07 22:00:42.914584: step 49950, loss = 0.78 (1532.9 examples/sec; 0.084 sec/batch)
2017-05-07 22:00:43.738394: step 49960, loss = 0.71 (1553.8 examples/sec; 0.082 sec/batch)
2017-05-07 22:00:44.561513: step 49970, loss = 0.80 (1555.1 examples/sec; 0.082 sec/batch)
2017-05-07 22:00:45.379569: step 49980, loss = 0.81 (1564.7 examples/sec; 0.082 sec/batch)
2017-05-07 22:00:46.207616: step 49990, loss = 0.79 (1545.8 examples/sec; 0.083 sec/batch)
2017-05-07 22:00:47.140273: step 50000, loss = 0.61 (1372.4 examples/sec; 0.093 sec/batch)
--- 6501.76996398 seconds ---
