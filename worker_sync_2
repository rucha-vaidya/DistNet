I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.7335
pciBusID 0000:04:00.0
Total memory: 7.92GiB
Free memory: 2.77GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x3066de0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: 
name: Quadro K620
major: 5 minor: 0 memoryClockRate (GHz) 1.124
pciBusID 0000:03:00.0
Total memory: 1.95GiB
Free memory: 1.83GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:04:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:962] Ignoring gpu device (device: 1, name: Quadro K620, pci bus id: 0000:03:00.0) with Cuda multiprocessor count: 3. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.
Connecting to port  16002
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Received variable values from ps
2017-05-07 19:15:55.174570: step 0, loss = 4.67 (79.2 examples/sec; 1.616 sec/batch)
2017-05-07 19:15:56.351328: step 10, loss = 4.56 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-07 19:15:57.668291: step 20, loss = 4.53 (971.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:15:59.011496: step 30, loss = 4.42 (952.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:16:00.370321: step 40, loss = 4.34 (942.0 examples/sec; 0.136 sec/batch)
2017-05-07 19:16:01.779104: step 50, loss = 4.24 (908.6 examples/sec; 0.141 sec/batch)
2017-05-07 19:16:03.078207: step 60, loss = 4.54 (985.3 examples/sec; 0.130 sec/batch)
2017-05-07 19:16:04.424025: step 70, loss = 4.12 (951.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:16:05.774937: step 80, loss = 3.97 (947.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:16:07.115186: step 90, loss = 4.23 (955.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:16:08.601227: step 100, loss = 3.79 (861.3 examples/sec; 0.149 sec/batch)
2017-05-07 19:16:09.782346: step 110, loss = 3.84 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-07 19:16:11.092781: step 120, loss = 3.82 (976.8 examples/sec; 0.131 sec/batch)
2017-05-07 19:16:12.407360: step 130, loss = 3.72 (973.7 examples/sec; 0.131 sec/batch)
2017-05-07 19:16:13.748897: step 140, loss = 3.64 (954.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:16:15.119244: step 150, loss = 3.81 (934.1 examples/sec; 0.137 sec/batch)
2017-05-07 19:16:16.445327: step 160, loss = 3.73 (965.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:16:17.770314: step 170, loss = 3.56 (966.0 examples/sec; 0.132 sec/batch)
2017-05-07 19:16:19.126059: step 180, loss = 3.73 (944.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:16:20.473153: step 190, loss = 3.64 (950.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:16:21.954843: step 200, loss = 3.54 (863.9 examples/sec; 0.148 sec/batch)
2017-05-07 19:16:23.182415: step 210, loss = 3.43 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-07 19:16:24.519952: step 220, loss = 3.53 (957.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:16:25.859853: step 230, loss = 3.68 (955.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:16:27.177912: step 240, loss = 3.49 (971.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:16:28.569885: step 250, loss = 3.20 (919.6 examples/sec; 0.139 sec/batch)
2017-05-07 19:16:29.888226: step 260, loss = 3.27 (970.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:16:31.231727: step 270, loss = 3.26 (952.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:16:32.584852: step 280, loss = 3.17 (946.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:16:33.886966: step 290, loss = 3.13 (983.0 examples/sec; 0.130 sec/batch)
2017-05-07 19:16:35.392348: step 300, loss = 3.20 (850.3 examples/sec; 0.151 sec/batch)
2017-05-07 19:16:36.578291: step 310, loss = 3.31 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-07 19:16:37.929259: step 320, loss = 2.97 (947.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:16:39.286416: step 330, loss = 2.97 (943.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:16:40.625980: step 340, loss = 2.93 (955.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:16:42.031010: step 350, loss = 2.91 (911.0 examples/sec; 0.141 sec/batch)
2017-05-07 19:16:43.342957: step 360, loss = 2.99 (975.7 examples/sec; 0.131 sec/batch)
2017-05-07 19:16:44.672697: step 370, loss = 2.90 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:16:46.026422: step 380, loss = 2.98 (945.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:16:47.372996: step 390, loss = 2.79 (950.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:16:48.863353: step 400, loss = 3.52 (858.9 examples/sec; 0.149 sec/batch)
2017-05-07 19:16:50.079312: step 410, loss = 2.75 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-07 19:16:51.409119: step 420, loss = 2.95 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:16:52.744700: step 430, loss = 2.91 (958.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:16:54.069404: step 440, loss = 2.71 (966.3 examples/sec; 0.132 sec/batch)
2017-05-07 19:16:55.459544: step 450, loss = 2.65 (920.8 examples/sec; 0.139 sec/batch)
2017-05-07 19:16:56.753235: step 460, loss = 2.69 (989.4 examples/sec; 0.129 sec/batch)
2017-05-07 19:16:58.111416: step 470, loss = 2.61 (942.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:16:59.440791: step 480, loss = 2.73 (962.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:17:00.774187: step 490, loss = 2.85 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:17:02.277123: step 500, loss = 2.56 (851.7 examples/sec; 0.150 sec/batch)
2017-05-07 19:17:03.477202: step 510, loss = 2.61 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-07 19:17:04.838258: step 520, loss = 2.42 (940.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:17:06.186831: step 530, loss = 2.49 (949.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:17:07.532058: step 540, loss = 2.47 (951.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:17:08.893778: step 550, loss = 2.38 (940.0 examples/sec; 0.136 sec/batch)
2017-05-07 19:17:10.228956: step 560, loss = 2.61 (958.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:17:11.559926: step 570, loss = 2.60 (961.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:17:12.915166: step 580, loss = 2.38 (944.5 examples/sec; 0.136 sec/batch)
2017-05-07 19:17:14.252211: step 590, loss = 2.42 (957.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:17:15.761055: step 600, loss = 2.25 (848.3 examples/sec; 0.151 sec/batch)
2017-05-07 19:17:16.914821: step 610, loss = 2.35 (1109.4 examples/sec; 0.115 sec/batch)
2017-05-07 19:17:18.271325: step 620, loss = 2.30 (943.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:17:19.616161: step 630, loss = 2.21 (951.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:17:20.957222: step 640, loss = 2.42 (954.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:17:22.358178: step 650, loss = 2.12 (913.7 examples/sec; 0.140 sec/batch)
2017-05-07 19:17:23.672879: step 660, loss = 2.28 (973.6 examples/sec; 0.131 sec/batch)
2017-05-07 19:17:25.027712: step 670, loss = 2.32 (944.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:17:26.364278: step 680, loss = 2.15 (957.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:17:27.696372: step 690, loss = 2.27 (960.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:17:29.182839: step 700, loss = 2.33 (861.1 examples/sec; 0.149 sec/batch)
2017-05-07 19:17:30.405792: step 710, loss = 2.08 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-07 19:17:31.725530: step 720, loss = 2.02 (969.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:17:33.061336: step 730, loss = 2.02 (958.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:17:34.411494: step 740, loss = 2.64 (948.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:17:35.801026: step 750, loss = 2.21 (921.2 examples/sec; 0.139 sec/batch)
2017-05-07 19:17:37.088207: step 760, loss = 2.09 (994.4 examples/sec; 0.129 sec/batch)
2017-05-07 19:17:38.421512: step 770, loss = 2.04 (960.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:17:39.779482: step 780, loss = 1.96 (942.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:17:41.109080: step 790, loss = 2.20 (962.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:17:42.597629: step 800, loss = 2.09 (859.9 examples/sec; 0.149 sec/batch)
2017-05-07 19:17:43.818393: step 810, loss = 2.00 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-07 19:17:45.177180: step 820, loss = 1.96 (942.0 examples/sec; 0.136 sec/batch)
2017-05-07 19:17:46.521894: step 830, loss = 2.25 (951.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:17:47.879500: step 840, loss = 1.90 (942.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:17:49.226676: step 850, loss = 1.99 (950.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:17:50.537782: step 860, loss = 1.95 (976.3 examples/sec; 0.131 sec/batch)
2017-05-07 19:17:51.890176: step 870, loss = 1.95 (946.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:17:53.250710: step 880, loss = 1.85 (940.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:17:54.593783: step 890, loss = 1.95 (953.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:17:56.153701: step 900, loss = 1.80 (820.5 examples/sec; 0.156 sec/batch)
2017-05-07 19:17:57.278928: step 910, loss = 2.00 (1137.6 examples/sec; 0.113 sec/batch)
2017-05-07 19:17:58.627160: step 920, loss = 1.92 (949.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:17:59.964320: step 930, loss = 2.17 (957.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:18:01.312356: step 940, loss = 2.01 (949.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:18:02.681880: step 950, loss = 1.75 (934.6 examples/sec; 0.137 sec/batch)
2017-05-07 19:18:03.972094: step 960, loss = 1.85 (992.1 examples/sec; 0.129 sec/batch)
2017-05-07 19:18:05.305115: step 970, loss = 1.79 (960.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:18:06.650976: step 980, loss = 1.71 (951.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:18:08.010399: step 990, loss = 1.89 (941.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:18:09.510884: step 1000, loss = 1.67 (853.1 examples/sec; 0.150 sec/batch)
2017-05-07 19:18:10.724417: step 1010, loss = 1.77 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-07 19:18:12.075346: step 1020, loss = 1.61 (947.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:18:13.418307: step 1030, loss = 1.62 (953.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:18:14.745894: step 1040, loss = 1.70 (964.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:18:16.146617: step 1050, loss = 1.54 (913.8 examples/sec; 0.140 sec/batch)
2017-05-07 19:18:17.442555: step 1060, loss = 1.70 (987.7 examples/sec; 0.130 sec/batch)
2017-05-07 19:18:18.799718: step 1070, loss = 1.88 (943.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:18:20.156174: step 1080, loss = 1.59 (943.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:18:21.473753: step 1090, loss = 1.76 (971.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:18:22.965276: step 1100, loss = 1.94 (858.2 examples/sec; 0.149 sec/batch)
2017-05-07 19:18:24.202743: step 1110, loss = 1.89 (1034.4 examples/sec; 0.124 sec/batch)
2017-05-07 19:18:25.539352: step 1120, loss = 1.72 (957.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:18:26.889703: step 1130, loss = 1.50 (947.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:18:28.208216: step 1140, loss = 1.59 (970.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:18:29.610789: step 1150, loss = 1.62 (912.6 examples/sec; 0.140 sec/batch)
2017-05-07 19:18:30.910262: step 1160, loss = 1.39 (985.0 examples/sec; 0.130 sec/batch)
2017-05-07 19:18:32.251334: step 1170, loss = 1.57 (954.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:18:33.606153: step 1180, loss = 1.72 (944.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:18:34.959159: step 1190, loss = 1.74 (946.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:18:36.443748: step 1200, loss = 1.83 (862.2 examples/sec; 0.148 sec/batch)
2017-05-07 19:18:37.683841: step 1210, loss = 1.66 (1032.2 examples/sec; 0.124 sec/batch)
2017-05-07 19:18:39.024526: step 1220, loss = 1.56 (954.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:18:40.383666: step 1230, loss = 1.59 (941.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:18:41.715079: step 1240, loss = 1.38 (961.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:18:43.130593: step 1250, loss = 1.57 (904.3 examples/sec; 0.142 sec/batch)
2017-05-07 19:18:44.450814: step 1260, loss = 1.44 (969.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:18:45.781683: step 1270, loss = 1.62 (961.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:18:47.144016: step 1280, loss = 1.34 (939.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:18:48.484859: step 1290, loss = 1.48 (954.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:18:49.990061: step 1300, loss = 1.45 (850.4 examples/sec; 0.151 sec/batch)
2017-05-07 19:18:51.213693: step 1310, loss = 1.60 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-07 19:18:52.564410: step 1320, loss = 1.60 (947.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:18:53.902269: step 1330, loss = 1.48 (956.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:18:55.248646: step 1340, loss = 1.36 (950.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:18:56.630693: step 1350, loss = 1.39 (926.2 examples/sec; 0.138 sec/batch)
2017-05-07 19:18:57.927950: step 1360, loss = 1.52 (986.7 examples/sec; 0.130 sec/batch)
2017-05-07 19:18:59.292239: step 1370, loss = 1.42 (938.2 examples/sec; 0.136 sec/batch)
2017-05-07 19:19:00.634712: step 1380, loss = 1.52 (953.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:19:01.967650: step 1390, loss = 1.48 (960.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:19:03.446337: step 1400, loss = 1.48 (865.6 examples/sec; 0.148 sec/batch)
2017-05-07 19:19:04.658332: step 1410, loss = 1.52 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-07 19:19:06.012504: step 1420, loss = 1.47 (945.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:19:07.371702: step 1430, loss = 1.58 (941.7 examples/sec; 0.136 sec/batch)
2017-05-07 19:19:08.686751: step 1440, loss = 1.56 (973.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:19:10.099517: step 1450, loss = 1.51 (906.0 examples/sec; 0.141 sec/batch)
2017-05-07 19:19:11.396565: step 1460, loss = 1.39 (986.9 examples/sec; 0.130 sec/batch)
2017-05-07 19:19:12.741984: step 1470, loss = 1.38 (951.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:19:14.082350: step 1480, loss = 1.53 (955.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:19:15.452257: step 1490, loss = 1.31 (934.4 examples/sec; 0.137 sec/batch)
2017-05-07 19:19:16.940768: step 1500, loss = 1.54 (859.9 examples/sec; 0.149 sec/batch)
2017-05-07 19:19:18.115919: step 1510, loss = 1.51 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-07 19:19:19.461979: step 1520, loss = 1.41 (950.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:19:20.805566: step 1530, loss = 1.26 (952.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:19:22.156119: step 1540, loss = 1.24 (947.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:19:23.539728: step 1550, loss = 1.37 (925.1 examples/sec; 0.138 sec/batch)
2017-05-07 19:19:24.850313: step 1560, loss = 1.37 (976.7 examples/sec; 0.131 sec/batch)
2017-05-07 19:19:26.187748: step 1570, loss = 1.40 (957.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:19:27.521650: step 1580, loss = 1.32 (959.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:19:28.859919: step 1590, loss = 1.41 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:19:30.340557: step 1600, loss = 1.09 (864.5 examples/sec; 0.148 sec/batch)
2017-05-07 19:19:31.558518: step 1610, loss = 1.33 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-07 19:19:32.902859: step 1620, loss = 1.24 (952.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:19:34.229898: step 1630, loss = 1.42 (964.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:19:35.585906: step 1640, loss = 1.27 (943.9 examples/sec; 0.136 sec/batch)
2017-05-07 19:19:36.958868: step 1650, loss = 1.47 (932.3 examples/sec; 0.137 sec/batch)
2017-05-07 19:19:38.290734: step 1660, loss = 1.25 (961.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:19:39.674578: step 1670, loss = 1.23 (925.0 examples/sec; 0.138 sec/batch)
2017-05-07 19:19:41.011072: step 1680, loss = 1.34 (957.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:19:42.364233: step 1690, loss = 1.47 (945.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:19:43.842614: step 1700, loss = 1.32 (865.8 examples/sec; 0.148 sec/batch)
2017-05-07 19:19:45.064623: step 1710, loss = 1.18 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-07 19:19:46.403768: step 1720, loss = 1.12 (955.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:19:47.751368: step 1730, loss = 1.49 (949.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:19:49.092130: step 1740, loss = 1.52 (954.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:19:50.468597: step 1750, loss = 1.24 (929.9 examples/sec; 0.138 sec/batch)
2017-05-07 19:19:51.779320: step 1760, loss = 1.19 (976.6 examples/sec; 0.131 sec/batch)
2017-05-07 19:19:53.122863: step 1770, loss = 1.23 (952.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:19:54.448977: step 1780, loss = 1.41 (965.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:19:55.822094: step 1790, loss = 1.45 (932.2 examples/sec; 0.137 sec/batch)
2017-05-07 19:19:57.303180: step 1800, loss = 1.33 (864.2 examples/sec; 0.148 sec/batch)
2017-05-07 19:19:58.521100: step 1810, loss = 1.04 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-07 19:19:59.874221: step 1820, loss = 1.63 (946.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:20:01.201345: step 1830, loss = 1.35 (964.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:20:02.536623: step 1840, loss = 1.16 (958.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:20:03.950726: step 1850, loss = 1.25 (905.2 examples/sec; 0.141 sec/batch)
2017-05-07 19:20:05.260128: step 1860, loss = 1.23 (977.5 examples/sec; 0.131 sec/batch)
2017-05-07 19:20:06.590818: step 1870, loss = 1.22 (961.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:20:07.941604: step 1880, loss = 1.28 (947.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:20:09.287186: step 1890, loss = 1.15 (951.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:20:10.773128: step 1900, loss = 1.21 (861.4 examples/sec; 0.149 sec/batch)
2017-05-07 19:20:11.953995: step 1910, loss = 1.14 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-07 19:20:13.337741: step 1920, loss = 1.06 (925.0 examples/sec; 0.138 sec/batch)
2017-05-07 19:20:14.696275: step 1930, loss = 1.27 (942.2 examples/sec; 0.136 sec/batch)
2017-05-07 19:20:16.058483: step 1940, loss = 1.33 (939.7 examples/sec; 0.136 sec/batch)
2017-05-07 19:20:17.456356: step 1950, loss = 1.37 (915.7 examples/sec; 0.140 sec/batch)
2017-05-07 19:20:18.772340: step 1960, loss = 0.98 (972.7 examples/sec; 0.132 sec/batch)
2017-05-07 19:20:20.111512: step 1970, loss = 1.27 (955.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:20:21.445170: step 1980, loss = 1.40 (959.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:20:22.805068: step 1990, loss = 1.10 (941.2 examples/sec; 0.136 sec/batch)
2017-05-07 19:20:24.304523: step 2000, loss = 1.22 (853.6 examples/sec; 0.150 sec/batch)
2017-05-07 19:20:25.527124: step 2010, loss = 1.12 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-07 19:20:26.895118: step 2020, loss = 1.03 (935.7 examples/sec; 0.137 sec/batch)
2017-05-07 19:20:28.304420: step 2030, loss = 1.13 (908.2 examples/sec; 0.141 sec/batch)
2017-05-07 19:20:29.677345: step 2040, loss = 1.38 (932.3 examples/sec; 0.137 sec/batch)
2017-05-07 19:20:31.100914: step 2050, loss = 1.14 (899.1 examples/sec; 0.142 sec/batch)
2017-05-07 19:20:32.480235: step 2060, loss = 1.28 (928.0 examples/sec; 0.138 sec/batch)
2017-05-07 19:20:33.813634: step 2070, loss = 1.02 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:20:35.242087: step 2080, loss = 1.40 (896.1 examples/sec; 0.143 sec/batch)
2017-05-07 19:20:36.664056: step 2090, loss = 1.10 (900.1 examples/sec; 0.142 sec/batch)
2017-05-07 19:20:38.137393: step 2100, loss = 1.26 (868.8 examples/sec; 0.147 sec/batch)
2017-05-07 19:20:39.375547: step 2110, loss = 1.11 (1033.8 examples/sec; 0.124 sec/batch)
2017-05-07 19:20:40.744623: step 2120, loss = 1.05 (934.9 examples/sec; 0.137 sec/batch)
2017-05-07 19:20:42.154928: step 2130, loss = 1.34 (907.6 examples/sec; 0.141 sec/batch)
2017-05-07 19:20:43.493739: step 2140, loss = 1.15 (956.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:20:44.890990: step 2150, loss = 1.00 (916.1 examples/sec; 0.140 sec/batch)
2017-05-07 19:20:46.216708: step 2160, loss = 1.13 (965.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:20:47.584558: step 2170, loss = 1.07 (935.8 examples/sec; 0.137 sec/batch)
2017-05-07 19:20:48.909322: step 2180, loss = 1.12 (966.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:20:50.261304: step 2190, loss = 1.32 (946.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:20:51.758885: step 2200, loss = 0.99 (854.7 examples/sec; 0.150 sec/batch)
2017-05-07 19:20:52.957998: step 2210, loss = 1.24 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-07 19:20:54.299581: step 2220, loss = 1.40 (954.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:20:55.633530: step 2230, loss = 1.04 (959.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:20:56.969127: step 2240, loss = 0.98 (958.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:20:58.390735: step 2250, loss = 1.16 (900.4 examples/sec; 0.142 sec/batch)
2017-05-07 19:20:59.721846: step 2260, loss = 1.02 (961.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:21:01.047180: step 2270, loss = 1.35 (965.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:21:02.400211: step 2280, loss = 1.17 (946.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:21:03.737992: step 2290, loss = 0.90 (956.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:21:05.238388: step 2300, loss = 1.16 (853.1 examples/sec; 0.150 sec/batch)
2017-05-07 19:21:06.474621: step 2310, loss = 1.20 (1035.4 examples/sec; 0.124 sec/batch)
2017-05-07 19:21:07.835083: step 2320, loss = 1.05 (940.9 examples/sec; 0.136 sec/batch)
2017-05-07 19:21:09.182823: step 2330, loss = 1.08 (949.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:21:10.539734: step 2340, loss = 1.16 (943.3 examples/sec; 0.136 sec/batch)
2017-05-07 19:21:11.963496: step 2350, loss = 1.21 (899.1 examples/sec; 0.142 sec/batch)
2017-05-07 19:21:13.295632: step 2360, loss = 1.11 (960.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:21:14.659702: step 2370, loss = 0.99 (938.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:21:16.024598: step 2380, loss = 1.15 (937.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:21:17.368774: step 2390, loss = 1.07 (952.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:21:18.828326: step 2400, loss = 1.09 (877.0 examples/sec; 0.146 sec/batch)
2017-05-07 19:21:20.041119: step 2410, loss = 1.07 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-07 19:21:21.369165: step 2420, loss = 0.91 (963.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:21:22.708578: step 2430, loss = 1.15 (955.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:21:24.070619: step 2440, loss = 1.35 (939.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:21:25.510556: step 2450, loss = 0.90 (888.9 examples/sec; 0.144 sec/batch)
2017-05-07 19:21:26.838905: step 2460, loss = 1.26 (963.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:21:28.188876: step 2470, loss = 1.23 (948.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:21:29.540183: step 2480, loss = 1.14 (947.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:21:30.862158: step 2490, loss = 1.10 (968.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:21:32.377519: step 2500, loss = 1.04 (844.7 examples/sec; 0.152 sec/batch)
2017-05-07 19:21:33.602121: step 2510, loss = 1.16 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-07 19:21:34.951233: step 2520, loss = 1.24 (948.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:21:36.290857: step 2530, loss = 1.35 (955.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:21:37.654561: step 2540, loss = 1.14 (938.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:21:39.066400: step 2550, loss = 1.10 (906.6 examples/sec; 0.141 sec/batch)
2017-05-07 19:21:40.410298: step 2560, loss = 1.01 (952.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:21:41.782889: step 2570, loss = 1.18 (932.5 examples/sec; 0.137 sec/batch)
2017-05-07 19:21:43.155812: step 2580, loss = 1.00 (932.3 examples/sec; 0.137 sec/batch)
2017-05-07 19:21:44.514374: step 2590, loss = 1.20 (942.2 examples/sec; 0.136 sec/batch)
2017-05-07 19:21:46.003440: step 2600, loss = 1.23 (859.6 examples/sec; 0.149 sec/batch)
2017-05-07 19:21:47.217940: step 2610, loss = 0.98 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-07 19:21:48.564164: step 2620, loss = 1.22 (950.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:21:49.892775: step 2630, loss = 1.05 (963.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:21:51.221913: step 2640, loss = 1.10 (963.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:21:52.598908: step 2650, loss = 1.10 (929.5 examples/sec; 0.138 sec/batch)
2017-05-07 19:21:53.922520: step 2660, loss = 1.08 (967.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:21:55.281092: step 2670, loss = 0.96 (942.2 examples/sec; 0.136 sec/batch)
2017-05-07 19:21:56.610028: step 2680, loss = 0.92 (963.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:21:57.976016: step 2690, loss = 1.21 (937.0 examples/sec; 0.137 sec/batch)
2017-05-07 19:21:59.459761: step 2700, loss = 1.29 (862.7 examples/sec; 0.148 sec/batch)
2017-05-07 19:22:00.662191: step 2710, loss = 1.11 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-07 19:22:02.028380: step 2720, loss = 1.10 (936.9 examples/sec; 0.137 sec/batch)
2017-05-07 19:22:03.376898: step 2730, loss = 1.12 (949.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:22:04.727936: step 2740, loss = 1.18 (947.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:22:06.092660: step 2750, loss = 1.18 (937.9 examples/sec; 0.136 sec/batch)
2017-05-07 19:22:07.388767: step 2760, loss = 1.16 (987.6 examples/sec; 0.130 sec/batch)
2017-05-07 19:22:08.713654: step 2770, loss = 1.04 (966.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:22:10.073695: step 2780, loss = 1.00 (941.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:22:11.425728: step 2790, loss = 1.25 (946.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:22:12.909436: step 2800, loss = 1.13 (862.7 examples/sec; 0.148 sec/batch)
2017-05-07 19:22:14.124876: step 2810, loss = 1.07 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-07 19:22:15.465296: step 2820, loss = 1.12 (954.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:22:16.816150: step 2830, loss = 1.04 (947.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:22:18.169767: step 2840, loss = 0.90 (945.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:22:19.566942: step 2850, loss = 1.08 (916.1 examples/sec; 0.140 sec/batch)
2017-05-07 19:22:20.873962: step 2860, loss = 1.00 (979.3 examples/sec; 0.131 sec/batch)
2017-05-07 19:22:22.203779: step 2870, loss = 0.95 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:22:23.532769: step 2880, loss = 1.20 (963.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:22:24.883519: step 2890, loss = 1.19 (947.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:22:26.416155: step 2900, loss = 0.91 (835.2 examples/sec; 0.153 sec/batch)
2017-05-07 19:22:27.618070: step 2910, loss = 1.34 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-07 19:22:28.948254: step 2920, loss = 0.98 (962.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:22:30.282571: step 2930, loss = 1.01 (959.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:22:31.627724: step 2940, loss = 0.97 (951.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:22:33.007737: step 2950, loss = 1.00 (927.5 examples/sec; 0.138 sec/batch)
2017-05-07 19:22:34.332496: step 2960, loss = 1.17 (966.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:22:35.682804: step 2970, loss = 0.85 (947.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:22:36.995377: step 2980, loss = 1.02 (975.2 examples/sec; 0.131 sec/batch)
2017-05-07 19:22:38.338247: step 2990, loss = 0.83 (953.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:22:39.830517: step 3000, loss = 1.26 (857.8 examples/sec; 0.149 sec/batch)
2017-05-07 19:22:41.044981: step 3010, loss = 1.26 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-07 19:22:42.427305: step 3020, loss = 1.17 (926.0 examples/sec; 0.138 sec/batch)
2017-05-07 19:22:43.759265: step 3030, loss = 1.02 (961.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:22:45.109223: step 3040, loss = 1.14 (948.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:22:46.515007: step 3050, loss = 1.13 (910.5 examples/sec; 0.141 sec/batch)
2017-05-07 19:22:47.848743: step 3060, loss = 1.04 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:22:49.211975: step 3070, loss = 0.91 (938.9 examples/sec; 0.136 sec/batch)
2017-05-07 19:22:50.553432: step 3080, loss = 0.87 (954.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:22:51.925495: step 3090, loss = 1.14 (932.9 examples/sec; 0.137 sec/batch)
2017-05-07 19:22:53.411824: step 3100, loss = 1.17 (861.2 examples/sec; 0.149 sec/batch)
2017-05-07 19:22:54.611693: step 3110, loss = 1.05 (1066.8 examples/sec; 0.120 sec/batch)
2017-05-07 19:22:55.960016: step 3120, loss = 0.95 (949.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:22:57.307670: step 3130, loss = 0.94 (949.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:22:58.659710: step 3140, loss = 1.11 (946.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:23:00.045704: step 3150, loss = 1.12 (923.5 examples/sec; 0.139 sec/batch)
2017-05-07 19:23:01.359979: step 3160, loss = 1.08 (973.9 examples/sec; 0.131 sec/batch)
2017-05-07 19:23:02.696182: step 3170, loss = 0.99 (957.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:23:04.024631: step 3180, loss = 0.85 (963.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:23:05.349282: step 3190, loss = 1.05 (966.3 examples/sec; 0.132 sec/batch)
2017-05-07 19:23:06.840669: step 3200, loss = 1.06 (858.3 examples/sec; 0.149 sec/batch)
2017-05-07 19:23:08.032624: step 3210, loss = 1.07 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-07 19:23:09.372129: step 3220, loss = 1.06 (955.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:23:10.663700: step 3230, loss = 1.00 (991.0 examples/sec; 0.129 sec/batch)
2017-05-07 19:23:12.000897: step 3240, loss = 0.96 (957.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:23:13.392677: step 3250, loss = 1.23 (919.7 examples/sec; 0.139 sec/batch)
2017-05-07 19:23:14.712600: step 3260, loss = 0.87 (969.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:23:16.046155: step 3270, loss = 1.14 (959.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:23:17.366985: step 3280, loss = 0.98 (969.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:23:18.700947: step 3290, loss = 1.17 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:23:20.190688: step 3300, loss = 1.03 (859.2 examples/sec; 0.149 sec/batch)
2017-05-07 19:23:21.415191: step 3310, loss = 0.99 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-07 19:23:22.753567: step 3320, loss = 1.45 (956.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:23:24.095129: step 3330, loss = 1.00 (954.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:23:25.418709: step 3340, loss = 0.97 (967.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:23:26.814910: step 3350, loss = 1.02 (916.8 examples/sec; 0.140 sec/batch)
2017-05-07 19:23:28.127370: step 3360, loss = 0.97 (975.3 examples/sec; 0.131 sec/batch)
2017-05-07 19:23:29.452915: step 3370, loss = 1.09 (965.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:23:30.811400: step 3380, loss = 0.89 (942.2 examples/sec; 0.136 sec/batch)
2017-05-07 19:23:32.168727: step 3390, loss = 1.15 (943.0 examples/sec; 0.136 sec/batch)
2017-05-07 19:23:33.622727: step 3400, loss = 0.98 (880.3 examples/sec; 0.145 sec/batch)
2017-05-07 19:23:34.849986: step 3410, loss = 0.91 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-07 19:23:36.180095: step 3420, loss = 0.84 (962.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:23:37.506056: step 3430, loss = 1.07 (965.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:23:38.847327: step 3440, loss = 0.71 (954.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:23:40.241349: step 3450, loss = 0.86 (918.2 examples/sec; 0.139 sec/batch)
2017-05-07 19:23:41.530745: step 3460, loss = 1.00 (992.7 examples/sec; 0.129 sec/batch)
2017-05-07 19:23:42.881671: step 3470, loss = 0.99 (947.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:23:44.239384: step 3480, loss = 1.34 (942.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:23:45.571646: step 3490, loss = 1.23 (960.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:23:47.059314: step 3500, loss = 1.13 (860.4 examples/sec; 0.149 sec/batch)
2017-05-07 19:23:48.285940: step 3510, loss = 1.23 (1043.5 examples/sec; 0.123 sec/batch)
2017-05-07 19:23:49.615710: step 3520, loss = 1.06 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:23:50.969398: step 3530, loss = 0.91 (945.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:23:52.339928: step 3540, loss = 0.91 (933.9 examples/sec; 0.137 sec/batch)
2017-05-07 19:23:53.737955: step 3550, loss = 1.08 (915.6 examples/sec; 0.140 sec/batch)
2017-05-07 19:23:55.027550: step 3560, loss = 0.92 (992.6 examples/sec; 0.129 sec/batch)
2017-05-07 19:23:56.376743: step 3570, loss = 1.09 (948.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:23:57.743370: step 3580, loss = 1.06 (936.6 examples/sec; 0.137 sec/batch)
2017-05-07 19:23:59.068846: step 3590, loss = 0.84 (965.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:24:00.551054: step 3600, loss = 1.02 (863.6 examples/sec; 0.148 sec/batch)
2017-05-07 19:24:01.776749: step 3610, loss = 0.90 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-07 19:24:03.114986: step 3620, loss = 1.11 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:24:04.467443: step 3630, loss = 0.83 (946.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:24:05.813809: step 3640, loss = 0.85 (950.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:24:07.206870: step 3650, loss = 1.13 (918.8 examples/sec; 0.139 sec/batch)
2017-05-07 19:24:08.503107: step 3660, loss = 0.86 (987.5 examples/sec; 0.130 sec/batch)
2017-05-07 19:24:09.854609: step 3670, loss = 0.99 (947.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:24:11.209555: step 3680, loss = 1.14 (944.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:24:12.541412: step 3690, loss = 0.79 (961.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:24:14.028358: step 3700, loss = 1.16 (860.8 examples/sec; 0.149 sec/batch)
2017-05-07 19:24:15.218912: step 3710, loss = 1.01 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-07 19:24:16.553504: step 3720, loss = 1.13 (959.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:24:17.889357: step 3730, loss = 1.05 (958.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:24:19.239793: step 3740, loss = 1.23 (947.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:24:20.636090: step 3750, loss = 0.85 (916.7 examples/sec; 0.140 sec/batch)
2017-05-07 19:24:21.921612: step 3760, loss = 1.13 (995.7 examples/sec; 0.129 sec/batch)
2017-05-07 19:24:23.286166: step 3770, loss = 0.93 (938.0 examples/sec; 0.136 sec/batch)
2017-05-07 19:24:24.599166: step 3780, loss = 1.05 (974.9 examples/sec; 0.131 sec/batch)
2017-05-07 19:24:25.959079: step 3790, loss = 0.99 (941.2 examples/sec; 0.136 sec/batch)
2017-05-07 19:24:27.429676: step 3800, loss = 0.91 (870.4 examples/sec; 0.147 sec/batch)
2017-05-07 19:24:28.644350: step 3810, loss = 1.22 (1053.8 examples/sec; 0.121 sec/batch)
2017-05-07 19:24:29.987349: step 3820, loss = 1.03 (953.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:24:31.352352: step 3830, loss = 1.02 (937.7 examples/sec; 0.137 sec/batch)
2017-05-07 19:24:32.693070: step 3840, loss = 0.89 (954.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:24:34.096795: step 3850, loss = 1.09 (911.9 examples/sec; 0.140 sec/batch)
2017-05-07 19:24:35.400619: step 3860, loss = 0.89 (981.7 examples/sec; 0.130 sec/batch)
2017-05-07 19:24:36.730330: step 3870, loss = 0.94 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:24:38.057258: step 3880, loss = 0.82 (964.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:24:39.413652: step 3890, loss = 1.02 (943.7 examples/sec; 0.136 sec/batch)
2017-05-07 19:24:40.904977: step 3900, loss = 0.86 (858.3 examples/sec; 0.149 sec/batch)
2017-05-07 19:24:42.119816: step 3910, loss = 0.86 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-07 19:24:43.461432: step 3920, loss = 0.92 (954.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:24:44.797305: step 3930, loss = 0.98 (958.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:24:46.148880: step 3940, loss = 1.03 (947.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:24:47.531270: step 3950, loss = 1.04 (925.9 examples/sec; 0.138 sec/batch)
2017-05-07 19:24:48.833394: step 3960, loss = 1.10 (983.0 examples/sec; 0.130 sec/batch)
2017-05-07 19:24:50.180948: step 3970, loss = 0.86 (949.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:24:51.525346: step 3980, loss = 0.87 (952.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:24:52.859257: step 3990, loss = 0.80 (959.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:24:54.354630: step 4000, loss = 0.94 (856.0 examples/sec; 0.150 sec/batch)
2017-05-07 19:24:55.563770: step 4010, loss = 0.91 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-07 19:24:56.900745: step 4020, loss = 1.01 (957.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:24:58.226748: step 4030, loss = 1.12 (965.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:24:59.583368: step 4040, loss = 0.93 (943.5 examples/sec; 0.136 sec/batch)
2017-05-07 19:25:00.978246: step 4050, loss = 0.80 (917.6 examples/sec; 0.139 sec/batch)
2017-05-07 19:25:02.302212: step 4060, loss = 0.99 (966.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:25:03.669311: step 4070, loss = 1.05 (936.3 examples/sec; 0.137 sec/batch)
2017-05-07 19:25:04.978821: step 4080, loss = 0.88 (977.5 examples/sec; 0.131 sec/batch)
2017-05-07 19:25:06.315850: step 4090, loss = 1.10 (957.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:25:07.776624: step 4100, loss = 0.95 (876.2 examples/sec; 0.146 sec/batch)
2017-05-07 19:25:08.995109: step 4110, loss = 0.89 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-07 19:25:10.351374: step 4120, loss = 0.96 (943.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:25:11.693301: step 4130, loss = 1.05 (953.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:25:13.034154: step 4140, loss = 0.82 (954.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:25:14.431030: step 4150, loss = 0.81 (916.3 examples/sec; 0.140 sec/batch)
2017-05-07 19:25:15.732406: step 4160, loss = 1.05 (983.6 examples/sec; 0.130 sec/batch)
2017-05-07 19:25:17.075658: step 4170, loss = 0.86 (952.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:25:18.441795: step 4180, loss = 0.91 (936.9 examples/sec; 0.137 sec/batch)
2017-05-07 19:25:19.771069: step 4190, loss = 0.82 (962.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:25:21.253396: step 4200, loss = 0.84 (863.5 examples/sec; 0.148 sec/batch)
2017-05-07 19:25:22.452381: step 4210, loss = 0.93 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-07 19:25:23.791897: step 4220, loss = 0.96 (955.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:25:25.148406: step 4230, loss = 1.11 (943.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:25:26.479831: step 4240, loss = 1.00 (961.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:25:27.856860: step 4250, loss = 0.91 (929.5 examples/sec; 0.138 sec/batch)
2017-05-07 19:25:29.166827: step 4260, loss = 0.98 (977.1 examples/sec; 0.131 sec/batch)
2017-05-07 19:25:30.497606: step 4270, loss = 1.02 (961.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:25:31.822776: step 4280, loss = 0.94 (965.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:25:33.166073: step 4290, loss = 0.89 (952.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:25:34.702104: step 4300, loss = 1.17 (833.3 examples/sec; 0.154 sec/batch)
2017-05-07 19:25:35.897369: step 4310, loss = 1.03 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-07 19:25:37.232550: step 4320, loss = 0.97 (958.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:25:38.551186: step 4330, loss = 1.09 (970.7 examples/sec; 0.132 sec/batch)
2017-05-07 19:25:39.886679: step 4340, loss = 0.94 (958.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:25:41.281765: step 4350, loss = 1.43 (917.5 examples/sec; 0.140 sec/batch)
2017-05-07 19:25:42.568592: step 4360, loss = 1.07 (994.7 examples/sec; 0.129 sec/batch)
2017-05-07 19:25:43.933776: step 4370, loss = 0.87 (937.6 examples/sec; 0.137 sec/batch)
2017-05-07 19:25:45.272989: step 4380, loss = 0.89 (955.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:25:46.614934: step 4390, loss = 0.89 (953.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:25:48.097081: step 4400, loss = 0.90 (863.6 examples/sec; 0.148 sec/batch)
2017-05-07 19:25:49.331797: step 4410, loss = 0.95 (1036.7 examples/sec; 0.123 sec/batch)
2017-05-07 19:25:50.634195: step 4420, loss = 0.77 (982.8 examples/sec; 0.130 sec/batch)
2017-05-07 19:25:52.004016: step 4430, loss = 0.98 (934.4 examples/sec; 0.137 sec/batch)
2017-05-07 19:25:53.343217: step 4440, loss = 0.85 (955.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:25:54.785684: step 4450, loss = 0.83 (887.4 examples/sec; 0.144 sec/batch)
2017-05-07 19:25:56.087019: step 4460, loss = 0.91 (983.6 examples/sec; 0.130 sec/batch)
2017-05-07 19:25:57.447089: step 4470, loss = 0.85 (941.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:25:58.765232: step 4480, loss = 0.99 (971.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:26:00.115915: step 4490, loss = 1.15 (947.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:26:01.579432: step 4500, loss = 1.09 (874.6 examples/sec; 0.146 sec/batch)
2017-05-07 19:26:02.794420: step 4510, loss = 0.91 (1053.5 examples/sec; 0.121 sec/batch)
2017-05-07 19:26:04.131007: step 4520, loss = 0.79 (957.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:26:05.468597: step 4530, loss = 0.99 (956.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:26:06.832008: step 4540, loss = 0.98 (938.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:26:08.222274: step 4550, loss = 0.99 (920.7 examples/sec; 0.139 sec/batch)
2017-05-07 19:26:09.522045: step 4560, loss = 1.04 (984.8 examples/sec; 0.130 sec/batch)
2017-05-07 19:26:10.862303: step 4570, loss = 1.08 (955.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:26:12.210663: step 4580, loss = 0.97 (949.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:26:13.544698: step 4590, loss = 1.00 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:26:15.023799: step 4600, loss = 1.00 (865.4 examples/sec; 0.148 sec/batch)
2017-05-07 19:26:16.244246: step 4610, loss = 1.14 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-07 19:26:17.587209: step 4620, loss = 0.98 (953.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:26:18.939753: step 4630, loss = 0.97 (946.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:26:20.289828: step 4640, loss = 1.02 (948.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:26:21.676830: step 4650, loss = 0.95 (922.9 examples/sec; 0.139 sec/batch)
2017-05-07 19:26:22.984308: step 4660, loss = 0.79 (979.0 examples/sec; 0.131 sec/batch)
2017-05-07 19:26:24.338345: step 4670, loss = 0.83 (945.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:26:25.687939: step 4680, loss = 1.16 (948.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:26:27.032047: step 4690, loss = 0.81 (952.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:26:28.504059: step 4700, loss = 0.96 (869.6 examples/sec; 0.147 sec/batch)
2017-05-07 19:26:29.714273: step 4710, loss = 0.80 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-07 19:26:31.046412: step 4720, loss = 0.93 (960.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:26:32.382149: step 4730, loss = 1.03 (958.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:26:33.715852: step 4740, loss = 0.82 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:26:35.116960: step 4750, loss = 0.89 (913.6 examples/sec; 0.140 sec/batch)
2017-05-07 19:26:36.427964: step 4760, loss = 0.95 (976.4 examples/sec; 0.131 sec/batch)
2017-05-07 19:26:37.764883: step 4770, loss = 0.89 (957.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:26:39.120618: step 4780, loss = 0.94 (944.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:26:40.476316: step 4790, loss = 0.92 (944.2 examples/sec; 0.136 sec/batch)
2017-05-07 19:26:41.919750: step 4800, loss = 0.88 (886.8 examples/sec; 0.144 sec/batch)
2017-05-07 19:26:43.111808: step 4810, loss = 0.97 (1073.8 examples/sec; 0.119 sec/batch)
2017-05-07 19:26:44.479085: step 4820, loss = 0.93 (936.2 examples/sec; 0.137 sec/batch)
2017-05-07 19:26:45.828356: step 4830, loss = 1.11 (948.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:26:47.140555: step 4840, loss = 0.94 (975.5 examples/sec; 0.131 sec/batch)
2017-05-07 19:26:48.521690: step 4850, loss = 1.05 (926.8 examples/sec; 0.138 sec/batch)
2017-05-07 19:26:49.840089: step 4860, loss = 0.73 (970.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:26:51.178176: step 4870, loss = 1.01 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:26:52.521663: step 4880, loss = 1.06 (952.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:26:53.869511: step 4890, loss = 0.91 (949.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:26:55.342254: step 4900, loss = 0.83 (869.1 examples/sec; 0.147 sec/batch)
2017-05-07 19:26:56.577351: step 4910, loss = 0.93 (1036.4 examples/sec; 0.124 sec/batch)
2017-05-07 19:26:57.907686: step 4920, loss = 0.85 (962.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:26:59.265980: step 4930, loss = 1.01 (942.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:27:00.612214: step 4940, loss = 0.96 (950.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:27:01.994483: step 4950, loss = 1.01 (926.0 examples/sec; 0.138 sec/batch)
2017-05-07 19:27:03.313390: step 4960, loss = 0.91 (970.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:27:04.673092: step 4970, loss = 0.85 (941.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:27:06.023623: step 4980, loss = 1.03 (947.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:27:07.353647: step 4990, loss = 0.79 (962.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:27:08.853078: step 5000, loss = 1.13 (853.7 examples/sec; 0.150 sec/batch)
2017-05-07 19:27:10.060656: step 5010, loss = 0.86 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-07 19:27:11.413079: step 5020, loss = 1.05 (946.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:27:12.746610: step 5030, loss = 0.84 (959.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:27:14.089148: step 5040, loss = 0.92 (953.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:27:15.480002: step 5050, loss = 0.95 (920.3 examples/sec; 0.139 sec/batch)
2017-05-07 19:27:16.782035: step 5060, loss = 0.87 (983.1 examples/sec; 0.130 sec/batch)
2017-05-07 19:27:18.109581: step 5070, loss = 0.82 (964.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:27:19.476712: step 5080, loss = 0.90 (936.3 examples/sec; 0.137 sec/batch)
2017-05-07 19:27:20.797957: step 5090, loss = 0.92 (968.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:27:22.266661: step 5100, loss = 0.88 (871.5 examples/sec; 0.147 sec/batch)
2017-05-07 19:27:23.475740: step 5110, loss = 0.90 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-07 19:27:24.841183: step 5120, loss = 0.77 (937.4 examples/sec; 0.137 sec/batch)
2017-05-07 19:27:26.180518: step 5130, loss = 0.85 (955.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:27:27.513269: step 5140, loss = 0.94 (960.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:27:28.873501: step 5150, loss = 0.81 (941.0 examples/sec; 0.136 sec/batch)
2017-05-07 19:27:30.183571: step 5160, loss = 0.95 (977.0 examples/sec; 0.131 sec/batch)
2017-05-07 19:27:31.538935: step 5170, loss = 0.75 (944.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:27:32.871789: step 5180, loss = 0.91 (960.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:27:34.216428: step 5190, loss = 0.92 (951.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:27:35.731506: step 5200, loss = 1.18 (844.8 examples/sec; 0.152 sec/batch)
2017-05-07 19:27:36.907288: step 5210, loss = 0.97 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-07 19:27:38.261897: step 5220, loss = 1.02 (944.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:27:39.577295: step 5230, loss = 0.97 (973.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:27:40.903443: step 5240, loss = 0.86 (965.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:27:42.319051: step 5250, loss = 0.87 (904.2 examples/sec; 0.142 sec/batch)
2017-05-07 19:27:43.642353: step 5260, loss = 0.98 (967.3 examples/sec; 0.132 sec/batch)
2017-05-07 19:27:44.984631: step 5270, loss = 1.12 (953.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:27:46.334145: step 5280, loss = 0.88 (948.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:27:47.672857: step 5290, loss = 0.97 (956.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:27:49.133645: step 5300, loss = 1.04 (876.2 examples/sec; 0.146 sec/batch)
2017-05-07 19:27:50.346384: step 5310, loss = 1.07 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-07 19:27:51.686086: step 5320, loss = 0.80 (955.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:27:53.018139: step 5330, loss = 0.92 (960.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:27:54.364621: step 5340, loss = 0.91 (950.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:27:55.766850: step 5350, loss = 0.89 (912.8 examples/sec; 0.140 sec/batch)
2017-05-07 19:27:57.083394: step 5360, loss = 0.85 (972.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:27:58.402837: step 5370, loss = 0.95 (970.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:27:59.754367: step 5380, loss = 0.83 (947.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:28:01.104017: step 5390, loss = 0.98 (948.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:28:02.625989: step 5400, loss = 0.86 (841.0 examples/sec; 0.152 sec/batch)
2017-05-07 19:28:03.839254: step 5410, loss = 1.04 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-07 19:28:05.204768: step 5420, loss = 1.02 (937.4 examples/sec; 0.137 sec/batch)
2017-05-07 19:28:06.554765: step 5430, loss = 1.04 (948.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:28:07.901696: step 5440, loss = 0.83 (950.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:28:09.280185: step 5450, loss = 0.79 (928.6 examples/sec; 0.138 sec/batch)
2017-05-07 19:28:10.591294: step 5460, loss = 0.93 (976.3 examples/sec; 0.131 sec/batch)
2017-05-07 19:28:11.932553: step 5470, loss = 1.01 (954.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:28:13.277627: step 5480, loss = 0.93 (951.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:28:14.618174: step 5490, loss = 0.80 (954.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:28:16.084775: step 5500, loss = 0.85 (872.8 examples/sec; 0.147 sec/batch)
2017-05-07 19:28:17.278824: step 5510, loss = 0.95 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-07 19:28:18.625789: step 5520, loss = 1.02 (950.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:28:19.950229: step 5530, loss = 0.95 (966.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:28:21.305966: step 5540, loss = 0.85 (944.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:28:22.701331: step 5550, loss = 0.90 (917.3 examples/sec; 0.140 sec/batch)
2017-05-07 19:28:24.004794: step 5560, loss = 0.92 (982.0 examples/sec; 0.130 sec/batch)
2017-05-07 19:28:25.336587: step 5570, loss = 1.13 (961.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:28:26.680748: step 5580, loss = 0.98 (952.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:28:28.034819: step 5590, loss = 0.88 (945.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:28:29.516545: step 5600, loss = 0.80 (863.9 examples/sec; 0.148 sec/batch)
2017-05-07 19:28:30.728523: step 5610, loss = 0.95 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-07 19:28:32.065674: step 5620, loss = 0.90 (957.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:28:33.406308: step 5630, loss = 1.00 (954.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:28:34.749070: step 5640, loss = 1.00 (953.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:28:36.140111: step 5650, loss = 0.73 (920.2 examples/sec; 0.139 sec/batch)
2017-05-07 19:28:37.446938: step 5660, loss = 1.00 (979.5 examples/sec; 0.131 sec/batch)
2017-05-07 19:28:38.772885: step 5670, loss = 0.80 (965.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:28:40.105379: step 5680, loss = 0.94 (960.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:28:41.418972: step 5690, loss = 0.92 (974.4 examples/sec; 0.131 sec/batch)
2017-05-07 19:28:42.933080: step 5700, loss = 1.01 (845.4 examples/sec; 0.151 sec/batch)
2017-05-07 19:28:44.110047: step 5710, loss = 0.87 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-07 19:28:45.430851: step 5720, loss = 0.94 (969.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:28:46.766371: step 5730, loss = 0.79 (958.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:28:48.121171: step 5740, loss = 0.83 (944.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:28:49.514484: step 5750, loss = 0.96 (918.7 examples/sec; 0.139 sec/batch)
2017-05-07 19:28:50.822917: step 5760, loss = 1.03 (978.3 examples/sec; 0.131 sec/batch)
2017-05-07 19:28:52.176801: step 5770, loss = 1.04 (945.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:28:53.512552: step 5780, loss = 0.88 (958.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:28:54.860606: step 5790, loss = 0.91 (949.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:28:56.340620: step 5800, loss = 0.91 (864.9 examples/sec; 0.148 sec/batch)
2017-05-07 19:28:57.550677: step 5810, loss = 1.02 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-07 19:28:58.901117: step 5820, loss = 0.97 (947.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:29:00.226691: step 5830, loss = 0.79 (965.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:29:01.579654: step 5840, loss = 0.96 (946.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:29:02.998866: step 5850, loss = 1.06 (901.9 examples/sec; 0.142 sec/batch)
2017-05-07 19:29:04.284000: step 5860, loss = 0.83 (996.0 examples/sec; 0.129 sec/batch)
2017-05-07 19:29:05.635016: step 5870, loss = 0.90 (947.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:29:06.957773: step 5880, loss = 0.85 (967.7 examples/sec; 0.132 sec/batch)
2017-05-07 19:29:08.312213: step 5890, loss = 0.89 (945.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:29:09.822091: step 5900, loss = 0.82 (847.8 examples/sec; 0.151 sec/batch)
2017-05-07 19:29:11.032312: step 5910, loss = 0.85 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-07 19:29:12.387905: step 5920, loss = 1.10 (944.2 examples/sec; 0.136 sec/batch)
2017-05-07 19:29:13.716443: step 5930, loss = 1.00 (963.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:29:15.046223: step 5940, loss = 0.92 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:29:16.430281: step 5950, loss = 0.68 (924.8 examples/sec; 0.138 sec/batch)
2017-05-07 19:29:17.697866: step 5960, loss = 0.82 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-07 19:29:19.045856: step 5970, loss = 1.08 (949.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:29:20.403900: step 5980, loss = 0.85 (942.5 examples/sec; 0.136 sec/batch)
2017-05-07 19:29:21.725844: step 5990, loss = 1.19 (968.3 examples/sec; 0.132 sec/batch)
2017-05-07 19:29:23.226694: step 6000, loss = 0.97 (852.9 examples/sec; 0.150 sec/batch)
2017-05-07 19:29:24.424889: step 6010, loss = 1.08 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-07 19:29:25.767015: step 6020, loss = 0.94 (953.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:29:27.092245: step 6030, loss = 0.84 (965.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:29:28.439514: step 6040, loss = 0.96 (950.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:29:29.841982: step 6050, loss = 0.91 (912.7 examples/sec; 0.140 sec/batch)
2017-05-07 19:29:31.165733: step 6060, loss = 0.69 (966.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:29:32.498413: step 6070, loss = 1.02 (960.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:29:33.838929: step 6080, loss = 1.07 (954.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:29:35.166547: step 6090, loss = 0.86 (964.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:29:36.640897: step 6100, loss = 0.91 (868.2 examples/sec; 0.147 sec/batch)
2017-05-07 19:29:37.859303: step 6110, loss = 1.03 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-07 19:29:39.195822: step 6120, loss = 1.12 (957.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:29:40.517505: step 6130, loss = 0.93 (968.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:29:41.866711: step 6140, loss = 0.89 (948.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:29:43.238198: step 6150, loss = 0.86 (933.3 examples/sec; 0.137 sec/batch)
2017-05-07 19:29:44.528154: step 6160, loss = 0.90 (992.3 examples/sec; 0.129 sec/batch)
2017-05-07 19:29:45.865873: step 6170, loss = 0.92 (956.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:29:47.187447: step 6180, loss = 0.94 (968.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:29:48.524967: step 6190, loss = 0.88 (957.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:29:49.979902: step 6200, loss = 0.95 (879.8 examples/sec; 0.145 sec/batch)
2017-05-07 19:29:51.211721: step 6210, loss = 0.89 (1039.1 examples/sec; 0.123 sec/batch)
2017-05-07 19:29:52.534261: step 6220, loss = 1.09 (967.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:29:53.870192: step 6230, loss = 0.93 (958.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:29:55.188649: step 6240, loss = 0.82 (970.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:29:56.580824: step 6250, loss = 0.82 (919.4 examples/sec; 0.139 sec/batch)
2017-05-07 19:29:57.862305: step 6260, loss = 0.91 (998.9 examples/sec; 0.128 sec/batch)
2017-05-07 19:29:59.194525: step 6270, loss = 1.19 (960.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:30:00.524925: step 6280, loss = 0.89 (962.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:30:01.862842: step 6290, loss = 0.94 (956.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:30:03.352354: step 6300, loss = 0.84 (859.3 examples/sec; 0.149 sec/batch)
2017-05-07 19:30:04.569263: step 6310, loss = 1.02 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-07 19:30:05.884800: step 6320, loss = 0.89 (973.0 examples/sec; 0.132 sec/batch)
2017-05-07 19:30:07.255255: step 6330, loss = 0.89 (934.0 examples/sec; 0.137 sec/batch)
2017-05-07 19:30:08.572092: step 6340, loss = 0.88 (972.0 examples/sec; 0.132 sec/batch)
2017-05-07 19:30:09.968174: step 6350, loss = 0.81 (916.9 examples/sec; 0.140 sec/batch)
2017-05-07 19:30:11.270190: step 6360, loss = 0.90 (983.1 examples/sec; 0.130 sec/batch)
2017-05-07 19:30:12.584948: step 6370, loss = 0.89 (973.6 examples/sec; 0.131 sec/batch)
2017-05-07 19:30:13.933244: step 6380, loss = 0.80 (949.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:30:15.286216: step 6390, loss = 0.83 (946.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:30:16.760584: step 6400, loss = 1.03 (868.2 examples/sec; 0.147 sec/batch)
2017-05-07 19:30:17.987655: step 6410, loss = 0.87 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-07 19:30:19.308515: step 6420, loss = 0.82 (969.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:30:20.649137: step 6430, loss = 0.92 (954.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:30:22.010748: step 6440, loss = 0.80 (940.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:30:23.384993: step 6450, loss = 0.94 (931.4 examples/sec; 0.137 sec/batch)
2017-05-07 19:30:24.673836: step 6460, loss = 1.24 (993.1 examples/sec; 0.129 sec/batch)
2017-05-07 19:30:26.041721: step 6470, loss = 0.90 (935.8 examples/sec; 0.137 sec/batch)
2017-05-07 19:30:27.389904: step 6480, loss = 0.90 (949.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:30:28.708504: step 6490, loss = 0.88 (970.7 examples/sec; 0.132 sec/batch)
2017-05-07 19:30:30.188805: step 6500, loss = 0.83 (864.7 examples/sec; 0.148 sec/batch)
2017-05-07 19:30:31.388736: step 6510, loss = 0.80 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-07 19:30:32.725751: step 6520, loss = 0.88 (957.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:30:34.083606: step 6530, loss = 0.93 (942.7 examples/sec; 0.136 sec/batch)
2017-05-07 19:30:35.430679: step 6540, loss = 0.80 (950.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:30:36.821683: step 6550, loss = 0.89 (920.2 examples/sec; 0.139 sec/batch)
2017-05-07 19:30:38.086644: step 6560, loss = 0.89 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-07 19:30:39.418827: step 6570, loss = 0.69 (960.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:30:40.742635: step 6580, loss = 0.91 (966.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:30:42.083752: step 6590, loss = 0.85 (954.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:30:43.567562: step 6600, loss = 0.91 (862.6 examples/sec; 0.148 sec/batch)
2017-05-07 19:30:44.762139: step 6610, loss = 0.85 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-07 19:30:46.111678: step 6620, loss = 0.91 (948.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:30:47.462241: step 6630, loss = 0.81 (947.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:30:48.811196: step 6640, loss = 0.92 (948.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:30:50.195007: step 6650, loss = 0.85 (925.0 examples/sec; 0.138 sec/batch)
2017-05-07 19:30:51.529064: step 6660, loss = 0.74 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:30:52.847807: step 6670, loss = 0.82 (970.6 examples/sec; 0.132 sec/batch)
2017-05-07 19:30:54.191333: step 6680, loss = 0.79 (952.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:30:55.530697: step 6690, loss = 0.81 (955.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:30:56.967884: step 6700, loss = 0.97 (890.6 examples/sec; 0.144 sec/batch)
2017-05-07 19:30:58.232156: step 6710, loss = 1.08 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-07 19:30:59.550710: step 6720, loss = 0.95 (970.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:31:00.897161: step 6730, loss = 0.77 (950.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:31:02.234556: step 6740, loss = 0.91 (957.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:31:03.608419: step 6750, loss = 0.97 (931.7 examples/sec; 0.137 sec/batch)
2017-05-07 19:31:04.951489: step 6760, loss = 0.77 (953.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:31:06.283296: step 6770, loss = 1.04 (961.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:31:07.635311: step 6780, loss = 0.85 (946.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:31:08.978806: step 6790, loss = 0.86 (952.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:31:10.434078: step 6800, loss = 0.84 (879.6 examples/sec; 0.146 sec/batch)
2017-05-07 19:31:11.660571: step 6810, loss = 0.88 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-07 19:31:13.013917: step 6820, loss = 0.88 (945.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:31:14.362006: step 6830, loss = 0.84 (949.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:31:15.709233: step 6840, loss = 0.72 (950.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:31:17.045782: step 6850, loss = 0.73 (957.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:31:18.402259: step 6860, loss = 0.80 (943.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:31:19.766072: step 6870, loss = 0.85 (938.5 examples/sec; 0.136 sec/batch)
2017-05-07 19:31:21.107825: step 6880, loss = 1.08 (954.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:31:22.434886: step 6890, loss = 1.02 (964.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:31:23.880658: step 6900, loss = 0.89 (885.3 examples/sec; 0.145 sec/batch)
2017-05-07 19:31:25.117736: step 6910, loss = 0.82 (1034.7 examples/sec; 0.124 sec/batch)
2017-05-07 19:31:26.458249: step 6920, loss = 0.87 (954.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:31:27.811126: step 6930, loss = 0.97 (946.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:31:29.138754: step 6940, loss = 0.95 (964.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:31:30.514665: step 6950, loss = 0.75 (930.3 examples/sec; 0.138 sec/batch)
2017-05-07 19:31:31.861018: step 6960, loss = 0.95 (950.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:31:33.224412: step 6970, loss = 0.90 (938.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:31:34.570650: step 6980, loss = 0.93 (950.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:31:35.912011: step 6990, loss = 0.87 (954.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:31:37.329064: step 7000, loss = 1.09 (903.3 examples/sec; 0.142 sec/batch)
2017-05-07 19:31:38.574613: step 7010, loss = 0.88 (1027.7 examples/sec; 0.125 sec/batch)
2017-05-07 19:31:39.921776: step 7020, loss = 0.91 (950.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:31:41.288698: step 7030, loss = 0.90 (936.4 examples/sec; 0.137 sec/batch)
2017-05-07 19:31:42.643181: step 7040, loss = 0.80 (945.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:31:44.008935: step 7050, loss = 0.86 (937.2 examples/sec; 0.137 sec/batch)
2017-05-07 19:31:45.319648: step 7060, loss = 0.99 (976.6 examples/sec; 0.131 sec/batch)
2017-05-07 19:31:46.693430: step 7070, loss = 0.96 (931.7 examples/sec; 0.137 sec/batch)
2017-05-07 19:31:48.045804: step 7080, loss = 0.96 (946.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:31:49.372983: step 7090, loss = 0.88 (964.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:31:50.868530: step 7100, loss = 0.95 (855.9 examples/sec; 0.150 sec/batch)
2017-05-07 19:31:52.024906: step 7110, loss = 1.04 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-07 19:31:53.359765: step 7120, loss = 0.94 (958.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:31:54.692653: step 7130, loss = 0.78 (960.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:31:56.038224: step 7140, loss = 0.92 (951.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:31:57.386656: step 7150, loss = 1.07 (949.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:31:58.704384: step 7160, loss = 0.82 (971.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:32:00.033190: step 7170, loss = 0.97 (963.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:32:01.369392: step 7180, loss = 0.77 (957.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:32:02.718893: step 7190, loss = 0.90 (948.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:32:04.190942: step 7200, loss = 0.86 (869.5 examples/sec; 0.147 sec/batch)
2017-05-07 19:32:05.443787: step 7210, loss = 0.95 (1021.7 examples/sec; 0.125 sec/batch)
2017-05-07 19:32:06.791234: step 7220, loss = 0.80 (949.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:32:08.166871: step 7230, loss = 0.81 (930.5 examples/sec; 0.138 sec/batch)
2017-05-07 19:32:09.484142: step 7240, loss = 0.77 (971.7 examples/sec; 0.132 sec/batch)
2017-05-07 19:32:10.851720: step 7250, loss = 0.90 (936.0 examples/sec; 0.137 sec/batch)
2017-05-07 19:32:12.169651: step 7260, loss = 0.80 (971.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:32:13.524222: step 7270, loss = 0.94 (944.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:32:14.876569: step 7280, loss = 1.09 (946.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:32:16.243521: step 7290, loss = 0.96 (936.4 examples/sec; 0.137 sec/batch)
2017-05-07 19:32:17.717948: step 7300, loss = 0.95 (868.1 examples/sec; 0.147 sec/batch)
2017-05-07 19:32:18.913432: step 7310, loss = 0.97 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-07 19:32:20.257573: step 7320, loss = 0.88 (952.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:32:21.590225: step 7330, loss = 0.85 (960.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:32:22.897304: step 7340, loss = 0.79 (979.3 examples/sec; 0.131 sec/batch)
2017-05-07 19:32:24.255266: step 7350, loss = 0.89 (942.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:32:25.588669: step 7360, loss = 0.93 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:32:26.923578: step 7370, loss = 0.69 (958.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:32:28.254524: step 7380, loss = 0.88 (961.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:32:29.582120: step 7390, loss = 0.84 (964.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:32:31.024236: step 7400, loss = 0.89 (887.6 examples/sec; 0.144 sec/batch)
2017-05-07 19:32:32.294663: step 7410, loss = 1.01 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-07 19:32:33.640660: step 7420, loss = 0.83 (951.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:32:34.992609: step 7430, loss = 0.94 (946.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:32:36.353738: step 7440, loss = 0.82 (940.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:32:37.705372: step 7450, loss = 0.93 (947.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:32:39.059481: step 7460, loss = 0.92 (945.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:32:40.399578: step 7470, loss = 0.76 (955.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:32:41.748247: step 7480, loss = 0.74 (949.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:32:43.114545: step 7490, loss = 0.89 (936.8 examples/sec; 0.137 sec/batch)
2017-05-07 19:32:44.571028: step 7500, loss = 0.92 (878.8 examples/sec; 0.146 sec/batch)
2017-05-07 19:32:45.813836: step 7510, loss = 1.01 (1029.9 examples/sec; 0.124 sec/batch)
2017-05-07 19:32:47.155919: step 7520, loss = 1.02 (953.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:32:48.497839: step 7530, loss = 0.83 (953.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:32:49.830334: step 7540, loss = 1.00 (960.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:32:51.189153: step 7550, loss = 1.01 (942.0 examples/sec; 0.136 sec/batch)
2017-05-07 19:32:52.526896: step 7560, loss = 1.06 (956.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:32:53.876081: step 7570, loss = 1.02 (948.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:32:55.226000: step 7580, loss = 0.92 (948.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:32:56.559007: step 7590, loss = 0.74 (960.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:32:58.003913: step 7600, loss = 0.97 (885.9 examples/sec; 0.144 sec/batch)
2017-05-07 19:32:59.259081: step 7610, loss = 1.04 (1019.8 examples/sec; 0.126 sec/batch)
2017-05-07 19:33:00.588948: step 7620, loss = 0.85 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:33:01.905981: step 7630, loss = 0.83 (971.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:33:03.260107: step 7640, loss = 0.95 (945.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:33:04.609284: step 7650, loss = 0.98 (948.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:33:05.939025: step 7660, loss = 0.99 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:33:07.283944: step 7670, loss = 1.10 (951.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:33:08.610248: step 7680, loss = 0.92 (965.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:33:09.961236: step 7690, loss = 0.86 (947.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:33:11.404245: step 7700, loss = 0.98 (887.0 examples/sec; 0.144 sec/batch)
2017-05-07 19:33:12.649628: step 7710, loss = 0.78 (1027.8 examples/sec; 0.125 sec/batch)
2017-05-07 19:33:14.002321: step 7720, loss = 0.88 (946.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:33:15.355877: step 7730, loss = 0.87 (945.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:33:16.687842: step 7740, loss = 1.00 (961.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:33:18.057605: step 7750, loss = 1.03 (934.5 examples/sec; 0.137 sec/batch)
2017-05-07 19:33:19.377434: step 7760, loss = 0.85 (969.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:33:20.711721: step 7770, loss = 0.85 (959.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:33:22.077523: step 7780, loss = 0.80 (937.2 examples/sec; 0.137 sec/batch)
2017-05-07 19:33:23.420574: step 7790, loss = 0.85 (953.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:33:24.875817: step 7800, loss = 0.91 (879.6 examples/sec; 0.146 sec/batch)
2017-05-07 19:33:26.111972: step 7810, loss = 0.71 (1035.5 examples/sec; 0.124 sec/batch)
2017-05-07 19:33:27.454557: step 7820, loss = 0.87 (953.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:33:28.800276: step 7830, loss = 0.83 (951.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:33:30.145530: step 7840, loss = 0.87 (951.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:33:31.516826: step 7850, loss = 1.01 (933.4 examples/sec; 0.137 sec/batch)
2017-05-07 19:33:32.880122: step 7860, loss = 0.79 (938.9 examples/sec; 0.136 sec/batch)
2017-05-07 19:33:34.223896: step 7870, loss = 1.16 (952.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:33:35.592315: step 7880, loss = 0.86 (935.4 examples/sec; 0.137 sec/batch)
2017-05-07 19:33:36.912818: step 7890, loss = 0.81 (969.3 examples/sec; 0.132 sec/batch)
2017-05-07 19:33:38.374017: step 7900, loss = 0.70 (876.0 examples/sec; 0.146 sec/batch)
2017-05-07 19:33:39.616399: step 7910, loss = 1.17 (1030.3 examples/sec; 0.124 sec/batch)
2017-05-07 19:33:40.944323: step 7920, loss = 1.14 (963.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:33:42.287026: step 7930, loss = 0.86 (953.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:33:43.635693: step 7940, loss = 0.65 (949.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:33:44.972915: step 7950, loss = 0.80 (957.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:33:46.307511: step 7960, loss = 0.95 (959.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:33:47.639787: step 7970, loss = 0.92 (960.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:33:48.951846: step 7980, loss = 0.96 (975.6 examples/sec; 0.131 sec/batch)
2017-05-07 19:33:50.293796: step 7990, loss = 0.92 (953.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:33:51.734690: step 8000, loss = 0.84 (888.3 examples/sec; 0.144 sec/batch)
2017-05-07 19:33:52.965123: step 8010, loss = 0.91 (1040.3 examples/sec; 0.123 sec/batch)
2017-05-07 19:33:54.315788: step 8020, loss = 1.05 (947.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:33:55.648920: step 8030, loss = 0.81 (960.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:33:56.986870: step 8040, loss = 0.86 (956.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:33:58.351840: step 8050, loss = 0.80 (937.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:33:59.693168: step 8060, loss = 0.73 (954.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:34:01.054236: step 8070, loss = 0.83 (940.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:34:02.400497: step 8080, loss = 0.74 (950.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:34:03.730550: step 8090, loss = 0.89 (962.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:34:05.202346: step 8100, loss = 1.08 (869.7 examples/sec; 0.147 sec/batch)
2017-05-07 19:34:06.394275: step 8110, loss = 0.91 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-07 19:34:07.731922: step 8120, loss = 0.95 (956.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:34:09.088136: step 8130, loss = 0.77 (943.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:34:10.460381: step 8140, loss = 0.81 (932.8 examples/sec; 0.137 sec/batch)
2017-05-07 19:34:11.811140: step 8150, loss = 0.73 (947.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:34:13.133616: step 8160, loss = 0.91 (967.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:34:14.493366: step 8170, loss = 0.83 (941.3 examples/sec; 0.136 sec/batch)
2017-05-07 19:34:15.818593: step 8180, loss = 0.89 (965.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:34:17.170672: step 8190, loss = 0.83 (946.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:34:18.614108: step 8200, loss = 0.83 (886.8 examples/sec; 0.144 sec/batch)
2017-05-07 19:34:19.854346: step 8210, loss = 0.91 (1032.1 examples/sec; 0.124 sec/batch)
2017-05-07 19:34:21.183967: step 8220, loss = 0.74 (962.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:34:22.567764: step 8230, loss = 0.75 (925.0 examples/sec; 0.138 sec/batch)
2017-05-07 19:34:23.893608: step 8240, loss = 0.87 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:34:25.262985: step 8250, loss = 0.85 (934.7 examples/sec; 0.137 sec/batch)
2017-05-07 19:34:26.592002: step 8260, loss = 0.85 (963.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:34:27.925709: step 8270, loss = 0.83 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:34:29.269565: step 8280, loss = 0.90 (952.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:34:30.610894: step 8290, loss = 0.84 (954.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:34:32.060327: step 8300, loss = 0.86 (883.1 examples/sec; 0.145 sec/batch)
2017-05-07 19:34:33.284517: step 8310, loss = 0.67 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-07 19:34:34.636116: step 8320, loss = 0.93 (947.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:34:35.979087: step 8330, loss = 0.90 (953.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:34:37.328939: step 8340, loss = 0.72 (948.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:34:38.672817: step 8350, loss = 0.98 (952.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:34:40.020632: step 8360, loss = 0.90 (949.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:34:41.354242: step 8370, loss = 1.14 (959.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:34:42.701427: step 8380, loss = 0.85 (950.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:34:44.027264: step 8390, loss = 0.88 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:34:45.466763: step 8400, loss = 0.76 (889.2 examples/sec; 0.144 sec/batch)
2017-05-07 19:34:46.724163: step 8410, loss = 0.99 (1018.0 examples/sec; 0.126 sec/batch)
2017-05-07 19:34:48.073317: step 8420, loss = 0.77 (948.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:34:49.432765: step 8430, loss = 1.02 (941.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:34:50.768060: step 8440, loss = 0.72 (958.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:34:52.121012: step 8450, loss = 0.89 (946.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:34:53.462628: step 8460, loss = 0.80 (954.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:34:54.806603: step 8470, loss = 0.76 (952.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:34:56.154509: step 8480, loss = 0.85 (949.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:34:57.515420: step 8490, loss = 0.94 (940.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:34:58.977569: step 8500, loss = 0.96 (875.4 examples/sec; 0.146 sec/batch)
2017-05-07 19:35:00.212608: step 8510, loss = 0.78 (1036.4 examples/sec; 0.124 sec/batch)
2017-05-07 19:35:01.551510: step 8520, loss = 0.99 (956.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:35:02.893884: step 8530, loss = 0.84 (953.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:35:04.251154: step 8540, loss = 0.90 (943.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:35:05.619645: step 8550, loss = 1.06 (935.3 examples/sec; 0.137 sec/batch)
2017-05-07 19:35:06.950686: step 8560, loss = 0.98 (961.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:35:08.314127: step 8570, loss = 0.81 (938.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:35:09.653593: step 8580, loss = 0.90 (955.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:35:10.980745: step 8590, loss = 0.91 (964.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:35:12.444917: step 8600, loss = 0.79 (874.2 examples/sec; 0.146 sec/batch)
2017-05-07 19:35:13.672697: step 8610, loss = 0.95 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-07 19:35:15.006655: step 8620, loss = 0.77 (959.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:35:16.342943: step 8630, loss = 0.88 (957.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:35:17.689829: step 8640, loss = 0.97 (950.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:35:19.038714: step 8650, loss = 0.86 (948.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:35:20.382867: step 8660, loss = 0.80 (952.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:35:21.720500: step 8670, loss = 0.95 (956.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:35:23.056547: step 8680, loss = 1.03 (958.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:35:24.393726: step 8690, loss = 0.89 (957.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:35:25.840266: step 8700, loss = 0.92 (884.9 examples/sec; 0.145 sec/batch)
2017-05-07 19:35:27.106080: step 8710, loss = 0.85 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-07 19:35:28.472412: step 8720, loss = 1.04 (936.8 examples/sec; 0.137 sec/batch)
2017-05-07 19:35:29.827374: step 8730, loss = 0.87 (944.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:35:31.169395: step 8740, loss = 0.80 (953.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:35:32.537156: step 8750, loss = 0.90 (935.8 examples/sec; 0.137 sec/batch)
2017-05-07 19:35:33.867504: step 8760, loss = 0.93 (962.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:35:35.220321: step 8770, loss = 0.83 (946.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:35:36.557729: step 8780, loss = 0.96 (957.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:35:37.866310: step 8790, loss = 0.86 (978.2 examples/sec; 0.131 sec/batch)
2017-05-07 19:35:39.306861: step 8800, loss = 0.78 (888.5 examples/sec; 0.144 sec/batch)
2017-05-07 19:35:40.538776: step 8810, loss = 0.80 (1039.0 examples/sec; 0.123 sec/batch)
2017-05-07 19:35:41.880815: step 8820, loss = 0.89 (953.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:35:43.200573: step 8830, loss = 0.85 (969.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:35:44.541659: step 8840, loss = 0.94 (954.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:35:45.893590: step 8850, loss = 0.56 (946.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:35:47.237372: step 8860, loss = 0.84 (952.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:35:48.559833: step 8870, loss = 0.74 (967.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:35:49.894876: step 8880, loss = 0.88 (958.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:35:51.252407: step 8890, loss = 0.78 (942.9 examples/sec; 0.136 sec/batch)
2017-05-07 19:35:52.713540: step 8900, loss = 0.87 (876.0 examples/sec; 0.146 sec/batch)
2017-05-07 19:35:53.992019: step 8910, loss = 0.90 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-07 19:35:55.354640: step 8920, loss = 0.89 (939.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:35:56.681674: step 8930, loss = 0.72 (964.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:35:58.042971: step 8940, loss = 0.90 (940.3 examples/sec; 0.136 sec/batch)
2017-05-07 19:35:59.429548: step 8950, loss = 0.92 (923.1 examples/sec; 0.139 sec/batch)
2017-05-07 19:36:00.747139: step 8960, loss = 0.83 (971.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:36:02.105021: step 8970, loss = 0.93 (942.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:36:03.439836: step 8980, loss = 0.83 (958.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:36:04.796229: step 8990, loss = 0.79 (943.7 examples/sec; 0.136 sec/batch)
2017-05-07 19:36:06.272118: step 9000, loss = 0.91 (867.3 examples/sec; 0.148 sec/batch)
2017-05-07 19:36:07.494022: step 9010, loss = 0.90 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-07 19:36:08.822918: step 9020, loss = 0.97 (963.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:36:10.133058: step 9030, loss = 1.04 (977.0 examples/sec; 0.131 sec/batch)
2017-05-07 19:36:11.478935: step 9040, loss = 0.78 (951.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:36:12.839944: step 9050, loss = 0.96 (940.5 examples/sec; 0.136 sec/batch)
2017-05-07 19:36:14.213606: step 9060, loss = 0.89 (931.8 examples/sec; 0.137 sec/batch)
2017-05-07 19:36:15.571299: step 9070, loss = 0.88 (942.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:36:16.895182: step 9080, loss = 0.99 (966.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:36:18.255841: step 9090, loss = 0.85 (940.7 examples/sec; 0.136 sec/batch)
2017-05-07 19:36:19.749357: step 9100, loss = 0.75 (857.0 examples/sec; 0.149 sec/batch)
2017-05-07 19:36:20.951570: step 9110, loss = 0.90 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-07 19:36:22.308846: step 9120, loss = 0.80 (943.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:36:23.664583: step 9130, loss = 0.97 (944.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:36:25.000285: step 9140, loss = 0.86 (958.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:36:26.355582: step 9150, loss = 0.99 (944.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:36:27.739975: step 9160, loss = 1.00 (924.6 examples/sec; 0.138 sec/batch)
2017-05-07 19:36:29.048645: step 9170, loss = 0.84 (978.1 examples/sec; 0.131 sec/batch)
2017-05-07 19:36:30.399009: step 9180, loss = 0.98 (947.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:36:31.729072: step 9190, loss = 0.86 (962.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:36:33.175843: step 9200, loss = 0.91 (884.7 examples/sec; 0.145 sec/batch)
2017-05-07 19:36:34.424893: step 9210, loss = 0.84 (1024.8 examples/sec; 0.125 sec/batch)
2017-05-07 19:36:35.790386: step 9220, loss = 0.89 (937.4 examples/sec; 0.137 sec/batch)
2017-05-07 19:36:37.135749: step 9230, loss = 0.82 (951.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:36:38.475613: step 9240, loss = 0.93 (955.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:36:39.829816: step 9250, loss = 0.89 (945.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:36:41.177653: step 9260, loss = 0.77 (949.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:36:42.523634: step 9270, loss = 1.07 (951.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:36:43.859824: step 9280, loss = 0.66 (957.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:36:45.179870: step 9290, loss = 0.95 (969.7 examples/sec; 0.132 sec/batch)
2017-05-07 19:36:46.635769: step 9300, loss = 0.75 (879.2 examples/sec; 0.146 sec/batch)
2017-05-07 19:36:47.900123: step 9310, loss = 0.83 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-07 19:36:49.241034: step 9320, loss = 0.82 (954.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:36:50.584149: step 9330, loss = 0.92 (953.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:36:51.929996: step 9340, loss = 0.86 (951.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:36:53.295231: step 9350, loss = 0.92 (937.6 examples/sec; 0.137 sec/batch)
2017-05-07 19:36:54.634249: step 9360, loss = 0.84 (955.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:36:55.963248: step 9370, loss = 0.92 (963.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:36:57.301536: step 9380, loss = 0.81 (956.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:36:58.667986: step 9390, loss = 0.86 (936.7 examples/sec; 0.137 sec/batch)
2017-05-07 19:37:00.141637: step 9400, loss = 0.82 (868.6 examples/sec; 0.147 sec/batch)
2017-05-07 19:37:01.372905: step 9410, loss = 0.82 (1039.6 examples/sec; 0.123 sec/batch)
2017-05-07 19:37:02.735897: step 9420, loss = 0.83 (939.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:37:04.078847: step 9430, loss = 1.04 (953.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:37:05.414237: step 9440, loss = 0.81 (958.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:37:06.783379: step 9450, loss = 1.06 (934.9 examples/sec; 0.137 sec/batch)
2017-05-07 19:37:08.122637: step 9460, loss = 0.76 (955.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:37:09.469275: step 9470, loss = 0.99 (950.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:37:10.810986: step 9480, loss = 0.84 (954.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:37:12.154570: step 9490, loss = 1.00 (952.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:37:13.591419: step 9500, loss = 0.83 (890.8 examples/sec; 0.144 sec/batch)
2017-05-07 19:37:14.809063: step 9510, loss = 1.10 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-07 19:37:16.156269: step 9520, loss = 0.87 (950.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:37:17.496929: step 9530, loss = 0.92 (954.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:37:18.842910: step 9540, loss = 0.77 (951.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:37:20.204068: step 9550, loss = 0.74 (940.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:37:21.509181: step 9560, loss = 0.86 (980.8 examples/sec; 0.131 sec/batch)
2017-05-07 19:37:22.853551: step 9570, loss = 0.91 (952.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:37:24.198052: step 9580, loss = 0.95 (952.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:37:25.528002: step 9590, loss = 0.79 (962.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:37:26.974831: step 9600, loss = 1.06 (884.7 examples/sec; 0.145 sec/batch)
2017-05-07 19:37:28.211209: step 9610, loss = 0.85 (1035.3 examples/sec; 0.124 sec/batch)
2017-05-07 19:37:29.563143: step 9620, loss = 0.83 (946.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:37:30.885708: step 9630, loss = 0.82 (967.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:37:32.219707: step 9640, loss = 0.74 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:37:33.553742: step 9650, loss = 1.02 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:37:34.902537: step 9660, loss = 0.98 (949.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:37:36.244592: step 9670, loss = 0.90 (953.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:37:37.580779: step 9680, loss = 0.92 (957.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:37:38.913686: step 9690, loss = 0.75 (960.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:37:40.364001: step 9700, loss = 0.85 (882.6 examples/sec; 0.145 sec/batch)
2017-05-07 19:37:41.593991: step 9710, loss = 0.97 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-07 19:37:42.945850: step 9720, loss = 0.88 (946.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:37:44.283825: step 9730, loss = 0.90 (956.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:37:45.627163: step 9740, loss = 0.76 (952.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:37:46.984533: step 9750, loss = 0.87 (943.0 examples/sec; 0.136 sec/batch)
2017-05-07 19:37:48.307675: step 9760, loss = 0.95 (967.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:37:49.636563: step 9770, loss = 0.77 (963.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:37:50.980893: step 9780, loss = 0.76 (952.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:37:52.319364: step 9790, loss = 0.87 (956.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:37:53.763366: step 9800, loss = 0.94 (886.4 examples/sec; 0.144 sec/batch)
2017-05-07 19:37:55.014755: step 9810, loss = 0.65 (1022.9 examples/sec; 0.125 sec/batch)
2017-05-07 19:37:56.351227: step 9820, loss = 0.85 (957.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:37:57.670719: step 9830, loss = 0.78 (970.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:37:59.038973: step 9840, loss = 0.84 (935.5 examples/sec; 0.137 sec/batch)
2017-05-07 19:38:00.386387: step 9850, loss = 0.75 (950.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:38:01.743118: step 9860, loss = 0.84 (943.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:38:03.095200: step 9870, loss = 0.79 (946.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:38:04.438376: step 9880, loss = 0.94 (953.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:38:05.766847: step 9890, loss = 0.96 (963.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:38:07.249056: step 9900, loss = 0.87 (863.6 examples/sec; 0.148 sec/batch)
2017-05-07 19:38:08.452937: step 9910, loss = 0.85 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-07 19:38:09.780164: step 9920, loss = 0.88 (964.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:38:11.124701: step 9930, loss = 0.87 (952.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:38:12.480425: step 9940, loss = 0.64 (944.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:38:13.833375: step 9950, loss = 0.87 (946.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:38:15.180604: step 9960, loss = 0.93 (950.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:38:16.528989: step 9970, loss = 0.96 (949.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:38:17.909565: step 9980, loss = 0.80 (927.1 examples/sec; 0.138 sec/batch)
2017-05-07 19:38:19.247067: step 9990, loss = 0.87 (957.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:38:20.687620: step 10000, loss = 0.83 (888.6 examples/sec; 0.144 sec/batch)
2017-05-07 19:38:21.905990: step 10010, loss = 0.82 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-07 19:38:23.253131: step 10020, loss = 0.72 (950.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:38:24.599970: step 10030, loss = 0.67 (950.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:38:25.964173: step 10040, loss = 0.83 (938.3 examples/sec; 0.136 sec/batch)
2017-05-07 19:38:27.312914: step 10050, loss = 0.90 (949.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:38:28.635783: step 10060, loss = 0.96 (967.6 examples/sec; 0.132 sec/batch)
2017-05-07 19:38:29.973268: step 10070, loss = 0.87 (957.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:38:31.352690: step 10080, loss = 0.77 (927.9 examples/sec; 0.138 sec/batch)
2017-05-07 19:38:32.679237: step 10090, loss = 0.75 (964.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:38:34.156896: step 10100, loss = 0.78 (866.2 examples/sec; 0.148 sec/batch)
2017-05-07 19:38:35.386754: step 10110, loss = 0.81 (1040.8 examples/sec; 0.123 sec/batch)
2017-05-07 19:38:36.732406: step 10120, loss = 0.78 (951.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:38:38.093338: step 10130, loss = 0.82 (940.5 examples/sec; 0.136 sec/batch)
2017-05-07 19:38:39.465848: step 10140, loss = 0.87 (932.6 examples/sec; 0.137 sec/batch)
2017-05-07 19:38:40.805149: step 10150, loss = 0.71 (955.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:38:42.143653: step 10160, loss = 0.84 (956.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:38:43.479769: step 10170, loss = 0.88 (958.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:38:44.809658: step 10180, loss = 0.87 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:38:46.164822: step 10190, loss = 0.88 (944.5 examples/sec; 0.136 sec/batch)
2017-05-07 19:38:47.630166: step 10200, loss = 1.06 (873.5 examples/sec; 0.147 sec/batch)
2017-05-07 19:38:48.862164: step 10210, loss = 0.87 (1039.0 examples/sec; 0.123 sec/batch)
2017-05-07 19:38:50.202933: step 10220, loss = 0.96 (954.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:38:51.549333: step 10230, loss = 0.85 (950.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:38:52.901978: step 10240, loss = 0.76 (946.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:38:54.257536: step 10250, loss = 0.73 (944.3 examples/sec; 0.136 sec/batch)
2017-05-07 19:38:55.587712: step 10260, loss = 0.90 (962.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:38:56.925012: step 10270, loss = 0.91 (957.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:38:58.288343: step 10280, loss = 0.82 (938.9 examples/sec; 0.136 sec/batch)
2017-05-07 19:38:59.623754: step 10290, loss = 0.77 (958.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:39:01.060082: step 10300, loss = 0.83 (891.2 examples/sec; 0.144 sec/batch)
2017-05-07 19:39:02.272789: step 10310, loss = 0.93 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-07 19:39:03.624680: step 10320, loss = 0.77 (946.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:39:04.960140: step 10330, loss = 0.86 (958.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:39:06.309873: step 10340, loss = 0.84 (948.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:39:07.666770: step 10350, loss = 0.76 (943.3 examples/sec; 0.136 sec/batch)
2017-05-07 19:39:09.050439: step 10360, loss = 0.85 (925.1 examples/sec; 0.138 sec/batch)
2017-05-07 19:39:10.405620: step 10370, loss = 1.01 (944.5 examples/sec; 0.136 sec/batch)
2017-05-07 19:39:11.737881: step 10380, loss = 0.80 (960.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:39:13.081346: step 10390, loss = 0.97 (952.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:39:14.505760: step 10400, loss = 0.84 (898.6 examples/sec; 0.142 sec/batch)
2017-05-07 19:39:15.722932: step 10410, loss = 0.74 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-07 19:39:17.078095: step 10420, loss = 0.84 (944.5 examples/sec; 0.136 sec/batch)
2017-05-07 19:39:18.405713: step 10430, loss = 0.96 (964.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:39:19.754129: step 10440, loss = 0.94 (949.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:39:21.081165: step 10450, loss = 0.88 (964.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:39:22.420866: step 10460, loss = 0.90 (955.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:39:23.757198: step 10470, loss = 0.89 (957.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:39:25.094230: step 10480, loss = 0.82 (957.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:39:26.455425: step 10490, loss = 0.86 (940.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:39:27.876635: step 10500, loss = 0.90 (900.6 examples/sec; 0.142 sec/batch)
2017-05-07 19:39:29.144125: step 10510, loss = 1.13 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-07 19:39:30.491201: step 10520, loss = 0.87 (950.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:39:31.836722: step 10530, loss = 0.85 (951.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:39:33.167772: step 10540, loss = 0.73 (961.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:39:34.538519: step 10550, loss = 0.85 (933.8 examples/sec; 0.137 sec/batch)
2017-05-07 19:39:35.857307: step 10560, loss = 0.86 (970.6 examples/sec; 0.132 sec/batch)
2017-05-07 19:39:37.196439: step 10570, loss = 1.13 (955.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:39:38.518211: step 10580, loss = 0.85 (968.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:39:39.860635: step 10590, loss = 0.79 (953.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:39:41.331613: step 10600, loss = 0.69 (870.2 examples/sec; 0.147 sec/batch)
2017-05-07 19:39:42.556327: step 10610, loss = 0.84 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-07 19:39:43.900528: step 10620, loss = 0.86 (952.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:39:45.231176: step 10630, loss = 0.99 (961.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:39:46.577240: step 10640, loss = 0.97 (950.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:39:47.926717: step 10650, loss = 0.80 (948.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:39:49.261150: step 10660, loss = 0.93 (959.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:39:50.610446: step 10670, loss = 0.88 (948.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:39:51.960811: step 10680, loss = 0.89 (947.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:39:53.303142: step 10690, loss = 0.89 (953.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:39:54.726737: step 10700, loss = 0.79 (899.1 examples/sec; 0.142 sec/batch)
2017-05-07 19:39:55.977156: step 10710, loss = 0.78 (1023.7 examples/sec; 0.125 sec/batch)
2017-05-07 19:39:57.310783: step 10720, loss = 0.98 (959.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:39:58.655846: step 10730, loss = 0.82 (951.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:40:00.015083: step 10740, loss = 0.86 (941.7 examples/sec; 0.136 sec/batch)
2017-05-07 19:40:01.350148: step 10750, loss = 0.74 (958.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:40:02.695877: step 10760, loss = 0.86 (951.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:40:04.049498: step 10770, loss = 0.83 (945.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:40:05.382772: step 10780, loss = 0.93 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:40:06.740127: step 10790, loss = 0.72 (943.0 examples/sec; 0.136 sec/batch)
2017-05-07 19:40:08.202648: step 10800, loss = 0.85 (875.2 examples/sec; 0.146 sec/batch)
2017-05-07 19:40:09.387272: step 10810, loss = 0.86 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-07 19:40:10.728954: step 10820, loss = 0.83 (954.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:40:12.075992: step 10830, loss = 0.90 (950.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:40:13.419702: step 10840, loss = 0.82 (952.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:40:14.757599: step 10850, loss = 0.96 (956.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:40:16.109877: step 10860, loss = 0.86 (946.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:40:17.470414: step 10870, loss = 0.82 (940.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:40:18.812894: step 10880, loss = 0.74 (953.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:40:20.177816: step 10890, loss = 0.67 (937.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:40:21.612734: step 10900, loss = 0.86 (892.0 examples/sec; 0.143 sec/batch)
2017-05-07 19:40:22.838796: step 10910, loss = 0.95 (1044.0 examples/sec; 0.123 sec/batch)
2017-05-07 19:40:24.196860: step 10920, loss = 0.69 (942.5 examples/sec; 0.136 sec/batch)
2017-05-07 19:40:25.523209: step 10930, loss = 0.83 (965.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:40:26.861266: step 10940, loss = 0.81 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:40:28.212766: step 10950, loss = 0.89 (947.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:40:29.557048: step 10960, loss = 0.90 (952.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:40:30.901660: step 10970, loss = 0.83 (952.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:40:32.252532: step 10980, loss = 0.85 (947.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:40:33.576605: step 10990, loss = 0.88 (966.7 examples/sec; 0.132 sec/batch)
2017-05-07 19:40:35.019878: step 11000, loss = 0.84 (886.9 examples/sec; 0.144 sec/batch)
2017-05-07 19:40:36.272425: step 11010, loss = 0.72 (1021.9 examples/sec; 0.125 sec/batch)
2017-05-07 19:40:37.620769: step 11020, loss = 0.68 (949.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:40:38.971081: step 11030, loss = 0.76 (947.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:40:40.296826: step 11040, loss = 0.92 (965.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:40:41.647362: step 11050, loss = 0.81 (947.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:40:42.972371: step 11060, loss = 0.75 (966.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:40:44.303905: step 11070, loss = 1.02 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:40:45.642439: step 11080, loss = 0.92 (956.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:40:46.994939: step 11090, loss = 0.92 (946.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:40:48.436668: step 11100, loss = 0.77 (887.8 examples/sec; 0.144 sec/batch)
2017-05-07 19:40:49.670919: step 11110, loss = 0.79 (1037.1 examples/sec; 0.123 sec/batch)
2017-05-07 19:40:51.013860: step 11120, loss = 0.89 (953.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:40:52.356823: step 11130, loss = 0.94 (953.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:40:53.688893: step 11140, loss = 0.93 (960.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:40:55.056576: step 11150, loss = 0.90 (935.9 examples/sec; 0.137 sec/batch)
2017-05-07 19:40:56.400717: step 11160, loss = 0.94 (952.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:40:57.738544: step 11170, loss = 0.74 (956.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:40:59.070181: step 11180, loss = 0.89 (961.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:41:00.404618: step 11190, loss = 0.79 (959.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:41:01.893369: step 11200, loss = 0.93 (859.8 examples/sec; 0.149 sec/batch)
2017-05-07 19:41:03.060790: step 11210, loss = 0.89 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-07 19:41:04.407681: step 11220, loss = 0.82 (950.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:41:05.754383: step 11230, loss = 1.09 (950.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:41:07.079300: step 11240, loss = 0.76 (966.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:41:08.437839: step 11250, loss = 0.98 (942.2 examples/sec; 0.136 sec/batch)
2017-05-07 19:41:09.765620: step 11260, loss = 0.81 (964.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:41:11.104279: step 11270, loss = 0.81 (956.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:41:12.446152: step 11280, loss = 0.82 (953.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:41:13.791260: step 11290, loss = 0.85 (951.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:41:15.227264: step 11300, loss = 0.82 (891.4 examples/sec; 0.144 sec/batch)
2017-05-07 19:41:16.479029: step 11310, loss = 0.89 (1022.6 examples/sec; 0.125 sec/batch)
2017-05-07 19:41:17.816777: step 11320, loss = 0.99 (956.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:41:19.140210: step 11330, loss = 0.86 (967.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:41:20.479614: step 11340, loss = 1.01 (955.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:41:21.839620: step 11350, loss = 0.82 (941.2 examples/sec; 0.136 sec/batch)
2017-05-07 19:41:23.186903: step 11360, loss = 0.90 (950.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:41:24.517356: step 11370, loss = 0.99 (962.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:41:25.862174: step 11380, loss = 0.83 (951.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:41:27.207761: step 11390, loss = 0.79 (951.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:41:28.664377: step 11400, loss = 0.91 (878.7 examples/sec; 0.146 sec/batch)
2017-05-07 19:41:29.866001: step 11410, loss = 0.82 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-07 19:41:31.203916: step 11420, loss = 0.88 (956.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:41:32.557061: step 11430, loss = 0.79 (945.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:41:33.929033: step 11440, loss = 0.73 (933.0 examples/sec; 0.137 sec/batch)
2017-05-07 19:41:35.273660: step 11450, loss = 0.83 (951.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:41:36.609663: step 11460, loss = 0.74 (958.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:41:37.947923: step 11470, loss = 0.76 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:41:39.294124: step 11480, loss = 0.84 (950.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:41:40.646247: step 11490, loss = 0.88 (946.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:41:42.064808: step 11500, loss = 0.83 (902.3 examples/sec; 0.142 sec/batch)
2017-05-07 19:41:43.313333: step 11510, loss = 0.75 (1025.2 examples/sec; 0.125 sec/batch)
2017-05-07 19:41:44.639219: step 11520, loss = 0.86 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:41:45.993278: step 11530, loss = 0.81 (945.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:41:47.331196: step 11540, loss = 0.67 (956.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:41:48.692215: step 11550, loss = 0.94 (940.5 examples/sec; 0.136 sec/batch)
2017-05-07 19:41:50.027124: step 11560, loss = 0.92 (958.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:41:51.375217: step 11570, loss = 0.92 (949.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:41:52.705074: step 11580, loss = 0.78 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:41:54.054691: step 11590, loss = 0.89 (948.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:41:55.526775: step 11600, loss = 0.88 (869.5 examples/sec; 0.147 sec/batch)
2017-05-07 19:41:56.759216: step 11610, loss = 0.85 (1038.6 examples/sec; 0.123 sec/batch)
2017-05-07 19:41:58.110424: step 11620, loss = 0.77 (947.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:41:59.447583: step 11630, loss = 0.70 (957.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:42:00.788360: step 11640, loss = 0.87 (954.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:42:02.143258: step 11650, loss = 0.77 (944.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:42:03.471431: step 11660, loss = 1.00 (963.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:42:04.802282: step 11670, loss = 0.94 (961.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:42:06.156833: step 11680, loss = 1.02 (945.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:42:07.509165: step 11690, loss = 0.81 (946.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:42:08.967662: step 11700, loss = 0.85 (877.6 examples/sec; 0.146 sec/batch)
2017-05-07 19:42:10.191379: step 11710, loss = 0.71 (1046.0 examples/sec; 0.122 sec/batch)
2017-05-07 19:42:11.548647: step 11720, loss = 0.74 (943.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:42:12.889606: step 11730, loss = 0.80 (954.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:42:14.232780: step 11740, loss = 0.83 (953.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:42:15.574302: step 11750, loss = 0.77 (954.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:42:16.906282: step 11760, loss = 0.96 (961.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:42:18.253441: step 11770, loss = 0.84 (950.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:42:19.611544: step 11780, loss = 0.93 (942.5 examples/sec; 0.136 sec/batch)
2017-05-07 19:42:20.946289: step 11790, loss = 0.75 (959.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:42:22.392479: step 11800, loss = 0.91 (885.1 examples/sec; 0.145 sec/batch)
2017-05-07 19:42:23.643736: step 11810, loss = 0.81 (1023.0 examples/sec; 0.125 sec/batch)
2017-05-07 19:42:24.968150: step 11820, loss = 0.71 (966.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:42:26.324940: step 11830, loss = 0.87 (943.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:42:27.669310: step 11840, loss = 0.85 (952.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:42:29.007933: step 11850, loss = 0.76 (956.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:42:30.341123: step 11860, loss = 0.87 (960.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:42:31.656733: step 11870, loss = 0.89 (973.0 examples/sec; 0.132 sec/batch)
2017-05-07 19:42:32.981531: step 11880, loss = 1.03 (966.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:42:34.313381: step 11890, loss = 0.81 (961.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:42:35.796723: step 11900, loss = 0.98 (862.9 examples/sec; 0.148 sec/batch)
2017-05-07 19:42:37.000290: step 11910, loss = 0.94 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-07 19:42:38.352500: step 11920, loss = 0.88 (946.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:42:39.673914: step 11930, loss = 0.84 (968.6 examples/sec; 0.132 sec/batch)
2017-05-07 19:42:40.993409: step 11940, loss = 0.80 (970.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:42:42.368261: step 11950, loss = 1.18 (931.0 examples/sec; 0.137 sec/batch)
2017-05-07 19:42:43.701229: step 11960, loss = 0.72 (960.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:42:45.028646: step 11970, loss = 0.90 (964.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:42:46.371431: step 11980, loss = 0.71 (953.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:42:47.718524: step 11990, loss = 0.81 (950.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:42:49.171037: step 12000, loss = 0.96 (881.2 examples/sec; 0.145 sec/batch)
2017-05-07 19:42:50.408490: step 12010, loss = 0.89 (1034.4 examples/sec; 0.124 sec/batch)
2017-05-07 19:42:51.739749: step 12020, loss = 0.80 (961.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:42:53.086123: step 12030, loss = 0.86 (950.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:42:54.409005: step 12040, loss = 0.84 (967.6 examples/sec; 0.132 sec/batch)
2017-05-07 19:42:55.751876: step 12050, loss = 0.97 (953.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:42:57.101290: step 12060, loss = 0.83 (948.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:42:58.429242: step 12070, loss = 0.84 (963.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:42:59.778866: step 12080, loss = 0.64 (948.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:43:01.126613: step 12090, loss = 0.92 (949.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:43:02.586041: step 12100, loss = 1.05 (877.1 examples/sec; 0.146 sec/batch)
2017-05-07 19:43:03.823504: step 12110, loss = 0.78 (1034.4 examples/sec; 0.124 sec/batch)
2017-05-07 19:43:05.151771: step 12120, loss = 0.93 (963.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:43:06.507934: step 12130, loss = 0.74 (943.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:43:07.825130: step 12140, loss = 1.03 (971.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:43:09.180950: step 12150, loss = 0.88 (944.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:43:10.527893: step 12160, loss = 0.93 (950.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:43:11.850607: step 12170, loss = 0.85 (967.7 examples/sec; 0.132 sec/batch)
2017-05-07 19:43:13.183366: step 12180, loss = 0.84 (960.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:43:14.538381: step 12190, loss = 0.80 (944.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:43:16.011552: step 12200, loss = 0.87 (868.9 examples/sec; 0.147 sec/batch)
2017-05-07 19:43:17.225909: step 12210, loss = 1.01 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-07 19:43:18.564078: step 12220, loss = 0.91 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:43:19.931507: step 12230, loss = 0.76 (936.1 examples/sec; 0.137 sec/batch)
2017-05-07 19:43:21.254144: step 12240, loss = 0.98 (967.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:43:22.597775: step 12250, loss = 0.91 (952.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:43:23.923628: step 12260, loss = 0.78 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:43:25.254101: step 12270, loss = 0.86 (962.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:43:26.570804: step 12280, loss = 0.95 (972.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:43:27.935792: step 12290, loss = 0.72 (937.7 examples/sec; 0.136 sec/batch)
2017-05-07 19:43:29.359981: step 12300, loss = 0.83 (898.8 examples/sec; 0.142 sec/batch)
2017-05-07 19:43:30.627640: step 12310, loss = 0.90 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-07 19:43:31.961431: step 12320, loss = 0.89 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:43:33.292652: step 12330, loss = 0.95 (961.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:43:34.596064: step 12340, loss = 0.87 (982.0 examples/sec; 0.130 sec/batch)
2017-05-07 19:43:35.950272: step 12350, loss = 0.65 (945.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:43:37.280319: step 12360, loss = 1.03 (962.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:43:38.607250: step 12370, loss = 0.85 (964.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:43:39.959949: step 12380, loss = 0.93 (946.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:43:41.311676: step 12390, loss = 0.77 (946.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:43:42.757510: step 12400, loss = 1.04 (885.3 examples/sec; 0.145 sec/batch)
2017-05-07 19:43:44.014390: step 12410, loss = 0.87 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-07 19:43:45.339219: step 12420, loss = 0.85 (966.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:43:46.681372: step 12430, loss = 1.00 (953.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:43:48.015404: step 12440, loss = 0.80 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:43:49.371715: step 12450, loss = 0.78 (943.7 examples/sec; 0.136 sec/batch)
2017-05-07 19:43:50.701520: step 12460, loss = 0.89 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:43:52.035796: step 12470, loss = 0.97 (959.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:43:53.344558: step 12480, loss = 0.85 (978.0 examples/sec; 0.131 sec/batch)
2017-05-07 19:43:54.684365: step 12490, loss = 0.75 (955.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:43:56.132664: step 12500, loss = 0.75 (883.8 examples/sec; 0.145 sec/batch)
2017-05-07 19:43:57.380788: step 12510, loss = 0.83 (1025.5 examples/sec; 0.125 sec/batch)
2017-05-07 19:43:58.726061: step 12520, loss = 0.82 (951.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:44:00.076496: step 12530, loss = 0.84 (947.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:44:01.392133: step 12540, loss = 0.83 (972.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:44:02.754188: step 12550, loss = 0.77 (939.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:44:04.084998: step 12560, loss = 0.83 (961.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:44:05.406637: step 12570, loss = 0.82 (968.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:44:06.742572: step 12580, loss = 0.88 (958.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:44:08.083823: step 12590, loss = 0.65 (954.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:44:09.510717: step 12600, loss = 0.67 (897.1 examples/sec; 0.143 sec/batch)
2017-05-07 19:44:10.727879: step 12610, loss = 0.90 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-07 19:44:12.054315: step 12620, loss = 0.83 (965.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:44:13.390346: step 12630, loss = 0.75 (958.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:44:14.744967: step 12640, loss = 1.05 (944.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:44:16.099198: step 12650, loss = 0.81 (945.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:44:17.442203: step 12660, loss = 0.79 (953.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:44:18.782329: step 12670, loss = 0.82 (955.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:44:20.107525: step 12680, loss = 0.76 (965.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:44:21.449819: step 12690, loss = 0.86 (953.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:44:22.885995: step 12700, loss = 0.71 (891.3 examples/sec; 0.144 sec/batch)
2017-05-07 19:44:24.118200: step 12710, loss = 0.85 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-07 19:44:25.463939: step 12720, loss = 0.65 (951.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:44:26.802987: step 12730, loss = 0.79 (955.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:44:28.146150: step 12740, loss = 0.92 (953.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:44:29.485611: step 12750, loss = 1.15 (955.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:44:30.826780: step 12760, loss = 0.79 (954.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:44:32.149991: step 12770, loss = 0.85 (967.3 examples/sec; 0.132 sec/batch)
2017-05-07 19:44:33.486174: step 12780, loss = 0.79 (958.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:44:34.813511: step 12790, loss = 0.80 (964.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:44:36.258541: step 12800, loss = 0.78 (885.8 examples/sec; 0.145 sec/batch)
2017-05-07 19:44:37.478024: step 12810, loss = 0.87 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-07 19:44:38.822131: step 12820, loss = 0.72 (952.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:44:40.160128: step 12830, loss = 0.98 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:44:41.488880: step 12840, loss = 0.80 (963.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:44:42.855311: step 12850, loss = 0.70 (936.7 examples/sec; 0.137 sec/batch)
2017-05-07 19:44:44.179111: step 12860, loss = 0.78 (966.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:44:45.499125: step 12870, loss = 0.96 (969.7 examples/sec; 0.132 sec/batch)
2017-05-07 19:44:46.830569: step 12880, loss = 0.88 (961.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:44:48.166354: step 12890, loss = 0.98 (958.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:44:49.586357: step 12900, loss = 0.91 (901.4 examples/sec; 0.142 sec/batch)
2017-05-07 19:44:50.839023: step 12910, loss = 0.83 (1021.8 examples/sec; 0.125 sec/batch)
2017-05-07 19:44:52.177146: step 12920, loss = 0.70 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:44:53.511332: step 12930, loss = 0.90 (959.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:44:54.852844: step 12940, loss = 0.94 (954.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:44:56.196296: step 12950, loss = 0.69 (952.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:44:57.520679: step 12960, loss = 0.94 (966.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:44:58.844621: step 12970, loss = 0.68 (966.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:45:00.173039: step 12980, loss = 0.70 (963.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:45:01.507202: step 12990, loss = 0.92 (959.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:45:02.947220: step 13000, loss = 0.80 (888.9 examples/sec; 0.144 sec/batch)
2017-05-07 19:45:04.184810: step 13010, loss = 0.94 (1034.3 examples/sec; 0.124 sec/batch)
2017-05-07 19:45:05.507115: step 13020, loss = 0.82 (968.0 examples/sec; 0.132 sec/batch)
2017-05-07 19:45:06.839545: step 13030, loss = 0.76 (960.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:45:08.179939: step 13040, loss = 0.76 (954.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:45:09.518434: step 13050, loss = 0.73 (956.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:45:10.868518: step 13060, loss = 0.84 (948.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:45:12.221240: step 13070, loss = 0.82 (946.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:45:13.559230: step 13080, loss = 0.85 (956.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:45:14.903596: step 13090, loss = 0.82 (952.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:45:16.370909: step 13100, loss = 1.03 (872.3 examples/sec; 0.147 sec/batch)
2017-05-07 19:45:17.598978: step 13110, loss = 0.87 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-07 19:45:18.926090: step 13120, loss = 0.87 (964.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:45:20.280503: step 13130, loss = 0.87 (945.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:45:21.608062: step 13140, loss = 0.99 (964.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:45:22.945944: step 13150, loss = 0.75 (956.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:45:24.268685: step 13160, loss = 0.84 (967.7 examples/sec; 0.132 sec/batch)
2017-05-07 19:45:25.597276: step 13170, loss = 0.86 (963.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:45:26.930657: step 13180, loss = 0.99 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:45:28.265631: step 13190, loss = 0.97 (958.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:45:29.708563: step 13200, loss = 0.79 (887.1 examples/sec; 0.144 sec/batch)
2017-05-07 19:45:30.959020: step 13210, loss = 0.95 (1023.6 examples/sec; 0.125 sec/batch)
2017-05-07 19:45:32.309745: step 13220, loss = 0.76 (947.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:45:33.641648: step 13230, loss = 0.83 (961.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:45:35.001432: step 13240, loss = 0.65 (941.3 examples/sec; 0.136 sec/batch)
2017-05-07 19:45:36.359095: step 13250, loss = 0.73 (942.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:45:37.692273: step 13260, loss = 0.79 (960.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:45:39.016143: step 13270, loss = 0.84 (966.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:45:40.346574: step 13280, loss = 0.93 (962.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:45:41.676041: step 13290, loss = 0.75 (962.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:45:43.114539: step 13300, loss = 0.81 (889.8 examples/sec; 0.144 sec/batch)
2017-05-07 19:45:44.345246: step 13310, loss = 0.96 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-07 19:45:45.684436: step 13320, loss = 0.83 (955.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:45:47.009163: step 13330, loss = 0.83 (966.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:45:48.348297: step 13340, loss = 0.84 (955.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:45:49.655030: step 13350, loss = 0.87 (979.5 examples/sec; 0.131 sec/batch)
2017-05-07 19:45:51.017289: step 13360, loss = 0.70 (939.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:45:52.331025: step 13370, loss = 0.80 (974.3 examples/sec; 0.131 sec/batch)
2017-05-07 19:45:53.672365: step 13380, loss = 0.72 (954.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:45:55.012991: step 13390, loss = 0.80 (954.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:45:56.485788: step 13400, loss = 0.96 (869.1 examples/sec; 0.147 sec/batch)
2017-05-07 19:45:57.757821: step 13410, loss = 0.88 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-07 19:45:59.089664: step 13420, loss = 1.07 (961.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:46:00.421678: step 13430, loss = 1.04 (961.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:46:01.770923: step 13440, loss = 0.74 (948.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:46:03.116788: step 13450, loss = 0.83 (951.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:46:04.418777: step 13460, loss = 0.79 (983.1 examples/sec; 0.130 sec/batch)
2017-05-07 19:46:05.761671: step 13470, loss = 0.96 (953.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:46:07.102591: step 13480, loss = 0.81 (954.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:46:08.430890: step 13490, loss = 0.80 (963.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:46:09.874066: step 13500, loss = 0.78 (886.9 examples/sec; 0.144 sec/batch)
2017-05-07 19:46:11.090200: step 13510, loss = 1.06 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-07 19:46:12.422881: step 13520, loss = 0.88 (960.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:46:13.766355: step 13530, loss = 0.78 (952.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:46:15.111923: step 13540, loss = 0.97 (951.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:46:16.463211: step 13550, loss = 0.87 (947.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:46:17.808157: step 13560, loss = 0.99 (951.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:46:19.166111: step 13570, loss = 0.85 (942.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:46:20.493933: step 13580, loss = 0.84 (964.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:46:21.853036: step 13590, loss = 0.80 (941.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:46:23.331170: step 13600, loss = 1.06 (866.0 examples/sec; 0.148 sec/batch)
2017-05-07 19:46:24.512924: step 13610, loss = 0.82 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-07 19:46:25.823676: step 13620, loss = 1.01 (976.5 examples/sec; 0.131 sec/batch)
2017-05-07 19:46:27.157747: step 13630, loss = 1.13 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:46:28.507250: step 13640, loss = 0.78 (948.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:46:29.850314: step 13650, loss = 0.81 (953.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:46:31.171716: step 13660, loss = 0.81 (968.7 examples/sec; 0.132 sec/batch)
2017-05-07 19:46:32.503572: step 13670, loss = 0.88 (961.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:46:33.823580: step 13680, loss = 1.03 (969.7 examples/sec; 0.132 sec/batch)
2017-05-07 19:46:35.162892: step 13690, loss = 0.83 (955.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:46:36.582389: step 13700, loss = 0.78 (901.7 examples/sec; 0.142 sec/batch)
2017-05-07 19:46:37.807076: step 13710, loss = 0.74 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-07 19:46:39.128678: step 13720, loss = 0.91 (968.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:46:40.472847: step 13730, loss = 0.68 (952.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:46:41.799978: step 13740, loss = 0.76 (964.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:46:43.154843: step 13750, loss = 0.77 (944.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:46:44.507340: step 13760, loss = 0.87 (946.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:46:45.843163: step 13770, loss = 1.08 (958.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:46:47.180803: step 13780, loss = 0.68 (956.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:46:48.511772: step 13790, loss = 0.79 (961.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:46:49.941132: step 13800, loss = 0.93 (895.5 examples/sec; 0.143 sec/batch)
2017-05-07 19:46:51.198709: step 13810, loss = 0.73 (1017.8 examples/sec; 0.126 sec/batch)
2017-05-07 19:46:52.571361: step 13820, loss = 0.74 (932.5 examples/sec; 0.137 sec/batch)
2017-05-07 19:46:53.910208: step 13830, loss = 0.86 (956.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:46:55.255729: step 13840, loss = 0.78 (951.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:46:56.598809: step 13850, loss = 0.78 (953.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:46:57.951544: step 13860, loss = 0.79 (946.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:46:59.289292: step 13870, loss = 1.00 (956.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:47:00.632834: step 13880, loss = 0.89 (952.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:47:01.956319: step 13890, loss = 0.89 (967.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:47:03.404573: step 13900, loss = 0.93 (883.8 examples/sec; 0.145 sec/batch)
2017-05-07 19:47:04.643109: step 13910, loss = 0.78 (1033.5 examples/sec; 0.124 sec/batch)
2017-05-07 19:47:05.976164: step 13920, loss = 0.90 (960.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:47:07.322056: step 13930, loss = 0.77 (951.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:47:08.659783: step 13940, loss = 0.81 (956.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:47:10.001090: step 13950, loss = 0.83 (954.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:47:11.341592: step 13960, loss = 0.89 (954.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:47:12.689491: step 13970, loss = 0.79 (949.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:47:14.034390: step 13980, loss = 0.86 (951.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:47:15.338788: step 13990, loss = 0.85 (981.3 examples/sec; 0.130 sec/batch)
2017-05-07 19:47:16.754253: step 14000, loss = 0.85 (904.3 examples/sec; 0.142 sec/batch)
2017-05-07 19:47:17.995841: step 14010, loss = 0.86 (1030.9 examples/sec; 0.124 sec/batch)
2017-05-07 19:47:19.340084: step 14020, loss = 0.94 (952.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:47:20.673445: step 14030, loss = 0.72 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:47:21.993818: step 14040, loss = 0.79 (969.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:47:23.342482: step 14050, loss = 0.84 (949.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:47:24.652881: step 14060, loss = 0.72 (976.8 examples/sec; 0.131 sec/batch)
2017-05-07 19:47:26.003000: step 14070, loss = 0.80 (948.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:47:27.345729: step 14080, loss = 0.92 (953.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:47:28.666410: step 14090, loss = 0.84 (969.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:47:30.116154: step 14100, loss = 1.00 (882.9 examples/sec; 0.145 sec/batch)
2017-05-07 19:47:31.330698: step 14110, loss = 0.80 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-07 19:47:32.660259: step 14120, loss = 0.74 (962.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:47:34.004369: step 14130, loss = 0.84 (952.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:47:35.345919: step 14140, loss = 0.90 (954.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:47:36.686607: step 14150, loss = 0.85 (954.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:47:38.023117: step 14160, loss = 0.87 (957.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:47:39.367266: step 14170, loss = 0.82 (952.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:47:40.684239: step 14180, loss = 0.80 (971.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:47:42.020337: step 14190, loss = 0.70 (958.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:47:43.454403: step 14200, loss = 0.81 (892.6 examples/sec; 0.143 sec/batch)
2017-05-07 19:47:44.718360: step 14210, loss = 0.96 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-07 19:47:46.032245: step 14220, loss = 0.92 (974.2 examples/sec; 0.131 sec/batch)
2017-05-07 19:47:47.371718: step 14230, loss = 0.78 (955.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:47:48.694264: step 14240, loss = 0.87 (967.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:47:50.035272: step 14250, loss = 0.91 (954.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:47:51.384527: step 14260, loss = 0.88 (948.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:47:52.700807: step 14270, loss = 0.77 (972.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:47:54.043120: step 14280, loss = 0.75 (953.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:47:55.373106: step 14290, loss = 0.61 (962.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:47:56.818917: step 14300, loss = 0.89 (885.3 examples/sec; 0.145 sec/batch)
2017-05-07 19:47:58.036886: step 14310, loss = 0.69 (1050.9 examples/sec; 0.122 sec/batch)
2017-05-07 19:47:59.375947: step 14320, loss = 0.82 (955.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:48:00.706900: step 14330, loss = 0.86 (961.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:48:02.033571: step 14340, loss = 1.07 (964.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:48:03.396796: step 14350, loss = 0.90 (938.9 examples/sec; 0.136 sec/batch)
2017-05-07 19:48:04.700504: step 14360, loss = 0.90 (981.9 examples/sec; 0.130 sec/batch)
2017-05-07 19:48:06.034915: step 14370, loss = 0.86 (959.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:48:07.373260: step 14380, loss = 0.82 (956.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:48:08.699519: step 14390, loss = 0.83 (965.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:48:10.146506: step 14400, loss = 0.99 (884.6 examples/sec; 0.145 sec/batch)
2017-05-07 19:48:11.391573: step 14410, loss = 0.70 (1028.1 examples/sec; 0.125 sec/batch)
2017-05-07 19:48:12.737975: step 14420, loss = 0.86 (950.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:48:14.072054: step 14430, loss = 0.89 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:48:15.403920: step 14440, loss = 1.04 (961.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:48:16.738271: step 14450, loss = 0.92 (959.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:48:18.055207: step 14460, loss = 0.87 (971.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:48:19.392155: step 14470, loss = 0.88 (957.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:48:20.708861: step 14480, loss = 0.79 (972.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:48:22.043417: step 14490, loss = 0.83 (959.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:48:23.484969: step 14500, loss = 1.02 (887.9 examples/sec; 0.144 sec/batch)
2017-05-07 19:48:24.720432: step 14510, loss = 0.81 (1036.1 examples/sec; 0.124 sec/batch)
2017-05-07 19:48:26.051048: step 14520, loss = 0.79 (962.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:48:27.378151: step 14530, loss = 0.83 (964.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:48:28.699393: step 14540, loss = 0.87 (968.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:48:30.054560: step 14550, loss = 0.70 (944.5 examples/sec; 0.136 sec/batch)
2017-05-07 19:48:31.423863: step 14560, loss = 0.86 (934.8 examples/sec; 0.137 sec/batch)
2017-05-07 19:48:32.771878: step 14570, loss = 1.22 (949.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:48:34.118565: step 14580, loss = 0.86 (950.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:48:35.468315: step 14590, loss = 0.81 (948.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:48:36.907828: step 14600, loss = 0.81 (889.2 examples/sec; 0.144 sec/batch)
2017-05-07 19:48:38.161931: step 14610, loss = 1.01 (1020.6 examples/sec; 0.125 sec/batch)
2017-05-07 19:48:39.480328: step 14620, loss = 0.62 (970.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:48:40.812719: step 14630, loss = 0.79 (960.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:48:42.141012: step 14640, loss = 0.72 (963.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:48:43.493057: step 14650, loss = 0.83 (946.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:48:44.819177: step 14660, loss = 0.91 (965.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:48:46.140780: step 14670, loss = 0.82 (968.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:48:47.469219: step 14680, loss = 0.78 (963.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:48:48.794004: step 14690, loss = 0.84 (966.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:48:50.226066: step 14700, loss = 0.70 (893.8 examples/sec; 0.143 sec/batch)
2017-05-07 19:48:51.472485: step 14710, loss = 0.96 (1027.0 examples/sec; 0.125 sec/batch)
2017-05-07 19:48:52.787353: step 14720, loss = 0.91 (973.5 examples/sec; 0.131 sec/batch)
2017-05-07 19:48:54.129100: step 14730, loss = 0.92 (954.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:48:55.462973: step 14740, loss = 0.84 (959.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:48:56.802975: step 14750, loss = 0.79 (955.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:48:58.131813: step 14760, loss = 0.77 (963.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:48:59.468734: step 14770, loss = 0.79 (957.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:49:00.816492: step 14780, loss = 0.73 (949.8 examples/sec; 0.135 sec/batch)
2017-05-07 19:49:02.147548: step 14790, loss = 0.62 (961.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:49:03.649688: step 14800, loss = 0.75 (852.1 examples/sec; 0.150 sec/batch)
2017-05-07 19:49:04.834762: step 14810, loss = 0.90 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-07 19:49:06.190179: step 14820, loss = 0.83 (944.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:49:07.529523: step 14830, loss = 0.81 (955.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:49:08.863764: step 14840, loss = 0.91 (959.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:49:10.221556: step 14850, loss = 0.76 (942.7 examples/sec; 0.136 sec/batch)
2017-05-07 19:49:11.569881: step 14860, loss = 0.83 (949.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:49:12.913715: step 14870, loss = 0.69 (952.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:49:14.233964: step 14880, loss = 0.82 (969.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:49:15.571855: step 14890, loss = 0.81 (956.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:49:17.004628: step 14900, loss = 0.90 (893.4 examples/sec; 0.143 sec/batch)
2017-05-07 19:49:18.249875: step 14910, loss = 0.80 (1027.9 examples/sec; 0.125 sec/batch)
2017-05-07 19:49:19.564297: step 14920, loss = 0.80 (973.8 examples/sec; 0.131 sec/batch)
2017-05-07 19:49:20.887096: step 14930, loss = 0.66 (967.6 examples/sec; 0.132 sec/batch)
2017-05-07 19:49:22.208166: step 14940, loss = 0.74 (968.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:49:23.555184: step 14950, loss = 0.97 (950.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:49:24.891264: step 14960, loss = 0.78 (958.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:49:26.219872: step 14970, loss = 0.69 (963.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:49:27.562917: step 14980, loss = 0.75 (953.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:49:28.912792: step 14990, loss = 0.65 (948.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:49:30.355213: step 15000, loss = 0.88 (887.4 examples/sec; 0.144 sec/batch)
2017-05-07 19:49:31.595380: step 15010, loss = 0.82 (1032.1 examples/sec; 0.124 sec/batch)
2017-05-07 19:49:32.923726: step 15020, loss = 0.78 (963.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:49:34.282906: step 15030, loss = 0.89 (941.7 examples/sec; 0.136 sec/batch)
2017-05-07 19:49:35.601783: step 15040, loss = 0.70 (970.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:49:36.946649: step 15050, loss = 0.80 (951.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:49:38.273523: step 15060, loss = 0.86 (964.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:49:39.615425: step 15070, loss = 0.95 (953.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:49:40.936994: step 15080, loss = 0.80 (968.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:49:42.269273: step 15090, loss = 0.73 (960.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:49:43.706335: step 15100, loss = 0.90 (890.7 examples/sec; 0.144 sec/batch)
2017-05-07 19:49:44.943841: step 15110, loss = 0.81 (1034.3 examples/sec; 0.124 sec/batch)
2017-05-07 19:49:46.266260: step 15120, loss = 0.77 (967.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:49:47.583273: step 15130, loss = 0.89 (971.9 examples/sec; 0.132 sec/batch)
2017-05-07 19:49:48.897959: step 15140, loss = 0.93 (973.6 examples/sec; 0.131 sec/batch)
2017-05-07 19:49:50.259985: step 15150, loss = 0.92 (939.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:49:51.589628: step 15160, loss = 0.74 (962.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:49:52.915486: step 15170, loss = 0.92 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:49:54.276885: step 15180, loss = 0.86 (940.2 examples/sec; 0.136 sec/batch)
2017-05-07 19:49:55.618663: step 15190, loss = 0.86 (954.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:49:57.054892: step 15200, loss = 0.95 (891.2 examples/sec; 0.144 sec/batch)
2017-05-07 19:49:58.315760: step 15210, loss = 0.82 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-07 19:49:59.654448: step 15220, loss = 0.86 (956.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:50:00.976401: step 15230, loss = 0.81 (968.3 examples/sec; 0.132 sec/batch)
2017-05-07 19:50:02.331835: step 15240, loss = 1.08 (944.3 examples/sec; 0.136 sec/batch)
2017-05-07 19:50:03.704956: step 15250, loss = 0.88 (932.2 examples/sec; 0.137 sec/batch)
2017-05-07 19:50:04.987919: step 15260, loss = 0.80 (997.7 examples/sec; 0.128 sec/batch)
2017-05-07 19:50:06.358980: step 15270, loss = 0.88 (933.6 examples/sec; 0.137 sec/batch)
2017-05-07 19:50:07.710042: step 15280, loss = 0.74 (947.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:50:09.043247: step 15290, loss = 0.79 (960.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:50:10.512338: step 15300, loss = 0.98 (871.3 examples/sec; 0.147 sec/batch)
2017-05-07 19:50:11.760175: step 15310, loss = 0.67 (1025.8 examples/sec; 0.125 sec/batch)
2017-05-07 19:50:13.057135: step 15320, loss = 0.89 (986.9 examples/sec; 0.130 sec/batch)
2017-05-07 19:50:14.394788: step 15330, loss = 0.80 (956.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:50:15.728291: step 15340, loss = 0.82 (959.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:50:17.085671: step 15350, loss = 0.78 (943.0 examples/sec; 0.136 sec/batch)
2017-05-07 19:50:18.397865: step 15360, loss = 0.82 (975.5 examples/sec; 0.131 sec/batch)
2017-05-07 19:50:19.745931: step 15370, loss = 0.94 (949.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:50:21.066083: step 15380, loss = 0.88 (969.6 examples/sec; 0.132 sec/batch)
2017-05-07 19:50:22.372271: step 15390, loss = 0.91 (980.0 examples/sec; 0.131 sec/batch)
2017-05-07 19:50:23.853837: step 15400, loss = 0.83 (864.0 examples/sec; 0.148 sec/batch)
2017-05-07 19:50:25.058025: step 15410, loss = 0.75 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-07 19:50:26.391812: step 15420, loss = 0.75 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:50:27.722193: step 15430, loss = 0.78 (962.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:50:29.059313: step 15440, loss = 0.89 (957.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:50:30.443369: step 15450, loss = 0.78 (924.8 examples/sec; 0.138 sec/batch)
2017-05-07 19:50:31.720096: step 15460, loss = 0.81 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-07 19:50:33.064371: step 15470, loss = 0.81 (952.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:50:34.404182: step 15480, loss = 0.74 (955.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:50:35.715242: step 15490, loss = 0.89 (976.3 examples/sec; 0.131 sec/batch)
2017-05-07 19:50:37.177974: step 15500, loss = 0.79 (875.1 examples/sec; 0.146 sec/batch)
2017-05-07 19:50:38.369539: step 15510, loss = 0.71 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-07 19:50:39.699138: step 15520, loss = 0.89 (962.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:50:41.015876: step 15530, loss = 0.75 (972.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:50:42.351963: step 15540, loss = 0.86 (958.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:50:43.737260: step 15550, loss = 0.96 (924.0 examples/sec; 0.139 sec/batch)
2017-05-07 19:50:45.015596: step 15560, loss = 0.75 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-07 19:50:46.354225: step 15570, loss = 1.10 (956.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:50:47.705052: step 15580, loss = 0.78 (947.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:50:49.013158: step 15590, loss = 0.78 (978.5 examples/sec; 0.131 sec/batch)
2017-05-07 19:50:50.500321: step 15600, loss = 0.94 (860.7 examples/sec; 0.149 sec/batch)
2017-05-07 19:50:51.723660: step 15610, loss = 0.80 (1046.3 examples/sec; 0.122 sec/batch)
2017-05-07 19:50:53.048075: step 15620, loss = 0.87 (966.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:50:54.399167: step 15630, loss = 0.76 (947.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:50:55.744504: step 15640, loss = 0.89 (951.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:50:57.122328: step 15650, loss = 0.75 (929.0 examples/sec; 0.138 sec/batch)
2017-05-07 19:50:58.402916: step 15660, loss = 1.07 (999.5 examples/sec; 0.128 sec/batch)
2017-05-07 19:50:59.745239: step 15670, loss = 0.87 (953.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:51:01.067040: step 15680, loss = 0.86 (968.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:51:02.417331: step 15690, loss = 0.97 (947.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:51:03.880097: step 15700, loss = 0.92 (875.1 examples/sec; 0.146 sec/batch)
2017-05-07 19:51:05.057334: step 15710, loss = 0.67 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-07 19:51:06.382225: step 15720, loss = 0.84 (966.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:51:07.735760: step 15730, loss = 1.06 (945.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:51:09.030157: step 15740, loss = 0.93 (988.9 examples/sec; 0.129 sec/batch)
2017-05-07 19:51:10.415106: step 15750, loss = 0.78 (924.2 examples/sec; 0.138 sec/batch)
2017-05-07 19:51:11.696237: step 15760, loss = 0.81 (999.1 examples/sec; 0.128 sec/batch)
2017-05-07 19:51:13.010833: step 15770, loss = 0.82 (973.7 examples/sec; 0.131 sec/batch)
2017-05-07 19:51:14.339311: step 15780, loss = 0.77 (963.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:51:15.664757: step 15790, loss = 0.85 (965.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:51:17.148021: step 15800, loss = 0.78 (863.0 examples/sec; 0.148 sec/batch)
2017-05-07 19:51:18.365937: step 15810, loss = 0.79 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-07 19:51:19.693114: step 15820, loss = 0.76 (964.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:51:21.025249: step 15830, loss = 0.96 (960.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:51:22.385355: step 15840, loss = 0.98 (941.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:51:23.781725: step 15850, loss = 0.69 (916.7 examples/sec; 0.140 sec/batch)
2017-05-07 19:51:25.069106: step 15860, loss = 0.80 (994.3 examples/sec; 0.129 sec/batch)
2017-05-07 19:51:26.394653: step 15870, loss = 0.79 (965.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:51:27.734146: step 15880, loss = 0.77 (955.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:51:29.078609: step 15890, loss = 0.92 (952.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:51:30.560822: step 15900, loss = 0.79 (863.6 examples/sec; 0.148 sec/batch)
2017-05-07 19:51:31.757415: step 15910, loss = 0.77 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-07 19:51:33.086407: step 15920, loss = 0.73 (963.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:51:34.398374: step 15930, loss = 0.75 (975.6 examples/sec; 0.131 sec/batch)
2017-05-07 19:51:35.715579: step 15940, loss = 1.10 (971.7 examples/sec; 0.132 sec/batch)
2017-05-07 19:51:37.086449: step 15950, loss = 0.70 (933.7 examples/sec; 0.137 sec/batch)
2017-05-07 19:51:38.383160: step 15960, loss = 0.91 (987.1 examples/sec; 0.130 sec/batch)
2017-05-07 19:51:39.711805: step 15970, loss = 0.91 (963.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:51:41.055105: step 15980, loss = 0.95 (952.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:51:42.382295: step 15990, loss = 0.68 (964.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:51:43.895754: step 16000, loss = 0.81 (845.7 examples/sec; 0.151 sec/batch)
2017-05-07 19:51:45.087573: step 16010, loss = 1.01 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-07 19:51:46.448307: step 16020, loss = 0.85 (940.7 examples/sec; 0.136 sec/batch)
2017-05-07 19:51:47.777045: step 16030, loss = 0.83 (963.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:51:49.117428: step 16040, loss = 0.81 (954.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:51:50.489180: step 16050, loss = 0.79 (933.1 examples/sec; 0.137 sec/batch)
2017-05-07 19:51:51.780934: step 16060, loss = 0.82 (990.9 examples/sec; 0.129 sec/batch)
2017-05-07 19:51:53.119654: step 16070, loss = 0.71 (956.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:51:54.459939: step 16080, loss = 0.68 (955.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:51:55.789847: step 16090, loss = 0.77 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:51:57.255894: step 16100, loss = 1.07 (873.1 examples/sec; 0.147 sec/batch)
2017-05-07 19:51:58.450709: step 16110, loss = 0.88 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-07 19:51:59.787767: step 16120, loss = 1.00 (957.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:52:01.128998: step 16130, loss = 0.79 (954.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:52:02.466490: step 16140, loss = 0.74 (957.1 examples/sec; 0.134 sec/batch)
2017-05-07 19:52:03.873658: step 16150, loss = 0.72 (909.6 examples/sec; 0.141 sec/batch)
2017-05-07 19:52:05.169601: step 16160, loss = 0.90 (987.7 examples/sec; 0.130 sec/batch)
2017-05-07 19:52:06.472056: step 16170, loss = 0.72 (982.8 examples/sec; 0.130 sec/batch)
2017-05-07 19:52:07.834064: step 16180, loss = 0.84 (939.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:52:09.153304: step 16190, loss = 0.89 (970.3 examples/sec; 0.132 sec/batch)
2017-05-07 19:52:10.642881: step 16200, loss = 0.81 (859.3 examples/sec; 0.149 sec/batch)
2017-05-07 19:52:11.880930: step 16210, loss = 0.77 (1033.9 examples/sec; 0.124 sec/batch)
2017-05-07 19:52:13.217144: step 16220, loss = 0.87 (957.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:52:14.544235: step 16230, loss = 0.87 (964.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:52:15.895632: step 16240, loss = 0.74 (947.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:52:17.274692: step 16250, loss = 0.76 (928.2 examples/sec; 0.138 sec/batch)
2017-05-07 19:52:18.565561: step 16260, loss = 0.89 (991.6 examples/sec; 0.129 sec/batch)
2017-05-07 19:52:19.891710: step 16270, loss = 0.85 (965.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:52:21.198589: step 16280, loss = 0.87 (979.4 examples/sec; 0.131 sec/batch)
2017-05-07 19:52:22.548446: step 16290, loss = 0.91 (948.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:52:24.018505: step 16300, loss = 0.78 (870.7 examples/sec; 0.147 sec/batch)
2017-05-07 19:52:25.219518: step 16310, loss = 1.00 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-07 19:52:26.575904: step 16320, loss = 0.88 (943.7 examples/sec; 0.136 sec/batch)
2017-05-07 19:52:27.911697: step 16330, loss = 0.80 (958.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:52:29.253853: step 16340, loss = 0.79 (953.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:52:30.627295: step 16350, loss = 0.88 (932.0 examples/sec; 0.137 sec/batch)
2017-05-07 19:52:31.915023: step 16360, loss = 0.78 (994.0 examples/sec; 0.129 sec/batch)
2017-05-07 19:52:33.258588: step 16370, loss = 0.92 (952.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:52:34.596537: step 16380, loss = 0.79 (956.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:52:35.934677: step 16390, loss = 0.76 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:52:37.424012: step 16400, loss = 0.80 (859.4 examples/sec; 0.149 sec/batch)
2017-05-07 19:52:38.643755: step 16410, loss = 0.81 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-07 19:52:39.960217: step 16420, loss = 0.80 (972.3 examples/sec; 0.132 sec/batch)
2017-05-07 19:52:41.316686: step 16430, loss = 0.80 (943.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:52:42.660390: step 16440, loss = 0.86 (952.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:52:44.058173: step 16450, loss = 0.97 (915.7 examples/sec; 0.140 sec/batch)
2017-05-07 19:52:45.353130: step 16460, loss = 0.80 (988.4 examples/sec; 0.129 sec/batch)
2017-05-07 19:52:46.692737: step 16470, loss = 0.77 (955.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:52:47.997945: step 16480, loss = 0.75 (980.7 examples/sec; 0.131 sec/batch)
2017-05-07 19:52:49.326302: step 16490, loss = 0.93 (963.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:52:50.829265: step 16500, loss = 0.79 (851.7 examples/sec; 0.150 sec/batch)
2017-05-07 19:52:51.998916: step 16510, loss = 0.76 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-07 19:52:53.322085: step 16520, loss = 0.85 (967.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:52:54.676796: step 16530, loss = 0.67 (944.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:52:55.996212: step 16540, loss = 0.80 (970.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:52:57.373353: step 16550, loss = 0.80 (929.4 examples/sec; 0.138 sec/batch)
2017-05-07 19:52:58.665559: step 16560, loss = 0.81 (990.6 examples/sec; 0.129 sec/batch)
2017-05-07 19:52:59.994776: step 16570, loss = 0.93 (963.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:53:01.298539: step 16580, loss = 0.68 (981.8 examples/sec; 0.130 sec/batch)
2017-05-07 19:53:02.643905: step 16590, loss = 0.79 (951.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:53:04.129362: step 16600, loss = 0.71 (861.7 examples/sec; 0.149 sec/batch)
2017-05-07 19:53:05.339098: step 16610, loss = 0.77 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-07 19:53:06.644861: step 16620, loss = 0.88 (980.3 examples/sec; 0.131 sec/batch)
2017-05-07 19:53:07.995100: step 16630, loss = 0.91 (948.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:53:09.336787: step 16640, loss = 0.79 (954.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:53:10.715402: step 16650, loss = 0.70 (928.5 examples/sec; 0.138 sec/batch)
2017-05-07 19:53:12.029930: step 16660, loss = 0.67 (973.7 examples/sec; 0.131 sec/batch)
2017-05-07 19:53:13.358808: step 16670, loss = 0.93 (963.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:53:14.700126: step 16680, loss = 0.91 (954.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:53:16.035256: step 16690, loss = 0.96 (958.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:53:17.494652: step 16700, loss = 0.87 (877.1 examples/sec; 0.146 sec/batch)
2017-05-07 19:53:18.708364: step 16710, loss = 0.94 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-07 19:53:20.059174: step 16720, loss = 0.78 (947.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:53:21.391170: step 16730, loss = 0.80 (961.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:53:22.717890: step 16740, loss = 0.87 (964.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:53:24.088252: step 16750, loss = 0.86 (934.1 examples/sec; 0.137 sec/batch)
2017-05-07 19:53:25.373333: step 16760, loss = 0.79 (996.0 examples/sec; 0.129 sec/batch)
2017-05-07 19:53:26.714729: step 16770, loss = 0.89 (954.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:53:28.057096: step 16780, loss = 0.80 (953.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:53:29.391967: step 16790, loss = 0.98 (958.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:53:30.879102: step 16800, loss = 0.74 (860.7 examples/sec; 0.149 sec/batch)
2017-05-07 19:53:32.058176: step 16810, loss = 0.82 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-07 19:53:33.409275: step 16820, loss = 0.90 (947.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:53:34.740264: step 16830, loss = 0.88 (961.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:53:36.074265: step 16840, loss = 0.91 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:53:37.443599: step 16850, loss = 0.86 (934.8 examples/sec; 0.137 sec/batch)
2017-05-07 19:53:38.737300: step 16860, loss = 0.73 (989.4 examples/sec; 0.129 sec/batch)
2017-05-07 19:53:40.080961: step 16870, loss = 0.81 (952.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:53:41.403933: step 16880, loss = 0.71 (967.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:53:42.759285: step 16890, loss = 0.66 (944.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:53:44.255559: step 16900, loss = 0.89 (855.5 examples/sec; 0.150 sec/batch)
2017-05-07 19:53:45.465347: step 16910, loss = 0.71 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-07 19:53:46.790743: step 16920, loss = 0.62 (965.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:53:48.129654: step 16930, loss = 0.87 (956.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:53:49.480359: step 16940, loss = 0.82 (947.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:53:50.836027: step 16950, loss = 0.77 (944.2 examples/sec; 0.136 sec/batch)
2017-05-07 19:53:52.129920: step 16960, loss = 0.85 (989.3 examples/sec; 0.129 sec/batch)
2017-05-07 19:53:53.467977: step 16970, loss = 0.76 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:53:54.806148: step 16980, loss = 0.74 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:53:56.150155: step 16990, loss = 0.63 (952.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:53:57.618569: step 17000, loss = 0.72 (871.7 examples/sec; 0.147 sec/batch)
2017-05-07 19:53:58.827386: step 17010, loss = 0.70 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-07 19:54:00.172581: step 17020, loss = 0.69 (951.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:54:01.495786: step 17030, loss = 0.84 (967.3 examples/sec; 0.132 sec/batch)
2017-05-07 19:54:02.819442: step 17040, loss = 0.79 (967.0 examples/sec; 0.132 sec/batch)
2017-05-07 19:54:04.202072: step 17050, loss = 0.81 (925.8 examples/sec; 0.138 sec/batch)
2017-05-07 19:54:05.491299: step 17060, loss = 0.95 (992.8 examples/sec; 0.129 sec/batch)
2017-05-07 19:54:06.814568: step 17070, loss = 0.91 (967.3 examples/sec; 0.132 sec/batch)
2017-05-07 19:54:08.140608: step 17080, loss = 0.75 (965.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:54:09.475646: step 17090, loss = 0.75 (958.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:54:10.946340: step 17100, loss = 0.84 (870.3 examples/sec; 0.147 sec/batch)
2017-05-07 19:54:12.137852: step 17110, loss = 0.87 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-07 19:54:13.468866: step 17120, loss = 1.02 (961.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:54:14.812447: step 17130, loss = 0.83 (952.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:54:16.163565: step 17140, loss = 0.50 (947.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:54:17.567122: step 17150, loss = 0.93 (912.0 examples/sec; 0.140 sec/batch)
2017-05-07 19:54:18.848513: step 17160, loss = 0.86 (998.9 examples/sec; 0.128 sec/batch)
2017-05-07 19:54:20.201376: step 17170, loss = 0.73 (946.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:54:21.533583: step 17180, loss = 0.74 (960.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:54:22.867813: step 17190, loss = 1.07 (959.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:54:24.338125: step 17200, loss = 0.72 (870.6 examples/sec; 0.147 sec/batch)
2017-05-07 19:54:25.536080: step 17210, loss = 0.85 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-07 19:54:26.872348: step 17220, loss = 0.84 (957.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:54:28.223917: step 17230, loss = 0.79 (947.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:54:29.577038: step 17240, loss = 0.79 (946.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:54:30.945914: step 17250, loss = 0.91 (935.1 examples/sec; 0.137 sec/batch)
2017-05-07 19:54:32.240650: step 17260, loss = 0.90 (988.6 examples/sec; 0.129 sec/batch)
2017-05-07 19:54:33.571902: step 17270, loss = 0.86 (961.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:54:34.902627: step 17280, loss = 0.72 (961.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:54:36.239284: step 17290, loss = 0.74 (957.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:54:37.692342: step 17300, loss = 0.91 (880.9 examples/sec; 0.145 sec/batch)
2017-05-07 19:54:38.875753: step 17310, loss = 0.84 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-07 19:54:40.225755: step 17320, loss = 0.81 (948.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:54:41.515634: step 17330, loss = 0.65 (992.3 examples/sec; 0.129 sec/batch)
2017-05-07 19:54:42.850005: step 17340, loss = 0.65 (959.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:54:44.225407: step 17350, loss = 0.83 (930.6 examples/sec; 0.138 sec/batch)
2017-05-07 19:54:45.512345: step 17360, loss = 0.81 (994.6 examples/sec; 0.129 sec/batch)
2017-05-07 19:54:46.820012: step 17370, loss = 0.88 (978.8 examples/sec; 0.131 sec/batch)
2017-05-07 19:54:48.172335: step 17380, loss = 0.95 (946.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:54:49.497422: step 17390, loss = 0.86 (966.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:54:50.977626: step 17400, loss = 0.81 (864.7 examples/sec; 0.148 sec/batch)
2017-05-07 19:54:52.216582: step 17410, loss = 1.00 (1033.1 examples/sec; 0.124 sec/batch)
2017-05-07 19:54:53.514001: step 17420, loss = 0.82 (986.6 examples/sec; 0.130 sec/batch)
2017-05-07 19:54:54.865203: step 17430, loss = 0.70 (947.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:54:56.196581: step 17440, loss = 0.89 (961.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:54:57.590706: step 17450, loss = 0.68 (918.1 examples/sec; 0.139 sec/batch)
2017-05-07 19:54:58.880840: step 17460, loss = 0.82 (992.1 examples/sec; 0.129 sec/batch)
2017-05-07 19:55:00.223309: step 17470, loss = 0.68 (953.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:55:01.565669: step 17480, loss = 0.94 (953.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:55:02.912827: step 17490, loss = 0.87 (950.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:55:04.390996: step 17500, loss = 0.84 (865.9 examples/sec; 0.148 sec/batch)
2017-05-07 19:55:05.590424: step 17510, loss = 0.80 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-07 19:55:06.922067: step 17520, loss = 0.79 (961.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:55:08.257305: step 17530, loss = 0.77 (958.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:55:09.566008: step 17540, loss = 0.96 (978.1 examples/sec; 0.131 sec/batch)
2017-05-07 19:55:10.930957: step 17550, loss = 0.75 (937.8 examples/sec; 0.136 sec/batch)
2017-05-07 19:55:12.236726: step 17560, loss = 0.84 (980.3 examples/sec; 0.131 sec/batch)
2017-05-07 19:55:13.548946: step 17570, loss = 0.80 (975.5 examples/sec; 0.131 sec/batch)
2017-05-07 19:55:14.874505: step 17580, loss = 0.81 (965.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:55:16.198819: step 17590, loss = 0.77 (966.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:55:17.671018: step 17600, loss = 0.77 (869.5 examples/sec; 0.147 sec/batch)
2017-05-07 19:55:18.892871: step 17610, loss = 0.77 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-07 19:55:20.223022: step 17620, loss = 0.71 (962.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:55:21.561077: step 17630, loss = 0.77 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:55:22.915349: step 17640, loss = 0.95 (945.2 examples/sec; 0.135 sec/batch)
2017-05-07 19:55:24.278399: step 17650, loss = 0.70 (939.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:55:25.590794: step 17660, loss = 0.69 (975.3 examples/sec; 0.131 sec/batch)
2017-05-07 19:55:26.942418: step 17670, loss = 0.81 (947.0 examples/sec; 0.135 sec/batch)
2017-05-07 19:55:28.290263: step 17680, loss = 0.86 (949.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:55:29.638135: step 17690, loss = 0.75 (949.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:55:31.114555: step 17700, loss = 1.01 (867.0 examples/sec; 0.148 sec/batch)
2017-05-07 19:55:32.308178: step 17710, loss = 0.91 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-07 19:55:33.649348: step 17720, loss = 0.77 (954.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:55:34.975191: step 17730, loss = 0.68 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:55:36.327670: step 17740, loss = 0.93 (946.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:55:37.731009: step 17750, loss = 0.97 (912.1 examples/sec; 0.140 sec/batch)
2017-05-07 19:55:38.987518: step 17760, loss = 0.79 (1018.7 examples/sec; 0.126 sec/batch)
2017-05-07 19:55:40.339069: step 17770, loss = 0.77 (947.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:55:41.673637: step 17780, loss = 1.03 (959.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:55:42.994851: step 17790, loss = 0.84 (968.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:55:44.475625: step 17800, loss = 0.79 (864.4 examples/sec; 0.148 sec/batch)
2017-05-07 19:55:45.677208: step 17810, loss = 0.86 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-07 19:55:47.005120: step 17820, loss = 0.73 (963.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:55:48.352700: step 17830, loss = 0.84 (949.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:55:49.657004: step 17840, loss = 0.81 (981.4 examples/sec; 0.130 sec/batch)
2017-05-07 19:55:51.049387: step 17850, loss = 0.79 (919.3 examples/sec; 0.139 sec/batch)
2017-05-07 19:55:52.352249: step 17860, loss = 1.00 (982.5 examples/sec; 0.130 sec/batch)
2017-05-07 19:55:53.684325: step 17870, loss = 0.86 (960.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:55:55.056767: step 17880, loss = 0.64 (932.7 examples/sec; 0.137 sec/batch)
2017-05-07 19:55:56.391337: step 17890, loss = 0.86 (959.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:55:57.872495: step 17900, loss = 0.88 (864.2 examples/sec; 0.148 sec/batch)
2017-05-07 19:55:59.087344: step 17910, loss = 0.75 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-07 19:56:00.394252: step 17920, loss = 0.84 (979.4 examples/sec; 0.131 sec/batch)
2017-05-07 19:56:01.712083: step 17930, loss = 0.76 (971.3 examples/sec; 0.132 sec/batch)
2017-05-07 19:56:03.033262: step 17940, loss = 0.80 (968.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:56:04.436158: step 17950, loss = 0.85 (912.4 examples/sec; 0.140 sec/batch)
2017-05-07 19:56:05.723070: step 17960, loss = 0.88 (994.6 examples/sec; 0.129 sec/batch)
2017-05-07 19:56:07.060342: step 17970, loss = 0.78 (957.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:56:08.400119: step 17980, loss = 0.72 (955.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:56:09.716306: step 17990, loss = 0.60 (972.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:56:11.196780: step 18000, loss = 0.72 (864.6 examples/sec; 0.148 sec/batch)
2017-05-07 19:56:12.425087: step 18010, loss = 0.70 (1042.1 examples/sec; 0.123 sec/batch)
2017-05-07 19:56:13.766150: step 18020, loss = 0.69 (954.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:56:15.083850: step 18030, loss = 0.79 (971.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:56:16.418048: step 18040, loss = 0.70 (959.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:56:17.798804: step 18050, loss = 0.73 (927.0 examples/sec; 0.138 sec/batch)
2017-05-07 19:56:19.092317: step 18060, loss = 0.63 (989.6 examples/sec; 0.129 sec/batch)
2017-05-07 19:56:20.448161: step 18070, loss = 0.68 (944.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:56:21.778239: step 18080, loss = 0.84 (962.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:56:23.104885: step 18090, loss = 0.83 (964.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:56:24.594922: step 18100, loss = 0.72 (859.0 examples/sec; 0.149 sec/batch)
2017-05-07 19:56:25.793549: step 18110, loss = 0.74 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-07 19:56:27.125243: step 18120, loss = 0.87 (961.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:56:28.453509: step 18130, loss = 0.96 (963.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:56:29.756395: step 18140, loss = 0.98 (982.4 examples/sec; 0.130 sec/batch)
2017-05-07 19:56:31.160141: step 18150, loss = 0.86 (911.9 examples/sec; 0.140 sec/batch)
2017-05-07 19:56:32.464516: step 18160, loss = 0.81 (981.3 examples/sec; 0.130 sec/batch)
2017-05-07 19:56:33.795655: step 18170, loss = 0.70 (961.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:56:35.130561: step 18180, loss = 0.76 (958.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:56:36.480921: step 18190, loss = 0.83 (947.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:56:37.976334: step 18200, loss = 0.87 (856.0 examples/sec; 0.150 sec/batch)
2017-05-07 19:56:39.175544: step 18210, loss = 0.85 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-07 19:56:40.508490: step 18220, loss = 0.70 (960.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:56:41.848005: step 18230, loss = 0.70 (955.5 examples/sec; 0.134 sec/batch)
2017-05-07 19:56:43.183007: step 18240, loss = 0.68 (958.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:56:44.546202: step 18250, loss = 0.76 (939.0 examples/sec; 0.136 sec/batch)
2017-05-07 19:56:45.852651: step 18260, loss = 0.78 (979.8 examples/sec; 0.131 sec/batch)
2017-05-07 19:56:47.179585: step 18270, loss = 0.79 (964.6 examples/sec; 0.133 sec/batch)
2017-05-07 19:56:48.523548: step 18280, loss = 0.80 (952.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:56:49.848699: step 18290, loss = 0.78 (965.9 examples/sec; 0.133 sec/batch)
2017-05-07 19:56:51.319396: step 18300, loss = 0.76 (870.3 examples/sec; 0.147 sec/batch)
2017-05-07 19:56:52.542940: step 18310, loss = 0.71 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-07 19:56:53.893357: step 18320, loss = 0.86 (947.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:56:55.243369: step 18330, loss = 0.83 (948.1 examples/sec; 0.135 sec/batch)
2017-05-07 19:56:56.605941: step 18340, loss = 0.89 (939.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:56:57.978472: step 18350, loss = 0.70 (932.6 examples/sec; 0.137 sec/batch)
2017-05-07 19:56:59.276917: step 18360, loss = 0.92 (985.8 examples/sec; 0.130 sec/batch)
2017-05-07 19:57:00.604181: step 18370, loss = 0.72 (964.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:57:01.950673: step 18380, loss = 0.68 (950.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:57:03.302488: step 18390, loss = 1.01 (946.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:57:04.751475: step 18400, loss = 0.72 (883.4 examples/sec; 0.145 sec/batch)
2017-05-07 19:57:05.972686: step 18410, loss = 0.71 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-07 19:57:07.295945: step 18420, loss = 0.86 (967.3 examples/sec; 0.132 sec/batch)
2017-05-07 19:57:08.664742: step 18430, loss = 0.74 (935.1 examples/sec; 0.137 sec/batch)
2017-05-07 19:57:09.983636: step 18440, loss = 0.78 (970.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:57:11.360127: step 18450, loss = 0.80 (929.9 examples/sec; 0.138 sec/batch)
2017-05-07 19:57:12.650599: step 18460, loss = 0.83 (991.9 examples/sec; 0.129 sec/batch)
2017-05-07 19:57:13.970739: step 18470, loss = 0.93 (969.6 examples/sec; 0.132 sec/batch)
2017-05-07 19:57:15.307937: step 18480, loss = 0.79 (957.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:57:16.624674: step 18490, loss = 0.80 (972.1 examples/sec; 0.132 sec/batch)
2017-05-07 19:57:18.099643: step 18500, loss = 0.87 (867.8 examples/sec; 0.147 sec/batch)
2017-05-07 19:57:19.320934: step 18510, loss = 0.60 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-07 19:57:20.632686: step 18520, loss = 0.83 (975.8 examples/sec; 0.131 sec/batch)
2017-05-07 19:57:21.951276: step 18530, loss = 0.88 (970.7 examples/sec; 0.132 sec/batch)
2017-05-07 19:57:23.292632: step 18540, loss = 0.86 (954.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:57:24.680630: step 18550, loss = 0.78 (922.2 examples/sec; 0.139 sec/batch)
2017-05-07 19:57:25.974071: step 18560, loss = 0.59 (989.6 examples/sec; 0.129 sec/batch)
2017-05-07 19:57:27.294584: step 18570, loss = 0.94 (969.3 examples/sec; 0.132 sec/batch)
2017-05-07 19:57:28.614922: step 18580, loss = 0.99 (969.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:57:29.932227: step 18590, loss = 0.66 (971.7 examples/sec; 0.132 sec/batch)
2017-05-07 19:57:31.406024: step 18600, loss = 0.65 (868.5 examples/sec; 0.147 sec/batch)
2017-05-07 19:57:32.623115: step 18610, loss = 0.81 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-07 19:57:33.974916: step 18620, loss = 0.72 (946.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:57:35.304948: step 18630, loss = 0.94 (962.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:57:36.633002: step 18640, loss = 0.72 (963.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:57:38.012123: step 18650, loss = 0.89 (928.1 examples/sec; 0.138 sec/batch)
2017-05-07 19:57:39.315098: step 18660, loss = 0.88 (982.4 examples/sec; 0.130 sec/batch)
2017-05-07 19:57:40.654940: step 18670, loss = 0.78 (955.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:57:41.963821: step 18680, loss = 0.88 (977.9 examples/sec; 0.131 sec/batch)
2017-05-07 19:57:43.293921: step 18690, loss = 0.72 (962.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:57:44.782917: step 18700, loss = 0.75 (859.6 examples/sec; 0.149 sec/batch)
2017-05-07 19:57:45.980751: step 18710, loss = 0.74 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-07 19:57:47.337915: step 18720, loss = 0.87 (943.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:57:48.687263: step 18730, loss = 0.71 (948.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:57:50.051027: step 18740, loss = 0.89 (938.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:57:51.427626: step 18750, loss = 0.86 (929.8 examples/sec; 0.138 sec/batch)
2017-05-07 19:57:52.753925: step 18760, loss = 0.95 (965.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:57:54.101755: step 18770, loss = 0.93 (949.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:57:55.442385: step 18780, loss = 0.92 (954.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:57:56.772295: step 18790, loss = 0.59 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:57:58.262815: step 18800, loss = 0.72 (858.8 examples/sec; 0.149 sec/batch)
2017-05-07 19:57:59.473346: step 18810, loss = 0.77 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-07 19:58:00.801566: step 18820, loss = 0.75 (963.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:58:02.156568: step 18830, loss = 0.70 (944.6 examples/sec; 0.136 sec/batch)
2017-05-07 19:58:03.517711: step 18840, loss = 0.71 (940.4 examples/sec; 0.136 sec/batch)
2017-05-07 19:58:04.889588: step 18850, loss = 0.97 (933.0 examples/sec; 0.137 sec/batch)
2017-05-07 19:58:06.198467: step 18860, loss = 0.59 (977.9 examples/sec; 0.131 sec/batch)
2017-05-07 19:58:07.541072: step 18870, loss = 0.72 (953.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:58:08.862291: step 18880, loss = 0.89 (968.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:58:10.196066: step 18890, loss = 0.87 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:58:11.692495: step 18900, loss = 0.75 (855.4 examples/sec; 0.150 sec/batch)
2017-05-07 19:58:12.916266: step 18910, loss = 0.82 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-07 19:58:14.255805: step 18920, loss = 0.94 (955.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:58:15.573362: step 18930, loss = 0.75 (971.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:58:16.905166: step 18940, loss = 0.72 (961.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:58:18.298939: step 18950, loss = 0.65 (918.4 examples/sec; 0.139 sec/batch)
2017-05-07 19:58:19.575374: step 18960, loss = 0.82 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-07 19:58:20.917664: step 18970, loss = 0.76 (953.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:58:22.261296: step 18980, loss = 0.78 (952.6 examples/sec; 0.134 sec/batch)
2017-05-07 19:58:23.602730: step 18990, loss = 0.82 (954.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:58:25.064526: step 19000, loss = 0.66 (875.6 examples/sec; 0.146 sec/batch)
2017-05-07 19:58:26.310140: step 19010, loss = 0.86 (1027.6 examples/sec; 0.125 sec/batch)
2017-05-07 19:58:27.649375: step 19020, loss = 0.76 (955.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:58:28.985778: step 19030, loss = 0.73 (957.8 examples/sec; 0.134 sec/batch)
2017-05-07 19:58:30.338337: step 19040, loss = 0.67 (946.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:58:31.733680: step 19050, loss = 0.77 (917.3 examples/sec; 0.140 sec/batch)
2017-05-07 19:58:33.025485: step 19060, loss = 0.92 (990.9 examples/sec; 0.129 sec/batch)
2017-05-07 19:58:34.358795: step 19070, loss = 0.76 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:58:35.690155: step 19080, loss = 0.74 (961.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:58:36.999872: step 19090, loss = 0.92 (977.3 examples/sec; 0.131 sec/batch)
2017-05-07 19:58:38.474969: step 19100, loss = 0.81 (867.7 examples/sec; 0.148 sec/batch)
2017-05-07 19:58:39.704300: step 19110, loss = 0.96 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-07 19:58:41.046977: step 19120, loss = 0.90 (953.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:58:42.376116: step 19130, loss = 0.73 (963.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:58:43.705065: step 19140, loss = 0.89 (963.2 examples/sec; 0.133 sec/batch)
2017-05-07 19:58:45.070304: step 19150, loss = 0.85 (937.6 examples/sec; 0.137 sec/batch)
2017-05-07 19:58:46.403216: step 19160, loss = 0.81 (960.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:58:47.759019: step 19170, loss = 0.88 (944.1 examples/sec; 0.136 sec/batch)
2017-05-07 19:58:49.105165: step 19180, loss = 0.93 (950.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:58:50.437339: step 19190, loss = 0.73 (960.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:58:51.906336: step 19200, loss = 0.67 (871.3 examples/sec; 0.147 sec/batch)
2017-05-07 19:58:53.157478: step 19210, loss = 0.90 (1023.1 examples/sec; 0.125 sec/batch)
2017-05-07 19:58:54.502847: step 19220, loss = 0.95 (951.4 examples/sec; 0.135 sec/batch)
2017-05-07 19:58:55.855151: step 19230, loss = 0.89 (946.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:58:57.169629: step 19240, loss = 0.69 (973.8 examples/sec; 0.131 sec/batch)
2017-05-07 19:58:58.567294: step 19250, loss = 0.73 (915.8 examples/sec; 0.140 sec/batch)
2017-05-07 19:58:59.843030: step 19260, loss = 0.87 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-07 19:59:01.172010: step 19270, loss = 0.89 (963.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:59:02.494659: step 19280, loss = 0.71 (967.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:59:03.865358: step 19290, loss = 0.75 (933.8 examples/sec; 0.137 sec/batch)
2017-05-07 19:59:05.307230: step 19300, loss = 0.76 (887.7 examples/sec; 0.144 sec/batch)
2017-05-07 19:59:06.515383: step 19310, loss = 0.63 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-07 19:59:07.849950: step 19320, loss = 0.77 (959.1 examples/sec; 0.133 sec/batch)
2017-05-07 19:59:09.159929: step 19330, loss = 0.83 (977.1 examples/sec; 0.131 sec/batch)
2017-05-07 19:59:10.500010: step 19340, loss = 0.76 (955.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:59:11.913380: step 19350, loss = 0.88 (905.6 examples/sec; 0.141 sec/batch)
2017-05-07 19:59:13.184430: step 19360, loss = 0.87 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-07 19:59:14.502003: step 19370, loss = 0.76 (971.5 examples/sec; 0.132 sec/batch)
2017-05-07 19:59:15.832084: step 19380, loss = 0.82 (962.4 examples/sec; 0.133 sec/batch)
2017-05-07 19:59:17.147528: step 19390, loss = 0.75 (973.0 examples/sec; 0.132 sec/batch)
2017-05-07 19:59:18.619957: step 19400, loss = 0.86 (869.3 examples/sec; 0.147 sec/batch)
2017-05-07 19:59:19.870744: step 19410, loss = 0.78 (1023.4 examples/sec; 0.125 sec/batch)
2017-05-07 19:59:21.197819: step 19420, loss = 0.69 (964.5 examples/sec; 0.133 sec/batch)
2017-05-07 19:59:22.545931: step 19430, loss = 0.66 (949.5 examples/sec; 0.135 sec/batch)
2017-05-07 19:59:23.898107: step 19440, loss = 0.73 (946.6 examples/sec; 0.135 sec/batch)
2017-05-07 19:59:25.235213: step 19450, loss = 0.79 (957.3 examples/sec; 0.134 sec/batch)
2017-05-07 19:59:26.572486: step 19460, loss = 0.86 (957.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:59:27.899248: step 19470, loss = 0.85 (964.7 examples/sec; 0.133 sec/batch)
2017-05-07 19:59:29.225671: step 19480, loss = 0.87 (965.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:59:30.578916: step 19490, loss = 0.96 (945.9 examples/sec; 0.135 sec/batch)
2017-05-07 19:59:32.022349: step 19500, loss = 0.77 (886.8 examples/sec; 0.144 sec/batch)
2017-05-07 19:59:33.281720: step 19510, loss = 0.90 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-07 19:59:34.621524: step 19520, loss = 0.68 (955.4 examples/sec; 0.134 sec/batch)
2017-05-07 19:59:35.965997: step 19530, loss = 0.67 (952.0 examples/sec; 0.134 sec/batch)
2017-05-07 19:59:37.282632: step 19540, loss = 0.82 (972.2 examples/sec; 0.132 sec/batch)
2017-05-07 19:59:38.621310: step 19550, loss = 0.85 (956.2 examples/sec; 0.134 sec/batch)
2017-05-07 19:59:39.948661: step 19560, loss = 0.86 (964.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:59:41.297063: step 19570, loss = 0.70 (949.3 examples/sec; 0.135 sec/batch)
2017-05-07 19:59:42.618285: step 19580, loss = 0.94 (968.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:59:43.944339: step 19590, loss = 0.76 (965.3 examples/sec; 0.133 sec/batch)
2017-05-07 19:59:45.368713: step 19600, loss = 0.86 (898.6 examples/sec; 0.142 sec/batch)
2017-05-07 19:59:46.604270: step 19610, loss = 0.84 (1036.0 examples/sec; 0.124 sec/batch)
2017-05-07 19:59:47.919294: step 19620, loss = 0.81 (973.4 examples/sec; 0.132 sec/batch)
2017-05-07 19:59:49.253995: step 19630, loss = 0.75 (959.0 examples/sec; 0.133 sec/batch)
2017-05-07 19:59:50.593396: step 19640, loss = 0.98 (955.7 examples/sec; 0.134 sec/batch)
2017-05-07 19:59:51.952874: step 19650, loss = 0.92 (941.5 examples/sec; 0.136 sec/batch)
2017-05-07 19:59:53.278195: step 19660, loss = 0.75 (965.8 examples/sec; 0.133 sec/batch)
2017-05-07 19:59:54.598080: step 19670, loss = 0.72 (969.8 examples/sec; 0.132 sec/batch)
2017-05-07 19:59:55.942687: step 19680, loss = 0.84 (951.9 examples/sec; 0.134 sec/batch)
2017-05-07 19:59:57.291849: step 19690, loss = 0.79 (948.7 examples/sec; 0.135 sec/batch)
2017-05-07 19:59:58.759167: step 19700, loss = 0.78 (872.3 examples/sec; 0.147 sec/batch)
2017-05-07 19:59:59.986767: step 19710, loss = 0.70 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-07 20:00:01.289320: step 19720, loss = 0.89 (982.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:00:02.656035: step 19730, loss = 0.84 (936.6 examples/sec; 0.137 sec/batch)
2017-05-07 20:00:03.983718: step 19740, loss = 0.79 (964.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:00:05.324570: step 19750, loss = 0.78 (954.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:00:06.648199: step 19760, loss = 0.95 (967.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:00:08.000639: step 19770, loss = 0.89 (946.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:00:09.311238: step 19780, loss = 0.79 (976.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:00:10.648026: step 19790, loss = 0.80 (957.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:00:12.083579: step 19800, loss = 0.76 (891.6 examples/sec; 0.144 sec/batch)
2017-05-07 20:00:13.327767: step 19810, loss = 0.88 (1028.8 examples/sec; 0.124 sec/batch)
2017-05-07 20:00:14.638511: step 19820, loss = 0.67 (976.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:00:15.972295: step 19830, loss = 0.72 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:00:17.295740: step 19840, loss = 0.83 (967.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:00:18.652309: step 19850, loss = 0.82 (943.6 examples/sec; 0.136 sec/batch)
2017-05-07 20:00:19.970684: step 19860, loss = 0.91 (970.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:00:21.297907: step 19870, loss = 0.80 (964.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:00:22.627414: step 19880, loss = 0.71 (962.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:00:23.932673: step 19890, loss = 0.86 (980.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:00:25.380764: step 19900, loss = 0.80 (883.9 examples/sec; 0.145 sec/batch)
2017-05-07 20:00:26.604838: step 19910, loss = 0.91 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-07 20:00:27.938580: step 19920, loss = 0.80 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:00:29.270630: step 19930, loss = 0.82 (960.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:00:30.588684: step 19940, loss = 0.89 (971.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:00:31.967966: step 19950, loss = 0.72 (928.0 examples/sec; 0.138 sec/batch)
2017-05-07 20:00:33.269588: step 19960, loss = 0.75 (983.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:00:34.609295: step 19970, loss = 0.76 (955.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:00:35.938887: step 19980, loss = 1.15 (962.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:00:37.282589: step 19990, loss = 0.85 (952.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:00:38.734186: step 20000, loss = 0.79 (881.8 examples/sec; 0.145 sec/batch)
2017-05-07 20:00:39.964153: step 20010, loss = 0.66 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-07 20:00:41.283116: step 20020, loss = 0.70 (970.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:00:42.625675: step 20030, loss = 0.69 (953.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:00:43.960716: step 20040, loss = 0.83 (958.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:00:45.327105: step 20050, loss = 0.89 (936.8 examples/sec; 0.137 sec/batch)
2017-05-07 20:00:46.620117: step 20060, loss = 0.75 (989.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:00:47.937598: step 20070, loss = 0.69 (971.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:00:49.256379: step 20080, loss = 0.71 (970.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:00:50.592015: step 20090, loss = 0.70 (958.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:00:52.056005: step 20100, loss = 0.92 (874.3 examples/sec; 0.146 sec/batch)
2017-05-07 20:00:53.268787: step 20110, loss = 0.77 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-07 20:00:54.584150: step 20120, loss = 0.73 (973.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:00:55.911090: step 20130, loss = 0.79 (964.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:00:57.242092: step 20140, loss = 0.78 (961.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:00:58.607970: step 20150, loss = 0.65 (937.1 examples/sec; 0.137 sec/batch)
2017-05-07 20:00:59.922734: step 20160, loss = 0.83 (973.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:01:01.317305: step 20170, loss = 0.73 (917.8 examples/sec; 0.139 sec/batch)
2017-05-07 20:01:02.649279: step 20180, loss = 0.92 (961.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:01:04.013426: step 20190, loss = 0.74 (938.3 examples/sec; 0.136 sec/batch)
2017-05-07 20:01:05.537758: step 20200, loss = 0.73 (839.7 examples/sec; 0.152 sec/batch)
2017-05-07 20:01:06.698491: step 20210, loss = 0.81 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-07 20:01:07.987404: step 20220, loss = 0.73 (993.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:01:09.314894: step 20230, loss = 0.89 (964.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:01:10.649102: step 20240, loss = 0.83 (959.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:01:12.013120: step 20250, loss = 0.84 (938.4 examples/sec; 0.136 sec/batch)
2017-05-07 20:01:13.303649: step 20260, loss = 0.81 (991.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:01:14.637443: step 20270, loss = 0.71 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:01:16.001262: step 20280, loss = 0.78 (938.5 examples/sec; 0.136 sec/batch)
2017-05-07 20:01:17.302062: step 20290, loss = 1.10 (984.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:01:18.824976: step 20300, loss = 0.90 (840.5 examples/sec; 0.152 sec/batch)
2017-05-07 20:01:19.992562: step 20310, loss = 0.71 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-07 20:01:21.308929: step 20320, loss = 0.83 (972.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:01:22.660545: step 20330, loss = 0.73 (947.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:01:23.976579: step 20340, loss = 0.73 (972.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:01:25.350381: step 20350, loss = 0.68 (931.7 examples/sec; 0.137 sec/batch)
2017-05-07 20:01:26.669608: step 20360, loss = 1.00 (970.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:01:28.006768: step 20370, loss = 0.67 (957.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:01:29.359121: step 20380, loss = 0.73 (946.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:01:30.687810: step 20390, loss = 0.83 (963.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:01:32.184300: step 20400, loss = 0.83 (855.3 examples/sec; 0.150 sec/batch)
2017-05-07 20:01:33.381902: step 20410, loss = 0.69 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-07 20:01:34.713143: step 20420, loss = 0.93 (961.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:01:36.061911: step 20430, loss = 0.88 (949.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:01:37.378152: step 20440, loss = 0.78 (972.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:01:38.764226: step 20450, loss = 0.83 (923.5 examples/sec; 0.139 sec/batch)
2017-05-07 20:01:40.036295: step 20460, loss = 0.81 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:01:41.332224: step 20470, loss = 0.84 (987.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:01:42.678822: step 20480, loss = 0.82 (950.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:01:44.022552: step 20490, loss = 0.88 (952.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:01:45.495677: step 20500, loss = 0.73 (868.9 examples/sec; 0.147 sec/batch)
2017-05-07 20:01:46.681876: step 20510, loss = 0.74 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-07 20:01:48.043938: step 20520, loss = 0.84 (939.8 examples/sec; 0.136 sec/batch)
2017-05-07 20:01:49.365446: step 20530, loss = 0.73 (968.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:01:50.688101: step 20540, loss = 0.71 (967.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:01:52.073410: step 20550, loss = 0.95 (924.0 examples/sec; 0.139 sec/batch)
2017-05-07 20:01:53.385468: step 20560, loss = 0.92 (975.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:01:54.710387: step 20570, loss = 0.90 (966.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:01:56.052501: step 20580, loss = 0.67 (953.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:01:57.383609: step 20590, loss = 0.80 (961.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:01:58.863368: step 20600, loss = 0.81 (865.0 examples/sec; 0.148 sec/batch)
2017-05-07 20:02:00.052106: step 20610, loss = 0.68 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-07 20:02:01.374841: step 20620, loss = 0.87 (967.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:02:02.702760: step 20630, loss = 0.78 (963.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:02:04.031962: step 20640, loss = 0.75 (963.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:02:05.410022: step 20650, loss = 0.70 (928.8 examples/sec; 0.138 sec/batch)
2017-05-07 20:02:06.698732: step 20660, loss = 0.67 (993.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:02:08.029881: step 20670, loss = 0.92 (961.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:02:09.368691: step 20680, loss = 0.79 (956.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:02:10.747733: step 20690, loss = 0.69 (928.2 examples/sec; 0.138 sec/batch)
2017-05-07 20:02:12.194230: step 20700, loss = 0.77 (884.9 examples/sec; 0.145 sec/batch)
2017-05-07 20:02:13.404641: step 20710, loss = 0.77 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-07 20:02:14.723393: step 20720, loss = 0.65 (970.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:02:16.075650: step 20730, loss = 0.93 (946.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:02:17.391438: step 20740, loss = 0.80 (972.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:02:18.780916: step 20750, loss = 0.80 (921.2 examples/sec; 0.139 sec/batch)
2017-05-07 20:02:20.058432: step 20760, loss = 0.77 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:02:21.370542: step 20770, loss = 0.79 (975.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:02:22.701858: step 20780, loss = 0.72 (961.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:02:24.044341: step 20790, loss = 0.80 (953.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:02:25.517059: step 20800, loss = 0.83 (869.1 examples/sec; 0.147 sec/batch)
2017-05-07 20:02:26.711069: step 20810, loss = 0.79 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-07 20:02:28.035423: step 20820, loss = 0.79 (966.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:02:29.359711: step 20830, loss = 0.76 (966.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:02:30.690234: step 20840, loss = 0.79 (962.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:02:32.099984: step 20850, loss = 0.66 (908.0 examples/sec; 0.141 sec/batch)
2017-05-07 20:02:33.383506: step 20860, loss = 0.69 (997.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:02:34.714376: step 20870, loss = 0.76 (961.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:02:36.052231: step 20880, loss = 0.73 (956.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:02:37.374803: step 20890, loss = 0.71 (967.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:02:38.850600: step 20900, loss = 0.69 (867.3 examples/sec; 0.148 sec/batch)
2017-05-07 20:02:40.067492: step 20910, loss = 0.72 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-07 20:02:41.404679: step 20920, loss = 0.86 (957.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:02:42.759959: step 20930, loss = 0.75 (944.4 examples/sec; 0.136 sec/batch)
2017-05-07 20:02:44.084007: step 20940, loss = 0.85 (966.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:02:45.475225: step 20950, loss = 0.78 (920.1 examples/sec; 0.139 sec/batch)
2017-05-07 20:02:46.737571: step 20960, loss = 0.77 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-07 20:02:48.041372: step 20970, loss = 0.80 (981.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:02:49.376165: step 20980, loss = 0.77 (958.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:02:50.720114: step 20990, loss = 0.83 (952.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:02:52.223041: step 21000, loss = 0.88 (851.7 examples/sec; 0.150 sec/batch)
2017-05-07 20:02:53.430119: step 21010, loss = 0.67 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-07 20:02:54.767937: step 21020, loss = 0.68 (956.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:02:56.086494: step 21030, loss = 0.85 (970.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:02:57.425055: step 21040, loss = 0.65 (956.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:02:58.784039: step 21050, loss = 0.69 (941.9 examples/sec; 0.136 sec/batch)
2017-05-07 20:03:00.043749: step 21060, loss = 0.72 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:03:01.394789: step 21070, loss = 0.74 (947.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:03:02.715415: step 21080, loss = 0.81 (969.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:03:04.040051: step 21090, loss = 0.68 (966.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:03:05.488278: step 21100, loss = 0.74 (883.8 examples/sec; 0.145 sec/batch)
2017-05-07 20:03:06.715789: step 21110, loss = 0.78 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-07 20:03:08.051584: step 21120, loss = 0.82 (958.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:03:09.374211: step 21130, loss = 0.69 (967.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:03:10.711269: step 21140, loss = 0.87 (957.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:03:12.130862: step 21150, loss = 0.91 (901.7 examples/sec; 0.142 sec/batch)
2017-05-07 20:03:13.391203: step 21160, loss = 0.85 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-07 20:03:14.705074: step 21170, loss = 0.71 (974.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:03:16.047840: step 21180, loss = 0.65 (953.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:03:17.376637: step 21190, loss = 0.76 (963.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:03:18.869819: step 21200, loss = 0.89 (857.2 examples/sec; 0.149 sec/batch)
2017-05-07 20:03:20.099041: step 21210, loss = 0.77 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-07 20:03:21.412753: step 21220, loss = 0.93 (974.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:03:22.767292: step 21230, loss = 0.85 (945.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:03:24.115228: step 21240, loss = 0.71 (949.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:03:25.507246: step 21250, loss = 0.90 (919.5 examples/sec; 0.139 sec/batch)
2017-05-07 20:03:26.812999: step 21260, loss = 0.83 (980.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:03:28.150592: step 21270, loss = 0.85 (956.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:03:29.485637: step 21280, loss = 0.58 (958.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:03:30.825721: step 21290, loss = 0.69 (955.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:03:32.293165: step 21300, loss = 0.78 (872.3 examples/sec; 0.147 sec/batch)
2017-05-07 20:03:33.490206: step 21310, loss = 0.88 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-07 20:03:34.828238: step 21320, loss = 0.75 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:03:36.188256: step 21330, loss = 0.80 (941.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:03:37.535117: step 21340, loss = 0.96 (950.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:03:38.894598: step 21350, loss = 0.82 (941.5 examples/sec; 0.136 sec/batch)
2017-05-07 20:03:40.199317: step 21360, loss = 0.95 (981.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:03:41.536197: step 21370, loss = 0.82 (957.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:03:42.859117: step 21380, loss = 0.78 (967.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:03:44.196550: step 21390, loss = 1.02 (957.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:03:45.650040: step 21400, loss = 0.79 (880.6 examples/sec; 0.145 sec/batch)
2017-05-07 20:03:46.851460: step 21410, loss = 0.88 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-07 20:03:48.212694: step 21420, loss = 0.82 (940.3 examples/sec; 0.136 sec/batch)
2017-05-07 20:03:49.561677: step 21430, loss = 0.91 (948.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:03:50.893210: step 21440, loss = 0.80 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:03:52.271704: step 21450, loss = 1.01 (928.6 examples/sec; 0.138 sec/batch)
2017-05-07 20:03:53.571872: step 21460, loss = 0.67 (984.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:03:54.916667: step 21470, loss = 0.82 (951.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:03:56.250702: step 21480, loss = 0.72 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:03:57.584424: step 21490, loss = 0.75 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:03:59.061981: step 21500, loss = 0.68 (866.3 examples/sec; 0.148 sec/batch)
2017-05-07 20:04:00.248850: step 21510, loss = 0.72 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-07 20:04:01.586320: step 21520, loss = 1.15 (957.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:04:02.919775: step 21530, loss = 0.99 (959.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:04:04.235160: step 21540, loss = 0.74 (973.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:04:05.624057: step 21550, loss = 0.78 (921.6 examples/sec; 0.139 sec/batch)
2017-05-07 20:04:06.904787: step 21560, loss = 0.92 (999.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:04:08.253002: step 21570, loss = 0.70 (949.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:04:09.604547: step 21580, loss = 0.58 (947.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:04:10.915831: step 21590, loss = 0.63 (976.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:04:12.395870: step 21600, loss = 0.86 (864.8 examples/sec; 0.148 sec/batch)
2017-05-07 20:04:13.572022: step 21610, loss = 0.86 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-07 20:04:14.914078: step 21620, loss = 0.98 (953.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:04:16.257103: step 21630, loss = 0.75 (953.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:04:17.592611: step 21640, loss = 0.90 (958.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:04:18.989498: step 21650, loss = 0.90 (916.3 examples/sec; 0.140 sec/batch)
2017-05-07 20:04:20.281125: step 21660, loss = 0.68 (991.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:04:21.595281: step 21670, loss = 0.65 (974.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:04:22.914767: step 21680, loss = 0.78 (970.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:04:24.247906: step 21690, loss = 0.83 (960.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:04:25.715779: step 21700, loss = 0.75 (872.0 examples/sec; 0.147 sec/batch)
2017-05-07 20:04:26.919336: step 21710, loss = 0.80 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-07 20:04:28.248214: step 21720, loss = 0.82 (963.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:04:29.567920: step 21730, loss = 0.64 (969.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:04:30.889740: step 21740, loss = 0.71 (968.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:04:32.281680: step 21750, loss = 0.78 (919.6 examples/sec; 0.139 sec/batch)
2017-05-07 20:04:33.574801: step 21760, loss = 0.92 (989.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:04:34.893995: step 21770, loss = 0.83 (970.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:04:36.248591: step 21780, loss = 0.81 (944.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:04:37.573782: step 21790, loss = 0.74 (965.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:04:39.032486: step 21800, loss = 0.66 (877.5 examples/sec; 0.146 sec/batch)
2017-05-07 20:04:40.244502: step 21810, loss = 0.95 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-07 20:04:41.562680: step 21820, loss = 0.95 (971.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:04:42.898160: step 21830, loss = 0.73 (958.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:04:44.261682: step 21840, loss = 0.89 (938.7 examples/sec; 0.136 sec/batch)
2017-05-07 20:04:45.646682: step 21850, loss = 0.70 (924.2 examples/sec; 0.139 sec/batch)
2017-05-07 20:04:46.934643: step 21860, loss = 0.77 (993.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:04:48.283591: step 21870, loss = 0.81 (948.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:04:49.592928: step 21880, loss = 0.73 (977.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:04:50.918887: step 21890, loss = 0.88 (965.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:04:52.386896: step 21900, loss = 0.78 (871.9 examples/sec; 0.147 sec/batch)
2017-05-07 20:04:53.558447: step 21910, loss = 0.83 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-07 20:04:54.898163: step 21920, loss = 0.93 (955.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:04:56.249234: step 21930, loss = 0.83 (947.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:04:57.560815: step 21940, loss = 0.63 (975.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:04:58.963141: step 21950, loss = 0.91 (912.8 examples/sec; 0.140 sec/batch)
2017-05-07 20:05:00.264889: step 21960, loss = 0.93 (983.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:05:01.634268: step 21970, loss = 0.80 (934.7 examples/sec; 0.137 sec/batch)
2017-05-07 20:05:02.962469: step 21980, loss = 0.78 (963.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:05:04.323395: step 21990, loss = 0.86 (940.5 examples/sec; 0.136 sec/batch)
2017-05-07 20:05:05.792838: step 22000, loss = 0.72 (871.1 examples/sec; 0.147 sec/batch)
2017-05-07 20:05:06.991580: step 22010, loss = 0.84 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-07 20:05:08.344159: step 22020, loss = 1.01 (946.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:05:09.672796: step 22030, loss = 0.74 (963.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:05:11.022429: step 22040, loss = 0.78 (948.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:05:12.382618: step 22050, loss = 0.82 (941.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:05:13.662103: step 22060, loss = 0.82 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:05:14.989773: step 22070, loss = 0.86 (964.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:05:16.338804: step 22080, loss = 0.66 (948.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:05:17.680706: step 22090, loss = 0.81 (953.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:05:19.165849: step 22100, loss = 0.60 (861.9 examples/sec; 0.149 sec/batch)
2017-05-07 20:05:20.353881: step 22110, loss = 0.91 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-07 20:05:21.705088: step 22120, loss = 0.79 (947.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:05:23.053788: step 22130, loss = 0.74 (949.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:05:24.380795: step 22140, loss = 0.83 (964.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:05:25.761029: step 22150, loss = 0.72 (927.4 examples/sec; 0.138 sec/batch)
2017-05-07 20:05:27.061680: step 22160, loss = 0.82 (984.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:05:28.422558: step 22170, loss = 0.83 (940.6 examples/sec; 0.136 sec/batch)
2017-05-07 20:05:29.708174: step 22180, loss = 0.65 (995.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:05:31.037341: step 22190, loss = 0.82 (963.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:05:32.509158: step 22200, loss = 0.63 (869.7 examples/sec; 0.147 sec/batch)
2017-05-07 20:05:33.721821: step 22210, loss = 0.76 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-07 20:05:35.052438: step 22220, loss = 0.86 (962.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:05:36.392663: step 22230, loss = 0.80 (955.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:05:37.713947: step 22240, loss = 0.69 (968.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:05:39.104387: step 22250, loss = 0.72 (920.6 examples/sec; 0.139 sec/batch)
2017-05-07 20:05:40.399760: step 22260, loss = 0.79 (988.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:05:41.725236: step 22270, loss = 0.82 (965.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:05:43.055353: step 22280, loss = 0.97 (962.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:05:44.403001: step 22290, loss = 0.75 (949.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:05:45.877480: step 22300, loss = 0.65 (868.1 examples/sec; 0.147 sec/batch)
2017-05-07 20:05:47.102018: step 22310, loss = 0.74 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-07 20:05:48.442774: step 22320, loss = 0.71 (954.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:05:49.772945: step 22330, loss = 0.75 (962.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:05:51.093814: step 22340, loss = 0.89 (969.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:05:52.496499: step 22350, loss = 0.67 (912.5 examples/sec; 0.140 sec/batch)
2017-05-07 20:05:53.756055: step 22360, loss = 0.74 (1016.2 examples/sec; 0.126 sec/batch)
2017-05-07 20:05:55.136653: step 22370, loss = 0.90 (927.1 examples/sec; 0.138 sec/batch)
2017-05-07 20:05:56.457909: step 22380, loss = 0.75 (968.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:05:57.809003: step 22390, loss = 0.82 (947.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:05:59.266866: step 22400, loss = 0.82 (878.0 examples/sec; 0.146 sec/batch)
2017-05-07 20:06:00.471785: step 22410, loss = 0.89 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-07 20:06:01.824031: step 22420, loss = 0.80 (946.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:06:03.157620: step 22430, loss = 0.73 (959.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:06:04.492927: step 22440, loss = 0.83 (958.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:06:05.886306: step 22450, loss = 0.80 (918.6 examples/sec; 0.139 sec/batch)
2017-05-07 20:06:07.163412: step 22460, loss = 0.74 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:06:08.492029: step 22470, loss = 0.63 (963.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:06:09.813130: step 22480, loss = 0.77 (968.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:06:11.150312: step 22490, loss = 0.74 (957.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:06:12.683514: step 22500, loss = 0.60 (834.8 examples/sec; 0.153 sec/batch)
2017-05-07 20:06:13.822498: step 22510, loss = 0.94 (1123.8 examples/sec; 0.114 sec/batch)
2017-05-07 20:06:15.153662: step 22520, loss = 1.00 (961.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:06:16.493607: step 22530, loss = 0.77 (955.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:06:17.842941: step 22540, loss = 0.84 (948.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:06:19.235433: step 22550, loss = 0.69 (919.2 examples/sec; 0.139 sec/batch)
2017-05-07 20:06:20.517473: step 22560, loss = 0.78 (998.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:06:21.842670: step 22570, loss = 0.71 (965.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:06:23.175574: step 22580, loss = 0.62 (960.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:06:24.493126: step 22590, loss = 0.70 (971.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:06:25.964665: step 22600, loss = 0.87 (869.8 examples/sec; 0.147 sec/batch)
2017-05-07 20:06:27.176261: step 22610, loss = 0.85 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-07 20:06:28.485544: step 22620, loss = 0.68 (977.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:06:29.799433: step 22630, loss = 0.82 (974.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:06:31.168929: step 22640, loss = 0.87 (934.7 examples/sec; 0.137 sec/batch)
2017-05-07 20:06:32.568101: step 22650, loss = 0.73 (914.8 examples/sec; 0.140 sec/batch)
2017-05-07 20:06:33.873009: step 22660, loss = 0.76 (980.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:06:35.203534: step 22670, loss = 0.65 (962.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:06:36.532014: step 22680, loss = 0.81 (963.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:06:37.848100: step 22690, loss = 0.79 (972.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:06:39.330189: step 22700, loss = 0.78 (863.6 examples/sec; 0.148 sec/batch)
2017-05-07 20:06:40.533283: step 22710, loss = 0.88 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-07 20:06:41.894790: step 22720, loss = 0.78 (940.1 examples/sec; 0.136 sec/batch)
2017-05-07 20:06:43.206908: step 22730, loss = 0.72 (975.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:06:44.514441: step 22740, loss = 0.68 (978.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:06:45.910374: step 22750, loss = 0.83 (917.0 examples/sec; 0.140 sec/batch)
2017-05-07 20:06:47.181952: step 22760, loss = 0.62 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:06:48.502781: step 22770, loss = 0.91 (969.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:06:49.831627: step 22780, loss = 0.85 (963.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:06:51.159800: step 22790, loss = 0.80 (963.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:06:52.608257: step 22800, loss = 0.76 (883.7 examples/sec; 0.145 sec/batch)
2017-05-07 20:06:53.805299: step 22810, loss = 0.72 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-07 20:06:55.153417: step 22820, loss = 0.76 (949.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:06:56.498522: step 22830, loss = 0.76 (951.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:06:57.818914: step 22840, loss = 0.78 (969.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:06:59.197396: step 22850, loss = 0.75 (928.6 examples/sec; 0.138 sec/batch)
2017-05-07 20:07:00.530251: step 22860, loss = 0.76 (960.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:07:01.869184: step 22870, loss = 0.81 (956.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:07:03.215132: step 22880, loss = 0.83 (951.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:07:04.562946: step 22890, loss = 0.78 (949.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:07:05.994680: step 22900, loss = 0.76 (894.0 examples/sec; 0.143 sec/batch)
2017-05-07 20:07:07.235987: step 22910, loss = 0.93 (1031.2 examples/sec; 0.124 sec/batch)
2017-05-07 20:07:08.565052: step 22920, loss = 0.81 (963.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:07:09.887420: step 22930, loss = 0.79 (968.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:07:11.229098: step 22940, loss = 0.75 (954.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:07:12.612219: step 22950, loss = 0.77 (925.5 examples/sec; 0.138 sec/batch)
2017-05-07 20:07:13.917925: step 22960, loss = 0.71 (980.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:07:15.251573: step 22970, loss = 0.60 (959.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:07:16.574851: step 22980, loss = 0.79 (967.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:07:17.924643: step 22990, loss = 0.78 (948.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:07:19.410175: step 23000, loss = 0.69 (861.6 examples/sec; 0.149 sec/batch)
2017-05-07 20:07:20.602262: step 23010, loss = 0.80 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-07 20:07:21.938154: step 23020, loss = 0.79 (958.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:07:23.257651: step 23030, loss = 0.85 (970.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:07:24.588065: step 23040, loss = 0.77 (962.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:07:25.983287: step 23050, loss = 0.68 (917.4 examples/sec; 0.140 sec/batch)
2017-05-07 20:07:27.259726: step 23060, loss = 0.69 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:07:28.594865: step 23070, loss = 0.80 (958.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:07:29.940639: step 23080, loss = 1.00 (951.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:07:31.299703: step 23090, loss = 0.81 (941.8 examples/sec; 0.136 sec/batch)
2017-05-07 20:07:32.813365: step 23100, loss = 0.83 (845.6 examples/sec; 0.151 sec/batch)
2017-05-07 20:07:33.976740: step 23110, loss = 0.82 (1100.3 examples/sec; 0.116 sec/batch)
2017-05-07 20:07:35.329569: step 23120, loss = 0.86 (946.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:07:36.655490: step 23130, loss = 0.97 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:07:37.985832: step 23140, loss = 0.85 (962.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:07:39.408183: step 23150, loss = 0.86 (899.9 examples/sec; 0.142 sec/batch)
2017-05-07 20:07:40.681060: step 23160, loss = 0.64 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-07 20:07:42.015331: step 23170, loss = 0.84 (959.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:07:43.381556: step 23180, loss = 0.82 (936.9 examples/sec; 0.137 sec/batch)
2017-05-07 20:07:44.714742: step 23190, loss = 0.69 (960.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:07:46.157742: step 23200, loss = 0.84 (887.0 examples/sec; 0.144 sec/batch)
2017-05-07 20:07:47.342391: step 23210, loss = 0.69 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-07 20:07:48.672329: step 23220, loss = 0.79 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:07:49.975373: step 23230, loss = 0.64 (982.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:07:51.332867: step 23240, loss = 0.87 (942.9 examples/sec; 0.136 sec/batch)
2017-05-07 20:07:52.741222: step 23250, loss = 0.83 (908.9 examples/sec; 0.141 sec/batch)
2017-05-07 20:07:54.020160: step 23260, loss = 0.68 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:07:55.358671: step 23270, loss = 0.72 (956.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:07:56.685283: step 23280, loss = 0.92 (964.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:07:58.016638: step 23290, loss = 0.67 (961.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:07:59.479743: step 23300, loss = 0.76 (874.9 examples/sec; 0.146 sec/batch)
2017-05-07 20:08:00.665791: step 23310, loss = 0.88 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-07 20:08:02.020029: step 23320, loss = 0.74 (945.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:08:03.355268: step 23330, loss = 0.82 (958.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:08:04.668157: step 23340, loss = 0.88 (975.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:08:06.043928: step 23350, loss = 0.85 (930.4 examples/sec; 0.138 sec/batch)
2017-05-07 20:08:07.337314: step 23360, loss = 0.71 (989.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:08:08.667394: step 23370, loss = 0.84 (962.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:08:10.001792: step 23380, loss = 0.75 (959.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:08:11.327781: step 23390, loss = 0.72 (965.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:08:12.794704: step 23400, loss = 0.96 (872.6 examples/sec; 0.147 sec/batch)
2017-05-07 20:08:13.979502: step 23410, loss = 0.86 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-07 20:08:15.315163: step 23420, loss = 0.82 (958.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:08:16.647823: step 23430, loss = 0.80 (960.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:08:18.000107: step 23440, loss = 0.77 (946.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:08:19.366253: step 23450, loss = 0.79 (936.9 examples/sec; 0.137 sec/batch)
2017-05-07 20:08:20.676732: step 23460, loss = 0.81 (976.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:08:22.037093: step 23470, loss = 0.84 (940.9 examples/sec; 0.136 sec/batch)
2017-05-07 20:08:23.353192: step 23480, loss = 0.80 (972.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:08:24.684197: step 23490, loss = 0.82 (961.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:08:26.151523: step 23500, loss = 0.74 (872.3 examples/sec; 0.147 sec/batch)
2017-05-07 20:08:27.363946: step 23510, loss = 0.78 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-07 20:08:28.678741: step 23520, loss = 0.66 (973.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:08:30.006240: step 23530, loss = 0.67 (964.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:08:31.336560: step 23540, loss = 0.67 (962.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:08:32.714856: step 23550, loss = 0.69 (928.7 examples/sec; 0.138 sec/batch)
2017-05-07 20:08:34.008825: step 23560, loss = 0.81 (989.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:08:35.337130: step 23570, loss = 0.70 (963.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:08:36.654841: step 23580, loss = 0.86 (971.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:08:37.990902: step 23590, loss = 0.52 (958.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:08:39.459167: step 23600, loss = 0.78 (871.8 examples/sec; 0.147 sec/batch)
2017-05-07 20:08:40.660525: step 23610, loss = 0.81 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-07 20:08:41.986979: step 23620, loss = 0.69 (965.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:08:43.322214: step 23630, loss = 0.77 (958.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:08:44.634030: step 23640, loss = 1.00 (975.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:08:46.018794: step 23650, loss = 0.73 (924.3 examples/sec; 0.138 sec/batch)
2017-05-07 20:08:47.310266: step 23660, loss = 0.81 (991.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:08:48.613842: step 23670, loss = 0.85 (981.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:08:49.965684: step 23680, loss = 0.72 (946.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:08:51.274749: step 23690, loss = 0.73 (977.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:08:52.751548: step 23700, loss = 0.82 (866.7 examples/sec; 0.148 sec/batch)
2017-05-07 20:08:53.959925: step 23710, loss = 0.78 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-07 20:08:55.309677: step 23720, loss = 0.62 (948.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:08:56.651873: step 23730, loss = 0.86 (953.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:08:57.969822: step 23740, loss = 0.74 (971.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:08:59.368634: step 23750, loss = 0.69 (915.1 examples/sec; 0.140 sec/batch)
2017-05-07 20:09:00.657443: step 23760, loss = 0.77 (993.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:09:01.980803: step 23770, loss = 0.78 (967.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:09:03.312352: step 23780, loss = 0.85 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:09:04.638051: step 23790, loss = 0.82 (965.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:09:06.099989: step 23800, loss = 0.80 (875.5 examples/sec; 0.146 sec/batch)
2017-05-07 20:09:07.297417: step 23810, loss = 0.75 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-07 20:09:08.611585: step 23820, loss = 0.85 (974.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:09:09.983527: step 23830, loss = 0.65 (933.0 examples/sec; 0.137 sec/batch)
2017-05-07 20:09:11.300577: step 23840, loss = 0.74 (971.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:09:12.693980: step 23850, loss = 0.95 (918.6 examples/sec; 0.139 sec/batch)
2017-05-07 20:09:14.004823: step 23860, loss = 0.68 (976.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:09:15.324258: step 23870, loss = 0.93 (970.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:09:16.652771: step 23880, loss = 0.64 (963.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:09:17.995653: step 23890, loss = 0.77 (953.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:09:19.457609: step 23900, loss = 0.69 (875.5 examples/sec; 0.146 sec/batch)
2017-05-07 20:09:20.634401: step 23910, loss = 0.76 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-07 20:09:21.950710: step 23920, loss = 0.72 (972.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:09:23.269677: step 23930, loss = 0.74 (970.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:09:24.583333: step 23940, loss = 0.86 (974.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:09:25.995549: step 23950, loss = 0.81 (906.4 examples/sec; 0.141 sec/batch)
2017-05-07 20:09:27.294011: step 23960, loss = 0.76 (985.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:09:28.599105: step 23970, loss = 0.73 (980.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:09:29.919302: step 23980, loss = 0.68 (969.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:09:31.266184: step 23990, loss = 0.95 (950.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:09:32.750120: step 24000, loss = 0.75 (862.6 examples/sec; 0.148 sec/batch)
2017-05-07 20:09:33.943497: step 24010, loss = 0.74 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-07 20:09:35.271717: step 24020, loss = 0.86 (963.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:09:36.612328: step 24030, loss = 0.74 (954.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:09:37.952472: step 24040, loss = 0.69 (955.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:09:39.313611: step 24050, loss = 0.81 (940.4 examples/sec; 0.136 sec/batch)
2017-05-07 20:09:40.593520: step 24060, loss = 0.91 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:09:41.923206: step 24070, loss = 0.80 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:09:43.259122: step 24080, loss = 0.72 (958.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:09:44.586089: step 24090, loss = 0.82 (964.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:09:46.035397: step 24100, loss = 0.94 (883.2 examples/sec; 0.145 sec/batch)
2017-05-07 20:09:47.259730: step 24110, loss = 0.88 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-07 20:09:48.583614: step 24120, loss = 0.71 (966.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:09:49.916961: step 24130, loss = 0.69 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:09:51.250672: step 24140, loss = 0.97 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:09:52.644234: step 24150, loss = 0.66 (918.5 examples/sec; 0.139 sec/batch)
2017-05-07 20:09:53.924530: step 24160, loss = 0.75 (999.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:09:55.257994: step 24170, loss = 0.87 (959.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:09:56.565210: step 24180, loss = 0.76 (979.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:09:57.901693: step 24190, loss = 0.84 (957.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:09:59.357691: step 24200, loss = 0.90 (879.1 examples/sec; 0.146 sec/batch)
2017-05-07 20:10:00.570303: step 24210, loss = 0.70 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-07 20:10:01.930262: step 24220, loss = 0.65 (941.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:10:03.283251: step 24230, loss = 0.65 (946.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:10:04.617371: step 24240, loss = 0.64 (959.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:10:06.000399: step 24250, loss = 0.89 (925.5 examples/sec; 0.138 sec/batch)
2017-05-07 20:10:07.271151: step 24260, loss = 0.67 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:10:08.600498: step 24270, loss = 0.62 (962.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:10:09.927382: step 24280, loss = 0.70 (964.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:10:11.262833: step 24290, loss = 0.78 (958.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:10:12.740756: step 24300, loss = 0.76 (866.1 examples/sec; 0.148 sec/batch)
2017-05-07 20:10:13.929100: step 24310, loss = 0.76 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-07 20:10:15.252708: step 24320, loss = 0.88 (967.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:10:16.566310: step 24330, loss = 0.77 (974.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:10:17.894010: step 24340, loss = 0.74 (964.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:10:19.246123: step 24350, loss = 0.86 (946.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:10:20.547084: step 24360, loss = 0.87 (983.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:10:21.884901: step 24370, loss = 1.06 (956.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:10:23.221115: step 24380, loss = 0.78 (957.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:10:24.544398: step 24390, loss = 0.76 (967.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:10:25.981535: step 24400, loss = 0.82 (890.7 examples/sec; 0.144 sec/batch)
2017-05-07 20:10:27.214545: step 24410, loss = 0.78 (1038.1 examples/sec; 0.123 sec/batch)
2017-05-07 20:10:28.554723: step 24420, loss = 0.66 (955.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:10:29.886107: step 24430, loss = 0.76 (961.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:10:31.236197: step 24440, loss = 0.73 (948.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:10:32.593927: step 24450, loss = 0.75 (942.8 examples/sec; 0.136 sec/batch)
2017-05-07 20:10:33.905081: step 24460, loss = 0.67 (976.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:10:35.252246: step 24470, loss = 0.71 (950.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:10:36.587118: step 24480, loss = 0.72 (958.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:10:37.933985: step 24490, loss = 0.98 (950.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:10:39.409214: step 24500, loss = 0.94 (867.7 examples/sec; 0.148 sec/batch)
2017-05-07 20:10:40.601648: step 24510, loss = 0.85 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-07 20:10:41.926148: step 24520, loss = 0.82 (966.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:10:43.250979: step 24530, loss = 0.75 (966.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:10:44.575532: step 24540, loss = 0.73 (966.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:10:45.993165: step 24550, loss = 0.93 (902.9 examples/sec; 0.142 sec/batch)
2017-05-07 20:10:47.253795: step 24560, loss = 0.64 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:10:48.583562: step 24570, loss = 0.80 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:10:49.909777: step 24580, loss = 0.70 (965.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:10:51.228684: step 24590, loss = 0.70 (970.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:10:52.713587: step 24600, loss = 0.78 (862.0 examples/sec; 0.148 sec/batch)
2017-05-07 20:10:53.932950: step 24610, loss = 0.77 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-07 20:10:55.282332: step 24620, loss = 0.71 (948.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:10:56.628122: step 24630, loss = 0.73 (951.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:10:57.933166: step 24640, loss = 0.87 (980.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:10:59.302700: step 24650, loss = 0.81 (934.6 examples/sec; 0.137 sec/batch)
2017-05-07 20:11:00.599054: step 24660, loss = 0.79 (987.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:11:01.935331: step 24670, loss = 0.78 (957.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:11:03.226896: step 24680, loss = 0.75 (991.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:11:04.565051: step 24690, loss = 0.87 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:11:06.004510: step 24700, loss = 0.61 (889.2 examples/sec; 0.144 sec/batch)
2017-05-07 20:11:07.212442: step 24710, loss = 0.80 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-07 20:11:08.512031: step 24720, loss = 0.83 (984.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:11:09.845569: step 24730, loss = 0.72 (959.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:11:11.172955: step 24740, loss = 0.73 (964.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:11:12.550760: step 24750, loss = 0.83 (929.0 examples/sec; 0.138 sec/batch)
2017-05-07 20:11:13.847112: step 24760, loss = 0.83 (987.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:11:15.180110: step 24770, loss = 0.73 (960.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:11:16.499627: step 24780, loss = 0.89 (970.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:11:17.813063: step 24790, loss = 0.80 (974.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:11:19.282990: step 24800, loss = 0.64 (870.8 examples/sec; 0.147 sec/batch)
2017-05-07 20:11:20.470937: step 24810, loss = 0.76 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-07 20:11:21.795717: step 24820, loss = 0.79 (966.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:11:23.139613: step 24830, loss = 0.95 (952.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:11:24.452854: step 24840, loss = 0.81 (974.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:11:25.834211: step 24850, loss = 0.76 (926.6 examples/sec; 0.138 sec/batch)
2017-05-07 20:11:27.152404: step 24860, loss = 0.76 (971.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:11:28.447730: step 24870, loss = 0.65 (988.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:11:29.820862: step 24880, loss = 0.69 (932.2 examples/sec; 0.137 sec/batch)
2017-05-07 20:11:31.163685: step 24890, loss = 0.76 (953.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:11:32.633673: step 24900, loss = 0.81 (870.8 examples/sec; 0.147 sec/batch)
2017-05-07 20:11:33.844638: step 24910, loss = 0.88 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-07 20:11:35.167579: step 24920, loss = 0.71 (967.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:11:36.520420: step 24930, loss = 0.74 (946.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:11:37.838325: step 24940, loss = 0.78 (971.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:11:39.219910: step 24950, loss = 0.86 (926.5 examples/sec; 0.138 sec/batch)
2017-05-07 20:11:40.509905: step 24960, loss = 0.75 (992.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:11:41.846056: step 24970, loss = 0.70 (958.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:11:43.181326: step 24980, loss = 0.73 (958.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:11:44.523898: step 24990, loss = 0.71 (953.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:11:46.002515: step 25000, loss = 0.82 (865.7 examples/sec; 0.148 sec/batch)
2017-05-07 20:11:47.190717: step 25010, loss = 0.68 (1077.3 examples/sec; 0.119 sec/batch)
2017-05-07 20:11:48.532133: step 25020, loss = 0.86 (954.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:11:49.870380: step 25030, loss = 0.76 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:11:51.201704: step 25040, loss = 0.94 (961.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:11:52.575047: step 25050, loss = 0.90 (932.0 examples/sec; 0.137 sec/batch)
2017-05-07 20:11:53.868631: step 25060, loss = 0.95 (989.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:11:55.209704: step 25070, loss = 0.65 (954.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:11:56.534866: step 25080, loss = 0.86 (965.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:11:57.862770: step 25090, loss = 0.70 (963.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:11:59.321079: step 25100, loss = 0.86 (877.7 examples/sec; 0.146 sec/batch)
2017-05-07 20:12:00.517765: step 25110, loss = 0.92 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-07 20:12:01.845441: step 25120, loss = 0.55 (964.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:12:03.178510: step 25130, loss = 0.73 (960.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:12:04.519090: step 25140, loss = 0.84 (954.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:12:05.896526: step 25150, loss = 0.77 (929.3 examples/sec; 0.138 sec/batch)
2017-05-07 20:12:07.173945: step 25160, loss = 0.74 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:12:08.500914: step 25170, loss = 0.70 (964.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:12:09.828525: step 25180, loss = 0.67 (964.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:12:11.154117: step 25190, loss = 0.63 (965.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:12:12.622228: step 25200, loss = 0.77 (871.9 examples/sec; 0.147 sec/batch)
2017-05-07 20:12:13.833617: step 25210, loss = 0.71 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-07 20:12:15.176673: step 25220, loss = 0.63 (953.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:12:16.525319: step 25230, loss = 0.76 (949.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:12:17.818882: step 25240, loss = 0.77 (989.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:12:19.221524: step 25250, loss = 0.90 (912.5 examples/sec; 0.140 sec/batch)
2017-05-07 20:12:20.509933: step 25260, loss = 0.73 (993.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:12:21.848637: step 25270, loss = 0.71 (956.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:12:23.187668: step 25280, loss = 0.94 (955.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:12:24.510231: step 25290, loss = 0.66 (967.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:12:25.981866: step 25300, loss = 0.87 (869.8 examples/sec; 0.147 sec/batch)
2017-05-07 20:12:27.206077: step 25310, loss = 0.79 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-07 20:12:28.520863: step 25320, loss = 0.80 (973.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:12:29.860684: step 25330, loss = 0.80 (955.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:12:31.183823: step 25340, loss = 0.78 (967.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:12:32.573203: step 25350, loss = 0.66 (921.3 examples/sec; 0.139 sec/batch)
2017-05-07 20:12:33.885640: step 25360, loss = 0.68 (975.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:12:35.196280: step 25370, loss = 0.84 (976.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:12:36.524383: step 25380, loss = 0.77 (963.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:12:37.825425: step 25390, loss = 0.90 (983.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:12:39.309874: step 25400, loss = 0.67 (862.3 examples/sec; 0.148 sec/batch)
2017-05-07 20:12:40.526016: step 25410, loss = 0.75 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-07 20:12:41.851659: step 25420, loss = 0.65 (965.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:12:43.188405: step 25430, loss = 0.79 (957.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:12:44.512762: step 25440, loss = 0.70 (966.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:12:45.887789: step 25450, loss = 0.73 (930.9 examples/sec; 0.138 sec/batch)
2017-05-07 20:12:47.184513: step 25460, loss = 0.79 (987.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:12:48.521138: step 25470, loss = 0.79 (957.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:12:49.864840: step 25480, loss = 0.57 (952.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:12:51.177041: step 25490, loss = 0.87 (975.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:12:52.639982: step 25500, loss = 0.69 (874.9 examples/sec; 0.146 sec/batch)
2017-05-07 20:12:53.847309: step 25510, loss = 0.77 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-07 20:12:55.178054: step 25520, loss = 0.78 (961.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:12:56.509431: step 25530, loss = 0.88 (961.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:12:57.862820: step 25540, loss = 0.72 (945.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:12:59.244707: step 25550, loss = 0.86 (926.3 examples/sec; 0.138 sec/batch)
2017-05-07 20:13:00.514036: step 25560, loss = 0.65 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:13:01.856543: step 25570, loss = 0.85 (953.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:13:03.190144: step 25580, loss = 0.83 (959.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:13:04.510031: step 25590, loss = 0.80 (969.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:13:05.979906: step 25600, loss = 1.00 (870.8 examples/sec; 0.147 sec/batch)
2017-05-07 20:13:07.180080: step 25610, loss = 0.82 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-07 20:13:08.490839: step 25620, loss = 0.73 (976.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:13:09.836806: step 25630, loss = 0.75 (951.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:13:11.156391: step 25640, loss = 0.81 (970.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:13:12.550762: step 25650, loss = 0.95 (918.0 examples/sec; 0.139 sec/batch)
2017-05-07 20:13:13.838813: step 25660, loss = 0.70 (993.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:13:15.175609: step 25670, loss = 0.78 (957.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:13:16.527759: step 25680, loss = 0.55 (946.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:13:17.856111: step 25690, loss = 0.83 (963.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:13:19.339144: step 25700, loss = 0.87 (863.1 examples/sec; 0.148 sec/batch)
2017-05-07 20:13:20.513621: step 25710, loss = 0.86 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-07 20:13:21.852886: step 25720, loss = 0.85 (955.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:13:23.188523: step 25730, loss = 0.75 (958.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:13:24.496658: step 25740, loss = 0.88 (978.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:13:25.892946: step 25750, loss = 0.77 (916.7 examples/sec; 0.140 sec/batch)
2017-05-07 20:13:27.170503: step 25760, loss = 0.85 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:13:28.516428: step 25770, loss = 0.77 (951.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:13:29.852132: step 25780, loss = 0.68 (958.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:13:31.175704: step 25790, loss = 0.82 (967.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:13:32.676327: step 25800, loss = 0.85 (853.0 examples/sec; 0.150 sec/batch)
2017-05-07 20:13:33.862376: step 25810, loss = 0.80 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-07 20:13:35.188030: step 25820, loss = 0.83 (965.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:13:36.505805: step 25830, loss = 0.74 (971.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:13:37.852497: step 25840, loss = 0.69 (950.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:13:39.235118: step 25850, loss = 0.75 (925.7 examples/sec; 0.138 sec/batch)
2017-05-07 20:13:40.530630: step 25860, loss = 0.75 (988.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:13:41.871674: step 25870, loss = 0.78 (954.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:13:43.191490: step 25880, loss = 0.68 (969.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:13:44.531960: step 25890, loss = 0.85 (954.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:13:45.990677: step 25900, loss = 0.74 (877.5 examples/sec; 0.146 sec/batch)
2017-05-07 20:13:47.202447: step 25910, loss = 0.77 (1056.3 examples/sec; 0.121 sec/batch)
2017-05-07 20:13:48.520690: step 25920, loss = 0.68 (971.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:13:49.888014: step 25930, loss = 0.83 (936.1 examples/sec; 0.137 sec/batch)
2017-05-07 20:13:51.218539: step 25940, loss = 0.73 (962.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:13:52.589888: step 25950, loss = 0.86 (933.4 examples/sec; 0.137 sec/batch)
2017-05-07 20:13:53.901503: step 25960, loss = 0.84 (975.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:13:55.217763: step 25970, loss = 0.74 (972.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:13:56.557220: step 25980, loss = 0.70 (955.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:13:57.890223: step 25990, loss = 0.68 (960.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:13:59.356821: step 26000, loss = 0.92 (872.8 examples/sec; 0.147 sec/batch)
2017-05-07 20:14:00.536831: step 26010, loss = 0.70 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-07 20:14:01.870792: step 26020, loss = 0.66 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:14:03.201242: step 26030, loss = 0.68 (962.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:14:04.523793: step 26040, loss = 0.69 (967.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:14:05.921397: step 26050, loss = 0.74 (915.9 examples/sec; 0.140 sec/batch)
2017-05-07 20:14:07.191840: step 26060, loss = 0.73 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:14:08.544731: step 26070, loss = 0.76 (946.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:14:09.879910: step 26080, loss = 0.90 (958.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:14:11.203126: step 26090, loss = 0.67 (967.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:14:12.714942: step 26100, loss = 0.89 (846.7 examples/sec; 0.151 sec/batch)
2017-05-07 20:14:13.878765: step 26110, loss = 0.87 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-07 20:14:15.236951: step 26120, loss = 0.78 (942.4 examples/sec; 0.136 sec/batch)
2017-05-07 20:14:16.574631: step 26130, loss = 0.80 (956.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:14:17.901765: step 26140, loss = 0.99 (964.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:14:19.298131: step 26150, loss = 0.81 (916.7 examples/sec; 0.140 sec/batch)
2017-05-07 20:14:20.599591: step 26160, loss = 0.82 (983.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:14:21.901971: step 26170, loss = 0.68 (982.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:14:23.214674: step 26180, loss = 0.80 (975.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:14:24.522894: step 26190, loss = 0.77 (978.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:14:26.009246: step 26200, loss = 0.74 (861.2 examples/sec; 0.149 sec/batch)
2017-05-07 20:14:27.217101: step 26210, loss = 0.94 (1059.7 examples/sec; 0.121 sec/batch)
2017-05-07 20:14:28.533900: step 26220, loss = 0.65 (972.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:14:29.863907: step 26230, loss = 0.68 (962.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:14:31.205905: step 26240, loss = 0.90 (953.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:14:32.562567: step 26250, loss = 0.76 (943.5 examples/sec; 0.136 sec/batch)
2017-05-07 20:14:33.872572: step 26260, loss = 0.72 (977.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:14:35.204555: step 26270, loss = 0.59 (961.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:14:36.535813: step 26280, loss = 0.56 (961.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:14:37.875478: step 26290, loss = 0.83 (955.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:14:39.336810: step 26300, loss = 0.96 (875.9 examples/sec; 0.146 sec/batch)
2017-05-07 20:14:40.531542: step 26310, loss = 0.81 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-07 20:14:41.858066: step 26320, loss = 0.80 (964.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:14:43.193632: step 26330, loss = 0.72 (958.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:14:44.518535: step 26340, loss = 1.05 (966.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:14:45.899441: step 26350, loss = 0.64 (926.9 examples/sec; 0.138 sec/batch)
2017-05-07 20:14:47.185297: step 26360, loss = 0.86 (995.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:14:48.552575: step 26370, loss = 0.83 (936.2 examples/sec; 0.137 sec/batch)
2017-05-07 20:14:49.892004: step 26380, loss = 0.79 (955.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:14:51.199029: step 26390, loss = 0.79 (979.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:14:52.669469: step 26400, loss = 0.76 (870.5 examples/sec; 0.147 sec/batch)
2017-05-07 20:14:53.861824: step 26410, loss = 0.71 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-07 20:14:55.197003: step 26420, loss = 0.79 (958.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:14:56.535620: step 26430, loss = 0.80 (956.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:14:57.863394: step 26440, loss = 0.88 (964.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:14:59.259645: step 26450, loss = 0.66 (916.7 examples/sec; 0.140 sec/batch)
2017-05-07 20:15:00.551943: step 26460, loss = 0.69 (990.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:15:01.894047: step 26470, loss = 0.90 (953.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:15:03.218748: step 26480, loss = 0.75 (966.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:15:04.543132: step 26490, loss = 0.74 (966.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:15:06.012672: step 26500, loss = 0.84 (871.0 examples/sec; 0.147 sec/batch)
2017-05-07 20:15:07.218495: step 26510, loss = 0.72 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-07 20:15:08.541122: step 26520, loss = 1.01 (967.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:15:09.868662: step 26530, loss = 0.75 (964.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:15:11.191578: step 26540, loss = 0.76 (967.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:15:12.578310: step 26550, loss = 0.73 (923.0 examples/sec; 0.139 sec/batch)
2017-05-07 20:15:13.854435: step 26560, loss = 0.80 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:15:15.178133: step 26570, loss = 1.02 (967.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:15:16.512599: step 26580, loss = 0.68 (959.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:15:17.851742: step 26590, loss = 0.65 (955.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:15:19.370625: step 26600, loss = 0.81 (842.7 examples/sec; 0.152 sec/batch)
2017-05-07 20:15:20.537667: step 26610, loss = 0.85 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-07 20:15:21.857357: step 26620, loss = 0.68 (969.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:15:23.203777: step 26630, loss = 0.88 (950.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:15:24.521776: step 26640, loss = 0.70 (971.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:15:25.889279: step 26650, loss = 0.74 (936.0 examples/sec; 0.137 sec/batch)
2017-05-07 20:15:27.161407: step 26660, loss = 0.87 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:15:28.533900: step 26670, loss = 0.95 (932.6 examples/sec; 0.137 sec/batch)
2017-05-07 20:15:29.843418: step 26680, loss = 0.72 (977.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:15:31.144764: step 26690, loss = 0.87 (983.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:15:32.631667: step 26700, loss = 0.89 (860.8 examples/sec; 0.149 sec/batch)
2017-05-07 20:15:33.881857: step 26710, loss = 0.67 (1023.8 examples/sec; 0.125 sec/batch)
2017-05-07 20:15:35.171894: step 26720, loss = 0.81 (992.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:15:36.526209: step 26730, loss = 0.76 (945.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:15:37.837487: step 26740, loss = 0.70 (976.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:15:39.216937: step 26750, loss = 0.76 (927.9 examples/sec; 0.138 sec/batch)
2017-05-07 20:15:40.495860: step 26760, loss = 0.83 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:15:41.820106: step 26770, loss = 0.65 (966.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:15:43.162949: step 26780, loss = 0.80 (953.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:15:44.484596: step 26790, loss = 0.80 (968.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:15:45.940886: step 26800, loss = 0.80 (878.9 examples/sec; 0.146 sec/batch)
2017-05-07 20:15:47.139311: step 26810, loss = 0.84 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-07 20:15:48.485179: step 26820, loss = 0.74 (951.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:15:49.801715: step 26830, loss = 0.97 (972.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:15:51.138372: step 26840, loss = 0.84 (957.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:15:52.538667: step 26850, loss = 0.80 (914.1 examples/sec; 0.140 sec/batch)
2017-05-07 20:15:53.821568: step 26860, loss = 0.74 (997.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:15:55.187958: step 26870, loss = 0.87 (936.8 examples/sec; 0.137 sec/batch)
2017-05-07 20:15:56.522912: step 26880, loss = 0.75 (958.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:15:57.840644: step 26890, loss = 0.78 (971.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:15:59.301998: step 26900, loss = 0.82 (875.9 examples/sec; 0.146 sec/batch)
2017-05-07 20:16:00.491419: step 26910, loss = 0.94 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-07 20:16:01.825588: step 26920, loss = 0.86 (959.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:16:03.146242: step 26930, loss = 0.87 (969.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:16:04.478961: step 26940, loss = 0.66 (960.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:16:05.877136: step 26950, loss = 0.62 (915.5 examples/sec; 0.140 sec/batch)
2017-05-07 20:16:07.166130: step 26960, loss = 0.89 (993.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:16:08.515755: step 26970, loss = 0.68 (948.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:16:09.851565: step 26980, loss = 0.81 (958.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:16:11.170847: step 26990, loss = 0.93 (970.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:16:12.671635: step 27000, loss = 0.69 (852.9 examples/sec; 0.150 sec/batch)
2017-05-07 20:16:13.846429: step 27010, loss = 0.80 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-07 20:16:15.201247: step 27020, loss = 0.87 (944.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:16:16.526578: step 27030, loss = 0.70 (965.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:16:17.857588: step 27040, loss = 0.83 (961.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:16:19.235734: step 27050, loss = 0.80 (928.8 examples/sec; 0.138 sec/batch)
2017-05-07 20:16:20.508675: step 27060, loss = 0.70 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:16:21.831305: step 27070, loss = 0.86 (967.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:16:23.183974: step 27080, loss = 0.73 (946.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:16:24.498982: step 27090, loss = 0.73 (973.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:16:26.037595: step 27100, loss = 0.83 (831.9 examples/sec; 0.154 sec/batch)
2017-05-07 20:16:27.166662: step 27110, loss = 0.66 (1133.7 examples/sec; 0.113 sec/batch)
2017-05-07 20:16:28.510778: step 27120, loss = 0.74 (952.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:16:29.831472: step 27130, loss = 0.68 (969.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:16:31.170649: step 27140, loss = 0.78 (955.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:16:32.557825: step 27150, loss = 0.78 (922.7 examples/sec; 0.139 sec/batch)
2017-05-07 20:16:33.857180: step 27160, loss = 0.73 (985.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:16:35.175963: step 27170, loss = 0.68 (970.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:16:36.499570: step 27180, loss = 0.80 (967.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:16:37.818252: step 27190, loss = 0.77 (970.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:16:39.297936: step 27200, loss = 0.79 (865.0 examples/sec; 0.148 sec/batch)
2017-05-07 20:16:40.533804: step 27210, loss = 0.86 (1035.7 examples/sec; 0.124 sec/batch)
2017-05-07 20:16:41.863592: step 27220, loss = 0.81 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:16:43.205858: step 27230, loss = 0.65 (953.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:16:44.527630: step 27240, loss = 0.74 (968.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:16:45.902523: step 27250, loss = 0.70 (931.0 examples/sec; 0.137 sec/batch)
2017-05-07 20:16:47.209553: step 27260, loss = 0.83 (979.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:16:48.548265: step 27270, loss = 0.74 (956.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:16:49.878504: step 27280, loss = 0.98 (962.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:16:51.212059: step 27290, loss = 0.82 (959.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:16:52.710573: step 27300, loss = 0.69 (854.2 examples/sec; 0.150 sec/batch)
2017-05-07 20:16:53.940937: step 27310, loss = 0.72 (1040.3 examples/sec; 0.123 sec/batch)
2017-05-07 20:16:55.264760: step 27320, loss = 0.92 (966.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:16:56.591088: step 27330, loss = 0.75 (965.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:16:57.899955: step 27340, loss = 0.75 (977.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:16:59.312319: step 27350, loss = 0.71 (906.3 examples/sec; 0.141 sec/batch)
2017-05-07 20:17:00.590307: step 27360, loss = 0.93 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:17:01.915588: step 27370, loss = 0.73 (965.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:17:03.254289: step 27380, loss = 0.72 (956.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:17:04.583723: step 27390, loss = 0.78 (962.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:17:06.035976: step 27400, loss = 0.78 (881.4 examples/sec; 0.145 sec/batch)
2017-05-07 20:17:07.223498: step 27410, loss = 0.83 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-07 20:17:08.540898: step 27420, loss = 0.70 (971.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:17:09.858305: step 27430, loss = 0.80 (971.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:17:11.189294: step 27440, loss = 0.74 (961.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:17:12.543311: step 27450, loss = 0.68 (945.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:17:13.818374: step 27460, loss = 0.74 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:17:15.145600: step 27470, loss = 0.89 (964.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:17:16.487715: step 27480, loss = 0.83 (953.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:17:17.813396: step 27490, loss = 0.79 (965.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:17:19.304743: step 27500, loss = 0.74 (858.3 examples/sec; 0.149 sec/batch)
2017-05-07 20:17:20.504512: step 27510, loss = 0.65 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-07 20:17:21.816342: step 27520, loss = 0.98 (975.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:17:23.164904: step 27530, loss = 0.77 (949.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:17:24.492501: step 27540, loss = 0.78 (964.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:17:25.844746: step 27550, loss = 0.59 (946.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:17:27.136925: step 27560, loss = 0.90 (990.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:17:28.460016: step 27570, loss = 0.81 (967.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:17:29.804930: step 27580, loss = 0.94 (951.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:17:31.147556: step 27590, loss = 0.79 (953.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:17:32.626960: step 27600, loss = 0.85 (865.2 examples/sec; 0.148 sec/batch)
2017-05-07 20:17:33.824168: step 27610, loss = 0.83 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-07 20:17:35.167513: step 27620, loss = 0.79 (952.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:17:36.493834: step 27630, loss = 0.92 (965.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:17:37.833200: step 27640, loss = 0.78 (955.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:17:39.226745: step 27650, loss = 0.73 (918.5 examples/sec; 0.139 sec/batch)
2017-05-07 20:17:40.524393: step 27660, loss = 0.73 (986.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:17:41.870576: step 27670, loss = 0.73 (950.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:17:43.210624: step 27680, loss = 0.71 (955.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:17:44.527593: step 27690, loss = 0.79 (971.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:17:46.028665: step 27700, loss = 0.77 (852.7 examples/sec; 0.150 sec/batch)
2017-05-07 20:17:47.177191: step 27710, loss = 0.75 (1114.5 examples/sec; 0.115 sec/batch)
2017-05-07 20:17:48.508755: step 27720, loss = 0.68 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:17:49.825651: step 27730, loss = 0.68 (972.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:17:51.154854: step 27740, loss = 0.72 (963.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:17:52.532473: step 27750, loss = 0.66 (929.2 examples/sec; 0.138 sec/batch)
2017-05-07 20:17:53.818438: step 27760, loss = 0.79 (995.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:17:55.149142: step 27770, loss = 0.88 (961.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:17:56.476584: step 27780, loss = 0.73 (964.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:17:57.818390: step 27790, loss = 0.87 (953.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:17:59.314616: step 27800, loss = 0.89 (855.5 examples/sec; 0.150 sec/batch)
2017-05-07 20:18:00.513146: step 27810, loss = 0.78 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-07 20:18:01.842860: step 27820, loss = 0.75 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:18:03.193303: step 27830, loss = 0.77 (947.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:18:04.511794: step 27840, loss = 0.76 (970.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:18:05.898154: step 27850, loss = 0.70 (923.3 examples/sec; 0.139 sec/batch)
2017-05-07 20:18:07.223394: step 27860, loss = 0.75 (965.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:18:08.568531: step 27870, loss = 0.78 (951.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:18:09.913516: step 27880, loss = 0.71 (951.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:18:11.250586: step 27890, loss = 0.80 (957.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:18:12.743950: step 27900, loss = 0.76 (857.1 examples/sec; 0.149 sec/batch)
2017-05-07 20:18:13.905729: step 27910, loss = 0.89 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-07 20:18:15.264911: step 27920, loss = 0.65 (941.7 examples/sec; 0.136 sec/batch)
2017-05-07 20:18:16.603093: step 27930, loss = 0.71 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:18:17.932516: step 27940, loss = 0.87 (962.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:18:19.307375: step 27950, loss = 0.82 (931.0 examples/sec; 0.137 sec/batch)
2017-05-07 20:18:20.625787: step 27960, loss = 0.84 (970.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:18:21.986351: step 27970, loss = 0.66 (940.8 examples/sec; 0.136 sec/batch)
2017-05-07 20:18:23.321327: step 27980, loss = 0.77 (958.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:18:24.618676: step 27990, loss = 0.79 (986.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:18:26.096043: step 28000, loss = 0.74 (866.4 examples/sec; 0.148 sec/batch)
2017-05-07 20:18:27.279245: step 28010, loss = 0.80 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-07 20:18:28.598222: step 28020, loss = 0.66 (970.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:18:29.938501: step 28030, loss = 0.73 (955.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:18:31.260223: step 28040, loss = 0.73 (968.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:18:32.652124: step 28050, loss = 0.68 (919.6 examples/sec; 0.139 sec/batch)
2017-05-07 20:18:33.951987: step 28060, loss = 0.71 (984.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:18:35.263027: step 28070, loss = 0.89 (976.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:18:36.583869: step 28080, loss = 0.85 (969.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:18:37.922986: step 28090, loss = 0.58 (955.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:18:39.430419: step 28100, loss = 0.76 (849.1 examples/sec; 0.151 sec/batch)
2017-05-07 20:18:40.636826: step 28110, loss = 0.68 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-07 20:18:41.974649: step 28120, loss = 0.67 (956.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:18:43.293019: step 28130, loss = 0.77 (970.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:18:44.616525: step 28140, loss = 0.54 (967.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:18:46.006603: step 28150, loss = 1.00 (920.8 examples/sec; 0.139 sec/batch)
2017-05-07 20:18:47.301547: step 28160, loss = 0.63 (988.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:18:48.633911: step 28170, loss = 0.89 (960.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:18:49.950800: step 28180, loss = 0.79 (972.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:18:51.295130: step 28190, loss = 0.91 (952.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:18:52.752984: step 28200, loss = 0.81 (878.0 examples/sec; 0.146 sec/batch)
2017-05-07 20:18:53.968505: step 28210, loss = 0.80 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-07 20:18:55.290058: step 28220, loss = 0.75 (968.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:18:56.613314: step 28230, loss = 0.55 (967.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:18:57.922713: step 28240, loss = 0.70 (977.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:18:59.297388: step 28250, loss = 0.75 (931.1 examples/sec; 0.137 sec/batch)
2017-05-07 20:19:00.592917: step 28260, loss = 0.70 (988.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:19:01.931988: step 28270, loss = 0.75 (955.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:19:03.247961: step 28280, loss = 0.74 (972.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:19:04.576730: step 28290, loss = 0.94 (963.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:19:06.058680: step 28300, loss = 0.63 (863.7 examples/sec; 0.148 sec/batch)
2017-05-07 20:19:07.236717: step 28310, loss = 0.80 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-07 20:19:08.547700: step 28320, loss = 1.00 (976.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:19:09.852214: step 28330, loss = 0.76 (981.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:19:11.194523: step 28340, loss = 0.71 (953.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:19:12.581293: step 28350, loss = 0.75 (923.0 examples/sec; 0.139 sec/batch)
2017-05-07 20:19:13.864247: step 28360, loss = 0.83 (997.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:19:15.205863: step 28370, loss = 0.84 (954.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:19:16.541227: step 28380, loss = 0.70 (958.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:19:17.893025: step 28390, loss = 0.72 (946.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:19:19.355987: step 28400, loss = 0.76 (874.9 examples/sec; 0.146 sec/batch)
2017-05-07 20:19:20.564337: step 28410, loss = 0.77 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-07 20:19:21.901306: step 28420, loss = 0.66 (957.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:19:23.250663: step 28430, loss = 0.73 (948.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:19:24.548815: step 28440, loss = 0.98 (986.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:19:25.917366: step 28450, loss = 0.82 (935.3 examples/sec; 0.137 sec/batch)
2017-05-07 20:19:27.192111: step 28460, loss = 0.75 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:19:28.521977: step 28470, loss = 0.73 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:19:29.872520: step 28480, loss = 0.61 (947.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:19:31.205375: step 28490, loss = 0.67 (960.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:19:32.676404: step 28500, loss = 0.74 (870.1 examples/sec; 0.147 sec/batch)
2017-05-07 20:19:33.887673: step 28510, loss = 0.71 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-07 20:19:35.227633: step 28520, loss = 0.51 (955.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:19:36.557855: step 28530, loss = 0.70 (962.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:19:37.880140: step 28540, loss = 0.71 (968.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:19:39.280913: step 28550, loss = 0.90 (913.8 examples/sec; 0.140 sec/batch)
2017-05-07 20:19:40.563067: step 28560, loss = 0.76 (998.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:19:41.885130: step 28570, loss = 0.80 (968.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:19:43.232293: step 28580, loss = 0.69 (950.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:19:44.547624: step 28590, loss = 0.82 (973.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:19:46.057838: step 28600, loss = 0.75 (847.6 examples/sec; 0.151 sec/batch)
2017-05-07 20:19:47.264595: step 28610, loss = 0.70 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-07 20:19:48.584616: step 28620, loss = 0.73 (969.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:19:49.925794: step 28630, loss = 0.70 (954.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:19:51.268246: step 28640, loss = 0.82 (953.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:19:52.640869: step 28650, loss = 0.74 (932.5 examples/sec; 0.137 sec/batch)
2017-05-07 20:19:53.938812: step 28660, loss = 0.93 (986.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:19:55.283900: step 28670, loss = 0.88 (951.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:19:56.611222: step 28680, loss = 0.65 (964.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:19:57.946061: step 28690, loss = 0.74 (958.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:19:59.432610: step 28700, loss = 0.89 (861.1 examples/sec; 0.149 sec/batch)
2017-05-07 20:20:00.618150: step 28710, loss = 0.69 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-07 20:20:01.958547: step 28720, loss = 0.83 (954.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:20:03.307083: step 28730, loss = 0.89 (949.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:20:04.639399: step 28740, loss = 0.65 (960.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:20:06.030743: step 28750, loss = 0.66 (920.0 examples/sec; 0.139 sec/batch)
2017-05-07 20:20:07.331811: step 28760, loss = 0.87 (983.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:20:08.664012: step 28770, loss = 0.80 (960.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:20:09.994220: step 28780, loss = 0.73 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:20:11.336175: step 28790, loss = 0.67 (953.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:20:12.793407: step 28800, loss = 0.83 (878.4 examples/sec; 0.146 sec/batch)
2017-05-07 20:20:13.973398: step 28810, loss = 0.62 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-07 20:20:15.303575: step 28820, loss = 0.74 (962.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:20:16.650961: step 28830, loss = 0.81 (950.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:20:17.971632: step 28840, loss = 0.72 (969.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:20:19.381001: step 28850, loss = 0.85 (908.2 examples/sec; 0.141 sec/batch)
2017-05-07 20:20:20.667861: step 28860, loss = 0.70 (994.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:20:21.992130: step 28870, loss = 0.65 (966.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:20:23.335921: step 28880, loss = 0.74 (952.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:20:24.674242: step 28890, loss = 0.65 (956.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:20:26.143833: step 28900, loss = 0.81 (871.0 examples/sec; 0.147 sec/batch)
2017-05-07 20:20:27.355276: step 28910, loss = 0.80 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-07 20:20:28.691403: step 28920, loss = 1.01 (958.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:20:30.024645: step 28930, loss = 0.85 (960.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:20:31.342759: step 28940, loss = 0.59 (971.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:20:32.731394: step 28950, loss = 0.78 (921.8 examples/sec; 0.139 sec/batch)
2017-05-07 20:20:34.047388: step 28960, loss = 0.72 (972.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:20:35.358695: step 28970, loss = 0.85 (976.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:20:36.695559: step 28980, loss = 0.76 (957.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:20:38.043399: step 28990, loss = 0.70 (949.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:20:39.506207: step 29000, loss = 0.73 (875.0 examples/sec; 0.146 sec/batch)
2017-05-07 20:20:40.696828: step 29010, loss = 0.76 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-07 20:20:42.041860: step 29020, loss = 0.81 (951.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:20:43.366053: step 29030, loss = 0.88 (966.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:20:44.683310: step 29040, loss = 0.77 (971.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:20:46.053886: step 29050, loss = 0.67 (933.9 examples/sec; 0.137 sec/batch)
2017-05-07 20:20:47.353677: step 29060, loss = 0.66 (984.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:20:48.672079: step 29070, loss = 0.74 (970.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:20:50.020001: step 29080, loss = 0.70 (949.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:20:51.349832: step 29090, loss = 0.74 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:20:52.804918: step 29100, loss = 0.81 (879.7 examples/sec; 0.146 sec/batch)
2017-05-07 20:20:54.011046: step 29110, loss = 0.74 (1061.3 examples/sec; 0.121 sec/batch)
2017-05-07 20:20:55.338848: step 29120, loss = 0.65 (964.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:20:56.656339: step 29130, loss = 0.81 (971.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:20:57.997340: step 29140, loss = 0.79 (954.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:20:59.351905: step 29150, loss = 0.75 (945.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:21:00.656396: step 29160, loss = 0.76 (981.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:21:01.999188: step 29170, loss = 0.85 (953.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:21:03.358882: step 29180, loss = 0.72 (941.4 examples/sec; 0.136 sec/batch)
2017-05-07 20:21:04.703846: step 29190, loss = 0.81 (951.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:21:06.172904: step 29200, loss = 0.85 (871.3 examples/sec; 0.147 sec/batch)
2017-05-07 20:21:07.410995: step 29210, loss = 0.71 (1033.9 examples/sec; 0.124 sec/batch)
2017-05-07 20:21:08.709873: step 29220, loss = 0.84 (985.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:21:10.038435: step 29230, loss = 0.71 (963.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:21:11.378725: step 29240, loss = 0.71 (955.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:21:12.759644: step 29250, loss = 0.75 (926.9 examples/sec; 0.138 sec/batch)
2017-05-07 20:21:14.032802: step 29260, loss = 0.79 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:21:15.393844: step 29270, loss = 0.57 (940.5 examples/sec; 0.136 sec/batch)
2017-05-07 20:21:16.725908: step 29280, loss = 0.79 (960.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:21:18.070064: step 29290, loss = 0.69 (952.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:21:19.539421: step 29300, loss = 0.78 (871.1 examples/sec; 0.147 sec/batch)
2017-05-07 20:21:20.706569: step 29310, loss = 0.76 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-07 20:21:22.048775: step 29320, loss = 0.81 (953.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:21:23.368180: step 29330, loss = 0.61 (970.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:21:24.677223: step 29340, loss = 0.65 (977.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:21:26.078168: step 29350, loss = 0.74 (913.7 examples/sec; 0.140 sec/batch)
2017-05-07 20:21:27.374094: step 29360, loss = 0.78 (987.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:21:28.692175: step 29370, loss = 0.82 (971.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:21:30.012277: step 29380, loss = 0.78 (969.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:21:31.353608: step 29390, loss = 0.74 (954.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:21:32.827711: step 29400, loss = 0.82 (868.3 examples/sec; 0.147 sec/batch)
2017-05-07 20:21:34.046476: step 29410, loss = 0.76 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-07 20:21:35.364353: step 29420, loss = 0.85 (971.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:21:36.699300: step 29430, loss = 0.76 (958.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:21:38.042677: step 29440, loss = 0.98 (952.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:21:39.407917: step 29450, loss = 0.76 (937.6 examples/sec; 0.137 sec/batch)
2017-05-07 20:21:40.728307: step 29460, loss = 0.94 (969.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:21:42.081811: step 29470, loss = 0.90 (945.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:21:43.412992: step 29480, loss = 0.83 (961.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:21:44.722779: step 29490, loss = 0.81 (977.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:21:46.173360: step 29500, loss = 0.69 (882.4 examples/sec; 0.145 sec/batch)
2017-05-07 20:21:47.433885: step 29510, loss = 0.89 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-07 20:21:48.758946: step 29520, loss = 0.73 (966.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:21:50.084803: step 29530, loss = 0.78 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:21:51.425806: step 29540, loss = 0.82 (954.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:21:52.810050: step 29550, loss = 0.74 (924.7 examples/sec; 0.138 sec/batch)
2017-05-07 20:21:54.100196: step 29560, loss = 0.82 (992.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:21:55.442238: step 29570, loss = 0.70 (953.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:21:56.810954: step 29580, loss = 0.72 (935.2 examples/sec; 0.137 sec/batch)
2017-05-07 20:21:58.129985: step 29590, loss = 0.87 (970.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:21:59.576866: step 29600, loss = 0.55 (884.7 examples/sec; 0.145 sec/batch)
2017-05-07 20:22:00.805760: step 29610, loss = 0.69 (1041.6 examples/sec; 0.123 sec/batch)
2017-05-07 20:22:02.161782: step 29620, loss = 0.66 (943.9 examples/sec; 0.136 sec/batch)
2017-05-07 20:22:03.531574: step 29630, loss = 0.79 (934.4 examples/sec; 0.137 sec/batch)
2017-05-07 20:22:04.843122: step 29640, loss = 0.85 (975.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:22:06.209328: step 29650, loss = 0.86 (936.9 examples/sec; 0.137 sec/batch)
2017-05-07 20:22:07.514054: step 29660, loss = 0.80 (981.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:22:08.847532: step 29670, loss = 0.67 (959.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:22:10.200107: step 29680, loss = 0.81 (946.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:22:11.531650: step 29690, loss = 0.78 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:22:13.004871: step 29700, loss = 0.73 (868.8 examples/sec; 0.147 sec/batch)
2017-05-07 20:22:14.212538: step 29710, loss = 0.80 (1059.9 examples/sec; 0.121 sec/batch)
2017-05-07 20:22:15.559191: step 29720, loss = 0.82 (950.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:22:16.870555: step 29730, loss = 0.74 (976.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:22:18.210780: step 29740, loss = 0.74 (955.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:22:19.615722: step 29750, loss = 0.89 (911.1 examples/sec; 0.140 sec/batch)
2017-05-07 20:22:20.897194: step 29760, loss = 0.71 (998.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:22:22.235718: step 29770, loss = 0.87 (956.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:22:23.569815: step 29780, loss = 0.74 (959.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:22:24.903882: step 29790, loss = 0.73 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:22:26.369264: step 29800, loss = 0.90 (873.5 examples/sec; 0.147 sec/batch)
2017-05-07 20:22:27.592306: step 29810, loss = 0.81 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-07 20:22:28.918031: step 29820, loss = 0.78 (965.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:22:30.257333: step 29830, loss = 0.68 (955.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:22:31.606895: step 29840, loss = 0.71 (948.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:22:32.983421: step 29850, loss = 0.64 (929.9 examples/sec; 0.138 sec/batch)
2017-05-07 20:22:34.264812: step 29860, loss = 0.73 (998.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:22:35.611763: step 29870, loss = 0.61 (950.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:22:36.930049: step 29880, loss = 0.75 (971.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:22:38.251716: step 29890, loss = 0.78 (968.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:22:39.708417: step 29900, loss = 0.78 (878.7 examples/sec; 0.146 sec/batch)
2017-05-07 20:22:40.894755: step 29910, loss = 1.02 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-07 20:22:42.234778: step 29920, loss = 0.93 (955.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:22:43.559946: step 29930, loss = 0.76 (965.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:22:44.909609: step 29940, loss = 0.77 (948.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:22:46.288707: step 29950, loss = 0.66 (928.1 examples/sec; 0.138 sec/batch)
2017-05-07 20:22:47.592215: step 29960, loss = 0.92 (982.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:22:48.926171: step 29970, loss = 0.87 (959.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:22:50.274517: step 29980, loss = 0.64 (949.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:22:51.571258: step 29990, loss = 0.65 (987.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:22:53.030000: step 30000, loss = 0.72 (877.5 examples/sec; 0.146 sec/batch)
2017-05-07 20:22:54.240586: step 30010, loss = 0.75 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-07 20:22:55.577510: step 30020, loss = 0.75 (957.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:22:56.903632: step 30030, loss = 0.68 (965.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:22:58.239972: step 30040, loss = 0.84 (957.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:22:59.623385: step 30050, loss = 0.67 (925.2 examples/sec; 0.138 sec/batch)
2017-05-07 20:23:00.902127: step 30060, loss = 0.77 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:23:02.217140: step 30070, loss = 0.79 (973.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:23:03.525607: step 30080, loss = 0.73 (978.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:23:04.868612: step 30090, loss = 0.94 (953.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:23:06.324488: step 30100, loss = 0.57 (879.2 examples/sec; 0.146 sec/batch)
2017-05-07 20:23:07.564136: step 30110, loss = 0.73 (1032.5 examples/sec; 0.124 sec/batch)
2017-05-07 20:23:08.897290: step 30120, loss = 0.77 (960.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:23:10.268108: step 30130, loss = 0.69 (933.7 examples/sec; 0.137 sec/batch)
2017-05-07 20:23:11.598725: step 30140, loss = 0.73 (962.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:23:12.958249: step 30150, loss = 0.97 (941.5 examples/sec; 0.136 sec/batch)
2017-05-07 20:23:14.264570: step 30160, loss = 0.72 (979.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:23:15.599428: step 30170, loss = 0.92 (958.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:23:16.896292: step 30180, loss = 0.62 (987.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:23:18.224224: step 30190, loss = 0.65 (963.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:23:19.682196: step 30200, loss = 0.66 (877.9 examples/sec; 0.146 sec/batch)
2017-05-07 20:23:20.891992: step 30210, loss = 0.88 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-07 20:23:22.221832: step 30220, loss = 0.69 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:23:23.559912: step 30230, loss = 0.86 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:23:24.887257: step 30240, loss = 0.76 (964.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:23:26.303123: step 30250, loss = 0.71 (904.1 examples/sec; 0.142 sec/batch)
2017-05-07 20:23:27.583034: step 30260, loss = 0.77 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:23:28.910952: step 30270, loss = 0.86 (963.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:23:30.251510: step 30280, loss = 0.70 (954.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:23:31.582394: step 30290, loss = 0.64 (961.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:23:33.029293: step 30300, loss = 0.82 (884.7 examples/sec; 0.145 sec/batch)
2017-05-07 20:23:34.217900: step 30310, loss = 0.68 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-07 20:23:35.567634: step 30320, loss = 0.84 (948.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:23:36.886387: step 30330, loss = 0.73 (970.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:23:38.235944: step 30340, loss = 0.72 (948.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:23:39.618120: step 30350, loss = 0.68 (926.1 examples/sec; 0.138 sec/batch)
2017-05-07 20:23:40.937249: step 30360, loss = 0.80 (970.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:23:42.288837: step 30370, loss = 0.86 (947.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:23:43.624439: step 30380, loss = 0.78 (958.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:23:44.970773: step 30390, loss = 0.63 (950.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:23:46.440398: step 30400, loss = 0.84 (871.0 examples/sec; 0.147 sec/batch)
2017-05-07 20:23:47.637983: step 30410, loss = 0.74 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-07 20:23:48.963660: step 30420, loss = 0.73 (965.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:23:50.287323: step 30430, loss = 0.64 (967.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:23:51.622949: step 30440, loss = 0.65 (958.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:23:53.006111: step 30450, loss = 0.72 (925.4 examples/sec; 0.138 sec/batch)
2017-05-07 20:23:54.315602: step 30460, loss = 0.81 (977.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:23:55.642020: step 30470, loss = 0.88 (965.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:23:56.958855: step 30480, loss = 0.78 (972.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:23:58.293532: step 30490, loss = 0.70 (959.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:23:59.761327: step 30500, loss = 0.75 (872.1 examples/sec; 0.147 sec/batch)
2017-05-07 20:24:00.977024: step 30510, loss = 0.91 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-07 20:24:02.297241: step 30520, loss = 0.77 (969.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:24:03.652929: step 30530, loss = 0.77 (944.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:24:04.965347: step 30540, loss = 0.71 (975.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:24:06.356941: step 30550, loss = 0.61 (919.8 examples/sec; 0.139 sec/batch)
2017-05-07 20:24:07.644129: step 30560, loss = 0.64 (994.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:24:08.972034: step 30570, loss = 0.77 (963.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:24:10.291608: step 30580, loss = 0.70 (970.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:24:11.633210: step 30590, loss = 0.79 (954.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:24:13.098121: step 30600, loss = 0.80 (873.8 examples/sec; 0.146 sec/batch)
2017-05-07 20:24:14.291412: step 30610, loss = 0.71 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-07 20:24:15.623851: step 30620, loss = 0.81 (960.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:24:16.957883: step 30630, loss = 0.89 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:24:18.303791: step 30640, loss = 0.76 (951.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:24:19.678805: step 30650, loss = 0.78 (930.9 examples/sec; 0.138 sec/batch)
2017-05-07 20:24:20.960927: step 30660, loss = 0.74 (998.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:24:22.315751: step 30670, loss = 0.76 (944.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:24:23.625551: step 30680, loss = 0.67 (977.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:24:24.967020: step 30690, loss = 0.75 (954.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:24:26.436870: step 30700, loss = 0.75 (870.8 examples/sec; 0.147 sec/batch)
2017-05-07 20:24:27.630417: step 30710, loss = 0.68 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-07 20:24:28.950739: step 30720, loss = 0.86 (969.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:24:30.290573: step 30730, loss = 0.73 (955.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:24:31.611649: step 30740, loss = 0.65 (968.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:24:32.996982: step 30750, loss = 0.77 (924.0 examples/sec; 0.139 sec/batch)
2017-05-07 20:24:34.301252: step 30760, loss = 0.69 (981.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:24:35.630994: step 30770, loss = 0.74 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:24:36.975598: step 30780, loss = 0.83 (952.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:24:38.304615: step 30790, loss = 0.80 (963.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:24:39.775925: step 30800, loss = 0.69 (870.0 examples/sec; 0.147 sec/batch)
2017-05-07 20:24:40.965381: step 30810, loss = 0.83 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-07 20:24:42.320490: step 30820, loss = 0.70 (944.6 examples/sec; 0.136 sec/batch)
2017-05-07 20:24:43.638092: step 30830, loss = 0.79 (971.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:24:44.971467: step 30840, loss = 0.76 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:24:46.365765: step 30850, loss = 0.94 (918.0 examples/sec; 0.139 sec/batch)
2017-05-07 20:24:47.654754: step 30860, loss = 0.90 (993.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:24:48.985443: step 30870, loss = 0.72 (961.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:24:50.332003: step 30880, loss = 0.88 (950.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:24:51.682839: step 30890, loss = 0.82 (947.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:24:53.130106: step 30900, loss = 0.77 (884.4 examples/sec; 0.145 sec/batch)
2017-05-07 20:24:54.346941: step 30910, loss = 0.88 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-07 20:24:55.670939: step 30920, loss = 1.08 (966.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:24:57.013739: step 30930, loss = 0.66 (953.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:24:58.344036: step 30940, loss = 0.73 (962.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:24:59.726378: step 30950, loss = 0.73 (926.0 examples/sec; 0.138 sec/batch)
2017-05-07 20:25:00.994587: step 30960, loss = 0.69 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:25:02.343742: step 30970, loss = 0.79 (948.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:25:03.695693: step 30980, loss = 0.66 (946.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:25:05.014414: step 30990, loss = 0.86 (970.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:25:06.491725: step 31000, loss = 0.74 (866.4 examples/sec; 0.148 sec/batch)
2017-05-07 20:25:07.696674: step 31010, loss = 0.84 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-07 20:25:09.023566: step 31020, loss = 0.85 (964.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:25:10.372726: step 31030, loss = 0.80 (948.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:25:11.675723: step 31040, loss = 0.77 (982.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:25:13.038633: step 31050, loss = 0.67 (939.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:25:14.346432: step 31060, loss = 0.65 (978.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:25:15.669506: step 31070, loss = 0.83 (967.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:25:16.996888: step 31080, loss = 0.66 (964.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:25:18.337182: step 31090, loss = 0.83 (955.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:25:19.797572: step 31100, loss = 0.70 (876.5 examples/sec; 0.146 sec/batch)
2017-05-07 20:25:20.971723: step 31110, loss = 0.76 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-07 20:25:22.296106: step 31120, loss = 0.78 (966.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:25:23.604267: step 31130, loss = 0.78 (978.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:25:24.931085: step 31140, loss = 0.54 (964.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:25:26.312712: step 31150, loss = 0.71 (926.4 examples/sec; 0.138 sec/batch)
2017-05-07 20:25:27.598689: step 31160, loss = 0.79 (995.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:25:28.965640: step 31170, loss = 0.88 (936.4 examples/sec; 0.137 sec/batch)
2017-05-07 20:25:30.303291: step 31180, loss = 0.69 (956.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:25:31.644345: step 31190, loss = 0.81 (954.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:25:33.149334: step 31200, loss = 0.80 (850.5 examples/sec; 0.150 sec/batch)
2017-05-07 20:25:34.350048: step 31210, loss = 0.66 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-07 20:25:35.688895: step 31220, loss = 0.81 (956.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:25:37.028726: step 31230, loss = 0.81 (955.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:25:38.373806: step 31240, loss = 0.64 (951.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:25:39.746064: step 31250, loss = 0.79 (932.8 examples/sec; 0.137 sec/batch)
2017-05-07 20:25:41.027670: step 31260, loss = 0.78 (998.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:25:42.349226: step 31270, loss = 0.69 (968.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:25:43.668539: step 31280, loss = 0.66 (970.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:25:44.999172: step 31290, loss = 0.74 (962.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:25:46.515087: step 31300, loss = 0.79 (844.4 examples/sec; 0.152 sec/batch)
2017-05-07 20:25:47.708853: step 31310, loss = 0.89 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-07 20:25:49.052256: step 31320, loss = 0.79 (952.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:25:50.406010: step 31330, loss = 0.65 (945.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:25:51.734759: step 31340, loss = 0.70 (963.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:25:53.114977: step 31350, loss = 0.74 (927.4 examples/sec; 0.138 sec/batch)
2017-05-07 20:25:54.483386: step 31360, loss = 0.71 (935.4 examples/sec; 0.137 sec/batch)
2017-05-07 20:25:55.838938: step 31370, loss = 0.83 (944.3 examples/sec; 0.136 sec/batch)
2017-05-07 20:25:57.100829: step 31380, loss = 0.72 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-07 20:25:58.451638: step 31390, loss = 0.99 (947.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:25:59.937976: step 31400, loss = 0.76 (861.2 examples/sec; 0.149 sec/batch)
2017-05-07 20:26:01.132013: step 31410, loss = 0.81 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-07 20:26:02.468664: step 31420, loss = 0.84 (957.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:26:03.776364: step 31430, loss = 0.69 (978.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:26:05.113216: step 31440, loss = 0.78 (957.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:26:06.516578: step 31450, loss = 0.95 (912.1 examples/sec; 0.140 sec/batch)
2017-05-07 20:26:07.810275: step 31460, loss = 0.63 (989.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:26:09.130579: step 31470, loss = 0.73 (969.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:26:10.476106: step 31480, loss = 0.70 (951.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:26:11.775730: step 31490, loss = 0.61 (984.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:26:13.225695: step 31500, loss = 0.73 (882.8 examples/sec; 0.145 sec/batch)
2017-05-07 20:26:14.441609: step 31510, loss = 0.71 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-07 20:26:15.780874: step 31520, loss = 0.87 (955.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:26:17.106885: step 31530, loss = 0.68 (965.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:26:18.458735: step 31540, loss = 0.71 (946.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:26:19.845956: step 31550, loss = 0.68 (922.7 examples/sec; 0.139 sec/batch)
2017-05-07 20:26:21.143356: step 31560, loss = 0.72 (986.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:26:22.491619: step 31570, loss = 0.65 (949.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:26:23.817093: step 31580, loss = 0.80 (965.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:26:25.151559: step 31590, loss = 0.56 (959.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:26:26.614126: step 31600, loss = 0.82 (875.2 examples/sec; 0.146 sec/batch)
2017-05-07 20:26:27.805400: step 31610, loss = 0.62 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-07 20:26:29.105640: step 31620, loss = 0.68 (984.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:26:30.429244: step 31630, loss = 0.82 (967.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:26:31.779361: step 31640, loss = 0.76 (948.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:26:33.171196: step 31650, loss = 0.77 (919.7 examples/sec; 0.139 sec/batch)
2017-05-07 20:26:34.476898: step 31660, loss = 0.77 (980.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:26:35.792843: step 31670, loss = 0.75 (972.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:26:37.124936: step 31680, loss = 0.94 (960.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:26:38.475910: step 31690, loss = 0.75 (947.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:26:39.927373: step 31700, loss = 0.78 (881.9 examples/sec; 0.145 sec/batch)
2017-05-07 20:26:41.129015: step 31710, loss = 0.75 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-07 20:26:42.473360: step 31720, loss = 0.82 (952.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:26:43.792994: step 31730, loss = 0.70 (970.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:26:45.113511: step 31740, loss = 0.78 (969.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:26:46.486497: step 31750, loss = 0.82 (932.3 examples/sec; 0.137 sec/batch)
2017-05-07 20:26:47.791404: step 31760, loss = 0.69 (980.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:26:49.137058: step 31770, loss = 0.79 (951.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:26:50.491160: step 31780, loss = 0.66 (945.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:26:51.837153: step 31790, loss = 0.67 (951.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:26:53.304762: step 31800, loss = 0.61 (872.2 examples/sec; 0.147 sec/batch)
2017-05-07 20:26:54.520134: step 31810, loss = 0.82 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-07 20:26:55.855572: step 31820, loss = 0.95 (958.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:26:57.185394: step 31830, loss = 0.81 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:26:58.527325: step 31840, loss = 0.72 (953.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:26:59.925391: step 31850, loss = 0.79 (915.5 examples/sec; 0.140 sec/batch)
2017-05-07 20:27:01.220145: step 31860, loss = 0.72 (988.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:27:02.546085: step 31870, loss = 0.71 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:27:03.906275: step 31880, loss = 0.66 (941.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:27:05.219340: step 31890, loss = 0.75 (974.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:27:06.682584: step 31900, loss = 0.74 (874.8 examples/sec; 0.146 sec/batch)
2017-05-07 20:27:07.883559: step 31910, loss = 0.77 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-07 20:27:09.211853: step 31920, loss = 0.68 (963.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:27:10.563652: step 31930, loss = 0.78 (946.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:27:11.909991: step 31940, loss = 0.77 (950.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:27:13.288794: step 31950, loss = 0.65 (928.3 examples/sec; 0.138 sec/batch)
2017-05-07 20:27:14.576564: step 31960, loss = 0.74 (994.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:27:15.906556: step 31970, loss = 0.70 (962.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:27:17.214248: step 31980, loss = 0.80 (978.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:27:18.535437: step 31990, loss = 0.62 (968.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:27:20.071045: step 32000, loss = 0.65 (833.5 examples/sec; 0.154 sec/batch)
2017-05-07 20:27:21.239666: step 32010, loss = 0.59 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-07 20:27:22.564148: step 32020, loss = 0.95 (966.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:27:23.919212: step 32030, loss = 0.65 (944.6 examples/sec; 0.136 sec/batch)
2017-05-07 20:27:25.219618: step 32040, loss = 0.63 (984.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:27:26.617827: step 32050, loss = 0.95 (915.5 examples/sec; 0.140 sec/batch)
2017-05-07 20:27:27.923140: step 32060, loss = 0.74 (980.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:27:29.240007: step 32070, loss = 0.77 (972.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:27:30.580058: step 32080, loss = 0.64 (955.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:27:31.937048: step 32090, loss = 0.95 (943.3 examples/sec; 0.136 sec/batch)
2017-05-07 20:27:33.398596: step 32100, loss = 0.74 (875.8 examples/sec; 0.146 sec/batch)
2017-05-07 20:27:34.614652: step 32110, loss = 0.82 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-07 20:27:35.957625: step 32120, loss = 0.82 (953.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:27:37.296522: step 32130, loss = 0.67 (956.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:27:38.620344: step 32140, loss = 0.65 (966.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:27:40.006195: step 32150, loss = 0.92 (923.6 examples/sec; 0.139 sec/batch)
2017-05-07 20:27:41.294171: step 32160, loss = 0.84 (993.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:27:42.644869: step 32170, loss = 0.72 (947.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:27:43.979217: step 32180, loss = 0.70 (959.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:27:45.302262: step 32190, loss = 0.70 (967.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:27:46.768386: step 32200, loss = 0.65 (873.1 examples/sec; 0.147 sec/batch)
2017-05-07 20:27:47.957104: step 32210, loss = 0.74 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-07 20:27:49.275702: step 32220, loss = 0.80 (970.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:27:50.596017: step 32230, loss = 0.66 (969.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:27:51.953610: step 32240, loss = 0.79 (942.8 examples/sec; 0.136 sec/batch)
2017-05-07 20:27:53.331736: step 32250, loss = 1.20 (928.8 examples/sec; 0.138 sec/batch)
2017-05-07 20:27:54.621699: step 32260, loss = 0.71 (992.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:27:55.955048: step 32270, loss = 0.76 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:27:57.292220: step 32280, loss = 0.78 (957.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:27:58.653059: step 32290, loss = 0.82 (940.6 examples/sec; 0.136 sec/batch)
2017-05-07 20:28:00.124995: step 32300, loss = 0.66 (869.6 examples/sec; 0.147 sec/batch)
2017-05-07 20:28:01.322621: step 32310, loss = 0.88 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-07 20:28:02.706530: step 32320, loss = 0.62 (924.9 examples/sec; 0.138 sec/batch)
2017-05-07 20:28:04.037017: step 32330, loss = 0.74 (962.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:28:05.343907: step 32340, loss = 0.85 (979.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:28:06.728812: step 32350, loss = 0.75 (924.3 examples/sec; 0.138 sec/batch)
2017-05-07 20:28:08.043120: step 32360, loss = 0.73 (973.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:28:09.391669: step 32370, loss = 0.73 (949.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:28:10.716244: step 32380, loss = 0.78 (966.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:28:12.047745: step 32390, loss = 0.63 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:28:13.509363: step 32400, loss = 0.68 (875.7 examples/sec; 0.146 sec/batch)
2017-05-07 20:28:14.704646: step 32410, loss = 0.81 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-07 20:28:16.036408: step 32420, loss = 0.69 (961.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:28:17.352542: step 32430, loss = 0.80 (972.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:28:18.717309: step 32440, loss = 0.83 (937.9 examples/sec; 0.136 sec/batch)
2017-05-07 20:28:20.119434: step 32450, loss = 0.66 (912.9 examples/sec; 0.140 sec/batch)
2017-05-07 20:28:21.402869: step 32460, loss = 0.68 (997.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:28:22.722813: step 32470, loss = 0.77 (969.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:28:24.057823: step 32480, loss = 0.68 (958.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:28:25.407475: step 32490, loss = 0.93 (948.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:28:26.866350: step 32500, loss = 0.69 (877.4 examples/sec; 0.146 sec/batch)
2017-05-07 20:28:28.071654: step 32510, loss = 0.85 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-07 20:28:29.400874: step 32520, loss = 0.81 (963.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:28:30.736285: step 32530, loss = 0.81 (958.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:28:32.038701: step 32540, loss = 0.73 (982.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:28:33.405710: step 32550, loss = 0.94 (936.3 examples/sec; 0.137 sec/batch)
2017-05-07 20:28:34.721521: step 32560, loss = 1.03 (972.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:28:36.035722: step 32570, loss = 0.79 (974.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:28:37.387931: step 32580, loss = 0.83 (946.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:28:38.697395: step 32590, loss = 0.74 (977.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:28:40.187907: step 32600, loss = 0.76 (858.8 examples/sec; 0.149 sec/batch)
2017-05-07 20:28:41.398275: step 32610, loss = 0.75 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-07 20:28:42.718052: step 32620, loss = 0.68 (969.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:28:44.072292: step 32630, loss = 0.76 (945.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:28:45.412836: step 32640, loss = 0.78 (954.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:28:46.813406: step 32650, loss = 0.78 (913.9 examples/sec; 0.140 sec/batch)
2017-05-07 20:28:48.106975: step 32660, loss = 0.58 (989.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:28:49.425407: step 32670, loss = 0.83 (970.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:28:50.743513: step 32680, loss = 0.77 (971.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:28:52.076834: step 32690, loss = 0.75 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:28:53.528872: step 32700, loss = 0.65 (881.5 examples/sec; 0.145 sec/batch)
2017-05-07 20:28:54.732387: step 32710, loss = 0.68 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-07 20:28:56.068367: step 32720, loss = 0.94 (958.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:28:57.405685: step 32730, loss = 0.83 (957.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:28:58.745209: step 32740, loss = 0.75 (955.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:29:00.089821: step 32750, loss = 0.57 (951.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:29:01.401898: step 32760, loss = 1.01 (975.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:29:02.741673: step 32770, loss = 0.67 (955.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:29:04.080805: step 32780, loss = 0.71 (955.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:29:05.409240: step 32790, loss = 0.95 (963.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:29:06.851922: step 32800, loss = 0.87 (887.2 examples/sec; 0.144 sec/batch)
2017-05-07 20:29:08.074495: step 32810, loss = 0.71 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-07 20:29:09.383827: step 32820, loss = 0.73 (977.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:29:10.706407: step 32830, loss = 0.68 (967.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:29:12.035036: step 32840, loss = 0.76 (963.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:29:13.405705: step 32850, loss = 0.68 (933.8 examples/sec; 0.137 sec/batch)
2017-05-07 20:29:14.691021: step 32860, loss = 0.75 (995.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:29:16.009727: step 32870, loss = 0.60 (970.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:29:17.344790: step 32880, loss = 0.87 (958.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:29:18.671042: step 32890, loss = 0.61 (965.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:29:20.143841: step 32900, loss = 0.59 (869.1 examples/sec; 0.147 sec/batch)
2017-05-07 20:29:21.371718: step 32910, loss = 0.71 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-07 20:29:22.689708: step 32920, loss = 0.78 (971.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:29:24.024935: step 32930, loss = 0.71 (958.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:29:25.383639: step 32940, loss = 0.83 (942.1 examples/sec; 0.136 sec/batch)
2017-05-07 20:29:26.741291: step 32950, loss = 0.68 (942.8 examples/sec; 0.136 sec/batch)
2017-05-07 20:29:28.061337: step 32960, loss = 0.80 (969.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:29:29.401241: step 32970, loss = 0.76 (955.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:29:30.736268: step 32980, loss = 0.76 (958.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:29:32.073786: step 32990, loss = 0.82 (957.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:29:33.521394: step 33000, loss = 0.78 (884.2 examples/sec; 0.145 sec/batch)
2017-05-07 20:29:34.715736: step 33010, loss = 0.83 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-07 20:29:36.047705: step 33020, loss = 0.77 (961.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:29:37.383885: step 33030, loss = 0.82 (958.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:29:38.722250: step 33040, loss = 0.75 (956.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:29:40.101315: step 33050, loss = 0.73 (928.2 examples/sec; 0.138 sec/batch)
2017-05-07 20:29:41.374527: step 33060, loss = 0.86 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:29:42.697886: step 33070, loss = 0.88 (967.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:29:44.041645: step 33080, loss = 0.80 (952.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:29:45.370436: step 33090, loss = 0.73 (963.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:29:46.830054: step 33100, loss = 0.64 (876.9 examples/sec; 0.146 sec/batch)
2017-05-07 20:29:48.047072: step 33110, loss = 0.72 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-07 20:29:49.375018: step 33120, loss = 0.77 (963.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:29:50.696192: step 33130, loss = 0.69 (968.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:29:52.028014: step 33140, loss = 0.60 (961.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:29:53.409280: step 33150, loss = 0.82 (926.7 examples/sec; 0.138 sec/batch)
2017-05-07 20:29:54.711839: step 33160, loss = 0.95 (982.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:29:56.034785: step 33170, loss = 0.72 (967.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:29:57.377665: step 33180, loss = 0.76 (953.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:29:58.708600: step 33190, loss = 0.71 (961.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:30:00.188570: step 33200, loss = 0.82 (864.9 examples/sec; 0.148 sec/batch)
2017-05-07 20:30:01.390277: step 33210, loss = 0.76 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-07 20:30:02.737561: step 33220, loss = 0.79 (950.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:30:04.059843: step 33230, loss = 0.68 (968.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:30:05.385666: step 33240, loss = 0.70 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:30:06.785424: step 33250, loss = 0.69 (914.4 examples/sec; 0.140 sec/batch)
2017-05-07 20:30:08.062592: step 33260, loss = 0.79 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:30:09.394043: step 33270, loss = 0.88 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:30:10.731979: step 33280, loss = 0.78 (956.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:30:12.092796: step 33290, loss = 0.66 (940.6 examples/sec; 0.136 sec/batch)
2017-05-07 20:30:13.537636: step 33300, loss = 0.66 (885.9 examples/sec; 0.144 sec/batch)
2017-05-07 20:30:14.725895: step 33310, loss = 0.82 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-07 20:30:16.054282: step 33320, loss = 0.72 (963.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:30:17.385515: step 33330, loss = 0.66 (961.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:30:18.722594: step 33340, loss = 0.72 (957.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:30:20.113246: step 33350, loss = 0.88 (920.4 examples/sec; 0.139 sec/batch)
2017-05-07 20:30:21.389320: step 33360, loss = 0.73 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-07 20:30:22.743302: step 33370, loss = 0.60 (945.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:30:24.054155: step 33380, loss = 0.78 (976.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:30:25.380795: step 33390, loss = 0.75 (964.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:30:26.900541: step 33400, loss = 0.77 (842.2 examples/sec; 0.152 sec/batch)
2017-05-07 20:30:28.044198: step 33410, loss = 0.79 (1119.2 examples/sec; 0.114 sec/batch)
2017-05-07 20:30:29.352168: step 33420, loss = 0.69 (978.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:30:30.691684: step 33430, loss = 0.74 (955.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:30:32.026323: step 33440, loss = 0.70 (959.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:30:33.411257: step 33450, loss = 0.89 (924.2 examples/sec; 0.138 sec/batch)
2017-05-07 20:30:34.713989: step 33460, loss = 0.78 (982.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:30:36.045050: step 33470, loss = 0.70 (961.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:30:37.389790: step 33480, loss = 0.79 (951.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:30:38.708214: step 33490, loss = 0.83 (970.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:30:40.186635: step 33500, loss = 0.72 (865.8 examples/sec; 0.148 sec/batch)
2017-05-07 20:30:41.406701: step 33510, loss = 0.77 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-07 20:30:42.730268: step 33520, loss = 0.56 (967.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:30:44.055668: step 33530, loss = 0.89 (965.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:30:45.375423: step 33540, loss = 0.77 (969.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:30:46.769803: step 33550, loss = 0.80 (918.0 examples/sec; 0.139 sec/batch)
2017-05-07 20:30:48.073354: step 33560, loss = 0.85 (981.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:30:49.412949: step 33570, loss = 0.87 (955.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:30:50.729334: step 33580, loss = 1.09 (972.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:30:52.050252: step 33590, loss = 0.71 (969.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:30:53.518844: step 33600, loss = 0.60 (871.6 examples/sec; 0.147 sec/batch)
2017-05-07 20:30:54.711530: step 33610, loss = 0.84 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-07 20:30:56.054703: step 33620, loss = 0.70 (953.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:30:57.362191: step 33630, loss = 0.67 (979.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:30:58.704801: step 33640, loss = 0.67 (953.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:31:00.096328: step 33650, loss = 0.79 (919.8 examples/sec; 0.139 sec/batch)
2017-05-07 20:31:01.401532: step 33660, loss = 0.90 (980.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:31:02.735503: step 33670, loss = 0.70 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:31:04.065974: step 33680, loss = 0.67 (962.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:31:05.384577: step 33690, loss = 0.68 (970.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:31:06.831697: step 33700, loss = 0.68 (884.5 examples/sec; 0.145 sec/batch)
2017-05-07 20:31:08.017164: step 33710, loss = 0.77 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-07 20:31:09.367979: step 33720, loss = 0.71 (947.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:31:10.687315: step 33730, loss = 0.73 (970.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:31:11.997411: step 33740, loss = 0.65 (977.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:31:13.393586: step 33750, loss = 0.80 (916.8 examples/sec; 0.140 sec/batch)
2017-05-07 20:31:14.711754: step 33760, loss = 0.72 (971.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:31:16.059430: step 33770, loss = 0.80 (949.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:31:17.371373: step 33780, loss = 0.71 (975.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:31:18.716161: step 33790, loss = 0.68 (951.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:31:20.184611: step 33800, loss = 0.77 (871.7 examples/sec; 0.147 sec/batch)
2017-05-07 20:31:21.365996: step 33810, loss = 0.73 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-07 20:31:22.687947: step 33820, loss = 0.76 (968.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:31:24.022811: step 33830, loss = 0.90 (958.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:31:25.350691: step 33840, loss = 0.66 (963.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:31:26.741518: step 33850, loss = 0.89 (920.3 examples/sec; 0.139 sec/batch)
2017-05-07 20:31:28.046211: step 33860, loss = 0.72 (981.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:31:29.389638: step 33870, loss = 0.70 (952.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:31:30.705403: step 33880, loss = 0.91 (972.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:31:32.065378: step 33890, loss = 0.74 (941.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:31:33.509027: step 33900, loss = 0.73 (886.6 examples/sec; 0.144 sec/batch)
2017-05-07 20:31:34.724074: step 33910, loss = 0.81 (1053.5 examples/sec; 0.122 sec/batch)
2017-05-07 20:31:36.060065: step 33920, loss = 0.75 (958.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:31:37.399350: step 33930, loss = 0.80 (955.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:31:38.688513: step 33940, loss = 0.66 (992.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:31:40.074108: step 33950, loss = 0.68 (923.8 examples/sec; 0.139 sec/batch)
2017-05-07 20:31:41.364623: step 33960, loss = 0.73 (991.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:31:42.707714: step 33970, loss = 0.74 (953.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:31:44.032546: step 33980, loss = 0.78 (966.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:31:45.345289: step 33990, loss = 0.86 (975.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:31:46.843361: step 34000, loss = 0.73 (854.4 examples/sec; 0.150 sec/batch)
2017-05-07 20:31:48.046805: step 34010, loss = 0.71 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-07 20:31:49.381534: step 34020, loss = 0.95 (959.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:31:50.729026: step 34030, loss = 0.97 (949.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:31:52.070285: step 34040, loss = 0.69 (954.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:31:53.455608: step 34050, loss = 0.88 (924.0 examples/sec; 0.139 sec/batch)
2017-05-07 20:31:54.762063: step 34060, loss = 0.69 (979.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:31:56.076432: step 34070, loss = 0.88 (973.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:31:57.395207: step 34080, loss = 0.60 (970.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:31:58.722091: step 34090, loss = 0.83 (964.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:32:00.249671: step 34100, loss = 0.71 (837.9 examples/sec; 0.153 sec/batch)
2017-05-07 20:32:01.402025: step 34110, loss = 0.73 (1110.8 examples/sec; 0.115 sec/batch)
2017-05-07 20:32:02.739851: step 34120, loss = 0.76 (956.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:32:04.042494: step 34130, loss = 0.65 (982.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:32:05.374797: step 34140, loss = 0.76 (960.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:32:06.761979: step 34150, loss = 0.79 (922.7 examples/sec; 0.139 sec/batch)
2017-05-07 20:32:08.058849: step 34160, loss = 0.63 (987.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:32:09.385127: step 34170, loss = 0.82 (965.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:32:10.726203: step 34180, loss = 0.79 (954.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:32:12.054158: step 34190, loss = 0.80 (963.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:32:13.521062: step 34200, loss = 0.80 (872.6 examples/sec; 0.147 sec/batch)
2017-05-07 20:32:14.730251: step 34210, loss = 0.76 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-07 20:32:16.058728: step 34220, loss = 0.68 (963.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:32:17.395638: step 34230, loss = 0.83 (957.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:32:18.714777: step 34240, loss = 0.91 (970.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:32:20.081011: step 34250, loss = 0.77 (936.9 examples/sec; 0.137 sec/batch)
2017-05-07 20:32:21.378037: step 34260, loss = 0.62 (986.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:32:22.721062: step 34270, loss = 0.69 (953.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:32:24.041636: step 34280, loss = 0.80 (969.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:32:25.364994: step 34290, loss = 0.74 (967.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:32:26.839237: step 34300, loss = 0.66 (868.2 examples/sec; 0.147 sec/batch)
2017-05-07 20:32:28.042674: step 34310, loss = 0.79 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-07 20:32:29.385869: step 34320, loss = 0.80 (952.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:32:30.723741: step 34330, loss = 0.70 (956.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:32:32.042627: step 34340, loss = 0.76 (970.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:32:33.418735: step 34350, loss = 0.82 (930.2 examples/sec; 0.138 sec/batch)
2017-05-07 20:32:34.703882: step 34360, loss = 0.81 (996.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:32:36.037173: step 34370, loss = 0.74 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:32:37.373726: step 34380, loss = 0.76 (957.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:32:38.709504: step 34390, loss = 0.82 (958.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:32:40.157637: step 34400, loss = 0.70 (883.9 examples/sec; 0.145 sec/batch)
2017-05-07 20:32:41.356540: step 34410, loss = 0.76 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-07 20:32:42.717924: step 34420, loss = 0.74 (940.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:32:44.021803: step 34430, loss = 0.81 (981.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:32:45.354971: step 34440, loss = 0.69 (960.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:32:46.750736: step 34450, loss = 0.79 (917.1 examples/sec; 0.140 sec/batch)
2017-05-07 20:32:48.060077: step 34460, loss = 0.76 (977.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:32:49.377069: step 34470, loss = 0.72 (971.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:32:50.712367: step 34480, loss = 0.70 (958.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:32:52.034290: step 34490, loss = 0.74 (968.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:32:53.504340: step 34500, loss = 0.83 (870.7 examples/sec; 0.147 sec/batch)
2017-05-07 20:32:54.721261: step 34510, loss = 0.92 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-07 20:32:56.052543: step 34520, loss = 0.83 (961.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:32:57.402006: step 34530, loss = 0.76 (948.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:32:58.712296: step 34540, loss = 0.63 (976.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:33:00.084923: step 34550, loss = 0.95 (932.5 examples/sec; 0.137 sec/batch)
2017-05-07 20:33:01.388598: step 34560, loss = 0.78 (981.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:33:02.732755: step 34570, loss = 0.73 (952.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:33:04.049126: step 34580, loss = 0.78 (972.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:33:05.379638: step 34590, loss = 0.68 (962.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:33:06.847799: step 34600, loss = 0.72 (871.9 examples/sec; 0.147 sec/batch)
2017-05-07 20:33:08.044882: step 34610, loss = 0.70 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-07 20:33:09.380510: step 34620, loss = 0.66 (958.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:33:10.726265: step 34630, loss = 0.79 (951.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:33:12.075966: step 34640, loss = 0.82 (948.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:33:13.459616: step 34650, loss = 0.71 (925.1 examples/sec; 0.138 sec/batch)
2017-05-07 20:33:14.763132: step 34660, loss = 0.75 (981.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:33:16.106776: step 34670, loss = 0.89 (952.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:33:17.465742: step 34680, loss = 0.71 (941.9 examples/sec; 0.136 sec/batch)
2017-05-07 20:33:18.792385: step 34690, loss = 0.81 (964.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:33:20.238989: step 34700, loss = 0.80 (884.8 examples/sec; 0.145 sec/batch)
2017-05-07 20:33:21.435770: step 34710, loss = 0.65 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-07 20:33:22.782252: step 34720, loss = 0.72 (950.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:33:24.111335: step 34730, loss = 0.70 (963.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:33:25.418923: step 34740, loss = 0.70 (978.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:33:26.825390: step 34750, loss = 0.79 (910.1 examples/sec; 0.141 sec/batch)
2017-05-07 20:33:28.116878: step 34760, loss = 0.78 (991.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:33:29.453594: step 34770, loss = 0.95 (957.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:33:30.795280: step 34780, loss = 0.68 (954.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:33:32.115250: step 34790, loss = 0.89 (969.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:33:33.639719: step 34800, loss = 0.75 (839.6 examples/sec; 0.152 sec/batch)
2017-05-07 20:33:34.794375: step 34810, loss = 0.79 (1108.5 examples/sec; 0.115 sec/batch)
2017-05-07 20:33:36.109878: step 34820, loss = 0.72 (973.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:33:37.433917: step 34830, loss = 0.75 (966.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:33:38.777102: step 34840, loss = 0.72 (953.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:33:40.163754: step 34850, loss = 0.74 (923.1 examples/sec; 0.139 sec/batch)
2017-05-07 20:33:41.436212: step 34860, loss = 1.00 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:33:42.765830: step 34870, loss = 0.73 (962.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:33:44.112666: step 34880, loss = 0.55 (950.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:33:45.431667: step 34890, loss = 0.91 (970.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:33:46.917130: step 34900, loss = 0.70 (861.7 examples/sec; 0.149 sec/batch)
2017-05-07 20:33:48.165803: step 34910, loss = 0.83 (1025.1 examples/sec; 0.125 sec/batch)
2017-05-07 20:33:49.470022: step 34920, loss = 0.79 (981.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:33:50.829894: step 34930, loss = 0.92 (941.3 examples/sec; 0.136 sec/batch)
2017-05-07 20:33:52.179749: step 34940, loss = 0.90 (948.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:33:53.565426: step 34950, loss = 0.79 (923.7 examples/sec; 0.139 sec/batch)
2017-05-07 20:33:54.855639: step 34960, loss = 0.66 (992.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:33:56.193764: step 34970, loss = 0.73 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:33:57.523049: step 34980, loss = 0.68 (962.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:33:58.859931: step 34990, loss = 0.76 (957.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:34:00.342225: step 35000, loss = 0.88 (863.5 examples/sec; 0.148 sec/batch)
2017-05-07 20:34:01.541999: step 35010, loss = 0.79 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-07 20:34:02.887360: step 35020, loss = 0.98 (951.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:34:04.207057: step 35030, loss = 0.79 (969.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:34:05.555291: step 35040, loss = 0.64 (949.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:34:06.933274: step 35050, loss = 0.71 (928.9 examples/sec; 0.138 sec/batch)
2017-05-07 20:34:08.227873: step 35060, loss = 0.65 (988.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:34:09.557926: step 35070, loss = 0.72 (962.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:34:10.899867: step 35080, loss = 0.73 (953.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:34:12.258696: step 35090, loss = 0.66 (942.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:34:13.725597: step 35100, loss = 0.86 (872.6 examples/sec; 0.147 sec/batch)
2017-05-07 20:34:14.909389: step 35110, loss = 0.64 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-07 20:34:16.240568: step 35120, loss = 0.74 (961.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:34:17.578421: step 35130, loss = 0.79 (956.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:34:18.914558: step 35140, loss = 0.81 (958.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:34:20.295007: step 35150, loss = 0.67 (927.2 examples/sec; 0.138 sec/batch)
2017-05-07 20:34:21.591488: step 35160, loss = 0.82 (987.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:34:22.918795: step 35170, loss = 0.71 (964.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:34:24.238902: step 35180, loss = 0.71 (969.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:34:25.546427: step 35190, loss = 0.67 (978.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:34:27.042198: step 35200, loss = 0.84 (855.7 examples/sec; 0.150 sec/batch)
2017-05-07 20:34:28.238135: step 35210, loss = 0.77 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-07 20:34:29.574061: step 35220, loss = 0.60 (958.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:34:30.921775: step 35230, loss = 0.73 (949.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:34:32.245566: step 35240, loss = 0.77 (966.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:34:33.623528: step 35250, loss = 0.90 (928.9 examples/sec; 0.138 sec/batch)
2017-05-07 20:34:34.927748: step 35260, loss = 0.64 (981.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:34:36.247264: step 35270, loss = 0.73 (970.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:34:37.587637: step 35280, loss = 0.92 (955.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:34:38.908007: step 35290, loss = 0.77 (969.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:34:40.360750: step 35300, loss = 0.83 (881.1 examples/sec; 0.145 sec/batch)
2017-05-07 20:34:41.593143: step 35310, loss = 0.87 (1038.6 examples/sec; 0.123 sec/batch)
2017-05-07 20:34:42.912686: step 35320, loss = 0.69 (970.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:34:44.240588: step 35330, loss = 0.81 (963.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:34:45.553246: step 35340, loss = 0.67 (975.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:34:46.945433: step 35350, loss = 0.78 (919.4 examples/sec; 0.139 sec/batch)
2017-05-07 20:34:48.230448: step 35360, loss = 0.74 (996.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:34:49.547182: step 35370, loss = 0.71 (972.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:34:50.889147: step 35380, loss = 0.96 (953.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:34:52.210986: step 35390, loss = 0.75 (968.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:34:53.667973: step 35400, loss = 0.89 (878.5 examples/sec; 0.146 sec/batch)
2017-05-07 20:34:54.886136: step 35410, loss = 0.77 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-07 20:34:56.236107: step 35420, loss = 0.75 (948.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:34:57.561662: step 35430, loss = 0.67 (965.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:34:58.894488: step 35440, loss = 0.64 (960.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:35:00.268483: step 35450, loss = 0.85 (931.6 examples/sec; 0.137 sec/batch)
2017-05-07 20:35:01.535672: step 35460, loss = 0.82 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-07 20:35:02.880629: step 35470, loss = 0.74 (951.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:35:04.217916: step 35480, loss = 0.68 (957.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:35:05.527859: step 35490, loss = 0.74 (977.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:35:07.008715: step 35500, loss = 0.60 (864.4 examples/sec; 0.148 sec/batch)
2017-05-07 20:35:08.213542: step 35510, loss = 0.85 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-07 20:35:09.514613: step 35520, loss = 0.72 (983.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:35:10.867958: step 35530, loss = 0.75 (945.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:35:12.193866: step 35540, loss = 0.83 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:35:13.581379: step 35550, loss = 0.81 (922.5 examples/sec; 0.139 sec/batch)
2017-05-07 20:35:14.883972: step 35560, loss = 0.77 (982.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:35:16.198095: step 35570, loss = 0.74 (974.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:35:17.495718: step 35580, loss = 0.68 (986.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:35:18.822370: step 35590, loss = 0.74 (964.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:35:20.306778: step 35600, loss = 0.77 (862.3 examples/sec; 0.148 sec/batch)
2017-05-07 20:35:21.519345: step 35610, loss = 0.69 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-07 20:35:22.873818: step 35620, loss = 0.76 (945.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:35:24.190382: step 35630, loss = 0.77 (972.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:35:25.520520: step 35640, loss = 0.74 (962.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:35:26.934622: step 35650, loss = 0.70 (905.2 examples/sec; 0.141 sec/batch)
2017-05-07 20:35:28.221004: step 35660, loss = 0.72 (995.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:35:29.549262: step 35670, loss = 0.92 (963.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:35:30.896527: step 35680, loss = 0.84 (950.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:35:32.236607: step 35690, loss = 0.65 (955.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:35:33.725735: step 35700, loss = 0.88 (859.6 examples/sec; 0.149 sec/batch)
2017-05-07 20:35:34.888851: step 35710, loss = 0.81 (1100.5 examples/sec; 0.116 sec/batch)
2017-05-07 20:35:36.214613: step 35720, loss = 0.66 (965.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:35:37.548842: step 35730, loss = 0.63 (959.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:35:38.888560: step 35740, loss = 0.86 (955.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:35:40.278848: step 35750, loss = 0.62 (920.7 examples/sec; 0.139 sec/batch)
2017-05-07 20:35:41.566239: step 35760, loss = 0.76 (994.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:35:42.918922: step 35770, loss = 0.72 (946.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:35:44.235408: step 35780, loss = 0.79 (972.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:35:45.552377: step 35790, loss = 0.73 (971.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:35:47.021314: step 35800, loss = 0.59 (871.4 examples/sec; 0.147 sec/batch)
2017-05-07 20:35:48.234841: step 35810, loss = 0.74 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-07 20:35:49.565203: step 35820, loss = 0.90 (962.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:35:50.910703: step 35830, loss = 0.82 (951.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:35:52.234429: step 35840, loss = 0.71 (967.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:35:53.612711: step 35850, loss = 0.95 (928.7 examples/sec; 0.138 sec/batch)
2017-05-07 20:35:54.941549: step 35860, loss = 0.86 (963.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:35:56.258001: step 35870, loss = 0.61 (972.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:35:57.611419: step 35880, loss = 0.75 (945.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:35:58.946369: step 35890, loss = 0.77 (958.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:36:00.432879: step 35900, loss = 0.76 (861.1 examples/sec; 0.149 sec/batch)
2017-05-07 20:36:01.636987: step 35910, loss = 0.76 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-07 20:36:02.966752: step 35920, loss = 0.75 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:36:04.329045: step 35930, loss = 0.71 (939.6 examples/sec; 0.136 sec/batch)
2017-05-07 20:36:05.668049: step 35940, loss = 0.65 (955.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:36:07.059927: step 35950, loss = 0.66 (919.6 examples/sec; 0.139 sec/batch)
2017-05-07 20:36:08.346382: step 35960, loss = 0.75 (995.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:36:09.680412: step 35970, loss = 0.77 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:36:11.012057: step 35980, loss = 0.68 (961.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:36:12.354762: step 35990, loss = 0.74 (953.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:36:13.857193: step 36000, loss = 0.69 (852.0 examples/sec; 0.150 sec/batch)
2017-05-07 20:36:15.035322: step 36010, loss = 0.93 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-07 20:36:16.368620: step 36020, loss = 0.65 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:36:17.707678: step 36030, loss = 0.90 (955.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:36:19.077075: step 36040, loss = 0.88 (934.7 examples/sec; 0.137 sec/batch)
2017-05-07 20:36:20.471395: step 36050, loss = 0.63 (918.0 examples/sec; 0.139 sec/batch)
2017-05-07 20:36:21.753367: step 36060, loss = 0.77 (998.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:36:23.090995: step 36070, loss = 0.68 (956.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:36:24.419886: step 36080, loss = 0.70 (963.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:36:25.730618: step 36090, loss = 0.81 (976.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:36:27.226076: step 36100, loss = 0.69 (855.9 examples/sec; 0.150 sec/batch)
2017-05-07 20:36:28.417847: step 36110, loss = 0.79 (1074.0 examples/sec; 0.119 sec/batch)
2017-05-07 20:36:29.752242: step 36120, loss = 0.75 (959.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:36:31.072518: step 36130, loss = 0.79 (969.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:36:32.380505: step 36140, loss = 0.65 (978.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:36:33.777165: step 36150, loss = 0.70 (916.5 examples/sec; 0.140 sec/batch)
2017-05-07 20:36:35.089887: step 36160, loss = 0.56 (975.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:36:36.436040: step 36170, loss = 0.79 (950.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:36:37.754923: step 36180, loss = 0.68 (970.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:36:39.099127: step 36190, loss = 0.75 (952.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:36:40.568380: step 36200, loss = 0.73 (871.2 examples/sec; 0.147 sec/batch)
2017-05-07 20:36:41.768725: step 36210, loss = 0.66 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-07 20:36:43.108769: step 36220, loss = 0.80 (955.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:36:44.431173: step 36230, loss = 0.65 (967.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:36:45.772434: step 36240, loss = 0.90 (954.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:36:47.153742: step 36250, loss = 0.80 (926.7 examples/sec; 0.138 sec/batch)
2017-05-07 20:36:48.452790: step 36260, loss = 0.64 (985.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:36:49.799177: step 36270, loss = 0.79 (950.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:36:51.127301: step 36280, loss = 1.09 (963.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:36:52.463863: step 36290, loss = 0.66 (957.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:36:53.933135: step 36300, loss = 0.82 (871.2 examples/sec; 0.147 sec/batch)
2017-05-07 20:36:55.124740: step 36310, loss = 0.65 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-07 20:36:56.449641: step 36320, loss = 0.77 (966.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:36:57.796028: step 36330, loss = 0.71 (950.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:36:59.135279: step 36340, loss = 0.75 (955.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:37:00.523091: step 36350, loss = 0.75 (922.3 examples/sec; 0.139 sec/batch)
2017-05-07 20:37:01.814184: step 36360, loss = 1.03 (991.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:37:03.115541: step 36370, loss = 0.68 (983.6 examples/sec; 0.130 sec/batch)
2017-05-07 20:37:04.439257: step 36380, loss = 0.83 (967.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:37:05.763632: step 36390, loss = 0.70 (966.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:37:07.231511: step 36400, loss = 0.91 (872.0 examples/sec; 0.147 sec/batch)
2017-05-07 20:37:08.439853: step 36410, loss = 0.65 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-07 20:37:09.764834: step 36420, loss = 0.79 (966.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:37:11.098788: step 36430, loss = 0.94 (959.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:37:12.430295: step 36440, loss = 0.75 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:37:13.803650: step 36450, loss = 0.78 (932.0 examples/sec; 0.137 sec/batch)
2017-05-07 20:37:15.110187: step 36460, loss = 0.84 (979.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:37:16.445833: step 36470, loss = 0.78 (958.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:37:17.777314: step 36480, loss = 0.88 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:37:19.116038: step 36490, loss = 0.68 (956.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:37:20.581377: step 36500, loss = 0.68 (873.5 examples/sec; 0.147 sec/batch)
2017-05-07 20:37:21.797787: step 36510, loss = 0.81 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-07 20:37:23.112549: step 36520, loss = 0.74 (973.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:37:24.446754: step 36530, loss = 0.81 (959.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:37:25.757057: step 36540, loss = 0.65 (976.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:37:27.129881: step 36550, loss = 0.69 (932.4 examples/sec; 0.137 sec/batch)
2017-05-07 20:37:28.426347: step 36560, loss = 0.91 (987.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:37:29.745711: step 36570, loss = 0.74 (970.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:37:31.076999: step 36580, loss = 0.87 (961.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:37:32.409037: step 36590, loss = 0.73 (960.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:37:33.893868: step 36600, loss = 0.84 (862.0 examples/sec; 0.148 sec/batch)
2017-05-07 20:37:35.126042: step 36610, loss = 0.55 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-07 20:37:36.447761: step 36620, loss = 0.75 (968.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:37:37.760012: step 36630, loss = 0.72 (975.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:37:39.105982: step 36640, loss = 0.67 (951.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:37:40.485197: step 36650, loss = 0.80 (928.1 examples/sec; 0.138 sec/batch)
2017-05-07 20:37:41.754179: step 36660, loss = 0.87 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-07 20:37:43.074137: step 36670, loss = 0.71 (969.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:37:44.400237: step 36680, loss = 0.75 (965.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:37:45.729850: step 36690, loss = 0.72 (962.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:37:47.219493: step 36700, loss = 0.75 (859.3 examples/sec; 0.149 sec/batch)
2017-05-07 20:37:48.417380: step 36710, loss = 0.75 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-07 20:37:49.764174: step 36720, loss = 0.77 (950.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:37:51.083851: step 36730, loss = 0.74 (969.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:37:52.399669: step 36740, loss = 0.97 (972.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:37:53.766086: step 36750, loss = 0.81 (936.8 examples/sec; 0.137 sec/batch)
2017-05-07 20:37:55.079556: step 36760, loss = 0.82 (974.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:37:56.385244: step 36770, loss = 0.77 (980.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:37:57.735621: step 36780, loss = 0.62 (947.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:37:59.091037: step 36790, loss = 0.70 (944.4 examples/sec; 0.136 sec/batch)
2017-05-07 20:38:00.527330: step 36800, loss = 0.95 (891.2 examples/sec; 0.144 sec/batch)
2017-05-07 20:38:01.715990: step 36810, loss = 0.76 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-07 20:38:03.043695: step 36820, loss = 0.64 (964.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:38:04.388201: step 36830, loss = 0.67 (952.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:38:05.696739: step 36840, loss = 0.72 (978.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:38:07.094588: step 36850, loss = 0.76 (915.7 examples/sec; 0.140 sec/batch)
2017-05-07 20:38:08.378697: step 36860, loss = 0.76 (996.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:38:09.709438: step 36870, loss = 0.73 (961.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:38:11.043814: step 36880, loss = 0.78 (959.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:38:12.373647: step 36890, loss = 0.73 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:38:13.822542: step 36900, loss = 0.77 (883.4 examples/sec; 0.145 sec/batch)
2017-05-07 20:38:15.038181: step 36910, loss = 0.66 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-07 20:38:16.346194: step 36920, loss = 0.66 (978.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:38:17.689690: step 36930, loss = 0.68 (952.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:38:19.018304: step 36940, loss = 0.86 (963.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:38:20.412877: step 36950, loss = 0.87 (917.8 examples/sec; 0.139 sec/batch)
2017-05-07 20:38:21.704913: step 36960, loss = 0.75 (990.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:38:23.041883: step 36970, loss = 0.65 (957.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:38:24.351226: step 36980, loss = 0.82 (977.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:38:25.685091: step 36990, loss = 0.81 (959.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:38:27.163732: step 37000, loss = 0.67 (865.7 examples/sec; 0.148 sec/batch)
2017-05-07 20:38:28.354941: step 37010, loss = 0.87 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-07 20:38:29.666155: step 37020, loss = 0.71 (976.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:38:30.998358: step 37030, loss = 0.75 (960.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:38:32.359911: step 37040, loss = 0.73 (940.1 examples/sec; 0.136 sec/batch)
2017-05-07 20:38:33.739725: step 37050, loss = 0.58 (927.7 examples/sec; 0.138 sec/batch)
2017-05-07 20:38:35.021327: step 37060, loss = 1.01 (998.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:38:36.350734: step 37070, loss = 0.76 (962.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:38:37.685081: step 37080, loss = 0.66 (959.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:38:39.034993: step 37090, loss = 0.70 (948.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:38:40.517326: step 37100, loss = 0.69 (863.5 examples/sec; 0.148 sec/batch)
2017-05-07 20:38:41.719436: step 37110, loss = 0.68 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-07 20:38:43.079901: step 37120, loss = 0.80 (940.9 examples/sec; 0.136 sec/batch)
2017-05-07 20:38:44.396203: step 37130, loss = 0.73 (972.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:38:45.753010: step 37140, loss = 0.70 (943.4 examples/sec; 0.136 sec/batch)
2017-05-07 20:38:47.105350: step 37150, loss = 0.64 (946.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:38:48.404118: step 37160, loss = 0.76 (985.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:38:49.711737: step 37170, loss = 0.71 (978.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:38:51.052341: step 37180, loss = 0.70 (954.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:38:52.402735: step 37190, loss = 0.65 (947.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:38:53.877118: step 37200, loss = 0.65 (868.2 examples/sec; 0.147 sec/batch)
2017-05-07 20:38:55.054742: step 37210, loss = 0.67 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-07 20:38:56.381253: step 37220, loss = 0.72 (964.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:38:57.720224: step 37230, loss = 0.91 (956.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:38:59.055180: step 37240, loss = 0.78 (958.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:39:00.446691: step 37250, loss = 0.62 (919.9 examples/sec; 0.139 sec/batch)
2017-05-07 20:39:01.721356: step 37260, loss = 0.85 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:39:03.072029: step 37270, loss = 0.74 (947.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:39:04.397687: step 37280, loss = 0.78 (965.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:39:05.746139: step 37290, loss = 0.84 (949.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:39:07.189245: step 37300, loss = 0.73 (887.0 examples/sec; 0.144 sec/batch)
2017-05-07 20:39:08.384241: step 37310, loss = 0.79 (1071.1 examples/sec; 0.119 sec/batch)
2017-05-07 20:39:09.704489: step 37320, loss = 0.82 (969.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:39:11.012347: step 37330, loss = 0.85 (978.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:39:12.379776: step 37340, loss = 0.84 (936.1 examples/sec; 0.137 sec/batch)
2017-05-07 20:39:13.738830: step 37350, loss = 0.71 (941.8 examples/sec; 0.136 sec/batch)
2017-05-07 20:39:15.045967: step 37360, loss = 0.68 (979.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:39:16.370383: step 37370, loss = 0.86 (966.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:39:17.696242: step 37380, loss = 0.76 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:39:19.038585: step 37390, loss = 0.74 (953.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:39:20.533980: step 37400, loss = 0.78 (856.0 examples/sec; 0.150 sec/batch)
2017-05-07 20:39:21.736061: step 37410, loss = 0.83 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-07 20:39:23.078282: step 37420, loss = 0.70 (953.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:39:24.402189: step 37430, loss = 0.71 (966.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:39:25.750102: step 37440, loss = 0.79 (949.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:39:27.153103: step 37450, loss = 0.67 (912.3 examples/sec; 0.140 sec/batch)
2017-05-07 20:39:28.469113: step 37460, loss = 0.60 (972.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:39:29.803382: step 37470, loss = 0.60 (959.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:39:31.127210: step 37480, loss = 0.68 (966.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:39:32.481012: step 37490, loss = 0.85 (945.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:39:33.951106: step 37500, loss = 0.89 (870.7 examples/sec; 0.147 sec/batch)
2017-05-07 20:39:35.139942: step 37510, loss = 0.74 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-07 20:39:36.468949: step 37520, loss = 0.91 (963.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:39:37.804076: step 37530, loss = 0.77 (958.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:39:39.136923: step 37540, loss = 0.74 (960.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:39:40.526661: step 37550, loss = 0.72 (921.0 examples/sec; 0.139 sec/batch)
2017-05-07 20:39:41.801628: step 37560, loss = 0.67 (1003.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:39:43.124536: step 37570, loss = 0.81 (967.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:39:44.439553: step 37580, loss = 0.83 (973.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:39:45.766265: step 37590, loss = 0.71 (964.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:39:47.249593: step 37600, loss = 0.84 (862.9 examples/sec; 0.148 sec/batch)
2017-05-07 20:39:48.461777: step 37610, loss = 0.70 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-07 20:39:49.794329: step 37620, loss = 0.66 (960.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:39:51.141880: step 37630, loss = 0.78 (949.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:39:52.472674: step 37640, loss = 0.94 (961.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:39:53.854396: step 37650, loss = 0.71 (926.4 examples/sec; 0.138 sec/batch)
2017-05-07 20:39:55.142544: step 37660, loss = 0.72 (993.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:39:56.477057: step 37670, loss = 0.95 (959.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:39:57.830024: step 37680, loss = 0.80 (946.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:39:59.172332: step 37690, loss = 0.67 (953.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:40:00.622194: step 37700, loss = 0.70 (882.8 examples/sec; 0.145 sec/batch)
2017-05-07 20:40:01.795393: step 37710, loss = 0.61 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-07 20:40:03.148302: step 37720, loss = 1.04 (946.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:40:04.492725: step 37730, loss = 0.96 (952.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:40:05.817253: step 37740, loss = 0.81 (966.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:40:07.197916: step 37750, loss = 0.73 (927.1 examples/sec; 0.138 sec/batch)
2017-05-07 20:40:08.487660: step 37760, loss = 0.97 (992.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:40:09.816915: step 37770, loss = 0.80 (962.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:40:11.129930: step 37780, loss = 0.84 (974.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:40:12.466485: step 37790, loss = 0.82 (957.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:40:13.924515: step 37800, loss = 0.57 (877.9 examples/sec; 0.146 sec/batch)
2017-05-07 20:40:15.139965: step 37810, loss = 0.65 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-07 20:40:16.478138: step 37820, loss = 0.86 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:40:17.801985: step 37830, loss = 0.69 (966.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:40:19.134645: step 37840, loss = 0.72 (960.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:40:20.524586: step 37850, loss = 0.68 (920.9 examples/sec; 0.139 sec/batch)
2017-05-07 20:40:21.796576: step 37860, loss = 0.72 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-07 20:40:23.117164: step 37870, loss = 0.62 (969.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:40:24.456124: step 37880, loss = 0.70 (956.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:40:25.797213: step 37890, loss = 0.63 (954.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:40:27.245528: step 37900, loss = 0.74 (883.8 examples/sec; 0.145 sec/batch)
2017-05-07 20:40:28.453304: step 37910, loss = 0.66 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-07 20:40:29.784579: step 37920, loss = 0.80 (961.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:40:31.120567: step 37930, loss = 0.76 (958.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:40:32.469106: step 37940, loss = 0.82 (949.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:40:33.862889: step 37950, loss = 0.83 (918.4 examples/sec; 0.139 sec/batch)
2017-05-07 20:40:35.141370: step 37960, loss = 0.74 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:40:36.483924: step 37970, loss = 0.86 (953.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:40:37.832502: step 37980, loss = 0.71 (949.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:40:39.171986: step 37990, loss = 0.58 (955.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:40:40.646276: step 38000, loss = 0.76 (868.2 examples/sec; 0.147 sec/batch)
2017-05-07 20:40:41.856064: step 38010, loss = 0.87 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-07 20:40:43.197989: step 38020, loss = 0.73 (953.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:40:44.538706: step 38030, loss = 0.71 (954.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:40:45.869316: step 38040, loss = 0.72 (962.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:40:47.265979: step 38050, loss = 0.67 (916.5 examples/sec; 0.140 sec/batch)
2017-05-07 20:40:48.559850: step 38060, loss = 0.81 (989.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:40:49.895817: step 38070, loss = 0.88 (958.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:40:51.219400: step 38080, loss = 0.80 (967.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:40:52.541985: step 38090, loss = 0.68 (967.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:40:54.024178: step 38100, loss = 0.89 (863.6 examples/sec; 0.148 sec/batch)
2017-05-07 20:40:55.242399: step 38110, loss = 0.71 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-07 20:40:56.582959: step 38120, loss = 0.74 (954.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:40:57.908100: step 38130, loss = 0.70 (965.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:40:59.259352: step 38140, loss = 0.83 (947.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:41:00.625659: step 38150, loss = 0.82 (936.8 examples/sec; 0.137 sec/batch)
2017-05-07 20:41:01.903739: step 38160, loss = 0.76 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:41:03.239765: step 38170, loss = 0.83 (958.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:41:04.567373: step 38180, loss = 0.60 (964.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:41:05.902892: step 38190, loss = 0.68 (958.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:41:07.371811: step 38200, loss = 0.63 (871.4 examples/sec; 0.147 sec/batch)
2017-05-07 20:41:08.560123: step 38210, loss = 0.70 (1077.2 examples/sec; 0.119 sec/batch)
2017-05-07 20:41:09.901055: step 38220, loss = 0.66 (954.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:41:11.230565: step 38230, loss = 0.77 (962.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:41:12.542574: step 38240, loss = 0.77 (975.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:41:13.905739: step 38250, loss = 0.66 (939.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:41:15.207340: step 38260, loss = 0.65 (983.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:41:16.521778: step 38270, loss = 0.76 (973.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:41:17.845246: step 38280, loss = 0.95 (967.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:41:19.180914: step 38290, loss = 0.75 (958.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:41:20.641981: step 38300, loss = 0.73 (876.1 examples/sec; 0.146 sec/batch)
2017-05-07 20:41:21.842344: step 38310, loss = 0.80 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-07 20:41:23.173246: step 38320, loss = 0.80 (961.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:41:24.504077: step 38330, loss = 0.75 (961.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:41:25.830964: step 38340, loss = 0.75 (964.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:41:27.226856: step 38350, loss = 0.71 (917.0 examples/sec; 0.140 sec/batch)
2017-05-07 20:41:28.523193: step 38360, loss = 0.77 (987.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:41:29.858200: step 38370, loss = 0.78 (958.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:41:31.173693: step 38380, loss = 0.78 (973.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:41:32.517820: step 38390, loss = 0.75 (952.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:41:33.990119: step 38400, loss = 0.87 (869.4 examples/sec; 0.147 sec/batch)
2017-05-07 20:41:35.169427: step 38410, loss = 0.70 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-07 20:41:36.507257: step 38420, loss = 0.84 (956.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:41:37.838264: step 38430, loss = 0.76 (961.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:41:39.180407: step 38440, loss = 0.84 (953.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:41:40.560118: step 38450, loss = 0.68 (927.7 examples/sec; 0.138 sec/batch)
2017-05-07 20:41:41.828125: step 38460, loss = 0.70 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-07 20:41:43.157812: step 38470, loss = 0.77 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:41:44.497307: step 38480, loss = 0.73 (955.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:41:45.827925: step 38490, loss = 0.64 (962.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:41:47.302427: step 38500, loss = 0.69 (868.1 examples/sec; 0.147 sec/batch)
2017-05-07 20:41:48.495383: step 38510, loss = 0.98 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-07 20:41:49.815926: step 38520, loss = 0.68 (969.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:41:51.156197: step 38530, loss = 0.77 (955.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:41:52.487206: step 38540, loss = 0.85 (961.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:41:53.879941: step 38550, loss = 0.74 (919.1 examples/sec; 0.139 sec/batch)
2017-05-07 20:41:55.166670: step 38560, loss = 0.84 (994.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:41:56.505692: step 38570, loss = 0.69 (955.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:41:57.821665: step 38580, loss = 0.66 (972.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:41:59.179351: step 38590, loss = 0.82 (942.8 examples/sec; 0.136 sec/batch)
2017-05-07 20:42:00.660402: step 38600, loss = 0.68 (864.2 examples/sec; 0.148 sec/batch)
2017-05-07 20:42:01.882727: step 38610, loss = 0.70 (1047.2 examples/sec; 0.122 sec/batch)
2017-05-07 20:42:03.209869: step 38620, loss = 0.73 (964.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:42:04.534738: step 38630, loss = 0.68 (966.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:42:05.863027: step 38640, loss = 0.80 (963.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:42:07.238588: step 38650, loss = 0.78 (930.5 examples/sec; 0.138 sec/batch)
2017-05-07 20:42:08.505393: step 38660, loss = 0.82 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:42:09.804508: step 38670, loss = 0.73 (985.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:42:11.161849: step 38680, loss = 0.70 (943.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:42:12.481575: step 38690, loss = 0.81 (969.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:42:13.947967: step 38700, loss = 0.70 (872.9 examples/sec; 0.147 sec/batch)
2017-05-07 20:42:15.175214: step 38710, loss = 0.72 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-07 20:42:16.496554: step 38720, loss = 0.68 (968.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:42:17.851044: step 38730, loss = 0.88 (945.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:42:19.185603: step 38740, loss = 0.72 (959.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:42:20.580886: step 38750, loss = 0.86 (917.4 examples/sec; 0.140 sec/batch)
2017-05-07 20:42:21.858416: step 38760, loss = 1.01 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-07 20:42:23.204694: step 38770, loss = 0.74 (950.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:42:24.524206: step 38780, loss = 0.81 (970.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:42:25.872822: step 38790, loss = 0.70 (949.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:42:27.359906: step 38800, loss = 0.84 (860.7 examples/sec; 0.149 sec/batch)
2017-05-07 20:42:28.582997: step 38810, loss = 0.64 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-07 20:42:29.903291: step 38820, loss = 0.64 (969.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:42:31.259336: step 38830, loss = 0.73 (943.9 examples/sec; 0.136 sec/batch)
2017-05-07 20:42:32.614945: step 38840, loss = 0.78 (944.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:42:33.974055: step 38850, loss = 0.76 (941.8 examples/sec; 0.136 sec/batch)
2017-05-07 20:42:35.256023: step 38860, loss = 0.71 (998.5 examples/sec; 0.128 sec/batch)
2017-05-07 20:42:36.568215: step 38870, loss = 0.85 (975.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:42:37.910701: step 38880, loss = 0.72 (953.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:42:39.242902: step 38890, loss = 0.74 (960.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:42:40.705758: step 38900, loss = 0.73 (875.0 examples/sec; 0.146 sec/batch)
2017-05-07 20:42:41.897227: step 38910, loss = 0.69 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-07 20:42:43.223129: step 38920, loss = 0.59 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:42:44.555714: step 38930, loss = 0.79 (960.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:42:45.878139: step 38940, loss = 0.64 (967.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:42:47.278152: step 38950, loss = 0.60 (914.3 examples/sec; 0.140 sec/batch)
2017-05-07 20:42:48.576730: step 38960, loss = 0.80 (985.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:42:49.914327: step 38970, loss = 0.69 (956.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:42:51.244516: step 38980, loss = 0.77 (962.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:42:52.573712: step 38990, loss = 0.75 (963.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:42:54.048717: step 39000, loss = 0.78 (867.8 examples/sec; 0.148 sec/batch)
2017-05-07 20:42:55.239764: step 39010, loss = 0.76 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-07 20:42:56.611382: step 39020, loss = 0.70 (933.2 examples/sec; 0.137 sec/batch)
2017-05-07 20:42:57.929586: step 39030, loss = 0.57 (971.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:42:59.272062: step 39040, loss = 0.70 (953.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:43:00.634447: step 39050, loss = 0.73 (939.5 examples/sec; 0.136 sec/batch)
2017-05-07 20:43:01.942132: step 39060, loss = 0.73 (978.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:43:03.281429: step 39070, loss = 0.73 (955.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:43:04.612378: step 39080, loss = 0.73 (961.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:43:05.957249: step 39090, loss = 0.95 (951.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:43:07.441244: step 39100, loss = 0.70 (862.5 examples/sec; 0.148 sec/batch)
2017-05-07 20:43:08.647729: step 39110, loss = 0.78 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-07 20:43:09.985931: step 39120, loss = 0.71 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:43:11.323529: step 39130, loss = 0.65 (956.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:43:12.664912: step 39140, loss = 0.70 (954.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:43:14.077779: step 39150, loss = 0.74 (906.0 examples/sec; 0.141 sec/batch)
2017-05-07 20:43:15.358321: step 39160, loss = 0.76 (999.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:43:16.693071: step 39170, loss = 0.70 (959.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:43:18.023875: step 39180, loss = 0.92 (961.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:43:19.353664: step 39190, loss = 1.05 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:43:20.830946: step 39200, loss = 0.78 (866.5 examples/sec; 0.148 sec/batch)
2017-05-07 20:43:22.039794: step 39210, loss = 0.72 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-07 20:43:23.380642: step 39220, loss = 0.83 (954.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:43:24.695558: step 39230, loss = 0.73 (973.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:43:26.036980: step 39240, loss = 0.76 (954.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:43:27.420884: step 39250, loss = 0.80 (924.9 examples/sec; 0.138 sec/batch)
2017-05-07 20:43:28.734982: step 39260, loss = 0.77 (974.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:43:30.055657: step 39270, loss = 0.77 (969.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:43:31.370109: step 39280, loss = 0.74 (973.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:43:32.676390: step 39290, loss = 0.66 (979.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:43:34.131505: step 39300, loss = 0.62 (879.7 examples/sec; 0.146 sec/batch)
2017-05-07 20:43:35.359266: step 39310, loss = 0.91 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-07 20:43:36.671873: step 39320, loss = 0.80 (975.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:43:38.025243: step 39330, loss = 0.73 (945.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:43:39.344276: step 39340, loss = 0.56 (970.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:43:40.727645: step 39350, loss = 0.64 (925.3 examples/sec; 0.138 sec/batch)
2017-05-07 20:43:42.032719: step 39360, loss = 0.80 (980.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:43:43.362887: step 39370, loss = 0.79 (962.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:43:44.712646: step 39380, loss = 0.66 (948.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:43:46.078704: step 39390, loss = 0.71 (937.0 examples/sec; 0.137 sec/batch)
2017-05-07 20:43:47.549789: step 39400, loss = 0.74 (870.1 examples/sec; 0.147 sec/batch)
2017-05-07 20:43:48.748091: step 39410, loss = 0.85 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-07 20:43:50.090007: step 39420, loss = 0.92 (953.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:43:51.425981: step 39430, loss = 0.66 (958.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:43:52.773139: step 39440, loss = 0.84 (950.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:43:54.149354: step 39450, loss = 0.61 (930.1 examples/sec; 0.138 sec/batch)
2017-05-07 20:43:55.460008: step 39460, loss = 0.77 (976.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:43:56.770009: step 39470, loss = 0.73 (977.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:43:58.073115: step 39480, loss = 0.79 (982.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:43:59.417093: step 39490, loss = 0.60 (952.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:44:00.892335: step 39500, loss = 0.71 (867.6 examples/sec; 0.148 sec/batch)
2017-05-07 20:44:02.065637: step 39510, loss = 0.80 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-07 20:44:03.407890: step 39520, loss = 0.70 (953.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:44:04.746891: step 39530, loss = 0.85 (955.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:44:06.069102: step 39540, loss = 0.67 (968.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:44:07.479194: step 39550, loss = 0.64 (907.7 examples/sec; 0.141 sec/batch)
2017-05-07 20:44:08.757040: step 39560, loss = 0.71 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-07 20:44:10.109760: step 39570, loss = 1.01 (946.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:44:11.436209: step 39580, loss = 0.80 (965.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:44:12.764740: step 39590, loss = 0.74 (963.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:44:14.220973: step 39600, loss = 0.94 (879.0 examples/sec; 0.146 sec/batch)
2017-05-07 20:44:15.422742: step 39610, loss = 0.69 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-07 20:44:16.764510: step 39620, loss = 0.66 (954.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:44:18.107927: step 39630, loss = 0.79 (952.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:44:19.447126: step 39640, loss = 0.84 (955.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:44:20.814708: step 39650, loss = 0.56 (936.0 examples/sec; 0.137 sec/batch)
2017-05-07 20:44:22.120140: step 39660, loss = 0.67 (980.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:44:23.465998: step 39670, loss = 0.79 (951.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:44:24.783110: step 39680, loss = 0.74 (971.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:44:26.139162: step 39690, loss = 0.94 (944.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:44:27.590637: step 39700, loss = 0.84 (881.8 examples/sec; 0.145 sec/batch)
2017-05-07 20:44:28.757735: step 39710, loss = 0.73 (1096.7 examples/sec; 0.117 sec/batch)
2017-05-07 20:44:30.089680: step 39720, loss = 0.65 (961.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:44:31.438099: step 39730, loss = 0.65 (949.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:44:32.784770: step 39740, loss = 0.95 (950.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:44:34.160146: step 39750, loss = 0.79 (930.7 examples/sec; 0.138 sec/batch)
2017-05-07 20:44:35.468364: step 39760, loss = 0.68 (978.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:44:36.786929: step 39770, loss = 0.84 (970.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:44:38.106498: step 39780, loss = 0.76 (970.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:44:39.452742: step 39790, loss = 0.75 (950.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:44:40.950284: step 39800, loss = 0.77 (854.7 examples/sec; 0.150 sec/batch)
2017-05-07 20:44:42.149750: step 39810, loss = 0.68 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-07 20:44:43.487318: step 39820, loss = 0.64 (957.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:44:44.832129: step 39830, loss = 0.82 (951.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:44:46.159936: step 39840, loss = 0.71 (964.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:44:47.574301: step 39850, loss = 0.74 (905.0 examples/sec; 0.141 sec/batch)
2017-05-07 20:44:48.863050: step 39860, loss = 0.75 (993.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:44:50.203819: step 39870, loss = 0.74 (954.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:44:51.510105: step 39880, loss = 0.79 (979.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:44:52.866531: step 39890, loss = 0.75 (943.7 examples/sec; 0.136 sec/batch)
2017-05-07 20:44:54.345639: step 39900, loss = 0.80 (865.4 examples/sec; 0.148 sec/batch)
2017-05-07 20:44:55.560437: step 39910, loss = 0.76 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-07 20:44:56.903171: step 39920, loss = 0.80 (953.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:44:58.248809: step 39930, loss = 0.86 (951.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:44:59.587042: step 39940, loss = 0.78 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:45:00.944266: step 39950, loss = 0.73 (943.1 examples/sec; 0.136 sec/batch)
2017-05-07 20:45:02.235587: step 39960, loss = 0.72 (991.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:45:03.588185: step 39970, loss = 0.73 (946.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:45:04.914709: step 39980, loss = 0.83 (964.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:45:06.279283: step 39990, loss = 0.88 (938.0 examples/sec; 0.136 sec/batch)
2017-05-07 20:45:07.738944: step 40000, loss = 0.97 (876.9 examples/sec; 0.146 sec/batch)
2017-05-07 20:45:08.926589: step 40010, loss = 0.61 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-07 20:45:10.242575: step 40020, loss = 0.56 (972.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:45:11.611021: step 40030, loss = 0.77 (935.4 examples/sec; 0.137 sec/batch)
2017-05-07 20:45:12.940375: step 40040, loss = 0.86 (962.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:45:14.342892: step 40050, loss = 0.76 (912.7 examples/sec; 0.140 sec/batch)
2017-05-07 20:45:15.654988: step 40060, loss = 0.91 (975.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:45:16.992146: step 40070, loss = 0.92 (957.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:45:18.334175: step 40080, loss = 0.85 (953.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:45:19.666661: step 40090, loss = 0.72 (960.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:45:21.136713: step 40100, loss = 0.66 (870.7 examples/sec; 0.147 sec/batch)
2017-05-07 20:45:22.361011: step 40110, loss = 0.73 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-07 20:45:23.706039: step 40120, loss = 0.71 (951.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:45:25.031388: step 40130, loss = 0.73 (965.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:45:26.385805: step 40140, loss = 0.63 (945.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:45:27.777407: step 40150, loss = 0.83 (919.8 examples/sec; 0.139 sec/batch)
2017-05-07 20:45:29.093596: step 40160, loss = 0.78 (972.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:45:30.415489: step 40170, loss = 0.69 (968.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:45:31.746994: step 40180, loss = 0.79 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:45:33.085522: step 40190, loss = 0.83 (956.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:45:34.539231: step 40200, loss = 0.62 (880.5 examples/sec; 0.145 sec/batch)
2017-05-07 20:45:35.744663: step 40210, loss = 0.98 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-07 20:45:37.065877: step 40220, loss = 0.78 (968.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:45:38.395534: step 40230, loss = 0.69 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:45:39.751261: step 40240, loss = 0.66 (944.1 examples/sec; 0.136 sec/batch)
2017-05-07 20:45:41.122891: step 40250, loss = 0.79 (933.2 examples/sec; 0.137 sec/batch)
2017-05-07 20:45:42.433318: step 40260, loss = 0.76 (976.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:45:43.765620: step 40270, loss = 0.79 (960.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:45:45.075995: step 40280, loss = 0.74 (976.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:45:46.406630: step 40290, loss = 0.81 (961.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:45:47.886595: step 40300, loss = 0.76 (864.9 examples/sec; 0.148 sec/batch)
2017-05-07 20:45:49.089067: step 40310, loss = 0.77 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-07 20:45:50.419374: step 40320, loss = 0.81 (962.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:45:51.771815: step 40330, loss = 0.64 (946.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:45:53.103318: step 40340, loss = 0.66 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:45:54.653255: step 40350, loss = 0.80 (825.8 examples/sec; 0.155 sec/batch)
2017-05-07 20:45:55.897437: step 40360, loss = 0.82 (1028.8 examples/sec; 0.124 sec/batch)
2017-05-07 20:45:57.189926: step 40370, loss = 0.64 (990.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:45:58.534232: step 40380, loss = 0.98 (952.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:45:59.864780: step 40390, loss = 0.76 (962.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:46:01.318016: step 40400, loss = 0.72 (880.8 examples/sec; 0.145 sec/batch)
2017-05-07 20:46:02.522605: step 40410, loss = 0.72 (1062.6 examples/sec; 0.120 sec/batch)
2017-05-07 20:46:03.842783: step 40420, loss = 0.73 (969.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:46:05.190171: step 40430, loss = 0.52 (950.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:46:06.505262: step 40440, loss = 0.66 (973.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:46:07.911368: step 40450, loss = 0.63 (910.3 examples/sec; 0.141 sec/batch)
2017-05-07 20:46:09.218185: step 40460, loss = 0.74 (979.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:46:10.544929: step 40470, loss = 0.78 (964.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:46:11.885971: step 40480, loss = 0.74 (954.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:46:13.219924: step 40490, loss = 0.65 (959.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:46:14.715796: step 40500, loss = 0.75 (855.7 examples/sec; 0.150 sec/batch)
2017-05-07 20:46:15.924591: step 40510, loss = 0.55 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-07 20:46:17.265776: step 40520, loss = 0.78 (954.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:46:18.611113: step 40530, loss = 0.65 (951.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:46:19.952009: step 40540, loss = 0.78 (954.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:46:21.330697: step 40550, loss = 0.73 (928.4 examples/sec; 0.138 sec/batch)
2017-05-07 20:46:22.640034: step 40560, loss = 0.83 (977.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:46:23.956160: step 40570, loss = 0.70 (972.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:46:25.280477: step 40580, loss = 0.71 (966.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:46:26.600668: step 40590, loss = 0.69 (969.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:46:28.075996: step 40600, loss = 0.70 (867.6 examples/sec; 0.148 sec/batch)
2017-05-07 20:46:29.292489: step 40610, loss = 0.69 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-07 20:46:30.612106: step 40620, loss = 0.81 (970.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:46:31.951041: step 40630, loss = 0.65 (956.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:46:33.289866: step 40640, loss = 0.87 (956.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:46:34.693716: step 40650, loss = 0.79 (911.8 examples/sec; 0.140 sec/batch)
2017-05-07 20:46:35.971480: step 40660, loss = 0.94 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:46:37.298557: step 40670, loss = 0.64 (964.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:46:38.635055: step 40680, loss = 0.73 (957.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:46:39.965544: step 40690, loss = 0.75 (962.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:46:41.446569: step 40700, loss = 0.81 (864.3 examples/sec; 0.148 sec/batch)
2017-05-07 20:46:42.613626: step 40710, loss = 0.84 (1096.8 examples/sec; 0.117 sec/batch)
2017-05-07 20:46:43.957992: step 40720, loss = 0.66 (952.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:46:45.291702: step 40730, loss = 0.70 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:46:46.618532: step 40740, loss = 0.73 (964.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:46:47.992757: step 40750, loss = 0.76 (931.4 examples/sec; 0.137 sec/batch)
2017-05-07 20:46:49.276336: step 40760, loss = 0.73 (997.2 examples/sec; 0.128 sec/batch)
2017-05-07 20:46:50.586996: step 40770, loss = 0.71 (976.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:46:51.918581: step 40780, loss = 0.74 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:46:53.252847: step 40790, loss = 0.61 (959.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:46:54.715210: step 40800, loss = 0.88 (875.3 examples/sec; 0.146 sec/batch)
2017-05-07 20:46:55.932931: step 40810, loss = 0.64 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-07 20:46:57.286141: step 40820, loss = 0.66 (945.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:46:58.625141: step 40830, loss = 0.62 (955.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:46:59.969849: step 40840, loss = 0.83 (951.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:47:01.330580: step 40850, loss = 0.68 (940.7 examples/sec; 0.136 sec/batch)
2017-05-07 20:47:02.627837: step 40860, loss = 0.75 (986.7 examples/sec; 0.130 sec/batch)
2017-05-07 20:47:03.950413: step 40870, loss = 0.95 (967.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:47:05.282441: step 40880, loss = 0.96 (960.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:47:06.620652: step 40890, loss = 0.73 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:47:08.102508: step 40900, loss = 0.59 (863.8 examples/sec; 0.148 sec/batch)
2017-05-07 20:47:09.306088: step 40910, loss = 0.82 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-07 20:47:10.634903: step 40920, loss = 0.76 (963.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:47:11.951013: step 40930, loss = 0.89 (972.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:47:13.288705: step 40940, loss = 0.96 (956.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:47:14.676274: step 40950, loss = 0.84 (922.5 examples/sec; 0.139 sec/batch)
2017-05-07 20:47:15.955011: step 40960, loss = 0.74 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:47:17.300221: step 40970, loss = 0.67 (951.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:47:18.629849: step 40980, loss = 0.84 (962.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:47:19.967257: step 40990, loss = 0.68 (957.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:47:21.412301: step 41000, loss = 0.96 (885.8 examples/sec; 0.145 sec/batch)
2017-05-07 20:47:22.654940: step 41010, loss = 0.65 (1030.1 examples/sec; 0.124 sec/batch)
2017-05-07 20:47:23.974906: step 41020, loss = 0.80 (969.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:47:25.292201: step 41030, loss = 0.72 (971.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:47:26.646136: step 41040, loss = 0.75 (945.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:47:28.051868: step 41050, loss = 0.55 (910.6 examples/sec; 0.141 sec/batch)
2017-05-07 20:47:29.348199: step 41060, loss = 0.80 (987.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:47:30.676734: step 41070, loss = 0.87 (963.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:47:32.025694: step 41080, loss = 0.90 (948.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:47:33.360223: step 41090, loss = 0.70 (959.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:47:34.818993: step 41100, loss = 0.82 (877.4 examples/sec; 0.146 sec/batch)
2017-05-07 20:47:36.002753: step 41110, loss = 0.72 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-07 20:47:37.333602: step 41120, loss = 0.90 (961.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:47:38.666399: step 41130, loss = 0.75 (960.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:47:40.002874: step 41140, loss = 0.72 (957.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:47:41.393706: step 41150, loss = 0.78 (920.3 examples/sec; 0.139 sec/batch)
2017-05-07 20:47:42.707769: step 41160, loss = 0.63 (974.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:47:44.041713: step 41170, loss = 0.74 (959.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:47:45.381199: step 41180, loss = 0.83 (955.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:47:46.698225: step 41190, loss = 0.83 (971.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:47:48.189942: step 41200, loss = 0.60 (858.1 examples/sec; 0.149 sec/batch)
2017-05-07 20:47:49.370259: step 41210, loss = 0.84 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-07 20:47:50.707085: step 41220, loss = 0.89 (957.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:47:52.019106: step 41230, loss = 0.63 (975.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:47:53.358516: step 41240, loss = 0.78 (955.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:47:54.740024: step 41250, loss = 0.72 (926.5 examples/sec; 0.138 sec/batch)
2017-05-07 20:47:56.049853: step 41260, loss = 0.61 (977.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:47:57.369908: step 41270, loss = 0.89 (969.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:47:58.703784: step 41280, loss = 0.68 (959.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:48:00.052588: step 41290, loss = 0.66 (949.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:48:01.523520: step 41300, loss = 0.61 (870.2 examples/sec; 0.147 sec/batch)
2017-05-07 20:48:02.738765: step 41310, loss = 0.78 (1053.3 examples/sec; 0.122 sec/batch)
2017-05-07 20:48:04.069256: step 41320, loss = 0.76 (962.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:48:05.381553: step 41330, loss = 0.87 (975.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:48:06.723781: step 41340, loss = 0.94 (953.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:48:08.117754: step 41350, loss = 0.69 (918.2 examples/sec; 0.139 sec/batch)
2017-05-07 20:48:09.398658: step 41360, loss = 0.68 (999.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:48:10.722123: step 41370, loss = 0.74 (967.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:48:12.082159: step 41380, loss = 0.78 (941.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:48:13.422025: step 41390, loss = 0.68 (955.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:48:14.885942: step 41400, loss = 0.74 (874.4 examples/sec; 0.146 sec/batch)
2017-05-07 20:48:16.086359: step 41410, loss = 0.74 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-07 20:48:17.411576: step 41420, loss = 0.69 (965.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:48:18.759184: step 41430, loss = 0.75 (949.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:48:20.080616: step 41440, loss = 0.71 (968.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:48:21.465654: step 41450, loss = 0.87 (924.2 examples/sec; 0.139 sec/batch)
2017-05-07 20:48:22.761265: step 41460, loss = 0.66 (988.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:48:24.075599: step 41470, loss = 0.73 (973.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:48:25.423552: step 41480, loss = 0.74 (949.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:48:26.751428: step 41490, loss = 0.63 (963.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:48:28.216418: step 41500, loss = 0.78 (873.7 examples/sec; 0.146 sec/batch)
2017-05-07 20:48:29.403967: step 41510, loss = 0.72 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-07 20:48:30.757343: step 41520, loss = 0.95 (945.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:48:32.094585: step 41530, loss = 0.82 (957.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:48:33.445858: step 41540, loss = 0.73 (947.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:48:34.842343: step 41550, loss = 0.67 (916.6 examples/sec; 0.140 sec/batch)
2017-05-07 20:48:36.123255: step 41560, loss = 0.80 (999.3 examples/sec; 0.128 sec/batch)
2017-05-07 20:48:37.464627: step 41570, loss = 0.77 (954.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:48:38.803240: step 41580, loss = 0.85 (956.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:48:40.121563: step 41590, loss = 0.88 (970.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:48:41.565078: step 41600, loss = 0.68 (886.7 examples/sec; 0.144 sec/batch)
2017-05-07 20:48:42.752776: step 41610, loss = 0.71 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-07 20:48:44.081429: step 41620, loss = 0.65 (963.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:48:45.396668: step 41630, loss = 0.71 (973.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:48:46.751809: step 41640, loss = 0.72 (944.5 examples/sec; 0.136 sec/batch)
2017-05-07 20:48:48.129698: step 41650, loss = 0.77 (929.0 examples/sec; 0.138 sec/batch)
2017-05-07 20:48:49.394110: step 41660, loss = 0.86 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-07 20:48:50.737303: step 41670, loss = 0.72 (953.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:48:52.082389: step 41680, loss = 0.59 (951.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:48:53.413770: step 41690, loss = 0.71 (961.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:48:54.897082: step 41700, loss = 0.71 (862.9 examples/sec; 0.148 sec/batch)
2017-05-07 20:48:56.111114: step 41710, loss = 0.74 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-07 20:48:57.457737: step 41720, loss = 0.85 (950.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:48:58.793595: step 41730, loss = 0.81 (958.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:49:00.133600: step 41740, loss = 0.84 (955.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:49:01.512350: step 41750, loss = 0.86 (928.4 examples/sec; 0.138 sec/batch)
2017-05-07 20:49:02.797571: step 41760, loss = 0.77 (995.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:49:04.115724: step 41770, loss = 0.80 (971.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:49:05.418972: step 41780, loss = 0.82 (982.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:49:06.767647: step 41790, loss = 0.72 (949.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:49:08.223494: step 41800, loss = 0.59 (879.2 examples/sec; 0.146 sec/batch)
2017-05-07 20:49:09.431563: step 41810, loss = 0.71 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-07 20:49:10.771821: step 41820, loss = 0.81 (955.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:49:12.128948: step 41830, loss = 0.76 (943.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:49:13.471171: step 41840, loss = 0.73 (953.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:49:14.845834: step 41850, loss = 0.81 (931.1 examples/sec; 0.137 sec/batch)
2017-05-07 20:49:16.155575: step 41860, loss = 0.88 (977.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:49:17.488301: step 41870, loss = 0.74 (960.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:49:18.814298: step 41880, loss = 0.74 (965.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:49:20.159047: step 41890, loss = 0.66 (951.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:49:21.656478: step 41900, loss = 0.71 (854.8 examples/sec; 0.150 sec/batch)
2017-05-07 20:49:22.794777: step 41910, loss = 0.72 (1124.5 examples/sec; 0.114 sec/batch)
2017-05-07 20:49:24.122715: step 41920, loss = 0.73 (963.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:49:25.475664: step 41930, loss = 0.75 (946.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:49:26.810571: step 41940, loss = 0.70 (958.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:49:28.220433: step 41950, loss = 0.78 (907.9 examples/sec; 0.141 sec/batch)
2017-05-07 20:49:29.477629: step 41960, loss = 0.86 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-07 20:49:30.825157: step 41970, loss = 0.85 (949.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:49:32.156358: step 41980, loss = 0.77 (961.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:49:33.472288: step 41990, loss = 0.82 (972.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:49:34.941831: step 42000, loss = 0.72 (871.0 examples/sec; 0.147 sec/batch)
2017-05-07 20:49:36.154814: step 42010, loss = 0.61 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-07 20:49:37.485620: step 42020, loss = 0.91 (961.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:49:38.843526: step 42030, loss = 0.73 (942.6 examples/sec; 0.136 sec/batch)
2017-05-07 20:49:40.156691: step 42040, loss = 0.72 (974.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:49:41.535445: step 42050, loss = 0.68 (928.4 examples/sec; 0.138 sec/batch)
2017-05-07 20:49:42.865934: step 42060, loss = 0.94 (962.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:49:44.196060: step 42070, loss = 0.79 (962.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:49:45.529184: step 42080, loss = 0.60 (960.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:49:46.874354: step 42090, loss = 0.75 (951.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:49:48.326773: step 42100, loss = 0.67 (881.3 examples/sec; 0.145 sec/batch)
2017-05-07 20:49:49.550331: step 42110, loss = 0.68 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-07 20:49:50.884190: step 42120, loss = 0.73 (959.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:49:52.227557: step 42130, loss = 0.71 (952.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:49:53.560008: step 42140, loss = 0.67 (960.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:49:54.955830: step 42150, loss = 0.76 (917.0 examples/sec; 0.140 sec/batch)
2017-05-07 20:49:56.272648: step 42160, loss = 0.86 (972.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:49:57.594697: step 42170, loss = 0.73 (968.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:49:58.913799: step 42180, loss = 0.74 (970.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:50:00.271384: step 42190, loss = 0.77 (942.9 examples/sec; 0.136 sec/batch)
2017-05-07 20:50:01.754415: step 42200, loss = 0.74 (863.1 examples/sec; 0.148 sec/batch)
2017-05-07 20:50:02.985434: step 42210, loss = 0.91 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-07 20:50:04.308480: step 42220, loss = 0.72 (967.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:50:05.627635: step 42230, loss = 0.73 (970.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:50:06.965331: step 42240, loss = 0.83 (956.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:50:08.350167: step 42250, loss = 0.66 (924.3 examples/sec; 0.138 sec/batch)
2017-05-07 20:50:09.644164: step 42260, loss = 0.61 (989.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:50:10.987658: step 42270, loss = 0.67 (952.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:50:12.314836: step 42280, loss = 0.71 (964.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:50:13.648334: step 42290, loss = 0.87 (959.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:50:15.127270: step 42300, loss = 0.94 (865.5 examples/sec; 0.148 sec/batch)
2017-05-07 20:50:16.347739: step 42310, loss = 0.71 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-07 20:50:17.662521: step 42320, loss = 0.65 (973.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:50:18.999656: step 42330, loss = 0.66 (957.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:50:20.311495: step 42340, loss = 0.76 (975.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:50:21.710225: step 42350, loss = 0.63 (915.1 examples/sec; 0.140 sec/batch)
2017-05-07 20:50:22.988937: step 42360, loss = 0.75 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:50:24.331274: step 42370, loss = 0.76 (953.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:50:25.643446: step 42380, loss = 0.66 (975.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:50:26.988984: step 42390, loss = 0.67 (951.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:50:28.461289: step 42400, loss = 0.80 (869.4 examples/sec; 0.147 sec/batch)
2017-05-07 20:50:29.661612: step 42410, loss = 0.80 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-07 20:50:31.017771: step 42420, loss = 0.63 (943.9 examples/sec; 0.136 sec/batch)
2017-05-07 20:50:32.335024: step 42430, loss = 0.70 (971.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:50:33.647785: step 42440, loss = 0.71 (975.0 examples/sec; 0.131 sec/batch)
2017-05-07 20:50:35.039837: step 42450, loss = 0.84 (919.5 examples/sec; 0.139 sec/batch)
2017-05-07 20:50:36.371934: step 42460, loss = 0.83 (960.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:50:37.699039: step 42470, loss = 0.60 (964.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:50:39.046195: step 42480, loss = 0.79 (950.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:50:40.393074: step 42490, loss = 0.81 (950.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:50:41.848087: step 42500, loss = 0.79 (879.7 examples/sec; 0.146 sec/batch)
2017-05-07 20:50:43.064227: step 42510, loss = 0.70 (1052.5 examples/sec; 0.122 sec/batch)
2017-05-07 20:50:44.375091: step 42520, loss = 0.79 (976.5 examples/sec; 0.131 sec/batch)
2017-05-07 20:50:45.728151: step 42530, loss = 0.92 (946.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:50:47.065754: step 42540, loss = 0.64 (956.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:50:48.447658: step 42550, loss = 0.92 (926.3 examples/sec; 0.138 sec/batch)
2017-05-07 20:50:49.734611: step 42560, loss = 0.78 (994.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:50:51.069494: step 42570, loss = 0.64 (958.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:50:52.403290: step 42580, loss = 0.72 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:50:53.742015: step 42590, loss = 0.90 (956.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:50:55.215092: step 42600, loss = 0.67 (868.9 examples/sec; 0.147 sec/batch)
2017-05-07 20:50:56.423966: step 42610, loss = 0.86 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-07 20:50:57.765594: step 42620, loss = 0.68 (954.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:50:59.090987: step 42630, loss = 0.83 (965.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:51:00.454912: step 42640, loss = 0.93 (938.5 examples/sec; 0.136 sec/batch)
2017-05-07 20:51:01.811499: step 42650, loss = 0.80 (943.6 examples/sec; 0.136 sec/batch)
2017-05-07 20:51:03.113608: step 42660, loss = 0.58 (983.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:51:04.424892: step 42670, loss = 0.66 (976.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:51:05.771040: step 42680, loss = 0.69 (950.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:51:07.107398: step 42690, loss = 0.90 (957.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:51:08.573664: step 42700, loss = 0.88 (873.0 examples/sec; 0.147 sec/batch)
2017-05-07 20:51:09.776186: step 42710, loss = 0.64 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-07 20:51:11.126352: step 42720, loss = 0.77 (948.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:51:12.484663: step 42730, loss = 0.79 (942.3 examples/sec; 0.136 sec/batch)
2017-05-07 20:51:13.816771: step 42740, loss = 0.93 (960.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:51:15.177277: step 42750, loss = 0.77 (940.8 examples/sec; 0.136 sec/batch)
2017-05-07 20:51:16.476152: step 42760, loss = 0.68 (985.5 examples/sec; 0.130 sec/batch)
2017-05-07 20:51:17.804673: step 42770, loss = 0.66 (963.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:51:19.150144: step 42780, loss = 0.66 (951.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:51:20.489692: step 42790, loss = 0.53 (955.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:51:21.944299: step 42800, loss = 0.68 (880.0 examples/sec; 0.145 sec/batch)
2017-05-07 20:51:23.174648: step 42810, loss = 0.57 (1040.4 examples/sec; 0.123 sec/batch)
2017-05-07 20:51:24.512954: step 42820, loss = 0.64 (956.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:51:25.841744: step 42830, loss = 0.64 (963.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:51:27.165316: step 42840, loss = 0.72 (967.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:51:28.566326: step 42850, loss = 0.73 (913.6 examples/sec; 0.140 sec/batch)
2017-05-07 20:51:29.835878: step 42860, loss = 0.64 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-07 20:51:31.142599: step 42870, loss = 0.75 (979.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:51:32.462356: step 42880, loss = 0.69 (969.9 examples/sec; 0.132 sec/batch)
2017-05-07 20:51:33.781838: step 42890, loss = 0.72 (970.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:51:35.267321: step 42900, loss = 0.67 (861.7 examples/sec; 0.149 sec/batch)
2017-05-07 20:51:36.477656: step 42910, loss = 0.53 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-07 20:51:37.803543: step 42920, loss = 0.75 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:51:39.151701: step 42930, loss = 0.73 (949.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:51:40.488238: step 42940, loss = 0.63 (957.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:51:41.891332: step 42950, loss = 0.76 (912.3 examples/sec; 0.140 sec/batch)
2017-05-07 20:51:43.177785: step 42960, loss = 0.72 (995.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:51:44.474717: step 42970, loss = 0.69 (986.9 examples/sec; 0.130 sec/batch)
2017-05-07 20:51:45.788520: step 42980, loss = 0.74 (974.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:51:47.134904: step 42990, loss = 0.69 (950.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:51:48.594407: step 43000, loss = 0.74 (877.0 examples/sec; 0.146 sec/batch)
2017-05-07 20:51:49.793881: step 43010, loss = 0.80 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-07 20:51:51.138117: step 43020, loss = 0.74 (952.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:51:52.474930: step 43030, loss = 0.64 (957.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:51:53.786924: step 43040, loss = 0.80 (975.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:51:55.180682: step 43050, loss = 0.66 (918.4 examples/sec; 0.139 sec/batch)
2017-05-07 20:51:56.477055: step 43060, loss = 0.63 (987.4 examples/sec; 0.130 sec/batch)
2017-05-07 20:51:57.790950: step 43070, loss = 0.64 (974.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:51:59.121645: step 43080, loss = 0.65 (961.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:52:00.452853: step 43090, loss = 0.77 (961.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:52:01.938947: step 43100, loss = 0.65 (861.3 examples/sec; 0.149 sec/batch)
2017-05-07 20:52:03.134286: step 43110, loss = 0.83 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-07 20:52:04.482841: step 43120, loss = 0.77 (949.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:52:05.779380: step 43130, loss = 0.64 (987.2 examples/sec; 0.130 sec/batch)
2017-05-07 20:52:07.134571: step 43140, loss = 0.71 (944.5 examples/sec; 0.136 sec/batch)
2017-05-07 20:52:08.521644: step 43150, loss = 0.78 (922.8 examples/sec; 0.139 sec/batch)
2017-05-07 20:52:09.807500: step 43160, loss = 0.71 (995.4 examples/sec; 0.129 sec/batch)
2017-05-07 20:52:11.126621: step 43170, loss = 0.82 (970.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:52:12.448431: step 43180, loss = 0.78 (968.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:52:13.762250: step 43190, loss = 0.75 (974.3 examples/sec; 0.131 sec/batch)
2017-05-07 20:52:15.242816: step 43200, loss = 0.77 (864.5 examples/sec; 0.148 sec/batch)
2017-05-07 20:52:16.439814: step 43210, loss = 0.97 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-07 20:52:17.769657: step 43220, loss = 0.57 (962.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:52:19.101627: step 43230, loss = 0.74 (961.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:52:20.412902: step 43240, loss = 0.71 (976.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:52:21.788861: step 43250, loss = 0.68 (930.3 examples/sec; 0.138 sec/batch)
2017-05-07 20:52:23.074310: step 43260, loss = 0.73 (995.8 examples/sec; 0.129 sec/batch)
2017-05-07 20:52:24.397819: step 43270, loss = 0.63 (967.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:52:25.721790: step 43280, loss = 0.72 (966.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:52:27.072954: step 43290, loss = 0.74 (947.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:52:28.543198: step 43300, loss = 0.66 (870.6 examples/sec; 0.147 sec/batch)
2017-05-07 20:52:29.775379: step 43310, loss = 0.90 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-07 20:52:31.132918: step 43320, loss = 0.63 (942.9 examples/sec; 0.136 sec/batch)
2017-05-07 20:52:32.471355: step 43330, loss = 0.67 (956.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:52:33.788940: step 43340, loss = 0.61 (971.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:52:35.176347: step 43350, loss = 0.70 (922.6 examples/sec; 0.139 sec/batch)
2017-05-07 20:52:36.501725: step 43360, loss = 0.71 (965.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:52:37.809089: step 43370, loss = 0.66 (979.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:52:39.145589: step 43380, loss = 0.74 (957.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:52:40.469677: step 43390, loss = 0.78 (966.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:52:41.955670: step 43400, loss = 0.77 (861.4 examples/sec; 0.149 sec/batch)
2017-05-07 20:52:43.186516: step 43410, loss = 0.77 (1039.9 examples/sec; 0.123 sec/batch)
2017-05-07 20:52:44.521652: step 43420, loss = 0.69 (958.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:52:45.887628: step 43430, loss = 0.83 (937.1 examples/sec; 0.137 sec/batch)
2017-05-07 20:52:47.214544: step 43440, loss = 0.62 (964.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:52:48.638381: step 43450, loss = 0.65 (899.0 examples/sec; 0.142 sec/batch)
2017-05-07 20:52:49.902302: step 43460, loss = 0.88 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-07 20:52:51.246383: step 43470, loss = 0.70 (952.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:52:52.589822: step 43480, loss = 0.90 (952.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:52:53.956351: step 43490, loss = 0.65 (936.7 examples/sec; 0.137 sec/batch)
2017-05-07 20:52:55.438909: step 43500, loss = 0.62 (863.4 examples/sec; 0.148 sec/batch)
2017-05-07 20:52:56.637720: step 43510, loss = 0.64 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-07 20:52:57.979200: step 43520, loss = 0.73 (954.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:52:59.319362: step 43530, loss = 0.72 (955.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:53:00.629187: step 43540, loss = 0.61 (977.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:53:02.001744: step 43550, loss = 0.55 (932.6 examples/sec; 0.137 sec/batch)
2017-05-07 20:53:03.293963: step 43560, loss = 0.74 (990.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:53:04.646759: step 43570, loss = 0.79 (946.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:53:05.988604: step 43580, loss = 0.78 (953.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:53:07.308145: step 43590, loss = 0.66 (970.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:53:08.775444: step 43600, loss = 0.72 (872.3 examples/sec; 0.147 sec/batch)
2017-05-07 20:53:09.977648: step 43610, loss = 0.71 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-07 20:53:11.323542: step 43620, loss = 0.90 (951.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:53:12.643872: step 43630, loss = 0.63 (969.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:53:13.977897: step 43640, loss = 0.65 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:53:15.374741: step 43650, loss = 0.74 (916.4 examples/sec; 0.140 sec/batch)
2017-05-07 20:53:16.651441: step 43660, loss = 0.77 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:53:17.982244: step 43670, loss = 0.80 (961.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:53:19.311501: step 43680, loss = 0.70 (962.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:53:20.657276: step 43690, loss = 0.69 (951.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:53:22.167356: step 43700, loss = 0.70 (847.6 examples/sec; 0.151 sec/batch)
2017-05-07 20:53:23.311084: step 43710, loss = 0.93 (1119.1 examples/sec; 0.114 sec/batch)
2017-05-07 20:53:24.655909: step 43720, loss = 0.86 (951.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:53:26.014647: step 43730, loss = 0.69 (942.1 examples/sec; 0.136 sec/batch)
2017-05-07 20:53:27.345497: step 43740, loss = 0.65 (961.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:53:28.720638: step 43750, loss = 0.77 (930.8 examples/sec; 0.138 sec/batch)
2017-05-07 20:53:30.001118: step 43760, loss = 0.90 (999.6 examples/sec; 0.128 sec/batch)
2017-05-07 20:53:31.318267: step 43770, loss = 0.82 (971.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:53:32.657324: step 43780, loss = 0.75 (955.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:53:33.979613: step 43790, loss = 0.78 (968.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:53:35.450937: step 43800, loss = 0.78 (870.0 examples/sec; 0.147 sec/batch)
2017-05-07 20:53:36.637301: step 43810, loss = 0.62 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-07 20:53:37.968362: step 43820, loss = 0.95 (961.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:53:39.302866: step 43830, loss = 0.79 (959.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:53:40.626558: step 43840, loss = 0.78 (967.0 examples/sec; 0.132 sec/batch)
2017-05-07 20:53:42.023661: step 43850, loss = 0.66 (916.2 examples/sec; 0.140 sec/batch)
2017-05-07 20:53:43.324130: step 43860, loss = 0.69 (984.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:53:44.647536: step 43870, loss = 0.73 (967.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:53:45.994041: step 43880, loss = 0.67 (950.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:53:47.346651: step 43890, loss = 0.79 (946.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:53:48.815898: step 43900, loss = 0.69 (871.2 examples/sec; 0.147 sec/batch)
2017-05-07 20:53:50.028218: step 43910, loss = 0.71 (1055.9 examples/sec; 0.121 sec/batch)
2017-05-07 20:53:51.359014: step 43920, loss = 0.72 (961.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:53:52.691617: step 43930, loss = 0.56 (960.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:53:54.015627: step 43940, loss = 0.73 (966.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:53:55.387740: step 43950, loss = 0.66 (932.9 examples/sec; 0.137 sec/batch)
2017-05-07 20:53:56.675240: step 43960, loss = 0.57 (994.2 examples/sec; 0.129 sec/batch)
2017-05-07 20:53:58.008302: step 43970, loss = 0.92 (960.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:53:59.328801: step 43980, loss = 0.77 (969.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:54:00.675997: step 43990, loss = 0.93 (950.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:54:02.124024: step 44000, loss = 0.71 (884.0 examples/sec; 0.145 sec/batch)
2017-05-07 20:54:03.341024: step 44010, loss = 0.80 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-07 20:54:04.674660: step 44020, loss = 0.69 (959.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:54:06.017863: step 44030, loss = 0.67 (952.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:54:07.351591: step 44040, loss = 0.59 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:54:08.744114: step 44050, loss = 0.73 (919.2 examples/sec; 0.139 sec/batch)
2017-05-07 20:54:10.025642: step 44060, loss = 0.64 (998.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:54:11.336329: step 44070, loss = 0.71 (976.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:54:12.676341: step 44080, loss = 0.76 (955.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:54:14.003818: step 44090, loss = 0.62 (964.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:54:15.473667: step 44100, loss = 0.77 (870.8 examples/sec; 0.147 sec/batch)
2017-05-07 20:54:16.674638: step 44110, loss = 0.74 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-07 20:54:18.007796: step 44120, loss = 0.84 (960.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:54:19.359880: step 44130, loss = 0.72 (946.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:54:20.708162: step 44140, loss = 0.82 (949.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:54:22.078333: step 44150, loss = 0.79 (934.2 examples/sec; 0.137 sec/batch)
2017-05-07 20:54:23.369822: step 44160, loss = 0.80 (991.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:54:24.707930: step 44170, loss = 0.82 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:54:26.035807: step 44180, loss = 0.61 (963.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:54:27.350238: step 44190, loss = 0.75 (973.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:54:28.806937: step 44200, loss = 0.66 (878.7 examples/sec; 0.146 sec/batch)
2017-05-07 20:54:30.014997: step 44210, loss = 0.78 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-07 20:54:31.355666: step 44220, loss = 0.64 (954.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:54:32.681558: step 44230, loss = 0.84 (965.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:54:34.011169: step 44240, loss = 0.70 (962.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:54:35.406097: step 44250, loss = 0.71 (917.6 examples/sec; 0.139 sec/batch)
2017-05-07 20:54:36.700171: step 44260, loss = 0.70 (989.1 examples/sec; 0.129 sec/batch)
2017-05-07 20:54:38.050383: step 44270, loss = 0.68 (948.0 examples/sec; 0.135 sec/batch)
2017-05-07 20:54:39.392270: step 44280, loss = 0.80 (953.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:54:40.717686: step 44290, loss = 0.77 (965.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:54:42.194205: step 44300, loss = 0.75 (866.9 examples/sec; 0.148 sec/batch)
2017-05-07 20:54:43.389736: step 44310, loss = 0.71 (1070.6 examples/sec; 0.120 sec/batch)
2017-05-07 20:54:44.696151: step 44320, loss = 0.94 (979.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:54:46.021404: step 44330, loss = 0.65 (965.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:54:47.351621: step 44340, loss = 0.70 (962.2 examples/sec; 0.133 sec/batch)
2017-05-07 20:54:48.733079: step 44350, loss = 0.75 (926.6 examples/sec; 0.138 sec/batch)
2017-05-07 20:54:50.011324: step 44360, loss = 0.69 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:54:51.369510: step 44370, loss = 0.68 (942.4 examples/sec; 0.136 sec/batch)
2017-05-07 20:54:52.677134: step 44380, loss = 0.93 (978.9 examples/sec; 0.131 sec/batch)
2017-05-07 20:54:54.016805: step 44390, loss = 0.71 (955.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:54:55.474356: step 44400, loss = 0.79 (878.2 examples/sec; 0.146 sec/batch)
2017-05-07 20:54:56.694624: step 44410, loss = 0.70 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-07 20:54:58.013269: step 44420, loss = 0.68 (970.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:54:59.370177: step 44430, loss = 0.65 (943.3 examples/sec; 0.136 sec/batch)
2017-05-07 20:55:00.697963: step 44440, loss = 0.75 (964.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:55:02.079824: step 44450, loss = 0.62 (926.3 examples/sec; 0.138 sec/batch)
2017-05-07 20:55:03.408953: step 44460, loss = 0.68 (963.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:55:04.757179: step 44470, loss = 0.59 (949.4 examples/sec; 0.135 sec/batch)
2017-05-07 20:55:06.073226: step 44480, loss = 0.84 (972.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:55:07.416831: step 44490, loss = 0.74 (952.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:55:08.869875: step 44500, loss = 0.56 (880.9 examples/sec; 0.145 sec/batch)
2017-05-07 20:55:10.058822: step 44510, loss = 0.61 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-07 20:55:11.398366: step 44520, loss = 0.54 (955.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:55:12.727483: step 44530, loss = 0.78 (963.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:55:14.071731: step 44540, loss = 0.73 (952.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:55:15.459812: step 44550, loss = 0.62 (922.1 examples/sec; 0.139 sec/batch)
2017-05-07 20:55:16.745883: step 44560, loss = 0.85 (995.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:55:18.083659: step 44570, loss = 0.86 (956.8 examples/sec; 0.134 sec/batch)
2017-05-07 20:55:19.432101: step 44580, loss = 0.72 (949.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:55:20.754947: step 44590, loss = 0.78 (967.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:55:22.225625: step 44600, loss = 0.77 (870.3 examples/sec; 0.147 sec/batch)
2017-05-07 20:55:23.414827: step 44610, loss = 0.78 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-07 20:55:24.752522: step 44620, loss = 0.65 (956.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:55:26.078885: step 44630, loss = 0.79 (965.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:55:27.418842: step 44640, loss = 0.79 (955.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:55:28.832291: step 44650, loss = 0.79 (905.6 examples/sec; 0.141 sec/batch)
2017-05-07 20:55:30.128075: step 44660, loss = 0.63 (987.8 examples/sec; 0.130 sec/batch)
2017-05-07 20:55:31.480195: step 44670, loss = 0.71 (946.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:55:32.813646: step 44680, loss = 0.71 (959.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:55:34.157260: step 44690, loss = 0.75 (952.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:55:35.630514: step 44700, loss = 0.93 (868.8 examples/sec; 0.147 sec/batch)
2017-05-07 20:55:36.807347: step 44710, loss = 0.74 (1087.7 examples/sec; 0.118 sec/batch)
2017-05-07 20:55:38.156041: step 44720, loss = 0.67 (949.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:55:39.506333: step 44730, loss = 0.70 (947.9 examples/sec; 0.135 sec/batch)
2017-05-07 20:55:40.831347: step 44740, loss = 0.85 (966.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:55:42.214976: step 44750, loss = 0.83 (925.1 examples/sec; 0.138 sec/batch)
2017-05-07 20:55:43.504062: step 44760, loss = 0.63 (993.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:55:44.825789: step 44770, loss = 0.69 (968.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:55:46.179634: step 44780, loss = 0.64 (945.5 examples/sec; 0.135 sec/batch)
2017-05-07 20:55:47.502242: step 44790, loss = 0.69 (967.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:55:48.973641: step 44800, loss = 0.64 (869.9 examples/sec; 0.147 sec/batch)
2017-05-07 20:55:50.195013: step 44810, loss = 0.70 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-07 20:55:51.510838: step 44820, loss = 0.66 (972.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:55:52.823107: step 44830, loss = 0.67 (975.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:55:54.171396: step 44840, loss = 0.66 (949.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:55:55.596722: step 44850, loss = 0.70 (898.0 examples/sec; 0.143 sec/batch)
2017-05-07 20:55:56.893574: step 44860, loss = 0.82 (987.0 examples/sec; 0.130 sec/batch)
2017-05-07 20:55:58.240131: step 44870, loss = 0.63 (950.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:55:59.577513: step 44880, loss = 0.85 (957.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:56:00.901880: step 44890, loss = 0.78 (966.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:56:02.371377: step 44900, loss = 0.73 (871.0 examples/sec; 0.147 sec/batch)
2017-05-07 20:56:03.594406: step 44910, loss = 0.79 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-07 20:56:04.931427: step 44920, loss = 0.62 (957.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:56:06.265488: step 44930, loss = 0.66 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:56:07.616627: step 44940, loss = 0.71 (947.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:56:08.990754: step 44950, loss = 0.90 (931.5 examples/sec; 0.137 sec/batch)
2017-05-07 20:56:10.316190: step 44960, loss = 0.80 (965.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:56:11.630124: step 44970, loss = 0.67 (974.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:56:12.966784: step 44980, loss = 0.87 (957.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:56:14.302374: step 44990, loss = 0.84 (958.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:56:15.777633: step 45000, loss = 0.67 (867.6 examples/sec; 0.148 sec/batch)
2017-05-07 20:56:16.966146: step 45010, loss = 0.73 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-07 20:56:18.307059: step 45020, loss = 0.66 (954.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:56:19.634123: step 45030, loss = 0.78 (964.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:56:20.953255: step 45040, loss = 0.78 (970.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:56:22.333943: step 45050, loss = 0.85 (927.1 examples/sec; 0.138 sec/batch)
2017-05-07 20:56:23.633331: step 45060, loss = 0.71 (985.1 examples/sec; 0.130 sec/batch)
2017-05-07 20:56:24.963534: step 45070, loss = 0.86 (962.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:56:26.277210: step 45080, loss = 0.69 (974.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:56:27.623524: step 45090, loss = 0.70 (950.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:56:29.102916: step 45100, loss = 0.69 (865.2 examples/sec; 0.148 sec/batch)
2017-05-07 20:56:30.284165: step 45110, loss = 0.77 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-07 20:56:31.602916: step 45120, loss = 0.60 (970.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:56:32.937255: step 45130, loss = 0.69 (959.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:56:34.289195: step 45140, loss = 0.71 (946.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:56:35.685507: step 45150, loss = 0.69 (916.7 examples/sec; 0.140 sec/batch)
2017-05-07 20:56:36.952276: step 45160, loss = 0.61 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-07 20:56:38.289517: step 45170, loss = 0.92 (957.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:56:39.625241: step 45180, loss = 0.65 (958.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:56:40.962433: step 45190, loss = 0.61 (957.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:56:42.420148: step 45200, loss = 0.73 (878.1 examples/sec; 0.146 sec/batch)
2017-05-07 20:56:43.649366: step 45210, loss = 0.74 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-07 20:56:45.002990: step 45220, loss = 0.69 (945.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:56:46.338145: step 45230, loss = 0.83 (958.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:56:47.696573: step 45240, loss = 0.75 (942.3 examples/sec; 0.136 sec/batch)
2017-05-07 20:56:49.055804: step 45250, loss = 0.60 (941.7 examples/sec; 0.136 sec/batch)
2017-05-07 20:56:50.349226: step 45260, loss = 0.73 (989.6 examples/sec; 0.129 sec/batch)
2017-05-07 20:56:51.707749: step 45270, loss = 0.62 (942.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:56:53.044222: step 45280, loss = 0.72 (957.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:56:54.366813: step 45290, loss = 0.53 (967.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:56:55.841732: step 45300, loss = 0.60 (867.8 examples/sec; 0.147 sec/batch)
2017-05-07 20:56:57.033419: step 45310, loss = 0.85 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-07 20:56:58.366807: step 45320, loss = 0.74 (960.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:56:59.672727: step 45330, loss = 0.78 (980.1 examples/sec; 0.131 sec/batch)
2017-05-07 20:57:01.005934: step 45340, loss = 0.74 (960.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:57:02.395364: step 45350, loss = 0.81 (921.2 examples/sec; 0.139 sec/batch)
2017-05-07 20:57:03.675328: step 45360, loss = 0.79 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:57:05.001573: step 45370, loss = 0.84 (965.1 examples/sec; 0.133 sec/batch)
2017-05-07 20:57:06.338502: step 45380, loss = 0.68 (957.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:57:07.689696: step 45390, loss = 0.66 (947.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:57:09.160385: step 45400, loss = 0.72 (870.3 examples/sec; 0.147 sec/batch)
2017-05-07 20:57:10.365195: step 45410, loss = 0.82 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-07 20:57:11.686391: step 45420, loss = 0.67 (968.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:57:12.999555: step 45430, loss = 0.64 (974.7 examples/sec; 0.131 sec/batch)
2017-05-07 20:57:14.329510: step 45440, loss = 0.72 (962.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:57:15.712020: step 45450, loss = 0.60 (925.9 examples/sec; 0.138 sec/batch)
2017-05-07 20:57:16.983191: step 45460, loss = 0.70 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-07 20:57:18.320626: step 45470, loss = 0.78 (957.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:57:19.666478: step 45480, loss = 0.68 (951.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:57:20.993048: step 45490, loss = 0.77 (964.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:57:22.465864: step 45500, loss = 0.73 (869.1 examples/sec; 0.147 sec/batch)
2017-05-07 20:57:23.676958: step 45510, loss = 0.63 (1056.9 examples/sec; 0.121 sec/batch)
2017-05-07 20:57:25.016938: step 45520, loss = 0.74 (955.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:57:26.352916: step 45530, loss = 0.68 (958.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:57:27.673378: step 45540, loss = 0.75 (969.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:57:29.076549: step 45550, loss = 0.78 (912.2 examples/sec; 0.140 sec/batch)
2017-05-07 20:57:30.365755: step 45560, loss = 0.72 (992.9 examples/sec; 0.129 sec/batch)
2017-05-07 20:57:31.710045: step 45570, loss = 0.82 (952.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:57:33.051256: step 45580, loss = 0.98 (954.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:57:34.411080: step 45590, loss = 0.71 (941.3 examples/sec; 0.136 sec/batch)
2017-05-07 20:57:35.874374: step 45600, loss = 0.68 (874.7 examples/sec; 0.146 sec/batch)
2017-05-07 20:57:37.050745: step 45610, loss = 0.80 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-07 20:57:38.384422: step 45620, loss = 0.67 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 20:57:39.732431: step 45630, loss = 0.78 (949.6 examples/sec; 0.135 sec/batch)
2017-05-07 20:57:41.053632: step 45640, loss = 0.76 (968.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:57:42.442776: step 45650, loss = 0.63 (921.4 examples/sec; 0.139 sec/batch)
2017-05-07 20:57:43.734749: step 45660, loss = 0.64 (990.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:57:45.050728: step 45670, loss = 0.79 (972.7 examples/sec; 0.132 sec/batch)
2017-05-07 20:57:46.380157: step 45680, loss = 0.80 (962.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:57:47.705935: step 45690, loss = 0.83 (965.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:57:49.177222: step 45700, loss = 0.75 (870.0 examples/sec; 0.147 sec/batch)
2017-05-07 20:57:50.371409: step 45710, loss = 0.73 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-07 20:57:51.699270: step 45720, loss = 0.62 (964.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:57:53.016816: step 45730, loss = 0.62 (971.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:57:54.354426: step 45740, loss = 0.70 (956.9 examples/sec; 0.134 sec/batch)
2017-05-07 20:57:55.749819: step 45750, loss = 0.63 (917.3 examples/sec; 0.140 sec/batch)
2017-05-07 20:57:57.030555: step 45760, loss = 0.75 (999.4 examples/sec; 0.128 sec/batch)
2017-05-07 20:57:58.357267: step 45770, loss = 0.73 (964.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:57:59.709270: step 45780, loss = 0.56 (946.7 examples/sec; 0.135 sec/batch)
2017-05-07 20:58:01.043405: step 45790, loss = 0.87 (959.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:58:02.564119: step 45800, loss = 0.71 (841.7 examples/sec; 0.152 sec/batch)
2017-05-07 20:58:03.767972: step 45810, loss = 0.73 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-07 20:58:05.104027: step 45820, loss = 0.75 (958.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:58:06.425862: step 45830, loss = 0.72 (968.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:58:07.786200: step 45840, loss = 0.61 (940.9 examples/sec; 0.136 sec/batch)
2017-05-07 20:58:09.126272: step 45850, loss = 0.85 (955.2 examples/sec; 0.134 sec/batch)
2017-05-07 20:58:10.439908: step 45860, loss = 0.71 (974.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:58:11.775536: step 45870, loss = 1.06 (958.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:58:13.122525: step 45880, loss = 0.72 (950.3 examples/sec; 0.135 sec/batch)
2017-05-07 20:58:14.441245: step 45890, loss = 0.60 (970.6 examples/sec; 0.132 sec/batch)
2017-05-07 20:58:15.936439: step 45900, loss = 0.63 (856.1 examples/sec; 0.150 sec/batch)
2017-05-07 20:58:17.116518: step 45910, loss = 0.71 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-07 20:58:18.460043: step 45920, loss = 0.78 (952.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:58:19.820926: step 45930, loss = 0.66 (940.6 examples/sec; 0.136 sec/batch)
2017-05-07 20:58:21.126853: step 45940, loss = 0.78 (980.2 examples/sec; 0.131 sec/batch)
2017-05-07 20:58:22.512893: step 45950, loss = 0.70 (923.5 examples/sec; 0.139 sec/batch)
2017-05-07 20:58:23.806262: step 45960, loss = 0.61 (989.7 examples/sec; 0.129 sec/batch)
2017-05-07 20:58:25.123382: step 45970, loss = 0.75 (971.8 examples/sec; 0.132 sec/batch)
2017-05-07 20:58:26.454122: step 45980, loss = 0.73 (961.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:58:27.796383: step 45990, loss = 0.74 (953.6 examples/sec; 0.134 sec/batch)
2017-05-07 20:58:29.255085: step 46000, loss = 0.83 (877.5 examples/sec; 0.146 sec/batch)
2017-05-07 20:58:30.496756: step 46010, loss = 0.83 (1030.9 examples/sec; 0.124 sec/batch)
2017-05-07 20:58:31.835195: step 46020, loss = 0.75 (956.3 examples/sec; 0.134 sec/batch)
2017-05-07 20:58:33.165799: step 46030, loss = 0.67 (962.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:58:34.530841: step 46040, loss = 0.71 (937.7 examples/sec; 0.137 sec/batch)
2017-05-07 20:58:35.919107: step 46050, loss = 0.72 (922.0 examples/sec; 0.139 sec/batch)
2017-05-07 20:58:37.196554: step 46060, loss = 0.73 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-07 20:58:38.548544: step 46070, loss = 0.81 (946.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:58:39.876379: step 46080, loss = 0.64 (964.0 examples/sec; 0.133 sec/batch)
2017-05-07 20:58:41.207956: step 46090, loss = 0.89 (961.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:58:42.682747: step 46100, loss = 0.73 (867.9 examples/sec; 0.147 sec/batch)
2017-05-07 20:58:43.897340: step 46110, loss = 0.68 (1053.9 examples/sec; 0.121 sec/batch)
2017-05-07 20:58:45.221830: step 46120, loss = 0.62 (966.4 examples/sec; 0.132 sec/batch)
2017-05-07 20:58:46.578840: step 46130, loss = 0.88 (943.2 examples/sec; 0.136 sec/batch)
2017-05-07 20:58:47.914344: step 46140, loss = 0.67 (958.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:58:49.294670: step 46150, loss = 0.81 (927.3 examples/sec; 0.138 sec/batch)
2017-05-07 20:58:50.601266: step 46160, loss = 0.70 (979.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:58:51.922921: step 46170, loss = 0.70 (968.5 examples/sec; 0.132 sec/batch)
2017-05-07 20:58:53.238340: step 46180, loss = 0.68 (973.1 examples/sec; 0.132 sec/batch)
2017-05-07 20:58:54.580523: step 46190, loss = 0.78 (953.7 examples/sec; 0.134 sec/batch)
2017-05-07 20:58:56.037686: step 46200, loss = 0.66 (878.4 examples/sec; 0.146 sec/batch)
2017-05-07 20:58:57.233177: step 46210, loss = 0.75 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-07 20:58:58.542717: step 46220, loss = 0.81 (977.4 examples/sec; 0.131 sec/batch)
2017-05-07 20:58:59.871200: step 46230, loss = 0.84 (963.5 examples/sec; 0.133 sec/batch)
2017-05-07 20:59:01.194671: step 46240, loss = 0.66 (967.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:59:02.574324: step 46250, loss = 0.84 (927.8 examples/sec; 0.138 sec/batch)
2017-05-07 20:59:03.861608: step 46260, loss = 0.73 (994.3 examples/sec; 0.129 sec/batch)
2017-05-07 20:59:05.205363: step 46270, loss = 0.77 (952.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:59:06.542851: step 46280, loss = 0.76 (957.0 examples/sec; 0.134 sec/batch)
2017-05-07 20:59:07.890028: step 46290, loss = 0.64 (950.1 examples/sec; 0.135 sec/batch)
2017-05-07 20:59:09.360615: step 46300, loss = 0.76 (870.4 examples/sec; 0.147 sec/batch)
2017-05-07 20:59:10.564913: step 46310, loss = 0.60 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-07 20:59:11.891662: step 46320, loss = 0.86 (964.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:59:13.226627: step 46330, loss = 0.67 (958.8 examples/sec; 0.133 sec/batch)
2017-05-07 20:59:14.608108: step 46340, loss = 0.87 (926.5 examples/sec; 0.138 sec/batch)
2017-05-07 20:59:15.972608: step 46350, loss = 0.60 (938.1 examples/sec; 0.136 sec/batch)
2017-05-07 20:59:17.260356: step 46360, loss = 0.83 (994.0 examples/sec; 0.129 sec/batch)
2017-05-07 20:59:18.594260: step 46370, loss = 0.71 (959.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:59:19.926994: step 46380, loss = 0.74 (960.4 examples/sec; 0.133 sec/batch)
2017-05-07 20:59:21.256329: step 46390, loss = 0.87 (962.9 examples/sec; 0.133 sec/batch)
2017-05-07 20:59:22.745740: step 46400, loss = 0.81 (859.4 examples/sec; 0.149 sec/batch)
2017-05-07 20:59:23.933810: step 46410, loss = 0.61 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-07 20:59:25.258551: step 46420, loss = 0.69 (966.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:59:26.585584: step 46430, loss = 0.70 (964.6 examples/sec; 0.133 sec/batch)
2017-05-07 20:59:27.943639: step 46440, loss = 0.81 (942.5 examples/sec; 0.136 sec/batch)
2017-05-07 20:59:29.312253: step 46450, loss = 0.69 (935.3 examples/sec; 0.137 sec/batch)
2017-05-07 20:59:30.599378: step 46460, loss = 0.59 (994.5 examples/sec; 0.129 sec/batch)
2017-05-07 20:59:31.940992: step 46470, loss = 0.77 (954.1 examples/sec; 0.134 sec/batch)
2017-05-07 20:59:33.296087: step 46480, loss = 0.60 (944.6 examples/sec; 0.136 sec/batch)
2017-05-07 20:59:34.649378: step 46490, loss = 0.83 (945.8 examples/sec; 0.135 sec/batch)
2017-05-07 20:59:36.104650: step 46500, loss = 0.69 (879.6 examples/sec; 0.146 sec/batch)
2017-05-07 20:59:37.295151: step 46510, loss = 0.63 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-07 20:59:38.617210: step 46520, loss = 0.66 (968.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:59:39.955550: step 46530, loss = 0.73 (956.4 examples/sec; 0.134 sec/batch)
2017-05-07 20:59:41.274844: step 46540, loss = 0.71 (970.2 examples/sec; 0.132 sec/batch)
2017-05-07 20:59:42.680326: step 46550, loss = 0.81 (910.7 examples/sec; 0.141 sec/batch)
2017-05-07 20:59:43.959318: step 46560, loss = 0.80 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-07 20:59:45.296186: step 46570, loss = 0.69 (957.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:59:46.609518: step 46580, loss = 0.79 (974.6 examples/sec; 0.131 sec/batch)
2017-05-07 20:59:47.960890: step 46590, loss = 0.66 (947.2 examples/sec; 0.135 sec/batch)
2017-05-07 20:59:49.414947: step 46600, loss = 0.71 (880.3 examples/sec; 0.145 sec/batch)
2017-05-07 20:59:50.631162: step 46610, loss = 0.64 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-07 20:59:51.959915: step 46620, loss = 0.70 (963.3 examples/sec; 0.133 sec/batch)
2017-05-07 20:59:53.283109: step 46630, loss = 0.70 (967.3 examples/sec; 0.132 sec/batch)
2017-05-07 20:59:54.592161: step 46640, loss = 0.78 (977.8 examples/sec; 0.131 sec/batch)
2017-05-07 20:59:55.993641: step 46650, loss = 0.76 (913.3 examples/sec; 0.140 sec/batch)
2017-05-07 20:59:57.295420: step 46660, loss = 0.84 (983.3 examples/sec; 0.130 sec/batch)
2017-05-07 20:59:58.632190: step 46670, loss = 0.66 (957.5 examples/sec; 0.134 sec/batch)
2017-05-07 20:59:59.952321: step 46680, loss = 0.73 (969.6 examples/sec; 0.132 sec/batch)
2017-05-07 21:00:01.275257: step 46690, loss = 0.69 (967.5 examples/sec; 0.132 sec/batch)
2017-05-07 21:00:02.734143: step 46700, loss = 0.61 (877.4 examples/sec; 0.146 sec/batch)
2017-05-07 21:00:03.948515: step 46710, loss = 0.74 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-07 21:00:05.260708: step 46720, loss = 0.79 (975.5 examples/sec; 0.131 sec/batch)
2017-05-07 21:00:06.602558: step 46730, loss = 0.82 (953.9 examples/sec; 0.134 sec/batch)
2017-05-07 21:00:07.948771: step 46740, loss = 0.76 (950.8 examples/sec; 0.135 sec/batch)
2017-05-07 21:00:09.325284: step 46750, loss = 0.83 (929.9 examples/sec; 0.138 sec/batch)
2017-05-07 21:00:10.613937: step 46760, loss = 0.61 (993.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:00:11.951329: step 46770, loss = 0.82 (957.1 examples/sec; 0.134 sec/batch)
2017-05-07 21:00:13.277280: step 46780, loss = 0.75 (965.3 examples/sec; 0.133 sec/batch)
2017-05-07 21:00:14.603871: step 46790, loss = 0.73 (964.9 examples/sec; 0.133 sec/batch)
2017-05-07 21:00:16.086573: step 46800, loss = 0.74 (863.3 examples/sec; 0.148 sec/batch)
2017-05-07 21:00:17.281641: step 46810, loss = 0.69 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-07 21:00:18.622269: step 46820, loss = 0.80 (954.8 examples/sec; 0.134 sec/batch)
2017-05-07 21:00:19.947836: step 46830, loss = 0.80 (965.6 examples/sec; 0.133 sec/batch)
2017-05-07 21:00:21.312748: step 46840, loss = 0.73 (937.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:00:22.715333: step 46850, loss = 0.64 (912.6 examples/sec; 0.140 sec/batch)
2017-05-07 21:00:24.003657: step 46860, loss = 0.72 (993.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:00:25.338264: step 46870, loss = 0.65 (959.1 examples/sec; 0.133 sec/batch)
2017-05-07 21:00:26.657961: step 46880, loss = 0.79 (969.9 examples/sec; 0.132 sec/batch)
2017-05-07 21:00:27.994857: step 46890, loss = 0.91 (957.4 examples/sec; 0.134 sec/batch)
2017-05-07 21:00:29.459223: step 46900, loss = 0.78 (874.1 examples/sec; 0.146 sec/batch)
2017-05-07 21:00:30.674640: step 46910, loss = 0.91 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-07 21:00:32.037058: step 46920, loss = 0.63 (939.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:00:33.374125: step 46930, loss = 0.71 (957.3 examples/sec; 0.134 sec/batch)
2017-05-07 21:00:34.696044: step 46940, loss = 0.74 (968.3 examples/sec; 0.132 sec/batch)
2017-05-07 21:00:36.066931: step 46950, loss = 0.79 (933.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:00:37.357719: step 46960, loss = 0.80 (991.6 examples/sec; 0.129 sec/batch)
2017-05-07 21:00:38.684145: step 46970, loss = 0.77 (965.0 examples/sec; 0.133 sec/batch)
2017-05-07 21:00:40.018539: step 46980, loss = 0.63 (959.2 examples/sec; 0.133 sec/batch)
2017-05-07 21:00:41.366365: step 46990, loss = 0.78 (949.7 examples/sec; 0.135 sec/batch)
2017-05-07 21:00:42.824065: step 47000, loss = 0.77 (878.1 examples/sec; 0.146 sec/batch)
2017-05-07 21:00:44.013413: step 47010, loss = 0.61 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-07 21:00:45.309492: step 47020, loss = 0.79 (987.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:00:46.630370: step 47030, loss = 0.78 (969.0 examples/sec; 0.132 sec/batch)
2017-05-07 21:00:47.949368: step 47040, loss = 0.72 (970.4 examples/sec; 0.132 sec/batch)
2017-05-07 21:00:49.356307: step 47050, loss = 0.74 (909.8 examples/sec; 0.141 sec/batch)
2017-05-07 21:00:50.625820: step 47060, loss = 0.64 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-07 21:00:51.962165: step 47070, loss = 0.64 (957.8 examples/sec; 0.134 sec/batch)
2017-05-07 21:00:53.310493: step 47080, loss = 0.72 (949.3 examples/sec; 0.135 sec/batch)
2017-05-07 21:00:54.659879: step 47090, loss = 0.63 (948.6 examples/sec; 0.135 sec/batch)
2017-05-07 21:00:56.132809: step 47100, loss = 0.71 (869.0 examples/sec; 0.147 sec/batch)
2017-05-07 21:00:57.319749: step 47110, loss = 0.69 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-07 21:00:58.660268: step 47120, loss = 0.80 (954.9 examples/sec; 0.134 sec/batch)
2017-05-07 21:01:00.016879: step 47130, loss = 0.77 (943.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:01:01.305248: step 47140, loss = 0.67 (993.5 examples/sec; 0.129 sec/batch)
2017-05-07 21:01:02.717064: step 47150, loss = 0.70 (906.6 examples/sec; 0.141 sec/batch)
2017-05-07 21:01:03.999276: step 47160, loss = 0.69 (998.3 examples/sec; 0.128 sec/batch)
2017-05-07 21:01:05.352600: step 47170, loss = 0.77 (945.8 examples/sec; 0.135 sec/batch)
2017-05-07 21:01:06.719886: step 47180, loss = 0.78 (936.2 examples/sec; 0.137 sec/batch)
2017-05-07 21:01:08.044389: step 47190, loss = 0.61 (966.4 examples/sec; 0.132 sec/batch)
2017-05-07 21:01:09.506016: step 47200, loss = 0.65 (875.7 examples/sec; 0.146 sec/batch)
2017-05-07 21:01:10.722717: step 47210, loss = 0.76 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-07 21:01:12.039643: step 47220, loss = 0.74 (972.0 examples/sec; 0.132 sec/batch)
2017-05-07 21:01:13.375046: step 47230, loss = 0.71 (958.5 examples/sec; 0.134 sec/batch)
2017-05-07 21:01:14.691992: step 47240, loss = 0.85 (971.9 examples/sec; 0.132 sec/batch)
2017-05-07 21:01:16.063115: step 47250, loss = 0.72 (933.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:01:17.367158: step 47260, loss = 0.76 (981.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:01:18.710511: step 47270, loss = 0.63 (952.8 examples/sec; 0.134 sec/batch)
2017-05-07 21:01:20.066244: step 47280, loss = 0.70 (944.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:01:21.388036: step 47290, loss = 0.74 (968.4 examples/sec; 0.132 sec/batch)
2017-05-07 21:01:22.888398: step 47300, loss = 0.76 (853.1 examples/sec; 0.150 sec/batch)
2017-05-07 21:01:24.069911: step 47310, loss = 0.63 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-07 21:01:25.378781: step 47320, loss = 0.71 (977.9 examples/sec; 0.131 sec/batch)
2017-05-07 21:01:26.711653: step 47330, loss = 0.85 (960.3 examples/sec; 0.133 sec/batch)
2017-05-07 21:01:28.056439: step 47340, loss = 0.77 (951.8 examples/sec; 0.134 sec/batch)
2017-05-07 21:01:29.427583: step 47350, loss = 0.67 (933.5 examples/sec; 0.137 sec/batch)
2017-05-07 21:01:30.717886: step 47360, loss = 0.75 (992.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:01:32.057243: step 47370, loss = 0.76 (955.7 examples/sec; 0.134 sec/batch)
2017-05-07 21:01:33.389439: step 47380, loss = 0.64 (960.8 examples/sec; 0.133 sec/batch)
2017-05-07 21:01:34.723596: step 47390, loss = 0.80 (959.4 examples/sec; 0.133 sec/batch)
2017-05-07 21:01:36.195708: step 47400, loss = 0.63 (869.5 examples/sec; 0.147 sec/batch)
2017-05-07 21:01:37.378680: step 47410, loss = 0.61 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-07 21:01:38.724127: step 47420, loss = 0.68 (951.4 examples/sec; 0.135 sec/batch)
2017-05-07 21:01:40.061310: step 47430, loss = 0.66 (957.2 examples/sec; 0.134 sec/batch)
2017-05-07 21:01:41.395663: step 47440, loss = 0.90 (959.3 examples/sec; 0.133 sec/batch)
2017-05-07 21:01:42.771076: step 47450, loss = 0.85 (930.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:01:44.060941: step 47460, loss = 0.67 (992.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:01:45.378516: step 47470, loss = 0.74 (971.5 examples/sec; 0.132 sec/batch)
2017-05-07 21:01:46.715142: step 47480, loss = 0.97 (957.6 examples/sec; 0.134 sec/batch)
2017-05-07 21:01:48.056522: step 47490, loss = 0.72 (954.2 examples/sec; 0.134 sec/batch)
2017-05-07 21:01:49.524539: step 47500, loss = 0.76 (871.9 examples/sec; 0.147 sec/batch)
2017-05-07 21:01:50.744907: step 47510, loss = 0.64 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-07 21:01:52.080744: step 47520, loss = 0.88 (958.2 examples/sec; 0.134 sec/batch)
2017-05-07 21:01:53.388889: step 47530, loss = 0.63 (978.5 examples/sec; 0.131 sec/batch)
2017-05-07 21:01:54.736761: step 47540, loss = 0.85 (949.6 examples/sec; 0.135 sec/batch)
2017-05-07 21:01:56.136647: step 47550, loss = 0.59 (914.4 examples/sec; 0.140 sec/batch)
2017-05-07 21:01:57.420900: step 47560, loss = 0.67 (996.7 examples/sec; 0.128 sec/batch)
2017-05-07 21:01:58.769426: step 47570, loss = 0.83 (949.2 examples/sec; 0.135 sec/batch)
2017-05-07 21:02:00.105257: step 47580, loss = 0.69 (958.2 examples/sec; 0.134 sec/batch)
2017-05-07 21:02:01.443232: step 47590, loss = 0.62 (956.7 examples/sec; 0.134 sec/batch)
2017-05-07 21:02:02.901366: step 47600, loss = 0.65 (877.8 examples/sec; 0.146 sec/batch)
2017-05-07 21:02:04.097058: step 47610, loss = 0.68 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-07 21:02:05.433357: step 47620, loss = 0.68 (957.9 examples/sec; 0.134 sec/batch)
2017-05-07 21:02:06.768075: step 47630, loss = 0.60 (959.0 examples/sec; 0.133 sec/batch)
2017-05-07 21:02:08.061973: step 47640, loss = 0.81 (989.3 examples/sec; 0.129 sec/batch)
2017-05-07 21:02:09.424009: step 47650, loss = 0.56 (939.8 examples/sec; 0.136 sec/batch)
2017-05-07 21:02:10.737657: step 47660, loss = 0.71 (974.4 examples/sec; 0.131 sec/batch)
2017-05-07 21:02:12.059432: step 47670, loss = 0.69 (968.4 examples/sec; 0.132 sec/batch)
2017-05-07 21:02:13.374557: step 47680, loss = 0.69 (973.3 examples/sec; 0.132 sec/batch)
2017-05-07 21:02:14.703395: step 47690, loss = 0.84 (963.2 examples/sec; 0.133 sec/batch)
2017-05-07 21:02:16.197260: step 47700, loss = 0.61 (856.8 examples/sec; 0.149 sec/batch)
2017-05-07 21:02:17.412810: step 47710, loss = 0.76 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-07 21:02:18.758540: step 47720, loss = 0.72 (951.2 examples/sec; 0.135 sec/batch)
2017-05-07 21:02:20.094078: step 47730, loss = 0.59 (958.4 examples/sec; 0.134 sec/batch)
2017-05-07 21:02:21.443351: step 47740, loss = 0.64 (948.7 examples/sec; 0.135 sec/batch)
2017-05-07 21:02:22.811280: step 47750, loss = 0.76 (935.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:02:24.110192: step 47760, loss = 0.62 (985.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:02:25.417500: step 47770, loss = 0.66 (979.1 examples/sec; 0.131 sec/batch)
2017-05-07 21:02:26.772178: step 47780, loss = 0.91 (944.9 examples/sec; 0.135 sec/batch)
2017-05-07 21:02:28.099990: step 47790, loss = 0.68 (964.0 examples/sec; 0.133 sec/batch)
2017-05-07 21:02:29.569602: step 47800, loss = 0.63 (871.0 examples/sec; 0.147 sec/batch)
2017-05-07 21:02:30.766244: step 47810, loss = 0.66 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-07 21:02:32.102977: step 47820, loss = 0.63 (957.6 examples/sec; 0.134 sec/batch)
2017-05-07 21:02:33.429053: step 47830, loss = 0.61 (965.2 examples/sec; 0.133 sec/batch)
2017-05-07 21:02:34.773953: step 47840, loss = 0.84 (951.7 examples/sec; 0.134 sec/batch)
2017-05-07 21:02:36.139395: step 47850, loss = 0.73 (937.4 examples/sec; 0.137 sec/batch)
2017-05-07 21:02:37.446806: step 47860, loss = 0.63 (979.0 examples/sec; 0.131 sec/batch)
2017-05-07 21:02:38.774605: step 47870, loss = 0.79 (964.0 examples/sec; 0.133 sec/batch)
2017-05-07 21:02:40.099626: step 47880, loss = 0.87 (966.0 examples/sec; 0.133 sec/batch)
2017-05-07 21:02:41.426932: step 47890, loss = 0.68 (964.4 examples/sec; 0.133 sec/batch)
2017-05-07 21:02:42.894501: step 47900, loss = 0.68 (872.2 examples/sec; 0.147 sec/batch)
2017-05-07 21:02:44.085728: step 47910, loss = 0.67 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-07 21:02:45.415398: step 47920, loss = 0.86 (962.7 examples/sec; 0.133 sec/batch)
2017-05-07 21:02:46.772311: step 47930, loss = 0.73 (943.3 examples/sec; 0.136 sec/batch)
2017-05-07 21:02:48.100698: step 47940, loss = 0.80 (963.6 examples/sec; 0.133 sec/batch)
2017-05-07 21:02:49.483834: step 47950, loss = 0.73 (925.4 examples/sec; 0.138 sec/batch)
2017-05-07 21:02:50.781790: step 47960, loss = 0.79 (986.2 examples/sec; 0.130 sec/batch)
2017-05-07 21:02:52.138606: step 47970, loss = 0.68 (943.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:02:53.454291: step 47980, loss = 0.71 (972.9 examples/sec; 0.132 sec/batch)
2017-05-07 21:02:54.776617: step 47990, loss = 0.84 (968.0 examples/sec; 0.132 sec/batch)
2017-05-07 21:02:56.260925: step 48000, loss = 0.74 (862.4 examples/sec; 0.148 sec/batch)
2017-05-07 21:02:57.455105: step 48010, loss = 0.78 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-07 21:02:58.795197: step 48020, loss = 0.57 (955.1 examples/sec; 0.134 sec/batch)
2017-05-07 21:03:00.130356: step 48030, loss = 0.77 (958.7 examples/sec; 0.134 sec/batch)
2017-05-07 21:03:01.470949: step 48040, loss = 0.79 (954.8 examples/sec; 0.134 sec/batch)
2017-05-07 21:03:02.841559: step 48050, loss = 0.68 (933.9 examples/sec; 0.137 sec/batch)
2017-05-07 21:03:04.143170: step 48060, loss = 0.83 (983.4 examples/sec; 0.130 sec/batch)
2017-05-07 21:03:05.474159: step 48070, loss = 0.63 (961.7 examples/sec; 0.133 sec/batch)
2017-05-07 21:03:06.790691: step 48080, loss = 0.86 (972.3 examples/sec; 0.132 sec/batch)
2017-05-07 21:03:08.125540: step 48090, loss = 0.81 (958.9 examples/sec; 0.133 sec/batch)
2017-05-07 21:03:09.599578: step 48100, loss = 0.67 (868.4 examples/sec; 0.147 sec/batch)
2017-05-07 21:03:10.790982: step 48110, loss = 0.72 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-07 21:03:12.124804: step 48120, loss = 0.64 (959.6 examples/sec; 0.133 sec/batch)
2017-05-07 21:03:13.444203: step 48130, loss = 0.77 (970.1 examples/sec; 0.132 sec/batch)
2017-05-07 21:03:14.765201: step 48140, loss = 0.89 (969.0 examples/sec; 0.132 sec/batch)
2017-05-07 21:03:16.133787: step 48150, loss = 0.73 (935.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:03:17.410235: step 48160, loss = 0.79 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:03:18.744314: step 48170, loss = 0.95 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 21:03:20.081839: step 48180, loss = 0.63 (957.0 examples/sec; 0.134 sec/batch)
2017-05-07 21:03:21.397785: step 48190, loss = 0.50 (972.7 examples/sec; 0.132 sec/batch)
2017-05-07 21:03:22.891539: step 48200, loss = 0.77 (856.9 examples/sec; 0.149 sec/batch)
2017-05-07 21:03:24.086540: step 48210, loss = 0.63 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-07 21:03:25.414763: step 48220, loss = 0.71 (963.7 examples/sec; 0.133 sec/batch)
2017-05-07 21:03:26.752608: step 48230, loss = 0.76 (956.8 examples/sec; 0.134 sec/batch)
2017-05-07 21:03:28.089736: step 48240, loss = 0.85 (957.3 examples/sec; 0.134 sec/batch)
2017-05-07 21:03:29.486097: step 48250, loss = 0.60 (916.7 examples/sec; 0.140 sec/batch)
2017-05-07 21:03:30.759790: step 48260, loss = 0.86 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-07 21:03:32.102228: step 48270, loss = 0.74 (953.5 examples/sec; 0.134 sec/batch)
2017-05-07 21:03:33.407745: step 48280, loss = 0.70 (980.5 examples/sec; 0.131 sec/batch)
2017-05-07 21:03:34.757643: step 48290, loss = 0.66 (948.2 examples/sec; 0.135 sec/batch)
2017-05-07 21:03:36.217843: step 48300, loss = 0.72 (876.6 examples/sec; 0.146 sec/batch)
2017-05-07 21:03:37.421406: step 48310, loss = 0.82 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-07 21:03:38.749014: step 48320, loss = 0.83 (964.1 examples/sec; 0.133 sec/batch)
2017-05-07 21:03:40.088753: step 48330, loss = 0.63 (955.4 examples/sec; 0.134 sec/batch)
2017-05-07 21:03:41.413214: step 48340, loss = 0.82 (966.4 examples/sec; 0.132 sec/batch)
2017-05-07 21:03:42.781795: step 48350, loss = 0.76 (935.3 examples/sec; 0.137 sec/batch)
2017-05-07 21:03:44.102023: step 48360, loss = 0.60 (969.5 examples/sec; 0.132 sec/batch)
2017-05-07 21:03:45.424703: step 48370, loss = 0.62 (967.7 examples/sec; 0.132 sec/batch)
2017-05-07 21:03:46.737944: step 48380, loss = 0.76 (974.7 examples/sec; 0.131 sec/batch)
2017-05-07 21:03:48.063380: step 48390, loss = 0.67 (965.7 examples/sec; 0.133 sec/batch)
2017-05-07 21:03:49.535711: step 48400, loss = 0.68 (869.4 examples/sec; 0.147 sec/batch)
2017-05-07 21:03:50.738621: step 48410, loss = 0.65 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-07 21:03:52.050626: step 48420, loss = 0.69 (975.6 examples/sec; 0.131 sec/batch)
2017-05-07 21:03:53.369212: step 48430, loss = 0.76 (970.7 examples/sec; 0.132 sec/batch)
2017-05-07 21:03:54.707349: step 48440, loss = 0.68 (956.6 examples/sec; 0.134 sec/batch)
2017-05-07 21:03:56.068837: step 48450, loss = 0.68 (940.1 examples/sec; 0.136 sec/batch)
2017-05-07 21:03:57.337716: step 48460, loss = 0.65 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-07 21:03:58.676878: step 48470, loss = 0.59 (955.8 examples/sec; 0.134 sec/batch)
2017-05-07 21:04:00.030840: step 48480, loss = 0.64 (945.4 examples/sec; 0.135 sec/batch)
2017-05-07 21:04:01.328705: step 48490, loss = 0.75 (986.2 examples/sec; 0.130 sec/batch)
2017-05-07 21:04:02.811114: step 48500, loss = 0.66 (863.5 examples/sec; 0.148 sec/batch)
2017-05-07 21:04:04.030608: step 48510, loss = 0.66 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-07 21:04:05.361768: step 48520, loss = 0.73 (961.6 examples/sec; 0.133 sec/batch)
2017-05-07 21:04:06.696288: step 48530, loss = 0.84 (959.1 examples/sec; 0.133 sec/batch)
2017-05-07 21:04:08.042708: step 48540, loss = 0.68 (950.7 examples/sec; 0.135 sec/batch)
2017-05-07 21:04:09.428830: step 48550, loss = 0.97 (923.4 examples/sec; 0.139 sec/batch)
2017-05-07 21:04:10.712501: step 48560, loss = 0.82 (997.1 examples/sec; 0.128 sec/batch)
2017-05-07 21:04:12.055849: step 48570, loss = 0.71 (952.8 examples/sec; 0.134 sec/batch)
2017-05-07 21:04:13.383544: step 48580, loss = 0.79 (964.1 examples/sec; 0.133 sec/batch)
2017-05-07 21:04:14.703576: step 48590, loss = 0.79 (969.7 examples/sec; 0.132 sec/batch)
2017-05-07 21:04:16.180622: step 48600, loss = 0.78 (866.6 examples/sec; 0.148 sec/batch)
2017-05-07 21:04:17.366678: step 48610, loss = 0.58 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-07 21:04:18.727852: step 48620, loss = 0.64 (940.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:04:20.040989: step 48630, loss = 0.70 (974.8 examples/sec; 0.131 sec/batch)
2017-05-07 21:04:21.366709: step 48640, loss = 0.74 (965.5 examples/sec; 0.133 sec/batch)
2017-05-07 21:04:22.773352: step 48650, loss = 0.64 (910.0 examples/sec; 0.141 sec/batch)
2017-05-07 21:04:24.065053: step 48660, loss = 0.65 (990.9 examples/sec; 0.129 sec/batch)
2017-05-07 21:04:25.398701: step 48670, loss = 0.64 (959.8 examples/sec; 0.133 sec/batch)
2017-05-07 21:04:26.755445: step 48680, loss = 0.65 (943.4 examples/sec; 0.136 sec/batch)
2017-05-07 21:04:28.096949: step 48690, loss = 0.80 (954.1 examples/sec; 0.134 sec/batch)
2017-05-07 21:04:29.563308: step 48700, loss = 0.65 (872.9 examples/sec; 0.147 sec/batch)
2017-05-07 21:04:30.748686: step 48710, loss = 0.62 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-07 21:04:32.102765: step 48720, loss = 0.70 (945.3 examples/sec; 0.135 sec/batch)
2017-05-07 21:04:33.450100: step 48730, loss = 0.77 (950.0 examples/sec; 0.135 sec/batch)
2017-05-07 21:04:34.799200: step 48740, loss = 0.77 (948.8 examples/sec; 0.135 sec/batch)
2017-05-07 21:04:36.187352: step 48750, loss = 0.76 (922.1 examples/sec; 0.139 sec/batch)
2017-05-07 21:04:37.472443: step 48760, loss = 0.83 (996.0 examples/sec; 0.129 sec/batch)
2017-05-07 21:04:38.834895: step 48770, loss = 0.70 (939.5 examples/sec; 0.136 sec/batch)
2017-05-07 21:04:40.167042: step 48780, loss = 0.88 (960.9 examples/sec; 0.133 sec/batch)
2017-05-07 21:04:41.475046: step 48790, loss = 0.67 (978.6 examples/sec; 0.131 sec/batch)
2017-05-07 21:04:42.933043: step 48800, loss = 0.62 (877.9 examples/sec; 0.146 sec/batch)
2017-05-07 21:04:44.136702: step 48810, loss = 0.64 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-07 21:04:45.461752: step 48820, loss = 0.70 (966.0 examples/sec; 0.133 sec/batch)
2017-05-07 21:04:46.794994: step 48830, loss = 0.75 (960.1 examples/sec; 0.133 sec/batch)
2017-05-07 21:04:48.121047: step 48840, loss = 0.54 (965.3 examples/sec; 0.133 sec/batch)
2017-05-07 21:04:49.502472: step 48850, loss = 0.67 (926.6 examples/sec; 0.138 sec/batch)
2017-05-07 21:04:50.781154: step 48860, loss = 0.65 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-07 21:04:52.132429: step 48870, loss = 0.88 (947.3 examples/sec; 0.135 sec/batch)
2017-05-07 21:04:53.470648: step 48880, loss = 0.70 (956.5 examples/sec; 0.134 sec/batch)
2017-05-07 21:04:54.802070: step 48890, loss = 0.74 (961.4 examples/sec; 0.133 sec/batch)
2017-05-07 21:04:56.275989: step 48900, loss = 0.76 (868.4 examples/sec; 0.147 sec/batch)
2017-05-07 21:04:57.465246: step 48910, loss = 0.77 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-07 21:04:58.817923: step 48920, loss = 0.78 (946.3 examples/sec; 0.135 sec/batch)
2017-05-07 21:05:00.151661: step 48930, loss = 0.86 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 21:05:01.509230: step 48940, loss = 0.65 (942.9 examples/sec; 0.136 sec/batch)
2017-05-07 21:05:02.905845: step 48950, loss = 0.69 (916.5 examples/sec; 0.140 sec/batch)
2017-05-07 21:05:04.195591: step 48960, loss = 0.72 (992.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:05:05.507444: step 48970, loss = 0.87 (975.7 examples/sec; 0.131 sec/batch)
2017-05-07 21:05:06.852168: step 48980, loss = 0.74 (951.9 examples/sec; 0.134 sec/batch)
2017-05-07 21:05:08.170729: step 48990, loss = 0.72 (970.8 examples/sec; 0.132 sec/batch)
2017-05-07 21:05:09.624403: step 49000, loss = 0.59 (880.5 examples/sec; 0.145 sec/batch)
2017-05-07 21:05:10.824688: step 49010, loss = 0.69 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-07 21:05:12.151952: step 49020, loss = 0.74 (964.4 examples/sec; 0.133 sec/batch)
2017-05-07 21:05:13.476161: step 49030, loss = 0.68 (966.6 examples/sec; 0.132 sec/batch)
2017-05-07 21:05:14.803752: step 49040, loss = 0.80 (964.2 examples/sec; 0.133 sec/batch)
2017-05-07 21:05:16.183062: step 49050, loss = 0.66 (928.0 examples/sec; 0.138 sec/batch)
2017-05-07 21:05:17.454619: step 49060, loss = 0.77 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-07 21:05:18.794637: step 49070, loss = 0.83 (955.2 examples/sec; 0.134 sec/batch)
2017-05-07 21:05:20.139707: step 49080, loss = 0.79 (951.6 examples/sec; 0.135 sec/batch)
2017-05-07 21:05:21.461243: step 49090, loss = 0.72 (968.6 examples/sec; 0.132 sec/batch)
2017-05-07 21:05:22.954719: step 49100, loss = 0.70 (857.1 examples/sec; 0.149 sec/batch)
2017-05-07 21:05:24.150529: step 49110, loss = 0.68 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-07 21:05:25.448796: step 49120, loss = 0.65 (985.9 examples/sec; 0.130 sec/batch)
2017-05-07 21:05:26.787983: step 49130, loss = 0.68 (955.8 examples/sec; 0.134 sec/batch)
2017-05-07 21:05:28.120645: step 49140, loss = 0.67 (960.5 examples/sec; 0.133 sec/batch)
2017-05-07 21:05:29.531822: step 49150, loss = 0.70 (907.0 examples/sec; 0.141 sec/batch)
2017-05-07 21:05:30.814592: step 49160, loss = 0.78 (997.8 examples/sec; 0.128 sec/batch)
2017-05-07 21:05:32.142697: step 49170, loss = 0.72 (963.8 examples/sec; 0.133 sec/batch)
2017-05-07 21:05:33.440789: step 49180, loss = 0.99 (986.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:05:34.788023: step 49190, loss = 0.63 (950.1 examples/sec; 0.135 sec/batch)
2017-05-07 21:05:36.308339: step 49200, loss = 0.71 (841.9 examples/sec; 0.152 sec/batch)
2017-05-07 21:05:37.490376: step 49210, loss = 0.87 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-07 21:05:38.808912: step 49220, loss = 0.52 (970.8 examples/sec; 0.132 sec/batch)
2017-05-07 21:05:40.143023: step 49230, loss = 0.63 (959.5 examples/sec; 0.133 sec/batch)
2017-05-07 21:05:41.474044: step 49240, loss = 0.82 (961.7 examples/sec; 0.133 sec/batch)
2017-05-07 21:05:42.868902: step 49250, loss = 0.59 (917.7 examples/sec; 0.139 sec/batch)
2017-05-07 21:05:44.169541: step 49260, loss = 0.75 (984.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:05:45.490073: step 49270, loss = 0.73 (969.3 examples/sec; 0.132 sec/batch)
2017-05-07 21:05:46.820887: step 49280, loss = 0.70 (961.8 examples/sec; 0.133 sec/batch)
2017-05-07 21:05:48.173770: step 49290, loss = 0.84 (946.1 examples/sec; 0.135 sec/batch)
2017-05-07 21:05:49.634382: step 49300, loss = 0.76 (876.3 examples/sec; 0.146 sec/batch)
2017-05-07 21:05:50.812966: step 49310, loss = 0.70 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-07 21:05:52.160675: step 49320, loss = 0.77 (949.8 examples/sec; 0.135 sec/batch)
2017-05-07 21:05:53.479384: step 49330, loss = 0.71 (970.7 examples/sec; 0.132 sec/batch)
2017-05-07 21:05:54.883070: step 49340, loss = 0.71 (911.9 examples/sec; 0.140 sec/batch)
2017-05-07 21:05:56.253896: step 49350, loss = 0.74 (933.7 examples/sec; 0.137 sec/batch)
2017-05-07 21:05:57.509019: step 49360, loss = 0.71 (1019.8 examples/sec; 0.126 sec/batch)
2017-05-07 21:05:58.844256: step 49370, loss = 0.70 (958.6 examples/sec; 0.134 sec/batch)
2017-05-07 21:06:00.190600: step 49380, loss = 0.74 (950.7 examples/sec; 0.135 sec/batch)
2017-05-07 21:06:01.510515: step 49390, loss = 0.65 (969.8 examples/sec; 0.132 sec/batch)
2017-05-07 21:06:02.972557: step 49400, loss = 0.75 (875.5 examples/sec; 0.146 sec/batch)
2017-05-07 21:06:04.208404: step 49410, loss = 0.68 (1035.7 examples/sec; 0.124 sec/batch)
2017-05-07 21:06:05.524527: step 49420, loss = 0.66 (972.6 examples/sec; 0.132 sec/batch)
2017-05-07 21:06:06.858213: step 49430, loss = 0.81 (959.7 examples/sec; 0.133 sec/batch)
2017-05-07 21:06:08.189404: step 49440, loss = 0.84 (961.5 examples/sec; 0.133 sec/batch)
2017-05-07 21:06:09.561844: step 49450, loss = 0.69 (932.6 examples/sec; 0.137 sec/batch)
2017-05-07 21:06:10.849385: step 49460, loss = 0.78 (994.1 examples/sec; 0.129 sec/batch)
2017-05-07 21:06:12.174502: step 49470, loss = 0.82 (966.0 examples/sec; 0.133 sec/batch)
2017-05-07 21:06:13.510241: step 49480, loss = 0.73 (958.3 examples/sec; 0.134 sec/batch)
2017-05-07 21:06:14.832673: step 49490, loss = 0.66 (967.9 examples/sec; 0.132 sec/batch)
2017-05-07 21:06:16.300841: step 49500, loss = 0.72 (871.8 examples/sec; 0.147 sec/batch)
2017-05-07 21:06:17.509444: step 49510, loss = 0.88 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-07 21:06:18.860010: step 49520, loss = 0.77 (947.7 examples/sec; 0.135 sec/batch)
2017-05-07 21:06:20.215492: step 49530, loss = 0.64 (944.3 examples/sec; 0.136 sec/batch)
2017-05-07 21:06:21.532290: step 49540, loss = 0.89 (972.0 examples/sec; 0.132 sec/batch)
2017-05-07 21:06:22.905552: step 49550, loss = 0.79 (932.1 examples/sec; 0.137 sec/batch)
2017-05-07 21:06:24.234579: step 49560, loss = 0.79 (963.1 examples/sec; 0.133 sec/batch)
2017-05-07 21:06:25.536540: step 49570, loss = 0.64 (983.1 examples/sec; 0.130 sec/batch)
2017-05-07 21:06:26.861274: step 49580, loss = 0.65 (966.2 examples/sec; 0.132 sec/batch)
2017-05-07 21:06:28.216315: step 49590, loss = 0.70 (944.6 examples/sec; 0.136 sec/batch)
2017-05-07 21:06:29.679776: step 49600, loss = 0.63 (874.6 examples/sec; 0.146 sec/batch)
2017-05-07 21:06:30.889343: step 49610, loss = 0.86 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-07 21:06:32.219664: step 49620, loss = 0.62 (962.2 examples/sec; 0.133 sec/batch)
2017-05-07 21:06:33.535768: step 49630, loss = 0.72 (972.6 examples/sec; 0.132 sec/batch)
2017-05-07 21:06:34.865267: step 49640, loss = 0.66 (962.8 examples/sec; 0.133 sec/batch)
2017-05-07 21:06:36.283783: step 49650, loss = 0.81 (902.4 examples/sec; 0.142 sec/batch)
2017-05-07 21:06:37.537098: step 49660, loss = 0.75 (1021.3 examples/sec; 0.125 sec/batch)
2017-05-07 21:06:38.872181: step 49670, loss = 0.65 (958.7 examples/sec; 0.134 sec/batch)
2017-05-07 21:06:40.223667: step 49680, loss = 0.86 (947.1 examples/sec; 0.135 sec/batch)
2017-05-07 21:06:41.557874: step 49690, loss = 0.66 (959.4 examples/sec; 0.133 sec/batch)
2017-05-07 21:06:43.028180: step 49700, loss = 0.74 (870.6 examples/sec; 0.147 sec/batch)
2017-05-07 21:06:44.253108: step 49710, loss = 0.84 (1045.0 examples/sec; 0.122 sec/batch)
2017-05-07 21:06:45.553119: step 49720, loss = 0.91 (984.6 examples/sec; 0.130 sec/batch)
2017-05-07 21:06:46.895172: step 49730, loss = 0.71 (953.8 examples/sec; 0.134 sec/batch)
2017-05-07 21:06:48.228707: step 49740, loss = 0.64 (959.8 examples/sec; 0.133 sec/batch)
2017-05-07 21:06:49.600899: step 49750, loss = 0.62 (932.8 examples/sec; 0.137 sec/batch)
2017-05-07 21:06:50.901996: step 49760, loss = 0.70 (983.8 examples/sec; 0.130 sec/batch)
2017-05-07 21:06:52.212744: step 49770, loss = 0.73 (976.5 examples/sec; 0.131 sec/batch)
2017-05-07 21:06:53.567858: step 49780, loss = 0.60 (944.6 examples/sec; 0.136 sec/batch)
2017-05-07 21:06:54.912157: step 49790, loss = 0.82 (952.2 examples/sec; 0.134 sec/batch)
2017-05-07 21:06:56.414578: step 49800, loss = 0.68 (852.0 examples/sec; 0.150 sec/batch)
2017-05-07 21:06:57.613184: step 49810, loss = 0.72 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-07 21:06:58.958693: step 49820, loss = 0.78 (951.3 examples/sec; 0.135 sec/batch)
2017-05-07 21:07:00.288432: step 49830, loss = 0.64 (962.6 examples/sec; 0.133 sec/batch)
2017-05-07 21:07:01.610632: step 49840, loss = 0.77 (968.1 examples/sec; 0.132 sec/batch)
2017-05-07 21:07:03.001545: step 49850, loss = 0.84 (920.3 examples/sec; 0.139 sec/batch)
2017-05-07 21:07:04.282935: step 49860, loss = 0.82 (998.9 examples/sec; 0.128 sec/batch)
2017-05-07 21:07:05.630977: step 49870, loss = 0.70 (949.5 examples/sec; 0.135 sec/batch)
2017-05-07 21:07:06.977561: step 49880, loss = 0.69 (950.5 examples/sec; 0.135 sec/batch)
2017-05-07 21:07:08.314626: step 49890, loss = 0.71 (957.3 examples/sec; 0.134 sec/batch)
2017-05-07 21:07:09.777352: step 49900, loss = 0.70 (875.1 examples/sec; 0.146 sec/batch)
2017-05-07 21:07:10.990834: step 49910, loss = 0.69 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-07 21:07:12.322844: step 49920, loss = 0.82 (961.0 examples/sec; 0.133 sec/batch)
2017-05-07 21:07:13.676766: step 49930, loss = 0.78 (945.4 examples/sec; 0.135 sec/batch)
2017-05-07 21:07:15.022093: step 49940, loss = 0.92 (951.4 examples/sec; 0.135 sec/batch)
2017-05-07 21:07:16.421242: step 49950, loss = 0.72 (914.9 examples/sec; 0.140 sec/batch)
2017-05-07 21:07:17.712295: step 49960, loss = 0.71 (991.4 examples/sec; 0.129 sec/batch)
2017-05-07 21:07:19.041345: step 49970, loss = 0.73 (963.1 examples/sec; 0.133 sec/batch)
2017-05-07 21:07:20.366823: step 49980, loss = 0.68 (965.7 examples/sec; 0.133 sec/batch)
2017-05-07 21:07:21.705308: step 49990, loss = 0.84 (956.3 examples/sec; 0.134 sec/batch)
2017-05-07 21:07:23.184355: step 50000, loss = 0.70 (865.4 examples/sec; 0.148 sec/batch)
--- 6704.73404789 seconds ---
