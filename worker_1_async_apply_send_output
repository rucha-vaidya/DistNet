I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.7335
pciBusID 0000:04:00.0
Total memory: 7.92GiB
Free memory: 5.29GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x2f4a750
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: 
name: Quadro K620
major: 5 minor: 0 memoryClockRate (GHz) 1.124
pciBusID 0000:03:00.0
Total memory: 1.95GiB
Free memory: 1.75GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:04:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:962] Ignoring gpu device (device: 1, name: Quadro K620, pci bus id: 0000:03:00.0) with Cuda multiprocessor count: 3. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.
Connecting to port  19001
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Received variable values from ps
2017-05-08 18:24:33.257214: step 0, loss = 4.67 (77.2 examples/sec; 1.658 sec/batch)
2017-05-08 18:24:34.238304: step 10, loss = 4.63 (1304.7 examples/sec; 0.098 sec/batch)
2017-05-08 18:24:35.255922: step 20, loss = 4.60 (1257.8 examples/sec; 0.102 sec/batch)
2017-05-08 18:24:36.550497: step 30, loss = 4.42 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:24:37.818856: step 40, loss = 4.31 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:24:39.067706: step 50, loss = 4.36 (1024.9 examples/sec; 0.125 sec/batch)
2017-05-08 18:24:40.344010: step 60, loss = 4.19 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:24:41.621133: step 70, loss = 4.28 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:24:42.889553: step 80, loss = 4.01 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:24:44.160890: step 90, loss = 4.04 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:24:45.548566: step 100, loss = 4.05 (922.4 examples/sec; 0.139 sec/batch)
2017-05-08 18:24:46.691630: step 110, loss = 3.91 (1119.8 examples/sec; 0.114 sec/batch)
2017-05-08 18:24:47.956473: step 120, loss = 3.99 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-08 18:24:49.268126: step 130, loss = 3.93 (975.9 examples/sec; 0.131 sec/batch)
2017-05-08 18:24:50.554766: step 140, loss = 3.78 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:24:51.840242: step 150, loss = 3.67 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:24:53.156795: step 160, loss = 3.60 (972.2 examples/sec; 0.132 sec/batch)
2017-05-08 18:24:54.415775: step 170, loss = 3.87 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:24:55.693574: step 180, loss = 3.70 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:24:56.965633: step 190, loss = 3.72 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:24:58.333852: step 200, loss = 3.46 (935.5 examples/sec; 0.137 sec/batch)
2017-05-08 18:24:59.529831: step 210, loss = 3.56 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-08 18:25:00.803964: step 220, loss = 3.41 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:25:02.148195: step 230, loss = 3.41 (952.2 examples/sec; 0.134 sec/batch)
2017-05-08 18:25:03.425733: step 240, loss = 3.21 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:25:04.716059: step 250, loss = 3.32 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:25:05.986755: step 260, loss = 3.14 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:25:07.255090: step 270, loss = 3.29 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:25:08.534523: step 280, loss = 3.20 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:25:09.802562: step 290, loss = 3.36 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-08 18:25:11.195867: step 300, loss = 3.29 (918.7 examples/sec; 0.139 sec/batch)
2017-05-08 18:25:12.398060: step 310, loss = 3.14 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-08 18:25:13.603873: step 320, loss = 3.07 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-08 18:25:14.885431: step 330, loss = 3.09 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:25:16.156620: step 340, loss = 3.01 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 18:25:17.421074: step 350, loss = 2.96 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-08 18:25:18.694075: step 360, loss = 2.82 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:25:19.951702: step 370, loss = 3.19 (1017.8 examples/sec; 0.126 sec/batch)
2017-05-08 18:25:21.233736: step 380, loss = 2.96 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:25:22.506762: step 390, loss = 3.14 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:25:23.893754: step 400, loss = 2.81 (922.9 examples/sec; 0.139 sec/batch)
2017-05-08 18:25:25.118492: step 410, loss = 2.86 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-08 18:25:26.327500: step 420, loss = 2.81 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-08 18:25:27.582170: step 430, loss = 2.71 (1020.1 examples/sec; 0.125 sec/batch)
2017-05-08 18:25:28.886581: step 440, loss = 2.77 (981.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:25:30.173964: step 450, loss = 2.69 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:25:31.446818: step 460, loss = 2.55 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:25:32.720255: step 470, loss = 2.64 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:25:34.018154: step 480, loss = 2.71 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:25:35.294433: step 490, loss = 2.69 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:25:36.675722: step 500, loss = 2.65 (926.7 examples/sec; 0.138 sec/batch)
2017-05-08 18:25:37.853144: step 510, loss = 2.70 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-08 18:25:39.119196: step 520, loss = 2.78 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:25:40.392617: step 530, loss = 2.52 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:25:41.666910: step 540, loss = 2.47 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:25:42.942889: step 550, loss = 2.60 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:25:44.210006: step 560, loss = 2.64 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:25:45.496526: step 570, loss = 2.53 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:25:46.780651: step 580, loss = 2.47 (996.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:25:48.061860: step 590, loss = 2.34 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:25:49.465151: step 600, loss = 2.40 (912.1 examples/sec; 0.140 sec/batch)
2017-05-08 18:25:50.643383: step 610, loss = 2.48 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-08 18:25:51.906235: step 620, loss = 2.37 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-08 18:25:53.177514: step 630, loss = 2.54 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 18:25:54.442815: step 640, loss = 2.25 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:25:55.706756: step 650, loss = 2.39 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:25:56.991895: step 660, loss = 2.16 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:25:58.278042: step 670, loss = 2.20 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:25:59.534396: step 680, loss = 2.15 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-08 18:26:00.811510: step 690, loss = 2.36 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:26:02.179347: step 700, loss = 2.21 (935.8 examples/sec; 0.137 sec/batch)
2017-05-08 18:26:03.394336: step 710, loss = 2.44 (1053.5 examples/sec; 0.121 sec/batch)
2017-05-08 18:26:04.667288: step 720, loss = 2.14 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:26:05.917981: step 730, loss = 1.99 (1023.4 examples/sec; 0.125 sec/batch)
2017-05-08 18:26:07.206231: step 740, loss = 1.98 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:26:08.479047: step 750, loss = 2.01 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:26:09.783196: step 760, loss = 2.13 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:26:11.092660: step 770, loss = 2.15 (977.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:26:12.369951: step 780, loss = 2.13 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:26:13.636872: step 790, loss = 2.10 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:26:15.027166: step 800, loss = 1.96 (920.7 examples/sec; 0.139 sec/batch)
2017-05-08 18:26:16.207632: step 810, loss = 2.00 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-08 18:26:17.489991: step 820, loss = 2.03 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:26:18.749775: step 830, loss = 1.96 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-08 18:26:20.048752: step 840, loss = 2.09 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:26:21.311418: step 850, loss = 1.87 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:26:22.612731: step 860, loss = 2.19 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 18:26:23.893325: step 870, loss = 2.16 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:26:25.182853: step 880, loss = 1.91 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:26:26.441179: step 890, loss = 1.80 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-08 18:26:27.822384: step 900, loss = 2.01 (926.7 examples/sec; 0.13E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 23 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
8 sec/batch)
2017-05-08 18:26:29.009814: step 910, loss = 1.89 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-08 18:26:30.267649: step 920, loss = 1.76 (1017.6 examples/sec; 0.126 sec/batch)
2017-05-08 18:26:31.533760: step 930, loss = 1.81 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:26:32.814176: step 940, loss = 2.14 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:26:34.108652: step 950, loss = 1.97 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:26:35.403983: step 960, loss = 1.89 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:26:36.683419: step 970, loss = 1.51 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:26:37.977705: step 980, loss = 1.88 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:26:39.267364: step 990, loss = 1.75 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:26:40.660998: step 1000, loss = 1.73 (918.5 examples/sec; 0.139 sec/batch)
2017-05-08 18:26:41.840595: step 1010, loss = 1.90 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-08 18:26:43.107876: step 1020, loss = 1.72 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:26:44.388644: step 1030, loss = 1.77 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:26:45.710628: step 1040, loss = 1.94 (968.2 examples/sec; 0.132 sec/batch)
2017-05-08 18:26:46.999641: step 1050, loss = 1.83 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:26:48.314753: step 1060, loss = 1.65 (973.3 examples/sec; 0.132 sec/batch)
2017-05-08 18:26:49.581991: step 1070, loss = 1.56 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:26:50.851689: step 1080, loss = 1.81 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:26:52.119747: step 1090, loss = 1.64 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-08 18:26:53.499703: step 1100, loss = 1.70 (927.6 examples/sec; 0.138 sec/batch)
2017-05-08 18:26:54.728329: step 1110, loss = 1.71 (1041.8 examples/sec; 0.123 sec/batch)
2017-05-08 18:26:55.964597: step 1120, loss = 1.72 (1035.4 examples/sec; 0.124 sec/batch)
2017-05-08 18:26:57.254836: step 1130, loss = 1.56 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:26:58.527549: step 1140, loss = 1.68 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-08 18:26:59.818796: step 1150, loss = 1.59 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:27:01.088551: step 1160, loss = 1.49 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:27:02.371153: step 1170, loss = 1.52 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:27:03.627015: step 1180, loss = 1.66 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-08 18:27:04.889086: step 1190, loss = 1.60 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-08 18:27:06.268646: step 1200, loss = 1.63 (927.8 examples/sec; 0.138 sec/batch)
2017-05-08 18:27:07.446992: step 1210, loss = 1.57 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-08 18:27:08.694806: step 1220, loss = 1.62 (1025.8 examples/sec; 0.125 sec/batch)
2017-05-08 18:27:09.975559: step 1230, loss = 1.45 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:27:11.264037: step 1240, loss = 1.56 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:27:12.523955: step 1250, loss = 1.45 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-08 18:27:13.845003: step 1260, loss = 1.49 (968.9 examples/sec; 0.132 sec/batch)
2017-05-08 18:27:15.152354: step 1270, loss = 1.55 (979.1 examples/sec; 0.131 sec/batch)
2017-05-08 18:27:16.455434: step 1280, loss = 1.47 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:27:17.724805: step 1290, loss = 1.73 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-08 18:27:19.094188: step 1300, loss = 1.54 (934.7 examples/sec; 0.137 sec/batch)
2017-05-08 18:27:20.394542: step 1310, loss = 1.41 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:27:21.575765: step 1320, loss = 1.28 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-08 18:27:22.897197: step 1330, loss = 1.37 (968.6 examples/sec; 0.132 sec/batch)
2017-05-08 18:27:24.167027: step 1340, loss = 1.45 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:27:25.436560: step 1350, loss = 1.33 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:27:26.722271: step 1360, loss = 1.58 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:27:27.981202: step 1370, loss = 1.36 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:27:29.289096: step 1380, loss = 1.43 (978.7 examples/sec; 0.131 sec/batch)
2017-05-08 18:27:30.623735: step 1390, loss = 1.58 (959.1 examples/sec; 0.133 sec/batch)
2017-05-08 18:27:32.001701: step 1400, loss = 1.44 (928.9 examples/sec; 0.138 sec/batch)
2017-05-08 18:27:33.178923: step 1410, loss = 1.43 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-08 18:27:34.448255: step 1420, loss = 1.79 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-08 18:27:35.736716: step 1430, loss = 1.34 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:27:37.024496: step 1440, loss = 1.30 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:27:38.287026: step 1450, loss = 1.63 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-08 18:27:39.562666: step 1460, loss = 1.40 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:27:40.823562: step 1470, loss = 1.16 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-08 18:27:42.102606: step 1480, loss = 1.73 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:27:43.409008: step 1490, loss = 1.32 (979.8 examples/sec; 0.131 sec/batch)
2017-05-08 18:27:44.765857: step 1500, loss = 1.41 (943.4 examples/sec; 0.136 sec/batch)
2017-05-08 18:27:45.992817: step 1510, loss = 1.22 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-08 18:27:47.235993: step 1520, loss = 1.43 (1029.6 examples/sec; 0.124 sec/batch)
2017-05-08 18:27:48.506320: step 1530, loss = 1.33 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:27:49.773981: step 1540, loss = 1.52 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 18:27:51.072435: step 1550, loss = 1.35 (985.8 examples/sec; 0.130 sec/batch)
2017-05-08 18:27:52.337114: step 1560, loss = 1.35 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-08 18:27:53.611985: step 1570, loss = 1.23 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:27:54.878119: step 1580, loss = 1.22 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:27:56.170578: step 1590, loss = 1.44 (990.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:27:57.563420: step 1600, loss = 1.13 (919.0 examples/sec; 0.139 sec/batch)
2017-05-08 18:27:58.733990: step 1610, loss = 1.30 (1093.5 examples/sec; 0.117 sec/batch)
2017-05-08 18:27:59.995784: step 1620, loss = 1.16 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-08 18:28:01.257577: step 1630, loss = 1.49 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-08 18:28:02.546672: step 1640, loss = 1.22 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:28:03.851623: step 1650, loss = 1.33 (980.9 examples/sec; 0.130 sec/batch)
2017-05-08 18:28:05.144710: step 1660, loss = 1.37 (989.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:28:06.405752: step 1670, loss = 1.27 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-08 18:28:07.694329: step 1680, loss = 1.48 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:28:08.964640: step 1690, loss = 1.16 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:28:10.358205: step 1700, loss = 1.07 (918.5 examples/sec; 0.139 sec/batch)
2017-05-08 18:28:11.539620: step 1710, loss = 1.46 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-08 18:28:12.810234: step 1720, loss = 1.38 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-08 18:28:14.081160: step 1730, loss = 1.41 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:28:15.339632: step 1740, loss = 1.32 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-08 18:28:16.622633: step 1750, loss = 1.36 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:28:17.897643: step 1760, loss = 1.30 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:28:19.212174: step 1770, loss = 1.20 (973.7 examples/sec; 0.131 sec/batch)
2017-05-08 18:28:20.471141: step 1780, loss = 1.19 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:28:21.736269: step 1790, loss = 1.19 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:28:23.104420: step 1800, loss = 1.35 (935.6 examples/sec; 0.137 sec/batch)
2017-05-08 18:28:24.284242: step 1810, loss = 1.02 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-08 18:28:25.564514: step 1820, lE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 43 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
oss = 1.36 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:28:26.833760: step 1830, loss = 1.09 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:28:28.116972: step 1840, loss = 1.21 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:28:29.401866: step 1850, loss = 1.26 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:28:30.705024: step 1860, loss = 1.20 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:28:31.972143: step 1870, loss = 1.31 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:28:33.230138: step 1880, loss = 1.33 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-08 18:28:34.515343: step 1890, loss = 1.29 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:28:35.885006: step 1900, loss = 1.28 (934.5 examples/sec; 0.137 sec/batch)
2017-05-08 18:28:37.096731: step 1910, loss = 1.24 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-08 18:28:38.387603: step 1920, loss = 1.04 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:28:39.665604: step 1930, loss = 1.24 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:28:40.956393: step 1940, loss = 1.33 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:28:42.251658: step 1950, loss = 1.22 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:28:43.546560: step 1960, loss = 1.13 (988.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:28:44.825176: step 1970, loss = 1.12 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:28:46.099044: step 1980, loss = 1.08 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:28:47.383517: step 1990, loss = 1.20 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:28:48.756614: step 2000, loss = 1.14 (932.2 examples/sec; 0.137 sec/batch)
2017-05-08 18:28:49.937166: step 2010, loss = 1.23 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-08 18:28:51.204762: step 2020, loss = 1.43 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:28:52.479821: step 2030, loss = 1.28 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:28:53.760740: step 2040, loss = 1.15 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:28:55.066310: step 2050, loss = 1.30 (980.4 examples/sec; 0.131 sec/batch)
2017-05-08 18:28:56.349554: step 2060, loss = 1.31 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:28:57.630361: step 2070, loss = 1.05 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:28:58.904752: step 2080, loss = 1.28 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-08 18:29:00.179283: step 2090, loss = 1.24 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:29:01.551332: step 2100, loss = 1.08 (932.9 examples/sec; 0.137 sec/batch)
2017-05-08 18:29:02.735326: step 2110, loss = 1.27 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-08 18:29:04.025676: step 2120, loss = 1.27 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:29:05.306953: step 2130, loss = 1.36 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:29:06.585236: step 2140, loss = 1.06 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:29:07.841931: step 2150, loss = 1.06 (1018.6 examples/sec; 0.126 sec/batch)
2017-05-08 18:29:09.110511: step 2160, loss = 1.27 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:29:10.385844: step 2170, loss = 1.03 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:29:11.637605: step 2180, loss = 1.03 (1022.6 examples/sec; 0.125 sec/batch)
2017-05-08 18:29:12.917523: step 2190, loss = 1.13 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:29:14.327754: step 2200, loss = 1.04 (907.7 examples/sec; 0.141 sec/batch)
2017-05-08 18:29:15.510300: step 2210, loss = 1.16 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-08 18:29:16.820558: step 2220, loss = 1.28 (976.9 examples/sec; 0.131 sec/batch)
2017-05-08 18:29:18.130999: step 2230, loss = 1.12 (976.8 examples/sec; 0.131 sec/batch)
2017-05-08 18:29:19.399396: step 2240, loss = 1.10 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:29:20.659266: step 2250, loss = 1.02 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-08 18:29:21.970300: step 2260, loss = 1.29 (976.3 examples/sec; 0.131 sec/batch)
2017-05-08 18:29:23.244479: step 2270, loss = 1.13 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:29:24.510287: step 2280, loss = 1.09 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:29:25.775780: step 2290, loss = 1.06 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:29:27.147666: step 2300, loss = 1.32 (933.0 examples/sec; 0.137 sec/batch)
2017-05-08 18:29:28.357431: step 2310, loss = 0.96 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-08 18:29:29.657917: step 2320, loss = 1.11 (984.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:29:30.961175: step 2330, loss = 1.03 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:29:32.243726: step 2340, loss = 1.07 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:29:33.530483: step 2350, loss = 1.19 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:29:34.835955: step 2360, loss = 1.18 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:29:36.101952: step 2370, loss = 1.27 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:29:37.369821: step 2380, loss = 1.50 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:29:38.675177: step 2390, loss = 1.20 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 18:29:40.075079: step 2400, loss = 1.20 (914.4 examples/sec; 0.140 sec/batch)
2017-05-08 18:29:41.278534: step 2410, loss = 1.19 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-08 18:29:42.562138: step 2420, loss = 1.15 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:29:43.836300: step 2430, loss = 0.93 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:29:45.119268: step 2440, loss = 1.03 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:29:46.411580: step 2450, loss = 1.09 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:29:47.679204: step 2460, loss = 1.25 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:29:48.967748: step 2470, loss = 1.23 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:29:50.249026: step 2480, loss = 1.01 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:29:51.522856: step 2490, loss = 0.94 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:29:52.901684: step 2500, loss = 1.05 (928.3 examples/sec; 0.138 sec/batch)
2017-05-08 18:29:54.076679: step 2510, loss = 1.19 (1089.4 examples/sec; 0.118 sec/batch)
2017-05-08 18:29:55.349969: step 2520, loss = 1.18 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:29:56.641960: step 2530, loss = 1.08 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:29:57.958496: step 2540, loss = 1.13 (972.2 examples/sec; 0.132 sec/batch)
2017-05-08 18:29:59.228369: step 2550, loss = 1.14 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:30:00.492751: step 2560, loss = 1.11 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-08 18:30:01.773959: step 2570, loss = 1.12 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:30:03.036267: step 2580, loss = 1.05 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-08 18:30:04.383856: step 2590, loss = 1.02 (949.8 examples/sec; 0.135 sec/batch)
2017-05-08 18:30:05.849735: step 2600, loss = 1.11 (873.2 examples/sec; 0.147 sec/batch)
2017-05-08 18:30:07.025052: step 2610, loss = 1.05 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-08 18:30:08.357484: step 2620, loss = 1.02 (960.7 examples/sec; 0.133 sec/batch)
2017-05-08 18:30:09.595366: step 2630, loss = 1.14 (1034.0 examples/sec; 0.124 sec/batch)
2017-05-08 18:30:10.876791: step 2640, loss = 1.06 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:30:12.184950: step 2650, loss = 1.06 (978.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:30:13.440859: step 2660, loss = 1.16 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-08 18:30:14.724923: step 2670, loss = 1.06 (996.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:30:15.996706: step 2680, loss = 1.11 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:30:17.289320: step 2690, loss = 1.27 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:30:18.685567: step 2700, loss = 1.09 (916.7 examples/sec; 0.140 sec/batch)
2017-05-08 18:30:19.868708: step 2710, loss = 0.87 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-08 18:30:21.156282: step 2720, loss = 1.06 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:30:22.482273: step 2730, loss = 1.02 (965.3 examples/sec; 0.133 sec/batch)
20E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 63 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
17-05-08 18:30:23.754416: step 2740, loss = 0.98 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:30:25.040035: step 2750, loss = 1.13 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:30:26.340940: step 2760, loss = 1.02 (983.9 examples/sec; 0.130 sec/batch)
2017-05-08 18:30:27.631360: step 2770, loss = 1.20 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:30:28.921644: step 2780, loss = 1.09 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:30:30.200555: step 2790, loss = 1.07 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:30:31.581019: step 2800, loss = 0.98 (927.2 examples/sec; 0.138 sec/batch)
2017-05-08 18:30:32.751968: step 2810, loss = 1.04 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-08 18:30:34.053858: step 2820, loss = 1.15 (983.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:30:35.342278: step 2830, loss = 1.11 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:30:36.645857: step 2840, loss = 1.04 (981.9 examples/sec; 0.130 sec/batch)
2017-05-08 18:30:37.946879: step 2850, loss = 1.10 (983.8 examples/sec; 0.130 sec/batch)
2017-05-08 18:30:39.268669: step 2860, loss = 0.96 (968.4 examples/sec; 0.132 sec/batch)
2017-05-08 18:30:40.576202: step 2870, loss = 1.12 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 18:30:41.821256: step 2880, loss = 1.05 (1028.1 examples/sec; 0.125 sec/batch)
2017-05-08 18:30:43.180383: step 2890, loss = 1.06 (941.8 examples/sec; 0.136 sec/batch)
2017-05-08 18:30:44.567137: step 2900, loss = 1.18 (923.0 examples/sec; 0.139 sec/batch)
2017-05-08 18:30:45.744759: step 2910, loss = 1.13 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-08 18:30:46.995213: step 2920, loss = 1.28 (1023.6 examples/sec; 0.125 sec/batch)
2017-05-08 18:30:48.267046: step 2930, loss = 0.98 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-08 18:30:49.556530: step 2940, loss = 0.91 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:30:50.846877: step 2950, loss = 1.11 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:30:52.113814: step 2960, loss = 1.02 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:30:53.434929: step 2970, loss = 1.13 (968.9 examples/sec; 0.132 sec/batch)
2017-05-08 18:30:54.703821: step 2980, loss = 1.17 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:30:56.007164: step 2990, loss = 0.86 (982.1 examples/sec; 0.130 sec/batch)
2017-05-08 18:30:57.405715: step 3000, loss = 0.88 (915.2 examples/sec; 0.140 sec/batch)
2017-05-08 18:30:58.602136: step 3010, loss = 1.17 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-08 18:30:59.887075: step 3020, loss = 1.06 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:31:01.139790: step 3030, loss = 1.07 (1021.8 examples/sec; 0.125 sec/batch)
2017-05-08 18:31:02.422064: step 3040, loss = 1.00 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:31:03.717958: step 3050, loss = 1.07 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 18:31:04.970669: step 3060, loss = 0.97 (1021.8 examples/sec; 0.125 sec/batch)
2017-05-08 18:31:06.279386: step 3070, loss = 1.03 (978.1 examples/sec; 0.131 sec/batch)
2017-05-08 18:31:07.561520: step 3080, loss = 0.97 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:31:08.839496: step 3090, loss = 1.07 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:31:10.212756: step 3100, loss = 1.11 (932.1 examples/sec; 0.137 sec/batch)
2017-05-08 18:31:11.409545: step 3110, loss = 1.14 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-08 18:31:12.690743: step 3120, loss = 0.98 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:31:13.978003: step 3130, loss = 0.88 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:31:15.259642: step 3140, loss = 1.19 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:31:16.546203: step 3150, loss = 0.99 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:31:17.844487: step 3160, loss = 0.93 (985.9 examples/sec; 0.130 sec/batch)
2017-05-08 18:31:19.098236: step 3170, loss = 0.94 (1020.9 examples/sec; 0.125 sec/batch)
2017-05-08 18:31:20.386420: step 3180, loss = 0.94 (993.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:31:21.666091: step 3190, loss = 0.99 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:31:23.041502: step 3200, loss = 0.99 (930.6 examples/sec; 0.138 sec/batch)
2017-05-08 18:31:24.244741: step 3210, loss = 1.03 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-08 18:31:25.520758: step 3220, loss = 0.95 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:31:26.811560: step 3230, loss = 1.02 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:31:28.057032: step 3240, loss = 1.08 (1027.7 examples/sec; 0.125 sec/batch)
2017-05-08 18:31:29.371510: step 3250, loss = 1.06 (973.8 examples/sec; 0.131 sec/batch)
2017-05-08 18:31:30.665477: step 3260, loss = 1.02 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:31:31.920399: step 3270, loss = 1.01 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-08 18:31:33.222860: step 3280, loss = 0.96 (982.8 examples/sec; 0.130 sec/batch)
2017-05-08 18:31:34.522330: step 3290, loss = 1.00 (985.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:31:35.887726: step 3300, loss = 1.04 (937.5 examples/sec; 0.137 sec/batch)
2017-05-08 18:31:37.065723: step 3310, loss = 0.99 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-08 18:31:38.353936: step 3320, loss = 0.87 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:31:39.616771: step 3330, loss = 0.95 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-08 18:31:40.884712: step 3340, loss = 0.94 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:31:42.163289: step 3350, loss = 0.94 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:31:43.446642: step 3360, loss = 1.06 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:31:44.755778: step 3370, loss = 0.98 (977.7 examples/sec; 0.131 sec/batch)
2017-05-08 18:31:46.038264: step 3380, loss = 0.94 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:31:47.317481: step 3390, loss = 0.99 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:31:48.699008: step 3400, loss = 1.03 (926.5 examples/sec; 0.138 sec/batch)
2017-05-08 18:31:49.901940: step 3410, loss = 1.23 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-08 18:31:51.238976: step 3420, loss = 1.03 (957.3 examples/sec; 0.134 sec/batch)
2017-05-08 18:31:52.510319: step 3430, loss = 0.93 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:31:53.793606: step 3440, loss = 0.99 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:31:55.059483: step 3450, loss = 0.89 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:31:56.326718: step 3460, loss = 0.96 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:31:57.599287: step 3470, loss = 1.05 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:31:58.886785: step 3480, loss = 1.05 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:32:00.165926: step 3490, loss = 0.85 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:32:01.568302: step 3500, loss = 1.24 (912.7 examples/sec; 0.140 sec/batch)
2017-05-08 18:32:02.760191: step 3510, loss = 0.98 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-08 18:32:04.048554: step 3520, loss = 1.02 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:32:05.311980: step 3530, loss = 0.84 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-08 18:32:06.607725: step 3540, loss = 1.18 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 18:32:07.897923: step 3550, loss = 1.12 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:32:09.191513: step 3560, loss = 1.04 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:32:10.507132: step 3570, loss = 0.98 (972.9 examples/sec; 0.132 sec/batch)
2017-05-08 18:32:11.804384: step 3580, loss = 0.99 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 18:32:13.084495: step 3590, loss = 1.07 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:32:14.452640: step 3600, loss = 1.07 (935.6 examples/sec; 0.137 sec/batch)
2017-05-08 18:32:15.625702: step 3610, loss = 1.05 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-08 18:32:16.958044: step 3620, loss = 1.23 (960.7 examples/sec; 0.133 sec/batch)
2017-05-08 18:32:18.263791: step 3630, loss = 1.01 (980.3 examples/sec; 0.131 sec/batch)
2017-05-08 18:32:19.547837: step 3640, loss = 1.07 (996.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:32:20.806597: step 3650, loss = 0.97 (1016.9 exE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 83 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
amples/sec; 0.126 sec/batch)
2017-05-08 18:32:22.073828: step 3660, loss = 0.86 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:32:23.380793: step 3670, loss = 1.19 (979.4 examples/sec; 0.131 sec/batch)
2017-05-08 18:32:24.699029: step 3680, loss = 0.84 (971.0 examples/sec; 0.132 sec/batch)
2017-05-08 18:32:26.010242: step 3690, loss = 1.01 (976.2 examples/sec; 0.131 sec/batch)
2017-05-08 18:32:27.371046: step 3700, loss = 0.97 (940.6 examples/sec; 0.136 sec/batch)
2017-05-08 18:32:28.562086: step 3710, loss = 0.93 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-08 18:32:29.841652: step 3720, loss = 1.13 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:32:31.123261: step 3730, loss = 0.86 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:32:32.379558: step 3740, loss = 0.93 (1018.9 examples/sec; 0.126 sec/batch)
2017-05-08 18:32:33.675954: step 3750, loss = 0.97 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:32:34.956478: step 3760, loss = 1.08 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:32:36.239846: step 3770, loss = 0.89 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:32:37.546353: step 3780, loss = 1.13 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 18:32:38.815070: step 3790, loss = 1.08 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 18:32:40.181078: step 3800, loss = 0.99 (937.0 examples/sec; 0.137 sec/batch)
2017-05-08 18:32:41.351755: step 3810, loss = 1.20 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-08 18:32:42.626112: step 3820, loss = 0.82 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-08 18:32:43.932617: step 3830, loss = 0.89 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 18:32:45.255981: step 3840, loss = 1.02 (967.2 examples/sec; 0.132 sec/batch)
2017-05-08 18:32:46.573225: step 3850, loss = 1.07 (971.7 examples/sec; 0.132 sec/batch)
2017-05-08 18:32:47.844930: step 3860, loss = 1.04 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:32:49.140819: step 3870, loss = 1.09 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 18:32:50.461927: step 3880, loss = 1.08 (968.9 examples/sec; 0.132 sec/batch)
2017-05-08 18:32:51.763585: step 3890, loss = 1.04 (983.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:32:53.132558: step 3900, loss = 1.01 (935.0 examples/sec; 0.137 sec/batch)
2017-05-08 18:32:54.324879: step 3910, loss = 0.84 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-08 18:32:55.614542: step 3920, loss = 0.90 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:32:56.923496: step 3930, loss = 0.97 (977.9 examples/sec; 0.131 sec/batch)
2017-05-08 18:32:58.209744: step 3940, loss = 0.91 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:32:59.464021: step 3950, loss = 0.97 (1020.5 examples/sec; 0.125 sec/batch)
2017-05-08 18:33:00.742555: step 3960, loss = 0.91 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:33:02.034027: step 3970, loss = 1.13 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:33:03.306842: step 3980, loss = 0.83 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:33:04.574202: step 3990, loss = 0.91 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:33:05.928692: step 4000, loss = 1.06 (945.0 examples/sec; 0.135 sec/batch)
2017-05-08 18:33:07.132412: step 4010, loss = 1.05 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-08 18:33:08.457196: step 4020, loss = 1.10 (966.2 examples/sec; 0.132 sec/batch)
2017-05-08 18:33:09.734575: step 4030, loss = 0.98 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:33:11.056042: step 4040, loss = 1.06 (968.6 examples/sec; 0.132 sec/batch)
2017-05-08 18:33:12.361622: step 4050, loss = 1.03 (980.4 examples/sec; 0.131 sec/batch)
2017-05-08 18:33:13.652517: step 4060, loss = 0.96 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:33:14.905674: step 4070, loss = 0.98 (1021.4 examples/sec; 0.125 sec/batch)
2017-05-08 18:33:16.196968: step 4080, loss = 1.04 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:33:17.502716: step 4090, loss = 0.93 (980.3 examples/sec; 0.131 sec/batch)
2017-05-08 18:33:18.918381: step 4100, loss = 1.06 (904.2 examples/sec; 0.142 sec/batch)
2017-05-08 18:33:20.117343: step 4110, loss = 1.09 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-08 18:33:21.383696: step 4120, loss = 1.00 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:33:22.652056: step 4130, loss = 0.96 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:33:23.920803: step 4140, loss = 1.05 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 18:33:25.194907: step 4150, loss = 0.90 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:33:26.472133: step 4160, loss = 0.90 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:33:27.736977: step 4170, loss = 1.13 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-08 18:33:29.015895: step 4180, loss = 0.95 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:33:30.297086: step 4190, loss = 0.99 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:33:31.657236: step 4200, loss = 0.88 (941.1 examples/sec; 0.136 sec/batch)
2017-05-08 18:33:32.853312: step 4210, loss = 1.06 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-08 18:33:34.126761: step 4220, loss = 1.01 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:33:35.407501: step 4230, loss = 0.86 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:33:36.664537: step 4240, loss = 1.12 (1018.3 examples/sec; 0.126 sec/batch)
2017-05-08 18:33:37.935423: step 4250, loss = 0.79 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:33:39.224379: step 4260, loss = 1.12 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:33:40.475094: step 4270, loss = 1.22 (1023.4 examples/sec; 0.125 sec/batch)
2017-05-08 18:33:41.784777: step 4280, loss = 0.81 (977.3 examples/sec; 0.131 sec/batch)
2017-05-08 18:33:43.046629: step 4290, loss = 0.87 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-08 18:33:44.434749: step 4300, loss = 0.77 (922.1 examples/sec; 0.139 sec/batch)
2017-05-08 18:33:45.624728: step 4310, loss = 0.95 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-08 18:33:46.911805: step 4320, loss = 1.00 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:33:48.198858: step 4330, loss = 1.03 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:33:49.442706: step 4340, loss = 0.99 (1029.1 examples/sec; 0.124 sec/batch)
2017-05-08 18:33:50.719091: step 4350, loss = 1.22 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:33:51.976594: step 4360, loss = 0.93 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-08 18:33:53.276289: step 4370, loss = 0.96 (984.8 examples/sec; 0.130 sec/batch)
2017-05-08 18:33:54.574139: step 4380, loss = 1.02 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:33:55.867647: step 4390, loss = 1.29 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:33:57.259888: step 4400, loss = 0.86 (919.4 examples/sec; 0.139 sec/batch)
2017-05-08 18:33:58.465822: step 4410, loss = 0.94 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-08 18:33:59.734971: step 4420, loss = 0.96 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:34:01.058435: step 4430, loss = 0.93 (967.2 examples/sec; 0.132 sec/batch)
2017-05-08 18:34:02.326348: step 4440, loss = 1.02 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:34:03.581789: step 4450, loss = 0.90 (1019.6 examples/sec; 0.126 sec/batch)
2017-05-08 18:34:04.880727: step 4460, loss = 0.94 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:34:06.197599: step 4470, loss = 1.00 (972.0 examples/sec; 0.132 sec/batch)
2017-05-08 18:34:07.508923: step 4480, loss = 0.96 (976.1 examples/sec; 0.131 sec/batch)
2017-05-08 18:34:08.783519: step 4490, loss = 0.86 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:34:10.174882: step 4500, loss = 1.16 (920.0 examples/sec; 0.139 sec/batch)
2017-05-08 18:34:11.354679: step 4510, loss = 1.01 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-08 18:34:12.636537: step 4520, loss = 1.00 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:34:13.911654: step 4530, loss = 0.94 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:34:15.204308: step 4540, loss = 1.05 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:34:16.497845: step 4550, loss = 0.91 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:34:17.776614: step 4560, loss = 1.00 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:34:19.05E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 104 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
3670: step 4570, loss = 1.03 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:34:20.333659: step 4580, loss = 1.01 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:34:21.634920: step 4590, loss = 1.10 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 18:34:23.031292: step 4600, loss = 1.09 (916.7 examples/sec; 0.140 sec/batch)
2017-05-08 18:34:24.233561: step 4610, loss = 0.89 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-08 18:34:25.494723: step 4620, loss = 1.01 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-08 18:34:26.785562: step 4630, loss = 0.88 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:34:28.123882: step 4640, loss = 0.96 (956.4 examples/sec; 0.134 sec/batch)
2017-05-08 18:34:29.405117: step 4650, loss = 1.08 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:34:30.684650: step 4660, loss = 0.97 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:34:31.951211: step 4670, loss = 0.91 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:34:33.300661: step 4680, loss = 1.07 (948.5 examples/sec; 0.135 sec/batch)
2017-05-08 18:34:34.534131: step 4690, loss = 1.02 (1037.7 examples/sec; 0.123 sec/batch)
2017-05-08 18:34:35.912731: step 4700, loss = 1.00 (928.5 examples/sec; 0.138 sec/batch)
2017-05-08 18:34:37.108997: step 4710, loss = 0.92 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-08 18:34:38.390720: step 4720, loss = 1.09 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:34:39.664977: step 4730, loss = 0.92 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:34:41.005774: step 4740, loss = 0.90 (954.6 examples/sec; 0.134 sec/batch)
2017-05-08 18:34:42.282752: step 4750, loss = 1.17 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:34:43.571740: step 4760, loss = 1.00 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:34:44.838950: step 4770, loss = 0.98 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:34:46.135617: step 4780, loss = 1.00 (987.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:34:47.407035: step 4790, loss = 0.77 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:34:48.797146: step 4800, loss = 1.17 (920.8 examples/sec; 0.139 sec/batch)
2017-05-08 18:34:49.967126: step 4810, loss = 0.95 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-08 18:34:51.240857: step 4820, loss = 0.97 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-08 18:34:52.494055: step 4830, loss = 1.11 (1021.4 examples/sec; 0.125 sec/batch)
2017-05-08 18:34:53.753885: step 4840, loss = 0.84 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-08 18:34:55.051229: step 4850, loss = 0.99 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 18:34:56.305139: step 4860, loss = 1.05 (1020.8 examples/sec; 0.125 sec/batch)
2017-05-08 18:34:57.592976: step 4870, loss = 0.86 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:34:58.851740: step 4880, loss = 1.15 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-08 18:35:00.134962: step 4890, loss = 0.76 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:35:01.509414: step 4900, loss = 0.90 (931.3 examples/sec; 0.137 sec/batch)
2017-05-08 18:35:02.770427: step 4910, loss = 0.91 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-08 18:35:04.026335: step 4920, loss = 0.89 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-08 18:35:05.296126: step 4930, loss = 0.77 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:35:06.622597: step 4940, loss = 0.92 (965.0 examples/sec; 0.133 sec/batch)
2017-05-08 18:35:07.939367: step 4950, loss = 1.16 (972.1 examples/sec; 0.132 sec/batch)
2017-05-08 18:35:09.236893: step 4960, loss = 1.08 (986.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:35:10.503550: step 4970, loss = 0.90 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:35:11.756176: step 4980, loss = 1.15 (1021.9 examples/sec; 0.125 sec/batch)
2017-05-08 18:35:13.102309: step 4990, loss = 0.96 (950.9 examples/sec; 0.135 sec/batch)
2017-05-08 18:35:14.422194: step 5000, loss = 0.85 (969.8 examples/sec; 0.132 sec/batch)
2017-05-08 18:35:15.589025: step 5010, loss = 1.04 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-08 18:35:16.876417: step 5020, loss = 0.89 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:35:18.141721: step 5030, loss = 0.98 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:35:19.399417: step 5040, loss = 1.11 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:35:20.742516: step 5050, loss = 0.99 (953.0 examples/sec; 0.134 sec/batch)
2017-05-08 18:35:22.016768: step 5060, loss = 0.87 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:35:23.298770: step 5070, loss = 0.94 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:35:24.568227: step 5080, loss = 0.97 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:35:25.862788: step 5090, loss = 1.04 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:35:27.202161: step 5100, loss = 0.99 (955.7 examples/sec; 0.134 sec/batch)
2017-05-08 18:35:28.408621: step 5110, loss = 0.90 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-08 18:35:29.681680: step 5120, loss = 0.85 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 18:35:30.994994: step 5130, loss = 0.91 (974.6 examples/sec; 0.131 sec/batch)
2017-05-08 18:35:32.358960: step 5140, loss = 0.96 (938.4 examples/sec; 0.136 sec/batch)
2017-05-08 18:35:33.586232: step 5150, loss = 0.94 (1043.0 examples/sec; 0.123 sec/batch)
2017-05-08 18:35:34.876170: step 5160, loss = 0.94 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:35:36.144532: step 5170, loss = 1.21 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:35:37.428376: step 5180, loss = 0.83 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:35:38.729694: step 5190, loss = 1.06 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 18:35:40.074114: step 5200, loss = 1.08 (952.1 examples/sec; 0.134 sec/batch)
2017-05-08 18:35:41.271074: step 5210, loss = 0.83 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-08 18:35:42.538545: step 5220, loss = 0.85 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-08 18:35:43.800006: step 5230, loss = 0.75 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:35:45.122486: step 5240, loss = 1.05 (967.9 examples/sec; 0.132 sec/batch)
2017-05-08 18:35:46.390790: step 5250, loss = 0.92 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:35:47.660990: step 5260, loss = 1.00 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-08 18:35:48.942197: step 5270, loss = 1.06 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:35:50.201375: step 5280, loss = 0.77 (1016.5 examples/sec; 0.126 sec/batch)
2017-05-08 18:35:51.518233: step 5290, loss = 0.93 (972.0 examples/sec; 0.132 sec/batch)
2017-05-08 18:35:52.825256: step 5300, loss = 1.07 (979.3 examples/sec; 0.131 sec/batch)
2017-05-08 18:35:53.994044: step 5310, loss = 0.88 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-08 18:35:55.271642: step 5320, loss = 0.91 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:35:56.532817: step 5330, loss = 0.96 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-08 18:35:57.825067: step 5340, loss = 0.81 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:35:59.081746: step 5350, loss = 0.90 (1018.6 examples/sec; 0.126 sec/batch)
2017-05-08 18:36:00.361127: step 5360, loss = 0.91 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:36:01.628324: step 5370, loss = 0.89 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:36:02.901164: step 5380, loss = 1.00 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:36:04.220956: step 5390, loss = 1.26 (969.9 examples/sec; 0.132 sec/batch)
2017-05-08 18:36:05.566385: step 5400, loss = 1.03 (951.4 examples/sec; 0.135 sec/batch)
2017-05-08 18:36:06.769602: step 5410, loss = 0.90 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-08 18:36:08.046740: step 5420, loss = 1.00 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:36:09.331589: step 5430, loss = 0.88 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:36:10.607115: step 5440, loss = 0.95 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:36:11.885773: step 5450, loss = 1.02 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:36:13.191045: step 5460, loss = 0.92 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 18:36:14.451386: step 5470, loss = 0.86 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-08 18:36:15.741639: step 5480, loss = 1.11 (992.1 exampleE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 124 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
s/sec; 0.129 sec/batch)
2017-05-08 18:36:17.028629: step 5490, loss = 0.93 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:36:18.410372: step 5500, loss = 0.79 (926.4 examples/sec; 0.138 sec/batch)
2017-05-08 18:36:19.614151: step 5510, loss = 0.93 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-08 18:36:20.902311: step 5520, loss = 0.80 (993.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:36:22.219656: step 5530, loss = 0.92 (971.7 examples/sec; 0.132 sec/batch)
2017-05-08 18:36:23.514244: step 5540, loss = 0.96 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:36:24.799774: step 5550, loss = 0.87 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:36:26.063369: step 5560, loss = 0.80 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-08 18:36:27.351569: step 5570, loss = 0.85 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:36:28.618734: step 5580, loss = 0.98 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:36:29.893439: step 5590, loss = 0.80 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:36:31.289049: step 5600, loss = 0.89 (917.2 examples/sec; 0.140 sec/batch)
2017-05-08 18:36:32.459006: step 5610, loss = 1.04 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-08 18:36:33.724798: step 5620, loss = 0.83 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:36:35.012597: step 5630, loss = 0.85 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:36:36.271270: step 5640, loss = 1.13 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-08 18:36:37.564986: step 5650, loss = 0.74 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:36:38.857593: step 5660, loss = 0.89 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:36:40.117414: step 5670, loss = 0.99 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-08 18:36:41.396218: step 5680, loss = 0.97 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:36:42.686053: step 5690, loss = 1.03 (992.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:36:44.085789: step 5700, loss = 1.03 (914.5 examples/sec; 0.140 sec/batch)
2017-05-08 18:36:45.306350: step 5710, loss = 0.76 (1048.7 examples/sec; 0.122 sec/batch)
2017-05-08 18:36:46.544140: step 5720, loss = 0.83 (1034.1 examples/sec; 0.124 sec/batch)
2017-05-08 18:36:47.811261: step 5730, loss = 0.78 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:36:49.084205: step 5740, loss = 1.13 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:36:50.400353: step 5750, loss = 1.02 (972.5 examples/sec; 0.132 sec/batch)
2017-05-08 18:36:51.662975: step 5760, loss = 1.00 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-08 18:36:52.946354: step 5770, loss = 0.84 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:36:54.230144: step 5780, loss = 0.98 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:36:55.511192: step 5790, loss = 0.90 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:36:56.889674: step 5800, loss = 0.94 (928.6 examples/sec; 0.138 sec/batch)
2017-05-08 18:36:58.076077: step 5810, loss = 0.78 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-08 18:36:59.356122: step 5820, loss = 1.01 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:37:00.627403: step 5830, loss = 0.96 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 18:37:01.931044: step 5840, loss = 1.09 (981.9 examples/sec; 0.130 sec/batch)
2017-05-08 18:37:03.212698: step 5850, loss = 0.86 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:37:04.488170: step 5860, loss = 1.21 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:37:05.787646: step 5870, loss = 0.99 (985.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:37:07.054560: step 5880, loss = 1.01 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:37:08.321249: step 5890, loss = 0.84 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:37:09.708150: step 5900, loss = 0.90 (922.9 examples/sec; 0.139 sec/batch)
2017-05-08 18:37:10.877775: step 5910, loss = 0.93 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-08 18:37:12.184693: step 5920, loss = 0.90 (979.4 examples/sec; 0.131 sec/batch)
2017-05-08 18:37:13.466301: step 5930, loss = 0.88 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:37:14.743546: step 5940, loss = 1.01 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:37:16.014135: step 5950, loss = 0.91 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-08 18:37:17.295947: step 5960, loss = 0.92 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:37:18.575714: step 5970, loss = 0.89 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:37:19.851131: step 5980, loss = 0.84 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:37:21.126700: step 5990, loss = 1.00 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:37:22.503920: step 6000, loss = 0.78 (929.4 examples/sec; 0.138 sec/batch)
2017-05-08 18:37:23.673811: step 6010, loss = 0.91 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-08 18:37:24.950749: step 6020, loss = 1.04 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:37:26.213655: step 6030, loss = 0.84 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-08 18:37:27.531875: step 6040, loss = 0.93 (971.0 examples/sec; 0.132 sec/batch)
2017-05-08 18:37:28.766563: step 6050, loss = 0.99 (1036.7 examples/sec; 0.123 sec/batch)
2017-05-08 18:37:30.049641: step 6060, loss = 0.87 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:37:31.325579: step 6070, loss = 0.86 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:37:32.606942: step 6080, loss = 0.88 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:37:33.902857: step 6090, loss = 0.90 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 18:37:35.279274: step 6100, loss = 1.03 (930.0 examples/sec; 0.138 sec/batch)
2017-05-08 18:37:36.448566: step 6110, loss = 0.98 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-08 18:37:37.751138: step 6120, loss = 0.82 (982.7 examples/sec; 0.130 sec/batch)
2017-05-08 18:37:39.036269: step 6130, loss = 0.82 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:37:40.301254: step 6140, loss = 0.81 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-08 18:37:41.581714: step 6150, loss = 0.98 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:37:42.883727: step 6160, loss = 0.83 (983.1 examples/sec; 0.130 sec/batch)
2017-05-08 18:37:44.188114: step 6170, loss = 0.94 (981.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:37:45.465471: step 6180, loss = 0.82 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:37:46.749824: step 6190, loss = 1.08 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:37:48.119489: step 6200, loss = 0.88 (934.5 examples/sec; 0.137 sec/batch)
2017-05-08 18:37:49.278035: step 6210, loss = 0.82 (1104.8 examples/sec; 0.116 sec/batch)
2017-05-08 18:37:50.540586: step 6220, loss = 0.96 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-08 18:37:51.809686: step 6230, loss = 0.87 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:37:53.112408: step 6240, loss = 0.84 (982.6 examples/sec; 0.130 sec/batch)
2017-05-08 18:37:54.423006: step 6250, loss = 0.86 (976.7 examples/sec; 0.131 sec/batch)
2017-05-08 18:37:55.676252: step 6260, loss = 0.96 (1021.3 examples/sec; 0.125 sec/batch)
2017-05-08 18:37:56.973702: step 6270, loss = 0.93 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 18:37:58.266768: step 6280, loss = 0.82 (989.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:37:59.540830: step 6290, loss = 0.95 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-08 18:38:00.926916: step 6300, loss = 0.98 (923.5 examples/sec; 0.139 sec/batch)
2017-05-08 18:38:02.110877: step 6310, loss = 0.96 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-08 18:38:03.395160: step 6320, loss = 0.82 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:38:04.680873: step 6330, loss = 1.07 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:38:05.967291: step 6340, loss = 0.89 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:38:07.245928: step 6350, loss = 0.87 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:38:08.524227: step 6360, loss = 1.07 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:38:09.788560: step 6370, loss = 0.84 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-08 18:38:11.049030: step 6380, loss = 0.90 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-08 18:38:12.336228: step 6390, loss = 0.93 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:38:13.713070: step 6400, loss = 0.71 (929.7 examples/sec; 0.138 sec/batch)
2017-05-08 18:38:14.902508: step 6410, loss = 0.97 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-08 18:38:16.172522: step 6420, loss = 0.90 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-08 18:38:17.432687: step 6430, loss = 0.79 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:38:18.716499: step 6440, loss = 0.91 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:38:20.012612: step 6450, loss = 0.91 (987.6 examples/sec; 0.130 sec/batch)
2017-05-08 18:38:21.335623: step 6460, loss = 0.93 (967.5 examples/sec; 0.132 sec/batch)
2017-05-08 18:38:22.595582: step 6470, loss = 0.93 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-08 18:38:23.915895: step 6480, loss = 0.99 (969.5 examples/sec; 0.132 sec/batch)
2017-05-08 18:38:25.212238: step 6490, loss = 0.74 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:38:26.616425: step 6500, loss = 0.97 (911.6 examples/sec; 0.140 sec/batch)
2017-05-08 18:38:27.803617: step 6510, loss = 1.07 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-08 18:38:29.068978: step 6520, loss = 0.92 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:38:30.346278: step 6530, loss = 0.73 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:38:31.628811: step 6540, loss = 1.00 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:38:32.950740: step 6550, loss = 0.93 (968.3 examples/sec; 0.132 sec/batch)
2017-05-08 18:38:34.236379: step 6560, loss = 0.94 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:38:35.503721: step 6570, loss = 0.94 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:38:36.765837: step 6580, loss = 0.96 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-08 18:38:38.071003: step 6590, loss = 0.90 (980.7 examples/sec; 0.131 sec/batch)
2017-05-08 18:38:39.438419: step 6600, loss = 0.98 (936.1 examples/sec; 0.137 sec/batch)
2017-05-08 18:38:40.611538: step 6610, loss = 1.06 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-08 18:38:41.907040: step 6620, loss = 1.01 (988.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:38:43.193852: step 6630, loss = 1.05 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:38:44.468617: step 6640, loss = 0.85 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:38:45.730347: step 6650, loss = 0.78 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-08 18:38:47.021828: step 6660, loss = 0.95 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:38:48.291614: step 6670, loss = 0.93 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:38:49.582022: step 6680, loss = 0.97 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:38:50.883119: step 6690, loss = 1.09 (983.8 examples/sec; 0.130 sec/batch)
2017-05-08 18:38:52.279923: step 6700, loss = 0.78 (916.4 examples/sec; 0.140 sec/batch)
2017-05-08 18:38:53.452563: step 6710, loss = 0.85 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-08 18:38:54.735990: step 6720, loss = 0.93 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:38:55.988910: step 6730, loss = 0.94 (1021.6 examples/sec; 0.125 sec/batch)
2017-05-08 18:38:57.297748: step 6740, loss = 1.05 (978.0 examples/sec; 0.131 sec/batch)
2017-05-08 18:38:58.582916: step 6750, loss = 1.13 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:38:59.863664: step 6760, loss = 0.89 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:39:01.146476: step 6770, loss = 0.97 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:39:02.428361: step 6780, loss = 1.00 (998.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:39:03.699808: step 6790, loss = 1.02 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 18:39:05.080694: step 6800, loss = 0.87 (926.9 examples/sec; 0.138 sec/batch)
2017-05-08 18:39:06.256640: step 6810, loss = 0.94 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-08 18:39:07.586917: step 6820, loss = 0.86 (962.2 examples/sec; 0.133 sec/batch)
2017-05-08 18:39:08.868767: step 6830, loss = 1.20 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:39:10.157574: step 6840, loss = 1.00 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:39:11.450693: step 6850, loss = 1.00 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 144 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
 18:39:12.733292: step 6860, loss = 0.89 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:39:14.016470: step 6870, loss = 1.03 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:39:15.292089: step 6880, loss = 1.13 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:39:16.586088: step 6890, loss = 0.89 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:39:17.961151: step 6900, loss = 0.84 (930.9 examples/sec; 0.138 sec/batch)
2017-05-08 18:39:19.188731: step 6910, loss = 0.97 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-08 18:39:20.502244: step 6920, loss = 0.92 (974.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:39:21.766377: step 6930, loss = 0.98 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-08 18:39:23.034123: step 6940, loss = 0.97 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 18:39:24.353656: step 6950, loss = 1.07 (970.0 examples/sec; 0.132 sec/batch)
2017-05-08 18:39:25.642484: step 6960, loss = 0.87 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:39:26.929012: step 6970, loss = 0.97 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:39:28.218089: step 6980, loss = 0.80 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:39:29.474777: step 6990, loss = 1.00 (1018.5 examples/sec; 0.126 sec/batch)
2017-05-08 18:39:30.877948: step 7000, loss = 0.92 (912.2 examples/sec; 0.140 sec/batch)
2017-05-08 18:39:32.069161: step 7010, loss = 0.71 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-08 18:39:33.344836: step 7020, loss = 0.80 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:39:34.613089: step 7030, loss = 0.88 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:39:35.928808: step 7040, loss = 1.09 (972.9 examples/sec; 0.132 sec/batch)
2017-05-08 18:39:37.258665: step 7050, loss = 0.85 (962.5 examples/sec; 0.133 sec/batch)
2017-05-08 18:39:38.529528: step 7060, loss = 0.91 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:39:39.804231: step 7070, loss = 0.95 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:39:41.079476: step 7080, loss = 0.86 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:39:42.362350: step 7090, loss = 1.08 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:39:43.736165: step 7100, loss = 0.92 (931.7 examples/sec; 0.137 sec/batch)
2017-05-08 18:39:44.922097: step 7110, loss = 1.13 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-08 18:39:46.235588: step 7120, loss = 0.98 (974.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:39:47.503155: step 7130, loss = 1.02 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:39:48.822490: step 7140, loss = 0.90 (970.2 examples/sec; 0.132 sec/batch)
2017-05-08 18:39:50.134470: step 7150, loss = 1.02 (975.6 examples/sec; 0.131 sec/batch)
2017-05-08 18:39:51.422935: step 7160, loss = 0.88 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:39:52.701108: step 7170, loss = 0.90 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:39:54.036049: step 7180, loss = 0.90 (958.8 examples/sec; 0.133 sec/batch)
2017-05-08 18:39:55.331881: step 7190, loss = 0.90 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 18:39:56.713441: step 7200, loss = 0.90 (926.5 examples/sec; 0.138 sec/batch)
2017-05-08 18:39:57.904574: step 7210, loss = 0.86 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-08 18:39:59.191067: step 7220, loss = 1.03 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:40:00.441693: step 7230, loss = 0.97 (1023.5 examples/sec; 0.125 sec/batch)
2017-05-08 18:40:01.703888: step 7240, loss = 0.91 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-08 18:40:03.008620: step 7250, loss = 0.90 (981.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:40:04.266866: step 7260, loss = 1.13 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-08 18:40:05.541932: step 7270, loss = 0.96 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:40:06.858297: step 7280, loss = 0.85 (972.4 examples/sec; 0.132 sec/batch)
2017-05-08 18:40:08.130624: step 7290, loss = 0.84 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:40:09.508200: step 7300, loss = 0.85 (929.2 examples/sec; 0.138 sec/batch)
2017-05-08 18:40:10.685929: step 7310, loss = 0.96 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-08 18:40:11.973833: step 7320, loss = 0.91 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:40:13.248437: step 7330, loss = 1.00 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:40:14.542632: step 7340, loss = 0.84 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:40:15.797602: step 7350, loss = 0.85 (1019.9 examples/sec; 0.125 sec/batch)
2017-05-08 18:40:17.071387: step 7360, loss = 0.82 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-08 18:40:18.362996: step 7370, loss = 0.95 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:40:19.633125: step 7380, loss = 1.19 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:40:20.909728: step 7390, loss = 0.84 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:40:22.297691: step 7400, loss = 0.73 (922.2 examples/sec; 0.139 sec/batch)
2017-05-08 18:40:23.523919: step 7410, loss = 0.84 (1043.9 examples/sec; 0.123 sec/batch)
2017-05-08 18:40:24.776904: step 7420, loss = 0.84 (1021.6 examples/sec; 0.125 sec/batch)
2017-05-08 18:40:26.064749: step 7430, loss = 0.88 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:40:27.333860: step 7440, loss = 0.99 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:40:28.586407: step 7450, loss = 0.79 (1021.9 examples/sec; 0.125 sec/batch)
2017-05-08 18:40:29.871080: step 7460, loss = 0.90 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:40:31.150148: step 7470, loss = 0.76 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:40:32.423037: step 7480, loss = 0.99 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:40:33.683360: step 7490, loss = 0.72 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-08 18:40:35.093038: step 7500, loss = 1.01 (908.0 examples/sec; 0.141 sec/batch)
2017-05-08 18:40:36.284389: step 7510, loss = 0.88 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-08 18:40:37.550825: step 7520, loss = 0.84 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-08 18:40:38.871726: step 7530, loss = 1.11 (969.0 examples/sec; 0.132 sec/batch)
2017-05-08 18:40:40.150542: step 7540, loss = 0.81 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:40:41.420667: step 7550, loss = 1.01 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:40:42.669869: step 7560, loss = 1.08 (1024.7 examples/sec; 0.125 sec/batch)
2017-05-08 18:40:43.988605: step 7570, loss = 0.95 (970.6 examples/sec; 0.132 sec/batch)
2017-05-08 18:40:45.244957: step 7580, loss = 0.91 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-08 18:40:46.521815: step 7590, loss = 1.27 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:40:47.866240: step 7600, loss = 0.90 (952.1 examples/sec; 0.134 sec/batch)
2017-05-08 18:40:49.035861: step 7610, loss = 0.90 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-08 18:40:50.305824: step 7620, loss = 0.79 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-08 18:40:51.580147: step 7630, loss = 0.90 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:40:52.862499: step 7640, loss = 1.29 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:40:54.146975: step 7650, loss = 0.97 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:40:55.461412: step 7660, loss = 0.84 (973.8 examples/sec; 0.131 sec/batch)
2017-05-08 18:40:56.743601: step 7670, loss = 0.89 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:40:58.020007: step 7680, loss = 0.99 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:40:59.317661: step 7690, loss = 1.01 (986.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:41:00.688003: step 7700, loss = 0.95 (934.1 examples/sec; 0.137 sec/batch)
2017-05-08 18:41:01.890858: step 7710, loss = 0.73 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-08 18:41:03.159685: step 7720, loss = 0.90 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:41:04.458746: step 7730, loss = 0.87 (985.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:41:05.761145: step 7740, loss = 0.83 (982.8 examples/sec; 0.130 sec/batch)
2017-05-08 18:41:07.048003: step 7750, loss = 1.01 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:41:08.326019: step 7760, loss = 0.70 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:41:09.619545: step 7770, loss = 1.00 (989.E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 164 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
6 examples/sec; 0.129 sec/batch)
2017-05-08 18:41:10.915354: step 7780, loss = 0.83 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 18:41:12.216497: step 7790, loss = 0.97 (983.8 examples/sec; 0.130 sec/batch)
2017-05-08 18:41:13.616888: step 7800, loss = 0.92 (914.0 examples/sec; 0.140 sec/batch)
2017-05-08 18:41:14.846713: step 7810, loss = 0.75 (1040.8 examples/sec; 0.123 sec/batch)
2017-05-08 18:41:16.127102: step 7820, loss = 0.83 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:41:17.420895: step 7830, loss = 0.91 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:41:18.702684: step 7840, loss = 1.12 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:41:19.958005: step 7850, loss = 0.90 (1019.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:41:21.217141: step 7860, loss = 0.73 (1016.6 examples/sec; 0.126 sec/batch)
2017-05-08 18:41:22.501522: step 7870, loss = 0.85 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:41:23.766110: step 7880, loss = 0.81 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 18:41:25.064006: step 7890, loss = 0.82 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:41:26.433384: step 7900, loss = 0.91 (934.7 examples/sec; 0.137 sec/batch)
2017-05-08 18:41:27.615688: step 7910, loss = 0.92 (1082.6 examples/sec; 0.118 sec/batch)
2017-05-08 18:41:28.903318: step 7920, loss = 0.76 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:41:30.194425: step 7930, loss = 0.90 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:41:31.517169: step 7940, loss = 0.85 (967.7 examples/sec; 0.132 sec/batch)
2017-05-08 18:41:32.775537: step 7950, loss = 1.04 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-08 18:41:34.051804: step 7960, loss = 0.89 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:41:35.342688: step 7970, loss = 0.74 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:41:36.618685: step 7980, loss = 0.86 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:41:37.894714: step 7990, loss = 0.82 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:41:39.266145: step 8000, loss = 0.81 (933.3 examples/sec; 0.137 sec/batch)
2017-05-08 18:41:40.427771: step 8010, loss = 0.92 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-08 18:41:41.686517: step 8020, loss = 1.07 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-08 18:41:42.965621: step 8030, loss = 1.06 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:41:44.236304: step 8040, loss = 0.82 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:41:45.510385: step 8050, loss = 0.78 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-08 18:41:46.808396: step 8060, loss = 0.98 (986.1 examples/sec; 0.130 sec/batch)
2017-05-08 18:41:48.069121: step 8070, loss = 0.99 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-08 18:41:49.346888: step 8080, loss = 1.11 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:41:50.633891: step 8090, loss = 0.86 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:41:51.998545: step 8100, loss = 0.97 (938.0 examples/sec; 0.136 sec/batch)
2017-05-08 18:41:53.171514: step 8110, loss = 0.91 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-08 18:41:54.461518: step 8120, loss = 0.92 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:41:55.756487: step 8130, loss = 0.77 (988.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:41:57.033537: step 8140, loss = 0.82 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:41:58.325585: step 8150, loss = 0.85 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:41:59.603432: step 8160, loss = 0.85 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:42:00.856245: step 8170, loss = 0.87 (1021.7 examples/sec; 0.125 sec/batch)
2017-05-08 18:42:02.148317: step 8180, loss = 0.72 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:42:03.415617: step 8190, loss = 0.92 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:42:04.781032: step 8200, loss = 0.75 (937.4 examples/sec; 0.137 sec/batch)
2017-05-08 18:42:05.957672: step 8210, loss = 0.91 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-08 18:42:07.232925: step 8220, loss = 1.03 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:42:08.510429: step 8230, loss = 0.92 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:42:09.794485: step 8240, loss = 0.79 (996.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:42:11.084845: step 8250, loss = 0.91 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:42:12.354043: step 8260, loss = 0.72 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:42:13.632557: step 8270, loss = 0.81 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:42:14.904308: step 8280, loss = 0.96 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:42:16.181877: step 8290, loss = 1.07 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:42:17.562639: step 8300, loss = 0.86 (927.0 examples/sec; 0.138 sec/batch)
2017-05-08 18:42:18.825748: step 8310, loss = 0.78 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-08 18:42:20.069362: step 8320, loss = 0.73 (1029.2 examples/sec; 0.124 sec/batch)
2017-05-08 18:42:21.339772: step 8330, loss = 0.95 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:42:22.647491: step 8340, loss = 0.91 (978.8 examples/sec; 0.131 sec/batch)
2017-05-08 18:42:23.917967: step 8350, loss = 1.02 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:42:25.184298: step 8360, loss = 0.79 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:42:26.459538: step 8370, loss = 1.12 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:42:27.783606: step 8380, loss = 1.06 (966.7 examples/sec; 0.132 sec/batch)
2017-05-08 18:42:29.053053: step 8390, loss = 0.91 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:42:30.425766: step 8400, loss = 1.12 (932.5 examples/sec; 0.137 sec/batch)
2017-05-08 18:42:31.610690: step 8410, loss = 0.73 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-08 18:42:32.880602: step 8420, loss = 0.87 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-08 18:42:34.160934: step 8430, loss = 0.78 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:42:35.435356: step 8440, loss = 0.98 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-08 18:42:36.698276: step 8450, loss = 0.88 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-08 18:42:38.005724: step 8460, loss = 1.07 (979.0 examples/sec; 0.131 sec/batch)
2017-05-08 18:42:39.265897: step 8470, loss = 0.92 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:42:40.553058: step 8480, loss = 0.76 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:42:41.810214: step 8490, loss = 1.02 (1018.2 examples/sec; 0.126 sec/batch)
2017-05-08 18:42:43.206896: step 8500, loss = 1.07 (916.5 examples/sec; 0.140 sec/batch)
2017-05-08 18:42:44.365768: step 8510, loss = 1.01 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-08 18:42:45.639878: step 8520, loss = 1.02 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:42:46.938878: step 8530, loss = 0.86 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:42:48.233291: step 8540, loss = 0.72 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:42:49.493889: step 8550, loss = 0.91 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-08 18:42:50.777176: step 8560, loss = 0.83 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:42:52.056701: step 8570, loss = 1.02 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:42:53.335779: step 8580, loss = 1.11 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:42:54.629798: step 8590, loss = 0.89 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:42:56.005139: step 8600, loss = 0.90 (930.7 examples/sec; 0.138 sec/batch)
2017-05-08 18:42:57.207109: step 8610, loss = 0.89 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-08 18:42:58.491732: step 8620, loss = 0.94 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:42:59.755705: step 8630, loss = 0.81 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:43:01.030738: step 8640, loss = 0.71 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:43:02.321028: step 8650, loss = 1.04 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:43:03.612487: step 8660, loss = 0.82 (991.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:43:04.904808: step 8670, loss = 0.85 (990.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:43:06.189619: step 8680, loss = 1.04 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 184 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
18:43:07.449057: step 8690, loss = 0.99 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-08 18:43:08.820716: step 8700, loss = 0.93 (933.2 examples/sec; 0.137 sec/batch)
2017-05-08 18:43:10.026498: step 8710, loss = 0.80 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-08 18:43:11.306281: step 8720, loss = 0.81 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:43:12.595599: step 8730, loss = 0.89 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:43:13.885570: step 8740, loss = 0.90 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:43:15.154971: step 8750, loss = 0.94 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-08 18:43:16.418018: step 8760, loss = 0.79 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-08 18:43:17.699794: step 8770, loss = 0.78 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:43:19.007906: step 8780, loss = 1.01 (978.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:43:20.258406: step 8790, loss = 0.79 (1023.6 examples/sec; 0.125 sec/batch)
2017-05-08 18:43:21.628165: step 8800, loss = 0.89 (934.5 examples/sec; 0.137 sec/batch)
2017-05-08 18:43:22.800222: step 8810, loss = 0.98 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-08 18:43:24.065946: step 8820, loss = 0.85 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:43:25.367582: step 8830, loss = 0.86 (983.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:43:26.628231: step 8840, loss = 0.76 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-08 18:43:27.927520: step 8850, loss = 0.79 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:43:29.205499: step 8860, loss = 1.05 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:43:30.500052: step 8870, loss = 0.86 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:43:31.806950: step 8880, loss = 0.79 (979.4 examples/sec; 0.131 sec/batch)
2017-05-08 18:43:33.055170: step 8890, loss = 0.83 (1025.5 examples/sec; 0.125 sec/batch)
2017-05-08 18:43:34.454678: step 8900, loss = 0.96 (914.6 examples/sec; 0.140 sec/batch)
2017-05-08 18:43:35.652651: step 8910, loss = 0.88 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-08 18:43:36.918565: step 8920, loss = 0.74 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:43:38.215897: step 8930, loss = 0.83 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 18:43:39.487481: step 8940, loss = 0.92 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:43:40.804878: step 8950, loss = 0.97 (971.6 examples/sec; 0.132 sec/batch)
2017-05-08 18:43:42.092910: step 8960, loss = 0.91 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:43:43.381670: step 8970, loss = 0.89 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:43:44.672391: step 8980, loss = 0.92 (991.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:43:45.974615: step 8990, loss = 0.81 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 18:43:47.379587: step 9000, loss = 0.94 (911.1 examples/sec; 0.140 sec/batch)
2017-05-08 18:43:48.589571: step 9010, loss = 0.87 (1057.9 examples/sec; 0.121 sec/batch)
2017-05-08 18:43:49.891955: step 9020, loss = 0.89 (982.8 examples/sec; 0.130 sec/batch)
2017-05-08 18:43:51.164072: step 9030, loss = 1.00 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:43:52.421541: step 9040, loss = 0.86 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-08 18:43:53.720305: step 9050, loss = 0.82 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 18:43:55.003798: step 9060, loss = 0.80 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:43:56.275860: step 9070, loss = 1.07 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:43:57.567367: step 9080, loss = 0.86 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:43:58.845988: step 9090, loss = 0.92 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:44:00.211893: step 9100, loss = 0.91 (937.1 examples/sec; 0.137 sec/batch)
2017-05-08 18:44:01.395053: step 9110, loss = 0.90 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-08 18:44:02.655593: step 9120, loss = 0.92 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-08 18:44:03.926335: step 9130, loss = 0.94 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:44:05.219425: step 9140, loss = 1.01 (989.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:44:06.479519: step 9150, loss = 0.86 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-08 18:44:07.749049: step 9160, loss = 0.90 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:44:09.042502: step 9170, loss = 0.96 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:44:10.343700: step 9180, loss = 0.75 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 18:44:11.604672: step 9190, loss = 1.06 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-08 18:44:13.003712: step 9200, loss = 0.84 (914.9 examples/sec; 0.140 sec/batch)
2017-05-08 18:44:14.200861: step 9210, loss = 0.94 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-08 18:44:15.477124: step 9220, loss = 1.07 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:44:16.771184: step 9230, loss = 0.77 (989.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:44:18.038175: step 9240, loss = 0.99 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:44:19.337615: step 9250, loss = 0.78 (985.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:44:20.610993: step 9260, loss = 1.02 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:44:21.933526: step 9270, loss = 0.99 (967.8 examples/sec; 0.132 sec/batch)
2017-05-08 18:44:23.186599: step 9280, loss = 0.76 (1021.5 examples/sec; 0.125 sec/batch)
2017-05-08 18:44:24.458542: step 9290, loss = 0.95 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:44:25.865021: step 9300, loss = 0.78 (910.1 examples/sec; 0.141 sec/batch)
2017-05-08 18:44:27.058208: step 9310, loss = 0.82 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-08 18:44:28.331147: step 9320, loss = 0.78 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:44:29.598081: step 9330, loss = 0.97 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:44:30.919077: step 9340, loss = 0.95 (969.0 examples/sec; 0.132 sec/batch)
2017-05-08 18:44:32.187257: step 9350, loss = 1.07 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:44:33.519735: step 9360, loss = 1.11 (960.6 examples/sec; 0.133 sec/batch)
2017-05-08 18:44:34.760650: step 9370, loss = 0.86 (1031.5 examples/sec; 0.124 sec/batch)
2017-05-08 18:44:36.016051: step 9380, loss = 0.94 (1019.6 examples/sec; 0.126 sec/batch)
2017-05-08 18:44:37.304280: step 9390, loss = 0.69 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:44:38.691715: step 9400, loss = 0.88 (922.6 examples/sec; 0.139 sec/batch)
2017-05-08 18:44:39.889326: step 9410, loss = 0.94 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-08 18:44:41.181122: step 9420, loss = 0.88 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:44:42.474122: step 9430, loss = 0.96 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:44:43.746137: step 9440, loss = 0.77 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:44:45.022700: step 9450, loss = 1.01 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:44:46.328195: step 9460, loss = 0.97 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:44:47.624817: step 9470, loss = 0.78 (987.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:44:48.888888: step 9480, loss = 0.91 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-08 18:44:50.168943: step 9490, loss = 0.98 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:44:51.557197: step 9500, loss = 0.81 (922.0 examples/sec; 0.139 sec/batch)
2017-05-08 18:44:52.760088: step 9510, loss = 0.96 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-08 18:44:54.036313: step 9520, loss = 0.68 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:44:55.333176: step 9530, loss = 0.87 (987.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:44:56.675699: step 9540, loss = 1.08 (953.4 examples/sec; 0.134 sec/batch)
2017-05-08 18:44:57.963949: step 9550, loss = 0.86 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:44:59.277388: step 9560, loss = 0.73 (974.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:45:00.566693: step 9570, loss = 0.82 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:45:01.905938: step 9580, loss = 0.85 (955.8 examples/sec; 0.134 sec/batch)
2017-05-08 18:45:03.173153: step 9590, loss = 0.90 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:45:04.536162: step 9600, loss = 0.95 (939.1 eE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 205 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
xamples/sec; 0.136 sec/batch)
2017-05-08 18:45:05.718181: step 9610, loss = 0.91 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-08 18:45:07.005650: step 9620, loss = 0.85 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:45:08.293924: step 9630, loss = 0.93 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:45:09.548200: step 9640, loss = 0.76 (1020.5 examples/sec; 0.125 sec/batch)
2017-05-08 18:45:10.843167: step 9650, loss = 0.82 (988.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:45:12.101647: step 9660, loss = 1.05 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-08 18:45:13.375588: step 9670, loss = 0.97 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:45:14.672151: step 9680, loss = 0.92 (987.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:45:15.982834: step 9690, loss = 0.85 (976.6 examples/sec; 0.131 sec/batch)
2017-05-08 18:45:17.383232: step 9700, loss = 1.00 (914.0 examples/sec; 0.140 sec/batch)
2017-05-08 18:45:18.589209: step 9710, loss = 1.10 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-08 18:45:19.921973: step 9720, loss = 0.86 (960.4 examples/sec; 0.133 sec/batch)
2017-05-08 18:45:21.227632: step 9730, loss = 0.81 (980.4 examples/sec; 0.131 sec/batch)
2017-05-08 18:45:22.511614: step 9740, loss = 1.01 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:45:23.778072: step 9750, loss = 0.84 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-08 18:45:25.072546: step 9760, loss = 0.72 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:45:26.339841: step 9770, loss = 0.96 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:45:27.634895: step 9780, loss = 0.82 (988.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:45:28.917248: step 9790, loss = 0.78 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:45:30.305038: step 9800, loss = 0.89 (922.3 examples/sec; 0.139 sec/batch)
2017-05-08 18:45:31.492406: step 9810, loss = 0.77 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-08 18:45:32.793595: step 9820, loss = 0.69 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 18:45:34.086383: step 9830, loss = 0.96 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:45:35.355534: step 9840, loss = 0.82 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:45:36.647584: step 9850, loss = 0.93 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:45:37.962924: step 9860, loss = 0.87 (973.1 examples/sec; 0.132 sec/batch)
2017-05-08 18:45:39.245427: step 9870, loss = 0.84 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:45:40.511996: step 9880, loss = 0.79 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:45:41.789518: step 9890, loss = 0.94 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:45:43.180139: step 9900, loss = 0.76 (920.4 examples/sec; 0.139 sec/batch)
2017-05-08 18:45:44.349438: step 9910, loss = 0.91 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-08 18:45:45.617706: step 9920, loss = 0.96 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:45:46.887252: step 9930, loss = 0.93 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:45:48.146152: step 9940, loss = 0.91 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-08 18:45:49.421428: step 9950, loss = 0.75 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:45:50.728537: step 9960, loss = 0.86 (979.3 examples/sec; 0.131 sec/batch)
2017-05-08 18:45:51.984896: step 9970, loss = 0.87 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-08 18:45:53.284240: step 9980, loss = 0.86 (985.1 examples/sec; 0.130 sec/batch)
2017-05-08 18:45:54.545916: step 9990, loss = 0.85 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-08 18:45:55.915021: step 10000, loss = 0.84 (934.9 examples/sec; 0.137 sec/batch)
2017-05-08 18:45:57.126940: step 10010, loss = 0.97 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-08 18:45:58.397555: step 10020, loss = 1.05 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-08 18:45:59.684005: step 10030, loss = 0.71 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:46:00.948426: step 10040, loss = 0.81 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-08 18:46:02.248680: step 10050, loss = 0.74 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:46:03.556914: step 10060, loss = 0.78 (978.4 examples/sec; 0.131 sec/batch)
2017-05-08 18:46:04.871227: step 10070, loss = 0.86 (973.9 examples/sec; 0.131 sec/batch)
2017-05-08 18:46:06.175735: step 10080, loss = 0.74 (981.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:46:07.476009: step 10090, loss = 0.97 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:46:08.857079: step 10100, loss = 0.97 (926.8 examples/sec; 0.138 sec/batch)
2017-05-08 18:46:10.032207: step 10110, loss = 1.06 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-08 18:46:11.327939: step 10120, loss = 0.87 (987.9 examples/sec; 0.130 sec/batch)
2017-05-08 18:46:12.598887: step 10130, loss = 0.76 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:46:13.888594: step 10140, loss = 1.05 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:46:15.157581: step 10150, loss = 0.81 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-08 18:46:16.465752: step 10160, loss = 0.79 (978.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:46:17.769420: step 10170, loss = 0.98 (981.8 examples/sec; 0.130 sec/batch)
2017-05-08 18:46:19.044359: step 10180, loss = 0.91 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:46:20.320994: step 10190, loss = 0.95 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:46:21.713625: step 10200, loss = 0.95 (919.1 examples/sec; 0.139 sec/batch)
2017-05-08 18:46:22.884362: step 10210, loss = 0.82 (1093.3 examples/sec; 0.117 sec/batch)
2017-05-08 18:46:24.160739: step 10220, loss = 0.83 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:46:25.443240: step 10230, loss = 0.85 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:46:26.712440: step 10240, loss = 0.70 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:46:27.977804: step 10250, loss = 0.91 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:46:29.270125: step 10260, loss = 0.84 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:46:30.541233: step 10270, loss = 0.98 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:46:31.873636: step 10280, loss = 0.77 (960.7 examples/sec; 0.133 sec/batch)
2017-05-08 18:46:33.175588: step 10290, loss = 1.29 (983.1 examples/sec; 0.130 sec/batch)
2017-05-08 18:46:34.546055: step 10300, loss = 0.99 (934.0 examples/sec; 0.137 sec/batch)
2017-05-08 18:46:35.739013: step 10310, loss = 0.99 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-08 18:46:37.009886: step 10320, loss = 0.79 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:46:38.269514: step 10330, loss = 0.97 (1016.2 examples/sec; 0.126 sec/batch)
2017-05-08 18:46:39.572803: step 10340, loss = 1.03 (982.1 examples/sec; 0.130 sec/batch)
2017-05-08 18:46:40.877242: step 10350, loss = 0.84 (981.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:46:42.186174: step 10360, loss = 0.85 (977.9 examples/sec; 0.131 sec/batch)
2017-05-08 18:46:43.478650: step 10370, loss = 0.95 (990.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:46:44.775057: step 10380, loss = 1.02 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:46:46.080584: step 10390, loss = 0.88 (980.4 examples/sec; 0.131 sec/batch)
2017-05-08 18:46:47.478145: step 10400, loss = 0.90 (915.9 examples/sec; 0.140 sec/batch)
2017-05-08 18:46:48.716914: step 10410, loss = 0.95 (1033.3 examples/sec; 0.124 sec/batch)
2017-05-08 18:46:49.993303: step 10420, loss = 0.94 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:46:51.301628: step 10430, loss = 0.82 (978.4 examples/sec; 0.131 sec/batch)
2017-05-08 18:46:52.611303: step 10440, loss = 0.80 (977.4 examples/sec; 0.131 sec/batch)
2017-05-08 18:46:53.889525: step 10450, loss = 0.91 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:46:55.152505: step 10460, loss = 0.91 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-08 18:46:56.432133: step 10470, loss = 0.80 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:46:57.724187: step 10480, loss = 0.77 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:46:58.991439: step 10490, loss = 0.93 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:47:00.363470: step 10500, loss = 0.82 (932.9 examples/sec; 0.137 sec/batch)
2017-05-08 18:47:01.534706: step 10510, loss = 0.88 (1092.8 eE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 225 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
xamples/sec; 0.117 sec/batch)
2017-05-08 18:47:02.825759: step 10520, loss = 0.77 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:47:04.089543: step 10530, loss = 0.89 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-08 18:47:05.374770: step 10540, loss = 0.92 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:47:06.648675: step 10550, loss = 0.91 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:47:07.904775: step 10560, loss = 0.91 (1019.0 examples/sec; 0.126 sec/batch)
2017-05-08 18:47:09.233418: step 10570, loss = 0.87 (963.4 examples/sec; 0.133 sec/batch)
2017-05-08 18:47:10.517868: step 10580, loss = 0.86 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:47:11.779646: step 10590, loss = 0.98 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-08 18:47:13.139587: step 10600, loss = 1.03 (941.2 examples/sec; 0.136 sec/batch)
2017-05-08 18:47:14.323735: step 10610, loss = 0.95 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-08 18:47:15.622736: step 10620, loss = 0.78 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:47:16.906478: step 10630, loss = 0.71 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:47:18.165031: step 10640, loss = 0.73 (1017.0 examples/sec; 0.126 sec/batch)
2017-05-08 18:47:19.428921: step 10650, loss = 1.03 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:47:20.694959: step 10660, loss = 0.69 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:47:21.942575: step 10670, loss = 0.93 (1026.0 examples/sec; 0.125 sec/batch)
2017-05-08 18:47:23.211089: step 10680, loss = 0.82 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:47:24.457057: step 10690, loss = 0.93 (1027.3 examples/sec; 0.125 sec/batch)
2017-05-08 18:47:25.842966: step 10700, loss = 0.82 (923.6 examples/sec; 0.139 sec/batch)
2017-05-08 18:47:27.087684: step 10710, loss = 0.85 (1028.4 examples/sec; 0.124 sec/batch)
2017-05-08 18:47:28.295835: step 10720, loss = 0.79 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-08 18:47:29.564999: step 10730, loss = 0.85 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:47:30.837552: step 10740, loss = 0.81 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:47:32.121435: step 10750, loss = 0.87 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:47:33.415902: step 10760, loss = 0.78 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:47:34.690228: step 10770, loss = 0.98 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:47:35.954071: step 10780, loss = 0.82 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-08 18:47:37.241736: step 10790, loss = 0.82 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:47:38.585120: step 10800, loss = 0.77 (952.8 examples/sec; 0.134 sec/batch)
2017-05-08 18:47:39.763943: step 10810, loss = 0.84 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-08 18:47:41.069504: step 10820, loss = 0.87 (980.4 examples/sec; 0.131 sec/batch)
2017-05-08 18:47:42.340384: step 10830, loss = 0.76 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:47:43.608432: step 10840, loss = 0.73 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-08 18:47:44.892163: step 10850, loss = 0.72 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:47:46.191278: step 10860, loss = 0.95 (985.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:47:47.457068: step 10870, loss = 0.82 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:47:48.747378: step 10880, loss = 0.83 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:47:50.021636: step 10890, loss = 0.88 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:47:51.401242: step 10900, loss = 0.92 (927.8 examples/sec; 0.138 sec/batch)
2017-05-08 18:47:52.593554: step 10910, loss = 0.91 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-08 18:47:53.840102: step 10920, loss = 0.96 (1026.8 examples/sec; 0.125 sec/batch)
2017-05-08 18:47:55.124672: step 10930, loss = 1.02 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:47:56.370308: step 10940, loss = 0.92 (1027.6 examples/sec; 0.125 sec/batch)
2017-05-08 18:47:57.643439: step 10950, loss = 0.99 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 18:47:58.892745: step 10960, loss = 0.95 (1024.6 examples/sec; 0.125 sec/batch)
2017-05-08 18:48:00.171356: step 10970, loss = 0.85 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:48:01.451957: step 10980, loss = 0.87 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:48:02.740281: step 10990, loss = 0.90 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:48:04.126813: step 11000, loss = 0.84 (923.2 examples/sec; 0.139 sec/batch)
2017-05-08 18:48:05.316256: step 11010, loss = 0.66 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-08 18:48:06.587705: step 11020, loss = 0.86 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 18:48:07.839328: step 11030, loss = 0.88 (1022.7 examples/sec; 0.125 sec/batch)
2017-05-08 18:48:09.134990: step 11040, loss = 0.94 (987.9 examples/sec; 0.130 sec/batch)
2017-05-08 18:48:10.399779: step 11050, loss = 0.81 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-08 18:48:11.653016: step 11060, loss = 0.92 (1021.4 examples/sec; 0.125 sec/batch)
2017-05-08 18:48:12.925115: step 11070, loss = 0.85 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:48:14.190817: step 11080, loss = 0.88 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:48:15.464854: step 11090, loss = 0.77 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-08 18:48:16.827034: step 11100, loss = 0.82 (939.7 examples/sec; 0.136 sec/batch)
2017-05-08 18:48:18.123800: step 11110, loss = 0.78 (987.1 examples/sec; 0.130 sec/batch)
2017-05-08 18:48:19.348353: step 11120, loss = 0.84 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-08 18:48:20.634123: step 11130, loss = 0.96 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:48:21.935928: step 11140, loss = 0.77 (983.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:48:23.229901: step 11150, loss = 1.05 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:48:24.518625: step 11160, loss = 0.85 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:48:25.769119: step 11170, loss = 0.84 (1023.6 examples/sec; 0.125 sec/batch)
2017-05-08 18:48:27.027005: step 11180, loss = 0.92 (1017.6 examples/sec; 0.126 sec/batch)
2017-05-08 18:48:28.317848: step 11190, loss = 0.98 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:48:29.676306: step 11200, loss = 0.86 (942.2 examples/sec; 0.136 sec/batch)
2017-05-08 18:48:30.866749: step 11210, loss = 0.69 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-08 18:48:32.125634: step 11220, loss = 0.89 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-08 18:48:33.400484: step 11230, loss = 1.00 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:48:34.683422: step 11240, loss = 0.76 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:48:35.958046: step 11250, loss = 0.83 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:48:37.231142: step 11260, loss = 0.84 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 18:48:38.513917: step 11270, loss = 0.78 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:48:39.781842: step 11280, loss = 0.91 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:48:41.098757: step 11290, loss = 1.01 (972.0 examples/sec; 0.132 sec/batch)
2017-05-08 18:48:42.467380: step 11300, loss = 0.84 (935.2 examples/sec; 0.137 sec/batch)
2017-05-08 18:48:43.658939: step 11310, loss = 0.89 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-08 18:48:44.948638: step 11320, loss = 0.76 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:48:46.215729: step 11330, loss = 0.89 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:48:47.461628: step 11340, loss = 0.80 (1027.4 examples/sec; 0.125 sec/batch)
2017-05-08 18:48:48.760115: step 11350, loss = 0.80 (985.8 examples/sec; 0.130 sec/batch)
2017-05-08 18:48:50.015191: step 11360, loss = 0.91 (1019.9 examples/sec; 0.126 sec/batch)
2017-05-08 18:48:51.296149: step 11370, loss = 0.73 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:48:52.574933: step 11380, loss = 1.09 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:48:53.847492: step 11390, loss = 0.84 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:48:55.207359: step 11400, loss = 0.91 (941.3 examples/sec; 0.136 sec/batch)
2017-05-08 18:48:56.383612: step 11410, loss = 0.89 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-08 18:48:57.653408: step 11420, loss = 0.84 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:48:58.932854: step 11430, loss = 0.73 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:49:00.188085: step 11440, loss = 0.94 (1019.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:49:01.428114: step 11450, loss = 1.01 (1032.2 examples/sec; 0.124 sec/batch)
2017-05-08 18:49:02.690576: step 11460, loss = 0.75 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-08 18:49:03.949664: step 11470, loss = 0.93 (1016.6 examples/sec; 0.126 sec/batch)
2017-05-08 18:49:05.228848: step 11480, loss = 0.85 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:49:06.537247: step 11490, loss = 0.78 (978.3 examples/sec; 0.131 sec/batch)
2017-05-08 18:49:07.949422: step 11500, loss = 0.76 (906.4 examples/sec; 0.141 sec/batch)
2017-05-08 18:49:09.111091: step 11510, loss = 0.84 (1101.9 examples/sec; 0.116 sec/batch)
2017-05-08 18:49:10.387185: step 11520, loss = 0.89 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:49:11.677991: step 11530, loss = 0.97 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:49:12.932434: step 11540, loss = 1.02 (1020.4 examples/sec; 0.125 sec/batch)
2017-05-08 18:49:14.213243: step 11550, loss = 0.82 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:49:15.518499: step 11560, loss = 0.80 (980.7 examples/sec; 0.131 sec/batch)
2017-05-08 18:49:16.793700: step 11570, loss = 1.00 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:49:18.094119: step 11580, loss = 0.93 (984.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:49:19.399573: step 11590, loss = 0.88 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:49:20.752199: step 11600, loss = 0.80 (946.3 examples/sec; 0.135 sec/batch)
2017-05-08 18:49:21.953929: step 11610, loss = 1.00 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-08 18:49:23.198839: step 11620, loss = 0.88 (1028.2 examples/sec; 0.124 sec/batch)
2017-05-08 18:49:24.476505: step 11630, loss = 0.76 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:49:25.769263: step 11640, loss = 1.15 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:49:27.020135: step 11650, loss = 0.76 (1023.3 examples/sec; 0.125 sec/batch)
2017-05-08 18:49:28.285930: step 11660, loss = 0.85 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:49:29.575009: step 11670, loss = 0.78 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:49:30.835209: step 11680, loss = 1.03 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:49:32.095339: step 11690, loss = 0.92 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-08 18:49:33.478795: step 11700, loss = 0.93 (925.2 examples/sec; 0.138 sec/batch)
2017-05-08 18:49:34.668940: step 11710, loss = 0.80 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-08 18:49:35.915771: step 11720, loss = 0.84 (1026.6 examples/sec; 0.125 sec/batch)
2017-05-08 18:49:37.204790: step 11730, loss = 0.91 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:49:38.486005: step 11740, loss = 0.79 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:49:39.737194: step 11750, loss = 0.86 (1023.0 examples/sec; 0.125 sec/batch)
2017-05-08 18:49:40.997500: step 11760, loss = 0.94 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-08 18:49:42.257278: step 11770, loss = 0.86 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-08 18:49:43.514388: step 11780, loss = 0.91 (1018.2 examples/sec; 0.126 sec/batch)
2017-05-08 18:49:44.791381: step 11790, loss = 0.94 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:49:46.160115: step 11800, loss = 0.77 (935.2 examples/sec; 0.137 sec/batch)
2017-05-08 18:49:47.350771: step 11810, loss = 0.83 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-08 18:49:48.617654: step 11820, loss = 1.09 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-08 18:49:49.898260: step 11830, loss = 0.70 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:49:51.152410: step 11840, loss = 0.98 (1020.6 examples/sec; 0.125 sec/batch)
2017-05-08 18:49:52.439887: step 11850, loss = 0.82 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:49:53.707579: step 11860, loss = 0.88 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 18:49:54.996551:E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 245 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
 step 11870, loss = 0.85 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:49:56.250178: step 11880, loss = 0.95 (1021.1 examples/sec; 0.125 sec/batch)
2017-05-08 18:49:57.506965: step 11890, loss = 0.81 (1018.5 examples/sec; 0.126 sec/batch)
2017-05-08 18:49:58.909410: step 11900, loss = 0.97 (912.7 examples/sec; 0.140 sec/batch)
2017-05-08 18:50:00.099489: step 11910, loss = 0.97 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-08 18:50:01.359689: step 11920, loss = 0.93 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:50:02.647186: step 11930, loss = 0.84 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:50:03.901037: step 11940, loss = 0.77 (1020.8 examples/sec; 0.125 sec/batch)
2017-05-08 18:50:05.176203: step 11950, loss = 1.08 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:50:06.466151: step 11960, loss = 0.82 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:50:07.738968: step 11970, loss = 0.84 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:50:08.991179: step 11980, loss = 0.88 (1022.2 examples/sec; 0.125 sec/batch)
2017-05-08 18:50:10.273055: step 11990, loss = 0.76 (998.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:50:11.620010: step 12000, loss = 0.82 (950.3 examples/sec; 0.135 sec/batch)
2017-05-08 18:50:12.772398: step 12010, loss = 0.80 (1110.7 examples/sec; 0.115 sec/batch)
2017-05-08 18:50:14.056548: step 12020, loss = 0.72 (996.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:50:15.347920: step 12030, loss = 0.70 (991.2 examples/sec; 0.129 sec/batch)
2017-05-08 18:50:16.614787: step 12040, loss = 0.93 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-08 18:50:17.898099: step 12050, loss = 0.93 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:50:19.151758: step 12060, loss = 0.85 (1021.0 examples/sec; 0.125 sec/batch)
2017-05-08 18:50:20.410024: step 12070, loss = 0.68 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-08 18:50:21.677673: step 12080, loss = 0.85 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 18:50:22.936644: step 12090, loss = 0.82 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:50:24.316235: step 12100, loss = 0.82 (927.8 examples/sec; 0.138 sec/batch)
2017-05-08 18:50:25.471269: step 12110, loss = 0.89 (1108.2 examples/sec; 0.116 sec/batch)
2017-05-08 18:50:26.738021: step 12120, loss = 0.95 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:50:28.001913: step 12130, loss = 0.83 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:50:29.286432: step 12140, loss = 0.71 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:50:30.530167: step 12150, loss = 0.72 (1029.2 examples/sec; 0.124 sec/batch)
2017-05-08 18:50:31.803712: step 12160, loss = 0.67 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:50:33.137717: step 12170, loss = 0.84 (959.5 examples/sec; 0.133 sec/batch)
2017-05-08 18:50:34.397896: step 12180, loss = 0.65 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:50:35.706595: step 12190, loss = 0.89 (978.1 examples/sec; 0.131 sec/batch)
2017-05-08 18:50:37.058833: step 12200, loss = 0.86 (946.6 examples/sec; 0.135 sec/batch)
2017-05-08 18:50:38.226125: step 12210, loss = 0.94 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-08 18:50:39.500574: step 12220, loss = 0.93 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-08 18:50:40.765786: step 12230, loss = 0.79 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-08 18:50:42.086004: step 12240, loss = 0.75 (969.5 examples/sec; 0.132 sec/batch)
2017-05-08 18:50:43.366354: step 12250, loss = 0.88 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:50:44.600287: step 12260, loss = 1.01 (1037.3 examples/sec; 0.123 sec/batch)
2017-05-08 18:50:45.874546: step 12270, loss = 0.71 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:50:47.139918: step 12280, loss = 0.89 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:50:48.427907: step 12290, loss = 0.91 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:50:49.794038: step 12300, loss = 0.78 (937.0 examples/sec; 0.137 sec/batch)
2017-05-08 18:50:50.975289: step 12310, loss = 0.83 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-08 18:50:52.238646: step 12320, loss = 0.74 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-08 18:50:53.511916: step 12330, loss = 0.75 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:50:54.777874: step 12340, loss = 0.78 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:50:56.034397: step 12350, loss = 0.76 (1018.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:50:57.282300: step 12360, loss = 0.89 (1025.7 examples/sec; 0.125 sec/batch)
2017-05-08 18:50:58.578017: step 12370, loss = 0.87 (987.9 examples/sec; 0.130 sec/batch)
2017-05-08 18:50:59.828754: step 12380, loss = 0.74 (1023.4 examples/sec; 0.125 sec/batch)
2017-05-08 18:51:01.091752: step 12390, loss = 0.81 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-08 18:51:02.470892: step 12400, loss = 0.89 (928.1 examples/sec; 0.138 sec/batch)
2017-05-08 18:51:03.667320: step 12410, loss = 0.83 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-08 18:51:04.918430: step 12420, loss = 0.86 (1023.1 examples/sec; 0.125 sec/batch)
2017-05-08 18:51:06.189167: step 12430, loss = 0.69 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:51:07.448489: step 12440, loss = 0.80 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-08 18:51:08.718974: step 12450, loss = 0.87 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:51:09.978275: step 12460, loss = 0.73 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-08 18:51:11.272655: step 12470, loss = 1.11 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:51:12.511057: step 12480, loss = 0.77 (1033.6 examples/sec; 0.124 sec/batch)
2017-05-08 18:51:13.796952: step 12490, loss = 0.83 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:51:15.167913: step 12500, loss = 0.71 (933.7 examples/sec; 0.137 sec/batch)
2017-05-08 18:51:16.354880: step 12510, loss = 0.88 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-08 18:51:17.631494: step 12520, loss = 0.83 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:51:18.872848: step 12530, loss = 0.92 (1031.1 examples/sec; 0.124 sec/batch)
2017-05-08 18:51:20.187910: step 12540, loss = 0.94 (973.3 examples/sec; 0.132 sec/batch)
2017-05-08 18:51:21.447996: step 12550, loss = 0.89 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-08 18:51:22.711748: step 12560, loss = 0.96 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-08 18:51:23.978508: step 12570, loss = 0.90 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:51:25.258006: step 12580, loss = 0.92 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:51:26.524241: step 12590, loss = 0.70 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-08 18:51:27.876708: step 12600, loss = 0.86 (946.4 examples/sec; 0.135 sec/batch)
2017-05-08 18:51:29.063385: step 12610, loss = 0.89 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-08 18:51:30.354360: step 12620, loss = 0.95 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:51:31.621924: step 12630, loss = 0.91 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:51:32.895335: step 12640, loss = 0.80 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:51:34.173435: step 12650, loss = 0.82 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:51:35.458984: step 12660, loss = 1.05 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:51:36.710189: step 12670, loss = 0.98 (1023.0 examples/sec; 0.125 sec/batch)
2017-05-08 18:51:37.976355: step 12680, loss = 0.84 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-08 18:51:39.250884: step 12690, loss = 0.95 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:51:40.608778: step 12700, loss = 0.85 (942.6 examples/sec; 0.136 sec/batch)
2017-05-08 18:51:41.766121: step 12710, loss = 0.86 (1106.0 examples/sec; 0.116 sec/batch)
2017-05-08 18:51:43.047728: step 12720, loss = 0.87 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:51:44.328944: step 12730, loss = 0.81 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:51:45.598211: step 12740, loss = 0.82 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:51:46.880169: step 12750, loss = 0.75 (998.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:51:48.162307: step 12760, loss = 0.73 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:51:49.418676: step 12770, loss = 0.78 (1018.8 eE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 265 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
xamples/sec; 0.126 sec/batch)
2017-05-08 18:51:50.726265: step 12780, loss = 0.80 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 18:51:52.017472: step 12790, loss = 0.74 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:51:53.365001: step 12800, loss = 0.93 (949.9 examples/sec; 0.135 sec/batch)
2017-05-08 18:51:54.540833: step 12810, loss = 0.75 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-08 18:51:55.821632: step 12820, loss = 0.77 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:51:57.118555: step 12830, loss = 1.12 (987.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:51:58.391357: step 12840, loss = 0.75 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:51:59.645971: step 12850, loss = 0.90 (1020.2 examples/sec; 0.125 sec/batch)
2017-05-08 18:52:00.910839: step 12860, loss = 0.89 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-08 18:52:02.187515: step 12870, loss = 0.94 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:52:03.455113: step 12880, loss = 0.86 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:52:04.754346: step 12890, loss = 0.90 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:52:06.116672: step 12900, loss = 0.69 (939.6 examples/sec; 0.136 sec/batch)
2017-05-08 18:52:07.366981: step 12910, loss = 0.98 (1023.7 examples/sec; 0.125 sec/batch)
2017-05-08 18:52:08.590025: step 12920, loss = 0.81 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-08 18:52:09.874068: step 12930, loss = 0.91 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:52:11.182627: step 12940, loss = 0.97 (978.2 examples/sec; 0.131 sec/batch)
2017-05-08 18:52:12.375791: step 12950, loss = 0.94 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-08 18:52:13.647066: step 12960, loss = 0.86 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 18:52:14.907017: step 12970, loss = 0.88 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-08 18:52:16.183383: step 12980, loss = 0.91 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:52:17.438638: step 12990, loss = 0.81 (1019.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:52:18.826603: step 13000, loss = 1.00 (922.2 examples/sec; 0.139 sec/batch)
2017-05-08 18:52:20.023242: step 13010, loss = 0.78 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-08 18:52:21.288823: step 13020, loss = 1.13 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-08 18:52:22.573603: step 13030, loss = 1.04 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:52:23.855621: step 13040, loss = 0.78 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:52:25.154910: step 13050, loss = 0.74 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:52:26.429174: step 13060, loss = 0.75 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:52:27.695509: step 13070, loss = 1.03 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:52:28.962189: step 13080, loss = 0.80 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:52:30.228110: step 13090, loss = 0.81 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:52:31.597864: step 13100, loss = 0.95 (934.5 examples/sec; 0.137 sec/batch)
2017-05-08 18:52:32.754023: step 13110, loss = 0.79 (1107.1 examples/sec; 0.116 sec/batch)
2017-05-08 18:52:34.035672: step 13120, loss = 0.89 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:52:35.326387: step 13130, loss = 0.78 (991.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:52:36.586726: step 13140, loss = 0.77 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-08 18:52:37.859170: step 13150, loss = 0.75 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-08 18:52:39.119394: step 13160, loss = 0.94 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:52:40.395806: step 13170, loss = 0.77 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:52:41.702634: step 13180, loss = 0.69 (979.5 examples/sec; 0.131 sec/batch)
2017-05-08 18:52:42.992399: step 13190, loss = 0.67 (992.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:52:44.333145: step 13200, loss = 0.86 (954.7 examples/sec; 0.134 sec/batch)
2017-05-08 18:52:45.535021: step 13210, loss = 0.76 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-08 18:52:46.829251: step 13220, loss = 0.87 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:52:48.077750: step 13230, loss = 0.76 (1025.2 examples/sec; 0.125 sec/batch)
2017-05-08 18:52:49.355549: step 13240, loss = 0.84 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:52:50.621913: step 13250, loss = 0.91 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:52:51.902798: step 13260, loss = 0.90 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:52:53.156784: step 13270, loss = 0.66 (1020.7 examples/sec; 0.125 sec/batch)
2017-05-08 18:52:54.458423: step 13280, loss = 0.90 (983.4 examples/sec; 0.130 sec/batch)
2017-05-08 18:52:55.771571: step 13290, loss = 0.83 (974.8 examples/sec; 0.131 sec/batch)
2017-05-08 18:52:57.106109: step 13300, loss = 0.90 (959.1 examples/sec; 0.133 sec/batch)
2017-05-08 18:52:58.280687: step 13310, loss = 0.95 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-08 18:52:59.570820: step 13320, loss = 1.02 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:53:00.849747: step 13330, loss = 0.75 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:53:02.106132: step 13340, loss = 0.77 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-08 18:53:03.377026: step 13350, loss = 0.99 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:53:04.651597: step 13360, loss = 0.73 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:53:05.936827: step 13370, loss = 0.72 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 18:53:07.198281: step 13380, loss = 0.76 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:53:08.458375: step 13390, loss = 0.88 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-08 18:53:09.850518: step 13400, loss = 0.90 (919.4 examples/sec; 0.139 sec/batch)
2017-05-08 18:53:11.024595: step 13410, loss = 0.79 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-08 18:53:12.306339: step 13420, loss = 0.82 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:53:13.540888: step 13430, loss = 0.78 (1036.8 examples/sec; 0.123 sec/batch)
2017-05-08 18:53:14.851563: step 13440, loss = 0.97 (976.6 examples/sec; 0.131 sec/batch)
2017-05-08 18:53:16.126563: step 13450, loss = 0.85 (1003.9 examples/sec; 0.127 sec/batch)
2017-05-08 18:53:17.410561: step 13460, loss = 0.84 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:53:18.658412: step 13470, loss = 0.84 (1025.7 examples/sec; 0.125 sec/batch)
2017-05-08 18:53:19.914762: step 13480, loss = 0.86 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-08 18:53:21.199948: step 13490, loss = 0.78 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:53:22.578420: step 13500, loss = 0.88 (928.6 examples/sec; 0.138 sec/batch)
2017-05-08 18:53:23.736203: step 13510, loss = 0.86 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-08 18:53:25.019842: step 13520, loss = 0.94 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:53:26.285210: step 13530, loss = 0.88 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:53:27.566373: step 13540, loss = 0.89 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:53:28.848138: step 13550, loss = 0.82 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:53:30.135777: step 13560, loss = 0.80 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:53:31.378392: step 13570, loss = 0.69 (1030.1 examples/sec; 0.124 sec/batch)
2017-05-08 18:53:32.653257: step 13580, loss = 0.99 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:53:33.908886: step 13590, loss = 1.08 (1019.4 examples/sec; 0.126 sec/batch)
2017-05-08 18:53:35.269990: step 13600, loss = 0.85 (940.4 examples/sec; 0.136 sec/batch)
2017-05-08 18:53:36.449163: step 13610, loss = 0.76 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-08 18:53:37.719547: step 13620, loss = 0.90 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:53:39.002590: step 13630, loss = 0.77 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:53:40.271789: step 13640, loss = 0.81 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:53:41.563639: step 13650, loss = 0.89 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:53:42.863013: step 13660, loss = 0.77 (985.1 examples/sec; 0.130 sec/batch)
2017-05-08 18:53:44.143896: step 13670, loss = 0.73 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 18E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 285 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
:53:45.396232: step 13680, loss = 0.95 (1022.1 examples/sec; 0.125 sec/batch)
2017-05-08 18:53:46.665740: step 13690, loss = 0.88 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:53:48.007944: step 13700, loss = 0.79 (953.7 examples/sec; 0.134 sec/batch)
2017-05-08 18:53:49.211660: step 13710, loss = 0.96 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-08 18:53:50.474385: step 13720, loss = 0.95 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:53:51.782433: step 13730, loss = 0.82 (978.6 examples/sec; 0.131 sec/batch)
2017-05-08 18:53:53.046896: step 13740, loss = 0.92 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-08 18:53:54.314652: step 13750, loss = 0.85 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 18:53:55.583545: step 13760, loss = 0.87 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:53:56.847518: step 13770, loss = 0.68 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:53:58.114889: step 13780, loss = 1.02 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:53:59.364013: step 13790, loss = 0.77 (1024.7 examples/sec; 0.125 sec/batch)
2017-05-08 18:54:00.733676: step 13800, loss = 0.80 (934.5 examples/sec; 0.137 sec/batch)
2017-05-08 18:54:01.905420: step 13810, loss = 1.29 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-08 18:54:03.178498: step 13820, loss = 0.83 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 18:54:04.430729: step 13830, loss = 0.95 (1022.2 examples/sec; 0.125 sec/batch)
2017-05-08 18:54:05.681040: step 13840, loss = 0.95 (1023.7 examples/sec; 0.125 sec/batch)
2017-05-08 18:54:06.958385: step 13850, loss = 0.81 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:54:08.236656: step 13860, loss = 1.02 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:54:09.507052: step 13870, loss = 0.88 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:54:10.774061: step 13880, loss = 0.77 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-08 18:54:12.060004: step 13890, loss = 1.00 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:54:13.398060: step 13900, loss = 0.86 (956.6 examples/sec; 0.134 sec/batch)
2017-05-08 18:54:14.590810: step 13910, loss = 0.76 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-08 18:54:15.893903: step 13920, loss = 0.82 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:54:17.237090: step 13930, loss = 1.00 (953.0 examples/sec; 0.134 sec/batch)
2017-05-08 18:54:18.416981: step 13940, loss = 0.75 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-08 18:54:19.679706: step 13950, loss = 0.71 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:54:20.956106: step 13960, loss = 1.02 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:54:22.247111: step 13970, loss = 0.78 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:54:23.532921: step 13980, loss = 0.78 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:54:24.786540: step 13990, loss = 0.87 (1021.0 examples/sec; 0.125 sec/batch)
2017-05-08 18:54:26.135842: step 14000, loss = 0.80 (948.6 examples/sec; 0.135 sec/batch)
2017-05-08 18:54:27.312721: step 14010, loss = 0.86 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-08 18:54:28.548386: step 14020, loss = 1.00 (1035.9 examples/sec; 0.124 sec/batch)
2017-05-08 18:54:29.834761: step 14030, loss = 0.89 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:54:31.090252: step 14040, loss = 0.85 (1019.5 examples/sec; 0.126 sec/batch)
2017-05-08 18:54:32.357768: step 14050, loss = 0.76 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:54:33.654199: step 14060, loss = 0.73 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:54:34.920548: step 14070, loss = 0.73 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:54:36.150761: step 14080, loss = 0.75 (1040.5 examples/sec; 0.123 sec/batch)
2017-05-08 18:54:37.422392: step 14090, loss = 0.88 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:54:38.774417: step 14100, loss = 0.96 (946.7 examples/sec; 0.135 sec/batch)
2017-05-08 18:54:39.942210: step 14110, loss = 0.82 (1096.1 examples/sec; 0.117 sec/batch)
2017-05-08 18:54:41.218520: step 14120, loss = 0.75 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:54:42.526405: step 14130, loss = 0.72 (978.7 examples/sec; 0.131 sec/batch)
2017-05-08 18:54:43.789736: step 14140, loss = 0.91 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-08 18:54:45.072958: step 14150, loss = 0.70 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:54:46.333703: step 14160, loss = 0.85 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-08 18:54:47.601370: step 14170, loss = 0.78 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 18:54:48.853305: step 14180, loss = 0.66 (1022.4 examples/sec; 0.125 sec/batch)
2017-05-08 18:54:50.129207: step 14190, loss = 0.84 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:54:51.493934: step 14200, loss = 0.67 (937.9 examples/sec; 0.136 sec/batch)
2017-05-08 18:54:52.673205: step 14210, loss = 0.58 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-08 18:54:53.944923: step 14220, loss = 0.84 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:54:55.211504: step 14230, loss = 0.84 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:54:56.496098: step 14240, loss = 0.87 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:54:57.767817: step 14250, loss = 0.83 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:54:59.020395: step 14260, loss = 0.82 (1021.9 examples/sec; 0.125 sec/batch)
2017-05-08 18:55:00.279318: step 14270, loss = 0.92 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:55:01.552862: step 14280, loss = 0.81 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:55:02.843999: step 14290, loss = 0.86 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:55:04.213100: step 14300, loss = 0.99 (934.9 examples/sec; 0.137 sec/batch)
2017-05-08 18:55:05.391618: step 14310, loss = 0.73 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-08 18:55:06.671848: step 14320, loss = 0.90 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:55:07.935658: step 14330, loss = 0.94 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-08 18:55:09.206692: step 14340, loss = 0.79 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:55:10.482098: step 14350, loss = 0.89 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:55:11.741173: step 14360, loss = 0.85 (1016.6 examples/sec; 0.126 sec/batch)
2017-05-08 18:55:13.011431: step 14370, loss = 0.99 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-08 18:55:14.295842: step 14380, loss = 0.83 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:55:15.575378: step 14390, loss = 0.94 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:55:16.923248: step 14400, loss = 0.80 (949.6 examples/sec; 0.135 sec/batch)
2017-05-08 18:55:18.103669: step 14410, loss = 0.90 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-08 18:55:19.382152: step 14420, loss = 0.76 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:55:20.662489: step 14430, loss = 0.78 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:55:21.924134: step 14440, loss = 0.83 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-08 18:55:23.205111: step 14450, loss = 0.70 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:55:24.439957: step 14460, loss = 0.90 (1036.5 examples/sec; 0.123 sec/batch)
2017-05-08 18:55:25.736368: step 14470, loss = 0.74 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 18:55:27.025931: step 14480, loss = 0.79 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:55:28.306309: step 14490, loss = 0.93 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:55:29.662997: step 14500, loss = 0.91 (943.5 examples/sec; 0.136 sec/batch)
2017-05-08 18:55:30.824373: step 14510, loss = 0.84 (1102.1 examples/sec; 0.116 sec/batch)
2017-05-08 18:55:32.083325: step 14520, loss = 0.76 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:55:33.364770: step 14530, loss = 0.84 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:55:34.617931: step 14540, loss = 0.94 (1021.4 examples/sec; 0.125 sec/batch)
2017-05-08 18:55:35.878156: step 14550, loss = 0.81 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:55:37.172437: step 14560, loss = 0.65 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:55:38.431922: step 14570, loss = 0.76 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-08 18:55:39.690971: step 14580, loss E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 306 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
= 0.90 (1016.6 examples/sec; 0.126 sec/batch)
2017-05-08 18:55:40.966145: step 14590, loss = 0.78 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:55:42.361729: step 14600, loss = 0.95 (917.2 examples/sec; 0.140 sec/batch)
2017-05-08 18:55:43.510206: step 14610, loss = 1.06 (1114.5 examples/sec; 0.115 sec/batch)
2017-05-08 18:55:44.774700: step 14620, loss = 0.76 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-08 18:55:46.042746: step 14630, loss = 0.97 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-08 18:55:47.306148: step 14640, loss = 0.90 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-08 18:55:48.591349: step 14650, loss = 0.85 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:55:49.857776: step 14660, loss = 0.94 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-08 18:55:51.104071: step 14670, loss = 1.17 (1027.1 examples/sec; 0.125 sec/batch)
2017-05-08 18:55:52.400869: step 14680, loss = 0.83 (987.0 examples/sec; 0.130 sec/batch)
2017-05-08 18:55:53.660064: step 14690, loss = 0.70 (1016.5 examples/sec; 0.126 sec/batch)
2017-05-08 18:55:55.037039: step 14700, loss = 0.79 (929.6 examples/sec; 0.138 sec/batch)
2017-05-08 18:55:56.218661: step 14710, loss = 0.96 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-08 18:55:57.472085: step 14720, loss = 0.93 (1021.2 examples/sec; 0.125 sec/batch)
2017-05-08 18:55:58.755600: step 14730, loss = 0.83 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:56:00.016729: step 14740, loss = 0.87 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-08 18:56:01.276226: step 14750, loss = 0.73 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-08 18:56:02.530156: step 14760, loss = 0.71 (1020.8 examples/sec; 0.125 sec/batch)
2017-05-08 18:56:03.806718: step 14770, loss = 0.79 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:56:05.082281: step 14780, loss = 0.93 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:56:06.340407: step 14790, loss = 0.91 (1017.4 examples/sec; 0.126 sec/batch)
2017-05-08 18:56:07.705822: step 14800, loss = 0.97 (937.4 examples/sec; 0.137 sec/batch)
2017-05-08 18:56:08.918874: step 14810, loss = 0.88 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-08 18:56:10.237058: step 14820, loss = 0.94 (971.0 examples/sec; 0.132 sec/batch)
2017-05-08 18:56:11.528119: step 14830, loss = 0.86 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:56:12.804031: step 14840, loss = 0.70 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:56:14.037116: step 14850, loss = 0.84 (1038.0 examples/sec; 0.123 sec/batch)
2017-05-08 18:56:15.295639: step 14860, loss = 1.06 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-08 18:56:16.530226: step 14870, loss = 0.91 (1036.8 examples/sec; 0.123 sec/batch)
2017-05-08 18:56:17.785138: step 14880, loss = 0.81 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-08 18:56:19.056123: step 14890, loss = 0.82 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:56:20.418220: step 14900, loss = 0.91 (939.7 examples/sec; 0.136 sec/batch)
2017-05-08 18:56:21.592916: step 14910, loss = 0.89 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-08 18:56:22.876180: step 14920, loss = 0.67 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:56:24.129780: step 14930, loss = 0.72 (1021.1 examples/sec; 0.125 sec/batch)
2017-05-08 18:56:25.412417: step 14940, loss = 1.05 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:56:26.675884: step 14950, loss = 0.83 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-08 18:56:27.926736: step 14960, loss = 0.80 (1023.3 examples/sec; 0.125 sec/batch)
2017-05-08 18:56:29.217789: step 14970, loss = 0.85 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:56:30.469750: step 14980, loss = 0.88 (1022.4 examples/sec; 0.125 sec/batch)
2017-05-08 18:56:31.721559: step 14990, loss = 0.84 (1022.5 examples/sec; 0.125 sec/batch)
2017-05-08 18:56:33.083131: step 15000, loss = 0.98 (940.1 examples/sec; 0.136 sec/batch)
2017-05-08 18:56:34.271565: step 15010, loss = 0.61 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-08 18:56:35.526487: step 15020, loss = 1.03 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-08 18:56:36.804907: step 15030, loss = 0.80 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:56:38.091780: step 15040, loss = 0.90 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 18:56:39.355068: step 15050, loss = 0.82 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-08 18:56:40.594846: step 15060, loss = 0.90 (1032.4 examples/sec; 0.124 sec/batch)
2017-05-08 18:56:41.852186: step 15070, loss = 0.70 (1018.0 examples/sec; 0.126 sec/batch)
2017-05-08 18:56:43.137145: step 15080, loss = 0.72 (996.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:56:44.400957: step 15090, loss = 0.70 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-08 18:56:45.757064: step 15100, loss = 0.73 (943.9 examples/sec; 0.136 sec/batch)
2017-05-08 18:56:46.915401: step 15110, loss = 0.85 (1105.0 examples/sec; 0.116 sec/batch)
2017-05-08 18:56:48.162007: step 15120, loss = 0.76 (1026.8 examples/sec; 0.125 sec/batch)
2017-05-08 18:56:49.421182: step 15130, loss = 0.90 (1016.5 examples/sec; 0.126 sec/batch)
2017-05-08 18:56:50.676353: step 15140, loss = 0.77 (1019.8 examples/sec; 0.126 sec/batch)
2017-05-08 18:56:51.933179: step 15150, loss = 0.85 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-08 18:56:53.230740: step 15160, loss = 0.73 (986.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:56:54.506690: step 15170, loss = 0.74 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:56:55.778889: step 15180, loss = 0.86 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:56:57.055472: step 15190, loss = 0.89 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 18:56:58.436598: step 15200, loss = 0.83 (926.8 examples/sec; 0.138 sec/batch)
2017-05-08 18:56:59.600670: step 15210, loss = 0.84 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-08 18:57:00.865197: step 15220, loss = 1.03 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 18:57:02.126521: step 15230, loss = 0.75 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-08 18:57:03.408688: step 15240, loss = 0.90 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 18:57:04.667180: step 15250, loss = 0.93 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-08 18:57:05.922380: step 15260, loss = 0.67 (1019.8 examples/sec; 0.126 sec/batch)
2017-05-08 18:57:07.211613: step 15270, loss = 0.75 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:57:08.506114: step 15280, loss = 0.82 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:57:09.778656: step 15290, loss = 0.83 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-08 18:57:11.152007: step 15300, loss = 0.85 (932.0 examples/sec; 0.137 sec/batch)
2017-05-08 18:57:12.303639: step 15310, loss = 0.82 (1111.5 examples/sec; 0.115 sec/batch)
2017-05-08 18:57:13.571073: step 15320, loss = 0.73 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-08 18:57:14.833950: step 15330, loss = 0.76 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-08 18:57:16.079843: step 15340, loss = 1.02 (1027.4 examples/sec; 0.125 sec/batch)
2017-05-08 18:57:17.310482: step 15350, loss = 0.81 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-08 18:57:18.582059: step 15360, loss = 0.81 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:57:19.848541: step 15370, loss = 0.77 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-08 18:57:21.126668: step 15380, loss = 0.74 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:57:22.410053: step 15390, loss = 0.80 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:57:23.760040: step 15400, loss = 0.77 (948.2 examples/sec; 0.135 sec/batch)
2017-05-08 18:57:24.932854: step 15410, loss = 0.68 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-08 18:57:26.187363: step 15420, loss = 0.88 (1020.3 examples/sec; 0.125 sec/batch)
2017-05-08 18:57:27.478522: step 15430, loss = 0.96 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:57:28.740230: step 15440, loss = 0.93 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-08 18:57:30.013897: step 15450, loss = 0.93 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:57:31.273653: step 15460, loss = 0.93 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-08 18:57:32.537357: step 15470, loss = 0.94 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-08 18:57:33.795715: step 15480, loss = 0.96 (1017.2 examples/secE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 326 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
; 0.126 sec/batch)
2017-05-08 18:57:35.078762: step 15490, loss = 0.80 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:57:36.441998: step 15500, loss = 0.86 (938.9 examples/sec; 0.136 sec/batch)
2017-05-08 18:57:37.604804: step 15510, loss = 0.95 (1100.8 examples/sec; 0.116 sec/batch)
2017-05-08 18:57:38.864535: step 15520, loss = 0.88 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-08 18:57:40.113518: step 15530, loss = 0.78 (1024.8 examples/sec; 0.125 sec/batch)
2017-05-08 18:57:41.412346: step 15540, loss = 0.92 (985.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:57:42.703376: step 15550, loss = 0.78 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 18:57:43.964704: step 15560, loss = 0.80 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-08 18:57:45.246127: step 15570, loss = 0.96 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:57:46.513742: step 15580, loss = 0.97 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:57:47.749362: step 15590, loss = 0.84 (1035.9 examples/sec; 0.124 sec/batch)
2017-05-08 18:57:49.102994: step 15600, loss = 0.84 (945.6 examples/sec; 0.135 sec/batch)
2017-05-08 18:57:50.303130: step 15610, loss = 0.85 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-08 18:57:51.578823: step 15620, loss = 0.83 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:57:52.814634: step 15630, loss = 0.90 (1035.8 examples/sec; 0.124 sec/batch)
2017-05-08 18:57:54.086901: step 15640, loss = 0.80 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-08 18:57:55.384786: step 15650, loss = 0.75 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 18:57:56.641276: step 15660, loss = 0.82 (1018.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:57:57.888878: step 15670, loss = 0.61 (1026.0 examples/sec; 0.125 sec/batch)
2017-05-08 18:57:59.171515: step 15680, loss = 0.68 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 18:58:00.448386: step 15690, loss = 0.90 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 18:58:01.843518: step 15700, loss = 0.73 (917.5 examples/sec; 0.140 sec/batch)
2017-05-08 18:58:03.069887: step 15710, loss = 0.92 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-08 18:58:04.328847: step 15720, loss = 1.31 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:58:05.607030: step 15730, loss = 0.77 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:58:06.875313: step 15740, loss = 0.66 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:58:08.141726: step 15750, loss = 0.83 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-08 18:58:09.395737: step 15760, loss = 0.73 (1020.7 examples/sec; 0.125 sec/batch)
2017-05-08 18:58:10.666090: step 15770, loss = 0.63 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:58:11.926854: step 15780, loss = 0.71 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-08 18:58:13.200935: step 15790, loss = 0.73 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:58:14.554578: step 15800, loss = 0.91 (945.6 examples/sec; 0.135 sec/batch)
2017-05-08 18:58:15.723694: step 15810, loss = 0.80 (1094.8 examples/sec; 0.117 sec/batch)
2017-05-08 18:58:17.009704: step 15820, loss = 0.86 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:58:18.280536: step 15830, loss = 0.82 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:58:19.553128: step 15840, loss = 0.73 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:58:20.815137: step 15850, loss = 0.90 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-08 18:58:22.092291: step 15860, loss = 0.90 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 18:58:23.350113: step 15870, loss = 0.79 (1017.6 examples/sec; 0.126 sec/batch)
2017-05-08 18:58:24.607758: step 15880, loss = 0.93 (1017.8 examples/sec; 0.126 sec/batch)
2017-05-08 18:58:25.899905: step 15890, loss = 0.80 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 18:58:27.279560: step 15900, loss = 0.88 (927.8 examples/sec; 0.138 sec/batch)
2017-05-08 18:58:28.457014: step 15910, loss = 0.85 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-08 18:58:29.726803: step 15920, loss = 0.97 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:58:30.983357: step 15930, loss = 0.80 (1018.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:58:32.237889: step 15940, loss = 0.88 (1020.3 examples/sec; 0.125 sec/batch)
2017-05-08 18:58:33.534124: step 15950, loss = 0.77 (987.5 examples/sec; 0.130 sec/batch)
2017-05-08 18:58:34.790043: step 15960, loss = 0.92 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-08 18:58:36.118542: step 15970, loss = 0.75 (963.5 examples/sec; 0.133 sec/batch)
2017-05-08 18:58:37.353154: step 15980, loss = 0.80 (1036.8 examples/sec; 0.123 sec/batch)
2017-05-08 18:58:38.628582: step 15990, loss = 0.69 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 18:58:39.999695: step 16000, loss = 0.80 (933.5 examples/sec; 0.137 sec/batch)
2017-05-08 18:58:41.168205: step 16010, loss = 0.73 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-08 18:58:42.433427: step 16020, loss = 0.70 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-08 18:58:43.687103: step 16030, loss = 0.90 (1021.0 examples/sec; 0.125 sec/batch)
2017-05-08 18:58:44.982935: step 16040, loss = 0.90 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 18:58:46.268003: step 16050, loss = 0.78 (996.1 examples/sec; 0.129 sec/batch)
2017-05-08 18:58:47.519525: step 16060, loss = 0.90 (1022.8 examples/sec; 0.125 sec/batch)
2017-05-08 18:58:48.794494: step 16070, loss = 0.83 (1003.9 examples/sec; 0.127 sec/batch)
2017-05-08 18:58:50.064379: step 16080, loss = 0.89 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:58:51.336788: step 16090, loss = 0.77 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:58:52.687009: step 16100, loss = 0.86 (948.0 examples/sec; 0.135 sec/batch)
2017-05-08 18:58:53.897250: step 16110, loss = 0.87 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-08 18:58:55.142439: step 16120, loss = 0.92 (1028.0 examples/sec; 0.125 sec/batch)
2017-05-08 18:58:56.386944: step 16130, loss = 0.85 (1028.5 examples/sec; 0.124 sec/batch)
2017-05-08 18:58:57.657008: step 16140, loss = 0.85 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:58:58.930352: step 16150, loss = 0.85 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:59:00.193293: step 16160, loss = 0.77 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-08 18:59:01.476006: step 16170, loss = 0.72 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 18:59:02.740043: step 16180, loss = 0.82 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-08 18:59:04.014625: step 16190, loss = 0.86 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:59:05.403878: step 16200, loss = 0.84 (921.4 examples/sec; 0.139 sec/batch)
2017-05-08 18:59:06.596417: step 16210, loss = 0.78 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-08 18:59:07.858361: step 16220, loss = 0.84 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-08 18:59:09.123236: step 16230, loss = 0.79 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-08 18:59:10.395091: step 16240, loss = 0.80 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-08 18:59:11.673268: step 16250, loss = 0.79 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 18:59:12.932232: step 16260, loss = 0.86 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:59:14.218611: step 16270, loss = 0.90 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 18:59:15.491952: step 16280, loss = 0.86 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:59:16.757234: step 16290, loss = 0.88 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:59:18.101776: step 16300, loss = 1.05 (952.0 examples/sec; 0.134 sec/batch)
2017-05-08 18:59:19.267746: step 16310, loss = 0.82 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-08 18:59:20.536931: step 16320, loss = 0.85 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:59:21.807034: step 16330, loss = 0.81 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-08 18:59:23.094958: step 16340, loss = 0.83 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 18:59:24.361729: step 16350, loss = 1.00 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-08 18:59:25.627238: step 16360, loss = 0.94 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:59:26.907535: step 16370, loss = 0.75 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 18:59:28.164067: step 16380, loss = 0.82 (1018.7 examples/sec; 0.126 sec/batch)
2017-05-08 18:59:29.432396: step 16390, loss = 0.87 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-08 18:59:30.779573: step 16400, loss = 0.72 (950.1 examples/sec; 0.135 sec/batch)
2017-05-08 18:59:31.952901: step 16410, loss = 0.75 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-08 18:59:33.200737: step 16420, loss = 0.74 (1025.8 examples/sec; 0.125 sec/batch)
2017-05-08 18:59:34.473609: step 16430, loss = 0.89 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:59:35.747185: step 16440, loss = 0.88 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:59:36.990953: step 16450, loss = 0.95 (1029.1 examples/sec; 0.124 sec/batch)
2017-05-08 18:59:38.262719: step 16460, loss = 0.84 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-08 18:59:39.507350: step 16470, loss = 0.94 (1028.4 examples/sec; 0.124 sec/batch)
2017-05-08 18:59:40.794640: step 16480, loss = 0.84 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:59:42.054120: step 16490, loss = 0.88 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-08 18:59:43.408366: step 16500, loss = 0.79 (945.2 examples/sec; 0.135 sec/batch)
2017-05-08 18:59:44.598518: step 16510, loss = 0.74 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-08 18:59:45.880891: step 16520, loss = 0.90 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 18:59:47.137208: step 16530, loss = 0.82 (1018.9 examples/sec; 0.126 sec/batch)
2017-05-08 18:59:48.410122: step 16540, loss = 0.79 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 18:59:49.669438: step 16550, loss = 0.90 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-08 18:59:50.970732: step 16560, loss = 0.98 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 18:59:52.236768: step 16570, loss = 0.84 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 18:59:53.522754: step 16580, loss = 0.98 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 18:59:54.773584: step 16590, loss = 0.88 (1023.3 examples/sec; 0.125 sec/batch)
2017-05-08 18:59:56.131531: step 16600, loss = 0.72 (942.6 examples/sec; 0.136 sec/batch)
2017-05-08 18:59:57.311528: step 16610, loss = 0.77 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-08 18:59:58.599995: step 16620, loss = 0.82 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 18:59:59.852552: step 16630, loss = 0.68 (1021.9 examples/sec; 0.125 sec/batch)
2017-05-08 19:00:01.157059: step 16640, loss = 0.88 (981.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:00:02.426303: step 16650, loss = 0.77 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:00:03.682239: step 16660, loss = 0.90 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:00:04.980427: step 16670, loss = 0.79 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:00:06.235653: step 16680, loss = 0.69 (1019.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:00:07.506725: step 16690, loss = 0.79 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:00:08.869289: step 16700, loss = 0.91 (939.4 examples/sec; 0.136 sec/batch)
2017-05-08 19:00:10.062352: step 16710, loss = 0.93 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-08 19:00:11.324193: step 16720, loss = 1.08 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:00:12.585293: step 16730, loss = 0.72 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:00:13.836965: step 16740, loss = 0.75 (1022.6 examples/sec; 0.125 sec/batch)
2017-05-08 19:00:15.106855: step 16750, loss = 0.88 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:00:16.392548: step 16760, loss = 0.83 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:00:17.650978: step 16770, loss = 0.79 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:00:18.934193: step 16780, loss = 0.82 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:00:20.216479: step 16790, loss = 0.91 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:00:21.595363: step 16800, loss = 0.96 (928.3 examples/sec; 0.138 sec/batch)
2017-05-08 19:00:22.789872: step 16810, loss = 0.84 (1071.6 examples/sec; 0.119 sec/batch)
2017-05-08 19:00:24.033162: step 16820, loss = 1.00 (1029.5 examples/sec; 0.124 sec/batch)
2017-05-08 19:00:25.300855: step 16830, loss = 0.87 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:00:26.5837E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 346 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
46: step 16840, loss = 0.88 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:00:27.842267: step 16850, loss = 0.78 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:00:29.106796: step 16860, loss = 0.77 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:00:30.371177: step 16870, loss = 0.81 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:00:31.637095: step 16880, loss = 0.89 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:00:32.888373: step 16890, loss = 0.76 (1022.9 examples/sec; 0.125 sec/batch)
2017-05-08 19:00:34.260471: step 16900, loss = 0.79 (932.9 examples/sec; 0.137 sec/batch)
2017-05-08 19:00:35.425290: step 16910, loss = 0.76 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-08 19:00:36.672202: step 16920, loss = 0.80 (1026.5 examples/sec; 0.125 sec/batch)
2017-05-08 19:00:37.952494: step 16930, loss = 0.91 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:00:39.221028: step 16940, loss = 0.67 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:00:40.476813: step 16950, loss = 0.82 (1019.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:00:41.750115: step 16960, loss = 1.16 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:00:43.035176: step 16970, loss = 0.80 (996.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:00:44.289990: step 16980, loss = 1.00 (1020.1 examples/sec; 0.125 sec/batch)
2017-05-08 19:00:45.564709: step 16990, loss = 0.84 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:00:46.916675: step 17000, loss = 0.95 (946.8 examples/sec; 0.135 sec/batch)
2017-05-08 19:00:48.118172: step 17010, loss = 0.80 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-08 19:00:49.379568: step 17020, loss = 0.85 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:00:50.646071: step 17030, loss = 0.90 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:00:51.959340: step 17040, loss = 0.79 (974.7 examples/sec; 0.131 sec/batch)
2017-05-08 19:00:53.207092: step 17050, loss = 0.77 (1025.8 examples/sec; 0.125 sec/batch)
2017-05-08 19:00:54.483919: step 17060, loss = 0.75 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:00:55.770055: step 17070, loss = 0.97 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:00:57.043112: step 17080, loss = 0.77 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:00:58.312789: step 17090, loss = 0.75 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:00:59.642190: step 17100, loss = 0.99 (962.8 examples/sec; 0.133 sec/batch)
2017-05-08 19:01:00.795795: step 17110, loss = 0.69 (1109.6 examples/sec; 0.115 sec/batch)
2017-05-08 19:01:02.094688: step 17120, loss = 0.93 (985.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:01:03.375853: step 17130, loss = 0.76 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:01:04.668044: step 17140, loss = 0.84 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:01:05.931913: step 17150, loss = 0.95 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:01:07.215715: step 17160, loss = 0.73 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:01:08.511068: step 17170, loss = 0.84 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:01:09.754426: step 17180, loss = 0.68 (1029.5 examples/sec; 0.124 sec/batch)
2017-05-08 19:01:11.071413: step 17190, loss = 0.83 (971.9 examples/sec; 0.132 sec/batch)
2017-05-08 19:01:12.406573: step 17200, loss = 1.00 (958.7 examples/sec; 0.134 sec/batch)
2017-05-08 19:01:13.595255: step 17210, loss = 0.92 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-08 19:01:14.879589: step 17220, loss = 0.89 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:01:16.151265: step 17230, loss = 0.72 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:01:17.458150: step 17240, loss = 0.72 (979.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:01:18.693699: step 17250, loss = 0.86 (1036.0 examples/sec; 0.124 sec/batch)
2017-05-08 19:01:19.954437: step 17260, loss = 0.88 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:01:21.212813: step 17270, loss = 0.95 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:01:22.498213: step 17280, loss = 0.70 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:01:23.754086: step 17290, loss = 0.79 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:01:25.137576: step 17300, loss = 0.80 (925.2 examples/sec; 0.138 sec/batch)
2017-05-08 19:01:26.326762: step 17310, loss = 0.82 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-08 19:01:27.590057: step 17320, loss = 0.71 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:01:28.867902: step 17330, loss = 0.77 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:01:30.174876: step 17340, loss = 0.96 (979.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:01:31.451611: step 17350, loss = 0.78 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:01:32.717276: step 17360, loss = 0.99 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:01:33.985325: step 17370, loss = 0.84 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:01:35.218868: step 17380, loss = 0.94 (1037.7 examples/sec; 0.123 sec/batch)
2017-05-08 19:01:36.493707: step 17390, loss = 1.03 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:01:37.881419: step 17400, loss = 0.68 (922.4 examples/sec; 0.139 sec/batch)
2017-05-08 19:01:39.120323: step 17410, loss = 0.87 (1033.2 examples/sec; 0.124 sec/batch)
2017-05-08 19:01:40.343429: step 17420, loss = 0.82 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-08 19:01:41.593411: step 17430, loss = 0.80 (1024.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:01:42.949892: step 17440, loss = 0.86 (943.6 examples/sec; 0.136 sec/batch)
2017-05-08 19:01:44.166295: step 17450, loss = 0.71 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-08 19:01:45.421917: step 17460, loss = 0.82 (1019.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:01:46.701862: step 17470, loss = 0.77 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:01:47.986700: step 17480, loss = 0.67 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:01:49.285961: step 17490, loss = 0.84 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:01:50.650779: step 17500, loss = 0.72 (937.9 examples/sec; 0.136 sec/batch)
2017-05-08 19:01:51.810599: step 17510, loss = 0.83 (1103.6 examples/sec; 0.116 sec/batch)
2017-05-08 19:01:53.080341: step 17520, loss = 0.65 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:01:54.362991: step 17530, loss = 1.00 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:01:55.628278: step 17540, loss = 0.76 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:01:56.887070: step 17550, loss = 0.96 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:01:58.161354: step 17560, loss = 0.80 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:01:59.423524: step 17570, loss = 0.71 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:02:00.700417: step 17580, loss = 0.93 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:02:01.986338: step 17590, loss = 0.97 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:02:03.333248: step 17600, loss = 0.82 (950.3 examples/sec; 0.135 sec/batch)
2017-05-08 19:02:04.509350: step 17610, loss = 0.74 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-08 19:02:05.757703: step 17620, loss = 0.69 (1025.4 examples/sec; 0.125 sec/batch)
2017-05-08 19:02:07.009800: step 17630, loss = 0.74 (1022.3 examples/sec; 0.125 sec/batch)
2017-05-08 19:02:08.284537: step 17640, loss = 0.70 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:02:09.561999: step 17650, loss = 0.85 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:02:10.815153: step 17660, loss = 0.81 (1021.4 examples/sec; 0.125 sec/batch)
2017-05-08 19:02:12.092253: step 17670, loss = 0.89 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:02:13.340037: step 17680, loss = 0.79 (1025.8 examples/sec; 0.125 sec/batch)
2017-05-08 19:02:14.647737: step 17690, loss = 0.83 (978.8 examples/sec; 0.131 sec/batch)
2017-05-08 19:02:15.975265: step 17700, loss = 0.84 (964.2 examples/sec; 0.133 sec/batch)
2017-05-08 19:02:17.174387: step 17710, loss = 0.75 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-08 19:02:18.453264: step 17720, loss = 0.84 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:02:19.693398: step 17730, loss = 1.07 (1032.1 examples/sec; 0.124 sec/batch)
2017-05-08 19:02:21.042973: step 17740, loss = 0.84 (948E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 366 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
.4 examples/sec; 0.135 sec/batch)
2017-05-08 19:02:22.274020: step 17750, loss = 0.96 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-08 19:02:23.533171: step 17760, loss = 0.78 (1016.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:02:24.855224: step 17770, loss = 0.77 (968.2 examples/sec; 0.132 sec/batch)
2017-05-08 19:02:26.165131: step 17780, loss = 0.85 (977.2 examples/sec; 0.131 sec/batch)
2017-05-08 19:02:27.498640: step 17790, loss = 0.88 (959.9 examples/sec; 0.133 sec/batch)
2017-05-08 19:02:28.834381: step 17800, loss = 0.83 (958.3 examples/sec; 0.134 sec/batch)
2017-05-08 19:02:30.058690: step 17810, loss = 0.77 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-08 19:02:31.339046: step 17820, loss = 0.76 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:02:32.577170: step 17830, loss = 0.94 (1033.8 examples/sec; 0.124 sec/batch)
2017-05-08 19:02:33.908872: step 17840, loss = 0.77 (961.2 examples/sec; 0.133 sec/batch)
2017-05-08 19:02:35.144371: step 17850, loss = 0.72 (1036.0 examples/sec; 0.124 sec/batch)
2017-05-08 19:02:36.422643: step 17860, loss = 0.89 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:02:37.677591: step 17870, loss = 0.73 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:02:38.956614: step 17880, loss = 0.68 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:02:40.235642: step 17890, loss = 0.77 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:02:41.613290: step 17900, loss = 0.73 (929.1 examples/sec; 0.138 sec/batch)
2017-05-08 19:02:42.818579: step 17910, loss = 0.83 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-08 19:02:44.067710: step 17920, loss = 0.82 (1024.7 examples/sec; 0.125 sec/batch)
2017-05-08 19:02:45.332881: step 17930, loss = 0.81 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:02:46.583886: step 17940, loss = 0.76 (1023.2 examples/sec; 0.125 sec/batch)
2017-05-08 19:02:47.860672: step 17950, loss = 0.71 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:02:49.149183: step 17960, loss = 0.71 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:02:50.404262: step 17970, loss = 0.88 (1019.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:02:51.668396: step 17980, loss = 0.82 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:02:52.941016: step 17990, loss = 0.87 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:02:54.337597: step 18000, loss = 0.79 (916.5 examples/sec; 0.140 sec/batch)
2017-05-08 19:02:55.481394: step 18010, loss = 0.87 (1119.1 examples/sec; 0.114 sec/batch)
2017-05-08 19:02:56.778057: step 18020, loss = 0.83 (987.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:02:58.042883: step 18030, loss = 0.86 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:02:59.328339: step 18040, loss = 0.75 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:03:00.583773: step 18050, loss = 0.90 (1019.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:03:01.830237: step 18060, loss = 0.94 (1026.9 examples/sec; 0.125 sec/batch)
2017-05-08 19:03:03.091397: step 18070, loss = 0.82 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:03:04.369796: step 18080, loss = 0.89 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:03:05.607607: step 18090, loss = 0.76 (1034.1 examples/sec; 0.124 sec/batch)
2017-05-08 19:03:06.965936: step 18100, loss = 0.73 (942.3 examples/sec; 0.136 sec/batch)
2017-05-08 19:03:08.142891: step 18110, loss = 0.77 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-08 19:03:09.390249: step 18120, loss = 0.70 (1026.2 examples/sec; 0.125 sec/batch)
2017-05-08 19:03:10.652894: step 18130, loss = 0.70 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:03:11.924252: step 18140, loss = 0.84 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:03:13.203550: step 18150, loss = 0.87 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:03:14.495554: step 18160, loss = 0.90 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:03:15.779522: step 18170, loss = 0.87 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:03:17.076076: step 18180, loss = 1.06 (987.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:03:18.311569: step 18190, loss = 0.91 (1036.0 examples/sec; 0.124 sec/batch)
2017-05-08 19:03:19.680925: step 18200, loss = 0.73 (934.8 examples/sec; 0.137 sec/batch)
2017-05-08 19:03:20.851785: step 18210, loss = 0.71 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-08 19:03:22.119575: step 18220, loss = 0.96 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:03:23.392416: step 18230, loss = 0.75 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:03:24.668088: step 18240, loss = 0.81 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:03:25.939811: step 18250, loss = 0.85 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:03:27.227373: step 18260, loss = 0.90 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:03:28.484268: step 18270, loss = 0.81 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:03:29.762056: step 18280, loss = 0.73 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:03:31.025661: step 18290, loss = 0.58 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:03:32.401038: step 18300, loss = 1.00 (930.7 examples/sec; 0.138 sec/batch)
2017-05-08 19:03:33.620958: step 18310, loss = 0.69 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-08 19:03:34.887425: step 18320, loss = 0.78 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:03:36.161680: step 18330, loss = 0.85 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:03:37.427691: step 18340, loss = 0.62 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:03:38.695361: step 18350, loss = 0.85 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:03:39.961952: step 18360, loss = 0.86 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:03:41.238812: step 18370, loss = 0.85 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:03:42.530718: step 18380, loss = 0.73 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:03:43.782869: step 18390, loss = 0.74 (1022.2 examples/sec; 0.125 sec/batch)
2017-05-08 19:03:45.148577: step 18400, loss = 0.80 (937.2 examples/sec; 0.137 sec/batch)
2017-05-08 19:03:46.326361: step 18410, loss = 0.81 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-08 19:03:47.591332: step 18420, loss = 0.77 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:03:48.842114: step 18430, loss = 0.80 (1023.4 examples/sec; 0.125 sec/batch)
2017-05-08 19:03:50.104174: step 18440, loss = 0.77 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:03:51.382239: step 18450, loss = 0.72 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:03:52.671090: step 18460, loss = 0.88 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:03:53.945015: step 18470, loss = 0.99 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:03:55.208023: step 18480, loss = 1.03 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:03:56.481564: step 18490, loss = 0.79 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:03:57.854576: step 18500, loss = 0.88 (932.3 examples/sec; 0.137 sec/batch)
2017-05-08 19:03:59.009891: step 18510, loss = 1.12 (1107.9 examples/sec; 0.116 sec/batch)
2017-05-08 19:04:00.358112: step 18520, loss = 0.92 (949.4 examples/sec; 0.135 sec/batch)
2017-05-08 19:04:01.596058: step 18530, loss = 0.67 (1034.0 examples/sec; 0.124 sec/batch)
2017-05-08 19:04:02.870618: step 18540, loss = 0.87 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:04:04.132891: step 18550, loss = 0.82 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:04:05.437314: step 18560, loss = 0.81 (981.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:04:06.727894: step 18570, loss = 0.80 (991.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:04:08.029918: step 18580, loss = 0.86 (983.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:04:09.298680: step 18590, loss = 0.86 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:04:10.682566: step 18600, loss = 0.79 (924.9 examples/sec; 0.138 sec/batch)
2017-05-08 19:04:11.867575: step 18610, loss = 0.84 (1080.2 examples/sec; 0.119 sec/batch)
2017-05-08 19:04:13.117174: step 18620, loss = 0.79 (1024.3 examples/sec; 0.125 sec/batch)
2017-05-08 19:04:14.387595: step 18630, loss = 0.80 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:04:15.661187: step 18640, loss = 0.92 (1005.0 examples/sec; 0.127 sec/baE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 387 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
tch)
2017-05-08 19:04:16.933313: step 18650, loss = 0.80 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:04:18.219386: step 18660, loss = 0.75 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:04:19.480627: step 18670, loss = 0.76 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:04:20.739089: step 18680, loss = 0.73 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:04:21.999285: step 18690, loss = 0.77 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:04:23.352289: step 18700, loss = 0.68 (946.0 examples/sec; 0.135 sec/batch)
2017-05-08 19:04:24.518234: step 18710, loss = 0.76 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-08 19:04:25.781366: step 18720, loss = 0.74 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:04:27.074388: step 18730, loss = 0.69 (989.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:04:28.316635: step 18740, loss = 0.84 (1030.4 examples/sec; 0.124 sec/batch)
2017-05-08 19:04:29.571107: step 18750, loss = 0.81 (1020.3 examples/sec; 0.125 sec/batch)
2017-05-08 19:04:30.860998: step 18760, loss = 0.96 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:04:32.122637: step 18770, loss = 0.69 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:04:33.463121: step 18780, loss = 0.65 (954.9 examples/sec; 0.134 sec/batch)
2017-05-08 19:04:34.716675: step 18790, loss = 0.86 (1021.1 examples/sec; 0.125 sec/batch)
2017-05-08 19:04:36.029329: step 18800, loss = 0.80 (975.1 examples/sec; 0.131 sec/batch)
2017-05-08 19:04:37.202743: step 18810, loss = 0.61 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-08 19:04:38.460516: step 18820, loss = 0.78 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:04:39.724483: step 18830, loss = 0.87 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:04:41.013290: step 18840, loss = 0.78 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:04:42.292963: step 18850, loss = 0.77 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:04:43.580292: step 18860, loss = 0.89 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:04:44.827779: step 18870, loss = 0.80 (1026.1 examples/sec; 0.125 sec/batch)
2017-05-08 19:04:46.114821: step 18880, loss = 0.70 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:04:47.409796: step 18890, loss = 0.88 (988.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:04:48.775052: step 18900, loss = 0.82 (937.6 examples/sec; 0.137 sec/batch)
2017-05-08 19:04:49.967750: step 18910, loss = 0.72 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-08 19:04:51.268996: step 18920, loss = 0.82 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:04:52.542422: step 18930, loss = 0.87 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:04:53.800935: step 18940, loss = 0.97 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:04:55.077104: step 18950, loss = 0.73 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:04:56.353903: step 18960, loss = 0.87 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:04:57.624488: step 18970, loss = 0.87 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:04:58.874706: step 18980, loss = 0.82 (1023.8 examples/sec; 0.125 sec/batch)
2017-05-08 19:05:00.124918: step 18990, loss = 0.87 (1023.8 examples/sec; 0.125 sec/batch)
2017-05-08 19:05:01.497440: step 19000, loss = 0.96 (932.6 examples/sec; 0.137 sec/batch)
2017-05-08 19:05:02.672687: step 19010, loss = 0.65 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-08 19:05:03.967925: step 19020, loss = 0.85 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:05:05.242998: step 19030, loss = 0.77 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:05:06.518732: step 19040, loss = 0.88 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:05:07.775596: step 19050, loss = 0.91 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:05:09.035190: step 19060, loss = 0.88 (1016.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:05:10.304269: step 19070, loss = 0.72 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:05:11.568625: step 19080, loss = 0.80 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:05:12.851199: step 19090, loss = 0.78 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:05:14.229925: step 19100, loss = 0.88 (928.4 examples/sec; 0.138 sec/batch)
2017-05-08 19:05:15.399641: step 19110, loss = 0.72 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-08 19:05:16.650187: step 19120, loss = 0.89 (1023.6 examples/sec; 0.125 sec/batch)
2017-05-08 19:05:17.923809: step 19130, loss = 0.90 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:05:19.223441: step 19140, loss = 0.81 (984.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:05:20.480103: step 19150, loss = 0.90 (1018.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:05:21.743987: step 19160, loss = 0.76 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:05:23.030502: step 19170, loss = 0.73 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:05:24.286802: step 19180, loss = 0.74 (1018.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:05:25.566808: step 19190, loss = 0.89 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:05:26.933876: step 19200, loss = 0.92 (936.3 examples/sec; 0.137 sec/batch)
2017-05-08 19:05:28.143575: step 19210, loss = 0.75 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-08 19:05:29.401163: step 19220, loss = 0.70 (1017.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:05:30.666252: step 19230, loss = 0.85 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:05:31.926934: step 19240, loss = 0.59 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:05:33.203196: step 19250, loss = 0.95 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:05:34.472432: step 19260, loss = 0.72 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:05:35.770504: step 19270, loss = 1.00 (986.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:05:37.075781: step 19280, loss = 0.87 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 19:05:38.367970: step 19290, loss = 0.84 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:05:39.757037: step 19300, loss = 0.87 (921.5 examples/sec; 0.139 sec/batch)
2017-05-08 19:05:40.913947: step 19310, loss = 0.74 (1106.4 examples/sec; 0.116 sec/batch)
2017-05-08 19:05:42.147195: step 19320, loss = 0.89 (1037.9 examples/sec; 0.123 sec/batch)
2017-05-08 19:05:43.446752: step 19330, loss = 0.72 (984.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:05:44.705428: step 19340, loss = 0.72 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:05:45.975970: step 19350, loss = 0.88 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:05:47.240104: step 19360, loss = 0.89 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:05:48.496642: step 19370, loss = 0.76 (1018.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:05:49.753076: step 19380, loss = 0.70 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:05:51.016669: step 19390, loss = 1.02 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:05:52.376291: step 19400, loss = 0.72 (941.4 examples/sec; 0.136 sec/batch)
2017-05-08 19:05:53.546219: step 19410, loss = 0.70 (1094.1 examples/sec; 0.117 sec/batch)
2017-05-08 19:05:54.802531: step 19420, loss = 0.91 (1018.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:05:56.075357: step 19430, loss = 0.87 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:05:57.339894: step 19440, loss = 0.84 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:05:58.633780: step 19450, loss = 0.87 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:05:59.878082: step 19460, loss = 0.87 (1028.7 examples/sec; 0.124 sec/batch)
2017-05-08 19:06:01.145904: step 19470, loss = 0.86 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:06:02.423802: step 19480, loss = 0.60 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:06:03.682400: step 19490, loss = 0.85 (1017.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:06:05.081572: step 19500, loss = 0.84 (914.8 examples/sec; 0.140 sec/batch)
2017-05-08 19:06:06.258615: step 19510, loss = 0.75 (1087.5 examples/sec; 0.118 sec/batch)
2017-05-08 19:06:07.525886: step 19520, loss = 0.70 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:06:08.788182: step 19530, loss = 0.88 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:06:10.071076: step 19540, loss = 0.87 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:06:11.33634E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 407 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
3: step 19550, loss = 0.79 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:06:12.577326: step 19560, loss = 0.94 (1031.4 examples/sec; 0.124 sec/batch)
2017-05-08 19:06:13.854774: step 19570, loss = 0.71 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:06:15.128798: step 19580, loss = 0.96 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:06:16.390404: step 19590, loss = 0.83 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:06:17.751997: step 19600, loss = 0.68 (940.1 examples/sec; 0.136 sec/batch)
2017-05-08 19:06:18.955746: step 19610, loss = 0.84 (1063.3 examples/sec; 0.120 sec/batch)
2017-05-08 19:06:20.218433: step 19620, loss = 0.78 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:06:21.478591: step 19630, loss = 0.92 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:06:22.759214: step 19640, loss = 0.65 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:06:24.014929: step 19650, loss = 0.88 (1019.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:06:25.296687: step 19660, loss = 0.67 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:06:26.553727: step 19670, loss = 0.71 (1018.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:06:27.818337: step 19680, loss = 0.78 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:06:29.097080: step 19690, loss = 0.72 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:06:30.473006: step 19700, loss = 0.85 (930.3 examples/sec; 0.138 sec/batch)
2017-05-08 19:06:31.666481: step 19710, loss = 0.82 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-08 19:06:32.917246: step 19720, loss = 0.77 (1023.4 examples/sec; 0.125 sec/batch)
2017-05-08 19:06:34.189570: step 19730, loss = 0.68 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:06:35.432086: step 19740, loss = 0.92 (1030.2 examples/sec; 0.124 sec/batch)
2017-05-08 19:06:36.694172: step 19750, loss = 0.86 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:06:37.966095: step 19760, loss = 0.97 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:06:39.250372: step 19770, loss = 0.73 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:06:40.529241: step 19780, loss = 0.84 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:06:41.761496: step 19790, loss = 0.74 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-08 19:06:43.143370: step 19800, loss = 0.81 (926.3 examples/sec; 0.138 sec/batch)
2017-05-08 19:06:44.367176: step 19810, loss = 0.70 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-08 19:06:45.636268: step 19820, loss = 0.76 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:06:46.905651: step 19830, loss = 0.94 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:06:48.155600: step 19840, loss = 0.88 (1024.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:06:49.441346: step 19850, loss = 0.71 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:06:50.729333: step 19860, loss = 0.91 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:06:52.023389: step 19870, loss = 0.85 (989.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:06:53.303775: step 19880, loss = 0.77 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:06:54.585468: step 19890, loss = 0.61 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:06:55.965655: step 19900, loss = 0.71 (927.4 examples/sec; 0.138 sec/batch)
2017-05-08 19:06:57.119896: step 19910, loss = 0.88 (1109.0 examples/sec; 0.115 sec/batch)
2017-05-08 19:06:58.390594: step 19920, loss = 0.83 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:06:59.658021: step 19930, loss = 0.92 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:07:00.916867: step 19940, loss = 0.81 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:07:02.214748: step 19950, loss = 0.79 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:07:03.475098: step 19960, loss = 0.70 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:07:04.724889: step 19970, loss = 0.80 (1024.2 examples/sec; 0.125 sec/batch)
2017-05-08 19:07:06.012646: step 19980, loss = 0.66 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:07:07.298771: step 19990, loss = 0.91 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:07:08.670170: step 20000, loss = 1.00 (933.3 examples/sec; 0.137 sec/batch)
2017-05-08 19:07:09.862996: step 20010, loss = 0.83 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-08 19:07:11.130896: step 20020, loss = 0.71 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:07:12.395280: step 20030, loss = 0.80 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:07:13.650049: step 20040, loss = 0.84 (1020.1 examples/sec; 0.125 sec/batch)
2017-05-08 19:07:14.922178: step 20050, loss = 0.69 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:07:16.186971: step 20060, loss = 0.88 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:07:17.462363: step 20070, loss = 0.82 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:07:18.741388: step 20080, loss = 0.79 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:07:19.993235: step 20090, loss = 0.91 (1022.5 examples/sec; 0.125 sec/batch)
2017-05-08 19:07:21.367267: step 20100, loss = 0.86 (931.6 examples/sec; 0.137 sec/batch)
2017-05-08 19:07:22.549780: step 20110, loss = 0.68 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-08 19:07:23.819475: step 20120, loss = 0.89 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:07:25.090084: step 20130, loss = 0.83 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:07:26.353776: step 20140, loss = 0.77 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:07:27.655392: step 20150, loss = 0.92 (983.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:07:28.893600: step 20160, loss = 0.97 (1033.8 examples/sec; 0.124 sec/batch)
2017-05-08 19:07:30.180651: step 20170, loss = 0.92 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:07:31.451117: step 20180, loss = 0.56 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:07:32.738076: step 20190, loss = 0.85 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:07:34.113333: step 20200, loss = 0.70 (930.7 examples/sec; 0.138 sec/batch)
2017-05-08 19:07:35.289602: step 20210, loss = 0.81 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-08 19:07:36.547775: step 20220, loss = 0.65 (1017.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:07:37.813721: step 20230, loss = 0.91 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:07:39.080062: step 20240, loss = 0.80 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:07:40.351121: step 20250, loss = 0.86 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:07:41.609030: step 20260, loss = 0.86 (1017.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:07:42.866368: step 20270, loss = 0.91 (1018.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:07:44.154001: step 20280, loss = 0.76 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:07:45.444152: step 20290, loss = 0.79 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:07:46.811487: step 20300, loss = 0.79 (936.2 examples/sec; 0.137 sec/batch)
2017-05-08 19:07:47.960617: step 20310, loss = 0.82 (1114.0 examples/sec; 0.115 sec/batch)
2017-05-08 19:07:49.221299: step 20320, loss = 0.78 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:07:50.487254: step 20330, loss = 0.76 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:07:51.775303: step 20340, loss = 0.77 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:07:53.045540: step 20350, loss = 0.85 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:07:54.346061: step 20360, loss = 0.96 (984.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:07:55.582112: step 20370, loss = 0.65 (1035.6 examples/sec; 0.124 sec/batch)
2017-05-08 19:07:56.842267: step 20380, loss = 0.78 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:07:58.131966: step 20390, loss = 0.71 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:07:59.502757: step 20400, loss = 0.63 (933.8 examples/sec; 0.137 sec/batch)
2017-05-08 19:08:00.674427: step 20410, loss = 0.85 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-08 19:08:01.934653: step 20420, loss = 0.96 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:08:03.212145: step 20430, loss = 0.64 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:08:04.489528: step 20440, loss = 0.64 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:08:05.754313: step 20450, loss = 0.57 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:08:07.027956: step 20460, loss = 0.94 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:08:08.295516: step 20470, loss = 0.87 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:08:09.548244: step 20480, loss = 1.16 (1021.8 examples/sec; 0.125 sec/batch)
2017-05-08 19:08:10.842165: step 20490, loss = 0.76 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:08:12.182286: step 20500, loss = 1.00 (955.1 examples/sec; 0.134 sec/batch)
2017-05-08 19:08:13.356936: step 20510, loss = 0.76 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-08 19:08:14.662944: step 20520, loss = 0.89 (980.1 examples/sec; 0.131 sec/batch)
2017-05-08 19:08:15.910533: step 20530, loss = 0.60 (1026.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:08:17.162907: step 20540, loss = 0.75 (1022.1 examples/sec; 0.125 sec/batch)
2017-05-08 19:08:18.445178: step 20550, loss = 0.76 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:08:19.702797: step 20560, loss = 0.83 (1017.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:08:21.009690: step 20570, loss = 0.82 (979.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:08:22.291780: step 20580, loss = 0.99 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:08:23.572211: step 20590, loss = 0.73 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:08:24.966108: step 20600, loss = 0.80 (918.3 examples/sec; 0.139 sec/batch)
2017-05-08 19:08:26.138955: step 20610, loss = 0.74 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-08 19:08:27.406430: step 20620, loss = 0.89 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:08:28.678081: step 20630, loss = 0.83 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:08:29.952503: step 20640, loss = 0.95 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:08:31.222559: step 20650, loss = 0.80 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:08:32.489798: step 20660, loss = 1.02 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:08:33.761756: step 20670, loss = 0.76 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:08:35.053632: step 20680, loss = 0.74 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:08:36.321324: step 20690, loss = 0.79 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:08:37.723541: step 20700, loss = 0.75 (912.8 examples/sec; 0.140 sec/batch)
2017-05-08 19:08:38.930593: step 20710, loss = 0.92 (1060.4 examples/sec; 0.121 sec/batch)
2017-05-08 19:08:40.200033: step 20720, loss = 0.80 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:08:41.471858: step 20730, loss = 0.83 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:08:42.737817: step 20740, loss = 0.82 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:08:44.007534: step 20750, loss = 0.91 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:08:45.272748: step 20760, loss = 0.68 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:08:46.530611: step 20770, loss = 0.76 (1017.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:08:47.816550: step 20780, loss = 0.80 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:08:49.070549: step 20790, loss = 0.78 (1020.7 examples/sec; 0.125 sec/batch)
2017-05-08 19:08:50.444508: step 20800, loss = 0.82 (931.6 examples/sec; 0.137 sec/batch)
2017-05-08 19:08:51.652255: step 20810, loss = 0.89 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-08 19:08:52.920247: step 20820, loss = 0.77 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:08:54.190939: step 20830, loss = 0.84 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:08:55.468512: step 20840, loss = 0.78 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:08:56.734660: step 20850, loss = 0.70 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:08:58.015228: step 20860, loss = 0.80 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:08:59.301637: step 20870, loss = 0.76 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:09:00.549206: step 20880, loss = 0.77 (1026.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:09:01.841237: step 20890, loss = 1.04 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:09:03.216582: step 20900, loss = 0.77 (930.7 examples/secE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 427 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
; 0.138 sec/batch)
2017-05-08 19:09:04.370906: step 20910, loss = 0.92 (1108.9 examples/sec; 0.115 sec/batch)
2017-05-08 19:09:05.654187: step 20920, loss = 0.98 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:09:06.947650: step 20930, loss = 0.74 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:09:08.209613: step 20940, loss = 0.73 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:09:09.468290: step 20950, loss = 0.80 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:09:10.732351: step 20960, loss = 0.76 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:09:12.027630: step 20970, loss = 0.88 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:09:13.281168: step 20980, loss = 0.84 (1021.1 examples/sec; 0.125 sec/batch)
2017-05-08 19:09:14.551631: step 20990, loss = 0.69 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:09:15.939405: step 21000, loss = 0.98 (922.3 examples/sec; 0.139 sec/batch)
2017-05-08 19:09:17.108212: step 21010, loss = 0.72 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-08 19:09:18.367188: step 21020, loss = 0.85 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:09:19.633744: step 21030, loss = 0.79 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:09:20.909902: step 21040, loss = 0.79 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:09:22.203815: step 21050, loss = 0.69 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:09:23.503864: step 21060, loss = 0.93 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:09:24.760797: step 21070, loss = 0.92 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:09:26.004411: step 21080, loss = 0.77 (1029.3 examples/sec; 0.124 sec/batch)
2017-05-08 19:09:27.263248: step 21090, loss = 0.82 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:09:28.628756: step 21100, loss = 0.82 (937.4 examples/sec; 0.137 sec/batch)
2017-05-08 19:09:29.836850: step 21110, loss = 0.86 (1059.5 examples/sec; 0.121 sec/batch)
2017-05-08 19:09:31.087573: step 21120, loss = 0.66 (1023.4 examples/sec; 0.125 sec/batch)
2017-05-08 19:09:32.363793: step 21130, loss = 0.95 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:09:33.659015: step 21140, loss = 0.66 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:09:34.945559: step 21150, loss = 0.71 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:09:36.191404: step 21160, loss = 0.78 (1027.4 examples/sec; 0.125 sec/batch)
2017-05-08 19:09:37.481973: step 21170, loss = 0.67 (991.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:09:38.735926: step 21180, loss = 0.68 (1020.8 examples/sec; 0.125 sec/batch)
2017-05-08 19:09:40.033341: step 21190, loss = 0.92 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:09:41.392838: step 21200, loss = 0.78 (941.5 examples/sec; 0.136 sec/batch)
2017-05-08 19:09:42.570837: step 21210, loss = 0.76 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-08 19:09:43.824736: step 21220, loss = 0.76 (1020.8 examples/sec; 0.125 sec/batch)
2017-05-08 19:09:45.115372: step 21230, loss = 0.91 (991.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:09:46.406182: step 21240, loss = 0.76 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:09:47.664104: step 21250, loss = 1.03 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:09:48.927977: step 21260, loss = 0.86 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:09:50.206873: step 21270, loss = 0.80 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:09:51.478065: step 21280, loss = 0.95 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:09:52.752383: step 21290, loss = 0.81 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:09:54.119797: step 21300, loss = 0.76 (936.1 examples/sec; 0.137 sec/batch)
2017-05-08 19:09:55.309696: step 21310, loss = 0.80 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-08 19:09:56.560348: step 21320, loss = 0.89 (1023.5 examples/sec; 0.125 sec/batch)
2017-05-08 19:09:57.841536: step 21330, loss = 0.76 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:09:59.099938: step 21340, loss = 0.77 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:10:00.383323: step 21350, loss = 0.72 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:10:01.638211: step 21360, loss = 1.01 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:10:02.938697: step 21370, loss = 0.74 (984.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:10:04.221139: step 21380, loss = 0.72 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:10:05.503014: step 21390, loss = 0.85 (998.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:10:06.843950: step 21400, loss = 0.96 (954.6 examples/sec; 0.134 sec/batch)
2017-05-08 19:10:08.039372: step 21410, loss = 0.82 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-08 19:10:09.295942: step 21420, loss = 0.78 (1018.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:10:10.568486: step 21430, loss = 0.70 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:10:11.816422: step 21440, loss = 0.75 (1025.7 examples/sec; 0.125 sec/batch)
2017-05-08 19:10:13.089793: step 21450, loss = 0.99 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:10:14.367030: step 21460, loss = 0.86 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:10:15.615130: step 21470, loss = 0.84 (1025.6 examples/sec; 0.125 sec/batch)
2017-05-08 19:10:16.900852: step 21480, loss = 0.88 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:10:18.177016: step 21490, loss = 0.89 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:10:19.534677: step 21500, loss = 0.87 (942.8 examples/sec; 0.136 sec/batch)
2017-05-08 19:10:20.701114: step 21510, loss = 0.70 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-08 19:10:21.961785: step 21520, loss = 0.64 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:10:23.223484: step 21530, loss = 0.89 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:10:24.497693: step 21540, loss = 0.90 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:10:25.745244: step 21550, loss = 0.95 (1026.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:10:27.020606: step 21560, loss = 0.97 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:10:28.287045: step 21570, loss = 0.72 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:10:29.570306: step 21580, loss = 0.80 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:10:30.844830: step 21590, loss = 0.74 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:10:32.214361: step 21600, loss = 0.90 (934.6 examples/sec; 0.137 sec/batch)
2017-05-08 19:10:33.388749: step 21610, loss = 0.88 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-08 19:10:34.641812: step 21620, loss = 0.89 (1021.5 examples/sec; 0.125 sec/batch)
2017-05-08 19:10:35.956368: step 21630, loss = 0.74 (973.7 examples/sec; 0.131 sec/batch)
2017-05-08 19:10:37.224431: step 21640, loss = 0.61 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:10:38.551762: step 21650, loss = 0.75 (964.3 examples/sec; 0.133 sec/batch)
2017-05-08 19:10:39.832694: step 21660, loss = 0.68 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:10:41.134331: step 21670, loss = 0.80 (983.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:10:42.394374: step 21680, loss = 0.64 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:10:43.656307: step 21690, loss = 0.84 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:10:45.000757: step 21700, loss = 0.80 (952.1 examples/sec; 0.134 sec/batch)
2017-05-08 19:10:46.202223: step 21710, loss = 0.91 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-08 19:10:47.484878: step 21720, loss = 0.87 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:10:48.747966: step 21730, loss = 0.81 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:10:50.015020: step 21740, loss = 0.85 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:10:51.256826: step 21750, loss = 0.80 (1030.8 examples/sec; 0.124 sec/batch)
2017-05-08 19:10:52.524352: step 21760, loss = 0.79 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:10:53.829875: step 21770, loss = 0.82 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:10:55.144723: step 21780, loss = 0.78 (973.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:10:56.451637: step 21790, loss = 0.91 (979.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:10:57.821887: step 21800, loss = 0.77 (934.1 examples/sec; 0.137 sec/batch)
2017-05-08 19:10:5E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 447 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
8.972358: step 21810, loss = 0.72 (1112.6 examples/sec; 0.115 sec/batch)
2017-05-08 19:11:00.228754: step 21820, loss = 0.75 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:11:01.503212: step 21830, loss = 0.65 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:11:02.783306: step 21840, loss = 0.81 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:11:04.069918: step 21850, loss = 0.72 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:11:05.313869: step 21860, loss = 0.87 (1029.0 examples/sec; 0.124 sec/batch)
2017-05-08 19:11:06.587771: step 21870, loss = 0.74 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:11:07.839345: step 21880, loss = 0.73 (1022.7 examples/sec; 0.125 sec/batch)
2017-05-08 19:11:09.139547: step 21890, loss = 0.69 (984.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:11:10.496248: step 21900, loss = 0.86 (943.5 examples/sec; 0.136 sec/batch)
2017-05-08 19:11:11.639099: step 21910, loss = 0.74 (1120.0 examples/sec; 0.114 sec/batch)
2017-05-08 19:11:12.920108: step 21920, loss = 0.81 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:11:14.178110: step 21930, loss = 0.76 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:11:15.431220: step 21940, loss = 0.93 (1021.5 examples/sec; 0.125 sec/batch)
2017-05-08 19:11:16.693761: step 21950, loss = 0.76 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:11:17.943267: step 21960, loss = 0.73 (1024.4 examples/sec; 0.125 sec/batch)
2017-05-08 19:11:19.226672: step 21970, loss = 0.80 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:11:20.496967: step 21980, loss = 0.95 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:11:21.777159: step 21990, loss = 0.90 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:11:23.150793: step 22000, loss = 0.81 (931.8 examples/sec; 0.137 sec/batch)
2017-05-08 19:11:24.331143: step 22010, loss = 0.97 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-08 19:11:25.562121: step 22020, loss = 0.89 (1039.8 examples/sec; 0.123 sec/batch)
2017-05-08 19:11:26.838733: step 22030, loss = 0.90 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:11:28.118677: step 22040, loss = 0.86 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:11:29.389729: step 22050, loss = 0.75 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:11:30.672050: step 22060, loss = 1.08 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:11:31.951016: step 22070, loss = 0.85 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:11:33.221660: step 22080, loss = 0.82 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:11:34.477157: step 22090, loss = 0.74 (1019.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:11:35.862906: step 22100, loss = 0.85 (923.7 examples/sec; 0.139 sec/batch)
2017-05-08 19:11:37.048107: step 22110, loss = 0.67 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-08 19:11:38.326479: step 22120, loss = 0.87 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:11:39.603572: step 22130, loss = 0.87 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:11:40.885271: step 22140, loss = 0.86 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:11:42.188174: step 22150, loss = 0.87 (982.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:11:43.468872: step 22160, loss = 0.75 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:11:44.754092: step 22170, loss = 0.74 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:11:46.024053: step 22180, loss = 0.68 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:11:47.307191: step 22190, loss = 0.69 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:11:48.660043: step 22200, loss = 0.96 (946.1 examples/sec; 0.135 sec/batch)
2017-05-08 19:11:49.838181: step 22210, loss = 1.00 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-08 19:11:51.111697: step 22220, loss = 0.81 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:11:52.382122: step 22230, loss = 0.84 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:11:53.641547: step 22240, loss = 0.83 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:11:54.922413: step 22250, loss = 0.70 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:11:56.230558: step 22260, loss = 0.72 (978.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:11:57.518441: step 22270, loss = 0.86 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:11:58.774962: step 22280, loss = 0.79 (1018.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:12:00.038397: step 22290, loss = 0.83 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:12:01.421917: step 22300, loss = 0.87 (925.2 examples/sec; 0.138 sec/batch)
2017-05-08 19:12:02.577793: step 22310, loss = 0.89 (1107.4 examples/sec; 0.116 sec/batch)
2017-05-08 19:12:03.859175: step 22320, loss = 0.71 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:12:05.129117: step 22330, loss = 0.75 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:12:06.407536: step 22340, loss = 0.93 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:12:07.688110: step 22350, loss = 0.69 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:12:08.962410: step 22360, loss = 0.86 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:12:10.276659: step 22370, loss = 0.83 (973.9 examples/sec; 0.131 sec/batch)
2017-05-08 19:12:11.536378: step 22380, loss = 0.79 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:12:12.825816: step 22390, loss = 0.78 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:12:14.186869: step 22400, loss = 0.86 (940.4 examples/sec; 0.136 sec/batch)
2017-05-08 19:12:15.364738: step 22410, loss = 0.74 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-08 19:12:16.629935: step 22420, loss = 0.80 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:12:17.919572: step 22430, loss = 0.79 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:12:19.193961: step 22440, loss = 0.80 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:12:20.434159: step 22450, loss = 0.77 (1032.1 examples/sec; 0.124 sec/batch)
2017-05-08 19:12:21.702085: step 22460, loss = 0.84 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:12:22.974093: step 22470, loss = 0.80 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:12:24.289632: step 22480, loss = 0.74 (973.0 examples/sec; 0.132 sec/batch)
2017-05-08 19:12:25.587970: step 22490, loss = 0.70 (985.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:12:26.939982: step 22500, loss = 0.90 (946.7 examples/sec; 0.135 sec/batch)
2017-05-08 19:12:28.145945: step 22510, loss = 0.74 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-08 19:12:29.393663: step 22520, loss = 0.95 (1025.9 examples/sec; 0.125 sec/batch)
2017-05-08 19:12:30.668924: step 22530, loss = 0.94 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:12:31.939892: step 22540, loss = 0.88 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:12:33.218553: step 22550, loss = 1.10 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:12:34.489717: step 22560, loss = 0.68 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:12:35.794194: step 22570, loss = 0.89 (981.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:12:37.109565: step 22580, loss = 0.80 (973.1 examples/sec; 0.132 sec/batch)
2017-05-08 19:12:38.366159: step 22590, loss = 0.93 (1018.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:12:39.751599: step 22600, loss = 0.80 (923.9 examples/sec; 0.139 sec/batch)
2017-05-08 19:12:40.936396: step 22610, loss = 0.78 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-08 19:12:42.227524: step 22620, loss = 0.75 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:12:43.479296: step 22630, loss = 0.92 (1022.5 examples/sec; 0.125 sec/batch)
2017-05-08 19:12:44.738414: step 22640, loss = 0.64 (1016.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:12:46.034909: step 22650, loss = 0.60 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:12:47.327358: step 22660, loss = 0.77 (990.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:12:48.636736: step 22670, loss = 0.78 (977.6 examples/sec; 0.131 sec/batch)
2017-05-08 19:12:49.947893: step 22680, loss = 0.84 (976.2 examples/sec; 0.131 sec/batch)
2017-05-08 19:12:51.283017: step 22690, loss = 0.96 (958.7 examples/sec; 0.134 sec/batch)
2017-05-08 19:12:52.662408: step 22700, loss = 0.82 (927.9 examples/sec; 0.138 sec/batch)
2017-05-08 19:12:53.856682: step 22710, loss = 0.68 (1071.8E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 467 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
 examples/sec; 0.119 sec/batch)
2017-05-08 19:12:55.128529: step 22720, loss = 0.79 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:12:56.406245: step 22730, loss = 0.83 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:12:57.667894: step 22740, loss = 0.95 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:12:58.916642: step 22750, loss = 0.80 (1025.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:13:00.174762: step 22760, loss = 0.87 (1017.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:13:01.459613: step 22770, loss = 0.73 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:13:02.776416: step 22780, loss = 0.64 (972.0 examples/sec; 0.132 sec/batch)
2017-05-08 19:13:04.060810: step 22790, loss = 0.98 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:13:05.445658: step 22800, loss = 0.72 (924.3 examples/sec; 0.138 sec/batch)
2017-05-08 19:13:06.590206: step 22810, loss = 0.57 (1118.3 examples/sec; 0.114 sec/batch)
2017-05-08 19:13:07.833048: step 22820, loss = 0.74 (1029.9 examples/sec; 0.124 sec/batch)
2017-05-08 19:13:09.119072: step 22830, loss = 0.70 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:13:10.394803: step 22840, loss = 0.76 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:13:11.677385: step 22850, loss = 0.86 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:13:12.955197: step 22860, loss = 0.81 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:13:14.228145: step 22870, loss = 0.96 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:13:15.515506: step 22880, loss = 0.92 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:13:16.762740: step 22890, loss = 0.89 (1026.3 examples/sec; 0.125 sec/batch)
2017-05-08 19:13:18.130683: step 22900, loss = 0.85 (935.7 examples/sec; 0.137 sec/batch)
2017-05-08 19:13:19.304077: step 22910, loss = 0.68 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-08 19:13:20.580848: step 22920, loss = 0.70 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:13:21.900011: step 22930, loss = 0.79 (970.3 examples/sec; 0.132 sec/batch)
2017-05-08 19:13:23.182677: step 22940, loss = 0.81 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:13:24.462279: step 22950, loss = 0.75 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:13:25.736165: step 22960, loss = 0.91 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:13:26.990290: step 22970, loss = 0.70 (1020.6 examples/sec; 0.125 sec/batch)
2017-05-08 19:13:28.256546: step 22980, loss = 0.76 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:13:29.530055: step 22990, loss = 0.78 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:13:30.919045: step 23000, loss = 0.67 (921.5 examples/sec; 0.139 sec/batch)
2017-05-08 19:13:32.070652: step 23010, loss = 0.91 (1111.5 examples/sec; 0.115 sec/batch)
2017-05-08 19:13:33.335526: step 23020, loss = 0.66 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:13:34.645613: step 23030, loss = 0.76 (977.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:13:35.909323: step 23040, loss = 0.71 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:13:37.173389: step 23050, loss = 0.71 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:13:38.464092: step 23060, loss = 0.83 (991.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:13:39.732718: step 23070, loss = 0.79 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:13:41.019999: step 23080, loss = 0.82 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:13:42.300972: step 23090, loss = 0.68 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:13:43.672076: step 23100, loss = 0.70 (933.6 examples/sec; 0.137 sec/batch)
2017-05-08 19:13:44.847038: step 23110, loss = 0.86 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-08 19:13:46.091385: step 23120, loss = 0.80 (1028.6 examples/sec; 0.124 sec/batch)
2017-05-08 19:13:47.382679: step 23130, loss = 0.81 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:13:48.673653: step 23140, loss = 0.78 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:13:49.964738: step 23150, loss = 0.75 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:13:51.258323: step 23160, loss = 0.89 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:13:52.514418: step 23170, loss = 0.84 (1019.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:13:53.800672: step 23180, loss = 0.77 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:13:55.066405: step 23190, loss = 0.81 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:13:56.438742: step 23200, loss = 0.70 (932.7 examples/sec; 0.137 sec/batch)
2017-05-08 19:13:57.609195: step 23210, loss = 0.73 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-08 19:13:58.879045: step 23220, loss = 0.58 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:14:00.181223: step 23230, loss = 0.75 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:14:01.463750: step 23240, loss = 0.84 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:14:02.727809: step 23250, loss = 0.74 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:14:03.995436: step 23260, loss = 0.80 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:14:05.281171: step 23270, loss = 0.89 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:14:06.535002: step 23280, loss = 0.67 (1020.9 examples/sec; 0.125 sec/batch)
2017-05-08 19:14:07.790603: step 23290, loss = 0.84 (1019.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:14:09.151212: step 23300, loss = 0.75 (940.8 examples/sec; 0.136 sec/batch)
2017-05-08 19:14:10.325488: step 23310, loss = 0.79 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-08 19:14:11.582689: step 23320, loss = 0.80 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:14:12.841845: step 23330, loss = 0.91 (1016.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:14:14.106828: step 23340, loss = 0.69 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:14:15.392424: step 23350, loss = 0.90 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:14:16.678216: step 23360, loss = 0.54 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:14:17.984528: step 23370, loss = 0.69 (979.9 examples/sec; 0.131 sec/batch)
2017-05-08 19:14:19.262399: step 23380, loss = 0.74 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:14:20.534206: step 23390, loss = 0.85 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:14:21.894648: step 23400, loss = 0.81 (940.9 examples/sec; 0.136 sec/batch)
2017-05-08 19:14:23.067421: step 23410, loss = 0.74 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-08 19:14:24.341180: step 23420, loss = 1.00 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:14:25.627925: step 23430, loss = 0.78 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:14:26.900336: step 23440, loss = 0.79 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:14:28.147091: step 23450, loss = 0.74 (1026.7 examples/sec; 0.125 sec/batch)
2017-05-08 19:14:29.424157: step 23460, loss = 0.93 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:14:30.685632: step 23470, loss = 0.90 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:14:31.945758: step 23480, loss = 0.87 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:14:33.222493: step 23490, loss = 0.68 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:14:34.609510: step 23500, loss = 0.90 (922.8 examples/sec; 0.139 sec/batch)
2017-05-08 19:14:35.765063: step 23510, loss = 0.69 (1107.7 examples/sec; 0.116 sec/batch)
2017-05-08 19:14:37.036083: step 23520, loss = 0.65 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:14:38.294375: step 23530, loss = 0.91 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:14:39.570164: step 23540, loss = 0.84 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:14:40.854575: step 23550, loss = 0.79 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:14:42.113873: step 23560, loss = 0.94 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:14:43.381521: step 23570, loss = 0.84 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:14:44.667527: step 23580, loss = 0.68 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:14:45.929965: step 23590, loss = 0.76 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:14:47.278878: step 23600, loss = 0.84 (948.9 examples/sec; 0.135 sec/batch)
2017-05-08 19:14:48.453584: step 23610, loss = 0.94 (1089.6 examples/sec; 0.117 sec/batch)
2017-E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 488 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
05-08 19:14:49.754479: step 23620, loss = 0.83 (984.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:14:51.022239: step 23630, loss = 0.78 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:14:52.273428: step 23640, loss = 0.69 (1023.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:14:53.547328: step 23650, loss = 0.66 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:14:54.874071: step 23660, loss = 0.73 (964.8 examples/sec; 0.133 sec/batch)
2017-05-08 19:14:56.134566: step 23670, loss = 0.88 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:14:57.457695: step 23680, loss = 0.95 (967.4 examples/sec; 0.132 sec/batch)
2017-05-08 19:14:58.774850: step 23690, loss = 0.71 (971.8 examples/sec; 0.132 sec/batch)
2017-05-08 19:15:00.114061: step 23700, loss = 1.02 (955.8 examples/sec; 0.134 sec/batch)
2017-05-08 19:15:01.316240: step 23710, loss = 0.81 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-08 19:15:02.607904: step 23720, loss = 0.83 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:15:03.939035: step 23730, loss = 0.73 (961.6 examples/sec; 0.133 sec/batch)
2017-05-08 19:15:05.201536: step 23740, loss = 0.75 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:15:06.468654: step 23750, loss = 0.79 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:15:07.727975: step 23760, loss = 0.83 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:15:09.007905: step 23770, loss = 0.84 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:15:10.279884: step 23780, loss = 0.72 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:15:11.558589: step 23790, loss = 0.80 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:15:12.918874: step 23800, loss = 0.84 (941.0 examples/sec; 0.136 sec/batch)
2017-05-08 19:15:14.103886: step 23810, loss = 0.68 (1080.2 examples/sec; 0.119 sec/batch)
2017-05-08 19:15:15.355256: step 23820, loss = 0.80 (1022.9 examples/sec; 0.125 sec/batch)
2017-05-08 19:15:16.635306: step 23830, loss = 0.93 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:15:17.907825: step 23840, loss = 0.69 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:15:19.172413: step 23850, loss = 0.82 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:15:20.437513: step 23860, loss = 0.74 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:15:21.704562: step 23870, loss = 0.95 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:15:22.965601: step 23880, loss = 0.84 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:15:24.233191: step 23890, loss = 0.79 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:15:25.587122: step 23900, loss = 0.86 (945.4 examples/sec; 0.135 sec/batch)
2017-05-08 19:15:26.778059: step 23910, loss = 0.68 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-08 19:15:28.047233: step 23920, loss = 0.86 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:15:29.313810: step 23930, loss = 0.87 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:15:30.581838: step 23940, loss = 0.82 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:15:31.878813: step 23950, loss = 0.90 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:15:33.168125: step 23960, loss = 0.76 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:15:34.449444: step 23970, loss = 0.74 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:15:35.702911: step 23980, loss = 0.74 (1021.2 examples/sec; 0.125 sec/batch)
2017-05-08 19:15:36.962132: step 23990, loss = 1.07 (1016.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:15:38.319654: step 24000, loss = 0.80 (942.9 examples/sec; 0.136 sec/batch)
2017-05-08 19:15:39.503910: step 24010, loss = 0.75 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-08 19:15:40.792502: step 24020, loss = 0.93 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:15:42.045779: step 24030, loss = 0.88 (1021.3 examples/sec; 0.125 sec/batch)
2017-05-08 19:15:43.334102: step 24040, loss = 0.77 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:15:44.591589: step 24050, loss = 0.92 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:15:45.859094: step 24060, loss = 0.84 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:15:47.160488: step 24070, loss = 0.73 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:15:48.452397: step 24080, loss = 0.93 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:15:49.747015: step 24090, loss = 0.77 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:15:51.120922: step 24100, loss = 0.97 (931.6 examples/sec; 0.137 sec/batch)
2017-05-08 19:15:52.275562: step 24110, loss = 0.75 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-08 19:15:53.549558: step 24120, loss = 0.85 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:15:54.805230: step 24130, loss = 0.92 (1019.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:15:56.064024: step 24140, loss = 0.65 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:15:57.343067: step 24150, loss = 0.96 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:15:58.610552: step 24160, loss = 0.84 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:15:59.893238: step 24170, loss = 0.82 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:16:01.175610: step 24180, loss = 0.70 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:16:02.424528: step 24190, loss = 0.75 (1024.9 examples/sec; 0.125 sec/batch)
2017-05-08 19:16:03.780623: step 24200, loss = 0.75 (943.9 examples/sec; 0.136 sec/batch)
2017-05-08 19:16:05.061228: step 24210, loss = 0.72 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:16:06.264201: step 24220, loss = 0.75 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-08 19:16:07.538223: step 24230, loss = 0.76 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:16:08.817238: step 24240, loss = 0.79 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:16:10.090227: step 24250, loss = 0.78 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:16:11.383904: step 24260, loss = 0.82 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:16:12.652663: step 24270, loss = 0.70 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:16:13.922710: step 24280, loss = 0.78 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:16:15.186063: step 24290, loss = 0.73 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:16:16.537667: step 24300, loss = 0.93 (947.0 examples/sec; 0.135 sec/batch)
2017-05-08 19:16:17.753952: step 24310, loss = 0.79 (1052.4 examples/sec; 0.122 sec/batch)
2017-05-08 19:16:19.008343: step 24320, loss = 0.70 (1020.4 examples/sec; 0.125 sec/batch)
2017-05-08 19:16:20.285754: step 24330, loss = 0.87 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:16:21.558362: step 24340, loss = 0.80 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:16:22.840394: step 24350, loss = 0.82 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:16:24.109314: step 24360, loss = 0.79 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:16:25.390818: step 24370, loss = 0.82 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:16:26.674666: step 24380, loss = 0.75 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:16:27.996741: step 24390, loss = 0.68 (968.2 examples/sec; 0.132 sec/batch)
2017-05-08 19:16:29.356851: step 24400, loss = 0.77 (941.1 examples/sec; 0.136 sec/batch)
2017-05-08 19:16:30.514708: step 24410, loss = 0.81 (1105.5 examples/sec; 0.116 sec/batch)
2017-05-08 19:16:31.763610: step 24420, loss = 0.94 (1024.9 examples/sec; 0.125 sec/batch)
2017-05-08 19:16:33.015967: step 24430, loss = 0.86 (1022.1 examples/sec; 0.125 sec/batch)
2017-05-08 19:16:34.300541: step 24440, loss = 0.77 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:16:35.575415: step 24450, loss = 1.04 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:16:36.846287: step 24460, loss = 0.72 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:16:38.109435: step 24470, loss = 0.69 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:16:39.364118: step 24480, loss = 0.71 (1020.2 examples/sec; 0.125 sec/batch)
2017-05-08 19:16:40.636051: step 24490, loss = 0.75 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:16:42.034359: step 24500, loss = 0.74 (915.4 examples/sec; 0.140 sec/batch)
2017-05-08 19:16:43.230243: step 24510, loss = 0.81 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-08 19:16:44.493278: step 24520,E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 508 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
 loss = 0.82 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:16:45.769966: step 24530, loss = 0.73 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:16:47.045968: step 24540, loss = 0.71 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:16:48.316894: step 24550, loss = 0.73 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:16:49.603831: step 24560, loss = 0.96 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:16:50.880758: step 24570, loss = 0.75 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:16:52.211415: step 24580, loss = 0.72 (961.9 examples/sec; 0.133 sec/batch)
2017-05-08 19:16:53.449201: step 24590, loss = 0.85 (1034.1 examples/sec; 0.124 sec/batch)
2017-05-08 19:16:54.852720: step 24600, loss = 0.76 (912.0 examples/sec; 0.140 sec/batch)
2017-05-08 19:16:56.011876: step 24610, loss = 0.62 (1104.3 examples/sec; 0.116 sec/batch)
2017-05-08 19:16:57.304792: step 24620, loss = 0.69 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:16:58.549365: step 24630, loss = 0.85 (1028.5 examples/sec; 0.124 sec/batch)
2017-05-08 19:16:59.807616: step 24640, loss = 0.99 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:17:01.090747: step 24650, loss = 0.87 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:17:02.362137: step 24660, loss = 0.78 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:17:03.638333: step 24670, loss = 0.74 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:17:04.908738: step 24680, loss = 0.84 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:17:06.187760: step 24690, loss = 0.80 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:17:07.528699: step 24700, loss = 0.85 (954.6 examples/sec; 0.134 sec/batch)
2017-05-08 19:17:08.716686: step 24710, loss = 0.79 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-08 19:17:10.016728: step 24720, loss = 0.82 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:17:11.268202: step 24730, loss = 0.73 (1022.8 examples/sec; 0.125 sec/batch)
2017-05-08 19:17:12.507265: step 24740, loss = 0.82 (1033.0 examples/sec; 0.124 sec/batch)
2017-05-08 19:17:13.775052: step 24750, loss = 0.77 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:17:15.034944: step 24760, loss = 0.73 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:17:16.323027: step 24770, loss = 0.97 (993.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:17:17.611367: step 24780, loss = 0.80 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:17:18.892065: step 24790, loss = 0.84 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:17:20.271216: step 24800, loss = 0.63 (928.1 examples/sec; 0.138 sec/batch)
2017-05-08 19:17:21.437939: step 24810, loss = 0.85 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-08 19:17:22.695991: step 24820, loss = 0.92 (1017.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:17:23.949875: step 24830, loss = 0.83 (1020.8 examples/sec; 0.125 sec/batch)
2017-05-08 19:17:25.245439: step 24840, loss = 0.91 (988.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:17:26.540220: step 24850, loss = 0.75 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:17:27.822274: step 24860, loss = 0.88 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:17:29.107030: step 24870, loss = 0.79 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:17:30.359302: step 24880, loss = 0.78 (1022.1 examples/sec; 0.125 sec/batch)
2017-05-08 19:17:31.600876: step 24890, loss = 0.82 (1031.0 examples/sec; 0.124 sec/batch)
2017-05-08 19:17:32.972623: step 24900, loss = 0.91 (933.1 examples/sec; 0.137 sec/batch)
2017-05-08 19:17:34.155348: step 24910, loss = 0.79 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-08 19:17:35.417037: step 24920, loss = 0.80 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:17:36.697501: step 24930, loss = 0.79 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:17:37.987959: step 24940, loss = 0.79 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:17:39.285579: step 24950, loss = 0.60 (986.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:17:40.536808: step 24960, loss = 0.64 (1023.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:17:41.814480: step 24970, loss = 0.83 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:17:43.083940: step 24980, loss = 0.83 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:17:44.358721: step 24990, loss = 1.13 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:17:45.709558: step 25000, loss = 0.67 (947.6 examples/sec; 0.135 sec/batch)
2017-05-08 19:17:46.891783: step 25010, loss = 0.80 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-08 19:17:48.148213: step 25020, loss = 0.80 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:17:49.424414: step 25030, loss = 0.80 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:17:50.667542: step 25040, loss = 0.73 (1029.7 examples/sec; 0.124 sec/batch)
2017-05-08 19:17:51.917926: step 25050, loss = 0.90 (1023.7 examples/sec; 0.125 sec/batch)
2017-05-08 19:17:53.185081: step 25060, loss = 0.95 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:17:54.467069: step 25070, loss = 0.75 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:17:55.747863: step 25080, loss = 0.67 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:17:57.030833: step 25090, loss = 0.88 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:17:58.412680: step 25100, loss = 0.77 (926.3 examples/sec; 0.138 sec/batch)
2017-05-08 19:17:59.581172: step 25110, loss = 0.68 (1095.4 examples/sec; 0.117 sec/batch)
2017-05-08 19:18:00.847582: step 25120, loss = 0.79 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:18:02.140491: step 25130, loss = 0.74 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:18:03.384413: step 25140, loss = 0.74 (1029.0 examples/sec; 0.124 sec/batch)
2017-05-08 19:18:04.679563: step 25150, loss = 0.69 (988.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:18:05.953923: step 25160, loss = 0.80 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:18:07.213619: step 25170, loss = 0.79 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:18:08.453815: step 25180, loss = 0.90 (1032.1 examples/sec; 0.124 sec/batch)
2017-05-08 19:18:09.755649: step 25190, loss = 0.79 (983.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:18:11.128948: step 25200, loss = 0.73 (932.1 examples/sec; 0.137 sec/batch)
2017-05-08 19:18:12.303079: step 25210, loss = 0.82 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-08 19:18:13.584715: step 25220, loss = 0.78 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:18:14.835099: step 25230, loss = 0.88 (1023.7 examples/sec; 0.125 sec/batch)
2017-05-08 19:18:16.093566: step 25240, loss = 0.73 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:18:17.373367: step 25250, loss = 0.88 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:18:18.680801: step 25260, loss = 0.91 (979.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:18:19.956147: step 25270, loss = 0.83 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:18:21.223574: step 25280, loss = 0.87 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:18:22.490822: step 25290, loss = 0.77 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:18:23.861700: step 25300, loss = 0.67 (933.7 examples/sec; 0.137 sec/batch)
2017-05-08 19:18:25.099684: step 25310, loss = 0.83 (1033.9 examples/sec; 0.124 sec/batch)
2017-05-08 19:18:26.367299: step 25320, loss = 0.73 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:18:27.703676: step 25330, loss = 1.01 (957.8 examples/sec; 0.134 sec/batch)
2017-05-08 19:18:28.991340: step 25340, loss = 0.69 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:18:30.272548: step 25350, loss = 0.81 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:18:31.603758: step 25360, loss = 0.73 (961.5 examples/sec; 0.133 sec/batch)
2017-05-08 19:18:32.926562: step 25370, loss = 0.76 (967.6 examples/sec; 0.132 sec/batch)
2017-05-08 19:18:34.194546: step 25380, loss = 0.75 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:18:35.458003: step 25390, loss = 0.86 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:18:36.844486: step 25400, loss = 0.77 (923.2 examples/sec; 0.139 sec/batch)
2017-05-08 19:18:38.115769: step 25410, loss = 0.74 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:18:39.373567: step 25420, loss = 0.76 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:18:40.668887: step 25430, loss = 0.67 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:18:41.935159: step 25440, loss = 0.86 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:18:43.278206: step 25450, loss = 0.74 (953.1 examples/sec; 0.134 sec/batch)
2017-05-08 19:18:44.657706: step 25460, loss = 0.77 (927.9 examples/sec; 0.138 sec/batch)
2017-05-08 19:18:45.954911: step 25470, loss = 0.80 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:18:47.249146: step 25480, loss = 0.62 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:18:48.515559: step 25490, loss = 0.68 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:18:49.871203: step 25500, loss = 0.68 (944.2 examples/sec; 0.136 sec/batch)
2017-05-08 19:18:51.090901: step 25510, loss = 0.98 (1049.4 examples/sec; 0.122 sec/batch)
2017-05-08 19:18:52.416595: step 25520, loss = 0.82 (965.5 examples/sec; 0.133 sec/batch)
2017-05-08 19:18:53.702794: step 25530, loss = 1.02 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:18:55.014567: step 25540, loss = 0.72 (975.8 examples/sec; 0.131 sec/batch)
2017-05-08 19:18:56.350392: step 25550, loss = 0.82 (958.2 examples/sec; 0.134 sec/batch)
2017-05-08 19:18:57.637253: step 25560, loss = 0.72 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:18:58.932910: step 25570, loss = 0.90 (987.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:19:00.204868: step 25580, loss = 0.77 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:19:01.535138: step 25590, loss = 0.76 (962.2 examples/sec; 0.133 sec/batch)
2017-05-08 19:19:02.900823: step 25600, loss = 0.79 (937.3 examples/sec; 0.137 sec/batch)
2017-05-08 19:19:04.085802: step 25610, loss = 0.79 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-08 19:19:05.380605: step 25620, loss = 0.63 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:19:06.661844: step 25630, loss = 0.95 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:19:07.925983: step 25640, loss = 0.92 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:19:09.200738: step 25650, loss = 0.75 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:19:10.477250: step 25660, loss = 0.80 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:19:11.710248: step 25670, loss = 0.69 (1038.1 examples/sec; 0.123 sec/batch)
2017-05-08 19:19:12.983357: step 25680, loss = 0.90 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:19:14.246474: step 25690, loss = 0.79 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:19:15.610852: step 25700, loss = 0.72 (938.2 examples/sec; 0.136 sec/batch)
2017-05-08 19:19:16.784494: step 25710, loss = 0.83 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-08 19:19:18.054398: step 25720, loss = 0.82 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:19:19.316051: step 25730, loss = 0.71 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:19:20.581119: step 25740, loss = 0.85 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:19:21.846534: step 25750, loss = 0.76 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:19:23.111147: step 25760, loss = 0.71 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:19:24.376850: step 25770, loss = 0.82 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:19:25.672030: step 25780, loss = 0.67 (988.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:19:26.944974: step 25790, loss = 0.72 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:19:28.322714: step 25800, loss = 0.77 (929.1 examples/sec; 0.138 sec/batch)
2017-05-08 19:19:29.532280: step 25810, loss = 0.72 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-08 19:19:30.847734: step 25820, loss = 0.83 (973.1 examples/sec; 0.132 sec/batch)
2017-05-08 19:19:32.128401: step 25830, loss = 0.82 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:19:33.383646: step 25840, loss = 0.79 (1019.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:19:34.631247: step 25850, loss = 0.82 (1026.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:19:35.908951: step 25860, loss = 0.72 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:19:37.174117: step 25870, loss = 0.84 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-0E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 528 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
8 19:19:38.479053: step 25880, loss = 0.69 (980.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:19:39.719952: step 25890, loss = 0.72 (1031.5 examples/sec; 0.124 sec/batch)
2017-05-08 19:19:41.105249: step 25900, loss = 0.73 (924.0 examples/sec; 0.139 sec/batch)
2017-05-08 19:19:42.314389: step 25910, loss = 0.95 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-08 19:19:43.582169: step 25920, loss = 0.82 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:19:44.881172: step 25930, loss = 0.81 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:19:46.185952: step 25940, loss = 0.68 (981.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:19:47.481696: step 25950, loss = 0.75 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 19:19:48.793489: step 25960, loss = 0.75 (975.8 examples/sec; 0.131 sec/batch)
2017-05-08 19:19:50.083874: step 25970, loss = 0.79 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:19:51.399162: step 25980, loss = 0.84 (973.2 examples/sec; 0.132 sec/batch)
2017-05-08 19:19:52.712060: step 25990, loss = 0.83 (974.9 examples/sec; 0.131 sec/batch)
2017-05-08 19:19:54.053806: step 26000, loss = 0.75 (954.0 examples/sec; 0.134 sec/batch)
2017-05-08 19:19:55.237419: step 26010, loss = 0.78 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-08 19:19:56.505759: step 26020, loss = 0.77 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:19:57.769349: step 26030, loss = 0.74 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:19:59.037933: step 26040, loss = 0.83 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:20:00.290811: step 26050, loss = 0.83 (1021.7 examples/sec; 0.125 sec/batch)
2017-05-08 19:20:01.613730: step 26060, loss = 0.71 (967.6 examples/sec; 0.132 sec/batch)
2017-05-08 19:20:02.870096: step 26070, loss = 0.87 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:20:04.127190: step 26080, loss = 0.70 (1018.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:20:05.391791: step 26090, loss = 0.85 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:20:06.772627: step 26100, loss = 0.80 (927.0 examples/sec; 0.138 sec/batch)
2017-05-08 19:20:07.935661: step 26110, loss = 0.79 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-08 19:20:09.170920: step 26120, loss = 0.73 (1036.2 examples/sec; 0.124 sec/batch)
2017-05-08 19:20:10.430371: step 26130, loss = 0.78 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:20:11.714977: step 26140, loss = 0.85 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:20:13.020947: step 26150, loss = 0.66 (980.1 examples/sec; 0.131 sec/batch)
2017-05-08 19:20:14.296165: step 26160, loss = 0.77 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:20:15.598368: step 26170, loss = 0.80 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:20:16.835961: step 26180, loss = 0.81 (1034.3 examples/sec; 0.124 sec/batch)
2017-05-08 19:20:18.114038: step 26190, loss = 0.83 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:20:19.467775: step 26200, loss = 0.74 (945.5 examples/sec; 0.135 sec/batch)
2017-05-08 19:20:20.747049: step 26210, loss = 0.73 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:20:21.907639: step 26220, loss = 0.78 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-08 19:20:23.190086: step 26230, loss = 0.74 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:20:24.440665: step 26240, loss = 0.75 (1023.5 examples/sec; 0.125 sec/batch)
2017-05-08 19:20:25.708521: step 26250, loss = 0.61 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:20:26.988668: step 26260, loss = 0.78 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:20:28.248506: step 26270, loss = 0.75 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:20:29.537705: step 26280, loss = 0.72 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:20:30.783812: step 26290, loss = 0.98 (1027.2 examples/sec; 0.125 sec/batch)
2017-05-08 19:20:32.127694: step 26300, loss = 0.86 (952.5 examples/sec; 0.134 sec/batch)
2017-05-08 19:20:33.314604: step 26310, loss = 0.84 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-08 19:20:34.592865: step 26320, loss = 0.77 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:20:35.869382: step 26330, loss = 0.72 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:20:37.137869: step 26340, loss = 0.80 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:20:38.396548: step 26350, loss = 0.72 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:20:39.682637: step 26360, loss = 0.80 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:20:40.943645: step 26370, loss = 0.82 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:20:42.221391: step 26380, loss = 0.69 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:20:43.492426: step 26390, loss = 0.72 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:20:44.839014: step 26400, loss = 0.70 (950.6 examples/sec; 0.135 sec/batch)
2017-05-08 19:20:46.024232: step 26410, loss = 0.74 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-08 19:20:47.323950: step 26420, loss = 0.89 (984.8 examples/sec; 0.130 sec/batch)
2017-05-08 19:20:48.583631: step 26430, loss = 0.74 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:20:49.839346: step 26440, loss = 0.76 (1019.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:20:51.115049: step 26450, loss = 0.74 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:20:52.384713: step 26460, loss = 0.89 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:20:53.680935: step 26470, loss = 0.90 (987.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:20:54.935054: step 26480, loss = 0.72 (1020.6 examples/sec; 0.125 sec/batch)
2017-05-08 19:20:56.167831: step 26490, loss = 1.03 (1038.3 examples/sec; 0.123 sec/batch)
2017-05-08 19:20:57.522307: step 26500, loss = 0.74 (945.0 examples/sec; 0.135 sec/batch)
2017-05-08 19:20:58.709539: step 26510, loss = 0.75 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-08 19:21:00.028219: step 26520, loss = 0.70 (970.7 examples/sec; 0.132 sec/batch)
2017-05-08 19:21:01.324516: step 26530, loss = 1.03 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:21:02.593009: step 26540, loss = 0.80 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:21:03.875676: step 26550, loss = 0.79 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:21:05.137063: step 26560, loss = 0.83 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:21:06.445940: step 26570, loss = 0.76 (977.9 examples/sec; 0.131 sec/batch)
2017-05-08 19:21:07.699935: step 26580, loss = 1.00 (1020.7 examples/sec; 0.125 sec/batch)
2017-05-08 19:21:08.970396: step 26590, loss = 0.67 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:21:10.320567: step 26600, loss = 0.72 (948.0 examples/sec; 0.135 sec/batch)
2017-05-08 19:21:11.602862: step 26610, loss = 0.74 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:21:12.797056: step 26620, loss = 0.72 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-08 19:21:14.085564: step 26630, loss = 0.71 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:21:15.354982: step 26640, loss = 0.68 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:21:16.610584: step 26650, loss = 1.04 (1019.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:21:17.924939: step 26660, loss = 0.79 (973.9 examples/sec; 0.131 sec/batch)
2017-05-08 19:21:19.148000: step 26670, loss = 0.60 (1046.6 examples/sec; 0.122 sec/batch)
2017-05-08 19:21:20.392210: step 26680, loss = 0.74 (1028.8 examples/sec; 0.124 sec/batch)
2017-05-08 19:21:21.660517: step 26690, loss = 0.75 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:21:23.035309: step 26700, loss = 0.78 (931.0 examples/sec; 0.137 sec/batch)
2017-05-08 19:21:24.257679: step 26710, loss = 0.83 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-08 19:21:25.500311: step 26720, loss = 0.88 (1030.1 examples/sec; 0.124 sec/batch)
2017-05-08 19:21:26.754804: step 26730, loss = 0.85 (1020.3 examples/sec; 0.125 sec/batch)
2017-05-08 19:21:28.021851: step 26740, loss = 0.86 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:21:29.305186: step 26750, loss = 0.69 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:21:30.668143: step 26760, loss = 0.76 (939.1 examples/sec; 0.136 sec/batch)
2017-05-08 19:21:31.863134: step 26770, loss = 0.78 (1071.1 examples/sec; 0.119 sec/batch)
2017-05-08 19:21:33.124983: step 26780, loss = E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 548 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
0.94 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:21:34.393671: step 26790, loss = 0.76 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:21:35.740951: step 26800, loss = 0.96 (950.1 examples/sec; 0.135 sec/batch)
2017-05-08 19:21:36.937025: step 26810, loss = 0.90 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-08 19:21:38.198556: step 26820, loss = 0.75 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:21:39.462912: step 26830, loss = 0.73 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:21:40.744417: step 26840, loss = 0.80 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:21:42.053027: step 26850, loss = 0.75 (978.1 examples/sec; 0.131 sec/batch)
2017-05-08 19:21:43.306975: step 26860, loss = 0.69 (1020.8 examples/sec; 0.125 sec/batch)
2017-05-08 19:21:44.572343: step 26870, loss = 0.81 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:21:45.842970: step 26880, loss = 0.73 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:21:47.101799: step 26890, loss = 0.76 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:21:48.454072: step 26900, loss = 0.79 (946.6 examples/sec; 0.135 sec/batch)
2017-05-08 19:21:49.658990: step 26910, loss = 0.84 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-08 19:21:50.925155: step 26920, loss = 1.00 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:21:52.193278: step 26930, loss = 0.82 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:21:53.433118: step 26940, loss = 0.64 (1032.4 examples/sec; 0.124 sec/batch)
2017-05-08 19:21:54.704184: step 26950, loss = 0.78 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:21:55.998378: step 26960, loss = 0.82 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:21:57.269590: step 26970, loss = 0.92 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:21:58.558371: step 26980, loss = 0.78 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:21:59.839714: step 26990, loss = 0.93 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:22:01.223168: step 27000, loss = 0.75 (925.2 examples/sec; 0.138 sec/batch)
2017-05-08 19:22:02.401181: step 27010, loss = 0.83 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-08 19:22:03.653411: step 27020, loss = 1.03 (1022.2 examples/sec; 0.125 sec/batch)
2017-05-08 19:22:04.904240: step 27030, loss = 0.89 (1023.3 examples/sec; 0.125 sec/batch)
2017-05-08 19:22:06.175788: step 27040, loss = 0.85 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:22:07.457022: step 27050, loss = 0.82 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:22:08.715281: step 27060, loss = 0.75 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:22:10.010133: step 27070, loss = 0.68 (988.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:22:11.291183: step 27080, loss = 0.82 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:22:12.585226: step 27090, loss = 0.93 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:22:13.948181: step 27100, loss = 0.74 (939.1 examples/sec; 0.136 sec/batch)
2017-05-08 19:22:15.217088: step 27110, loss = 0.76 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:22:16.426903: step 27120, loss = 0.86 (1058.0 examples/sec; 0.121 sec/batch)
2017-05-08 19:22:17.723049: step 27130, loss = 0.87 (987.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:22:19.023032: step 27140, loss = 0.83 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:22:20.320032: step 27150, loss = 0.77 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:22:21.602437: step 27160, loss = 0.76 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:22:22.880972: step 27170, loss = 0.78 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:22:24.138808: step 27180, loss = 0.81 (1017.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:22:25.415003: step 27190, loss = 0.80 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:22:26.790612: step 27200, loss = 0.71 (930.5 examples/sec; 0.138 sec/batch)
2017-05-08 19:22:27.972587: step 27210, loss = 0.71 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-08 19:22:29.224604: step 27220, loss = 0.72 (1022.3 examples/sec; 0.125 sec/batch)
2017-05-08 19:22:30.525068: step 27230, loss = 0.71 (984.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:22:31.813342: step 27240, loss = 0.91 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:22:33.079617: step 27250, loss = 0.83 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:22:34.347849: step 27260, loss = 0.85 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:22:35.629208: step 27270, loss = 0.77 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:22:36.901391: step 27280, loss = 0.73 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:22:38.147768: step 27290, loss = 0.69 (1027.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:22:39.511832: step 27300, loss = 0.67 (938.4 examples/sec; 0.136 sec/batch)
2017-05-08 19:22:40.708474: step 27310, loss = 0.67 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-08 19:22:41.938565: step 27320, loss = 0.74 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-08 19:22:43.209477: step 27330, loss = 0.66 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:22:44.477711: step 27340, loss = 0.90 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:22:45.757266: step 27350, loss = 0.85 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:22:47.011290: step 27360, loss = 0.86 (1020.7 examples/sec; 0.125 sec/batch)
2017-05-08 19:22:48.280191: step 27370, loss = 0.86 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:22:49.526522: step 27380, loss = 0.84 (1027.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:22:50.812343: step 27390, loss = 0.86 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:22:52.162380: step 27400, loss = 0.98 (948.1 examples/sec; 0.135 sec/batch)
2017-05-08 19:22:53.329749: step 27410, loss = 0.84 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-08 19:22:54.601139: step 27420, loss = 0.75 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:22:55.853319: step 27430, loss = 0.89 (1022.2 examples/sec; 0.125 sec/batch)
2017-05-08 19:22:57.130293: step 27440, loss = 0.71 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:22:58.385312: step 27450, loss = 0.84 (1019.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:22:59.676607: step 27460, loss = 0.69 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:23:00.945533: step 27470, loss = 0.67 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:23:02.215385: step 27480, loss = 0.83 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:23:03.483662: step 27490, loss = 0.79 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:23:04.829803: step 27500, loss = 0.84 (950.9 examples/sec; 0.135 sec/batch)
2017-05-08 19:23:06.115315: step 27510, loss = 0.86 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:23:07.297331: step 27520, loss = 0.80 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-08 19:23:08.555471: step 27530, loss = 0.74 (1017.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:23:09.832686: step 27540, loss = 0.80 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:23:11.091598: step 27550, loss = 0.74 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:23:12.345057: step 27560, loss = 0.74 (1021.2 examples/sec; 0.125 sec/batch)
2017-05-08 19:23:13.621280: step 27570, loss = 0.77 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:23:14.914091: step 27580, loss = 0.81 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:23:16.190368: step 27590, loss = 0.84 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:23:17.568354: step 27600, loss = 0.69 (928.9 examples/sec; 0.138 sec/batch)
2017-05-08 19:23:18.790566: step 27610, loss = 0.76 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-08 19:23:20.038243: step 27620, loss = 0.86 (1025.9 examples/sec; 0.125 sec/batch)
2017-05-08 19:23:21.346718: step 27630, loss = 0.70 (978.2 examples/sec; 0.131 sec/batch)
2017-05-08 19:23:22.640569: step 27640, loss = 0.73 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:23:23.961756: step 27650, loss = 0.79 (968.8 examples/sec; 0.132 sec/batch)
2017-05-08 19:23:25.233640: step 27660, loss = 0.86 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:23:26.507783: step 27670, loss = 0.93 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:23:27.766974: step 27680, loss = 0.81 (1016.5 examples/sec; 0.126 sE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 568 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
ec/batch)
2017-05-08 19:23:29.055956: step 27690, loss = 0.82 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:23:30.433654: step 27700, loss = 0.76 (929.6 examples/sec; 0.138 sec/batch)
2017-05-08 19:23:31.668812: step 27710, loss = 0.79 (1035.7 examples/sec; 0.124 sec/batch)
2017-05-08 19:23:32.875971: step 27720, loss = 1.01 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-08 19:23:34.154158: step 27730, loss = 0.80 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:23:35.411871: step 27740, loss = 0.86 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:23:36.698613: step 27750, loss = 0.75 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:23:37.975286: step 27760, loss = 0.68 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:23:39.239149: step 27770, loss = 0.65 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:23:40.513118: step 27780, loss = 0.88 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:23:41.773836: step 27790, loss = 0.77 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:23:43.163732: step 27800, loss = 0.94 (920.9 examples/sec; 0.139 sec/batch)
2017-05-08 19:23:44.463303: step 27810, loss = 0.77 (984.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:23:45.644952: step 27820, loss = 0.76 (1083.2 examples/sec; 0.118 sec/batch)
2017-05-08 19:23:46.933329: step 27830, loss = 0.70 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:23:48.216335: step 27840, loss = 0.66 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:23:49.506967: step 27850, loss = 0.99 (991.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:23:50.791425: step 27860, loss = 0.84 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:23:52.078084: step 27870, loss = 0.73 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:23:53.389497: step 27880, loss = 0.70 (976.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:23:54.656269: step 27890, loss = 0.62 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:23:56.033837: step 27900, loss = 0.82 (929.2 examples/sec; 0.138 sec/batch)
2017-05-08 19:23:57.209882: step 27910, loss = 0.74 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-08 19:23:58.473301: step 27920, loss = 0.74 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:23:59.758550: step 27930, loss = 0.85 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:24:01.072201: step 27940, loss = 0.74 (974.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:24:02.340717: step 27950, loss = 0.69 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:24:03.638413: step 27960, loss = 0.78 (986.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:24:04.884945: step 27970, loss = 0.74 (1026.9 examples/sec; 0.125 sec/batch)
2017-05-08 19:24:06.143241: step 27980, loss = 0.81 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:24:07.423990: step 27990, loss = 0.74 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:24:08.795562: step 28000, loss = 0.87 (933.2 examples/sec; 0.137 sec/batch)
2017-05-08 19:24:09.970746: step 28010, loss = 0.86 (1089.2 examples/sec; 0.118 sec/batch)
2017-05-08 19:24:11.221391: step 28020, loss = 0.80 (1023.5 examples/sec; 0.125 sec/batch)
2017-05-08 19:24:12.494706: step 28030, loss = 0.87 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:24:13.764914: step 28040, loss = 0.82 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:24:15.046950: step 28050, loss = 0.73 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:24:16.312433: step 28060, loss = 0.86 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:24:17.582947: step 28070, loss = 0.88 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:24:18.844481: step 28080, loss = 0.72 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:24:20.087238: step 28090, loss = 0.78 (1029.9 examples/sec; 0.124 sec/batch)
2017-05-08 19:24:21.467431: step 28100, loss = 0.82 (927.4 examples/sec; 0.138 sec/batch)
2017-05-08 19:24:22.649700: step 28110, loss = 0.71 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-08 19:24:23.916224: step 28120, loss = 0.84 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:24:25.194972: step 28130, loss = 0.71 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:24:26.446358: step 28140, loss = 0.85 (1022.9 examples/sec; 0.125 sec/batch)
2017-05-08 19:24:27.707713: step 28150, loss = 0.77 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:24:28.990263: step 28160, loss = 0.83 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:24:30.259807: step 28170, loss = 0.77 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:24:31.522189: step 28180, loss = 0.69 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:24:32.805568: step 28190, loss = 0.69 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:24:34.174954: step 28200, loss = 0.72 (934.7 examples/sec; 0.137 sec/batch)
2017-05-08 19:24:35.349292: step 28210, loss = 0.84 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-08 19:24:36.626424: step 28220, loss = 0.89 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:24:37.899243: step 28230, loss = 0.84 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:24:39.186860: step 28240, loss = 0.72 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:24:40.498319: step 28250, loss = 0.78 (976.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:24:41.766840: step 28260, loss = 0.95 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:24:43.022854: step 28270, loss = 0.72 (1019.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:24:44.307359: step 28280, loss = 0.73 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:24:45.576228: step 28290, loss = 0.69 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:24:46.958729: step 28300, loss = 0.80 (925.9 examples/sec; 0.138 sec/batch)
2017-05-08 19:24:48.157076: step 28310, loss = 0.94 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-08 19:24:49.408068: step 28320, loss = 0.72 (1023.2 examples/sec; 0.125 sec/batch)
2017-05-08 19:24:50.684482: step 28330, loss = 0.72 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:24:51.935642: step 28340, loss = 0.78 (1023.1 examples/sec; 0.125 sec/batch)
2017-05-08 19:24:53.243159: step 28350, loss = 0.92 (979.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:24:54.483605: step 28360, loss = 0.82 (1031.9 examples/sec; 0.124 sec/batch)
2017-05-08 19:24:55.758416: step 28370, loss = 0.81 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:24:57.046485: step 28380, loss = 0.78 (993.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:24:58.329139: step 28390, loss = 0.78 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:24:59.686938: step 28400, loss = 0.75 (942.7 examples/sec; 0.136 sec/batch)
2017-05-08 19:25:00.887900: step 28410, loss = 0.75 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-08 19:25:02.191296: step 28420, loss = 0.82 (982.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:25:03.460551: step 28430, loss = 0.79 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:25:04.731957: step 28440, loss = 0.75 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:25:05.999173: step 28450, loss = 0.76 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:25:07.267391: step 28460, loss = 0.75 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:25:08.523943: step 28470, loss = 0.77 (1018.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:25:09.802321: step 28480, loss = 0.90 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:25:11.077870: step 28490, loss = 0.89 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:25:12.505263: step 28500, loss = 0.80 (896.7 examples/sec; 0.143 sec/batch)
2017-05-08 19:25:13.640024: step 28510, loss = 0.66 (1128.0 examples/sec; 0.113 sec/batch)
2017-05-08 19:25:14.983083: step 28520, loss = 0.85 (953.0 examples/sec; 0.134 sec/batch)
2017-05-08 19:25:16.267259: step 28530, loss = 0.74 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:25:17.583966: step 28540, loss = 0.72 (972.1 examples/sec; 0.132 sec/batch)
2017-05-08 19:25:18.881627: step 28550, loss = 0.78 (986.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:25:20.184784: step 28560, loss = 0.71 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:25:21.464743: step 28570, loss = 0.79 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:25:22.773316: step 28580, loss = 0.90 (978.2 examples/sec; 0.131 sec/batch)
2017-05-08 19:25:24.067245: stE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 589 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
ep 28590, loss = 1.07 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:25:25.446621: step 28600, loss = 0.64 (928.0 examples/sec; 0.138 sec/batch)
2017-05-08 19:25:26.642359: step 28610, loss = 0.91 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-08 19:25:27.883594: step 28620, loss = 0.97 (1031.2 examples/sec; 0.124 sec/batch)
2017-05-08 19:25:29.145857: step 28630, loss = 0.84 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:25:30.451503: step 28640, loss = 0.92 (980.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:25:31.692010: step 28650, loss = 0.83 (1031.8 examples/sec; 0.124 sec/batch)
2017-05-08 19:25:32.969516: step 28660, loss = 0.78 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:25:34.240275: step 28670, loss = 0.65 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:25:35.515267: step 28680, loss = 0.76 (1003.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:25:36.804156: step 28690, loss = 0.69 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:25:38.171726: step 28700, loss = 1.00 (936.0 examples/sec; 0.137 sec/batch)
2017-05-08 19:25:39.349830: step 28710, loss = 0.72 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-08 19:25:40.608948: step 28720, loss = 0.80 (1016.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:25:41.860987: step 28730, loss = 0.75 (1022.3 examples/sec; 0.125 sec/batch)
2017-05-08 19:25:43.136858: step 28740, loss = 0.80 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:25:44.389672: step 28750, loss = 0.82 (1021.7 examples/sec; 0.125 sec/batch)
2017-05-08 19:25:45.629239: step 28760, loss = 0.94 (1032.6 examples/sec; 0.124 sec/batch)
2017-05-08 19:25:46.901977: step 28770, loss = 0.73 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:25:48.170518: step 28780, loss = 0.78 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:25:49.450656: step 28790, loss = 0.70 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:25:50.804964: step 28800, loss = 0.83 (945.1 examples/sec; 0.135 sec/batch)
2017-05-08 19:25:51.985631: step 28810, loss = 0.98 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-08 19:25:53.255952: step 28820, loss = 0.79 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:25:54.515390: step 28830, loss = 0.79 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:25:55.783501: step 28840, loss = 0.83 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:25:57.033102: step 28850, loss = 0.69 (1024.3 examples/sec; 0.125 sec/batch)
2017-05-08 19:25:58.320380: step 28860, loss = 0.82 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:25:59.597738: step 28870, loss = 0.66 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:26:00.873329: step 28880, loss = 0.79 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:26:02.168791: step 28890, loss = 1.04 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:26:03.528669: step 28900, loss = 0.82 (941.3 examples/sec; 0.136 sec/batch)
2017-05-08 19:26:04.727953: step 28910, loss = 0.84 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-08 19:26:05.947995: step 28920, loss = 0.83 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-08 19:26:07.219828: step 28930, loss = 0.66 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:26:08.504876: step 28940, loss = 0.73 (996.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:26:09.788587: step 28950, loss = 0.78 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:26:11.060853: step 28960, loss = 0.79 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:26:12.340255: step 28970, loss = 0.81 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:26:13.624227: step 28980, loss = 0.63 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:26:14.933023: step 28990, loss = 0.80 (978.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:26:16.289988: step 29000, loss = 0.90 (943.3 examples/sec; 0.136 sec/batch)
2017-05-08 19:26:17.469257: step 29010, loss = 0.67 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-08 19:26:18.734835: step 29020, loss = 0.82 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:26:19.981533: step 29030, loss = 0.72 (1026.7 examples/sec; 0.125 sec/batch)
2017-05-08 19:26:21.252659: step 29040, loss = 0.88 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:26:22.531232: step 29050, loss = 0.69 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:26:23.835891: step 29060, loss = 0.82 (981.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:26:25.130249: step 29070, loss = 0.89 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:26:26.419606: step 29080, loss = 0.73 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:26:27.698923: step 29090, loss = 0.79 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:26:29.094358: step 29100, loss = 0.69 (917.3 examples/sec; 0.140 sec/batch)
2017-05-08 19:26:30.270649: step 29110, loss = 0.84 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-08 19:26:31.536750: step 29120, loss = 0.82 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:26:32.810074: step 29130, loss = 0.89 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:26:34.085196: step 29140, loss = 0.72 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:26:35.372936: step 29150, loss = 0.86 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:26:36.615358: step 29160, loss = 0.82 (1030.2 examples/sec; 0.124 sec/batch)
2017-05-08 19:26:37.896642: step 29170, loss = 0.76 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:26:39.189106: step 29180, loss = 0.63 (990.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:26:40.463836: step 29190, loss = 0.65 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:26:41.902946: step 29200, loss = 0.79 (889.4 examples/sec; 0.144 sec/batch)
2017-05-08 19:26:43.072276: step 29210, loss = 0.77 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-08 19:26:44.350750: step 29220, loss = 0.65 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:26:45.611551: step 29230, loss = 0.83 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:26:46.864590: step 29240, loss = 0.85 (1021.5 examples/sec; 0.125 sec/batch)
2017-05-08 19:26:48.171218: step 29250, loss = 0.73 (979.6 examples/sec; 0.131 sec/batch)
2017-05-08 19:26:49.386646: step 29260, loss = 0.71 (1053.1 examples/sec; 0.122 sec/batch)
2017-05-08 19:26:50.662721: step 29270, loss = 0.79 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:26:51.943914: step 29280, loss = 0.76 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:26:53.237505: step 29290, loss = 0.74 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:26:54.607152: step 29300, loss = 0.70 (934.6 examples/sec; 0.137 sec/batch)
2017-05-08 19:26:55.772988: step 29310, loss = 0.92 (1097.9 examples/sec; 0.117 sec/batch)
2017-05-08 19:26:57.010171: step 29320, loss = 0.88 (1034.6 examples/sec; 0.124 sec/batch)
2017-05-08 19:26:58.289991: step 29330, loss = 0.84 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:26:59.555673: step 29340, loss = 0.70 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:27:00.840019: step 29350, loss = 0.91 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:27:02.128617: step 29360, loss = 0.75 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:27:03.403036: step 29370, loss = 1.02 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:27:04.651791: step 29380, loss = 0.70 (1025.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:27:05.936656: step 29390, loss = 0.85 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:27:07.301490: step 29400, loss = 0.88 (937.8 examples/sec; 0.136 sec/batch)
2017-05-08 19:27:08.446626: step 29410, loss = 0.69 (1117.8 examples/sec; 0.115 sec/batch)
2017-05-08 19:27:09.719526: step 29420, loss = 0.73 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:27:10.989937: step 29430, loss = 0.83 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:27:12.286078: step 29440, loss = 0.80 (987.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:27:13.584726: step 29450, loss = 0.66 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:27:14.846289: step 29460, loss = 0.61 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:27:16.110024: step 29470, loss = 0.79 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:27:17.388648: step 29480, loss = 0.68 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:27:18.644008: step 29490, loss = 0.81 (1019.6 exampE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 609 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
les/sec; 0.126 sec/batch)
2017-05-08 19:27:20.026852: step 29500, loss = 0.84 (925.6 examples/sec; 0.138 sec/batch)
2017-05-08 19:27:21.276442: step 29510, loss = 0.73 (1024.3 examples/sec; 0.125 sec/batch)
2017-05-08 19:27:22.497587: step 29520, loss = 0.89 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-08 19:27:23.776844: step 29530, loss = 0.72 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:27:25.033363: step 29540, loss = 0.73 (1018.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:27:26.302247: step 29550, loss = 0.94 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:27:27.571637: step 29560, loss = 0.75 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:27:28.848648: step 29570, loss = 0.73 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:27:30.113261: step 29580, loss = 0.75 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:27:31.375843: step 29590, loss = 0.82 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:27:32.731407: step 29600, loss = 0.81 (944.3 examples/sec; 0.136 sec/batch)
2017-05-08 19:27:33.924173: step 29610, loss = 0.69 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-08 19:27:35.173082: step 29620, loss = 0.72 (1024.9 examples/sec; 0.125 sec/batch)
2017-05-08 19:27:36.437236: step 29630, loss = 0.82 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:27:37.685572: step 29640, loss = 0.76 (1025.4 examples/sec; 0.125 sec/batch)
2017-05-08 19:27:38.983727: step 29650, loss = 0.87 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:27:40.237852: step 29660, loss = 0.73 (1020.6 examples/sec; 0.125 sec/batch)
2017-05-08 19:27:41.547392: step 29670, loss = 0.82 (977.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:27:42.824123: step 29680, loss = 0.86 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:27:44.123662: step 29690, loss = 0.63 (985.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:27:45.510091: step 29700, loss = 0.69 (923.2 examples/sec; 0.139 sec/batch)
2017-05-08 19:27:46.660232: step 29710, loss = 0.70 (1112.9 examples/sec; 0.115 sec/batch)
2017-05-08 19:27:47.932962: step 29720, loss = 0.86 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:27:49.200070: step 29730, loss = 0.72 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:27:50.462189: step 29740, loss = 0.81 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:27:51.756703: step 29750, loss = 0.78 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:27:53.013364: step 29760, loss = 0.72 (1018.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:27:54.284012: step 29770, loss = 0.84 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:27:55.569921: step 29780, loss = 0.97 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:27:56.872732: step 29790, loss = 0.81 (982.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:27:58.231771: step 29800, loss = 0.74 (941.9 examples/sec; 0.136 sec/batch)
2017-05-08 19:27:59.440723: step 29810, loss = 0.75 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-08 19:28:00.750573: step 29820, loss = 0.75 (977.2 examples/sec; 0.131 sec/batch)
2017-05-08 19:28:02.038965: step 29830, loss = 0.82 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:28:03.337226: step 29840, loss = 0.71 (985.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:28:04.601113: step 29850, loss = 0.88 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:28:05.876066: step 29860, loss = 0.72 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:28:07.173144: step 29870, loss = 1.15 (986.8 examples/sec; 0.130 sec/batch)
2017-05-08 19:28:08.432750: step 29880, loss = 0.68 (1016.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:28:09.710555: step 29890, loss = 0.72 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:28:11.073632: step 29900, loss = 0.72 (939.1 examples/sec; 0.136 sec/batch)
2017-05-08 19:28:12.241908: step 29910, loss = 0.65 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-08 19:28:13.509721: step 29920, loss = 0.70 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:28:14.760614: step 29930, loss = 0.97 (1023.3 examples/sec; 0.125 sec/batch)
2017-05-08 19:28:16.015536: step 29940, loss = 0.67 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:28:17.279531: step 29950, loss = 0.80 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:28:18.583253: step 29960, loss = 0.98 (981.8 examples/sec; 0.130 sec/batch)
2017-05-08 19:28:19.836123: step 29970, loss = 0.64 (1021.7 examples/sec; 0.125 sec/batch)
2017-05-08 19:28:21.101863: step 29980, loss = 0.80 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:28:22.371772: step 29990, loss = 0.85 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:28:23.719960: step 30000, loss = 0.88 (949.4 examples/sec; 0.135 sec/batch)
2017-05-08 19:28:24.885200: step 30010, loss = 0.89 (1098.5 examples/sec; 0.117 sec/batch)
2017-05-08 19:28:26.153195: step 30020, loss = 0.67 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:28:27.443692: step 30030, loss = 0.92 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:28:28.705839: step 30040, loss = 0.74 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:28:29.967941: step 30050, loss = 0.73 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:28:31.216583: step 30060, loss = 0.84 (1025.1 examples/sec; 0.125 sec/batch)
2017-05-08 19:28:32.487012: step 30070, loss = 0.71 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:28:33.750968: step 30080, loss = 0.81 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:28:35.015299: step 30090, loss = 0.60 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:28:36.359153: step 30100, loss = 0.78 (952.5 examples/sec; 0.134 sec/batch)
2017-05-08 19:28:37.568772: step 30110, loss = 0.86 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-08 19:28:38.844326: step 30120, loss = 0.83 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:28:40.117297: step 30130, loss = 0.86 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:28:41.414170: step 30140, loss = 0.68 (987.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:28:42.665090: step 30150, loss = 0.68 (1023.2 examples/sec; 0.125 sec/batch)
2017-05-08 19:28:43.920113: step 30160, loss = 0.85 (1019.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:28:45.177140: step 30170, loss = 0.68 (1018.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:28:46.460625: step 30180, loss = 0.80 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:28:47.748770: step 30190, loss = 0.84 (993.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:28:49.142796: step 30200, loss = 0.75 (918.2 examples/sec; 0.139 sec/batch)
2017-05-08 19:28:50.358170: step 30210, loss = 0.82 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-08 19:28:51.594095: step 30220, loss = 0.98 (1035.7 examples/sec; 0.124 sec/batch)
2017-05-08 19:28:52.856728: step 30230, loss = 0.79 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:28:54.117874: step 30240, loss = 0.85 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:28:55.375901: step 30250, loss = 0.68 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:28:56.627251: step 30260, loss = 0.91 (1022.9 examples/sec; 0.125 sec/batch)
2017-05-08 19:28:57.886427: step 30270, loss = 0.82 (1016.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:28:59.152346: step 30280, loss = 0.63 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:29:00.412968: step 30290, loss = 0.87 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:29:01.783845: step 30300, loss = 0.91 (933.7 examples/sec; 0.137 sec/batch)
2017-05-08 19:29:02.947237: step 30310, loss = 0.88 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-08 19:29:04.195279: step 30320, loss = 0.79 (1025.6 examples/sec; 0.125 sec/batch)
2017-05-08 19:29:05.455870: step 30330, loss = 0.86 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:29:06.739492: step 30340, loss = 0.80 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:29:08.039183: step 30350, loss = 0.80 (984.8 examples/sec; 0.130 sec/batch)
2017-05-08 19:29:09.326922: step 30360, loss = 0.65 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:29:10.595512: step 30370, loss = 0.79 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:29:11.854489: step 30380, loss = 0.81 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:29:13.127373: step 30390, loss = 0.68 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:29:14.480524: step 30400, loss = 0.94 (945.9 examples/sec; 0.135 sec/batch)
2017-05-08 19:29:15.655864: step 30410, loss = 0.71 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-08 19:29:16.932191: step 30420, loss = 0.78 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:29:18.218612: step 30430, loss = 0.94 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:29:19.496767: step 30440, loss = 0.86 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:29:20.772614: step 30450, loss = 1.06 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:29:22.084277: step 30460, loss = 0.70 (975.9 examples/sec; 0.131 sec/batch)
2017-05-08 19:29:23.354584: step 30470, loss = 0.77 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:29:24.619233: step 30480, loss = 0.81 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:29:25.890531: step 30490, loss = 0.82 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:29:27.254389: step 30500, loss = 0.68 (938.5 examples/sec; 0.136 sec/batch)
2017-05-08 19:29:28.408096: step 30510, loss = 0.74 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-08 19:29:29.679886: step 30520, loss = 0.85 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:29:30.963236: step 30530, loss = 0.85 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:29:32.226637: step 30540, loss = 0.66 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:29:33.488929: step 30550, loss = 0.73 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:29:34.749915: step 30560, loss = 0.91 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:29:35.998881: step 30570, loss = 0.90 (1024.8 examples/sec; 0.125 sec/batch)
2017-05-08 19:29:37.287099: step 30580, loss = 0.71 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:29:38.594808: step 30590, loss = 0.79 (978.8 examples/sec; 0.131 sec/batch)
2017-05-08 19:29:39.958808: step 30600, loss = 0.92 (938.4 examples/sec; 0.136 sec/batch)
2017-05-08 19:29:41.136166: step 30610, loss = 0.78 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-08 19:29:42.429155: step 30620, loss = 0.68 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:29:43.722496: step 30630, loss = 0.76 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:29:44.961870: step 30640, loss = 0.74 (1032.7 examples/sec; 0.124 sec/batch)
2017-05-08 19:29:46.252682: step 30650, loss = 0.86 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:29:47.567559: step 30660, loss = 0.74 (973.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:29:48.835435: step 30670, loss = 0.76 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:29:50.121253: step 30680, loss = 0.63 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:29:51.395030: step 30690, loss = 0.76 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:29:52.765050: step 30700, loss = 0.65 (934.3 examples/sec; 0.137 sec/batch)
2017-05-08 19:29:53.926908: step 30710, loss = 0.88 (1101.7 examples/sec; 0.116 sec/batch)
2017-05-08 19:29:55.223737: step 30720, loss = 0.68 (987.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:29:56.479910: step 30730, loss = 0.99 (1019.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:29:57.750198: step 30740, loss = 0.90 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:29:58.998911: step 30750, loss = 0.78 (1025.1 examples/sec; 0.125 sec/batch)
2017-05-08 19:30:00.277229: step 30760, loss = 0.80 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:30:01.542301: step 30770, loss = 0.65 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:30:02.839348: step 30780, loss = 0.66 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:30:04.109632: step 30790, loss = 0.82 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:30:05.468395: step 30800, loss = 0.67 (942.0 examples/sec; 0.136 sec/batch)
2017-05-08 19:30:06.650388: step 30810, loss = 0.82 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-08 19:30:07.897306: step 30820, loss = 0.86 (1026.5 examples/sec; 0.125 sec/batch)
2017-05-08 19:30:09.205721: step 30830, loss = 0.80 (978.3 examples/sec; 0.131 sec/batch)
2017-05-08 19:30:10.453645: step 30840, loss = 0.80 (1025.7 examples/sec; 0.125 sec/batch)
2017-05-08 19:30:11.69E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 629 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
9524: step 30850, loss = 0.89 (1027.4 examples/sec; 0.125 sec/batch)
2017-05-08 19:30:12.967246: step 30860, loss = 0.75 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:30:14.228060: step 30870, loss = 0.72 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:30:15.481909: step 30880, loss = 0.67 (1020.9 examples/sec; 0.125 sec/batch)
2017-05-08 19:30:16.745540: step 30890, loss = 0.70 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:30:18.129034: step 30900, loss = 0.72 (925.2 examples/sec; 0.138 sec/batch)
2017-05-08 19:30:19.301638: step 30910, loss = 0.81 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-08 19:30:20.596949: step 30920, loss = 0.81 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:30:21.867278: step 30930, loss = 0.84 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:30:23.134331: step 30940, loss = 0.75 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:30:24.394641: step 30950, loss = 0.68 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:30:25.646028: step 30960, loss = 0.91 (1022.9 examples/sec; 0.125 sec/batch)
2017-05-08 19:30:26.898589: step 30970, loss = 0.70 (1021.9 examples/sec; 0.125 sec/batch)
2017-05-08 19:30:28.149847: step 30980, loss = 0.70 (1023.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:30:29.422860: step 30990, loss = 0.64 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:30:30.795493: step 31000, loss = 0.84 (932.5 examples/sec; 0.137 sec/batch)
2017-05-08 19:30:31.993869: step 31010, loss = 0.67 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-08 19:30:33.248184: step 31020, loss = 0.82 (1020.5 examples/sec; 0.125 sec/batch)
2017-05-08 19:30:34.537510: step 31030, loss = 0.67 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:30:35.830850: step 31040, loss = 0.72 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:30:37.146475: step 31050, loss = 0.74 (972.8 examples/sec; 0.132 sec/batch)
2017-05-08 19:30:38.402256: step 31060, loss = 0.61 (1019.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:30:39.687310: step 31070, loss = 0.69 (996.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:30:40.943946: step 31080, loss = 0.84 (1018.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:30:42.271596: step 31090, loss = 0.73 (964.1 examples/sec; 0.133 sec/batch)
2017-05-08 19:30:43.634286: step 31100, loss = 0.65 (939.3 examples/sec; 0.136 sec/batch)
2017-05-08 19:30:44.846655: step 31110, loss = 0.64 (1055.8 examples/sec; 0.121 sec/batch)
2017-05-08 19:30:46.131516: step 31120, loss = 0.92 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:30:47.405567: step 31130, loss = 0.83 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:30:48.673061: step 31140, loss = 0.70 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:30:49.951473: step 31150, loss = 0.77 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:30:51.217790: step 31160, loss = 0.79 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:30:52.470836: step 31170, loss = 0.90 (1021.5 examples/sec; 0.125 sec/batch)
2017-05-08 19:30:53.744324: step 31180, loss = 0.79 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:30:55.022738: step 31190, loss = 0.90 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:30:56.385482: step 31200, loss = 0.67 (939.3 examples/sec; 0.136 sec/batch)
2017-05-08 19:30:57.670170: step 31210, loss = 0.70 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:30:58.859363: step 31220, loss = 0.89 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-08 19:31:00.150258: step 31230, loss = 0.80 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:31:01.427647: step 31240, loss = 0.77 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:31:02.691601: step 31250, loss = 0.82 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:31:03.969218: step 31260, loss = 0.64 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:31:05.236343: step 31270, loss = 0.78 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:31:06.495721: step 31280, loss = 0.66 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:31:07.768583: step 31290, loss = 0.83 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:31:09.121567: step 31300, loss = 0.69 (946.1 examples/sec; 0.135 sec/batch)
2017-05-08 19:31:10.301104: step 31310, loss = 0.79 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-08 19:31:11.551059: step 31320, loss = 0.80 (1024.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:31:12.844735: step 31330, loss = 0.78 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:31:14.090155: step 31340, loss = 0.75 (1027.8 examples/sec; 0.125 sec/batch)
2017-05-08 19:31:15.351874: step 31350, loss = 0.82 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:31:16.612248: step 31360, loss = 0.76 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:31:17.876918: step 31370, loss = 0.67 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:31:19.143032: step 31380, loss = 0.76 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:31:20.412273: step 31390, loss = 0.80 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:31:21.798384: step 31400, loss = 0.73 (923.4 examples/sec; 0.139 sec/batch)
2017-05-08 19:31:22.977540: step 31410, loss = 0.86 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-08 19:31:24.255662: step 31420, loss = 0.68 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:31:25.528634: step 31430, loss = 0.74 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:31:26.824373: step 31440, loss = 0.76 (987.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:31:28.086964: step 31450, loss = 0.82 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:31:29.354304: step 31460, loss = 0.85 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:31:30.626425: step 31470, loss = 0.80 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:31:31.903478: step 31480, loss = 0.80 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:31:33.203933: step 31490, loss = 0.75 (984.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:31:34.575782: step 31500, loss = 0.95 (933.1 examples/sec; 0.137 sec/batch)
2017-05-08 19:31:35.750737: step 31510, loss = 0.97 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-08 19:31:37.038645: step 31520, loss = 0.92 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:31:38.302796: step 31530, loss = 0.80 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:31:39.556133: step 31540, loss = 0.89 (1021.3 examples/sec; 0.125 sec/batch)
2017-05-08 19:31:40.815326: step 31550, loss = 0.69 (1016.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:31:42.074529: step 31560, loss = 0.78 (1016.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:31:43.336598: step 31570, loss = 0.54 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:31:44.595671: step 31580, loss = 0.72 (1016.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:31:45.850129: step 31590, loss = 0.82 (1020.4 examples/sec; 0.125 sec/batch)
2017-05-08 19:31:47.224744: step 31600, loss = 0.77 (931.2 examples/sec; 0.137 sec/batch)
2017-05-08 19:31:48.434468: step 31610, loss = 0.82 (1058.1 examples/sec; 0.121 sec/batch)
2017-05-08 19:31:49.744435: step 31620, loss = 0.86 (977.1 examples/sec; 0.131 sec/batch)
2017-05-08 19:31:51.033956: step 31630, loss = 0.73 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:31:52.296272: step 31640, loss = 0.65 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:31:53.560604: step 31650, loss = 0.70 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:31:54.838130: step 31660, loss = 0.72 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:31:56.107361: step 31670, loss = 0.90 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:31:57.347917: step 31680, loss = 0.68 (1031.8 examples/sec; 0.124 sec/batch)
2017-05-08 19:31:58.637929: step 31690, loss = 0.70 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:31:59.975297: step 31700, loss = 0.92 (957.1 examples/sec; 0.134 sec/batch)
2017-05-08 19:32:01.135814: step 31710, loss = 0.83 (1103.0 examples/sec; 0.116 sec/batch)
2017-05-08 19:32:02.391820: step 31720, loss = 0.64 (1019.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:32:03.662262: step 31730, loss = 0.77 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:32:04.921240: step 31740, loss = 0.88 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:32:06.199219: step 31750, loss = 0.E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 649 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
96 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:32:07.462974: step 31760, loss = 0.81 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:32:08.739273: step 31770, loss = 0.85 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:32:10.028329: step 31780, loss = 0.91 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:32:11.275908: step 31790, loss = 0.66 (1026.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:32:12.653266: step 31800, loss = 0.81 (929.3 examples/sec; 0.138 sec/batch)
2017-05-08 19:32:13.839016: step 31810, loss = 0.92 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-08 19:32:15.105517: step 31820, loss = 0.76 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:32:16.391052: step 31830, loss = 0.91 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:32:17.663680: step 31840, loss = 0.77 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:32:18.945436: step 31850, loss = 0.96 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:32:20.213954: step 31860, loss = 0.73 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:32:21.516904: step 31870, loss = 0.70 (982.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:32:22.774274: step 31880, loss = 0.88 (1018.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:32:24.061168: step 31890, loss = 0.72 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:32:25.423774: step 31900, loss = 0.56 (939.4 examples/sec; 0.136 sec/batch)
2017-05-08 19:32:26.623016: step 31910, loss = 0.76 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-08 19:32:27.876198: step 31920, loss = 0.68 (1021.4 examples/sec; 0.125 sec/batch)
2017-05-08 19:32:29.172355: step 31930, loss = 0.86 (987.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:32:30.452279: step 31940, loss = 0.76 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:32:31.701727: step 31950, loss = 0.88 (1024.5 examples/sec; 0.125 sec/batch)
2017-05-08 19:32:32.942480: step 31960, loss = 0.68 (1031.7 examples/sec; 0.124 sec/batch)
2017-05-08 19:32:34.238725: step 31970, loss = 0.69 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:32:35.474956: step 31980, loss = 0.87 (1035.4 examples/sec; 0.124 sec/batch)
2017-05-08 19:32:36.775589: step 31990, loss = 0.86 (984.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:32:38.117768: step 32000, loss = 0.81 (953.7 examples/sec; 0.134 sec/batch)
2017-05-08 19:32:39.303429: step 32010, loss = 0.71 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-08 19:32:40.602587: step 32020, loss = 0.81 (985.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:32:41.901750: step 32030, loss = 0.88 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:32:43.212571: step 32040, loss = 0.67 (976.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:32:44.495153: step 32050, loss = 0.73 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:32:45.768725: step 32060, loss = 0.88 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:32:47.026623: step 32070, loss = 0.71 (1017.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:32:48.280007: step 32080, loss = 0.91 (1021.2 examples/sec; 0.125 sec/batch)
2017-05-08 19:32:49.557671: step 32090, loss = 0.72 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:32:50.899227: step 32100, loss = 0.66 (954.1 examples/sec; 0.134 sec/batch)
2017-05-08 19:32:52.075301: step 32110, loss = 0.75 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-08 19:32:53.363267: step 32120, loss = 0.77 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:32:54.659290: step 32130, loss = 0.86 (987.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:32:55.972818: step 32140, loss = 0.74 (974.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:32:57.184895: step 32150, loss = 0.71 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-08 19:32:58.457408: step 32160, loss = 0.76 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:32:59.716304: step 32170, loss = 0.80 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:33:00.997698: step 32180, loss = 0.79 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:33:02.333628: step 32190, loss = 0.71 (958.1 examples/sec; 0.134 sec/batch)
2017-05-08 19:33:03.674887: step 32200, loss = 0.78 (954.3 examples/sec; 0.134 sec/batch)
2017-05-08 19:33:04.831400: step 32210, loss = 0.73 (1106.8 examples/sec; 0.116 sec/batch)
2017-05-08 19:33:06.123923: step 32220, loss = 0.74 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:33:07.384223: step 32230, loss = 0.82 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:33:08.673311: step 32240, loss = 0.84 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:33:09.935349: step 32250, loss = 0.86 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:33:11.194510: step 32260, loss = 0.74 (1016.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:33:12.502440: step 32270, loss = 0.83 (978.6 examples/sec; 0.131 sec/batch)
2017-05-08 19:33:13.748497: step 32280, loss = 0.75 (1027.2 examples/sec; 0.125 sec/batch)
2017-05-08 19:33:15.033935: step 32290, loss = 0.65 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:33:16.371441: step 32300, loss = 0.66 (957.0 examples/sec; 0.134 sec/batch)
2017-05-08 19:33:17.648820: step 32310, loss = 0.86 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:33:18.860866: step 32320, loss = 0.72 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-08 19:33:20.115154: step 32330, loss = 0.89 (1020.5 examples/sec; 0.125 sec/batch)
2017-05-08 19:33:21.424613: step 32340, loss = 0.82 (977.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:33:22.711585: step 32350, loss = 0.73 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:33:23.970395: step 32360, loss = 0.81 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:33:25.239215: step 32370, loss = 0.70 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:33:26.516260: step 32380, loss = 0.77 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:33:27.770531: step 32390, loss = 0.75 (1020.5 examples/sec; 0.125 sec/batch)
2017-05-08 19:33:29.135856: step 32400, loss = 0.70 (937.5 examples/sec; 0.137 sec/batch)
2017-05-08 19:33:30.335319: step 32410, loss = 0.78 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-08 19:33:31.615174: step 32420, loss = 0.65 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:33:32.884729: step 32430, loss = 0.81 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:33:34.177237: step 32440, loss = 0.77 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:33:35.451038: step 32450, loss = 0.72 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:33:36.750325: step 32460, loss = 0.82 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:33:38.053375: step 32470, loss = 0.90 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:33:39.359932: step 32480, loss = 0.64 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 19:33:40.612323: step 32490, loss = 0.61 (1022.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:33:42.014366: step 32500, loss = 0.98 (913.0 examples/sec; 0.140 sec/batch)
2017-05-08 19:33:43.182578: step 32510, loss = 0.78 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-08 19:33:44.438590: step 32520, loss = 0.80 (1019.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:33:45.698698: step 32530, loss = 0.67 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:33:46.989377: step 32540, loss = 0.75 (991.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:33:48.252219: step 32550, loss = 0.65 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:33:49.514632: step 32560, loss = 0.63 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:33:50.806591: step 32570, loss = 0.73 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:33:52.104507: step 32580, loss = 0.74 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:33:53.425215: step 32590, loss = 0.66 (969.2 examples/sec; 0.132 sec/batch)
2017-05-08 19:33:54.802297: step 32600, loss = 0.86 (929.5 examples/sec; 0.138 sec/batch)
2017-05-08 19:33:55.984563: step 32610, loss = 0.68 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-08 19:33:57.306848: step 32620, loss = 0.88 (968.0 examples/sec; 0.132 sec/batch)
2017-05-08 19:33:58.608291: step 32630, loss = 0.93 (983.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:33:59.887828: step 32640, loss = 0.82 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:34:01.189788: step 32650, loss = 0.73 (983.1 examples/sec; 0.130 sec/batch)
201E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 669 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
7-05-08 19:34:02.467340: step 32660, loss = 0.76 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:34:03.699541: step 32670, loss = 0.63 (1038.8 examples/sec; 0.123 sec/batch)
2017-05-08 19:34:04.972812: step 32680, loss = 0.87 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:34:06.266726: step 32690, loss = 0.81 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:34:07.647595: step 32700, loss = 0.70 (927.0 examples/sec; 0.138 sec/batch)
2017-05-08 19:34:08.833178: step 32710, loss = 0.85 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-08 19:34:10.094433: step 32720, loss = 0.83 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:34:11.364515: step 32730, loss = 0.88 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:34:12.678671: step 32740, loss = 0.80 (974.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:34:13.933271: step 32750, loss = 0.74 (1020.2 examples/sec; 0.125 sec/batch)
2017-05-08 19:34:15.187265: step 32760, loss = 0.86 (1020.7 examples/sec; 0.125 sec/batch)
2017-05-08 19:34:16.463977: step 32770, loss = 0.72 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:34:17.746029: step 32780, loss = 0.81 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:34:19.026785: step 32790, loss = 0.76 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:34:20.409818: step 32800, loss = 0.90 (925.5 examples/sec; 0.138 sec/batch)
2017-05-08 19:34:21.602912: step 32810, loss = 0.84 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-08 19:34:22.876417: step 32820, loss = 0.73 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:34:24.146334: step 32830, loss = 0.78 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:34:25.410973: step 32840, loss = 0.73 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:34:26.719343: step 32850, loss = 0.69 (978.3 examples/sec; 0.131 sec/batch)
2017-05-08 19:34:28.004812: step 32860, loss = 0.70 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:34:29.270332: step 32870, loss = 0.81 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:34:30.552140: step 32880, loss = 0.88 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:34:31.801390: step 32890, loss = 0.96 (1024.6 examples/sec; 0.125 sec/batch)
2017-05-08 19:34:33.176014: step 32900, loss = 0.71 (931.2 examples/sec; 0.137 sec/batch)
2017-05-08 19:34:34.393885: step 32910, loss = 0.84 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-08 19:34:35.664964: step 32920, loss = 0.83 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:34:36.950838: step 32930, loss = 0.75 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:34:38.207778: step 32940, loss = 0.76 (1018.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:34:39.485129: step 32950, loss = 0.73 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:34:40.744343: step 32960, loss = 0.67 (1016.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:34:42.005666: step 32970, loss = 0.77 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:34:43.275290: step 32980, loss = 0.76 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:34:44.537637: step 32990, loss = 0.78 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:34:45.915790: step 33000, loss = 0.58 (928.8 examples/sec; 0.138 sec/batch)
2017-05-08 19:34:47.099237: step 33010, loss = 0.72 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-08 19:34:48.363806: step 33020, loss = 0.77 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:34:49.625505: step 33030, loss = 0.80 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:34:50.870512: step 33040, loss = 0.78 (1028.1 examples/sec; 0.125 sec/batch)
2017-05-08 19:34:52.172131: step 33050, loss = 0.74 (983.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:34:53.480374: step 33060, loss = 0.73 (978.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:34:54.783400: step 33070, loss = 0.76 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:34:56.089713: step 33080, loss = 0.72 (979.8 examples/sec; 0.131 sec/batch)
2017-05-08 19:34:57.372076: step 33090, loss = 0.79 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:34:58.770093: step 33100, loss = 0.71 (915.6 examples/sec; 0.140 sec/batch)
2017-05-08 19:34:59.952567: step 33110, loss = 0.83 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-08 19:35:01.218764: step 33120, loss = 0.92 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:35:02.522017: step 33130, loss = 0.64 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:35:03.803931: step 33140, loss = 0.75 (998.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:35:05.104585: step 33150, loss = 0.74 (984.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:35:06.371268: step 33160, loss = 0.79 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:35:07.669400: step 33170, loss = 0.76 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:35:08.954831: step 33180, loss = 0.68 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:35:10.263749: step 33190, loss = 0.65 (977.9 examples/sec; 0.131 sec/batch)
2017-05-08 19:35:11.605779: step 33200, loss = 0.67 (953.8 examples/sec; 0.134 sec/batch)
2017-05-08 19:35:12.780122: step 33210, loss = 0.64 (1090.0 examples/sec; 0.117 sec/batch)
2017-05-08 19:35:14.052768: step 33220, loss = 0.94 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:35:15.325201: step 33230, loss = 0.85 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:35:16.587297: step 33240, loss = 0.91 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:35:17.880143: step 33250, loss = 0.67 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:35:19.153982: step 33260, loss = 0.88 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:35:20.438928: step 33270, loss = 0.77 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:35:21.704861: step 33280, loss = 0.67 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:35:22.961067: step 33290, loss = 0.74 (1018.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:35:24.335056: step 33300, loss = 0.68 (931.6 examples/sec; 0.137 sec/batch)
2017-05-08 19:35:25.497494: step 33310, loss = 0.77 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-08 19:35:26.768041: step 33320, loss = 0.78 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:35:28.072662: step 33330, loss = 0.86 (981.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:35:29.347952: step 33340, loss = 0.87 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:35:30.630284: step 33350, loss = 0.81 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:35:31.889170: step 33360, loss = 0.81 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:35:33.150927: step 33370, loss = 0.73 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:35:34.422622: step 33380, loss = 0.71 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:35:35.672677: step 33390, loss = 0.71 (1024.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:35:37.053831: step 33400, loss = 0.78 (926.8 examples/sec; 0.138 sec/batch)
2017-05-08 19:35:38.237404: step 33410, loss = 0.64 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-08 19:35:39.554361: step 33420, loss = 0.81 (971.9 examples/sec; 0.132 sec/batch)
2017-05-08 19:35:40.821694: step 33430, loss = 0.99 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:35:42.087517: step 33440, loss = 0.77 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:35:43.366547: step 33450, loss = 0.67 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:35:44.628484: step 33460, loss = 0.73 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:35:45.923236: step 33470, loss = 0.77 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:35:47.186508: step 33480, loss = 0.71 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:35:48.422000: step 33490, loss = 0.72 (1036.0 examples/sec; 0.124 sec/batch)
2017-05-08 19:35:49.771157: step 33500, loss = 0.75 (948.7 examples/sec; 0.135 sec/batch)
2017-05-08 19:35:50.950155: step 33510, loss = 0.92 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-08 19:35:52.232403: step 33520, loss = 0.71 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:35:53.512213: step 33530, loss = 0.70 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:35:54.795503: step 33540, loss = 0.81 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:35:56.055939: step 33550, loss = 0.70 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:35:57.318402: step 33560, lE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 690 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
oss = 0.69 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:35:58.591888: step 33570, loss = 0.78 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:35:59.876754: step 33580, loss = 0.75 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:36:01.156060: step 33590, loss = 0.76 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:36:02.529043: step 33600, loss = 0.81 (932.3 examples/sec; 0.137 sec/batch)
2017-05-08 19:36:03.668805: step 33610, loss = 0.75 (1123.0 examples/sec; 0.114 sec/batch)
2017-05-08 19:36:04.941579: step 33620, loss = 0.67 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:36:06.221369: step 33630, loss = 0.77 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:36:07.491404: step 33640, loss = 0.71 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:36:08.788272: step 33650, loss = 0.74 (987.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:36:10.072144: step 33660, loss = 0.79 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:36:11.366916: step 33670, loss = 0.86 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:36:12.647609: step 33680, loss = 0.71 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:36:13.937027: step 33690, loss = 0.77 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:36:15.307488: step 33700, loss = 0.69 (934.0 examples/sec; 0.137 sec/batch)
2017-05-08 19:36:16.491608: step 33710, loss = 0.91 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-08 19:36:17.773176: step 33720, loss = 0.85 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:36:19.054770: step 33730, loss = 0.76 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:36:20.289225: step 33740, loss = 0.63 (1036.9 examples/sec; 0.123 sec/batch)
2017-05-08 19:36:21.542092: step 33750, loss = 0.74 (1021.7 examples/sec; 0.125 sec/batch)
2017-05-08 19:36:22.811611: step 33760, loss = 0.84 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:36:24.059909: step 33770, loss = 0.73 (1025.4 examples/sec; 0.125 sec/batch)
2017-05-08 19:36:25.343180: step 33780, loss = 0.87 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:36:26.626345: step 33790, loss = 0.87 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:36:27.989746: step 33800, loss = 1.00 (938.8 examples/sec; 0.136 sec/batch)
2017-05-08 19:36:29.169920: step 33810, loss = 0.70 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-08 19:36:30.471220: step 33820, loss = 0.78 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:36:31.759002: step 33830, loss = 0.76 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:36:33.018039: step 33840, loss = 0.68 (1016.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:36:34.309749: step 33850, loss = 0.87 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:36:35.566598: step 33860, loss = 0.79 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:36:36.832872: step 33870, loss = 0.71 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:36:38.114996: step 33880, loss = 0.73 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:36:39.424004: step 33890, loss = 0.80 (977.8 examples/sec; 0.131 sec/batch)
2017-05-08 19:36:40.786985: step 33900, loss = 0.73 (939.1 examples/sec; 0.136 sec/batch)
2017-05-08 19:36:41.982164: step 33910, loss = 0.80 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-08 19:36:43.244954: step 33920, loss = 0.73 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:36:44.548235: step 33930, loss = 0.72 (982.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:36:45.857183: step 33940, loss = 0.77 (977.9 examples/sec; 0.131 sec/batch)
2017-05-08 19:36:47.135884: step 33950, loss = 0.77 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:36:48.443560: step 33960, loss = 0.72 (978.8 examples/sec; 0.131 sec/batch)
2017-05-08 19:36:49.724151: step 33970, loss = 0.78 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:36:50.969307: step 33980, loss = 0.79 (1028.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:36:52.254090: step 33990, loss = 0.75 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:36:53.612465: step 34000, loss = 0.68 (942.3 examples/sec; 0.136 sec/batch)
2017-05-08 19:36:54.814221: step 34010, loss = 0.78 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-08 19:36:56.056790: step 34020, loss = 0.83 (1030.1 examples/sec; 0.124 sec/batch)
2017-05-08 19:36:57.382116: step 34030, loss = 0.64 (965.8 examples/sec; 0.133 sec/batch)
2017-05-08 19:36:58.607259: step 34040, loss = 0.67 (1044.8 examples/sec; 0.123 sec/batch)
2017-05-08 19:36:59.865050: step 34050, loss = 0.66 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:37:01.175231: step 34060, loss = 0.73 (977.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:37:02.435988: step 34070, loss = 0.83 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:37:03.699606: step 34080, loss = 0.85 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:37:04.964180: step 34090, loss = 0.78 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:37:06.315342: step 34100, loss = 0.71 (947.3 examples/sec; 0.135 sec/batch)
2017-05-08 19:37:07.482677: step 34110, loss = 0.77 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-08 19:37:08.784740: step 34120, loss = 0.83 (983.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:37:10.085145: step 34130, loss = 0.61 (984.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:37:11.386858: step 34140, loss = 0.71 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:37:12.688757: step 34150, loss = 0.80 (983.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:37:13.973379: step 34160, loss = 0.65 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:37:15.236795: step 34170, loss = 0.68 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:37:16.519372: step 34180, loss = 0.74 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:37:17.759951: step 34190, loss = 0.80 (1031.8 examples/sec; 0.124 sec/batch)
2017-05-08 19:37:19.128964: step 34200, loss = 0.96 (935.0 examples/sec; 0.137 sec/batch)
2017-05-08 19:37:20.302946: step 34210, loss = 0.80 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-08 19:37:21.561419: step 34220, loss = 0.93 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:37:22.840531: step 34230, loss = 0.66 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:37:24.104270: step 34240, loss = 0.94 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:37:25.367430: step 34250, loss = 0.77 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:37:26.660132: step 34260, loss = 0.65 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:37:27.930381: step 34270, loss = 0.86 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:37:29.207654: step 34280, loss = 0.69 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:37:30.488776: step 34290, loss = 0.69 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:37:31.845750: step 34300, loss = 0.66 (943.3 examples/sec; 0.136 sec/batch)
2017-05-08 19:37:33.050575: step 34310, loss = 0.82 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-08 19:37:34.320020: step 34320, loss = 0.64 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:37:35.585576: step 34330, loss = 0.88 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:37:36.858432: step 34340, loss = 0.64 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:37:38.145583: step 34350, loss = 0.65 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:37:39.414867: step 34360, loss = 0.77 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:37:40.698194: step 34370, loss = 0.76 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:37:41.978870: step 34380, loss = 0.80 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:37:43.235824: step 34390, loss = 0.86 (1018.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:37:44.631002: step 34400, loss = 0.83 (917.5 examples/sec; 0.140 sec/batch)
2017-05-08 19:37:45.787047: step 34410, loss = 0.69 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-08 19:37:47.038960: step 34420, loss = 0.81 (1022.4 examples/sec; 0.125 sec/batch)
2017-05-08 19:37:48.306669: step 34430, loss = 0.92 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:37:49.553293: step 34440, loss = 0.85 (1026.8 examples/sec; 0.125 sec/batch)
2017-05-08 19:37:50.819499: step 34450, loss = 0.83 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:37:52.132296: step 34460, loss = 0.63 (975.0 examples/sec; 0.131 sec/baE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 710 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
tch)
2017-05-08 19:37:53.413624: step 34470, loss = 0.82 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:37:54.731781: step 34480, loss = 0.81 (971.1 examples/sec; 0.132 sec/batch)
2017-05-08 19:37:56.040323: step 34490, loss = 0.80 (978.2 examples/sec; 0.131 sec/batch)
2017-05-08 19:37:57.411503: step 34500, loss = 0.67 (933.5 examples/sec; 0.137 sec/batch)
2017-05-08 19:37:58.606890: step 34510, loss = 0.99 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-08 19:37:59.905429: step 34520, loss = 0.71 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:38:01.133375: step 34530, loss = 0.70 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-08 19:38:02.392265: step 34540, loss = 0.73 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:38:03.644635: step 34550, loss = 0.82 (1022.1 examples/sec; 0.125 sec/batch)
2017-05-08 19:38:04.936756: step 34560, loss = 0.82 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:38:06.242344: step 34570, loss = 0.72 (980.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:38:07.498926: step 34580, loss = 0.90 (1018.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:38:08.771427: step 34590, loss = 0.84 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:38:10.150037: step 34600, loss = 0.82 (928.5 examples/sec; 0.138 sec/batch)
2017-05-08 19:38:11.360497: step 34610, loss = 0.90 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-08 19:38:12.621646: step 34620, loss = 0.80 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:38:13.894912: step 34630, loss = 0.66 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:38:15.139978: step 34640, loss = 0.65 (1028.1 examples/sec; 0.125 sec/batch)
2017-05-08 19:38:16.408969: step 34650, loss = 0.74 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:38:17.692102: step 34660, loss = 0.75 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:38:18.951415: step 34670, loss = 0.88 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:38:20.240057: step 34680, loss = 0.73 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:38:21.498771: step 34690, loss = 0.87 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:38:22.869102: step 34700, loss = 0.72 (934.1 examples/sec; 0.137 sec/batch)
2017-05-08 19:38:24.084776: step 34710, loss = 0.73 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-08 19:38:25.359250: step 34720, loss = 0.84 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:38:26.642423: step 34730, loss = 0.64 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:38:27.923124: step 34740, loss = 0.68 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:38:29.203013: step 34750, loss = 0.68 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:38:30.469686: step 34760, loss = 0.89 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:38:31.725118: step 34770, loss = 0.75 (1019.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:38:33.000914: step 34780, loss = 0.84 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:38:34.318641: step 34790, loss = 0.89 (971.4 examples/sec; 0.132 sec/batch)
2017-05-08 19:38:35.691650: step 34800, loss = 0.71 (932.3 examples/sec; 0.137 sec/batch)
2017-05-08 19:38:36.866569: step 34810, loss = 0.77 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-08 19:38:38.124216: step 34820, loss = 0.73 (1017.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:38:39.402790: step 34830, loss = 0.78 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:38:40.679778: step 34840, loss = 0.68 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:38:41.948873: step 34850, loss = 0.77 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:38:43.227350: step 34860, loss = 0.82 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:38:44.531534: step 34870, loss = 0.66 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:38:45.821799: step 34880, loss = 0.70 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:38:47.101962: step 34890, loss = 0.79 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:38:48.449416: step 34900, loss = 0.67 (949.9 examples/sec; 0.135 sec/batch)
2017-05-08 19:38:49.631015: step 34910, loss = 0.67 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-08 19:38:50.890708: step 34920, loss = 0.72 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:38:52.155588: step 34930, loss = 0.80 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:38:53.433730: step 34940, loss = 0.75 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:38:54.703868: step 34950, loss = 0.80 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:38:55.979229: step 34960, loss = 0.78 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:38:57.247065: step 34970, loss = 0.88 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:38:58.528031: step 34980, loss = 0.97 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:38:59.797543: step 34990, loss = 0.72 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:39:01.157714: step 35000, loss = 0.75 (941.1 examples/sec; 0.136 sec/batch)
2017-05-08 19:39:02.370843: step 35010, loss = 0.77 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-08 19:39:03.630882: step 35020, loss = 0.70 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:39:04.896971: step 35030, loss = 0.91 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:39:06.164778: step 35040, loss = 0.57 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:39:07.420790: step 35050, loss = 0.69 (1019.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:39:08.683786: step 35060, loss = 0.70 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:39:09.985131: step 35070, loss = 0.87 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:39:11.236818: step 35080, loss = 0.65 (1022.6 examples/sec; 0.125 sec/batch)
2017-05-08 19:39:12.511029: step 35090, loss = 0.80 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:39:13.862223: step 35100, loss = 0.78 (947.3 examples/sec; 0.135 sec/batch)
2017-05-08 19:39:15.064817: step 35110, loss = 0.69 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-08 19:39:16.328815: step 35120, loss = 0.88 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:39:17.586339: step 35130, loss = 0.90 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:39:18.863681: step 35140, loss = 0.68 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:39:20.155992: step 35150, loss = 0.84 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:39:21.411899: step 35160, loss = 0.66 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:39:22.694917: step 35170, loss = 0.82 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:39:23.962374: step 35180, loss = 0.76 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:39:25.226122: step 35190, loss = 0.72 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:39:26.612759: step 35200, loss = 0.97 (923.1 examples/sec; 0.139 sec/batch)
2017-05-08 19:39:27.771654: step 35210, loss = 0.83 (1104.5 examples/sec; 0.116 sec/batch)
2017-05-08 19:39:29.089625: step 35220, loss = 0.91 (971.2 examples/sec; 0.132 sec/batch)
2017-05-08 19:39:30.394532: step 35230, loss = 0.81 (980.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:39:31.657066: step 35240, loss = 0.76 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:39:32.933142: step 35250, loss = 0.66 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:39:34.210125: step 35260, loss = 0.70 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:39:35.498505: step 35270, loss = 0.90 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:39:36.770909: step 35280, loss = 0.84 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:39:38.041135: step 35290, loss = 0.87 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:39:39.399115: step 35300, loss = 0.70 (942.6 examples/sec; 0.136 sec/batch)
2017-05-08 19:39:40.590440: step 35310, loss = 0.75 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-08 19:39:41.873782: step 35320, loss = 0.87 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:39:43.143229: step 35330, loss = 0.80 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:39:44.423116: step 35340, loss = 0.73 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:39:45.677582: step 35350, loss = 0.89 (1020.4 examples/sec; 0.125 sec/batch)
2017-05-08 19:39:46.932211: step 35360, loss = 0.68 (1020.2 examples/sec; 0.125 sec/batch)
2017-05-08 19:39:48.231984: step 35370, loss = 0.80 (984.8 examples/sec; 0.130 sec/batch)
2017-05-08 19:39:49.491273: step 35380, loss = 0.96 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:39:50.791612: step 35390, loss = 0.71 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:39:52.205525: step 35400, loss = 0.67 (905.3 examples/sec; 0.141 sec/batch)
2017-05-08 19:39:53.379074: step 35410, loss = 0.81 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-08 19:39:54.643247: step 35420, loss = 0.59 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:39:55.921765: step 35430, loss = 0.84 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:39:57.188424: step 35440, loss = 0.55 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:39:58.444486: step 35450, loss = 0.83 (1019.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:39:59.745255: step 35460, loss = 0.86 (984.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:40:00.998192: step 35470, loss = 0.83 (1021.6 examples/sec; 0.125 sec/batch)
2017-05-08 19:40:02.274603: step 35480, loss = 0.74 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:40:03.543246: step 35490, loss = 0.99 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:40:04.901783: step 35500, loss = 0.78 (942.2 examples/sec; 0.136 sec/batch)
2017-05-08 19:40:06.089793: step 35510, loss = 0.75 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-08 19:40:07.393134: step 35520, loss = 0.71 (982.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:40:08.658100: step 35530, loss = 0.68 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:40:09.914593: step 35540, loss = 0.91 (1018.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:40:11.169671: step 35550, loss = 0.82 (1019.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:40:12.420268: step 35560, loss = 0.85 (1023.5 examples/sec; 0.125 sec/batch)
2017-05-08 19:40:13.701523: step 35570, loss = 0.67 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:40:14.969970: step 35580, loss = 0.79 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:40:16.247921: step 35590, loss = 0.84 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:40:17.571765: step 35600, loss = 0.77 (966.9 examples/sec; 0.132 sec/batch)
2017-05-08 19:40:18.790777: step 35610, loss = 0.73 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-08 19:40:20.097382: step 35620, loss = 0.78 (979.6 examples/sec; 0.131 sec/batch)
2017-05-08 19:40:21.391284: step 35630, loss = 0.70 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:40:22.678485: step 35640, loss = 0.72 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:40:23.963857: step 35650, loss = 0.68 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:40:25.259902: step 35660, loss = 0.62 (987.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:40:26.529337: step 35670, loss = 0.70 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:40:27.800792: step 35680, loss = 0.90 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:40:29.124539: step 35690, loss = 0.83 (966.9 examples/sec; 0.132 sec/batch)
2017-05-08 19:40:30.512297: step 35700, loss = 0.76 (922.4 examples/sec; 0.139 sec/batch)
2017-05-08 19:40:31.680607: step 35710, loss = 0.90 (1095.6 examples/sec; 0.117 sec/batch)
2017-05-08 19:40:32.947349: step 35720, loss = 0.62 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:40:34.218752: step 35730, loss = 0.79 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:40:35.476520: step 35740, loss = 0.76 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:40:36.754969: step 35750, loss = 0.71 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:40:38.024642: step 35760, loss = 0.72 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:40:39.304348: step 35770, loss = 0.71 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:40:40.556575: step 35780, loss = 0.78 (1022.2 examples/sec; 0.125 sec/batch)
2017-05-08 19:40:41.870048: step 35790, loss = 0.75 (974.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:40:43.229951: step 35800, loss = 0.76 (941.2 examples/sec; 0.136 sec/batch)
2017-05-08 19:40:44.417311: step 35810, loss = 0.84 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-08 19:40:45.703810: step 35820, loss =E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 730 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
 0.81 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:40:46.960563: step 35830, loss = 0.84 (1018.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:40:48.210922: step 35840, loss = 0.88 (1023.7 examples/sec; 0.125 sec/batch)
2017-05-08 19:40:49.475509: step 35850, loss = 0.78 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:40:50.741158: step 35860, loss = 0.67 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:40:52.028298: step 35870, loss = 0.69 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:40:53.308869: step 35880, loss = 0.70 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:40:54.583771: step 35890, loss = 0.83 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:40:55.956920: step 35900, loss = 0.72 (932.2 examples/sec; 0.137 sec/batch)
2017-05-08 19:40:57.120144: step 35910, loss = 0.75 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-08 19:40:58.377676: step 35920, loss = 0.84 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:40:59.637518: step 35930, loss = 0.75 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:41:00.915206: step 35940, loss = 0.70 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:41:02.171888: step 35950, loss = 0.77 (1018.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:41:03.445144: step 35960, loss = 0.80 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:41:04.723987: step 35970, loss = 0.67 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:41:06.016735: step 35980, loss = 0.63 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:41:07.301249: step 35990, loss = 0.75 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:41:08.692628: step 36000, loss = 0.85 (920.0 examples/sec; 0.139 sec/batch)
2017-05-08 19:41:09.855023: step 36010, loss = 0.77 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-08 19:41:11.116173: step 36020, loss = 0.77 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:41:12.388861: step 36030, loss = 0.85 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:41:13.661374: step 36040, loss = 0.80 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:41:14.900242: step 36050, loss = 0.83 (1033.2 examples/sec; 0.124 sec/batch)
2017-05-08 19:41:16.146732: step 36060, loss = 0.61 (1026.9 examples/sec; 0.125 sec/batch)
2017-05-08 19:41:17.431222: step 36070, loss = 0.69 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:41:18.730872: step 36080, loss = 1.01 (984.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:41:20.006173: step 36090, loss = 0.69 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:41:21.382993: step 36100, loss = 0.79 (929.7 examples/sec; 0.138 sec/batch)
2017-05-08 19:41:22.572030: step 36110, loss = 0.76 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-08 19:41:23.833555: step 36120, loss = 0.72 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:41:25.143575: step 36130, loss = 0.86 (977.1 examples/sec; 0.131 sec/batch)
2017-05-08 19:41:26.436076: step 36140, loss = 0.90 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:41:27.717568: step 36150, loss = 0.76 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:41:29.004354: step 36160, loss = 0.70 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:41:30.325305: step 36170, loss = 0.84 (969.0 examples/sec; 0.132 sec/batch)
2017-05-08 19:41:31.538023: step 36180, loss = 0.59 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-08 19:41:32.779683: step 36190, loss = 0.81 (1030.9 examples/sec; 0.124 sec/batch)
2017-05-08 19:41:34.141140: step 36200, loss = 0.74 (940.2 examples/sec; 0.136 sec/batch)
2017-05-08 19:41:35.317250: step 36210, loss = 0.63 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-08 19:41:36.630865: step 36220, loss = 0.87 (974.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:41:37.915729: step 36230, loss = 0.66 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:41:39.203826: step 36240, loss = 0.71 (993.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:41:40.465882: step 36250, loss = 0.78 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:41:41.732730: step 36260, loss = 0.72 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:41:43.042926: step 36270, loss = 0.72 (977.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:41:44.254832: step 36280, loss = 0.63 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-08 19:41:45.574999: step 36290, loss = 0.75 (969.6 examples/sec; 0.132 sec/batch)
2017-05-08 19:41:46.980830: step 36300, loss = 0.73 (910.5 examples/sec; 0.141 sec/batch)
2017-05-08 19:41:48.168912: step 36310, loss = 0.94 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-08 19:41:49.446239: step 36320, loss = 0.94 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:41:50.738151: step 36330, loss = 0.67 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:41:52.020681: step 36340, loss = 0.66 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:41:53.274229: step 36350, loss = 0.74 (1021.1 examples/sec; 0.125 sec/batch)
2017-05-08 19:41:54.553031: step 36360, loss = 0.75 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:41:55.828589: step 36370, loss = 0.66 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:41:57.120315: step 36380, loss = 0.68 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:41:58.424544: step 36390, loss = 0.77 (981.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:41:59.815085: step 36400, loss = 0.60 (920.5 examples/sec; 0.139 sec/batch)
2017-05-08 19:42:00.979920: step 36410, loss = 0.69 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-08 19:42:02.278433: step 36420, loss = 0.70 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:42:03.559568: step 36430, loss = 0.75 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:42:04.838613: step 36440, loss = 0.74 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:42:06.147065: step 36450, loss = 0.96 (978.3 examples/sec; 0.131 sec/batch)
2017-05-08 19:42:07.414642: step 36460, loss = 0.84 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:42:08.675637: step 36470, loss = 0.93 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:42:09.931860: step 36480, loss = 0.67 (1018.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:42:11.212425: step 36490, loss = 0.61 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:42:12.586217: step 36500, loss = 0.76 (931.7 examples/sec; 0.137 sec/batch)
2017-05-08 19:42:13.778908: step 36510, loss = 0.75 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-08 19:42:15.042891: step 36520, loss = 0.68 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:42:16.309468: step 36530, loss = 0.65 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:42:17.595587: step 36540, loss = 0.80 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:42:18.870890: step 36550, loss = 0.77 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:42:20.194725: step 36560, loss = 0.86 (966.9 examples/sec; 0.132 sec/batch)
2017-05-08 19:42:21.424429: step 36570, loss = 0.78 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-08 19:42:22.705920: step 36580, loss = 0.77 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:42:23.979817: step 36590, loss = 0.69 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:42:25.336276: step 36600, loss = 0.76 (943.6 examples/sec; 0.136 sec/batch)
2017-05-08 19:42:26.517836: step 36610, loss = 0.68 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-08 19:42:27.800612: step 36620, loss = 0.88 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:42:29.076908: step 36630, loss = 0.90 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:42:30.361803: step 36640, loss = 0.79 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:42:31.622935: step 36650, loss = 0.88 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:42:32.884542: step 36660, loss = 0.67 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:42:34.144572: step 36670, loss = 0.63 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:42:35.420044: step 36680, loss = 0.78 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:42:36.689597: step 36690, loss = 0.71 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:42:38.074189: step 36700, loss = 0.67 (924.5 examples/sec; 0.138 sec/batch)
2017-05-08 19:42:39.261748: step 36710, loss = 0.81 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-08 19:42:40.518090: step 36720, loss = 0.83 (1018.8 examples/sec; 0.126 sec/batE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 750 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
ch)
2017-05-08 19:42:41.773039: step 36730, loss = 0.83 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:42:43.040931: step 36740, loss = 0.84 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:42:44.315401: step 36750, loss = 0.65 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:42:45.572756: step 36760, loss = 0.67 (1018.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:42:46.852505: step 36770, loss = 0.69 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:42:48.145263: step 36780, loss = 0.86 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:42:49.460188: step 36790, loss = 0.88 (973.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:42:50.831619: step 36800, loss = 0.69 (933.3 examples/sec; 0.137 sec/batch)
2017-05-08 19:42:52.027116: step 36810, loss = 0.71 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-08 19:42:53.313030: step 36820, loss = 0.79 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:42:54.585754: step 36830, loss = 0.59 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:42:55.857087: step 36840, loss = 0.72 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:42:57.138055: step 36850, loss = 0.83 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:42:58.420645: step 36860, loss = 0.71 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:42:59.696076: step 36870, loss = 0.83 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:43:00.958338: step 36880, loss = 0.80 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:43:02.269069: step 36890, loss = 0.76 (976.6 examples/sec; 0.131 sec/batch)
2017-05-08 19:43:03.656910: step 36900, loss = 0.86 (922.3 examples/sec; 0.139 sec/batch)
2017-05-08 19:43:04.861266: step 36910, loss = 0.64 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-08 19:43:06.134170: step 36920, loss = 0.77 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:43:07.405973: step 36930, loss = 0.77 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:43:08.677009: step 36940, loss = 0.80 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:43:09.950272: step 36950, loss = 0.88 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:43:11.225020: step 36960, loss = 0.78 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:43:12.488046: step 36970, loss = 1.02 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:43:13.762832: step 36980, loss = 0.97 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:43:15.023047: step 36990, loss = 0.70 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:43:16.391094: step 37000, loss = 0.85 (935.6 examples/sec; 0.137 sec/batch)
2017-05-08 19:43:17.626269: step 37010, loss = 0.75 (1036.3 examples/sec; 0.124 sec/batch)
2017-05-08 19:43:18.860074: step 37020, loss = 0.74 (1037.4 examples/sec; 0.123 sec/batch)
2017-05-08 19:43:20.134620: step 37030, loss = 0.75 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:43:21.408759: step 37040, loss = 0.71 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:43:22.660254: step 37050, loss = 0.65 (1022.8 examples/sec; 0.125 sec/batch)
2017-05-08 19:43:23.927928: step 37060, loss = 1.01 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:43:25.186889: step 37070, loss = 0.84 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:43:26.432529: step 37080, loss = 0.56 (1027.6 examples/sec; 0.125 sec/batch)
2017-05-08 19:43:27.697525: step 37090, loss = 0.75 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:43:29.091521: step 37100, loss = 0.69 (918.2 examples/sec; 0.139 sec/batch)
2017-05-08 19:43:30.272321: step 37110, loss = 0.74 (1084.0 examples/sec; 0.118 sec/batch)
2017-05-08 19:43:31.537916: step 37120, loss = 0.76 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:43:32.790891: step 37130, loss = 0.67 (1021.6 examples/sec; 0.125 sec/batch)
2017-05-08 19:43:34.065878: step 37140, loss = 0.80 (1003.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:43:35.343945: step 37150, loss = 0.68 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:43:36.632510: step 37160, loss = 0.61 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:43:37.923534: step 37170, loss = 0.80 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:43:39.174213: step 37180, loss = 0.74 (1023.4 examples/sec; 0.125 sec/batch)
2017-05-08 19:43:40.464351: step 37190, loss = 0.84 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:43:41.807159: step 37200, loss = 0.69 (953.2 examples/sec; 0.134 sec/batch)
2017-05-08 19:43:42.988429: step 37210, loss = 0.66 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-08 19:43:44.270963: step 37220, loss = 0.75 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:43:45.537721: step 37230, loss = 0.57 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:43:46.816265: step 37240, loss = 0.80 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:43:48.073022: step 37250, loss = 0.84 (1018.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:43:49.342578: step 37260, loss = 0.78 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:43:50.606938: step 37270, loss = 0.69 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:43:51.862961: step 37280, loss = 0.86 (1019.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:43:53.149343: step 37290, loss = 0.77 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:43:54.524603: step 37300, loss = 0.79 (930.7 examples/sec; 0.138 sec/batch)
2017-05-08 19:43:55.728981: step 37310, loss = 0.67 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-08 19:43:56.998307: step 37320, loss = 0.70 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:43:58.253874: step 37330, loss = 0.80 (1019.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:43:59.516271: step 37340, loss = 0.74 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:44:00.757775: step 37350, loss = 0.69 (1031.0 examples/sec; 0.124 sec/batch)
2017-05-08 19:44:02.049837: step 37360, loss = 0.79 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:44:03.321920: step 37370, loss = 0.96 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:44:04.575026: step 37380, loss = 0.67 (1021.5 examples/sec; 0.125 sec/batch)
2017-05-08 19:44:05.886014: step 37390, loss = 0.85 (976.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:44:07.256008: step 37400, loss = 0.79 (934.3 examples/sec; 0.137 sec/batch)
2017-05-08 19:44:08.433299: step 37410, loss = 0.82 (1087.2 examples/sec; 0.118 sec/batch)
2017-05-08 19:44:09.717583: step 37420, loss = 0.77 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:44:11.011277: step 37430, loss = 0.68 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:44:12.262957: step 37440, loss = 0.78 (1022.6 examples/sec; 0.125 sec/batch)
2017-05-08 19:44:13.560183: step 37450, loss = 0.76 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:44:14.838605: step 37460, loss = 0.60 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:44:16.118327: step 37470, loss = 0.76 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:44:17.386432: step 37480, loss = 0.76 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:44:18.666038: step 37490, loss = 0.71 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:44:20.020105: step 37500, loss = 0.84 (945.3 examples/sec; 0.135 sec/batch)
2017-05-08 19:44:21.293243: step 37510, loss = 0.73 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:44:22.505792: step 37520, loss = 0.79 (1055.6 examples/sec; 0.121 sec/batch)
2017-05-08 19:44:23.761055: step 37530, loss = 0.96 (1019.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:44:25.017967: step 37540, loss = 0.88 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:44:26.291696: step 37550, loss = 0.80 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:44:27.585047: step 37560, loss = 0.71 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:44:28.847967: step 37570, loss = 0.77 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:44:30.133504: step 37580, loss = 0.68 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:44:31.428959: step 37590, loss = 0.76 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:44:32.820829: step 37600, loss = 0.73 (919.6 examples/sec; 0.139 sec/batch)
2017-05-08 19:44:34.056152: step 37610, loss = 0.82 (1036.2 examples/sec; 0.124 sec/batch)
2017-05-08 19:44:35.357940: step 37620, loss = 0.76 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:44:36.596251:E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 771 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
 step 37630, loss = 0.85 (1033.7 examples/sec; 0.124 sec/batch)
2017-05-08 19:44:37.879496: step 37640, loss = 0.81 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:44:39.170190: step 37650, loss = 0.86 (991.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:44:40.451490: step 37660, loss = 0.90 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:44:41.737085: step 37670, loss = 0.77 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:44:42.987302: step 37680, loss = 0.88 (1023.8 examples/sec; 0.125 sec/batch)
2017-05-08 19:44:44.292916: step 37690, loss = 0.68 (980.4 examples/sec; 0.131 sec/batch)
2017-05-08 19:44:45.671767: step 37700, loss = 0.81 (928.3 examples/sec; 0.138 sec/batch)
2017-05-08 19:44:46.900528: step 37710, loss = 0.68 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-08 19:44:48.159388: step 37720, loss = 0.82 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:44:49.475568: step 37730, loss = 0.85 (972.5 examples/sec; 0.132 sec/batch)
2017-05-08 19:44:50.751797: step 37740, loss = 0.81 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:44:52.011331: step 37750, loss = 0.76 (1016.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:44:53.294381: step 37760, loss = 0.88 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:44:54.551796: step 37770, loss = 0.73 (1018.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:44:55.819394: step 37780, loss = 0.78 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:44:57.085600: step 37790, loss = 0.77 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:44:58.446190: step 37800, loss = 0.78 (940.8 examples/sec; 0.136 sec/batch)
2017-05-08 19:44:59.628699: step 37810, loss = 0.78 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-08 19:45:00.876427: step 37820, loss = 0.76 (1025.9 examples/sec; 0.125 sec/batch)
2017-05-08 19:45:02.176030: step 37830, loss = 0.69 (984.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:45:03.428426: step 37840, loss = 0.64 (1022.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:45:04.688215: step 37850, loss = 0.70 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:45:05.932793: step 37860, loss = 0.77 (1028.5 examples/sec; 0.124 sec/batch)
2017-05-08 19:45:07.240426: step 37870, loss = 0.83 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 19:45:08.490636: step 37880, loss = 0.65 (1023.8 examples/sec; 0.125 sec/batch)
2017-05-08 19:45:09.777253: step 37890, loss = 0.84 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:45:11.151420: step 37900, loss = 0.60 (931.5 examples/sec; 0.137 sec/batch)
2017-05-08 19:45:12.312553: step 37910, loss = 0.99 (1102.4 examples/sec; 0.116 sec/batch)
2017-05-08 19:45:13.593914: step 37920, loss = 0.68 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:45:14.851862: step 37930, loss = 0.61 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:45:16.138368: step 37940, loss = 0.75 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:45:17.390417: step 37950, loss = 0.82 (1022.3 examples/sec; 0.125 sec/batch)
2017-05-08 19:45:18.671340: step 37960, loss = 0.75 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:45:19.930405: step 37970, loss = 0.77 (1016.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:45:21.192505: step 37980, loss = 0.81 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:45:22.463699: step 37990, loss = 0.58 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:45:23.810817: step 38000, loss = 0.77 (950.2 examples/sec; 0.135 sec/batch)
2017-05-08 19:45:25.012274: step 38010, loss = 0.81 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-08 19:45:26.285319: step 38020, loss = 0.72 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:45:27.552511: step 38030, loss = 0.91 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:45:28.809133: step 38040, loss = 0.73 (1018.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:45:30.097136: step 38050, loss = 0.74 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:45:31.408615: step 38060, loss = 0.74 (976.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:45:32.637872: step 38070, loss = 0.77 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-08 19:45:33.891009: step 38080, loss = 0.65 (1021.4 examples/sec; 0.125 sec/batch)
2017-05-08 19:45:35.136631: step 38090, loss = 0.81 (1027.6 examples/sec; 0.125 sec/batch)
2017-05-08 19:45:36.525690: step 38100, loss = 0.85 (921.5 examples/sec; 0.139 sec/batch)
2017-05-08 19:45:37.747247: step 38110, loss = 0.92 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-08 19:45:38.969437: step 38120, loss = 0.69 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-08 19:45:40.238671: step 38130, loss = 0.72 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:45:41.492669: step 38140, loss = 0.83 (1020.7 examples/sec; 0.125 sec/batch)
2017-05-08 19:45:42.768673: step 38150, loss = 0.93 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:45:44.065268: step 38160, loss = 0.80 (987.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:45:45.289291: step 38170, loss = 0.88 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-08 19:45:46.581060: step 38180, loss = 0.79 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:45:47.845604: step 38190, loss = 0.79 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:45:49.212131: step 38200, loss = 0.77 (936.7 examples/sec; 0.137 sec/batch)
2017-05-08 19:45:50.439245: step 38210, loss = 0.72 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-08 19:45:51.677293: step 38220, loss = 0.78 (1033.9 examples/sec; 0.124 sec/batch)
2017-05-08 19:45:52.947718: step 38230, loss = 0.75 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:45:54.220101: step 38240, loss = 0.81 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:45:55.470103: step 38250, loss = 0.89 (1024.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:45:56.755113: step 38260, loss = 0.75 (996.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:45:58.009960: step 38270, loss = 0.90 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:45:59.312183: step 38280, loss = 0.69 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:46:00.566282: step 38290, loss = 0.73 (1020.7 examples/sec; 0.125 sec/batch)
2017-05-08 19:46:01.945366: step 38300, loss = 0.73 (928.2 examples/sec; 0.138 sec/batch)
2017-05-08 19:46:03.128841: step 38310, loss = 0.87 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-08 19:46:04.390045: step 38320, loss = 0.66 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:46:05.667941: step 38330, loss = 0.58 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:46:06.927738: step 38340, loss = 0.87 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:46:08.188861: step 38350, loss = 0.93 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:46:09.438680: step 38360, loss = 0.65 (1024.2 examples/sec; 0.125 sec/batch)
2017-05-08 19:46:10.696221: step 38370, loss = 0.71 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:46:11.966736: step 38380, loss = 0.78 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:46:13.212499: step 38390, loss = 0.73 (1027.5 examples/sec; 0.125 sec/batch)
2017-05-08 19:46:14.597963: step 38400, loss = 0.79 (923.9 examples/sec; 0.139 sec/batch)
2017-05-08 19:46:15.821056: step 38410, loss = 0.65 (1046.5 examples/sec; 0.122 sec/batch)
2017-05-08 19:46:17.064716: step 38420, loss = 0.80 (1029.2 examples/sec; 0.124 sec/batch)
2017-05-08 19:46:18.341934: step 38430, loss = 0.73 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:46:19.602878: step 38440, loss = 0.67 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:46:20.865002: step 38450, loss = 0.86 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:46:22.131768: step 38460, loss = 0.82 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:46:23.400956: step 38470, loss = 0.93 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:46:24.678889: step 38480, loss = 0.84 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:46:25.977068: step 38490, loss = 0.80 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:46:27.329946: step 38500, loss = 0.81 (946.1 examples/sec; 0.135 sec/batch)
2017-05-08 19:46:28.554568: step 38510, loss = 0.91 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-08 19:46:29.776597: step 38520, loss = 0.70 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-08 19:46:31.035644: step 38530, loss = 0.81 (1016.E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 791 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
6 examples/sec; 0.126 sec/batch)
2017-05-08 19:46:32.278813: step 38540, loss = 0.70 (1029.6 examples/sec; 0.124 sec/batch)
2017-05-08 19:46:33.568772: step 38550, loss = 0.80 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:46:34.846851: step 38560, loss = 0.87 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:46:36.114679: step 38570, loss = 0.76 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:46:37.400285: step 38580, loss = 0.69 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:46:38.667914: step 38590, loss = 0.79 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:46:40.018377: step 38600, loss = 0.91 (947.8 examples/sec; 0.135 sec/batch)
2017-05-08 19:46:41.211708: step 38610, loss = 0.64 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-08 19:46:42.464646: step 38620, loss = 0.81 (1021.6 examples/sec; 0.125 sec/batch)
2017-05-08 19:46:43.699854: step 38630, loss = 0.74 (1036.3 examples/sec; 0.124 sec/batch)
2017-05-08 19:46:44.955778: step 38640, loss = 0.81 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:46:46.218503: step 38650, loss = 0.75 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:46:47.503819: step 38660, loss = 0.67 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:46:48.740421: step 38670, loss = 0.81 (1035.1 examples/sec; 0.124 sec/batch)
2017-05-08 19:46:50.021928: step 38680, loss = 0.85 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:46:51.307340: step 38690, loss = 0.88 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:46:52.656783: step 38700, loss = 0.72 (948.5 examples/sec; 0.135 sec/batch)
2017-05-08 19:46:53.884193: step 38710, loss = 0.72 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-08 19:46:55.124521: step 38720, loss = 0.78 (1032.0 examples/sec; 0.124 sec/batch)
2017-05-08 19:46:56.417377: step 38730, loss = 0.85 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:46:57.732084: step 38740, loss = 0.64 (973.6 examples/sec; 0.131 sec/batch)
2017-05-08 19:46:59.028559: step 38750, loss = 0.92 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:47:00.292482: step 38760, loss = 0.73 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:47:01.548928: step 38770, loss = 0.78 (1018.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:47:02.845298: step 38780, loss = 1.01 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:47:04.130798: step 38790, loss = 0.73 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:47:05.527828: step 38800, loss = 0.79 (916.2 examples/sec; 0.140 sec/batch)
2017-05-08 19:47:06.753079: step 38810, loss = 0.72 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-08 19:47:08.008399: step 38820, loss = 0.92 (1019.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:47:09.259183: step 38830, loss = 0.76 (1023.4 examples/sec; 0.125 sec/batch)
2017-05-08 19:47:10.525725: step 38840, loss = 0.86 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:47:11.846147: step 38850, loss = 0.63 (969.4 examples/sec; 0.132 sec/batch)
2017-05-08 19:47:13.085841: step 38860, loss = 0.78 (1032.5 examples/sec; 0.124 sec/batch)
2017-05-08 19:47:14.373994: step 38870, loss = 0.86 (993.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:47:15.624044: step 38880, loss = 0.96 (1024.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:47:16.881221: step 38890, loss = 0.85 (1018.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:47:18.284185: step 38900, loss = 0.74 (912.3 examples/sec; 0.140 sec/batch)
2017-05-08 19:47:19.485535: step 38910, loss = 0.92 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-08 19:47:20.758698: step 38920, loss = 0.78 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:47:22.045325: step 38930, loss = 0.88 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:47:23.366525: step 38940, loss = 0.75 (968.8 examples/sec; 0.132 sec/batch)
2017-05-08 19:47:24.617880: step 38950, loss = 0.67 (1022.9 examples/sec; 0.125 sec/batch)
2017-05-08 19:47:25.888690: step 38960, loss = 0.72 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:47:27.155474: step 38970, loss = 0.78 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:47:28.434682: step 38980, loss = 0.59 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:47:29.711919: step 38990, loss = 0.76 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:47:31.085305: step 39000, loss = 0.72 (932.0 examples/sec; 0.137 sec/batch)
2017-05-08 19:47:32.282995: step 39010, loss = 0.81 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-08 19:47:33.545647: step 39020, loss = 0.59 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:47:34.801070: step 39030, loss = 0.65 (1019.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:47:36.087522: step 39040, loss = 0.77 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:47:37.388370: step 39050, loss = 0.80 (984.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:47:38.667842: step 39060, loss = 0.69 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:47:39.959277: step 39070, loss = 0.73 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:47:41.246078: step 39080, loss = 0.78 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:47:42.545728: step 39090, loss = 0.65 (984.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:47:43.875950: step 39100, loss = 0.69 (962.2 examples/sec; 0.133 sec/batch)
2017-05-08 19:47:45.044851: step 39110, loss = 0.86 (1095.0 examples/sec; 0.117 sec/batch)
2017-05-08 19:47:46.300686: step 39120, loss = 0.74 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:47:47.568530: step 39130, loss = 0.77 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:47:48.834826: step 39140, loss = 0.80 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:47:50.102604: step 39150, loss = 0.66 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:47:51.340626: step 39160, loss = 0.83 (1033.9 examples/sec; 0.124 sec/batch)
2017-05-08 19:47:52.615665: step 39170, loss = 0.64 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:47:53.915215: step 39180, loss = 0.64 (985.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:47:55.170683: step 39190, loss = 0.79 (1019.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:47:56.509473: step 39200, loss = 0.70 (956.1 examples/sec; 0.134 sec/batch)
2017-05-08 19:47:57.675645: step 39210, loss = 0.69 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-08 19:47:58.960058: step 39220, loss = 0.68 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:48:00.251155: step 39230, loss = 0.67 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:48:01.528921: step 39240, loss = 0.72 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:48:02.780926: step 39250, loss = 0.79 (1022.4 examples/sec; 0.125 sec/batch)
2017-05-08 19:48:04.060662: step 39260, loss = 0.94 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:48:05.320041: step 39270, loss = 0.71 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:48:06.578624: step 39280, loss = 0.76 (1017.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:48:07.872617: step 39290, loss = 0.77 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:48:09.241174: step 39300, loss = 0.76 (935.3 examples/sec; 0.137 sec/batch)
2017-05-08 19:48:10.400222: step 39310, loss = 0.72 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-08 19:48:11.683002: step 39320, loss = 0.68 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:48:12.970897: step 39330, loss = 0.83 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:48:14.234609: step 39340, loss = 0.80 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:48:15.507513: step 39350, loss = 0.75 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:48:16.782075: step 39360, loss = 0.64 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:48:18.038796: step 39370, loss = 0.75 (1018.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:48:19.305184: step 39380, loss = 0.88 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:48:20.590700: step 39390, loss = 0.77 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:48:21.971866: step 39400, loss = 0.77 (926.8 examples/sec; 0.138 sec/batch)
2017-05-08 19:48:23.144373: step 39410, loss = 0.80 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-08 19:48:24.401637: step 39420, loss = 0.67 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:48:25.683693: step 39430, loss = 0.79 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:48:26.949185: step 39440, loss = 0.61 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:48:28.192847: step 39450, loss = 0.78 (1029.2 examples/sec; 0.124 sec/batch)
2017-05-08 19:48:29.479614: step 39460, loss = 0.61 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:48:30.737386: step 39470, loss = 0.70 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:48:32.000328: step 39480, loss = 0.73 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:48:33.258055: step 39490, loss = 0.76 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:48:34.619505: step 39500, loss = 0.87 (940.2 examples/sec; 0.136 sec/batch)
2017-05-08 19:48:35.832295: step 39510, loss = 0.59 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-08 19:48:37.092318: step 39520, loss = 0.79 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:48:38.351699: step 39530, loss = 0.67 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:48:39.641488: step 39540, loss = 0.64 (992.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:48:40.928218: step 39550, loss = 0.99 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:48:42.246418: step 39560, loss = 0.80 (971.0 examples/sec; 0.132 sec/batch)
2017-05-08 19:48:43.521550: step 39570, loss = 0.55 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:48:44.787313: step 39580, loss = 0.70 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:48:46.050670: step 39590, loss = 0.70 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:48:47.428359: step 39600, loss = 0.85 (929.1 examples/sec; 0.138 sec/batch)
2017-05-08 19:48:48.623408: step 39610, loss = 1.00 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-08 19:48:49.879278: step 39620, loss = 0.77 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:48:51.195304: step 39630, loss = 0.84 (972.6 examples/sec; 0.132 sec/batch)
2017-05-08 19:48:52.489038: step 39640, loss = 0.56 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:48:53.747868: step 39650, loss = 0.74 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:48:54.997669: step 39660, loss = 0.82 (1024.2 examples/sec; 0.125 sec/batch)
2017-05-08 19:48:56.287565: step 39670, loss = 0.60 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:48:57.546706: step 39680, loss = 0.77 (1016.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:48:58.846539: step 39690, loss = 0.79 (984.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:49:00.184914: step 39700, loss = 0.81 (956.4 examples/sec; 0.134 sec/batch)
2017-05-08 19:49:01.385580: step 39710, loss = 0.95 (1066.1 examples/sec; 0.120 sec/batch)
2017-05-08 19:49:02.665712: step 39720, loss = 0.62 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:49:03.979691: step 39730, loss = 0.66 (974.1 examples/sec; 0.131 sec/batch)
2017-05-08 19:49:05.237551: step 39740, loss = 0.92 (1017.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:49:06.504723: step 39750, loss = 0.72 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:49:07.772228: step 39760, loss = 0.75 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:49:09.029289: step 39770, loss = 0.78 (1018.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:49:10.302755: step 39780, loss = 0.82 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:49:11.577005: step 39790, loss = 0.95 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:49:12.952335: step 39800, loss = 0.75 (930.7 examples/sec; 0.138 sec/batch)
2017-05-08 19:49:14.141532: step 39810, loss = 0.66 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-08 19:49:15.466332: step 39820, loss = 0.65 (966.2 examples/sec; 0.132 sec/batch)
2017-05-08 19:49:16.730802: step 39830, loss = 0.81 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:49:17.991639: step 39840, loss = 0.81 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:49:19.258136: step 39850, loss = 0.98 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:49:20.530496: step 39860, loss = 0.80 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:49:21.791287: step 39870, loss = 0.71 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:49:23.069720: step 39880, loss = 0.88 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:49:24.3E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 811 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
37083: step 39890, loss = 0.78 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:49:25.717984: step 39900, loss = 0.75 (926.9 examples/sec; 0.138 sec/batch)
2017-05-08 19:49:26.888995: step 39910, loss = 0.82 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-08 19:49:28.194287: step 39920, loss = 0.81 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 19:49:29.478091: step 39930, loss = 0.76 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:49:30.756656: step 39940, loss = 0.75 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:49:32.012095: step 39950, loss = 0.79 (1019.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:49:33.287978: step 39960, loss = 0.76 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:49:34.547880: step 39970, loss = 0.75 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:49:35.807582: step 39980, loss = 0.72 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:49:37.119977: step 39990, loss = 0.73 (975.3 examples/sec; 0.131 sec/batch)
2017-05-08 19:49:38.480299: step 40000, loss = 0.72 (940.9 examples/sec; 0.136 sec/batch)
2017-05-08 19:49:39.627032: step 40010, loss = 0.67 (1116.2 examples/sec; 0.115 sec/batch)
2017-05-08 19:49:40.929385: step 40020, loss = 0.83 (982.8 examples/sec; 0.130 sec/batch)
2017-05-08 19:49:42.195150: step 40030, loss = 0.84 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:49:43.466748: step 40040, loss = 0.88 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:49:44.745994: step 40050, loss = 0.75 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:49:45.999195: step 40060, loss = 0.81 (1021.4 examples/sec; 0.125 sec/batch)
2017-05-08 19:49:47.264639: step 40070, loss = 0.65 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:49:48.542381: step 40080, loss = 0.65 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:49:49.801566: step 40090, loss = 0.84 (1016.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:49:51.189417: step 40100, loss = 0.70 (922.3 examples/sec; 0.139 sec/batch)
2017-05-08 19:49:52.367334: step 40110, loss = 0.76 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-08 19:49:53.671655: step 40120, loss = 0.81 (981.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:49:54.925380: step 40130, loss = 0.80 (1021.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:49:56.192979: step 40140, loss = 0.70 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:49:57.460934: step 40150, loss = 0.79 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:49:58.727488: step 40160, loss = 0.67 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:50:00.011182: step 40170, loss = 0.76 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:50:01.278808: step 40180, loss = 0.77 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:50:02.570707: step 40190, loss = 0.68 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:50:03.940362: step 40200, loss = 0.68 (934.5 examples/sec; 0.137 sec/batch)
2017-05-08 19:50:05.122123: step 40210, loss = 0.76 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-08 19:50:06.407128: step 40220, loss = 0.72 (996.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:50:07.689307: step 40230, loss = 0.76 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:50:08.972887: step 40240, loss = 0.80 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:50:10.227647: step 40250, loss = 0.72 (1020.1 examples/sec; 0.125 sec/batch)
2017-05-08 19:50:11.477240: step 40260, loss = 0.72 (1024.3 examples/sec; 0.125 sec/batch)
2017-05-08 19:50:12.748683: step 40270, loss = 0.62 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:50:14.006852: step 40280, loss = 0.87 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:50:15.290834: step 40290, loss = 0.83 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:50:16.683254: step 40300, loss = 0.65 (919.3 examples/sec; 0.139 sec/batch)
2017-05-08 19:50:17.868157: step 40310, loss = 0.76 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-08 19:50:19.196120: step 40320, loss = 0.68 (963.9 examples/sec; 0.133 sec/batch)
2017-05-08 19:50:20.460606: step 40330, loss = 0.68 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:50:21.776685: step 40340, loss = 0.91 (972.6 examples/sec; 0.132 sec/batch)
2017-05-08 19:50:23.008717: step 40350, loss = 0.69 (1038.9 examples/sec; 0.123 sec/batch)
2017-05-08 19:50:24.291674: step 40360, loss = 0.78 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:50:25.569797: step 40370, loss = 0.78 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:50:26.824769: step 40380, loss = 0.57 (1019.9 examples/sec; 0.125 sec/batch)
2017-05-08 19:50:28.095420: step 40390, loss = 0.70 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:50:29.503180: step 40400, loss = 0.68 (909.3 examples/sec; 0.141 sec/batch)
2017-05-08 19:50:30.679268: step 40410, loss = 0.73 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-08 19:50:31.978905: step 40420, loss = 0.76 (984.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:50:33.288159: step 40430, loss = 0.72 (977.7 examples/sec; 0.131 sec/batch)
2017-05-08 19:50:34.548303: step 40440, loss = 0.75 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:50:35.821113: step 40450, loss = 0.69 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:50:37.094696: step 40460, loss = 0.65 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:50:38.352100: step 40470, loss = 0.66 (1018.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:50:39.645610: step 40480, loss = 0.89 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:50:40.909900: step 40490, loss = 0.75 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:50:42.291602: step 40500, loss = 0.80 (926.4 examples/sec; 0.138 sec/batch)
2017-05-08 19:50:43.488615: step 40510, loss = 0.51 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-08 19:50:44.772048: step 40520, loss = 0.77 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:50:46.048217: step 40530, loss = 0.63 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:50:47.307912: step 40540, loss = 0.80 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:50:48.569944: step 40550, loss = 0.74 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:50:49.837725: step 40560, loss = 0.74 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:50:51.089440: step 40570, loss = 0.92 (1022.6 examples/sec; 0.125 sec/batch)
2017-05-08 19:50:52.368118: step 40580, loss = 0.68 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:50:53.622635: step 40590, loss = 0.65 (1020.3 examples/sec; 0.125 sec/batch)
2017-05-08 19:50:54.999115: step 40600, loss = 0.69 (929.9 examples/sec; 0.138 sec/batch)
2017-05-08 19:50:56.185743: step 40610, loss = 0.81 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-08 19:50:57.464645: step 40620, loss = 0.70 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:50:58.738209: step 40630, loss = 0.73 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:51:00.000614: step 40640, loss = 0.85 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:51:01.281900: step 40650, loss = 0.97 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:51:02.518680: step 40660, loss = 0.62 (1034.9 examples/sec; 0.124 sec/batch)
2017-05-08 19:51:03.793250: step 40670, loss = 0.93 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:51:05.050131: step 40680, loss = 0.90 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:51:06.330438: step 40690, loss = 0.61 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:51:07.743466: step 40700, loss = 0.75 (905.9 examples/sec; 0.141 sec/batch)
2017-05-08 19:51:08.961021: step 40710, loss = 0.79 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-08 19:51:10.182664: step 40720, loss = 0.83 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-08 19:51:11.454691: step 40730, loss = 0.75 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:51:12.734975: step 40740, loss = 0.70 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:51:14.003932: step 40750, loss = 0.69 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:51:15.262696: step 40760, loss = 0.73 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:51:16.547256: step 40770, loss = 0.71 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:51:17.827090: step 40780, loss = 0.74 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:51:19.109801: step 40790, loss = 0.85 (9E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 831 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
97.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:51:20.471682: step 40800, loss = 0.84 (939.9 examples/sec; 0.136 sec/batch)
2017-05-08 19:51:21.673661: step 40810, loss = 0.82 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-08 19:51:22.946602: step 40820, loss = 0.75 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:51:24.201768: step 40830, loss = 0.76 (1019.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:51:25.448034: step 40840, loss = 0.64 (1027.1 examples/sec; 0.125 sec/batch)
2017-05-08 19:51:26.733415: step 40850, loss = 0.84 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:51:28.014961: step 40860, loss = 0.86 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:51:29.326038: step 40870, loss = 0.71 (976.3 examples/sec; 0.131 sec/batch)
2017-05-08 19:51:30.612331: step 40880, loss = 0.83 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:51:31.867094: step 40890, loss = 0.79 (1020.1 examples/sec; 0.125 sec/batch)
2017-05-08 19:51:33.236407: step 40900, loss = 0.87 (934.8 examples/sec; 0.137 sec/batch)
2017-05-08 19:51:34.422127: step 40910, loss = 0.66 (1079.5 examples/sec; 0.119 sec/batch)
2017-05-08 19:51:35.671514: step 40920, loss = 0.63 (1024.5 examples/sec; 0.125 sec/batch)
2017-05-08 19:51:36.972484: step 40930, loss = 0.74 (983.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:51:38.258766: step 40940, loss = 0.76 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 19:51:39.553490: step 40950, loss = 0.61 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:51:40.807328: step 40960, loss = 0.71 (1020.8 examples/sec; 0.125 sec/batch)
2017-05-08 19:51:42.075764: step 40970, loss = 0.70 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:51:43.343693: step 40980, loss = 0.87 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:51:44.601178: step 40990, loss = 0.80 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:51:45.967730: step 41000, loss = 0.80 (936.7 examples/sec; 0.137 sec/batch)
2017-05-08 19:51:47.136314: step 41010, loss = 0.62 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-08 19:51:48.386155: step 41020, loss = 0.72 (1024.1 examples/sec; 0.125 sec/batch)
2017-05-08 19:51:49.688654: step 41030, loss = 0.63 (982.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:51:50.973253: step 41040, loss = 0.72 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:51:52.283789: step 41050, loss = 0.75 (976.7 examples/sec; 0.131 sec/batch)
2017-05-08 19:51:53.544651: step 41060, loss = 0.83 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:51:54.833255: step 41070, loss = 0.69 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:51:56.093603: step 41080, loss = 0.68 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:51:57.367908: step 41090, loss = 0.73 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:51:58.753816: step 41100, loss = 0.75 (923.6 examples/sec; 0.139 sec/batch)
2017-05-08 19:51:59.905115: step 41110, loss = 0.80 (1111.8 examples/sec; 0.115 sec/batch)
2017-05-08 19:52:01.148881: step 41120, loss = 0.67 (1029.1 examples/sec; 0.124 sec/batch)
2017-05-08 19:52:02.441765: step 41130, loss = 0.83 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:52:03.720531: step 41140, loss = 0.67 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:52:05.025972: step 41150, loss = 0.67 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:52:06.293617: step 41160, loss = 0.68 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:52:07.593888: step 41170, loss = 0.83 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:52:08.872357: step 41180, loss = 0.72 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:52:10.142911: step 41190, loss = 0.74 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:52:11.517288: step 41200, loss = 0.59 (931.3 examples/sec; 0.137 sec/batch)
2017-05-08 19:52:12.667828: step 41210, loss = 0.79 (1112.5 examples/sec; 0.115 sec/batch)
2017-05-08 19:52:13.941272: step 41220, loss = 0.81 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:52:15.198511: step 41230, loss = 0.84 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:52:16.480420: step 41240, loss = 0.73 (998.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:52:17.776278: step 41250, loss = 0.61 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 19:52:19.050950: step 41260, loss = 0.79 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:52:20.309060: step 41270, loss = 0.70 (1017.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:52:21.590709: step 41280, loss = 0.72 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:52:22.857429: step 41290, loss = 0.77 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:52:24.197094: step 41300, loss = 0.85 (955.5 examples/sec; 0.134 sec/batch)
2017-05-08 19:52:25.386519: step 41310, loss = 0.64 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-08 19:52:26.670072: step 41320, loss = 0.79 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:52:27.973696: step 41330, loss = 0.85 (981.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:52:29.248910: step 41340, loss = 0.73 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:52:30.551366: step 41350, loss = 0.73 (982.8 examples/sec; 0.130 sec/batch)
2017-05-08 19:52:31.817494: step 41360, loss = 0.70 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:52:33.072773: step 41370, loss = 0.64 (1019.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:52:34.343255: step 41380, loss = 0.74 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:52:35.593872: step 41390, loss = 0.76 (1023.5 examples/sec; 0.125 sec/batch)
2017-05-08 19:52:36.944581: step 41400, loss = 0.61 (947.7 examples/sec; 0.135 sec/batch)
2017-05-08 19:52:38.137904: step 41410, loss = 0.81 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-08 19:52:39.385578: step 41420, loss = 0.79 (1025.9 examples/sec; 0.125 sec/batch)
2017-05-08 19:52:40.687834: step 41430, loss = 0.82 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:52:41.948588: step 41440, loss = 0.89 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:52:43.206790: step 41450, loss = 0.90 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:52:44.468667: step 41460, loss = 0.98 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:52:45.745630: step 41470, loss = 0.82 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:52:47.012320: step 41480, loss = 0.74 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:52:48.280907: step 41490, loss = 0.74 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:52:49.650288: step 41500, loss = 0.77 (934.7 examples/sec; 0.137 sec/batch)
2017-05-08 19:52:50.833920: step 41510, loss = 0.69 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-08 19:52:52.147678: step 41520, loss = 0.78 (974.3 examples/sec; 0.131 sec/batch)
2017-05-08 19:52:53.416726: step 41530, loss = 0.89 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:52:54.698367: step 41540, loss = 0.60 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:52:55.979317: step 41550, loss = 0.71 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:52:57.233418: step 41560, loss = 0.78 (1020.7 examples/sec; 0.125 sec/batch)
2017-05-08 19:52:58.515422: step 41570, loss = 0.80 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:52:59.782046: step 41580, loss = 0.80 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:53:01.056853: step 41590, loss = 0.78 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:53:02.409359: step 41600, loss = 0.69 (946.4 examples/sec; 0.135 sec/batch)
2017-05-08 19:53:03.614074: step 41610, loss = 0.64 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-08 19:53:04.878057: step 41620, loss = 0.97 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:53:06.146577: step 41630, loss = 0.73 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:53:07.402078: step 41640, loss = 0.70 (1019.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:53:08.676535: step 41650, loss = 0.72 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:53:09.926856: step 41660, loss = 0.62 (1023.7 examples/sec; 0.125 sec/batch)
2017-05-08 19:53:11.192192: step 41670, loss = 0.77 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:53:12.449786: step 41680, loss = 0.82 (1017.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:53:13.740246: step 41690, loss = 0.86 (991.9 examples/sec; 0.129 sec/batch)
2E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 851 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
017-05-08 19:53:15.140758: step 41700, loss = 0.76 (913.9 examples/sec; 0.140 sec/batch)
2017-05-08 19:53:16.313308: step 41710, loss = 0.77 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-08 19:53:17.565661: step 41720, loss = 0.93 (1022.1 examples/sec; 0.125 sec/batch)
2017-05-08 19:53:18.868332: step 41730, loss = 0.70 (982.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:53:20.140935: step 41740, loss = 0.82 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:53:21.393648: step 41750, loss = 0.66 (1021.8 examples/sec; 0.125 sec/batch)
2017-05-08 19:53:22.673090: step 41760, loss = 0.61 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:53:23.924485: step 41770, loss = 0.70 (1022.9 examples/sec; 0.125 sec/batch)
2017-05-08 19:53:25.182481: step 41780, loss = 0.83 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:53:26.428993: step 41790, loss = 0.74 (1026.9 examples/sec; 0.125 sec/batch)
2017-05-08 19:53:27.794601: step 41800, loss = 0.78 (937.3 examples/sec; 0.137 sec/batch)
2017-05-08 19:53:28.979795: step 41810, loss = 0.68 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-08 19:53:30.239401: step 41820, loss = 0.85 (1016.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:53:31.507276: step 41830, loss = 0.85 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:53:32.774869: step 41840, loss = 0.75 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:53:34.027704: step 41850, loss = 0.64 (1021.7 examples/sec; 0.125 sec/batch)
2017-05-08 19:53:35.304239: step 41860, loss = 0.78 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:53:36.566505: step 41870, loss = 0.66 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:53:37.860509: step 41880, loss = 0.69 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:53:39.126604: step 41890, loss = 0.82 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:53:40.492699: step 41900, loss = 0.73 (937.0 examples/sec; 0.137 sec/batch)
2017-05-08 19:53:41.653898: step 41910, loss = 0.80 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-08 19:53:42.898158: step 41920, loss = 0.87 (1028.7 examples/sec; 0.124 sec/batch)
2017-05-08 19:53:44.149015: step 41930, loss = 0.77 (1023.3 examples/sec; 0.125 sec/batch)
2017-05-08 19:53:45.442429: step 41940, loss = 0.73 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:53:46.737434: step 41950, loss = 0.63 (988.4 examples/sec; 0.130 sec/batch)
2017-05-08 19:53:47.996205: step 41960, loss = 0.70 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:53:49.260758: step 41970, loss = 0.79 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:53:50.533244: step 41980, loss = 0.80 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:53:51.796737: step 41990, loss = 0.81 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:53:53.177586: step 42000, loss = 0.84 (927.0 examples/sec; 0.138 sec/batch)
2017-05-08 19:53:54.357080: step 42010, loss = 0.83 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-08 19:53:55.603041: step 42020, loss = 0.81 (1027.3 examples/sec; 0.125 sec/batch)
2017-05-08 19:53:56.863490: step 42030, loss = 0.64 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:53:58.140132: step 42040, loss = 0.81 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:53:59.398201: step 42050, loss = 0.91 (1017.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:54:00.668824: step 42060, loss = 0.73 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:54:01.934477: step 42070, loss = 0.63 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:54:03.194237: step 42080, loss = 0.94 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:54:04.441388: step 42090, loss = 0.85 (1026.3 examples/sec; 0.125 sec/batch)
2017-05-08 19:54:05.806850: step 42100, loss = 0.64 (937.4 examples/sec; 0.137 sec/batch)
2017-05-08 19:54:06.974139: step 42110, loss = 0.71 (1096.6 examples/sec; 0.117 sec/batch)
2017-05-08 19:54:08.218568: step 42120, loss = 0.90 (1028.6 examples/sec; 0.124 sec/batch)
2017-05-08 19:54:09.477496: step 42130, loss = 0.69 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:54:10.754424: step 42140, loss = 0.83 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:54:12.048378: step 42150, loss = 0.73 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 19:54:13.340729: step 42160, loss = 0.80 (990.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:54:14.592514: step 42170, loss = 0.90 (1022.5 examples/sec; 0.125 sec/batch)
2017-05-08 19:54:15.843754: step 42180, loss = 0.63 (1023.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:54:17.131883: step 42190, loss = 0.76 (993.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:54:18.486865: step 42200, loss = 0.82 (944.7 examples/sec; 0.135 sec/batch)
2017-05-08 19:54:19.658675: step 42210, loss = 0.69 (1092.3 examples/sec; 0.117 sec/batch)
2017-05-08 19:54:20.925165: step 42220, loss = 0.65 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:54:22.193827: step 42230, loss = 0.69 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:54:23.463720: step 42240, loss = 0.59 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:54:24.757603: step 42250, loss = 0.65 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:54:26.035653: step 42260, loss = 0.69 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:54:27.296743: step 42270, loss = 1.02 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:54:28.546673: step 42280, loss = 0.65 (1024.1 examples/sec; 0.125 sec/batch)
2017-05-08 19:54:29.810913: step 42290, loss = 0.67 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:54:31.161271: step 42300, loss = 0.72 (947.9 examples/sec; 0.135 sec/batch)
2017-05-08 19:54:32.357551: step 42310, loss = 0.71 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-08 19:54:33.695678: step 42320, loss = 0.73 (956.6 examples/sec; 0.134 sec/batch)
2017-05-08 19:54:34.907532: step 42330, loss = 0.74 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-08 19:54:36.179356: step 42340, loss = 0.66 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:54:37.450258: step 42350, loss = 0.80 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:54:38.739547: step 42360, loss = 0.85 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:54:40.038647: step 42370, loss = 0.92 (985.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:54:41.352364: step 42380, loss = 0.66 (974.3 examples/sec; 0.131 sec/batch)
2017-05-08 19:54:42.643381: step 42390, loss = 0.69 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 19:54:43.988619: step 42400, loss = 0.99 (951.5 examples/sec; 0.135 sec/batch)
2017-05-08 19:54:45.127253: step 42410, loss = 0.75 (1124.2 examples/sec; 0.114 sec/batch)
2017-05-08 19:54:46.423500: step 42420, loss = 0.78 (987.5 examples/sec; 0.130 sec/batch)
2017-05-08 19:54:47.686407: step 42430, loss = 0.85 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:54:48.955218: step 42440, loss = 0.68 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:54:50.239263: step 42450, loss = 0.71 (996.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:54:51.500329: step 42460, loss = 0.74 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:54:52.740167: step 42470, loss = 0.75 (1032.4 examples/sec; 0.124 sec/batch)
2017-05-08 19:54:54.037006: step 42480, loss = 0.91 (987.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:54:55.310833: step 42490, loss = 0.88 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:54:56.688246: step 42500, loss = 0.87 (929.3 examples/sec; 0.138 sec/batch)
2017-05-08 19:54:57.944436: step 42510, loss = 0.75 (1019.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:54:59.187324: step 42520, loss = 0.96 (1029.9 examples/sec; 0.124 sec/batch)
2017-05-08 19:55:00.447848: step 42530, loss = 0.69 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:55:01.712363: step 42540, loss = 0.68 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:55:02.963823: step 42550, loss = 0.63 (1022.8 examples/sec; 0.125 sec/batch)
2017-05-08 19:55:04.236705: step 42560, loss = 0.83 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:55:05.507561: step 42570, loss = 0.78 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:55:06.779579: step 42580, loss = 1.01 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:55:08.066107: step 42590, loss = 0.71 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:55:09.445412: sE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 872 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
tep 42600, loss = 0.70 (928.0 examples/sec; 0.138 sec/batch)
2017-05-08 19:55:10.648393: step 42610, loss = 0.91 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-08 19:55:11.950652: step 42620, loss = 0.79 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:55:13.251402: step 42630, loss = 0.55 (984.1 examples/sec; 0.130 sec/batch)
2017-05-08 19:55:14.530200: step 42640, loss = 0.81 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:55:15.797602: step 42650, loss = 0.63 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:55:17.107784: step 42660, loss = 0.81 (977.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:55:18.369340: step 42670, loss = 0.73 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:55:19.631414: step 42680, loss = 0.66 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:55:20.915187: step 42690, loss = 0.71 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:55:22.269571: step 42700, loss = 0.85 (945.1 examples/sec; 0.135 sec/batch)
2017-05-08 19:55:23.460815: step 42710, loss = 0.81 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-08 19:55:24.710622: step 42720, loss = 0.73 (1024.2 examples/sec; 0.125 sec/batch)
2017-05-08 19:55:25.993904: step 42730, loss = 0.68 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:55:27.312856: step 42740, loss = 0.58 (970.5 examples/sec; 0.132 sec/batch)
2017-05-08 19:55:28.600770: step 42750, loss = 0.66 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:55:29.895287: step 42760, loss = 0.69 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:55:31.197581: step 42770, loss = 0.88 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 19:55:32.506881: step 42780, loss = 0.93 (977.6 examples/sec; 0.131 sec/batch)
2017-05-08 19:55:33.797935: step 42790, loss = 0.70 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 19:55:35.141168: step 42800, loss = 0.67 (952.9 examples/sec; 0.134 sec/batch)
2017-05-08 19:55:36.304144: step 42810, loss = 0.86 (1100.6 examples/sec; 0.116 sec/batch)
2017-05-08 19:55:37.580964: step 42820, loss = 0.80 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:55:38.839410: step 42830, loss = 0.60 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:55:40.103046: step 42840, loss = 0.72 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:55:41.378061: step 42850, loss = 0.78 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:55:42.641051: step 42860, loss = 0.64 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:55:43.907130: step 42870, loss = 0.76 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:55:45.156611: step 42880, loss = 0.90 (1024.4 examples/sec; 0.125 sec/batch)
2017-05-08 19:55:46.464713: step 42890, loss = 0.85 (978.5 examples/sec; 0.131 sec/batch)
2017-05-08 19:55:47.797111: step 42900, loss = 0.80 (960.7 examples/sec; 0.133 sec/batch)
2017-05-08 19:55:48.967491: step 42910, loss = 0.68 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-08 19:55:50.239932: step 42920, loss = 0.77 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:55:51.496820: step 42930, loss = 0.71 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:55:52.757911: step 42940, loss = 0.63 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:55:54.060015: step 42950, loss = 0.79 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:55:55.325656: step 42960, loss = 0.73 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:55:56.593915: step 42970, loss = 0.73 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:55:57.847503: step 42980, loss = 0.77 (1021.1 examples/sec; 0.125 sec/batch)
2017-05-08 19:55:59.140860: step 42990, loss = 0.65 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:56:00.476090: step 43000, loss = 0.81 (958.6 examples/sec; 0.134 sec/batch)
2017-05-08 19:56:01.682648: step 43010, loss = 0.77 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-08 19:56:02.933916: step 43020, loss = 0.79 (1023.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:56:04.175585: step 43030, loss = 0.94 (1030.9 examples/sec; 0.124 sec/batch)
2017-05-08 19:56:05.428052: step 43040, loss = 0.77 (1022.0 examples/sec; 0.125 sec/batch)
2017-05-08 19:56:06.717613: step 43050, loss = 0.94 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:56:07.980675: step 43060, loss = 0.73 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:56:09.229826: step 43070, loss = 0.96 (1024.7 examples/sec; 0.125 sec/batch)
2017-05-08 19:56:10.511183: step 43080, loss = 0.82 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:56:11.769517: step 43090, loss = 0.61 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:56:13.118639: step 43100, loss = 0.74 (948.8 examples/sec; 0.135 sec/batch)
2017-05-08 19:56:14.324884: step 43110, loss = 0.84 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-08 19:56:15.578914: step 43120, loss = 0.69 (1020.7 examples/sec; 0.125 sec/batch)
2017-05-08 19:56:16.842526: step 43130, loss = 0.70 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:56:18.114941: step 43140, loss = 0.60 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:56:19.376572: step 43150, loss = 0.79 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-08 19:56:20.613991: step 43160, loss = 0.80 (1034.4 examples/sec; 0.124 sec/batch)
2017-05-08 19:56:21.884117: step 43170, loss = 0.81 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:56:23.165312: step 43180, loss = 0.63 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:56:24.441938: step 43190, loss = 0.92 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:56:25.801096: step 43200, loss = 0.89 (941.8 examples/sec; 0.136 sec/batch)
2017-05-08 19:56:26.969703: step 43210, loss = 0.63 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-08 19:56:28.248157: step 43220, loss = 0.54 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:56:29.511818: step 43230, loss = 0.81 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:56:30.763682: step 43240, loss = 0.77 (1022.5 examples/sec; 0.125 sec/batch)
2017-05-08 19:56:32.023879: step 43250, loss = 0.71 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:56:33.279739: step 43260, loss = 0.79 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:56:34.555937: step 43270, loss = 0.96 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:56:35.821057: step 43280, loss = 0.75 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:56:37.110625: step 43290, loss = 0.69 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:56:38.484049: step 43300, loss = 0.94 (932.0 examples/sec; 0.137 sec/batch)
2017-05-08 19:56:39.741428: step 43310, loss = 0.90 (1018.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:56:40.956351: step 43320, loss = 0.74 (1053.6 examples/sec; 0.121 sec/batch)
2017-05-08 19:56:42.219656: step 43330, loss = 0.84 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:56:43.512856: step 43340, loss = 0.77 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:56:44.780442: step 43350, loss = 0.85 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:56:46.061198: step 43360, loss = 0.92 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:56:47.328124: step 43370, loss = 0.68 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:56:48.620278: step 43380, loss = 0.73 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:56:49.883863: step 43390, loss = 0.75 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:56:51.247156: step 43400, loss = 0.67 (938.9 examples/sec; 0.136 sec/batch)
2017-05-08 19:56:52.407956: step 43410, loss = 0.79 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-08 19:56:53.686687: step 43420, loss = 0.84 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:56:54.952273: step 43430, loss = 0.61 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:56:56.270061: step 43440, loss = 0.65 (971.3 examples/sec; 0.132 sec/batch)
2017-05-08 19:56:57.532100: step 43450, loss = 0.61 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:56:58.800071: step 43460, loss = 0.72 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:57:00.057653: step 43470, loss = 0.73 (1017.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:57:01.336887: step 43480, loss = 0.84 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:57:02.613845: step 43490, loss = 0.69 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:57:04.000960: step 43500, loss = 0.82 (922.8 exaE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 892 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
mples/sec; 0.139 sec/batch)
2017-05-08 19:57:05.156599: step 43510, loss = 0.68 (1107.6 examples/sec; 0.116 sec/batch)
2017-05-08 19:57:06.412066: step 43520, loss = 0.75 (1019.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:57:07.676366: step 43530, loss = 0.82 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-08 19:57:08.943682: step 43540, loss = 0.83 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:57:10.195857: step 43550, loss = 0.63 (1022.2 examples/sec; 0.125 sec/batch)
2017-05-08 19:57:11.472727: step 43560, loss = 0.75 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:57:12.744784: step 43570, loss = 0.78 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:57:14.041609: step 43580, loss = 0.71 (987.0 examples/sec; 0.130 sec/batch)
2017-05-08 19:57:15.270631: step 43590, loss = 0.85 (1041.5 examples/sec; 0.123 sec/batch)
2017-05-08 19:57:16.611790: step 43600, loss = 0.84 (954.4 examples/sec; 0.134 sec/batch)
2017-05-08 19:57:17.782998: step 43610, loss = 0.66 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-08 19:57:19.042900: step 43620, loss = 0.61 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-08 19:57:20.322788: step 43630, loss = 0.61 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:57:21.604808: step 43640, loss = 0.86 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:57:22.891336: step 43650, loss = 0.92 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:57:24.143390: step 43660, loss = 0.68 (1022.3 examples/sec; 0.125 sec/batch)
2017-05-08 19:57:25.407268: step 43670, loss = 0.57 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:57:26.677049: step 43680, loss = 0.68 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-08 19:57:27.935989: step 43690, loss = 0.73 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:57:29.286691: step 43700, loss = 0.79 (947.7 examples/sec; 0.135 sec/batch)
2017-05-08 19:57:30.484639: step 43710, loss = 0.63 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-08 19:57:31.750582: step 43720, loss = 0.74 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:57:32.994005: step 43730, loss = 0.86 (1029.4 examples/sec; 0.124 sec/batch)
2017-05-08 19:57:34.263493: step 43740, loss = 0.66 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:57:35.534774: step 43750, loss = 0.79 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:57:36.819563: step 43760, loss = 0.67 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:57:38.141066: step 43770, loss = 0.69 (968.6 examples/sec; 0.132 sec/batch)
2017-05-08 19:57:39.422220: step 43780, loss = 0.87 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:57:40.759915: step 43790, loss = 0.81 (956.9 examples/sec; 0.134 sec/batch)
2017-05-08 19:57:42.125482: step 43800, loss = 0.74 (937.4 examples/sec; 0.137 sec/batch)
2017-05-08 19:57:43.307374: step 43810, loss = 0.84 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-08 19:57:44.610016: step 43820, loss = 0.91 (982.6 examples/sec; 0.130 sec/batch)
2017-05-08 19:57:45.882931: step 43830, loss = 0.80 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:57:47.188741: step 43840, loss = 0.77 (980.2 examples/sec; 0.131 sec/batch)
2017-05-08 19:57:48.475652: step 43850, loss = 0.79 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 19:57:49.744635: step 43860, loss = 0.76 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:57:51.017792: step 43870, loss = 0.80 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:57:52.306861: step 43880, loss = 0.72 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:57:53.608141: step 43890, loss = 0.73 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:57:54.982965: step 43900, loss = 0.61 (931.0 examples/sec; 0.137 sec/batch)
2017-05-08 19:57:56.136331: step 43910, loss = 0.76 (1109.8 examples/sec; 0.115 sec/batch)
2017-05-08 19:57:57.395264: step 43920, loss = 0.75 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:57:58.655934: step 43930, loss = 0.65 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-08 19:57:59.953884: step 43940, loss = 0.69 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 19:58:01.228418: step 43950, loss = 0.80 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:58:02.539963: step 43960, loss = 0.67 (976.0 examples/sec; 0.131 sec/batch)
2017-05-08 19:58:03.788816: step 43970, loss = 0.71 (1024.9 examples/sec; 0.125 sec/batch)
2017-05-08 19:58:05.034858: step 43980, loss = 0.72 (1027.3 examples/sec; 0.125 sec/batch)
2017-05-08 19:58:06.317653: step 43990, loss = 0.67 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 19:58:07.692125: step 44000, loss = 0.87 (931.3 examples/sec; 0.137 sec/batch)
2017-05-08 19:58:08.881198: step 44010, loss = 0.68 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-08 19:58:10.158428: step 44020, loss = 0.71 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:58:11.418532: step 44030, loss = 0.80 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:58:12.697295: step 44040, loss = 0.74 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:58:13.968593: step 44050, loss = 0.69 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:58:15.239839: step 44060, loss = 0.71 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:58:16.518920: step 44070, loss = 0.75 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:58:17.787786: step 44080, loss = 0.67 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-08 19:58:19.069005: step 44090, loss = 0.75 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:58:20.446484: step 44100, loss = 0.85 (929.2 examples/sec; 0.138 sec/batch)
2017-05-08 19:58:21.610612: step 44110, loss = 0.82 (1099.5 examples/sec; 0.116 sec/batch)
2017-05-08 19:58:22.895336: step 44120, loss = 0.74 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:58:24.203043: step 44130, loss = 0.78 (978.8 examples/sec; 0.131 sec/batch)
2017-05-08 19:58:25.480410: step 44140, loss = 0.76 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:58:26.753581: step 44150, loss = 0.71 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:58:28.022375: step 44160, loss = 0.81 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:58:29.303795: step 44170, loss = 0.65 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:58:30.565146: step 44180, loss = 0.84 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-08 19:58:31.839960: step 44190, loss = 0.97 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:58:33.201608: step 44200, loss = 0.87 (940.0 examples/sec; 0.136 sec/batch)
2017-05-08 19:58:34.403684: step 44210, loss = 0.77 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-08 19:58:35.674177: step 44220, loss = 0.85 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 19:58:37.010066: step 44230, loss = 0.72 (958.2 examples/sec; 0.134 sec/batch)
2017-05-08 19:58:38.317254: step 44240, loss = 0.78 (979.2 examples/sec; 0.131 sec/batch)
2017-05-08 19:58:39.604629: step 44250, loss = 0.91 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 19:58:40.868036: step 44260, loss = 0.76 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:58:42.121210: step 44270, loss = 0.69 (1021.4 examples/sec; 0.125 sec/batch)
2017-05-08 19:58:43.376239: step 44280, loss = 0.77 (1019.9 examples/sec; 0.126 sec/batch)
2017-05-08 19:58:44.665273: step 44290, loss = 0.74 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:58:46.037079: step 44300, loss = 0.80 (933.1 examples/sec; 0.137 sec/batch)
2017-05-08 19:58:47.233929: step 44310, loss = 0.80 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-08 19:58:48.512379: step 44320, loss = 0.88 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 19:58:49.785283: step 44330, loss = 0.90 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 19:58:51.056498: step 44340, loss = 0.72 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:58:52.339150: step 44350, loss = 0.77 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:58:53.640345: step 44360, loss = 0.73 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:58:54.907537: step 44370, loss = 0.64 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-08 19:58:56.182078: step 44380, loss = 0.68 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 19:58:57.457730: step 44390, loss = 0.70 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 19:58:58.847713: step 44400, loss = 0.71 (920.9 examples/sec; 0.139 sec/batch)
2017-05-08 19:59:00.047816: step 44410, loss = 0.63 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-08 19:59:01.330430: step 44420, loss = 0.90 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 19:59:02.590013: step 44430, loss = 0.67 (1016.2 examples/sec; 0.126 sec/batch)
2017-05-08 19:59:03.873090: step 44440, loss = 0.79 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:59:05.132051: step 44450, loss = 0.95 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:59:06.416418: step 44460, loss = 0.80 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 19:59:07.697539: step 44470, loss = 0.68 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:59:08.984161: step 44480, loss = 0.75 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 19:59:10.270605: step 44490, loss = 0.76 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 19:59:11.661397: step 44500, loss = 0.73 (920.3 examples/sec; 0.139 sec/batch)
2017-05-08 19:59:12.804074: step 44510, loss = 0.81 (1120.2 examples/sec; 0.114 sec/batch)
2017-05-08 19:59:14.082861: step 44520, loss = 0.73 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 19:59:15.363496: step 44530, loss = 0.74 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 19:59:16.620700: step 44540, loss = 0.85 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-08 19:59:17.894698: step 44550, loss = 0.81 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:59:19.191936: step 44560, loss = 0.68 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 19:59:20.460051: step 44570, loss = 0.81 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-08 19:59:21.738422: step 44580, loss = 0.81 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:59:22.991758: step 44590, loss = 0.80 (1021.3 examples/sec; 0.125 sec/batch)
2017-05-08 19:59:24.331269: step 44600, loss = 0.85 (955.6 examples/sec; 0.134 sec/batch)
2017-05-08 19:59:25.526744: step 44610, loss = 0.67 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-08 19:59:26.790643: step 44620, loss = 0.73 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:59:28.088394: step 44630, loss = 0.65 (986.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:59:29.329231: step 44640, loss = 0.87 (1031.6 examples/sec; 0.124 sec/batch)
2017-05-08 19:59:30.593390: step 44650, loss = 0.77 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:59:31.871267: step 44660, loss = 0.69 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 19:59:33.140800: step 44670, loss = 0.76 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-08 19:59:34.422983: step 44680, loss = 0.65 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:59:35.654994: step 44690, loss = 0.81 (1038.9 examples/sec; 0.123 sec/batch)
2017-05-08 19:59:37.014991: step 44700, loss = 0.70 (941.2 examples/sec; 0.136 sec/batch)
2017-05-08 19:59:38.193360: step 44710, loss = 0.85 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-08 19:59:39.482701: step 44720, loss = 0.75 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 19:59:40.768111: step 44730, loss = 0.74 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:59:42.020002: step 44740, loss = 0.92 (1022.5 examples/sec; 0.125 sec/batch)
2017-05-08 19:59:43.317781: step 44750, loss = 0.65 (986.3 examples/sec; 0.130 sec/batch)
2017-05-08 19:59:44.597405: step 44760, loss = 0.72 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 19:59:45.866340: step 44770, loss = 0.62 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:59:47.137823: step 44780, loss = 0.79 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 19:59:48.392822: step 44790, loss = 0.75 (1019.9 examples/sec; 0.125 sec/batch)
2017-05-08 19:59:49.755375: step 44800, loss = 0.88 (939.4 examples/sec; 0.136 sec/batch)
2017-05-08 19:59:50.971163: step 44810, loss = 0.61 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-08 19:59:52.233847: step 44820, loss = 0.79 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-08 19:59:53.501307: step 44830, loss = 0.76 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:59:54.783786: step 44840, loss = 0.80 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 19:59:56.052542: step 44850, loss = 0.58 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 19:59:57.339294:E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 912 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
 step 44860, loss = 0.73 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 19:59:58.600980: step 44870, loss = 0.66 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-08 19:59:59.858559: step 44880, loss = 0.65 (1017.8 examples/sec; 0.126 sec/batch)
2017-05-08 20:00:01.153929: step 44890, loss = 0.85 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 20:00:02.542180: step 44900, loss = 0.51 (922.0 examples/sec; 0.139 sec/batch)
2017-05-08 20:00:03.745741: step 44910, loss = 0.72 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-08 20:00:05.021976: step 44920, loss = 0.86 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 20:00:06.314058: step 44930, loss = 0.64 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 20:00:07.596413: step 44940, loss = 0.79 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 20:00:08.906712: step 44950, loss = 0.79 (976.9 examples/sec; 0.131 sec/batch)
2017-05-08 20:00:10.209935: step 44960, loss = 0.79 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 20:00:11.514925: step 44970, loss = 0.76 (980.8 examples/sec; 0.131 sec/batch)
2017-05-08 20:00:12.781380: step 44980, loss = 0.74 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-08 20:00:14.037131: step 44990, loss = 0.75 (1019.3 examples/sec; 0.126 sec/batch)
2017-05-08 20:00:15.441753: step 45000, loss = 0.68 (911.3 examples/sec; 0.140 sec/batch)
2017-05-08 20:00:16.601187: step 45010, loss = 0.73 (1104.0 examples/sec; 0.116 sec/batch)
2017-05-08 20:00:17.844351: step 45020, loss = 0.65 (1029.6 examples/sec; 0.124 sec/batch)
2017-05-08 20:00:19.137243: step 45030, loss = 0.70 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 20:00:20.400896: step 45040, loss = 0.76 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-08 20:00:21.689878: step 45050, loss = 0.59 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 20:00:22.980724: step 45060, loss = 0.70 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 20:00:24.305297: step 45070, loss = 0.76 (966.3 examples/sec; 0.132 sec/batch)
2017-05-08 20:00:25.542831: step 45080, loss = 0.83 (1034.3 examples/sec; 0.124 sec/batch)
2017-05-08 20:00:26.796317: step 45090, loss = 0.76 (1021.1 examples/sec; 0.125 sec/batch)
2017-05-08 20:00:28.173505: step 45100, loss = 0.70 (929.4 examples/sec; 0.138 sec/batch)
2017-05-08 20:00:29.347014: step 45110, loss = 0.52 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-08 20:00:30.606187: step 45120, loss = 0.73 (1016.5 examples/sec; 0.126 sec/batch)
2017-05-08 20:00:31.872276: step 45130, loss = 0.88 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 20:00:33.152056: step 45140, loss = 0.72 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 20:00:34.412552: step 45150, loss = 0.87 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-08 20:00:35.679228: step 45160, loss = 0.69 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-08 20:00:36.950608: step 45170, loss = 0.70 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-08 20:00:38.229558: step 45180, loss = 0.78 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 20:00:39.542661: step 45190, loss = 0.81 (974.8 examples/sec; 0.131 sec/batch)
2017-05-08 20:00:40.917351: step 45200, loss = 0.67 (931.1 examples/sec; 0.137 sec/batch)
2017-05-08 20:00:42.116131: step 45210, loss = 0.73 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-08 20:00:43.389499: step 45220, loss = 0.76 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-08 20:00:44.650206: step 45230, loss = 0.70 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-08 20:00:45.929110: step 45240, loss = 0.63 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 20:00:47.223102: step 45250, loss = 0.79 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 20:00:48.468793: step 45260, loss = 0.77 (1027.5 examples/sec; 0.125 sec/batch)
2017-05-08 20:00:49.728380: step 45270, loss = 0.74 (1016.2 examples/sec; 0.126 sec/batch)
2017-05-08 20:00:50.999356: step 45280, loss = 0.70 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 20:00:52.266181: step 45290, loss = 0.85 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-08 20:00:53.647348: step 45300, loss = 0.62 (926.8 examples/sec; 0.138 sec/batch)
2017-05-08 20:00:54.839615: step 45310, loss = 0.91 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-08 20:00:56.114054: step 45320, loss = 0.66 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-08 20:00:57.388531: step 45330, loss = 0.72 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 20:00:58.667261: step 45340, loss = 0.72 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 20:00:59.935956: step 45350, loss = 0.70 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 20:01:01.204131: step 45360, loss = 0.79 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-08 20:01:02.486076: step 45370, loss = 0.65 (998.5 examples/sec; 0.128 sec/batch)
2017-05-08 20:01:03.785307: step 45380, loss = 0.77 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 20:01:05.079017: step 45390, loss = 0.69 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 20:01:06.447879: step 45400, loss = 0.65 (935.1 examples/sec; 0.137 sec/batch)
2017-05-08 20:01:07.623825: step 45410, loss = 0.67 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-08 20:01:08.898261: step 45420, loss = 0.68 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-08 20:01:10.162860: step 45430, loss = 0.69 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 20:01:11.414281: step 45440, loss = 0.69 (1022.8 examples/sec; 0.125 sec/batch)
2017-05-08 20:01:12.677151: step 45450, loss = 0.84 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-08 20:01:13.974151: step 45460, loss = 0.82 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 20:01:15.234045: step 45470, loss = 0.91 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-08 20:01:16.535870: step 45480, loss = 0.64 (983.2 examples/sec; 0.130 sec/batch)
2017-05-08 20:01:17.809657: step 45490, loss = 0.76 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-08 20:01:19.135138: step 45500, loss = 0.78 (965.7 examples/sec; 0.133 sec/batch)
2017-05-08 20:01:20.389812: step 45510, loss = 0.86 (1020.2 examples/sec; 0.125 sec/batch)
2017-05-08 20:01:21.595739: step 45520, loss = 0.95 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-08 20:01:22.861217: step 45530, loss = 0.88 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-08 20:01:24.115601: step 45540, loss = 0.71 (1020.4 examples/sec; 0.125 sec/batch)
2017-05-08 20:01:25.397621: step 45550, loss = 0.71 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 20:01:26.677284: step 45560, loss = 0.72 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 20:01:27.919841: step 45570, loss = 1.03 (1030.1 examples/sec; 0.124 sec/batch)
2017-05-08 20:01:29.176393: step 45580, loss = 0.58 (1018.7 examples/sec; 0.126 sec/batch)
2017-05-08 20:01:30.441438: step 45590, loss = 0.66 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-08 20:01:31.810821: step 45600, loss = 0.79 (934.7 examples/sec; 0.137 sec/batch)
2017-05-08 20:01:33.011818: step 45610, loss = 0.73 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-08 20:01:34.265871: step 45620, loss = 0.85 (1020.7 examples/sec; 0.125 sec/batch)
2017-05-08 20:01:35.533578: step 45630, loss = 0.77 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 20:01:36.793384: step 45640, loss = 0.66 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-08 20:01:38.052032: step 45650, loss = 1.05 (1017.0 examples/sec; 0.126 sec/batch)
2017-05-08 20:01:39.315932: step 45660, loss = 0.56 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-08 20:01:40.611756: step 45670, loss = 0.89 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 20:01:41.914235: step 45680, loss = 0.76 (982.8 examples/sec; 0.130 sec/batch)
2017-05-08 20:01:43.201516: step 45690, loss = 0.56 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 20:01:44.575534: step 45700, loss = 0.71 (931.6 examples/sec; 0.137 sec/batch)
2017-05-08 20:01:45.730396: step 45710, loss = 0.63 (1108.3 examples/sec; 0.115 sec/batch)
2017-05-08 20:01:46.985393: step 45720, loss = 0.64 (1019.9 examples/sec; 0.125 sec/batch)
2017-05-08 20:01:48.238676: step 45730, loss = 0.71 (1021.3 examples/sec; 0.125 sec/batch)
2017-05-08 20:01:49.478239: step 45740, loss = 0.74 (1032.6 examples/sec; 0.124 sec/batch)
2017-05-08 20:01:50.770559: step 45750, loss = 0.85 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 20:01:52.034045: step 45760, loss = 0.76 (1013.1 eE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 932 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
xamples/sec; 0.126 sec/batch)
2017-05-08 20:01:53.319526: step 45770, loss = 0.64 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 20:01:54.608337: step 45780, loss = 0.67 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 20:01:55.895100: step 45790, loss = 0.65 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 20:01:57.282661: step 45800, loss = 0.63 (922.5 examples/sec; 0.139 sec/batch)
2017-05-08 20:01:58.457413: step 45810, loss = 0.69 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-08 20:01:59.721096: step 45820, loss = 0.76 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-08 20:02:01.007742: step 45830, loss = 0.73 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 20:02:02.253358: step 45840, loss = 0.60 (1027.6 examples/sec; 0.125 sec/batch)
2017-05-08 20:02:03.515152: step 45850, loss = 0.75 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-08 20:02:04.772990: step 45860, loss = 0.70 (1017.6 examples/sec; 0.126 sec/batch)
2017-05-08 20:02:06.096891: step 45870, loss = 0.94 (966.8 examples/sec; 0.132 sec/batch)
2017-05-08 20:02:07.375821: step 45880, loss = 0.75 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 20:02:08.642672: step 45890, loss = 0.78 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-08 20:02:10.029775: step 45900, loss = 0.71 (922.8 examples/sec; 0.139 sec/batch)
2017-05-08 20:02:11.222512: step 45910, loss = 0.76 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-08 20:02:12.496869: step 45920, loss = 0.70 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-08 20:02:13.754369: step 45930, loss = 0.89 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-08 20:02:15.019980: step 45940, loss = 0.69 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-08 20:02:16.283556: step 45950, loss = 0.99 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-08 20:02:17.537754: step 45960, loss = 0.59 (1020.6 examples/sec; 0.125 sec/batch)
2017-05-08 20:02:18.834141: step 45970, loss = 0.82 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 20:02:20.123033: step 45980, loss = 0.87 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 20:02:21.395817: step 45990, loss = 0.79 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-08 20:02:22.749821: step 46000, loss = 0.72 (945.3 examples/sec; 0.135 sec/batch)
2017-05-08 20:02:23.935704: step 46010, loss = 0.85 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-08 20:02:25.192103: step 46020, loss = 0.77 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-08 20:02:26.459681: step 46030, loss = 0.69 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 20:02:27.702251: step 46040, loss = 0.79 (1030.1 examples/sec; 0.124 sec/batch)
2017-05-08 20:02:28.940733: step 46050, loss = 0.58 (1033.5 examples/sec; 0.124 sec/batch)
2017-05-08 20:02:30.200550: step 46060, loss = 0.85 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-08 20:02:31.471597: step 46070, loss = 0.78 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 20:02:32.730653: step 46080, loss = 0.61 (1016.6 examples/sec; 0.126 sec/batch)
2017-05-08 20:02:33.998205: step 46090, loss = 0.67 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 20:02:35.392993: step 46100, loss = 0.66 (917.7 examples/sec; 0.139 sec/batch)
2017-05-08 20:02:36.557981: step 46110, loss = 0.83 (1098.7 examples/sec; 0.116 sec/batch)
2017-05-08 20:02:37.837665: step 46120, loss = 0.83 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 20:02:39.068226: step 46130, loss = 0.74 (1040.2 examples/sec; 0.123 sec/batch)
2017-05-08 20:02:40.333143: step 46140, loss = 0.68 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-08 20:02:41.614526: step 46150, loss = 0.75 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 20:02:42.879928: step 46160, loss = 0.66 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-08 20:02:44.158176: step 46170, loss = 0.78 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 20:02:45.431632: step 46180, loss = 0.79 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 20:02:46.698637: step 46190, loss = 0.61 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-08 20:02:48.054541: step 46200, loss = 0.67 (944.0 examples/sec; 0.136 sec/batch)
2017-05-08 20:02:49.249147: step 46210, loss = 0.65 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-08 20:02:50.507059: step 46220, loss = 0.73 (1017.6 examples/sec; 0.126 sec/batch)
2017-05-08 20:02:51.782084: step 46230, loss = 0.80 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-08 20:02:53.039073: step 46240, loss = 0.80 (1018.3 examples/sec; 0.126 sec/batch)
2017-05-08 20:02:54.305272: step 46250, loss = 0.82 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-08 20:02:55.565201: step 46260, loss = 0.64 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-08 20:02:56.831736: step 46270, loss = 0.64 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 20:02:58.114162: step 46280, loss = 0.79 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 20:02:59.376371: step 46290, loss = 0.74 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-08 20:03:00.727978: step 46300, loss = 0.81 (947.0 examples/sec; 0.135 sec/batch)
2017-05-08 20:03:01.982204: step 46310, loss = 0.84 (1020.5 examples/sec; 0.125 sec/batch)
2017-05-08 20:03:03.201720: step 46320, loss = 0.69 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-08 20:03:04.466348: step 46330, loss = 0.72 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 20:03:05.769144: step 46340, loss = 0.63 (982.5 examples/sec; 0.130 sec/batch)
2017-05-08 20:03:07.064688: step 46350, loss = 0.66 (988.0 examples/sec; 0.130 sec/batch)
2017-05-08 20:03:08.346753: step 46360, loss = 0.82 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 20:03:09.629220: step 46370, loss = 0.57 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 20:03:10.892493: step 46380, loss = 0.79 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-08 20:03:12.173419: step 46390, loss = 0.76 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 20:03:13.505990: step 46400, loss = 0.60 (960.6 examples/sec; 0.133 sec/batch)
2017-05-08 20:03:14.709382: step 46410, loss = 0.91 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-08 20:03:15.960182: step 46420, loss = 0.66 (1023.3 examples/sec; 0.125 sec/batch)
2017-05-08 20:03:17.212591: step 46430, loss = 0.84 (1022.0 examples/sec; 0.125 sec/batch)
2017-05-08 20:03:18.493187: step 46440, loss = 0.74 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 20:03:19.784613: step 46450, loss = 0.75 (991.2 examples/sec; 0.129 sec/batch)
2017-05-08 20:03:21.042547: step 46460, loss = 0.67 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-08 20:03:22.283743: step 46470, loss = 0.62 (1031.3 examples/sec; 0.124 sec/batch)
2017-05-08 20:03:23.540000: step 46480, loss = 0.75 (1018.9 examples/sec; 0.126 sec/batch)
2017-05-08 20:03:24.815994: step 46490, loss = 0.73 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 20:03:26.188551: step 46500, loss = 0.83 (932.6 examples/sec; 0.137 sec/batch)
2017-05-08 20:03:27.353892: step 46510, loss = 0.67 (1098.4 examples/sec; 0.117 sec/batch)
2017-05-08 20:03:28.612120: step 46520, loss = 0.95 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-08 20:03:29.873811: step 46530, loss = 0.68 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-08 20:03:31.165851: step 46540, loss = 0.78 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 20:03:32.458859: step 46550, loss = 0.70 (989.9 examples/sec; 0.129 sec/batch)
2017-05-08 20:03:33.771963: step 46560, loss = 0.76 (974.8 examples/sec; 0.131 sec/batch)
2017-05-08 20:03:35.060588: step 46570, loss = 0.84 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 20:03:36.331036: step 46580, loss = 0.74 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 20:03:37.650216: step 46590, loss = 0.78 (970.3 examples/sec; 0.132 sec/batch)
2017-05-08 20:03:39.020121: step 46600, loss = 0.62 (934.4 examples/sec; 0.137 sec/batch)
2017-05-08 20:03:40.204206: step 46610, loss = 0.73 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-08 20:03:41.475149: step 46620, loss = 0.71 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 20:03:42.745878: step 46630, loss = 0.67 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-08 20:03:43.997440: step 46640, loss = 0.70 (1022.7 examples/sec; 0.125 sec/batch)
2017-05-08 20:03:45.264701: step 46650, loss = 0.75 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-08 20:03:46.546487: step 46660, loss = 0.82 (998.6 examples/sec; 0.128 sec/batch)
201E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 952 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
7-05-08 20:03:47.808869: step 46670, loss = 0.67 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-08 20:03:49.085289: step 46680, loss = 0.72 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 20:03:50.383170: step 46690, loss = 0.57 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 20:03:51.742967: step 46700, loss = 0.77 (941.3 examples/sec; 0.136 sec/batch)
2017-05-08 20:03:52.910328: step 46710, loss = 0.79 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-08 20:03:54.210024: step 46720, loss = 0.71 (984.9 examples/sec; 0.130 sec/batch)
2017-05-08 20:03:55.430154: step 46730, loss = 0.82 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-08 20:03:56.744794: step 46740, loss = 0.86 (973.7 examples/sec; 0.131 sec/batch)
2017-05-08 20:03:58.011934: step 46750, loss = 0.85 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-08 20:03:59.271624: step 46760, loss = 0.74 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-08 20:04:00.547305: step 46770, loss = 0.74 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 20:04:01.811221: step 46780, loss = 0.62 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-08 20:04:03.066792: step 46790, loss = 0.69 (1019.5 examples/sec; 0.126 sec/batch)
2017-05-08 20:04:04.461015: step 46800, loss = 0.81 (918.1 examples/sec; 0.139 sec/batch)
2017-05-08 20:04:05.655007: step 46810, loss = 0.74 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-08 20:04:06.935259: step 46820, loss = 0.65 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 20:04:08.206740: step 46830, loss = 0.65 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 20:04:09.482131: step 46840, loss = 0.81 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 20:04:10.742238: step 46850, loss = 0.77 (1015.8 examples/sec; 0.126 sec/batch)
2017-05-08 20:04:11.985360: step 46860, loss = 0.67 (1029.7 examples/sec; 0.124 sec/batch)
2017-05-08 20:04:13.280717: step 46870, loss = 1.09 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 20:04:14.517053: step 46880, loss = 0.68 (1035.3 examples/sec; 0.124 sec/batch)
2017-05-08 20:04:15.801624: step 46890, loss = 0.69 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 20:04:17.181189: step 46900, loss = 0.78 (927.8 examples/sec; 0.138 sec/batch)
2017-05-08 20:04:18.365104: step 46910, loss = 0.73 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-08 20:04:19.646177: step 46920, loss = 0.81 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 20:04:20.913440: step 46930, loss = 0.83 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-08 20:04:22.190641: step 46940, loss = 0.91 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 20:04:23.448861: step 46950, loss = 0.80 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-08 20:04:24.721063: step 46960, loss = 0.79 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-08 20:04:25.981010: step 46970, loss = 0.60 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-08 20:04:27.243863: step 46980, loss = 0.74 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-08 20:04:28.504637: step 46990, loss = 0.84 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-08 20:04:29.865833: step 47000, loss = 0.81 (940.4 examples/sec; 0.136 sec/batch)
2017-05-08 20:04:31.043009: step 47010, loss = 0.64 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-08 20:04:32.317731: step 47020, loss = 0.66 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 20:04:33.601369: step 47030, loss = 0.73 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 20:04:34.888214: step 47040, loss = 0.77 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 20:04:36.139936: step 47050, loss = 0.86 (1022.6 examples/sec; 0.125 sec/batch)
2017-05-08 20:04:37.428175: step 47060, loss = 0.76 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 20:04:38.678499: step 47070, loss = 0.84 (1023.8 examples/sec; 0.125 sec/batch)
2017-05-08 20:04:39.945622: step 47080, loss = 0.64 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-08 20:04:41.209305: step 47090, loss = 0.88 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-08 20:04:42.600911: step 47100, loss = 0.56 (919.8 examples/sec; 0.139 sec/batch)
2017-05-08 20:04:43.784898: step 47110, loss = 0.71 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-08 20:04:45.091126: step 47120, loss = 0.95 (979.9 examples/sec; 0.131 sec/batch)
2017-05-08 20:04:46.414934: step 47130, loss = 0.65 (966.9 examples/sec; 0.132 sec/batch)
2017-05-08 20:04:47.668568: step 47140, loss = 0.71 (1021.0 examples/sec; 0.125 sec/batch)
2017-05-08 20:04:48.952597: step 47150, loss = 0.66 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 20:04:50.230403: step 47160, loss = 0.75 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 20:04:51.506220: step 47170, loss = 0.85 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 20:04:52.789085: step 47180, loss = 0.65 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 20:04:54.068033: step 47190, loss = 0.72 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 20:04:55.401868: step 47200, loss = 0.66 (959.6 examples/sec; 0.133 sec/batch)
2017-05-08 20:04:56.575674: step 47210, loss = 0.79 (1090.5 examples/sec; 0.117 sec/batch)
2017-05-08 20:04:57.838833: step 47220, loss = 0.66 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-08 20:04:59.112714: step 47230, loss = 0.83 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 20:05:00.374544: step 47240, loss = 0.71 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-08 20:05:01.628201: step 47250, loss = 0.80 (1021.0 examples/sec; 0.125 sec/batch)
2017-05-08 20:05:02.921802: step 47260, loss = 0.73 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 20:05:04.186589: step 47270, loss = 0.75 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-08 20:05:05.457517: step 47280, loss = 0.86 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 20:05:06.735653: step 47290, loss = 0.65 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 20:05:08.113211: step 47300, loss = 0.67 (929.2 examples/sec; 0.138 sec/batch)
2017-05-08 20:05:09.282737: step 47310, loss = 0.77 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-08 20:05:10.565777: step 47320, loss = 0.74 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 20:05:11.842939: step 47330, loss = 0.80 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 20:05:13.120972: step 47340, loss = 0.70 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 20:05:14.387027: step 47350, loss = 0.86 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 20:05:15.654785: step 47360, loss = 0.81 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 20:05:16.908369: step 47370, loss = 0.59 (1021.1 examples/sec; 0.125 sec/batch)
2017-05-08 20:05:18.183210: step 47380, loss = 0.65 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-08 20:05:19.455827: step 47390, loss = 0.89 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-08 20:05:20.844928: step 47400, loss = 0.65 (921.5 examples/sec; 0.139 sec/batch)
2017-05-08 20:05:22.030201: step 47410, loss = 0.71 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-08 20:05:23.334659: step 47420, loss = 0.57 (981.3 examples/sec; 0.130 sec/batch)
2017-05-08 20:05:24.623661: step 47430, loss = 0.67 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 20:05:25.911028: step 47440, loss = 0.86 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 20:05:27.183817: step 47450, loss = 0.75 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-08 20:05:28.449716: step 47460, loss = 0.74 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-08 20:05:29.732393: step 47470, loss = 0.64 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 20:05:31.019089: step 47480, loss = 0.67 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 20:05:32.319414: step 47490, loss = 0.57 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 20:05:33.706613: step 47500, loss = 0.71 (922.7 examples/sec; 0.139 sec/batch)
2017-05-08 20:05:34.911902: step 47510, loss = 0.71 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-08 20:05:36.210509: step 47520, loss = 0.62 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 20:05:37.494846: step 47530, loss = 0.83 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 20:05:38.755003: step 47540, loss = 0.79 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-08 20:05:40.015395: step 47550, loss = 0.68 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-08 20:05:41.276148: step 47560, loss = 0.86 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-08 20:05:42.557344: step 47570E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 973 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
, loss = 0.89 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 20:05:43.814039: step 47580, loss = 0.83 (1018.5 examples/sec; 0.126 sec/batch)
2017-05-08 20:05:45.069332: step 47590, loss = 0.73 (1019.7 examples/sec; 0.126 sec/batch)
2017-05-08 20:05:46.455408: step 47600, loss = 0.87 (923.5 examples/sec; 0.139 sec/batch)
2017-05-08 20:05:47.627535: step 47610, loss = 0.72 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-08 20:05:48.885227: step 47620, loss = 0.82 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-08 20:05:50.146671: step 47630, loss = 0.79 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-08 20:05:51.439571: step 47640, loss = 0.66 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 20:05:52.713860: step 47650, loss = 0.74 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 20:05:53.998022: step 47660, loss = 0.79 (996.8 examples/sec; 0.128 sec/batch)
2017-05-08 20:05:55.280260: step 47670, loss = 0.82 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 20:05:56.519787: step 47680, loss = 0.76 (1032.6 examples/sec; 0.124 sec/batch)
2017-05-08 20:05:57.790710: step 47690, loss = 0.86 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 20:05:59.191620: step 47700, loss = 0.73 (913.7 examples/sec; 0.140 sec/batch)
2017-05-08 20:06:00.419177: step 47710, loss = 0.94 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-08 20:06:01.668124: step 47720, loss = 0.68 (1024.9 examples/sec; 0.125 sec/batch)
2017-05-08 20:06:02.942280: step 47730, loss = 0.95 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 20:06:04.209824: step 47740, loss = 0.72 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 20:06:05.479910: step 47750, loss = 0.65 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-08 20:06:06.788943: step 47760, loss = 0.78 (977.8 examples/sec; 0.131 sec/batch)
2017-05-08 20:06:08.029574: step 47770, loss = 0.83 (1031.7 examples/sec; 0.124 sec/batch)
2017-05-08 20:06:09.297001: step 47780, loss = 0.67 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-08 20:06:10.568295: step 47790, loss = 0.73 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 20:06:11.967256: step 47800, loss = 0.79 (915.0 examples/sec; 0.140 sec/batch)
2017-05-08 20:06:13.140217: step 47810, loss = 0.73 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-08 20:06:14.406844: step 47820, loss = 0.64 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 20:06:15.668360: step 47830, loss = 0.72 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-08 20:06:16.928750: step 47840, loss = 0.70 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-08 20:06:18.239138: step 47850, loss = 0.78 (976.8 examples/sec; 0.131 sec/batch)
2017-05-08 20:06:19.529496: step 47860, loss = 0.72 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 20:06:20.805265: step 47870, loss = 0.77 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 20:06:22.065983: step 47880, loss = 0.88 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-08 20:06:23.323220: step 47890, loss = 0.83 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-08 20:06:24.678320: step 47900, loss = 0.72 (944.6 examples/sec; 0.136 sec/batch)
2017-05-08 20:06:25.899858: step 47910, loss = 0.82 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-08 20:06:27.158310: step 47920, loss = 0.64 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-08 20:06:28.413089: step 47930, loss = 0.93 (1020.1 examples/sec; 0.125 sec/batch)
2017-05-08 20:06:29.692081: step 47940, loss = 0.79 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 20:06:30.945179: step 47950, loss = 0.67 (1021.5 examples/sec; 0.125 sec/batch)
2017-05-08 20:06:32.213316: step 47960, loss = 0.87 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-08 20:06:33.497733: step 47970, loss = 0.78 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 20:06:34.770034: step 47980, loss = 0.63 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-08 20:06:36.025004: step 47990, loss = 0.76 (1019.9 examples/sec; 0.125 sec/batch)
2017-05-08 20:06:37.395089: step 48000, loss = 0.67 (934.3 examples/sec; 0.137 sec/batch)
2017-05-08 20:06:38.593759: step 48010, loss = 0.66 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-08 20:06:39.865682: step 48020, loss = 0.76 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-08 20:06:41.161838: step 48030, loss = 0.69 (987.6 examples/sec; 0.130 sec/batch)
2017-05-08 20:06:42.484831: step 48040, loss = 0.66 (967.5 examples/sec; 0.132 sec/batch)
2017-05-08 20:06:43.778144: step 48050, loss = 0.72 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 20:06:45.018639: step 48060, loss = 0.68 (1031.8 examples/sec; 0.124 sec/batch)
2017-05-08 20:06:46.282083: step 48070, loss = 0.71 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-08 20:06:47.536918: step 48080, loss = 0.98 (1020.1 examples/sec; 0.125 sec/batch)
2017-05-08 20:06:48.803419: step 48090, loss = 0.70 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-08 20:06:50.184969: step 48100, loss = 0.85 (926.5 examples/sec; 0.138 sec/batch)
2017-05-08 20:06:51.360849: step 48110, loss = 0.72 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-08 20:06:52.646358: step 48120, loss = 0.92 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 20:06:53.905127: step 48130, loss = 0.65 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-08 20:06:55.158721: step 48140, loss = 0.72 (1021.1 examples/sec; 0.125 sec/batch)
2017-05-08 20:06:56.418389: step 48150, loss = 0.67 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-08 20:06:57.718690: step 48160, loss = 0.60 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 20:06:58.975283: step 48170, loss = 0.67 (1018.6 examples/sec; 0.126 sec/batch)
2017-05-08 20:07:00.210683: step 48180, loss = 0.71 (1036.1 examples/sec; 0.124 sec/batch)
2017-05-08 20:07:01.476794: step 48190, loss = 0.65 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 20:07:02.819160: step 48200, loss = 0.66 (953.5 examples/sec; 0.134 sec/batch)
2017-05-08 20:07:03.992799: step 48210, loss = 0.65 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-08 20:07:05.266395: step 48220, loss = 0.68 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-08 20:07:06.529655: step 48230, loss = 0.84 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-08 20:07:07.786271: step 48240, loss = 0.72 (1018.6 examples/sec; 0.126 sec/batch)
2017-05-08 20:07:09.053266: step 48250, loss = 0.73 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-08 20:07:10.337294: step 48260, loss = 0.79 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 20:07:11.622268: step 48270, loss = 0.65 (996.1 examples/sec; 0.128 sec/batch)
2017-05-08 20:07:12.878961: step 48280, loss = 0.77 (1018.5 examples/sec; 0.126 sec/batch)
2017-05-08 20:07:14.149838: step 48290, loss = 0.56 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 20:07:15.541322: step 48300, loss = 0.82 (919.9 examples/sec; 0.139 sec/batch)
2017-05-08 20:07:16.726297: step 48310, loss = 0.66 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-08 20:07:18.012511: step 48320, loss = 0.70 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 20:07:19.273254: step 48330, loss = 0.68 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-08 20:07:20.532054: step 48340, loss = 0.84 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-08 20:07:21.796907: step 48350, loss = 0.64 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-08 20:07:23.088779: step 48360, loss = 0.93 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 20:07:24.353355: step 48370, loss = 0.96 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 20:07:25.635019: step 48380, loss = 0.78 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 20:07:26.896672: step 48390, loss = 0.98 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-08 20:07:28.271672: step 48400, loss = 0.78 (930.9 examples/sec; 0.137 sec/batch)
2017-05-08 20:07:29.429320: step 48410, loss = 0.96 (1105.7 examples/sec; 0.116 sec/batch)
2017-05-08 20:07:30.701657: step 48420, loss = 0.83 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-08 20:07:31.965725: step 48430, loss = 0.67 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-08 20:07:33.218179: step 48440, loss = 0.89 (1022.0 examples/sec; 0.125 sec/batch)
2017-05-08 20:07:34.498462: step 48450, loss = 0.67 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 20:07:35.771033: step 48460, loss = 0.64 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-08 20:07:37.048185: step 48470, loss = 0.69 (1002.3 examplesE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 993 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
/sec; 0.128 sec/batch)
2017-05-08 20:07:38.308604: step 48480, loss = 0.86 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-08 20:07:39.554264: step 48490, loss = 0.70 (1027.6 examples/sec; 0.125 sec/batch)
2017-05-08 20:07:40.906286: step 48500, loss = 0.69 (946.7 examples/sec; 0.135 sec/batch)
2017-05-08 20:07:42.084560: step 48510, loss = 0.82 (1086.3 examples/sec; 0.118 sec/batch)
2017-05-08 20:07:43.344719: step 48520, loss = 0.76 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-08 20:07:44.623086: step 48530, loss = 0.77 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 20:07:45.923552: step 48540, loss = 0.70 (984.3 examples/sec; 0.130 sec/batch)
2017-05-08 20:07:47.212619: step 48550, loss = 0.84 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 20:07:48.522489: step 48560, loss = 0.86 (977.2 examples/sec; 0.131 sec/batch)
2017-05-08 20:07:49.769267: step 48570, loss = 0.86 (1026.6 examples/sec; 0.125 sec/batch)
2017-05-08 20:07:51.049356: step 48580, loss = 0.72 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 20:07:52.321577: step 48590, loss = 0.68 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-08 20:07:53.668249: step 48600, loss = 0.63 (950.5 examples/sec; 0.135 sec/batch)
2017-05-08 20:07:54.855152: step 48610, loss = 0.85 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-08 20:07:56.126581: step 48620, loss = 0.67 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 20:07:57.399721: step 48630, loss = 0.67 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 20:07:58.665284: step 48640, loss = 0.66 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-08 20:07:59.932525: step 48650, loss = 0.62 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-08 20:08:01.193163: step 48660, loss = 0.76 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-08 20:08:02.453608: step 48670, loss = 0.61 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-08 20:08:03.700049: step 48680, loss = 0.87 (1026.9 examples/sec; 0.125 sec/batch)
2017-05-08 20:08:04.977734: step 48690, loss = 0.87 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 20:08:06.358041: step 48700, loss = 0.60 (927.3 examples/sec; 0.138 sec/batch)
2017-05-08 20:08:07.521877: step 48710, loss = 0.69 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-08 20:08:08.787012: step 48720, loss = 0.75 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-08 20:08:10.057533: step 48730, loss = 0.75 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 20:08:11.320640: step 48740, loss = 0.58 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-08 20:08:12.591140: step 48750, loss = 0.72 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 20:08:13.851412: step 48760, loss = 0.67 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-08 20:08:15.112737: step 48770, loss = 0.76 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-08 20:08:16.390563: step 48780, loss = 0.75 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 20:08:17.662899: step 48790, loss = 0.67 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-08 20:08:19.017220: step 48800, loss = 0.76 (945.1 examples/sec; 0.135 sec/batch)
2017-05-08 20:08:20.203481: step 48810, loss = 0.55 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-08 20:08:21.486621: step 48820, loss = 0.66 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 20:08:22.752673: step 48830, loss = 0.69 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 20:08:24.004126: step 48840, loss = 0.87 (1022.8 examples/sec; 0.125 sec/batch)
2017-05-08 20:08:25.270264: step 48850, loss = 0.65 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 20:08:26.534793: step 48860, loss = 0.72 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 20:08:27.795211: step 48870, loss = 0.63 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-08 20:08:29.068362: step 48880, loss = 0.67 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 20:08:30.378900: step 48890, loss = 0.56 (976.7 examples/sec; 0.131 sec/batch)
2017-05-08 20:08:31.724877: step 48900, loss = 0.58 (951.0 examples/sec; 0.135 sec/batch)
2017-05-08 20:08:32.900866: step 48910, loss = 0.78 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-08 20:08:34.202115: step 48920, loss = 0.77 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 20:08:35.505354: step 48930, loss = 0.75 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 20:08:36.769966: step 48940, loss = 0.66 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 20:08:38.032641: step 48950, loss = 0.69 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-08 20:08:39.317712: step 48960, loss = 0.78 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 20:08:40.578875: step 48970, loss = 0.83 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-08 20:08:41.842731: step 48980, loss = 0.76 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-08 20:08:43.119073: step 48990, loss = 0.64 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 20:08:44.498914: step 49000, loss = 0.67 (927.6 examples/sec; 0.138 sec/batch)
2017-05-08 20:08:45.760285: step 49010, loss = 0.73 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-08 20:08:46.990881: step 49020, loss = 0.65 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-08 20:08:48.253037: step 49030, loss = 0.88 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-08 20:08:49.525662: step 49040, loss = 0.79 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-08 20:08:50.792488: step 49050, loss = 0.85 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-08 20:08:52.081699: step 49060, loss = 0.71 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 20:08:53.322794: step 49070, loss = 0.88 (1031.3 examples/sec; 0.124 sec/batch)
2017-05-08 20:08:54.600297: step 49080, loss = 0.70 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 20:08:55.865404: step 49090, loss = 0.78 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-08 20:08:57.204767: step 49100, loss = 0.74 (955.7 examples/sec; 0.134 sec/batch)
2017-05-08 20:08:58.406518: step 49110, loss = 0.79 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-08 20:08:59.687192: step 49120, loss = 0.82 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 20:09:00.957504: step 49130, loss = 0.73 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-08 20:09:02.234725: step 49140, loss = 0.87 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 20:09:03.524995: step 49150, loss = 0.70 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 20:09:04.811224: step 49160, loss = 0.72 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 20:09:06.069128: step 49170, loss = 0.76 (1017.6 examples/sec; 0.126 sec/batch)
2017-05-08 20:09:07.345639: step 49180, loss = 0.69 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 20:09:08.605037: step 49190, loss = 0.68 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-08 20:09:09.947483: step 49200, loss = 0.72 (953.5 examples/sec; 0.134 sec/batch)
2017-05-08 20:09:11.134891: step 49210, loss = 0.78 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-08 20:09:12.410257: step 49220, loss = 0.67 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 20:09:13.669744: step 49230, loss = 0.79 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-08 20:09:14.972123: step 49240, loss = 0.66 (982.8 examples/sec; 0.130 sec/batch)
2017-05-08 20:09:16.232067: step 49250, loss = 0.77 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-08 20:09:17.495304: step 49260, loss = 0.73 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-08 20:09:18.798506: step 49270, loss = 0.73 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 20:09:20.094486: step 49280, loss = 0.80 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 20:09:21.383548: step 49290, loss = 0.72 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 20:09:22.767397: step 49300, loss = 0.87 (924.9 examples/sec; 0.138 sec/batch)
2017-05-08 20:09:23.934763: step 49310, loss = 0.78 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-08 20:09:25.228110: step 49320, loss = 0.82 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 20:09:26.546831: step 49330, loss = 0.69 (970.6 examples/sec; 0.132 sec/batch)
2017-05-08 20:09:27.825202: step 49340, loss = 0.68 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 20:09:29.096295: step 49350, loss = 0.67 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 20:09:30.373523: step 49360, loss = 0.61 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 20:09:31.624915: step 49370, loss = 0.80 (1022.9 examples/sec; 0.125 sec/batch)
2017-05-08 20:09:32.892906: step 49380, loss = 0.60 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-08 20:09:34.167094: step 49390, loss = 0.78 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 20:09:35.567703: step 49400, loss = 0.72 (913.9 examples/sec; 0.140 sec/batch)
2017-05-08 20:09:36.740570: step 49410, loss = 0.70 (1091.3 examples/sec; 0.117 sec/batch)
2017-05-08 20:09:38.013960: step 49420, loss = 0.86 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-08 20:09:39.284109: step 49430, loss = 0.63 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-08 20:09:40.546795: step 49440, loss = 0.68 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-08 20:09:41.804547: step 49450, loss = 0.70 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-08 20:09:43.122022: step 49460, loss = 0.73 (971.6 examples/sec; 0.132 sec/batch)
2017-05-08 20:09:44.429698: step 49470, loss = 0.67 (978.8 examples/sec; 0.131 sec/batch)
2017-05-08 20:09:45.702323: step 49480, loss = 0.80 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-08 20:09:46.981862: step 49490, loss = 0.62 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 20:09:48.351016: step 49500, loss = 0.55 (934.9 examples/sec; 0.137 sec/batch)
2017-05-08 20:09:49.519269: step 49510, loss = 0.78 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-08 20:09:50.829964: step 49520, loss = 0.82 (976.6 examples/sec; 0.131 sec/batch)
2017-05-08 20:09:52.112014: step 49530, loss = 0.92 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 20:09:53.378713: step 49540, loss = 0.80 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-08 20:09:54.649974: step 49550, loss = 0.65 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 20:09:55.921897: step 49560, loss = 0.69 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-08 20:09:57.190190: step 49570, loss = 0.79 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-08 20:09:58.446916: step 49580, loss = 0.79 (1018.5 examples/sec; 0.126 sec/batch)
2017-05-08 20:09:59.696348: step 49590, loss = 0.77 (1024.5 examples/sec; 0.125 sec/batch)
2017-05-08 20:10:01.107789: step 49600, loss = 0.83 (906.9 examples/sec; 0.141 sec/batch)
2017-05-08 20:10:02.292166: step 49610, loss = 0.74 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-08 20:10:03.583306: step 49620, loss = 0.72 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 20:10:04.832378: step 49630, loss = 0.70 (1024.8 examples/sec; 0.125 sec/batch)
2017-05-08 20:10:06.081984: step 49640, loss = 0.81 (1024.3 examples/sec; 0.125 sec/batch)
2017-05-08 20:10:07.349402: step 49650, loss = 0.76 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-08 20:10:08.615881: step 49660, loss = 0.83 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-08 20:10:09.882884: step 49670, loss = 0.74 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-08 20:10:11.183275: step 49680, loss = 0.81 (984.3 examples/sec; 0.130 sec/batch)
2017-05-08 20:10:12.417963: step 49690, loss = 0.64 (1036.7 examples/sec; 0.123 sec/batch)
2017-05-08 20:10:13.757383: step 49700, loss = 0.73 (955.6 examples/sec; 0.134 sec/batch)
2017-05-08 20:10:14.935760: step 49710, loss = 0.70 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-08 20:10:16.194062: step 49720, loss = 0.78 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-08 20:10:17.482469: step 49730, loss = 0.82 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 20:10:18.749021: step 49740, loss = 0.71 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 20:10:20.006492: step 49750, loss = 0.60 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-08 20:10:21.271703: step 49760, loss = 0.90 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-08 20:10:22.534617: step 49770, loss = 0.68 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-08 20:10:23.818137: step 49780, loss = 0.78 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 20:10:25.111763: step 49790, loss = 0.80 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 20:10:26.440433: step 49800, loss = 0.76 (963.4 examples/sec; 0.133 sec/batch)
2017-05-08 20:10:27.631094: step 49810, loss = 0.80 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-08 20:10:28.930855: step 49820, loss = 0.75 (984.8 examples/sec; 0.130 sec/batch)
2017-05-08 20:10:30.208E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1013 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1016 events to /tmp/cifar10_train/events.out.tfevents.1494282256.GHC31.GHC.ANDREW.CMU.EDU
605: step 49830, loss = 0.67 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 20:10:31.469490: step 49840, loss = 0.62 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-08 20:10:32.703162: step 49850, loss = 0.69 (1037.5 examples/sec; 0.123 sec/batch)
2017-05-08 20:10:33.983728: step 49860, loss = 0.77 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 20:10:35.248214: step 49870, loss = 0.91 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-08 20:10:36.507833: step 49880, loss = 0.71 (1016.2 examples/sec; 0.126 sec/batch)
2017-05-08 20:10:37.793817: step 49890, loss = 0.72 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 20:10:39.181983: step 49900, loss = 0.60 (922.1 examples/sec; 0.139 sec/batch)
2017-05-08 20:10:40.422821: step 49910, loss = 0.70 (1031.6 examples/sec; 0.124 sec/batch)
2017-05-08 20:10:41.669976: step 49920, loss = 0.90 (1026.3 examples/sec; 0.125 sec/batch)
2017-05-08 20:10:42.950605: step 49930, loss = 0.78 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 20:10:44.231245: step 49940, loss = 0.74 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 20:10:45.475300: step 49950, loss = 0.75 (1028.9 examples/sec; 0.124 sec/batch)
2017-05-08 20:10:46.738705: step 49960, loss = 0.78 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-08 20:10:48.017153: step 49970, loss = 0.66 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 20:10:49.274227: step 49980, loss = 0.69 (1018.2 examples/sec; 0.126 sec/batch)
2017-05-08 20:10:50.558948: step 49990, loss = 0.77 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 20:10:51.921245: step 50000, loss = 0.75 (939.6 examples/sec; 0.136 sec/batch)
--- 6395.73741508 seconds ---
