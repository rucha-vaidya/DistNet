I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.7335
pciBusID 0000:04:00.0
Total memory: 7.92GiB
Free memory: 5.29GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x4384560
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: 
name: Quadro K620
major: 5 minor: 0 memoryClockRate (GHz) 1.124
pciBusID 0000:03:00.0
Total memory: 1.95GiB
Free memory: 1.85GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:04:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:962] Ignoring gpu device (device: 1, name: Quadro K620, pci bus id: 0000:03:00.0) with Cuda multiprocessor count: 3. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.
Connecting to port  17001
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Received variable values from ps
2017-05-09 00:34:57.406263: step 0, loss = 4.67 (76.8 examples/sec; 1.667 sec/batch)
2017-05-09 00:34:58.308738: step 10, loss = 4.62 (1418.3 examples/sec; 0.090 sec/batch)
2017-05-09 00:34:59.474537: step 20, loss = 4.52 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-09 00:35:00.783123: step 30, loss = 4.58 (978.2 examples/sec; 0.131 sec/batch)
2017-05-09 00:35:02.091135: step 40, loss = 4.50 (978.6 examples/sec; 0.131 sec/batch)
2017-05-09 00:35:03.376072: step 50, loss = 4.31 (996.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:35:04.656588: step 60, loss = 4.41 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 00:35:05.925509: step 70, loss = 4.27 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-09 00:35:07.240659: step 80, loss = 4.15 (973.3 examples/sec; 0.132 sec/batch)
2017-05-09 00:35:08.545823: step 90, loss = 4.21 (980.7 examples/sec; 0.131 sec/batch)
2017-05-09 00:35:09.937802: step 100, loss = 4.01 (919.5 examples/sec; 0.139 sec/batch)
2017-05-09 00:35:11.085713: step 110, loss = 4.11 (1115.1 examples/sec; 0.115 sec/batch)
2017-05-09 00:35:12.377120: step 120, loss = 3.87 (991.2 examples/sec; 0.129 sec/batch)
2017-05-09 00:35:13.655655: step 130, loss = 4.26 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:35:14.927972: step 140, loss = 3.89 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 00:35:16.205644: step 150, loss = 3.84 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-09 00:35:17.488666: step 160, loss = 4.05 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 00:35:18.815832: step 170, loss = 3.85 (964.5 examples/sec; 0.133 sec/batch)
2017-05-09 00:35:20.101647: step 180, loss = 3.66 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 00:35:21.382214: step 190, loss = 3.74 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 00:35:22.783888: step 200, loss = 3.61 (913.2 examples/sec; 0.140 sec/batch)
2017-05-09 00:35:23.969530: step 210, loss = 3.63 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-09 00:35:25.257211: step 220, loss = 3.56 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:35:26.551216: step 230, loss = 3.51 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 00:35:27.877769: step 240, loss = 3.87 (964.9 examples/sec; 0.133 sec/batch)
2017-05-09 00:35:29.168701: step 250, loss = 3.55 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 00:35:30.438362: step 260, loss = 3.32 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-09 00:35:31.751729: step 270, loss = 3.43 (974.6 examples/sec; 0.131 sec/batch)
2017-05-09 00:35:33.058078: step 280, loss = 3.40 (979.8 examples/sec; 0.131 sec/batch)
2017-05-09 00:35:34.364983: step 290, loss = 3.47 (979.4 examples/sec; 0.131 sec/batch)
2017-05-09 00:35:35.764604: step 300, loss = 3.20 (914.5 examples/sec; 0.140 sec/batch)
2017-05-09 00:35:36.943160: step 310, loss = 3.61 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-09 00:35:38.223040: step 320, loss = 3.28 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:35:39.490600: step 330, loss = 3.26 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-09 00:35:40.756003: step 340, loss = 3.21 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-09 00:35:42.042344: step 350, loss = 3.63 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 00:35:43.330095: step 360, loss = 3.07 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:35:44.611764: step 370, loss = 3.17 (998.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:35:45.889227: step 380, loss = 3.14 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:35:47.192795: step 390, loss = 3.20 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 00:35:48.587809: step 400, loss = 2.90 (917.6 examples/sec; 0.140 sec/batch)
2017-05-09 00:35:49.758232: step 410, loss = 3.05 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-09 00:35:51.070884: step 420, loss = 3.10 (975.1 examples/sec; 0.131 sec/batch)
2017-05-09 00:35:52.369637: step 430, loss = 3.02 (985.6 examples/sec; 0.130 sec/batch)
2017-05-09 00:35:53.655916: step 440, loss = 3.13 (995.1 examples/seE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 23 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
c; 0.129 sec/batch)
2017-05-09 00:35:54.956176: step 450, loss = 2.84 (984.4 examples/sec; 0.130 sec/batch)
2017-05-09 00:35:56.249927: step 460, loss = 2.95 (989.4 examples/sec; 0.129 sec/batch)
2017-05-09 00:35:57.500135: step 470, loss = 2.86 (1023.8 examples/sec; 0.125 sec/batch)
2017-05-09 00:35:58.776731: step 480, loss = 2.92 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:36:00.050967: step 490, loss = 3.08 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-09 00:36:01.462437: step 500, loss = 2.95 (906.9 examples/sec; 0.141 sec/batch)
2017-05-09 00:36:02.619101: step 510, loss = 2.75 (1106.6 examples/sec; 0.116 sec/batch)
2017-05-09 00:36:03.900348: step 520, loss = 2.62 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:36:05.172843: step 530, loss = 2.83 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-09 00:36:06.421638: step 540, loss = 2.70 (1025.0 examples/sec; 0.125 sec/batch)
2017-05-09 00:36:07.735896: step 550, loss = 2.79 (973.9 examples/sec; 0.131 sec/batch)
2017-05-09 00:36:09.061600: step 560, loss = 2.70 (965.5 examples/sec; 0.133 sec/batch)
2017-05-09 00:36:10.352192: step 570, loss = 2.49 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:36:11.649523: step 580, loss = 2.81 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 00:36:12.946578: step 590, loss = 2.49 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 00:36:14.358233: step 600, loss = 2.56 (906.7 examples/sec; 0.141 sec/batch)
2017-05-09 00:36:15.564896: step 610, loss = 2.53 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-09 00:36:16.855020: step 620, loss = 2.55 (992.1 examples/sec; 0.129 sec/batch)
2017-05-09 00:36:18.127738: step 630, loss = 2.51 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-09 00:36:19.419008: step 640, loss = 2.39 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 00:36:20.733091: step 650, loss = 2.46 (974.1 examples/sec; 0.131 sec/batch)
2017-05-09 00:36:22.054146: step 660, loss = 2.40 (968.9 examples/sec; 0.132 sec/batch)
2017-05-09 00:36:23.339192: step 670, loss = 2.53 (996.1 examples/sec; 0.129 sec/batch)
2017-05-09 00:36:24.633212: step 680, loss = 2.32 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 00:36:25.931245: step 690, loss = 2.57 (986.1 examples/sec; 0.130 sec/batch)
2017-05-09 00:36:27.340267: step 700, loss = 2.52 (908.4 examples/sec; 0.141 sec/batch)
2017-05-09 00:36:28.513603: step 710, loss = 2.22 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-09 00:36:29.806986: step 720, loss = 2.24 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 00:36:31.136175: step 730, loss = 2.45 (963.0 examples/sec; 0.133 sec/batch)
2017-05-09 00:36:32.457528: step 740, loss = 2.27 (968.7 examples/sec; 0.132 sec/batch)
2017-05-09 00:36:33.729648: step 750, loss = 2.37 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-09 00:36:35.017573: step 760, loss = 2.66 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:36:36.301207: step 770, loss = 2.22 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:36:37.579269: step 780, loss = 2.29 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:36:38.849502: step 790, loss = 2.15 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-09 00:36:40.235935: step 800, loss = 2.32 (923.2 examples/sec; 0.139 sec/batch)
2017-05-09 00:36:41.401965: step 810, loss = 2.02 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-09 00:36:42.707723: step 820, loss = 2.17 (980.3 examples/sec; 0.131 sec/batch)
2017-05-09 00:36:43.996834: step 830, loss = 2.16 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:36:45.287847: step 840, loss = 2.20 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 00:36:46.575469: step 850, loss = 2.20 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 00:36:47.872128: step 860, loss = 2.23 (987.2 examples/sec; 0.130 sec/batch)
2017-05-09 00:36:49.185235: step 870, loss = 2.05 (974.8 examples/sec; 0.131 sec/batch)
2017-05-09 00:36:50.462409: step 880, loss = 1.99 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:36:51.744143: step 890, loss = 2.27 (998.6 examples/sec; 0.128 sec/batch)
2017-05-09 00:36:53.149556: step 900, loss = 1.95 (910.8 examples/sec; 0.141 sec/batch)
2017-05-09 00:36:54.315504: step 910, loss = 2.13 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-09 00:36:55.605239: step 920, loss = 2.20 (992.5 examples/sec; 0.129 sec/batch)
2017-05-09 00:36:56.908465: step 930, loss = 1.97 (982.2 examples/sec; 0.130 sec/batch)
2017-05-09 00:36:58.217223: step 940, loss = 2.10 (978.0 examples/sec; 0.131 sec/batch)
2017-05-09 00:36:59.512256: step 950, loss = 2.10 (988.4 examples/sec; 0.130 sec/batch)
2017-05-09 00:37:00.794639: step 960, loss = 2.01 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:37:02.100949: step 970, loss = 2.00 (979.9 examples/sec; 0.131 sec/batch)
2017-05-09 00:37:03.407628: step 980, loss = 1.94 (979.6 examples/sec; 0.131 sec/batch)
2017-05-09 00:37:04.723493: step 990, loss = 2.29 (972.7 examples/sec; 0.132 sec/batch)
2017-05-09 00:37:06.142647: step 1000, loss = 2.09 (901.9 examples/sec; 0.142 sec/batch)
2017-05-09 00:37:07.328203: step 1010, loss = 1.94 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-09 00:37:08.618850: step 1020, loss = 2.19 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:37:09.943563: step 1030, loss = 1.70 (966.2 examples/sec; 0.132 sec/batch)
2017-05-09 00:37:11.214064: step 1040, loss = 1.96 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-09 00:37:12.561083: step 1050, loss = 1.96 (950.2 examples/sec; 0.135 sec/batch)
2017-05-09 00:37:13.869752: step 1060, loss = 1.79 (978.1 examples/sec; 0.131 sec/batch)
2017-05-09 00:37:15.181786: step 1070, loss = 1.76 (975.6 examples/sec; 0.131 sec/batch)
2017-05-09 00:37:16.477799: step 1080, loss = 1.84 (987.6 examples/sec; 0.130 sec/batch)
2017-05-09 00:37:17.786598: step 1090, loss = 2.07 (978.0 examples/sec; 0.131 sec/batch)
2017-05-09 00:37:19.194285: step 1100, loss = 1.83 (909.3 examples/sec; 0.141 sec/batch)
2017-05-09 00:37:20.370816: step 1110, loss = 1.91 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-09 00:37:21.680591: step 1120, loss = 1.71 (977.3 examples/sec; 0.131 sec/batch)
2017-05-09 00:37:22.953716: step 1130, loss = 1.77 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-09 00:37:24.278769: step 1140, loss = 1.79 (966.0 examples/sec; 0.133 sec/batch)
2017-05-09 00:37:25.567285: step 1150, loss = 1.91 (993.4 examples/sec; 0.129 sec/batch)
2017-05-09 00:37:26.910810: step 1160, loss = 1.66 (952.7 examples/sec; 0.134 sec/batch)
2017-05-09 00:37:28.247582: step 1170, loss = 1.88 (957.5 examples/sec; 0.134 sec/batch)
2017-05-09 00:37:29.568262: step 1180, loss = 1.77 (969.2 examples/sec; 0.132 sec/batch)
2017-05-09 00:37:30.867205: step 1190, loss = 1.81 (985.4 examples/sec; 0.130 sec/batch)
2017-05-09 00:37:32.259837: step 1200, loss = 1.68 (919.1 examples/sec; 0.139 sec/batch)
2017-05-09 00:37:33.444584: step 1210, loss = 1.71 (1080.4 examples/sec; 0.118 sec/batch)
2017-05-09 00:37:34.756736: step 1220, loss = 1.91 (975.5 examples/sec; 0.131 sec/batch)
2017-05-09 00:37:36.054940: step 1230, loss = 1.85 (986.0 examples/sec; 0.130 sec/batch)
2017-05-09 00:37:37.366466: step 1240, loss = 1.91 (976.0 examples/sec; 0.131 sec/batch)
2017-05-09 00:37:38.668933: step 1250, loss = 1.62 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 00:37:40.004665: step 1260, loss = 1.88 (958.3 examples/sec; 0.134 sec/batch)
2017-05-09 00:37:41.336468: step 1270, loss = 1.69 (961.1 examples/sec; 0.133 sec/batch)
2017-05-09 00:37:42.693899: step 1280, loss = 1.80 (943.0 examples/sec; 0.136 sec/batch)
2017-05-09 00:37:43.995947: step 1290, loss = 1.66 (983.1 examples/sec; 0.130 sec/batch)
2017-05-09 00:37:45.419027: step 1300, loss = 1.75 (899.5 examples/sec; 0.142 sec/batch)
2017-05-09 00:37:46.633046: step 1310, loss = 1.88 (1054.3 examples/sec; 0.121 sec/batch)
2017-05-09 00:37:47.926450: step 1320, loss = 1.60 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 00:37:49.232457: step 1330, loss = 1.80 (980.1 examples/sec; 0.131 sec/batch)
2017-05-09 00:37:50.529432: step 1340, loss = 1.59 (986.9 examples/sec; 0.130 sec/batch)
2017-05-09 00:37:51.807353: step 1350, loss = 1.46 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:37:53.082301: step 1360, loss = 1.67 (1003.9 examples/sec; 0.127 sec/batch)
2017-05-09 00:37:54.391000: step 1370, loss = 1.70 (978.1 examples/sec; 0.131 sec/batch)
2017-05-09 00:37:55.717602: step 1380, loss = 1.82 (964.9 examples/sec; 0.133 sec/batch)
2017-05-09 00:37:57.040066: step 1390, loss = 1.77 (967.9 examples/sec; 0.132 sec/batch)
2017-05-09 00:37:58.523120: step 1400, loss = 1.55 (863.1 examples/sec; 0.148 sec/batch)
2017-05-09 00:37:59.706827: step 1410, loss = 1.52 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-09 00:38:01.050535: step 1420, loss = 1.66 (952.6 examples/sec; 0.134 sec/batch)
2017-05-09 00:38:02.349159: step 1430, loss = 1.48 (985.7 examples/sec; 0.130 sec/batch)
2017-05-09 00:38:03.666177: step 1440, loss = 1.69 (971.9 examples/sec; 0.132 sec/batch)
2017-05-09 00:38:04.976648: step 1450, loss = 1.75 (976.8 examples/sec; 0.131 sec/batch)
2017-05-09 00:38:06.293681: step 1460, loss = 1.47 (971.9 examples/sec; 0.132 sec/batch)
2017-05-09 00:38:07.626825: step 1470, loss = 1.72 (960.1 examples/sec; 0.133 sec/batch)
2017-05-09 00:38:08.984829: step 1480, loss = 1.94 (942.6 examples/sec; 0.136 sec/batch)
2017-05-09 00:38:10.300883: step 1490, loss = 1.63 (972.6 examples/sec; 0.132 sec/batch)
2017-05-09 00:38:11.733932: step 1500, loss = 1.79 (893.2 examples/sec; 0.143 sec/batch)
2017-05-09 00:38:13.031022: step 1510, loss = 1.50 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 00:38:14.347158: step 1520, loss = 1.55 (972.5 examples/sec; 0.132 sec/batch)
2017-05-09 00:38:15.676723: step 1530, loss = 1.41 (962.7 examples/sec; 0.133 sec/batch)
2017-05-09 00:38:17.003676: step 1540, loss = 1.76 (964.6 examples/sec; 0.133 sec/batch)
2017-05-09 00:38:18.327433: step 1550, loss = 1.36 (966.9 examples/sec; 0.132 sec/batch)
2017-05-09 00:38:19.657133: step 1560, loss = 1.63 (962.6 examples/sec; 0.133 sec/batch)
2017-05-09 00:38:20.994990: step 1570, loss = 1.60 (956.8 examples/sec; 0.134 sec/batch)
2017-05-09 00:38:22.291919: step 1580, loss = 1.66 (986.9 examples/sec; 0.130 sec/batch)
2017-05-09 00:38:23.609500: step 1590, loss = 1.48 (971.5 examples/sec; 0.132 sec/batch)
2017-05-09 00:38:25.080075: step 1600, loss = 1.39 (870.4 examples/sec; 0.147 sec/batch)
2017-05-09 00:38:26.236735: step 1610, loss = 1.29 (1106.6 examples/sec; 0.116 sec/batch)
2017-05-09 00:38:27.520829: step 1620, loss = 1.57 (996.8 examples/sec; 0.128 sec/batch)
2017-05-09 00:38:28.791127: step 1630, loss = 1.32 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-09 00:38:30.093958: step 1640, loss = 1.57 (982.5 examples/sec; 0.130 sec/batch)
2017-05-09 00:38:31.421763: step 1650, loss = 1.35 (964.0 examples/sec; 0.133 sec/batch)
2017-05-09 00:38:32.732161: step 1660, loss = 1.50 (976.8 examples/sec; 0.131 sec/batch)
2017-05-09 00:38:34.060234: step 1670, loss = 1.52 (963.8 examples/sec; 0.133 sec/batch)
2017-05-09 00:38:35.390010: step 1680, loss = 1.49 (962.6 examples/sec; 0.133 sec/batch)
2017-05-09 00:38:36.719022: step 1690, loss = 1.41 (963.1 examples/sec; 0.133 sec/batch)
2017-05-09 00:38:38.152867: step 1700, loss = 1.47 (892.7 examples/sec; 0.143 sec/batch)
2017-05-09 00:38:39.381644: step 1710, loss = 1.41 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-09 00:38:40.679119: step 1720, loss = 1.43 (986.5 examples/sec; 0.130 sec/batch)
2017-05-09 00:38:41.981656: step 1730, loss = 1.31 (982.7 examples/sec; 0.130 sec/batch)
2017-05-09 00:38:43.310284: step 1740, loss = 1.63 (963.4 examples/sec; 0.133 sec/batch)
2017-05-09 00:38:44.624990: step 1750, loss = 1.46 (973.6 examples/sec; 0.131 sec/batch)
2017-05-09 00:38:45.947185: step 1760, loss = 1.42 (968.1 examples/sec; 0.132 sec/batch)
2017-05-09 00:38:47.280540: step 1770, loss = 1.57 (960.0 examples/sec; 0.133 sec/batch)
2017-05-09 00:38:48.612824: step 1780, loss = 1.33 (960.8 examples/sec; 0.133 sec/batch)
2017-05-09 00:38:49.935043: step 1790, loss = 1.55 (968.1 examples/sec; 0.132 sec/batch)
2017-05-09 00:38:51.323525: step 1800, loss = 1.55 (921.9 examples/sec; 0.139 sec/batch)
2017-05-09 00:38:52.568162: step 1810, loss = 1.25 (1028.4 examples/sec; 0.124 sec/batch)
2017-05-09 00:38:53.885602: step 1820, loss = 1.33 (971.6 examples/sec; 0.132 sec/batch)
2017-05-09 00E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 43 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
:38:55.175393: step 1830, loss = 1.62 (992.4 examples/sec; 0.129 sec/batch)
2017-05-09 00:38:56.497828: step 1840, loss = 1.35 (967.9 examples/sec; 0.132 sec/batch)
2017-05-09 00:38:57.814896: step 1850, loss = 1.78 (971.9 examples/sec; 0.132 sec/batch)
2017-05-09 00:38:59.092846: step 1860, loss = 1.74 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 00:39:00.366434: step 1870, loss = 1.36 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-09 00:39:01.689792: step 1880, loss = 1.34 (967.2 examples/sec; 0.132 sec/batch)
2017-05-09 00:39:03.006680: step 1890, loss = 1.42 (972.0 examples/sec; 0.132 sec/batch)
2017-05-09 00:39:04.399603: step 1900, loss = 1.32 (918.9 examples/sec; 0.139 sec/batch)
2017-05-09 00:39:05.578529: step 1910, loss = 1.52 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-09 00:39:06.858401: step 1920, loss = 1.13 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:39:08.116403: step 1930, loss = 1.54 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-09 00:39:09.422639: step 1940, loss = 1.20 (979.9 examples/sec; 0.131 sec/batch)
2017-05-09 00:39:10.689408: step 1950, loss = 1.19 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-09 00:39:11.991129: step 1960, loss = 1.55 (983.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:39:13.278661: step 1970, loss = 1.41 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 00:39:14.577697: step 1980, loss = 1.62 (985.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:39:15.864245: step 1990, loss = 1.19 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:39:17.243522: step 2000, loss = 1.25 (928.0 examples/sec; 0.138 sec/batch)
2017-05-09 00:39:18.432286: step 2010, loss = 1.29 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-09 00:39:19.714888: step 2020, loss = 1.56 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:39:20.998586: step 2030, loss = 1.25 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:39:22.277541: step 2040, loss = 1.50 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-09 00:39:23.573199: step 2050, loss = 1.23 (987.9 examples/sec; 0.130 sec/batch)
2017-05-09 00:39:24.872236: step 2060, loss = 1.38 (985.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:39:26.186329: step 2070, loss = 1.71 (974.1 examples/sec; 0.131 sec/batch)
2017-05-09 00:39:27.491162: step 2080, loss = 1.37 (981.0 examples/sec; 0.130 sec/batch)
2017-05-09 00:39:28.792069: step 2090, loss = 1.40 (983.9 examples/sec; 0.130 sec/batch)
2017-05-09 00:39:30.142913: step 2100, loss = 1.25 (947.6 examples/sec; 0.135 sec/batch)
2017-05-09 00:39:31.341033: step 2110, loss = 1.29 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-09 00:39:32.621427: step 2120, loss = 1.22 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:39:33.910579: step 2130, loss = 1.47 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:39:35.207666: step 2140, loss = 1.09 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 00:39:36.503484: step 2150, loss = 1.31 (987.8 examples/sec; 0.130 sec/batch)
2017-05-09 00:39:37.799268: step 2160, loss = 1.38 (987.8 examples/sec; 0.130 sec/batch)
2017-05-09 00:39:39.096307: step 2170, loss = 1.20 (986.9 examples/sec; 0.130 sec/batch)
2017-05-09 00:39:40.407143: step 2180, loss = 1.32 (976.5 examples/sec; 0.131 sec/batch)
2017-05-09 00:39:41.690158: step 2190, loss = 1.20 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 00:39:43.069179: step 2200, loss = 1.29 (928.2 examples/sec; 0.138 sec/batch)
2017-05-09 00:39:44.262539: step 2210, loss = 1.32 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-09 00:39:45.545251: step 2220, loss = 1.24 (997.9 examples/sec; 0.128 sec/batch)
2017-05-09 00:39:46.862474: step 2230, loss = 1.69 (971.7 examples/sec; 0.132 sec/batch)
2017-05-09 00:39:48.155693: step 2240, loss = 1.45 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:39:49.455630: step 2250, loss = 1.15 (984.7 examples/sec; 0.130 sec/batch)
2017-05-09 00:39:50.766210: step 2260, loss = 1.35 (976.7 examples/sec; 0.131 sec/batch)
2017-05-09 00:39:52.058465: step 2270, loss = 1.10 (990.5 examples/sec; 0.129 sec/batch)
2017-05-09 00:39:53.339278: step 2280, loss = 1.22 (999.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:39:54.610024: step 2290, loss = 1.16 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-09 00:39:56.006648: step 2300, loss = 1.45 (916.5 examples/sec; 0.140 sec/batch)
2017-05-09 00:39:57.216697: step 2310, loss = 1.27 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-09 00:39:58.540984: step 2320, loss = 1.30 (966.6 examples/sec; 0.132 sec/batch)
2017-05-09 00:39:59.826364: step 2330, loss = 1.35 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:40:01.116676: step 2340, loss = 1.21 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:40:02.420077: step 2350, loss = 1.37 (982.0 examples/sec; 0.130 sec/batch)
2017-05-09 00:40:03.705540: step 2360, loss = 1.17 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 00:40:05.005622: step 2370, loss = 1.76 (984.6 examples/sec; 0.130 sec/batch)
2017-05-09 00:40:06.282780: step 2380, loss = 1.72 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:40:07.573620: step 2390, loss = 1.12 (991.6 examples/sec; 0.129 sec/batch)
2017-05-09 00:40:08.945090: step 2400, loss = 1.23 (933.3 examples/sec; 0.137 sec/batch)
2017-05-09 00:40:10.108546: step 2410, loss = 1.17 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-09 00:40:11.400077: step 2420, loss = 1.34 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 00:40:12.703084: step 2430, loss = 1.26 (982.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:40:13.984007: step 2440, loss = 1.24 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 00:40:15.272302: step 2450, loss = 1.44 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 00:40:16.542183: step 2460, loss = 1.15 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-09 00:40:17.818539: step 2470, loss = 1.22 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-09 00:40:19.092010: step 2480, loss = 1.12 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-09 00:40:20.367099: step 2490, loss = 1.05 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-09 00:40:21.767111: step 2500, loss = 1.57 (914.3 examples/sec; 0.140 sec/batch)
2017-05-09 00:40:22.975289: step 2510, loss = 1.28 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-09 00:40:24.274293: step 2520, loss = 1.33 (985.4 examples/sec; 0.130 sec/batch)
2017-05-09 00:40:25.577781: step 2530, loss = 1.18 (982.0 examples/sec; 0.130 sec/batch)
2017-05-09 00:40:26.897685: step 2540, loss = 1.15 (970.1 examples/sec; 0.132 sec/batch)
2017-05-09 00:40:28.229673: step 2550, loss = 1.21 (960.6 examples/sec; 0.133 sec/batch)
2017-05-09 00:40:29.546859: step 2560, loss = 1.23 (971.8 examples/sec; 0.132 sec/batch)
2017-05-09 00:40:30.866579: step 2570, loss = 1.24 (969.9 examples/sec; 0.132 sec/batch)
2017-05-09 00:40:32.177805: step 2580, loss = 1.36 (976.2 examples/sec; 0.131 sec/batch)
2017-05-09 00:40:33.510573: step 2590, loss = 1.48 (960.4 examples/sec; 0.133 sec/batch)
2017-05-09 00:40:34.957179: step 2600, loss = 1.18 (884.8 examples/sec; 0.145 sec/batch)
2017-05-09 00:40:36.232763: step 2610, loss = 1.47 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:40:37.523718: step 2620, loss = 1.37 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 00:40:38.813895: step 2630, loss = 1.21 (992.1 examples/sec; 0.129 sec/batch)
2017-05-09 00:40:40.105480: step 2640, loss = 1.32 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:40:41.393283: step 2650, loss = 1.09 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:40:42.689719: step 2660, loss = 1.10 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:40:44.000103: step 2670, loss = 1.06 (976.8 examples/sec; 0.131 sec/batch)
2017-05-09 00:40:45.319196: step 2680, loss = 1.02 (970.4 examples/sec; 0.132 sec/batch)
2017-05-09 00:40:46.667382: step 2690, loss = 1.01 (949.4 examples/sec; 0.135 sec/batch)
2017-05-09 00:40:48.101402: step 2700, loss = 1.42 (892.6 examples/sec; 0.143 sec/batch)
2017-05-09 00:40:49.335985: step 2710, loss = 1.18 (1036.8 examples/sec; 0.123 sec/batch)
2017-05-09 00:40:50.663730: step 2720, loss = 1.10 (964.0 examples/sec; 0.133 sec/batch)
2017-05-09 00:40:51.964108: step 2730, loss = 1.18 (984.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:40:53.266501: step 2740, loss = 1.28 (982.8 examples/sec; 0.130 sec/batE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 63 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
ch)
2017-05-09 00:40:54.600279: step 2750, loss = 1.27 (959.7 examples/sec; 0.133 sec/batch)
2017-05-09 00:40:55.946170: step 2760, loss = 1.12 (951.0 examples/sec; 0.135 sec/batch)
2017-05-09 00:40:57.293466: step 2770, loss = 1.26 (950.0 examples/sec; 0.135 sec/batch)
2017-05-09 00:40:58.595885: step 2780, loss = 1.09 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 00:40:59.888644: step 2790, loss = 1.37 (990.1 examples/sec; 0.129 sec/batch)
2017-05-09 00:41:01.290949: step 2800, loss = 1.15 (912.8 examples/sec; 0.140 sec/batch)
2017-05-09 00:41:02.452294: step 2810, loss = 1.33 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-09 00:41:03.731340: step 2820, loss = 1.24 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-09 00:41:05.029063: step 2830, loss = 1.10 (986.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:41:06.347714: step 2840, loss = 1.28 (970.7 examples/sec; 0.132 sec/batch)
2017-05-09 00:41:07.644990: step 2850, loss = 1.23 (986.7 examples/sec; 0.130 sec/batch)
2017-05-09 00:41:08.950937: step 2860, loss = 1.40 (980.1 examples/sec; 0.131 sec/batch)
2017-05-09 00:41:10.260278: step 2870, loss = 1.21 (977.6 examples/sec; 0.131 sec/batch)
2017-05-09 00:41:11.530620: step 2880, loss = 1.35 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-09 00:41:12.815307: step 2890, loss = 1.27 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 00:41:14.200129: step 2900, loss = 1.01 (924.3 examples/sec; 0.138 sec/batch)
2017-05-09 00:41:15.425854: step 2910, loss = 1.13 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-09 00:41:16.732764: step 2920, loss = 1.04 (979.4 examples/sec; 0.131 sec/batch)
2017-05-09 00:41:18.027368: step 2930, loss = 1.51 (988.7 examples/sec; 0.129 sec/batch)
2017-05-09 00:41:19.346579: step 2940, loss = 1.29 (970.2 examples/sec; 0.132 sec/batch)
2017-05-09 00:41:20.626966: step 2950, loss = 1.21 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:41:21.906392: step 2960, loss = 1.32 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:41:23.177448: step 2970, loss = 0.96 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-09 00:41:24.473573: step 2980, loss = 1.17 (987.6 examples/sec; 0.130 sec/batch)
2017-05-09 00:41:25.756010: step 2990, loss = 1.28 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:41:27.145411: step 3000, loss = 1.10 (921.3 examples/sec; 0.139 sec/batch)
2017-05-09 00:41:28.326687: step 3010, loss = 1.02 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-09 00:41:29.611254: step 3020, loss = 1.08 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:41:30.883182: step 3030, loss = 0.95 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-09 00:41:32.189025: step 3040, loss = 1.13 (980.2 examples/sec; 0.131 sec/batch)
2017-05-09 00:41:33.485298: step 3050, loss = 0.93 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 00:41:34.763527: step 3060, loss = 1.20 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:41:36.052864: step 3070, loss = 1.16 (992.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:41:37.353976: step 3080, loss = 1.16 (983.8 examples/sec; 0.130 sec/batch)
2017-05-09 00:41:38.699759: step 3090, loss = 1.52 (951.1 examples/sec; 0.135 sec/batch)
2017-05-09 00:41:40.128735: step 3100, loss = 1.27 (895.7 examples/sec; 0.143 sec/batch)
2017-05-09 00:41:41.334904: step 3110, loss = 0.99 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-09 00:41:42.686259: step 3120, loss = 1.22 (947.2 examples/sec; 0.135 sec/batch)
2017-05-09 00:41:43.966311: step 3130, loss = 1.18 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:41:45.294234: step 3140, loss = 0.98 (963.9 examples/sec; 0.133 sec/batch)
2017-05-09 00:41:46.587021: step 3150, loss = 1.10 (990.1 examples/sec; 0.129 sec/batch)
2017-05-09 00:41:47.869201: step 3160, loss = 1.09 (998.3 examples/sec; 0.128 sec/batch)
2017-05-09 00:41:49.188895: step 3170, loss = 1.16 (969.9 examples/sec; 0.132 sec/batch)
2017-05-09 00:41:50.482644: step 3180, loss = 1.20 (989.4 examples/sec; 0.129 sec/batch)
2017-05-09 00:41:51.766470: step 3190, loss = 1.04 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:41:53.161827: step 3200, loss = 1.10 (917.3 examples/sec; 0.140 sec/batch)
2017-05-09 00:41:54.356250: step 3210, loss = 1.40 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-09 00:41:55.683642: step 3220, loss = 1.33 (964.3 examples/sec; 0.133 sec/batch)
2017-05-09 00:41:56.986728: step 3230, loss = 1.16 (982.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:41:58.321678: step 3240, loss = 1.36 (958.8 examples/sec; 0.133 sec/batch)
2017-05-09 00:41:59.619440: step 3250, loss = 1.07 (986.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:42:00.929441: step 3260, loss = 1.20 (977.1 examples/sec; 0.131 sec/batch)
2017-05-09 00:42:02.223559: step 3270, loss = 1.37 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 00:42:03.545484: step 3280, loss = 1.04 (968.3 examples/sec; 0.132 sec/batch)
2017-05-09 00:42:04.820648: step 3290, loss = 1.44 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 00:42:06.189741: step 3300, loss = 1.14 (934.9 examples/sec; 0.137 sec/batch)
2017-05-09 00:42:07.419251: step 3310, loss = 0.96 (1041.1 examples/sec; 0.123 sec/batch)
2017-05-09 00:42:08.716531: step 3320, loss = 1.20 (986.7 examples/sec; 0.130 sec/batch)
2017-05-09 00:42:10.000147: step 3330, loss = 1.20 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:42:11.289714: step 3340, loss = 1.13 (992.6 examples/sec; 0.129 sec/batch)
2017-05-09 00:42:12.580024: step 3350, loss = 1.09 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:42:13.876503: step 3360, loss = 1.05 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:42:15.163463: step 3370, loss = 1.03 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 00:42:16.479782: step 3380, loss = 1.18 (972.4 examples/sec; 0.132 sec/batch)
2017-05-09 00:42:17.785904: step 3390, loss = 1.36 (980.0 examples/sec; 0.131 sec/batch)
2017-05-09 00:42:19.153342: step 3400, loss = 1.05 (936.1 examples/sec; 0.137 sec/batch)
2017-05-09 00:42:20.340925: step 3410, loss = 1.14 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-09 00:42:21.607646: step 3420, loss = 1.09 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-09 00:42:22.883262: step 3430, loss = 1.40 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:42:24.172881: step 3440, loss = 1.17 (992.5 examples/sec; 0.129 sec/batch)
2017-05-09 00:42:25.457284: step 3450, loss = 1.45 (996.6 examples/sec; 0.128 sec/batch)
2017-05-09 00:42:26.735116: step 3460, loss = 1.10 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:42:28.002975: step 3470, loss = 1.23 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-09 00:42:29.316648: step 3480, loss = 1.19 (974.4 examples/sec; 0.131 sec/batch)
2017-05-09 00:42:30.635989: step 3490, loss = 0.94 (970.2 examples/sec; 0.132 sec/batch)
2017-05-09 00:42:32.021382: step 3500, loss = 1.02 (923.9 examples/sec; 0.139 sec/batch)
2017-05-09 00:42:33.218854: step 3510, loss = 1.12 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-09 00:42:34.491011: step 3520, loss = 1.09 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-09 00:42:35.773580: step 3530, loss = 0.90 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:42:37.093493: step 3540, loss = 1.02 (969.8 examples/sec; 0.132 sec/batch)
2017-05-09 00:42:38.448891: step 3550, loss = 1.06 (944.4 examples/sec; 0.136 sec/batch)
2017-05-09 00:42:39.789031: step 3560, loss = 1.37 (955.1 examples/sec; 0.134 sec/batch)
2017-05-09 00:42:41.108111: step 3570, loss = 1.07 (970.4 examples/sec; 0.132 sec/batch)
2017-05-09 00:42:42.446479: step 3580, loss = 1.18 (956.4 examples/sec; 0.134 sec/batch)
2017-05-09 00:42:43.770858: step 3590, loss = 1.09 (966.5 examples/sec; 0.132 sec/batch)
2017-05-09 00:42:45.213879: step 3600, loss = 1.26 (887.0 examples/sec; 0.144 sec/batch)
2017-05-09 00:42:46.439479: step 3610, loss = 1.13 (1044.4 examples/sec; 0.123 sec/batch)
2017-05-09 00:42:47.769719: step 3620, loss = 1.33 (962.2 examples/sec; 0.133 sec/batch)
2017-05-09 00:42:49.100649: step 3630, loss = 1.00 (961.7 examples/sec; 0.133 sec/batch)
2017-05-09 00:42:50.360856: step 3640, loss = 1.10 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-09 00:42:51.715477: step 3650, loss = 1.12 (944.9 examples/sec; 0.135 sec/batch)
2017-05-09 00:42:53.051378: step 3660, loss = 1.13 (958.2 examplesE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 83 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
/sec; 0.134 sec/batch)
2017-05-09 00:42:54.392986: step 3670, loss = 1.27 (954.1 examples/sec; 0.134 sec/batch)
2017-05-09 00:42:55.716827: step 3680, loss = 1.25 (966.9 examples/sec; 0.132 sec/batch)
2017-05-09 00:42:57.040843: step 3690, loss = 0.98 (966.8 examples/sec; 0.132 sec/batch)
2017-05-09 00:42:58.480467: step 3700, loss = 0.98 (889.1 examples/sec; 0.144 sec/batch)
2017-05-09 00:42:59.701221: step 3710, loss = 1.15 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-09 00:43:01.021832: step 3720, loss = 1.21 (969.2 examples/sec; 0.132 sec/batch)
2017-05-09 00:43:02.370701: step 3730, loss = 1.12 (949.0 examples/sec; 0.135 sec/batch)
2017-05-09 00:43:03.713885: step 3740, loss = 1.21 (953.0 examples/sec; 0.134 sec/batch)
2017-05-09 00:43:05.051461: step 3750, loss = 1.33 (957.0 examples/sec; 0.134 sec/batch)
2017-05-09 00:43:06.349098: step 3760, loss = 1.26 (986.4 examples/sec; 0.130 sec/batch)
2017-05-09 00:43:07.701017: step 3770, loss = 1.22 (946.8 examples/sec; 0.135 sec/batch)
2017-05-09 00:43:09.042442: step 3780, loss = 1.41 (954.2 examples/sec; 0.134 sec/batch)
2017-05-09 00:43:10.390830: step 3790, loss = 1.08 (949.3 examples/sec; 0.135 sec/batch)
2017-05-09 00:43:11.817765: step 3800, loss = 1.14 (897.0 examples/sec; 0.143 sec/batch)
2017-05-09 00:43:13.041590: step 3810, loss = 1.10 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-09 00:43:14.364101: step 3820, loss = 1.01 (967.9 examples/sec; 0.132 sec/batch)
2017-05-09 00:43:15.695603: step 3830, loss = 1.11 (961.3 examples/sec; 0.133 sec/batch)
2017-05-09 00:43:17.014845: step 3840, loss = 1.06 (970.3 examples/sec; 0.132 sec/batch)
2017-05-09 00:43:18.381927: step 3850, loss = 0.99 (936.3 examples/sec; 0.137 sec/batch)
2017-05-09 00:43:19.728569: step 3860, loss = 0.99 (950.5 examples/sec; 0.135 sec/batch)
2017-05-09 00:43:21.057529: step 3870, loss = 1.13 (963.2 examples/sec; 0.133 sec/batch)
2017-05-09 00:43:22.421910: step 3880, loss = 1.02 (938.2 examples/sec; 0.136 sec/batch)
2017-05-09 00:43:23.754030: step 3890, loss = 1.27 (960.9 examples/sec; 0.133 sec/batch)
2017-05-09 00:43:25.248953: step 3900, loss = 0.95 (856.2 examples/sec; 0.149 sec/batch)
2017-05-09 00:43:26.467197: step 3910, loss = 1.23 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-09 00:43:27.822686: step 3920, loss = 1.13 (944.3 examples/sec; 0.136 sec/batch)
2017-05-09 00:43:29.176698: step 3930, loss = 1.16 (945.3 examples/sec; 0.135 sec/batch)
2017-05-09 00:43:30.515468: step 3940, loss = 1.13 (956.1 examples/sec; 0.134 sec/batch)
2017-05-09 00:43:31.829251: step 3950, loss = 1.33 (974.3 examples/sec; 0.131 sec/batch)
2017-05-09 00:43:33.163907: step 3960, loss = 1.13 (959.0 examples/sec; 0.133 sec/batch)
2017-05-09 00:43:34.531235: step 3970, loss = 0.84 (936.1 examples/sec; 0.137 sec/batch)
2017-05-09 00:43:35.890618: step 3980, loss = 1.58 (941.6 examples/sec; 0.136 sec/batch)
2017-05-09 00:43:37.234733: step 3990, loss = 1.22 (952.3 examples/sec; 0.134 sec/batch)
2017-05-09 00:43:38.708394: step 4000, loss = 1.06 (868.6 examples/sec; 0.147 sec/batch)
2017-05-09 00:43:39.927814: step 4010, loss = 1.04 (1049.7 examples/sec; 0.122 sec/batch)
2017-05-09 00:43:41.268277: step 4020, loss = 1.09 (954.9 examples/sec; 0.134 sec/batch)
2017-05-09 00:43:42.527626: step 4030, loss = 0.99 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-09 00:43:43.833327: step 4040, loss = 0.92 (980.3 examples/sec; 0.131 sec/batch)
2017-05-09 00:43:45.118202: step 4050, loss = 1.10 (996.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:43:46.395637: step 4060, loss = 0.87 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:43:47.687713: step 4070, loss = 1.09 (990.7 examples/sec; 0.129 sec/batch)
2017-05-09 00:43:48.951977: step 4080, loss = 1.12 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-09 00:43:50.292152: step 4090, loss = 0.92 (955.1 examples/sec; 0.134 sec/batch)
2017-05-09 00:43:51.753751: step 4100, loss = 1.23 (875.8 examples/sec; 0.146 sec/batch)
2017-05-09 00:43:52.991467: step 4110, loss = 1.11 (1034.2 examples/sec; 0.124 sec/batch)
2017-05-09 00:43:54.329594: step 4120, loss = 0.92 (956.6 examples/sec; 0.134 sec/batch)
2017-05-09 00:43:55.716914: step 4130, loss = 1.04 (922.6 examples/sec; 0.139 sec/batch)
2017-05-09 00:43:57.045285: step 4140, loss = 1.05 (963.6 examples/sec; 0.133 sec/batch)
2017-05-09 00:43:58.389851: step 4150, loss = 1.18 (952.0 examples/sec; 0.134 sec/batch)
2017-05-09 00:43:59.782001: step 4160, loss = 0.93 (919.4 examples/sec; 0.139 sec/batch)
2017-05-09 00:44:01.105671: step 4170, loss = 1.06 (967.0 examples/sec; 0.132 sec/batch)
2017-05-09 00:44:02.453626: step 4180, loss = 0.97 (949.6 examples/sec; 0.135 sec/batch)
2017-05-09 00:44:03.806009: step 4190, loss = 1.24 (946.5 examples/sec; 0.135 sec/batch)
2017-05-09 00:44:05.301699: step 4200, loss = 1.14 (855.8 examples/sec; 0.150 sec/batch)
2017-05-09 00:44:06.526032: step 4210, loss = 1.08 (1045.5 examples/sec; 0.122 sec/batch)
2017-05-09 00:44:07.810342: step 4220, loss = 1.02 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:44:09.129617: step 4230, loss = 0.92 (970.2 examples/sec; 0.132 sec/batch)
2017-05-09 00:44:10.413625: step 4240, loss = 1.06 (996.9 examples/sec; 0.128 sec/batch)
2017-05-09 00:44:11.715419: step 4250, loss = 1.29 (983.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:44:12.962267: step 4260, loss = 1.13 (1026.6 examples/sec; 0.125 sec/batch)
2017-05-09 00:44:14.220957: step 4270, loss = 1.11 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-09 00:44:15.551859: step 4280, loss = 0.99 (961.8 examples/sec; 0.133 sec/batch)
2017-05-09 00:44:16.830147: step 4290, loss = 0.94 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 00:44:18.201887: step 4300, loss = 1.20 (933.1 examples/sec; 0.137 sec/batch)
2017-05-09 00:44:19.419590: step 4310, loss = 1.15 (1051.2 examples/sec; 0.122 sec/batch)
2017-05-09 00:44:20.700203: step 4320, loss = 1.21 (999.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:44:21.974945: step 4330, loss = 1.04 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-09 00:44:23.285973: step 4340, loss = 0.99 (976.3 examples/sec; 0.131 sec/batch)
2017-05-09 00:44:24.605429: step 4350, loss = 0.89 (970.1 examples/sec; 0.132 sec/batch)
2017-05-09 00:44:25.895010: step 4360, loss = 1.01 (992.6 examples/sec; 0.129 sec/batch)
2017-05-09 00:44:27.196569: step 4370, loss = 0.88 (983.4 examples/sec; 0.130 sec/batch)
2017-05-09 00:44:28.481235: step 4380, loss = 1.29 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:44:29.741606: step 4390, loss = 0.87 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-09 00:44:31.140055: step 4400, loss = 0.96 (915.3 examples/sec; 0.140 sec/batch)
2017-05-09 00:44:32.332979: step 4410, loss = 0.95 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-09 00:44:33.624707: step 4420, loss = 1.16 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:44:34.927427: step 4430, loss = 1.20 (982.6 examples/sec; 0.130 sec/batch)
2017-05-09 00:44:36.204849: step 4440, loss = 1.16 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:44:37.475720: step 4450, loss = 0.96 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-09 00:44:38.746572: step 4460, loss = 1.08 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-09 00:44:40.029172: step 4470, loss = 1.15 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:44:41.305195: step 4480, loss = 0.92 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:44:42.599332: step 4490, loss = 1.05 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 00:44:43.972990: step 4500, loss = 1.13 (931.8 examples/sec; 0.137 sec/batch)
2017-05-09 00:44:45.156304: step 4510, loss = 1.00 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-09 00:44:46.438215: step 4520, loss = 1.05 (998.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:44:47.710450: step 4530, loss = 0.97 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-09 00:44:48.998416: step 4540, loss = 1.19 (993.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:44:50.272852: step 4550, loss = 1.05 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-09 00:44:51.564015: step 4560, loss = 1.02 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 00:44:52.872158: step 4570, loss = 1.08 (978.5 examples/sec; 0.131 sec/batch)
2017-05-09 00:44:54.145043: step 4580, loss = 1E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 104 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
.16 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-09 00:44:55.472826: step 4590, loss = 1.18 (964.0 examples/sec; 0.133 sec/batch)
2017-05-09 00:44:56.865919: step 4600, loss = 1.01 (918.8 examples/sec; 0.139 sec/batch)
2017-05-09 00:44:58.068619: step 4610, loss = 1.10 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-09 00:44:59.361853: step 4620, loss = 1.05 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:45:00.628274: step 4630, loss = 0.87 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-09 00:45:01.892428: step 4640, loss = 0.96 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-09 00:45:03.187578: step 4650, loss = 0.97 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:45:04.510361: step 4660, loss = 1.04 (967.7 examples/sec; 0.132 sec/batch)
2017-05-09 00:45:05.797057: step 4670, loss = 1.51 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:45:07.117203: step 4680, loss = 1.28 (969.6 examples/sec; 0.132 sec/batch)
2017-05-09 00:45:08.407716: step 4690, loss = 0.95 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:45:09.766063: step 4700, loss = 1.00 (942.3 examples/sec; 0.136 sec/batch)
2017-05-09 00:45:10.954417: step 4710, loss = 1.20 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-09 00:45:12.239353: step 4720, loss = 1.05 (996.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:45:13.521671: step 4730, loss = 1.05 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:45:14.826141: step 4740, loss = 1.23 (981.2 examples/sec; 0.130 sec/batch)
2017-05-09 00:45:16.083940: step 4750, loss = 1.17 (1017.6 examples/sec; 0.126 sec/batch)
2017-05-09 00:45:17.396247: step 4760, loss = 1.10 (975.4 examples/sec; 0.131 sec/batch)
2017-05-09 00:45:18.711745: step 4770, loss = 1.25 (973.0 examples/sec; 0.132 sec/batch)
2017-05-09 00:45:20.013859: step 4780, loss = 1.14 (983.0 examples/sec; 0.130 sec/batch)
2017-05-09 00:45:21.296821: step 4790, loss = 0.93 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:45:22.706387: step 4800, loss = 1.27 (908.1 examples/sec; 0.141 sec/batch)
2017-05-09 00:45:23.914365: step 4810, loss = 1.20 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-09 00:45:25.234585: step 4820, loss = 1.10 (969.5 examples/sec; 0.132 sec/batch)
2017-05-09 00:45:26.527845: step 4830, loss = 0.95 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 00:45:27.851395: step 4840, loss = 1.14 (967.1 examples/sec; 0.132 sec/batch)
2017-05-09 00:45:29.146887: step 4850, loss = 1.12 (988.1 examples/sec; 0.130 sec/batch)
2017-05-09 00:45:30.425924: step 4860, loss = 0.82 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:45:31.698264: step 4870, loss = 1.10 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 00:45:33.025679: step 4880, loss = 0.88 (964.6 examples/sec; 0.133 sec/batch)
2017-05-09 00:45:34.315165: step 4890, loss = 0.93 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 00:45:35.705737: step 4900, loss = 1.05 (920.5 examples/sec; 0.139 sec/batch)
2017-05-09 00:45:36.911478: step 4910, loss = 0.88 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-09 00:45:38.174398: step 4920, loss = 1.03 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-09 00:45:39.479907: step 4930, loss = 1.00 (980.5 examples/sec; 0.131 sec/batch)
2017-05-09 00:45:40.781462: step 4940, loss = 1.26 (983.4 examples/sec; 0.130 sec/batch)
2017-05-09 00:45:42.091573: step 4950, loss = 1.10 (977.0 examples/sec; 0.131 sec/batch)
2017-05-09 00:45:43.384808: step 4960, loss = 0.96 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:45:44.701537: step 4970, loss = 1.10 (972.1 examples/sec; 0.132 sec/batch)
2017-05-09 00:45:46.022140: step 4980, loss = 1.05 (969.3 examples/sec; 0.132 sec/batch)
2017-05-09 00:45:47.325430: step 4990, loss = 1.22 (982.1 examples/sec; 0.130 sec/batch)
2017-05-09 00:45:48.713687: step 5000, loss = 0.86 (922.0 examples/sec; 0.139 sec/batch)
2017-05-09 00:45:49.911105: step 5010, loss = 1.07 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-09 00:45:51.228937: step 5020, loss = 1.26 (971.3 examples/sec; 0.132 sec/batch)
2017-05-09 00:45:52.503522: step 5030, loss = 1.29 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-09 00:45:53.785498: step 5040, loss = 0.92 (998.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:45:55.116393: step 5050, loss = 1.00 (961.8 examples/sec; 0.133 sec/batch)
2017-05-09 00:45:56.418732: step 5060, loss = 1.04 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 00:45:57.696265: step 5070, loss = 0.96 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 00:45:58.999107: step 5080, loss = 1.01 (982.5 examples/sec; 0.130 sec/batch)
2017-05-09 00:46:00.315010: step 5090, loss = 1.04 (972.7 examples/sec; 0.132 sec/batch)
2017-05-09 00:46:01.730022: step 5100, loss = 1.00 (904.6 examples/sec; 0.142 sec/batch)
2017-05-09 00:46:02.929177: step 5110, loss = 0.92 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-09 00:46:04.210413: step 5120, loss = 1.16 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:46:05.524544: step 5130, loss = 1.56 (974.0 examples/sec; 0.131 sec/batch)
2017-05-09 00:46:06.855151: step 5140, loss = 1.06 (962.0 examples/sec; 0.133 sec/batch)
2017-05-09 00:46:08.145550: step 5150, loss = 0.95 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:46:09.437578: step 5160, loss = 1.15 (990.7 examples/sec; 0.129 sec/batch)
2017-05-09 00:46:10.707415: step 5170, loss = 1.04 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-09 00:46:11.975082: step 5180, loss = 1.18 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-09 00:46:13.266430: step 5190, loss = 1.20 (991.2 examples/sec; 0.129 sec/batch)
2017-05-09 00:46:14.652290: step 5200, loss = 1.21 (923.6 examples/sec; 0.139 sec/batch)
2017-05-09 00:46:15.862724: step 5210, loss = 1.13 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-09 00:46:17.174637: step 5220, loss = 1.08 (975.7 examples/sec; 0.131 sec/batch)
2017-05-09 00:46:18.490179: step 5230, loss = 1.13 (973.0 examples/sec; 0.132 sec/batch)
2017-05-09 00:46:19.793983: step 5240, loss = 0.95 (981.7 examples/sec; 0.130 sec/batch)
2017-05-09 00:46:21.095423: step 5250, loss = 0.94 (983.5 examples/sec; 0.130 sec/batch)
2017-05-09 00:46:22.388569: step 5260, loss = 1.23 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:46:23.707609: step 5270, loss = 1.38 (970.4 examples/sec; 0.132 sec/batch)
2017-05-09 00:46:25.019855: step 5280, loss = 1.05 (975.4 examples/sec; 0.131 sec/batch)
2017-05-09 00:46:26.300444: step 5290, loss = 0.94 (999.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:46:27.685653: step 5300, loss = 1.03 (924.0 examples/sec; 0.139 sec/batch)
2017-05-09 00:46:28.842081: step 5310, loss = 0.86 (1106.9 examples/sec; 0.116 sec/batch)
2017-05-09 00:46:30.150556: step 5320, loss = 1.04 (978.2 examples/sec; 0.131 sec/batch)
2017-05-09 00:46:31.419824: step 5330, loss = 0.84 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-09 00:46:32.692941: step 5340, loss = 0.96 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-09 00:46:33.969566: step 5350, loss = 0.99 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-09 00:46:35.248272: step 5360, loss = 1.10 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:46:36.530027: step 5370, loss = 0.98 (998.6 examples/sec; 0.128 sec/batch)
2017-05-09 00:46:37.799201: step 5380, loss = 0.89 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-09 00:46:39.081081: step 5390, loss = 1.18 (998.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:46:40.463611: step 5400, loss = 1.08 (925.8 examples/sec; 0.138 sec/batch)
2017-05-09 00:46:41.670844: step 5410, loss = 0.94 (1060.3 examples/sec; 0.121 sec/batch)
2017-05-09 00:46:42.963110: step 5420, loss = 0.86 (990.5 examples/sec; 0.129 sec/batch)
2017-05-09 00:46:44.267596: step 5430, loss = 1.32 (981.2 examples/sec; 0.130 sec/batch)
2017-05-09 00:46:45.572379: step 5440, loss = 0.98 (981.0 examples/sec; 0.130 sec/batch)
2017-05-09 00:46:46.860404: step 5450, loss = 0.92 (993.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:46:48.134698: step 5460, loss = 1.16 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-09 00:46:49.395739: step 5470, loss = 0.99 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-09 00:46:50.688109: step 5480, loss = 1.06 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 00:46:51.993242: step 5490, loss = 1.22 (980.7 examples/sec; 0.131 sec/batch)
2017-05-09 00:46:53.3712E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 124 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
19: step 5500, loss = 0.95 (928.9 examples/sec; 0.138 sec/batch)
2017-05-09 00:46:54.540676: step 5510, loss = 1.22 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-09 00:46:55.826794: step 5520, loss = 0.90 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 00:46:57.119969: step 5530, loss = 1.13 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:46:58.407184: step 5540, loss = 1.01 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 00:46:59.706943: step 5550, loss = 1.11 (984.8 examples/sec; 0.130 sec/batch)
2017-05-09 00:47:01.006891: step 5560, loss = 1.05 (984.7 examples/sec; 0.130 sec/batch)
2017-05-09 00:47:02.300942: step 5570, loss = 1.28 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 00:47:03.636816: step 5580, loss = 1.20 (958.2 examples/sec; 0.134 sec/batch)
2017-05-09 00:47:04.929896: step 5590, loss = 1.16 (989.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:47:06.311623: step 5600, loss = 1.07 (926.4 examples/sec; 0.138 sec/batch)
2017-05-09 00:47:07.502536: step 5610, loss = 0.88 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-09 00:47:08.799195: step 5620, loss = 1.04 (987.2 examples/sec; 0.130 sec/batch)
2017-05-09 00:47:10.073292: step 5630, loss = 0.95 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-09 00:47:11.382872: step 5640, loss = 0.94 (977.4 examples/sec; 0.131 sec/batch)
2017-05-09 00:47:12.686151: step 5650, loss = 1.10 (982.1 examples/sec; 0.130 sec/batch)
2017-05-09 00:47:13.949823: step 5660, loss = 1.05 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-09 00:47:15.297136: step 5670, loss = 0.89 (950.0 examples/sec; 0.135 sec/batch)
2017-05-09 00:47:16.650765: step 5680, loss = 1.17 (945.6 examples/sec; 0.135 sec/batch)
2017-05-09 00:47:18.017018: step 5690, loss = 1.15 (936.9 examples/sec; 0.137 sec/batch)
2017-05-09 00:47:19.507605: step 5700, loss = 1.04 (858.7 examples/sec; 0.149 sec/batch)
2017-05-09 00:47:20.700302: step 5710, loss = 0.93 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-09 00:47:22.052629: step 5720, loss = 1.01 (946.5 examples/sec; 0.135 sec/batch)
2017-05-09 00:47:23.421005: step 5730, loss = 1.28 (935.4 examples/sec; 0.137 sec/batch)
2017-05-09 00:47:24.781988: step 5740, loss = 1.00 (940.5 examples/sec; 0.136 sec/batch)
2017-05-09 00:47:26.113927: step 5750, loss = 0.90 (961.0 examples/sec; 0.133 sec/batch)
2017-05-09 00:47:27.449358: step 5760, loss = 0.92 (958.5 examples/sec; 0.134 sec/batch)
2017-05-09 00:47:28.796122: step 5770, loss = 1.13 (950.4 examples/sec; 0.135 sec/batch)
2017-05-09 00:47:30.117628: step 5780, loss = 0.94 (968.6 examples/sec; 0.132 sec/batch)
2017-05-09 00:47:31.461724: step 5790, loss = 1.13 (952.3 examples/sec; 0.134 sec/batch)
2017-05-09 00:47:32.844289: step 5800, loss = 0.94 (925.8 examples/sec; 0.138 sec/batch)
2017-05-09 00:47:34.014897: step 5810, loss = 0.94 (1093.4 examples/sec; 0.117 sec/batch)
2017-05-09 00:47:35.295683: step 5820, loss = 1.00 (999.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:47:36.566734: step 5830, loss = 0.98 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-09 00:47:37.851728: step 5840, loss = 0.83 (996.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:47:39.170387: step 5850, loss = 1.14 (970.7 examples/sec; 0.132 sec/batch)
2017-05-09 00:47:40.482610: step 5860, loss = 1.08 (975.4 examples/sec; 0.131 sec/batch)
2017-05-09 00:47:41.824860: step 5870, loss = 1.09 (953.6 examples/sec; 0.134 sec/batch)
2017-05-09 00:47:43.176652: step 5880, loss = 1.57 (946.9 examples/sec; 0.135 sec/batch)
2017-05-09 00:47:44.473131: step 5890, loss = 1.12 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:47:45.865886: step 5900, loss = 0.95 (919.0 examples/sec; 0.139 sec/batch)
2017-05-09 00:47:47.047219: step 5910, loss = 0.96 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-09 00:47:48.318695: step 5920, loss = 0.94 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-09 00:47:49.593977: step 5930, loss = 1.08 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:47:50.858819: step 5940, loss = 0.95 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-09 00:47:52.162205: step 5950, loss = 1.12 (982.1 examples/sec; 0.130 sec/batch)
2017-05-09 00:47:53.441120: step 5960, loss = 1.00 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-09 00:47:54.702597: step 5970, loss = 1.13 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-09 00:47:56.021448: step 5980, loss = 1.24 (970.5 examples/sec; 0.132 sec/batch)
2017-05-09 00:47:57.317521: step 5990, loss = 0.94 (987.6 examples/sec; 0.130 sec/batch)
2017-05-09 00:47:58.700304: step 6000, loss = 0.98 (925.7 examples/sec; 0.138 sec/batch)
2017-05-09 00:47:59.880173: step 6010, loss = 1.11 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-09 00:48:01.179713: step 6020, loss = 1.09 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 00:48:02.448840: step 6030, loss = 1.02 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-09 00:48:03.767904: step 6040, loss = 1.02 (970.3 examples/sec; 0.132 sec/batch)
2017-05-09 00:48:05.018751: step 6050, loss = 0.85 (1023.3 examples/sec; 0.125 sec/batch)
2017-05-09 00:48:06.337805: step 6060, loss = 1.08 (970.4 examples/sec; 0.132 sec/batch)
2017-05-09 00:48:07.626889: step 6070, loss = 1.10 (993.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:48:08.923846: step 6080, loss = 0.96 (986.9 examples/sec; 0.130 sec/batch)
2017-05-09 00:48:10.198649: step 6090, loss = 0.95 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-09 00:48:11.622136: step 6100, loss = 1.20 (899.2 examples/sec; 0.142 sec/batch)
2017-05-09 00:48:12.813245: step 6110, loss = 1.11 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-09 00:48:14.095589: step 6120, loss = 1.29 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:48:15.399350: step 6130, loss = 0.90 (981.8 examples/sec; 0.130 sec/batch)
2017-05-09 00:48:16.675849: step 6140, loss = 0.95 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:48:17.959516: step 6150, loss = 1.18 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:48:19.289712: step 6160, loss = 0.94 (962.3 examples/sec; 0.133 sec/batch)
2017-05-09 00:48:20.623418: step 6170, loss = 0.98 (959.7 examples/sec; 0.133 sec/batch)
2017-05-09 00:48:21.945611: step 6180, loss = 1.14 (968.1 examples/sec; 0.132 sec/batch)
2017-05-09 00:48:23.285136: step 6190, loss = 0.99 (955.6 examples/sec; 0.134 sec/batch)
2017-05-09 00:48:24.743511: step 6200, loss = 0.94 (877.7 examples/sec; 0.146 sec/batch)
2017-05-09 00:48:26.003437: step 6210, loss = 1.19 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-09 00:48:27.343528: step 6220, loss = 0.97 (955.1 examples/sec; 0.134 sec/batch)
2017-05-09 00:48:28.675031: step 6230, loss = 0.98 (961.3 examples/sec; 0.133 sec/batch)
2017-05-09 00:48:30.014059: step 6240, loss = 1.02 (955.9 examples/sec; 0.134 sec/batch)
2017-05-09 00:48:31.284859: step 6250, loss = 0.94 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-09 00:48:32.650918: step 6260, loss = 0.95 (937.0 examples/sec; 0.137 sec/batch)
2017-05-09 00:48:33.971541: step 6270, loss = 1.15 (969.2 examples/sec; 0.132 sec/batch)
2017-05-09 00:48:35.356435: step 6280, loss = 1.04 (924.3 examples/sec; 0.138 sec/batch)
2017-05-09 00:48:36.706227: step 6290, loss = 0.98 (948.3 examples/sec; 0.135 sec/batch)
2017-05-09 00:48:38.135631: step 6300, loss = 0.97 (895.5 examples/sec; 0.143 sec/batch)
2017-05-09 00:48:39.418169: step 6310, loss = 0.87 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:48:40.743867: step 6320, loss = 0.88 (965.5 examples/sec; 0.133 sec/batch)
2017-05-09 00:48:42.052002: step 6330, loss = 1.04 (978.5 examples/sec; 0.131 sec/batch)
2017-05-09 00:48:43.367011: step 6340, loss = 0.88 (973.4 examples/sec; 0.132 sec/batch)
2017-05-09 00:48:44.709833: step 6350, loss = 1.00 (953.2 examples/sec; 0.134 sec/batch)
2017-05-09 00:48:46.029789: step 6360, loss = 1.10 (969.7 examples/sec; 0.132 sec/batch)
2017-05-09 00:48:47.392572: step 6370, loss = 1.36 (939.3 examples/sec; 0.136 sec/batch)
2017-05-09 00:48:48.757898: step 6380, loss = 1.07 (937.5 examples/sec; 0.137 sec/batch)
2017-05-09 00:48:50.064754: step 6390, loss = 1.25 (979.4 examples/sec; 0.131 sec/batch)
2017-05-09 00:48:51.526732: step 6400, loss = 1.07 (875.5 examples/sec; 0.146 sec/batch)
2017-05-09 00:48:52.750617: step 6410, loss = 1.07 (1045.8 examples/sec; 0.122 sec/batch)
2017-05-09 00:48:54.123548: step 6420, loss = 0.80 (932.3 examples/sec; 0.137 sec/batch)
2017-05-09 00:48:55.461624: step 6430, loss = 1.04 (956.6 examples/sec; 0.134 sec/batch)
2017-05-09 00:48:56.805837: step 6440, loss = 1.05 (952.2 examples/sec; 0.134 sec/batch)
2017-05-09 00:48:58.176150: step 6450, loss = 0.90 (934.1 examples/sec; 0.137 sec/batch)
2017-05-09 00:48:59.561200: step 6460, loss = 1.15 (924.2 examples/sec; 0.139 sec/batch)
2017-05-09 00:49:00.922556: step 6470, loss = 0.98 (940.2 examples/sec; 0.136 sec/batch)
2017-05-09 00:49:02.261254: step 6480, loss = 0.85 (956.2 examples/sec; 0.134 sec/batch)
2017-05-09 00:49:03.608082: step 6490, loss = 1.01 (950.4 examples/sec; 0.135 sec/batch)
2017-05-09 00:49:05.079758: step 6500, loss = 1.06 (869.8 examples/sec; 0.147 sec/batch)
2017-05-09 00:49:06.302391: step 6510, loss = 0.92 (1046.9 examples/sec; 0.122 sec/batch)
2017-05-09 00:49:07.674783: step 6520, loss = 1.17 (932.7 examples/sec; 0.137 sec/batch)
2017-05-09 00:49:09.033593: step 6530, loss = 0.97 (942.0 examples/sec; 0.136 sec/batch)
2017-05-09 00:49:10.381381: step 6540, loss = 1.02 (949.7 examples/sec; 0.135 sec/batch)
2017-05-09 00:49:11.731111: step 6550, loss = 1.12 (948.3 examples/sec; 0.135 sec/batch)
2017-05-09 00:49:13.050558: step 6560, loss = 1.16 (970.1 examples/sec; 0.132 sec/batch)
2017-05-09 00:49:14.405262: step 6570, loss = 1.08 (944.9 examples/sec; 0.135 sec/batch)
2017-05-09 00:49:15.775269: step 6580, loss = 1.00 (934.3 examples/sec; 0.137 sec/batch)
2017-05-09 00:49:17.129878: step 6590, loss = 0.96 (944.9 examples/sec; 0.135 sec/batch)
2017-05-09 00:49:18.547529: step 6600, loss = 0.97 (902.9 examples/sec; 0.142 sec/batch)
2017-05-09 00:49:19.739040: step 6610, loss = 1.18 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-09 00:49:21.002960: step 6620, loss = 1.09 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-09 00:49:22.294842: step 6630, loss = 1.00 (990.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:49:23.579093: step 6640, loss = 1.02 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:49:24.853415: step 6650, loss = 0.99 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-09 00:49:26.129381: step 6660, loss = 1.15 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:49:27.421023: step 6670, loss = 1.00 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:49:28.708666: step 6680, loss = 1.00 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 00:49:29.996399: step 6690, loss = 0.92 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:49:31.380841: step 6700, loss = 0.93 (924.6 examples/sec; 0.138 sec/batch)
2017-05-09 00:49:32.562130: step 6710, loss = 1.01 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-09 00:49:33.817495: step 6720, loss = 1.04 (1019.6 examples/sec; 0.126 sec/batch)
2017-05-09 00:49:35.111300: step 6730, loss = 1.27 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 00:49:36.425467: step 6740, loss = 0.99 (974.0 examples/sec; 0.131 sec/batch)
2017-05-09 00:49:37.731186: step 6750, loss = 1.62 (980.3 examples/sec; 0.131 sec/batch)
2017-05-09 00:49:38.996991: step 6760, loss = 1.11 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-09 00:49:40.286370: step 6770, loss = 0.98 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 00:49:41.547975: step 6780, loss = 1.03 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-09 00:49:42.849955: step 6790, loss = 1.13 (983.1 examples/sec; 0.130 sec/batch)
2017-05-09 00:49:44.234377: step 6800, loss = 1.03 (924.6 examples/sec; 0.138 sec/batch)
2017-05-09 00:49:45.424038: step 6810, loss = 1.02 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-09 00:49:46.725507: step 6820, loss = 1.13 (983.5 examples/sec; 0.130 sec/batch)
2017-05-09 00:49:48.019388: step 6830, loss = 0.87 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 00:49:49.307722: step 6840, loss = 1.05 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 00:49:50.586438: step 6850, loss = 0.88 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:49:51.859894: step 6860, loss = 0.99 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-09 00:49:53.142266: step 6870, loss = 0.98 (998.2 examples/sec; 0.128 sec/batE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 144 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
ch)
2017-05-09 00:49:54.417219: step 6880, loss = 0.95 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-09 00:49:55.714371: step 6890, loss = 0.87 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 00:49:57.111772: step 6900, loss = 1.13 (916.0 examples/sec; 0.140 sec/batch)
2017-05-09 00:49:58.315200: step 6910, loss = 1.22 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-09 00:49:59.610214: step 6920, loss = 1.16 (988.4 examples/sec; 0.130 sec/batch)
2017-05-09 00:50:00.941468: step 6930, loss = 1.09 (961.5 examples/sec; 0.133 sec/batch)
2017-05-09 00:50:02.241650: step 6940, loss = 1.05 (984.5 examples/sec; 0.130 sec/batch)
2017-05-09 00:50:03.518150: step 6950, loss = 1.05 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:50:04.797284: step 6960, loss = 0.89 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:50:06.069048: step 6970, loss = 1.24 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-09 00:50:07.371204: step 6980, loss = 1.10 (983.0 examples/sec; 0.130 sec/batch)
2017-05-09 00:50:08.658263: step 6990, loss = 0.98 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 00:50:10.018767: step 7000, loss = 1.16 (940.8 examples/sec; 0.136 sec/batch)
2017-05-09 00:50:11.212928: step 7010, loss = 0.86 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-09 00:50:12.493459: step 7020, loss = 0.84 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 00:50:13.786037: step 7030, loss = 1.17 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 00:50:15.092564: step 7040, loss = 1.01 (979.7 examples/sec; 0.131 sec/batch)
2017-05-09 00:50:16.403775: step 7050, loss = 0.90 (976.2 examples/sec; 0.131 sec/batch)
2017-05-09 00:50:17.705539: step 7060, loss = 1.07 (983.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:50:19.009278: step 7070, loss = 1.08 (981.8 examples/sec; 0.130 sec/batch)
2017-05-09 00:50:20.298071: step 7080, loss = 0.75 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 00:50:21.562402: step 7090, loss = 1.01 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-09 00:50:22.935709: step 7100, loss = 0.97 (932.1 examples/sec; 0.137 sec/batch)
2017-05-09 00:50:24.121072: step 7110, loss = 0.90 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-09 00:50:25.437651: step 7120, loss = 1.32 (972.2 examples/sec; 0.132 sec/batch)
2017-05-09 00:50:26.720046: step 7130, loss = 1.29 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:50:28.011925: step 7140, loss = 0.91 (990.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:50:29.335140: step 7150, loss = 0.79 (967.3 examples/sec; 0.132 sec/batch)
2017-05-09 00:50:30.635065: step 7160, loss = 0.84 (984.7 examples/sec; 0.130 sec/batch)
2017-05-09 00:50:31.937976: step 7170, loss = 1.03 (982.4 examples/sec; 0.130 sec/batch)
2017-05-09 00:50:33.251047: step 7180, loss = 0.91 (974.8 examples/sec; 0.131 sec/batch)
2017-05-09 00:50:34.505442: step 7190, loss = 0.83 (1020.4 examples/sec; 0.125 sec/batch)
2017-05-09 00:50:35.890615: step 7200, loss = 0.96 (924.1 examples/sec; 0.139 sec/batch)
2017-05-09 00:50:37.077285: step 7210, loss = 1.07 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-09 00:50:38.380968: step 7220, loss = 0.95 (981.8 examples/sec; 0.130 sec/batch)
2017-05-09 00:50:39.662009: step 7230, loss = 1.07 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:50:40.942494: step 7240, loss = 0.90 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 00:50:42.212589: step 7250, loss = 0.96 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-09 00:50:43.518171: step 7260, loss = 0.95 (980.4 examples/sec; 0.131 sec/batch)
2017-05-09 00:50:44.794194: step 7270, loss = 1.08 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:50:46.091010: step 7280, loss = 0.93 (987.0 examples/sec; 0.130 sec/batch)
2017-05-09 00:50:47.352741: step 7290, loss = 0.77 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-09 00:50:48.707233: step 7300, loss = 1.15 (945.0 examples/sec; 0.135 sec/batch)
2017-05-09 00:50:49.882299: step 7310, loss = 1.09 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-09 00:50:51.158518: step 7320, loss = 1.26 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-09 00:50:52.447583: step 7330, loss = 0.91 (993.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:50:53.729680: step 7340, loss = 1.04 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:50:55.031640: step 7350, loss = 1.07 (983.1 examples/sec; 0.130 sec/batch)
2017-05-09 00:50:56.333057: step 7360, loss = 0.97 (983.5 examples/sec; 0.130 sec/batch)
2017-05-09 00:50:57.622377: step 7370, loss = 0.92 (992.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:50:58.945793: step 7380, loss = 0.98 (967.2 examples/sec; 0.132 sec/batch)
2017-05-09 00:51:00.228548: step 7390, loss = 1.19 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 00:51:01.605915: step 7400, loss = 0.89 (929.3 examples/sec; 0.138 sec/batch)
2017-05-09 00:51:02.793447: step 7410, loss = 1.01 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-09 00:51:04.117769: step 7420, loss = 1.04 (966.5 examples/sec; 0.132 sec/batch)
2017-05-09 00:51:05.399400: step 7430, loss = 0.91 (998.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:51:06.671038: step 7440, loss = 1.06 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-09 00:51:07.939449: step 7450, loss = 0.95 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-09 00:51:09.210212: step 7460, loss = 0.95 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-09 00:51:10.497857: step 7470, loss = 1.18 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 00:51:11.802770: step 7480, loss = 0.88 (980.9 examples/sec; 0.130 sec/batch)
2017-05-09 00:51:13.095615: step 7490, loss = 0.99 (990.1 examples/sec; 0.129 sec/batch)
2017-05-09 00:51:14.476262: step 7500, loss = 1.02 (927.1 examples/sec; 0.138 sec/batch)
2017-05-09 00:51:15.674150: step 7510, loss = 1.02 (1068.5 examples/sec; 0.120 sec/batch)
2017-05-09 00:51:17.007793: step 7520, loss = 0.97 (959.8 examples/sec; 0.133 sec/batch)
2017-05-09 00:51:18.281310: step 7530, loss = 1.07 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-09 00:51:19.568193: step 7540, loss = 1.15 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 00:51:20.894195: step 7550, loss = 1.08 (965.3 examples/sec; 0.133 sec/batch)
2017-05-09 00:51:22.168552: step 7560, loss = 1.01 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-09 00:51:23.451749: step 7570, loss = 0.77 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:51:24.760915: step 7580, loss = 1.00 (977.7 examples/sec; 0.131 sec/batch)
2017-05-09 00:51:26.047438: step 7590, loss = 0.89 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:51:27.450969: step 7600, loss = 1.46 (912.0 examples/sec; 0.140 sec/batch)
2017-05-09 00:51:28.641023: step 7610, loss = 0.98 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-09 00:51:29.926496: step 7620, loss = 0.96 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 00:51:31.218839: step 7630, loss = 0.85 (990.5 examples/sec; 0.129 sec/batch)
2017-05-09 00:51:32.502684: step 7640, loss = 1.11 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:51:33.799372: step 7650, loss = 0.94 (987.1 examples/sec; 0.130 sec/batch)
2017-05-09 00:51:35.088587: step 7660, loss = 1.07 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:51:36.400665: step 7670, loss = 0.86 (975.6 examples/sec; 0.131 sec/batch)
2017-05-09 00:51:37.712944: step 7680, loss = 1.19 (975.4 examples/sec; 0.131 sec/batch)
2017-05-09 00:51:38.994141: step 7690, loss = 0.84 (999.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:51:40.362409: step 7700, loss = 1.00 (935.5 examples/sec; 0.137 sec/batch)
2017-05-09 00:51:41.569823: step 7710, loss = 0.91 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-09 00:51:42.858064: step 7720, loss = 1.01 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 00:51:44.185374: step 7730, loss = 0.89 (964.4 examples/sec; 0.133 sec/batch)
2017-05-09 00:51:45.449812: step 7740, loss = 0.91 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-09 00:51:46.757075: step 7750, loss = 0.85 (979.1 examples/sec; 0.131 sec/batch)
2017-05-09 00:51:48.056126: step 7760, loss = 1.10 (985.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:51:49.331261: step 7770, loss = 0.85 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 00:51:50.602294: step 7780, loss = 1.09 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-09 00:51:51.889726: step 7790, loss = 0.89 (994.2 examE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 164 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
ples/sec; 0.129 sec/batch)
2017-05-09 00:51:53.268051: step 7800, loss = 1.06 (928.7 examples/sec; 0.138 sec/batch)
2017-05-09 00:51:54.486523: step 7810, loss = 0.84 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-09 00:51:55.799412: step 7820, loss = 1.05 (974.9 examples/sec; 0.131 sec/batch)
2017-05-09 00:51:57.105819: step 7830, loss = 0.88 (979.8 examples/sec; 0.131 sec/batch)
2017-05-09 00:51:58.374945: step 7840, loss = 0.93 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-09 00:51:59.645444: step 7850, loss = 0.96 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-09 00:52:00.915778: step 7860, loss = 0.83 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-09 00:52:02.203209: step 7870, loss = 0.97 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 00:52:03.508977: step 7880, loss = 0.97 (980.3 examples/sec; 0.131 sec/batch)
2017-05-09 00:52:04.811831: step 7890, loss = 1.12 (982.5 examples/sec; 0.130 sec/batch)
2017-05-09 00:52:06.196686: step 7900, loss = 1.13 (924.3 examples/sec; 0.138 sec/batch)
2017-05-09 00:52:07.399789: step 7910, loss = 1.02 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-09 00:52:08.675822: step 7920, loss = 0.91 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:52:09.981421: step 7930, loss = 0.95 (980.4 examples/sec; 0.131 sec/batch)
2017-05-09 00:52:11.262655: step 7940, loss = 1.05 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:52:12.571111: step 7950, loss = 0.98 (978.3 examples/sec; 0.131 sec/batch)
2017-05-09 00:52:13.875280: step 7960, loss = 1.19 (981.5 examples/sec; 0.130 sec/batch)
2017-05-09 00:52:15.172845: step 7970, loss = 1.20 (986.5 examples/sec; 0.130 sec/batch)
2017-05-09 00:52:16.490903: step 7980, loss = 0.97 (971.1 examples/sec; 0.132 sec/batch)
2017-05-09 00:52:17.791364: step 7990, loss = 1.10 (984.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:52:19.189030: step 8000, loss = 1.08 (915.8 examples/sec; 0.140 sec/batch)
2017-05-09 00:52:20.416555: step 8010, loss = 1.16 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-09 00:52:21.724188: step 8020, loss = 0.89 (978.9 examples/sec; 0.131 sec/batch)
2017-05-09 00:52:23.014733: step 8030, loss = 0.80 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:52:24.297379: step 8040, loss = 0.82 (997.9 examples/sec; 0.128 sec/batch)
2017-05-09 00:52:25.575668: step 8050, loss = 1.07 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 00:52:26.880285: step 8060, loss = 0.74 (981.1 examples/sec; 0.130 sec/batch)
2017-05-09 00:52:28.166971: step 8070, loss = 0.94 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:52:29.491093: step 8080, loss = 1.12 (966.7 examples/sec; 0.132 sec/batch)
2017-05-09 00:52:30.778363: step 8090, loss = 1.05 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 00:52:32.246519: step 8100, loss = 1.01 (871.8 examples/sec; 0.147 sec/batch)
2017-05-09 00:52:33.476119: step 8110, loss = 0.88 (1041.0 examples/sec; 0.123 sec/batch)
2017-05-09 00:52:34.802114: step 8120, loss = 1.06 (965.3 examples/sec; 0.133 sec/batch)
2017-05-09 00:52:36.105145: step 8130, loss = 0.90 (982.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:52:37.401531: step 8140, loss = 0.77 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 00:52:38.679524: step 8150, loss = 0.98 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 00:52:39.955847: step 8160, loss = 0.96 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-09 00:52:41.232703: step 8170, loss = 0.89 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:52:42.507710: step 8180, loss = 1.28 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-09 00:52:43.817277: step 8190, loss = 1.03 (977.4 examples/sec; 0.131 sec/batch)
2017-05-09 00:52:45.190711: step 8200, loss = 1.06 (932.0 examples/sec; 0.137 sec/batch)
2017-05-09 00:52:46.409077: step 8210, loss = 1.11 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-09 00:52:47.670655: step 8220, loss = 1.07 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-09 00:52:48.960817: step 8230, loss = 1.01 (992.1 examples/sec; 0.129 sec/batch)
2017-05-09 00:52:50.283428: step 8240, loss = 0.96 (967.8 examples/sec; 0.132 sec/batch)
2017-05-09 00:52:51.646438: step 8250, loss = 1.00 (939.1 examples/sec; 0.136 sec/batch)
2017-05-09 00:52:52.991607: step 8260, loss = 0.92 (951.6 examples/sec; 0.135 sec/batch)
2017-05-09 00:52:54.330404: step 8270, loss = 1.13 (956.1 examples/sec; 0.134 sec/batch)
2017-05-09 00:52:55.692236: step 8280, loss = 0.91 (939.9 examples/sec; 0.136 sec/batch)
2017-05-09 00:52:57.077716: step 8290, loss = 1.05 (923.9 examples/sec; 0.139 sec/batch)
2017-05-09 00:52:58.540828: step 8300, loss = 1.02 (874.8 examples/sec; 0.146 sec/batch)
2017-05-09 00:52:59.796118: step 8310, loss = 0.96 (1019.7 examples/sec; 0.126 sec/batch)
2017-05-09 00:53:01.085734: step 8320, loss = 0.90 (992.5 examples/sec; 0.129 sec/batch)
2017-05-09 00:53:02.372622: step 8330, loss = 0.81 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 00:53:03.662021: step 8340, loss = 0.92 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 00:53:04.973556: step 8350, loss = 1.07 (976.0 examples/sec; 0.131 sec/batch)
2017-05-09 00:53:06.318442: step 8360, loss = 0.88 (951.8 examples/sec; 0.134 sec/batch)
2017-05-09 00:53:07.685875: step 8370, loss = 0.90 (936.1 examples/sec; 0.137 sec/batch)
2017-05-09 00:53:09.045574: step 8380, loss = 1.09 (941.4 examples/sec; 0.136 sec/batch)
2017-05-09 00:53:10.429079: step 8390, loss = 1.01 (925.2 examples/sec; 0.138 sec/batch)
2017-05-09 00:53:11.872829: step 8400, loss = 0.96 (886.6 examples/sec; 0.144 sec/batch)
2017-05-09 00:53:13.172288: step 8410, loss = 0.93 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 00:53:14.465585: step 8420, loss = 0.81 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 00:53:15.820054: step 8430, loss = 0.83 (945.0 examples/sec; 0.135 sec/batch)
2017-05-09 00:53:17.193536: step 8440, loss = 1.02 (931.9 examples/sec; 0.137 sec/batch)
2017-05-09 00:53:18.568434: step 8450, loss = 0.98 (931.0 examples/sec; 0.137 sec/batch)
2017-05-09 00:53:19.870110: step 8460, loss = 1.15 (983.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:53:21.174040: step 8470, loss = 1.07 (981.6 examples/sec; 0.130 sec/batch)
2017-05-09 00:53:22.448784: step 8480, loss = 0.92 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-09 00:53:23.774175: step 8490, loss = 1.05 (965.7 examples/sec; 0.133 sec/batch)
2017-05-09 00:53:25.177577: step 8500, loss = 0.79 (912.1 examples/sec; 0.140 sec/batch)
2017-05-09 00:53:26.397241: step 8510, loss = 0.99 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-09 00:53:27.698197: step 8520, loss = 0.91 (983.9 examples/sec; 0.130 sec/batch)
2017-05-09 00:53:28.983001: step 8530, loss = 0.91 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 00:53:30.245615: step 8540, loss = 1.09 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-09 00:53:31.530665: step 8550, loss = 0.96 (996.1 examples/sec; 0.129 sec/batch)
2017-05-09 00:53:32.824668: step 8560, loss = 0.98 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 00:53:34.083672: step 8570, loss = 0.99 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-09 00:53:35.382401: step 8580, loss = 0.91 (985.6 examples/sec; 0.130 sec/batch)
2017-05-09 00:53:36.690699: step 8590, loss = 1.14 (978.4 examples/sec; 0.131 sec/batch)
2017-05-09 00:53:38.078569: step 8600, loss = 0.99 (922.3 examples/sec; 0.139 sec/batch)
2017-05-09 00:53:39.258971: step 8610, loss = 0.87 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-09 00:53:40.537050: step 8620, loss = 1.08 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:53:41.819732: step 8630, loss = 1.06 (997.9 examples/sec; 0.128 sec/batch)
2017-05-09 00:53:43.140615: step 8640, loss = 1.05 (969.0 examples/sec; 0.132 sec/batch)
2017-05-09 00:53:44.429811: step 8650, loss = 0.93 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:53:45.745515: step 8660, loss = 0.95 (972.9 examples/sec; 0.132 sec/batch)
2017-05-09 00:53:47.059572: step 8670, loss = 1.01 (974.1 examples/sec; 0.131 sec/batch)
2017-05-09 00:53:48.343433: step 8680, loss = 0.92 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:53:49.609999: step 8690, loss = 1.05 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-09 00:53:50.990051: step 8700, loss = 0.96 (927.5 examples/sec; 0.138 sec/batch)
2017-05-09 00:53:52.164882: step 8710, lossE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 184 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
 = 1.05 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-09 00:53:53.445197: step 8720, loss = 1.04 (999.8 examples/sec; 0.128 sec/batch)
2017-05-09 00:53:54.725356: step 8730, loss = 0.99 (999.9 examples/sec; 0.128 sec/batch)
2017-05-09 00:53:56.001620: step 8740, loss = 0.96 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-09 00:53:57.279425: step 8750, loss = 1.08 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:53:58.546886: step 8760, loss = 0.95 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-09 00:53:59.854032: step 8770, loss = 0.96 (979.2 examples/sec; 0.131 sec/batch)
2017-05-09 00:54:01.128477: step 8780, loss = 0.96 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-09 00:54:02.404394: step 8790, loss = 0.87 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:54:03.813786: step 8800, loss = 0.94 (908.2 examples/sec; 0.141 sec/batch)
2017-05-09 00:54:05.013423: step 8810, loss = 0.90 (1067.0 examples/sec; 0.120 sec/batch)
2017-05-09 00:54:06.319595: step 8820, loss = 1.02 (980.0 examples/sec; 0.131 sec/batch)
2017-05-09 00:54:07.605817: step 8830, loss = 1.10 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 00:54:08.924085: step 8840, loss = 1.01 (971.0 examples/sec; 0.132 sec/batch)
2017-05-09 00:54:10.238661: step 8850, loss = 1.03 (973.7 examples/sec; 0.131 sec/batch)
2017-05-09 00:54:11.527110: step 8860, loss = 1.32 (993.4 examples/sec; 0.129 sec/batch)
2017-05-09 00:54:12.811913: step 8870, loss = 0.75 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 00:54:14.118496: step 8880, loss = 1.23 (979.7 examples/sec; 0.131 sec/batch)
2017-05-09 00:54:15.406050: step 8890, loss = 1.11 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 00:54:16.819211: step 8900, loss = 0.94 (905.8 examples/sec; 0.141 sec/batch)
2017-05-09 00:54:17.997765: step 8910, loss = 1.18 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-09 00:54:19.308727: step 8920, loss = 1.20 (976.4 examples/sec; 0.131 sec/batch)
2017-05-09 00:54:20.602464: step 8930, loss = 0.90 (989.4 examples/sec; 0.129 sec/batch)
2017-05-09 00:54:21.912773: step 8940, loss = 0.87 (976.9 examples/sec; 0.131 sec/batch)
2017-05-09 00:54:23.226347: step 8950, loss = 0.92 (974.4 examples/sec; 0.131 sec/batch)
2017-05-09 00:54:24.509277: step 8960, loss = 0.86 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:54:25.779385: step 8970, loss = 0.92 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-09 00:54:27.056069: step 8980, loss = 0.94 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-09 00:54:28.359174: step 8990, loss = 0.77 (982.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:54:29.734270: step 9000, loss = 0.91 (930.8 examples/sec; 0.138 sec/batch)
2017-05-09 00:54:30.959043: step 9010, loss = 0.82 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-09 00:54:32.286505: step 9020, loss = 0.89 (964.3 examples/sec; 0.133 sec/batch)
2017-05-09 00:54:33.618827: step 9030, loss = 0.98 (960.7 examples/sec; 0.133 sec/batch)
2017-05-09 00:54:34.940983: step 9040, loss = 0.90 (968.1 examples/sec; 0.132 sec/batch)
2017-05-09 00:54:36.266143: step 9050, loss = 1.13 (965.9 examples/sec; 0.133 sec/batch)
2017-05-09 00:54:37.581939: step 9060, loss = 1.10 (972.8 examples/sec; 0.132 sec/batch)
2017-05-09 00:54:38.910198: step 9070, loss = 0.88 (963.7 examples/sec; 0.133 sec/batch)
2017-05-09 00:54:40.251179: step 9080, loss = 0.85 (954.5 examples/sec; 0.134 sec/batch)
2017-05-09 00:54:41.571754: step 9090, loss = 1.00 (969.3 examples/sec; 0.132 sec/batch)
2017-05-09 00:54:43.090928: step 9100, loss = 1.07 (842.6 examples/sec; 0.152 sec/batch)
2017-05-09 00:54:44.244626: step 9110, loss = 1.00 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-09 00:54:45.590341: step 9120, loss = 1.12 (951.2 examples/sec; 0.135 sec/batch)
2017-05-09 00:54:46.893696: step 9130, loss = 1.28 (982.1 examples/sec; 0.130 sec/batch)
2017-05-09 00:54:48.194615: step 9140, loss = 1.03 (983.9 examples/sec; 0.130 sec/batch)
2017-05-09 00:54:49.514643: step 9150, loss = 0.85 (969.7 examples/sec; 0.132 sec/batch)
2017-05-09 00:54:50.807577: step 9160, loss = 0.92 (990.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:54:52.108229: step 9170, loss = 0.91 (984.1 examples/sec; 0.130 sec/batch)
2017-05-09 00:54:53.396136: step 9180, loss = 0.99 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:54:54.676655: step 9190, loss = 0.76 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 00:54:56.087794: step 9200, loss = 0.94 (907.1 examples/sec; 0.141 sec/batch)
2017-05-09 00:54:57.290392: step 9210, loss = 1.08 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-09 00:54:58.590541: step 9220, loss = 0.78 (984.5 examples/sec; 0.130 sec/batch)
2017-05-09 00:54:59.867852: step 9230, loss = 0.83 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:55:01.150608: step 9240, loss = 1.20 (997.9 examples/sec; 0.128 sec/batch)
2017-05-09 00:55:02.446116: step 9250, loss = 0.90 (988.0 examples/sec; 0.130 sec/batch)
2017-05-09 00:55:03.759118: step 9260, loss = 1.07 (974.9 examples/sec; 0.131 sec/batch)
2017-05-09 00:55:05.060730: step 9270, loss = 0.94 (983.4 examples/sec; 0.130 sec/batch)
2017-05-09 00:55:06.355064: step 9280, loss = 1.17 (988.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:55:07.668385: step 9290, loss = 0.89 (974.6 examples/sec; 0.131 sec/batch)
2017-05-09 00:55:09.044194: step 9300, loss = 1.01 (930.4 examples/sec; 0.138 sec/batch)
2017-05-09 00:55:10.247806: step 9310, loss = 0.90 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-09 00:55:11.519619: step 9320, loss = 0.96 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-09 00:55:12.801744: step 9330, loss = 1.00 (998.3 examples/sec; 0.128 sec/batch)
2017-05-09 00:55:14.080473: step 9340, loss = 0.97 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:55:15.367997: step 9350, loss = 0.90 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 00:55:16.644280: step 9360, loss = 0.99 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-09 00:55:17.931973: step 9370, loss = 0.84 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:55:19.198745: step 9380, loss = 0.87 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-09 00:55:20.469933: step 9390, loss = 0.86 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-09 00:55:21.854218: step 9400, loss = 0.87 (924.7 examples/sec; 0.138 sec/batch)
2017-05-09 00:55:23.088659: step 9410, loss = 1.00 (1036.9 examples/sec; 0.123 sec/batch)
2017-05-09 00:55:24.391092: step 9420, loss = 0.98 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 00:55:25.692309: step 9430, loss = 0.85 (983.7 examples/sec; 0.130 sec/batch)
2017-05-09 00:55:27.001925: step 9440, loss = 0.95 (977.4 examples/sec; 0.131 sec/batch)
2017-05-09 00:55:28.275983: step 9450, loss = 0.82 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-09 00:55:29.593144: step 9460, loss = 0.97 (971.8 examples/sec; 0.132 sec/batch)
2017-05-09 00:55:30.891007: step 9470, loss = 0.99 (986.2 examples/sec; 0.130 sec/batch)
2017-05-09 00:55:32.176861: step 9480, loss = 0.99 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 00:55:33.467488: step 9490, loss = 1.04 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:55:34.839221: step 9500, loss = 0.97 (933.1 examples/sec; 0.137 sec/batch)
2017-05-09 00:55:36.007090: step 9510, loss = 0.94 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-09 00:55:37.285732: step 9520, loss = 0.99 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:55:38.547489: step 9530, loss = 0.84 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-09 00:55:39.825353: step 9540, loss = 1.05 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:55:41.095974: step 9550, loss = 0.89 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-09 00:55:42.354218: step 9560, loss = 0.96 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-09 00:55:43.620916: step 9570, loss = 0.97 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-09 00:55:44.905880: step 9580, loss = 1.13 (996.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:55:46.198676: step 9590, loss = 1.20 (990.1 examples/sec; 0.129 sec/batch)
2017-05-09 00:55:47.596979: step 9600, loss = 1.07 (915.4 examples/sec; 0.140 sec/batch)
2017-05-09 00:55:48.801936: step 9610, loss = 0.91 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-09 00:55:50.103719: step 9620, loss = 0.87 (983.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:55:E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 205 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
51.384213: step 9630, loss = 1.18 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 00:55:52.674191: step 9640, loss = 0.86 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 00:55:53.975751: step 9650, loss = 1.18 (983.4 examples/sec; 0.130 sec/batch)
2017-05-09 00:55:55.268893: step 9660, loss = 1.11 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:55:56.523838: step 9670, loss = 1.06 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-09 00:55:57.804641: step 9680, loss = 0.98 (999.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:55:59.072219: step 9690, loss = 0.92 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-09 00:56:00.438665: step 9700, loss = 1.02 (936.7 examples/sec; 0.137 sec/batch)
2017-05-09 00:56:01.604292: step 9710, loss = 0.88 (1098.1 examples/sec; 0.117 sec/batch)
2017-05-09 00:56:02.878175: step 9720, loss = 1.01 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-09 00:56:04.157229: step 9730, loss = 0.88 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:56:05.435294: step 9740, loss = 1.04 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-09 00:56:06.708783: step 9750, loss = 0.85 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-09 00:56:07.983049: step 9760, loss = 1.03 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-09 00:56:09.293337: step 9770, loss = 0.81 (976.9 examples/sec; 0.131 sec/batch)
2017-05-09 00:56:10.596917: step 9780, loss = 1.15 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 00:56:11.883385: step 9790, loss = 0.94 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:56:13.267496: step 9800, loss = 0.94 (924.8 examples/sec; 0.138 sec/batch)
2017-05-09 00:56:14.479593: step 9810, loss = 0.96 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-09 00:56:15.776044: step 9820, loss = 0.96 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:56:17.078644: step 9830, loss = 1.16 (982.6 examples/sec; 0.130 sec/batch)
2017-05-09 00:56:18.372514: step 9840, loss = 0.98 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 00:56:19.653959: step 9850, loss = 1.00 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 00:56:20.945557: step 9860, loss = 1.05 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:56:22.204244: step 9870, loss = 0.98 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-09 00:56:23.494406: step 9880, loss = 0.91 (992.1 examples/sec; 0.129 sec/batch)
2017-05-09 00:56:24.772738: step 9890, loss = 0.88 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 00:56:26.162220: step 9900, loss = 0.85 (921.2 examples/sec; 0.139 sec/batch)
2017-05-09 00:56:27.385065: step 9910, loss = 0.94 (1046.7 examples/sec; 0.122 sec/batch)
2017-05-09 00:56:28.662988: step 9920, loss = 1.01 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 00:56:29.920147: step 9930, loss = 0.94 (1018.2 examples/sec; 0.126 sec/batch)
2017-05-09 00:56:31.199732: step 9940, loss = 0.85 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-09 00:56:32.466022: step 9950, loss = 0.93 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-09 00:56:33.733898: step 9960, loss = 1.09 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-09 00:56:35.015436: step 9970, loss = 0.92 (998.8 examples/sec; 0.128 sec/batch)
2017-05-09 00:56:36.295788: step 9980, loss = 0.92 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:56:37.563751: step 9990, loss = 0.98 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-09 00:56:38.953276: step 10000, loss = 1.18 (921.2 examples/sec; 0.139 sec/batch)
2017-05-09 00:56:40.163350: step 10010, loss = 1.37 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-09 00:56:41.468429: step 10020, loss = 1.10 (980.8 examples/sec; 0.131 sec/batch)
2017-05-09 00:56:42.741027: step 10030, loss = 0.98 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-09 00:56:44.009531: step 10040, loss = 0.85 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-09 00:56:45.325115: step 10050, loss = 0.93 (973.0 examples/sec; 0.132 sec/batch)
2017-05-09 00:56:46.621898: step 10060, loss = 0.98 (987.1 examples/sec; 0.130 sec/batch)
2017-05-09 00:56:47.905597: step 10070, loss = 1.14 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:56:49.213633: step 10080, loss = 0.85 (978.6 examples/sec; 0.131 sec/batch)
2017-05-09 00:56:50.480367: step 10090, loss = 1.04 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-09 00:56:51.877233: step 10100, loss = 1.00 (916.3 examples/sec; 0.140 sec/batch)
2017-05-09 00:56:53.079667: step 10110, loss = 0.93 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-09 00:56:54.357442: step 10120, loss = 1.20 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:56:55.612848: step 10130, loss = 1.01 (1019.6 examples/sec; 0.126 sec/batch)
2017-05-09 00:56:56.898823: step 10140, loss = 0.87 (995.3 examples/sec; 0.129 sec/batch)
2017-05-09 00:56:58.174594: step 10150, loss = 0.91 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-09 00:56:59.480562: step 10160, loss = 0.90 (980.1 examples/sec; 0.131 sec/batch)
2017-05-09 00:57:00.756786: step 10170, loss = 1.14 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:57:02.050232: step 10180, loss = 0.90 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 00:57:03.335671: step 10190, loss = 0.81 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:57:04.725612: step 10200, loss = 1.04 (920.9 examples/sec; 0.139 sec/batch)
2017-05-09 00:57:05.906549: step 10210, loss = 1.10 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-09 00:57:07.174491: step 10220, loss = 0.94 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-09 00:57:08.492349: step 10230, loss = 0.94 (971.3 examples/sec; 0.132 sec/batch)
2017-05-09 00:57:09.768965: step 10240, loss = 1.02 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-09 00:57:11.068704: step 10250, loss = 0.99 (984.8 examples/sec; 0.130 sec/batch)
2017-05-09 00:57:12.391085: step 10260, loss = 0.92 (968.0 examples/sec; 0.132 sec/batch)
2017-05-09 00:57:13.690548: step 10270, loss = 1.01 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 00:57:14.962920: step 10280, loss = 1.03 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 00:57:16.234147: step 10290, loss = 0.90 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-09 00:57:17.590146: step 10300, loss = 0.80 (944.0 examples/sec; 0.136 sec/batch)
2017-05-09 00:57:18.777777: step 10310, loss = 0.96 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-09 00:57:20.059462: step 10320, loss = 1.11 (998.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:57:21.334565: step 10330, loss = 1.12 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 00:57:22.613347: step 10340, loss = 0.99 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:57:23.919605: step 10350, loss = 1.08 (979.9 examples/sec; 0.131 sec/batch)
2017-05-09 00:57:25.214586: step 10360, loss = 0.85 (988.4 examples/sec; 0.129 sec/batch)
2017-05-09 00:57:26.513765: step 10370, loss = 1.08 (985.2 examples/sec; 0.130 sec/batch)
2017-05-09 00:57:27.829075: step 10380, loss = 1.00 (973.2 examples/sec; 0.132 sec/batch)
2017-05-09 00:57:29.127161: step 10390, loss = 0.98 (986.1 examples/sec; 0.130 sec/batch)
2017-05-09 00:57:30.522877: step 10400, loss = 1.05 (917.1 examples/sec; 0.140 sec/batch)
2017-05-09 00:57:31.683707: step 10410, loss = 0.91 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-09 00:57:32.957283: step 10420, loss = 0.91 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-09 00:57:34.231519: step 10430, loss = 0.89 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-09 00:57:35.496757: step 10440, loss = 0.81 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-09 00:57:36.782099: step 10450, loss = 0.86 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 00:57:38.045862: step 10460, loss = 0.81 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-09 00:57:39.341187: step 10470, loss = 0.75 (988.2 examples/sec; 0.130 sec/batch)
2017-05-09 00:57:40.658028: step 10480, loss = 0.87 (972.0 examples/sec; 0.132 sec/batch)
2017-05-09 00:57:41.928269: step 10490, loss = 0.91 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-09 00:57:43.396859: step 10500, loss = 0.97 (871.6 examples/sec; 0.147 sec/batch)
2017-05-09 00:57:44.593506: step 10510, loss = 0.88 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-09 00:57:45.908619: step 10520, loss = 1.13 (973.3 examples/sec; 0.132 sec/batch)
2017-05-09 00:57:47.236197: step 10530, loss = 1.03 (964.2 examples/sec; 0.133 sec/batch)
2017-05-09 00:E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 225 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
57:48.574388: step 10540, loss = 0.97 (956.5 examples/sec; 0.134 sec/batch)
2017-05-09 00:57:49.917926: step 10550, loss = 0.83 (952.7 examples/sec; 0.134 sec/batch)
2017-05-09 00:57:51.267918: step 10560, loss = 0.96 (948.2 examples/sec; 0.135 sec/batch)
2017-05-09 00:57:52.580147: step 10570, loss = 0.84 (975.4 examples/sec; 0.131 sec/batch)
2017-05-09 00:57:53.890896: step 10580, loss = 0.94 (976.5 examples/sec; 0.131 sec/batch)
2017-05-09 00:57:55.202445: step 10590, loss = 0.77 (976.0 examples/sec; 0.131 sec/batch)
2017-05-09 00:57:56.653862: step 10600, loss = 1.25 (881.9 examples/sec; 0.145 sec/batch)
2017-05-09 00:57:57.860344: step 10610, loss = 1.13 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-09 00:57:59.181921: step 10620, loss = 0.94 (968.5 examples/sec; 0.132 sec/batch)
2017-05-09 00:58:00.596495: step 10630, loss = 0.85 (904.9 examples/sec; 0.141 sec/batch)
2017-05-09 00:58:01.915247: step 10640, loss = 1.08 (970.6 examples/sec; 0.132 sec/batch)
2017-05-09 00:58:03.233824: step 10650, loss = 1.09 (970.7 examples/sec; 0.132 sec/batch)
2017-05-09 00:58:04.537023: step 10660, loss = 1.12 (982.2 examples/sec; 0.130 sec/batch)
2017-05-09 00:58:05.795851: step 10670, loss = 0.92 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-09 00:58:07.066532: step 10680, loss = 0.87 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-09 00:58:08.398970: step 10690, loss = 1.05 (960.6 examples/sec; 0.133 sec/batch)
2017-05-09 00:58:09.778422: step 10700, loss = 0.98 (927.9 examples/sec; 0.138 sec/batch)
2017-05-09 00:58:10.994236: step 10710, loss = 0.96 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-09 00:58:12.279215: step 10720, loss = 0.90 (996.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:58:13.584685: step 10730, loss = 0.97 (980.5 examples/sec; 0.131 sec/batch)
2017-05-09 00:58:14.872235: step 10740, loss = 0.95 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 00:58:16.154677: step 10750, loss = 0.84 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:58:17.421286: step 10760, loss = 0.93 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-09 00:58:18.703101: step 10770, loss = 1.01 (998.6 examples/sec; 0.128 sec/batch)
2017-05-09 00:58:19.982695: step 10780, loss = 0.82 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-09 00:58:21.245252: step 10790, loss = 0.86 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-09 00:58:22.624879: step 10800, loss = 0.92 (927.8 examples/sec; 0.138 sec/batch)
2017-05-09 00:58:23.807112: step 10810, loss = 0.86 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-09 00:58:25.075815: step 10820, loss = 0.88 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-09 00:58:26.330849: step 10830, loss = 0.80 (1019.9 examples/sec; 0.126 sec/batch)
2017-05-09 00:58:27.612137: step 10840, loss = 1.04 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:58:28.892453: step 10850, loss = 1.24 (999.8 examples/sec; 0.128 sec/batch)
2017-05-09 00:58:30.210180: step 10860, loss = 0.78 (971.4 examples/sec; 0.132 sec/batch)
2017-05-09 00:58:31.489048: step 10870, loss = 1.07 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-09 00:58:32.752294: step 10880, loss = 0.98 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-09 00:58:34.033478: step 10890, loss = 0.86 (999.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:58:35.430006: step 10900, loss = 0.92 (916.6 examples/sec; 0.140 sec/batch)
2017-05-09 00:58:36.607123: step 10910, loss = 1.09 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-09 00:58:37.882377: step 10920, loss = 0.89 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-09 00:58:39.180919: step 10930, loss = 1.03 (985.7 examples/sec; 0.130 sec/batch)
2017-05-09 00:58:40.488807: step 10940, loss = 0.84 (978.7 examples/sec; 0.131 sec/batch)
2017-05-09 00:58:41.783880: step 10950, loss = 1.08 (988.4 examples/sec; 0.130 sec/batch)
2017-05-09 00:58:43.082649: step 10960, loss = 0.97 (985.6 examples/sec; 0.130 sec/batch)
2017-05-09 00:58:44.375983: step 10970, loss = 1.08 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 00:58:45.659508: step 10980, loss = 1.07 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:58:46.931007: step 10990, loss = 0.89 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-09 00:58:48.346780: step 11000, loss = 0.96 (904.1 examples/sec; 0.142 sec/batch)
2017-05-09 00:58:49.550721: step 11010, loss = 1.01 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-09 00:58:50.838592: step 11020, loss = 0.86 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:58:52.139435: step 11030, loss = 0.79 (984.0 examples/sec; 0.130 sec/batch)
2017-05-09 00:58:53.431176: step 11040, loss = 0.71 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 00:58:54.706830: step 11050, loss = 0.96 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-09 00:58:55.982785: step 11060, loss = 1.05 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-09 00:58:57.272870: step 11070, loss = 0.96 (992.2 examples/sec; 0.129 sec/batch)
2017-05-09 00:58:58.578016: step 11080, loss = 0.88 (980.7 examples/sec; 0.131 sec/batch)
2017-05-09 00:58:59.877813: step 11090, loss = 0.88 (984.8 examples/sec; 0.130 sec/batch)
2017-05-09 00:59:01.276246: step 11100, loss = 0.93 (915.3 examples/sec; 0.140 sec/batch)
2017-05-09 00:59:02.462665: step 11110, loss = 1.01 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-09 00:59:03.752892: step 11120, loss = 0.90 (992.1 examples/sec; 0.129 sec/batch)
2017-05-09 00:59:05.081464: step 11130, loss = 1.06 (963.4 examples/sec; 0.133 sec/batch)
2017-05-09 00:59:06.375228: step 11140, loss = 0.86 (989.4 examples/sec; 0.129 sec/batch)
2017-05-09 00:59:07.680985: step 11150, loss = 0.91 (980.3 examples/sec; 0.131 sec/batch)
2017-05-09 00:59:08.979144: step 11160, loss = 0.93 (986.0 examples/sec; 0.130 sec/batch)
2017-05-09 00:59:10.276793: step 11170, loss = 0.96 (986.4 examples/sec; 0.130 sec/batch)
2017-05-09 00:59:11.572696: step 11180, loss = 0.79 (987.7 examples/sec; 0.130 sec/batch)
2017-05-09 00:59:12.870045: step 11190, loss = 1.05 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 00:59:14.262765: step 11200, loss = 0.92 (919.1 examples/sec; 0.139 sec/batch)
2017-05-09 00:59:15.474191: step 11210, loss = 1.13 (1056.6 examples/sec; 0.121 sec/batch)
2017-05-09 00:59:16.778824: step 11220, loss = 1.08 (981.1 examples/sec; 0.130 sec/batch)
2017-05-09 00:59:18.050432: step 11230, loss = 0.96 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-09 00:59:19.321424: step 11240, loss = 0.97 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-09 00:59:20.597403: step 11250, loss = 0.85 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:59:21.851474: step 11260, loss = 0.86 (1020.7 examples/sec; 0.125 sec/batch)
2017-05-09 00:59:23.127560: step 11270, loss = 1.07 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:59:24.401256: step 11280, loss = 0.87 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-09 00:59:25.667078: step 11290, loss = 0.82 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-09 00:59:27.031203: step 11300, loss = 0.88 (938.3 examples/sec; 0.136 sec/batch)
2017-05-09 00:59:28.202458: step 11310, loss = 1.00 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-09 00:59:29.483549: step 11320, loss = 0.81 (999.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:59:30.740773: step 11330, loss = 0.79 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-09 00:59:32.042387: step 11340, loss = 1.01 (983.4 examples/sec; 0.130 sec/batch)
2017-05-09 00:59:33.328877: step 11350, loss = 0.97 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 00:59:34.599524: step 11360, loss = 0.95 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-09 00:59:35.882016: step 11370, loss = 1.00 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 00:59:37.178111: step 11380, loss = 0.85 (987.6 examples/sec; 0.130 sec/batch)
2017-05-09 00:59:38.475990: step 11390, loss = 0.96 (986.2 examples/sec; 0.130 sec/batch)
2017-05-09 00:59:39.859510: step 11400, loss = 0.87 (925.2 examples/sec; 0.138 sec/batch)
2017-05-09 00:59:41.048780: step 11410, loss = 1.10 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-09 00:59:42.334754: step 11420, loss = 1.11 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 00:59:43.666599: step 11430, loss = 1.12 (961.1 examples/sec; 0.133 sec/batch)
2017-05-09 00:59:44.928690: step 11440, loss = 1.05 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-09 00:59:46.215919: step 11450, loss = 0.87 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 00:59:47.525007: step 11460, loss = 1.07 (977.8 examples/sec; 0.131 sec/batch)
2017-05-09 00:59:48.822876: step 11470, loss = 1.13 (986.2 examples/sec; 0.130 sec/batch)
2017-05-09 00:59:50.104168: step 11480, loss = 0.96 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 00:59:51.361101: step 11490, loss = 0.97 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-09 00:59:52.737126: step 11500, loss = 0.89 (930.2 examples/sec; 0.138 sec/batch)
2017-05-09 00:59:53.911701: step 11510, loss = 0.92 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-09 00:59:55.175948: step 11520, loss = 0.89 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-09 00:59:56.464021: step 11530, loss = 1.00 (993.7 examples/sec; 0.129 sec/batch)
2017-05-09 00:59:57.764474: step 11540, loss = 1.04 (984.3 examples/sec; 0.130 sec/batch)
2017-05-09 00:59:59.042109: step 11550, loss = 1.30 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:00:00.310705: step 11560, loss = 0.93 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:00:01.600916: step 11570, loss = 0.89 (992.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:00:02.897337: step 11580, loss = 1.08 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 01:00:04.186309: step 11590, loss = 0.99 (993.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:00:05.559748: step 11600, loss = 0.89 (932.0 examples/sec; 0.137 sec/batch)
2017-05-09 01:00:06.726121: step 11610, loss = 0.96 (1097.4 examples/sec; 0.117 sec/batch)
2017-05-09 01:00:08.016636: step 11620, loss = 0.72 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:00:09.284758: step 11630, loss = 0.98 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:00:10.544582: step 11640, loss = 1.05 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:00:11.807097: step 11650, loss = 1.02 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:00:13.097402: step 11660, loss = 0.94 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:00:14.374048: step 11670, loss = 1.07 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:00:15.679847: step 11680, loss = 1.09 (980.2 examples/sec; 0.131 sec/batch)
2017-05-09 01:00:16.961196: step 11690, loss = 1.28 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:00:18.399149: step 11700, loss = 1.17 (890.2 examples/sec; 0.144 sec/batch)
2017-05-09 01:00:19.645438: step 11710, loss = 0.96 (1027.1 examples/sec; 0.125 sec/batch)
2017-05-09 01:00:21.006287: step 11720, loss = 0.77 (940.6 examples/sec; 0.136 sec/batch)
2017-05-09 01:00:22.340588: step 11730, loss = 0.99 (959.3 examples/sec; 0.133 sec/batch)
2017-05-09 01:00:23.684509: step 11740, loss = 1.00 (952.4 examples/sec; 0.134 sec/batch)
2017-05-09 01:00:25.042479: step 11750, loss = 0.92 (942.6 examples/sec; 0.136 sec/batch)
2017-05-09 01:00:26.357085: step 11760, loss = 0.89 (973.7 examples/sec; 0.131 sec/batch)
2017-05-09 01:00:27.700071: step 11770, loss = 1.05 (953.1 examples/sec; 0.134 sec/batch)
2017-05-09 01:00:29.024181: step 11780, loss = 1.02 (966.7 examples/sec; 0.132 sec/batch)
2017-05-09 01:00:30.284086: step 11790, loss = 0.85 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:00:31.675941: step 11800, loss = 0.84 (919.6 examples/sec; 0.139 sec/batch)
2017-05-09 01:00:32.911749: step 11810, loss = 0.91 (1035.8 examples/sec; 0.124 sec/batch)
2017-05-09 01:00:34.218051: step 11820, loss = 0.85 (979.9 examples/sec; 0.131 sec/batch)
2017-05-09 01:00:35.555828: step 11830, loss = 0.94 (956.8 examples/sec; 0.134 sec/batch)
2017-05-09 01:00:36.886621: step 11840, loss = 0.86 (961.8 examples/sec; 0.133 sec/batch)
2017-05-09 01:00:38.226280: step 11850, loss = 0.88 (955.5 examples/sec; 0.134 sec/batch)
2017-05-09 01:00:39.558203: step 11860, loss = 0.93 (961.0 examples/sec; 0.133 sec/batch)
2017-05-09 01:00:40.827516: step 11870, loss = 1.15 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:00:42.147617: step 11880, loss = 0.87 (969.6 examples/sec; 0.132 sec/batch)
2017-05-09 01:00:43.471220: step 11890, loss = 0.98 (967.1 examples/sec; 0.132 sec/batch)
2017-05-09 01:00E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 245 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
:44.857518: step 11900, loss = 0.64 (923.3 examples/sec; 0.139 sec/batch)
2017-05-09 01:00:46.128142: step 11910, loss = 0.96 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:00:47.447645: step 11920, loss = 1.02 (970.1 examples/sec; 0.132 sec/batch)
2017-05-09 01:00:48.823060: step 11930, loss = 0.83 (930.6 examples/sec; 0.138 sec/batch)
2017-05-09 01:00:50.174362: step 11940, loss = 1.00 (947.2 examples/sec; 0.135 sec/batch)
2017-05-09 01:00:51.494498: step 11950, loss = 0.92 (969.6 examples/sec; 0.132 sec/batch)
2017-05-09 01:00:52.790185: step 11960, loss = 0.91 (987.9 examples/sec; 0.130 sec/batch)
2017-05-09 01:00:54.149197: step 11970, loss = 0.90 (941.9 examples/sec; 0.136 sec/batch)
2017-05-09 01:00:55.514915: step 11980, loss = 0.98 (937.2 examples/sec; 0.137 sec/batch)
2017-05-09 01:00:56.871628: step 11990, loss = 0.97 (943.5 examples/sec; 0.136 sec/batch)
2017-05-09 01:00:58.323899: step 12000, loss = 0.94 (881.4 examples/sec; 0.145 sec/batch)
2017-05-09 01:00:59.529531: step 12010, loss = 0.93 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-09 01:01:00.805869: step 12020, loss = 0.98 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:01:02.095715: step 12030, loss = 1.10 (992.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:01:03.358617: step 12040, loss = 1.01 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:01:04.640534: step 12050, loss = 0.89 (998.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:01:05.900254: step 12060, loss = 0.79 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:01:07.187429: step 12070, loss = 1.00 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:01:08.479119: step 12080, loss = 1.18 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:01:09.777976: step 12090, loss = 1.03 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 01:01:11.159373: step 12100, loss = 1.11 (926.6 examples/sec; 0.138 sec/batch)
2017-05-09 01:01:12.349221: step 12110, loss = 1.05 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-09 01:01:13.633807: step 12120, loss = 1.02 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:01:14.911220: step 12130, loss = 0.93 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:01:16.186507: step 12140, loss = 0.86 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:01:17.462932: step 12150, loss = 0.99 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:01:18.724187: step 12160, loss = 0.94 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:01:20.011068: step 12170, loss = 0.98 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:01:21.301725: step 12180, loss = 0.98 (991.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:01:22.616452: step 12190, loss = 1.13 (973.6 examples/sec; 0.131 sec/batch)
2017-05-09 01:01:24.010420: step 12200, loss = 0.99 (918.2 examples/sec; 0.139 sec/batch)
2017-05-09 01:01:25.191857: step 12210, loss = 0.92 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-09 01:01:26.490138: step 12220, loss = 0.94 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 01:01:27.781427: step 12230, loss = 0.81 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:01:29.062947: step 12240, loss = 0.93 (998.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:01:30.354141: step 12250, loss = 1.16 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:01:31.629623: step 12260, loss = 1.00 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:01:32.888209: step 12270, loss = 0.71 (1017.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:01:34.160388: step 12280, loss = 0.81 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:01:35.456853: step 12290, loss = 0.95 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 01:01:36.830242: step 12300, loss = 0.86 (932.0 examples/sec; 0.137 sec/batch)
2017-05-09 01:01:38.015731: step 12310, loss = 0.87 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-09 01:01:39.330374: step 12320, loss = 0.83 (973.7 examples/sec; 0.131 sec/batch)
2017-05-09 01:01:40.642312: step 12330, loss = 1.00 (975.7 examples/sec; 0.131 sec/batch)
2017-05-09 01:01:41.940272: step 12340, loss = 1.21 (986.2 examples/sec; 0.130 sec/batch)
2017-05-09 01:01:43.230706: step 12350, loss = 0.98 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:01:44.525978: step 12360, loss = 0.97 (988.2 examples/sec; 0.130 sec/batch)
2017-05-09 01:01:45.825728: step 12370, loss = 0.78 (984.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:01:47.077305: step 12380, loss = 1.01 (1022.7 examples/sec; 0.125 sec/batch)
2017-05-09 01:01:48.366681: step 12390, loss = 0.93 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:01:49.726471: step 12400, loss = 1.03 (941.3 examples/sec; 0.136 sec/batch)
2017-05-09 01:01:50.881528: step 12410, loss = 0.92 (1108.2 examples/sec; 0.116 sec/batch)
2017-05-09 01:01:52.166127: step 12420, loss = 1.13 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:01:53.481993: step 12430, loss = 1.02 (972.7 examples/sec; 0.132 sec/batch)
2017-05-09 01:01:54.785274: step 12440, loss = 0.90 (982.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:01:56.098388: step 12450, loss = 1.06 (974.8 examples/sec; 0.131 sec/batch)
2017-05-09 01:01:57.401705: step 12460, loss = 0.81 (982.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:01:58.683151: step 12470, loss = 0.90 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:01:59.957802: step 12480, loss = 0.89 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:02:01.229123: step 12490, loss = 0.85 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:02:02.593012: step 12500, loss = 0.91 (938.5 examples/sec; 0.136 sec/batch)
2017-05-09 01:02:03.767390: step 12510, loss = 1.06 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-09 01:02:05.077546: step 12520, loss = 1.05 (977.0 examples/sec; 0.131 sec/batch)
2017-05-09 01:02:06.346040: step 12530, loss = 0.97 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:02:07.603466: step 12540, loss = 1.00 (1018.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:02:08.857690: step 12550, loss = 0.92 (1020.5 examples/sec; 0.125 sec/batch)
2017-05-09 01:02:10.125381: step 12560, loss = 0.98 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:02:11.428213: step 12570, loss = 0.80 (982.5 examples/sec; 0.130 sec/batch)
2017-05-09 01:02:12.723378: step 12580, loss = 0.87 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 01:02:14.018798: step 12590, loss = 1.25 (988.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:02:15.399862: step 12600, loss = 0.98 (926.8 examples/sec; 0.138 sec/batch)
2017-05-09 01:02:16.589613: step 12610, loss = 0.81 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-09 01:02:17.876132: step 12620, loss = 1.08 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:02:19.173041: step 12630, loss = 1.01 (987.0 examples/sec; 0.130 sec/batch)
2017-05-09 01:02:20.474963: step 12640, loss = 0.91 (983.2 examples/sec; 0.130 sec/batch)
2017-05-09 01:02:21.784532: step 12650, loss = 1.00 (977.4 examples/sec; 0.131 sec/batch)
2017-05-09 01:02:23.086058: step 12660, loss = 1.00 (983.5 examples/sec; 0.130 sec/batch)
2017-05-09 01:02:24.381738: step 12670, loss = 0.98 (987.9 examples/sec; 0.130 sec/batch)
2017-05-09 01:02:25.650781: step 12680, loss = 1.01 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:02:26.919744: step 12690, loss = 0.92 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:02:28.279529: step 12700, loss = 0.79 (941.3 examples/sec; 0.136 sec/batch)
2017-05-09 01:02:29.434906: step 12710, loss = 0.94 (1107.9 examples/sec; 0.116 sec/batch)
2017-05-09 01:02:30.721444: step 12720, loss = 0.82 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:02:32.005045: step 12730, loss = 0.89 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:02:33.276673: step 12740, loss = 0.93 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:02:34.552846: step 12750, loss = 0.98 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:02:35.818693: step 12760, loss = 0.80 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:02:37.106743: step 12770, loss = 0.94 (993.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:02:38.384678: step 12780, loss = 1.06 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:02:39.669176: step 12790, loss = 0.90 (996.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:02:41.023664: step 12800, loss = 0.90 (945.0 examples/sec; 0.1E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 265 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
35 sec/batch)
2017-05-09 01:02:42.199349: step 12810, loss = 1.07 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-09 01:02:43.460078: step 12820, loss = 0.85 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:02:44.739546: step 12830, loss = 0.90 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:02:46.026072: step 12840, loss = 0.94 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:02:47.301091: step 12850, loss = 0.82 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:02:48.573258: step 12860, loss = 0.90 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:02:49.839796: step 12870, loss = 0.87 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:02:51.089736: step 12880, loss = 1.03 (1024.1 examples/sec; 0.125 sec/batch)
2017-05-09 01:02:52.357702: step 12890, loss = 1.04 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:02:53.741617: step 12900, loss = 0.85 (924.9 examples/sec; 0.138 sec/batch)
2017-05-09 01:02:54.925901: step 12910, loss = 0.75 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-09 01:02:56.205538: step 12920, loss = 0.95 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:02:57.461296: step 12930, loss = 1.04 (1019.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:02:58.721147: step 12940, loss = 0.90 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:02:59.997120: step 12950, loss = 1.01 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:03:01.272879: step 12960, loss = 0.85 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:03:02.552827: step 12970, loss = 0.86 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:03:03.843381: step 12980, loss = 0.99 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:03:05.113404: step 12990, loss = 0.86 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:03:06.505226: step 13000, loss = 0.97 (919.7 examples/sec; 0.139 sec/batch)
2017-05-09 01:03:07.683480: step 13010, loss = 0.93 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-09 01:03:08.982024: step 13020, loss = 0.91 (985.7 examples/sec; 0.130 sec/batch)
2017-05-09 01:03:10.252049: step 13030, loss = 0.92 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:03:11.520059: step 13040, loss = 1.05 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:03:12.795639: step 13050, loss = 0.97 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:03:14.053059: step 13060, loss = 0.76 (1018.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:03:15.332052: step 13070, loss = 0.89 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:03:16.603397: step 13080, loss = 0.79 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:03:17.864873: step 13090, loss = 0.77 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:03:19.208529: step 13100, loss = 0.91 (952.6 examples/sec; 0.134 sec/batch)
2017-05-09 01:03:20.417202: step 13110, loss = 1.05 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-09 01:03:21.687232: step 13120, loss = 1.01 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:03:22.991810: step 13130, loss = 0.97 (981.2 examples/sec; 0.130 sec/batch)
2017-05-09 01:03:24.267497: step 13140, loss = 1.11 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:03:25.553757: step 13150, loss = 0.96 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:03:26.843251: step 13160, loss = 1.00 (992.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:03:28.146572: step 13170, loss = 0.89 (982.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:03:29.454394: step 13180, loss = 0.95 (978.7 examples/sec; 0.131 sec/batch)
2017-05-09 01:03:30.735496: step 13190, loss = 0.89 (999.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:03:32.130202: step 13200, loss = 1.11 (917.8 examples/sec; 0.139 sec/batch)
2017-05-09 01:03:33.302792: step 13210, loss = 0.83 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-09 01:03:34.591554: step 13220, loss = 0.97 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:03:35.869244: step 13230, loss = 0.85 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:03:37.168059: step 13240, loss = 0.99 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 01:03:38.466801: step 13250, loss = 0.96 (985.6 examples/sec; 0.130 sec/batch)
2017-05-09 01:03:39.787372: step 13260, loss = 0.85 (969.3 examples/sec; 0.132 sec/batch)
2017-05-09 01:03:41.076437: step 13270, loss = 0.99 (992.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:03:42.342416: step 13280, loss = 0.88 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:03:43.617608: step 13290, loss = 0.90 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:03:45.004916: step 13300, loss = 0.87 (922.6 examples/sec; 0.139 sec/batch)
2017-05-09 01:03:46.177472: step 13310, loss = 0.93 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-09 01:03:47.454155: step 13320, loss = 0.91 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:03:48.750630: step 13330, loss = 0.87 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 01:03:50.032715: step 13340, loss = 0.85 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:03:51.306385: step 13350, loss = 0.83 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:03:52.578860: step 13360, loss = 0.87 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:03:53.851624: step 13370, loss = 0.82 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:03:55.144146: step 13380, loss = 0.90 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:03:56.418032: step 13390, loss = 0.87 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:03:57.773724: step 13400, loss = 0.74 (944.2 examples/sec; 0.136 sec/batch)
2017-05-09 01:03:58.952381: step 13410, loss = 0.94 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-09 01:04:00.217247: step 13420, loss = 0.94 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:04:01.481548: step 13430, loss = 1.02 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:04:02.752734: step 13440, loss = 1.05 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:04:04.026014: step 13450, loss = 1.05 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:04:05.298624: step 13460, loss = 0.98 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:04:06.564254: step 13470, loss = 0.84 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:04:07.832757: step 13480, loss = 0.93 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:04:09.118831: step 13490, loss = 1.35 (995.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:04:10.494211: step 13500, loss = 0.85 (930.7 examples/sec; 0.138 sec/batch)
2017-05-09 01:04:11.645411: step 13510, loss = 1.06 (1111.9 examples/sec; 0.115 sec/batch)
2017-05-09 01:04:12.939537: step 13520, loss = 1.01 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:04:14.196065: step 13530, loss = 0.82 (1018.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:04:15.466936: step 13540, loss = 0.92 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:04:16.722585: step 13550, loss = 0.90 (1019.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:04:18.004539: step 13560, loss = 0.88 (998.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:04:19.304805: step 13570, loss = 1.01 (984.4 examples/sec; 0.130 sec/batch)
2017-05-09 01:04:20.602053: step 13580, loss = 0.86 (986.7 examples/sec; 0.130 sec/batch)
2017-05-09 01:04:21.882464: step 13590, loss = 0.85 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:04:23.271709: step 13600, loss = 1.12 (921.4 examples/sec; 0.139 sec/batch)
2017-05-09 01:04:24.473122: step 13610, loss = 1.15 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-09 01:04:25.749533: step 13620, loss = 0.95 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:04:26.998693: step 13630, loss = 0.78 (1024.7 examples/sec; 0.125 sec/batch)
2017-05-09 01:04:28.284489: step 13640, loss = 0.85 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:04:29.570773: step 13650, loss = 0.78 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:04:30.855562: step 13660, loss = 0.98 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:04:32.156534: step 13670, loss = 0.95 (983.9 examples/sec; 0.130 sec/batch)
2017-05-09 01:04:33.419801: step 13680, loss = 1.02 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:04:34.699139: step 13690, loss = 0.68 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:04:36.069259: step 13700, loss = 0.83 (934.2 examples/sec; 0.137 sec/batch)
2017-05-09 01:04:37.2198E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 285 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
31: step 13710, loss = 1.02 (1112.5 examples/sec; 0.115 sec/batch)
2017-05-09 01:04:38.523413: step 13720, loss = 0.89 (981.9 examples/sec; 0.130 sec/batch)
2017-05-09 01:04:39.790695: step 13730, loss = 0.88 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:04:41.090272: step 13740, loss = 0.97 (984.9 examples/sec; 0.130 sec/batch)
2017-05-09 01:04:42.337072: step 13750, loss = 0.82 (1026.6 examples/sec; 0.125 sec/batch)
2017-05-09 01:04:43.601436: step 13760, loss = 0.80 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:04:44.878798: step 13770, loss = 1.01 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:04:46.156201: step 13780, loss = 0.75 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:04:47.439533: step 13790, loss = 0.77 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:04:48.815270: step 13800, loss = 0.93 (930.4 examples/sec; 0.138 sec/batch)
2017-05-09 01:04:49.996300: step 13810, loss = 1.00 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-09 01:04:51.286601: step 13820, loss = 1.10 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:04:52.560075: step 13830, loss = 0.92 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:04:53.808975: step 13840, loss = 0.72 (1024.9 examples/sec; 0.125 sec/batch)
2017-05-09 01:04:55.086181: step 13850, loss = 1.10 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:04:56.344096: step 13860, loss = 0.76 (1017.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:04:57.678768: step 13870, loss = 0.94 (959.0 examples/sec; 0.133 sec/batch)
2017-05-09 01:04:58.920571: step 13880, loss = 1.04 (1030.8 examples/sec; 0.124 sec/batch)
2017-05-09 01:05:00.201222: step 13890, loss = 0.76 (999.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:05:01.609078: step 13900, loss = 0.78 (909.2 examples/sec; 0.141 sec/batch)
2017-05-09 01:05:02.769605: step 13910, loss = 1.07 (1102.9 examples/sec; 0.116 sec/batch)
2017-05-09 01:05:04.038855: step 13920, loss = 0.99 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:05:05.316796: step 13930, loss = 0.72 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:05:06.577396: step 13940, loss = 0.94 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:05:07.863190: step 13950, loss = 0.89 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:05:09.135449: step 13960, loss = 0.91 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:05:10.407637: step 13970, loss = 0.75 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:05:11.702595: step 13980, loss = 0.85 (988.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:05:12.988170: step 13990, loss = 0.90 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:05:14.349515: step 14000, loss = 0.82 (940.2 examples/sec; 0.136 sec/batch)
2017-05-09 01:05:15.541676: step 14010, loss = 0.81 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-09 01:05:16.821189: step 14020, loss = 0.97 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:05:18.091434: step 14030, loss = 1.12 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:05:19.394276: step 14040, loss = 1.02 (982.5 examples/sec; 0.130 sec/batch)
2017-05-09 01:05:20.678123: step 14050, loss = 0.95 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:05:21.949373: step 14060, loss = 0.91 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:05:23.221497: step 14070, loss = 0.91 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:05:24.485848: step 14080, loss = 0.88 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:05:25.742148: step 14090, loss = 0.96 (1018.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:05:27.123120: step 14100, loss = 0.87 (926.9 examples/sec; 0.138 sec/batch)
2017-05-09 01:05:28.285720: step 14110, loss = 0.83 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-09 01:05:29.534796: step 14120, loss = 0.87 (1024.8 examples/sec; 0.125 sec/batch)
2017-05-09 01:05:30.822449: step 14130, loss = 0.93 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:05:32.103642: step 14140, loss = 0.96 (999.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:05:33.386057: step 14150, loss = 0.94 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:05:34.670351: step 14160, loss = 0.70 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:05:35.947956: step 14170, loss = 0.99 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:05:37.239754: step 14180, loss = 0.85 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:05:38.515277: step 14190, loss = 0.90 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:05:39.889191: step 14200, loss = 0.94 (931.6 examples/sec; 0.137 sec/batch)
2017-05-09 01:05:41.063998: step 14210, loss = 0.87 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-09 01:05:42.334952: step 14220, loss = 1.07 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:05:43.614290: step 14230, loss = 1.09 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:05:44.879395: step 14240, loss = 0.88 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:05:46.138241: step 14250, loss = 0.79 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:05:47.412024: step 14260, loss = 0.81 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:05:48.708685: step 14270, loss = 1.02 (987.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:05:49.975008: step 14280, loss = 0.94 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:05:51.261369: step 14290, loss = 0.94 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:05:52.637026: step 14300, loss = 0.93 (930.5 examples/sec; 0.138 sec/batch)
2017-05-09 01:05:53.803001: step 14310, loss = 1.03 (1097.8 examples/sec; 0.117 sec/batch)
2017-05-09 01:05:55.096160: step 14320, loss = 0.91 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:05:56.411945: step 14330, loss = 0.98 (972.8 examples/sec; 0.132 sec/batch)
2017-05-09 01:05:57.714591: step 14340, loss = 1.10 (982.6 examples/sec; 0.130 sec/batch)
2017-05-09 01:05:59.015669: step 14350, loss = 1.07 (983.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:06:00.311433: step 14360, loss = 1.05 (987.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:06:01.573816: step 14370, loss = 0.73 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:06:02.840518: step 14380, loss = 0.73 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:06:04.112268: step 14390, loss = 1.07 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:06:05.480362: step 14400, loss = 0.84 (935.6 examples/sec; 0.137 sec/batch)
2017-05-09 01:06:06.675036: step 14410, loss = 0.87 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-09 01:06:07.947533: step 14420, loss = 0.90 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:06:09.206988: step 14430, loss = 0.98 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:06:10.476583: step 14440, loss = 0.86 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:06:11.752455: step 14450, loss = 1.01 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:06:13.023717: step 14460, loss = 1.11 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:06:14.314273: step 14470, loss = 0.95 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:06:15.587485: step 14480, loss = 1.10 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:06:16.856529: step 14490, loss = 0.94 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:06:18.224767: step 14500, loss = 0.81 (935.5 examples/sec; 0.137 sec/batch)
2017-05-09 01:06:19.397859: step 14510, loss = 1.00 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-09 01:06:20.678025: step 14520, loss = 0.86 (999.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:06:21.947841: step 14530, loss = 0.91 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:06:23.231277: step 14540, loss = 0.96 (997.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:06:24.501435: step 14550, loss = 0.87 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:06:25.765625: step 14560, loss = 1.00 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:06:27.048233: step 14570, loss = 0.92 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:06:28.308456: step 14580, loss = 1.14 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:06:29.587372: step 14590, loss = 0.76 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:06:30.957935: step 14600, loss = 0.91 (933.9 examples/sec; 0.137 sec/batch)
2017-05-09 01:06:32.119512: step 14610, loss = 0.94 (1102.0 exE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 306 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
amples/sec; 0.116 sec/batch)
2017-05-09 01:06:33.397028: step 14620, loss = 1.02 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:06:34.690649: step 14630, loss = 0.83 (989.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:06:35.972842: step 14640, loss = 0.94 (998.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:06:37.257503: step 14650, loss = 0.94 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:06:38.555483: step 14660, loss = 0.97 (986.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:06:39.822780: step 14670, loss = 1.08 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:06:41.084975: step 14680, loss = 0.80 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:06:42.350717: step 14690, loss = 0.95 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:06:43.707220: step 14700, loss = 0.84 (943.6 examples/sec; 0.136 sec/batch)
2017-05-09 01:06:44.878835: step 14710, loss = 0.81 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-09 01:06:46.159041: step 14720, loss = 1.11 (999.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:06:47.460129: step 14730, loss = 0.68 (983.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:06:48.761018: step 14740, loss = 0.93 (983.9 examples/sec; 0.130 sec/batch)
2017-05-09 01:06:50.024829: step 14750, loss = 0.68 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:06:51.336459: step 14760, loss = 0.95 (975.9 examples/sec; 0.131 sec/batch)
2017-05-09 01:06:52.636018: step 14770, loss = 0.70 (985.0 examples/sec; 0.130 sec/batch)
2017-05-09 01:06:53.927879: step 14780, loss = 1.06 (990.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:06:55.193929: step 14790, loss = 1.04 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:06:56.570635: step 14800, loss = 0.75 (929.8 examples/sec; 0.138 sec/batch)
2017-05-09 01:06:57.732374: step 14810, loss = 0.81 (1101.8 examples/sec; 0.116 sec/batch)
2017-05-09 01:06:59.003452: step 14820, loss = 0.97 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:07:00.278562: step 14830, loss = 1.04 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:07:01.548243: step 14840, loss = 1.10 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:07:02.817542: step 14850, loss = 0.76 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:07:04.090415: step 14860, loss = 0.86 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:07:05.373157: step 14870, loss = 0.86 (997.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:07:06.637513: step 14880, loss = 0.86 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:07:07.902492: step 14890, loss = 1.12 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:07:09.313966: step 14900, loss = 0.93 (906.9 examples/sec; 0.141 sec/batch)
2017-05-09 01:07:10.493627: step 14910, loss = 0.85 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-09 01:07:11.804978: step 14920, loss = 0.92 (976.1 examples/sec; 0.131 sec/batch)
2017-05-09 01:07:13.117701: step 14930, loss = 0.77 (975.1 examples/sec; 0.131 sec/batch)
2017-05-09 01:07:14.379001: step 14940, loss = 1.01 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:07:15.644031: step 14950, loss = 0.81 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:07:16.910871: step 14960, loss = 1.09 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:07:18.227127: step 14970, loss = 0.84 (972.4 examples/sec; 0.132 sec/batch)
2017-05-09 01:07:19.535926: step 14980, loss = 0.90 (978.0 examples/sec; 0.131 sec/batch)
2017-05-09 01:07:20.840932: step 14990, loss = 0.90 (980.8 examples/sec; 0.131 sec/batch)
2017-05-09 01:07:22.188710: step 15000, loss = 0.90 (949.7 examples/sec; 0.135 sec/batch)
2017-05-09 01:07:23.368641: step 15010, loss = 0.90 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-09 01:07:24.669513: step 15020, loss = 0.87 (984.0 examples/sec; 0.130 sec/batch)
2017-05-09 01:07:25.952583: step 15030, loss = 0.88 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:07:27.224230: step 15040, loss = 1.02 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:07:28.508830: step 15050, loss = 0.92 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:07:29.784239: step 15060, loss = 1.00 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:07:31.051262: step 15070, loss = 1.01 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:07:32.347588: step 15080, loss = 0.78 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 01:07:33.637896: step 15090, loss = 0.98 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:07:35.032661: step 15100, loss = 0.95 (917.7 examples/sec; 0.139 sec/batch)
2017-05-09 01:07:36.240947: step 15110, loss = 0.89 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-09 01:07:37.516123: step 15120, loss = 0.70 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:07:38.774472: step 15130, loss = 1.01 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:07:40.065114: step 15140, loss = 0.82 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:07:41.355600: step 15150, loss = 0.72 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:07:42.626033: step 15160, loss = 0.77 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:07:43.903935: step 15170, loss = 0.73 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:07:45.178458: step 15180, loss = 0.88 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:07:46.452777: step 15190, loss = 0.85 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:07:47.840493: step 15200, loss = 1.12 (922.4 examples/sec; 0.139 sec/batch)
2017-05-09 01:07:48.990947: step 15210, loss = 0.91 (1112.6 examples/sec; 0.115 sec/batch)
2017-05-09 01:07:50.289171: step 15220, loss = 0.77 (986.0 examples/sec; 0.130 sec/batch)
2017-05-09 01:07:51.569954: step 15230, loss = 0.90 (999.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:07:52.818700: step 15240, loss = 1.07 (1025.0 examples/sec; 0.125 sec/batch)
2017-05-09 01:07:54.097614: step 15250, loss = 0.73 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:07:55.377583: step 15260, loss = 0.92 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:07:56.642985: step 15270, loss = 0.84 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:07:57.919296: step 15280, loss = 0.95 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:07:59.205012: step 15290, loss = 0.82 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:08:00.587172: step 15300, loss = 0.91 (926.1 examples/sec; 0.138 sec/batch)
2017-05-09 01:08:01.769668: step 15310, loss = 0.92 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-09 01:08:03.042012: step 15320, loss = 0.78 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:08:04.336169: step 15330, loss = 0.94 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:08:05.617609: step 15340, loss = 0.84 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:08:06.947400: step 15350, loss = 0.92 (962.6 examples/sec; 0.133 sec/batch)
2017-05-09 01:08:08.230601: step 15360, loss = 0.80 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:08:09.504228: step 15370, loss = 0.90 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:08:10.793695: step 15380, loss = 0.77 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:08:12.052098: step 15390, loss = 1.14 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:08:13.427230: step 15400, loss = 0.88 (930.8 examples/sec; 0.138 sec/batch)
2017-05-09 01:08:14.601971: step 15410, loss = 0.97 (1089.6 examples/sec; 0.117 sec/batch)
2017-05-09 01:08:15.904386: step 15420, loss = 1.03 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:08:17.196481: step 15430, loss = 1.05 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:08:18.478114: step 15440, loss = 0.72 (998.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:08:19.797031: step 15450, loss = 0.90 (970.5 examples/sec; 0.132 sec/batch)
2017-05-09 01:08:21.096730: step 15460, loss = 1.14 (984.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:08:22.352351: step 15470, loss = 0.92 (1019.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:08:23.635588: step 15480, loss = 1.03 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:08:24.929690: step 15490, loss = 0.77 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:08:26.301173: step 15500, loss = 0.88 (933.3 examples/sec; 0.137 sec/batch)
2017-05-09 01:08:27.498665: step 15510, loss = 1.04 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-09 01:08:28.E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 326 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
801116: step 15520, loss = 1.08 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:08:30.064625: step 15530, loss = 0.78 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:08:31.356205: step 15540, loss = 0.72 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:08:32.646651: step 15550, loss = 1.04 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:08:33.937752: step 15560, loss = 0.84 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:08:35.243531: step 15570, loss = 0.94 (980.3 examples/sec; 0.131 sec/batch)
2017-05-09 01:08:36.543958: step 15580, loss = 0.77 (984.3 examples/sec; 0.130 sec/batch)
2017-05-09 01:08:37.827831: step 15590, loss = 0.97 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:08:39.213555: step 15600, loss = 0.85 (923.7 examples/sec; 0.139 sec/batch)
2017-05-09 01:08:40.415054: step 15610, loss = 1.07 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-09 01:08:41.722635: step 15620, loss = 0.95 (978.9 examples/sec; 0.131 sec/batch)
2017-05-09 01:08:43.021263: step 15630, loss = 1.12 (985.7 examples/sec; 0.130 sec/batch)
2017-05-09 01:08:44.303415: step 15640, loss = 1.04 (998.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:08:45.602618: step 15650, loss = 0.77 (985.2 examples/sec; 0.130 sec/batch)
2017-05-09 01:08:46.880745: step 15660, loss = 0.92 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:08:48.165420: step 15670, loss = 0.82 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:08:49.454313: step 15680, loss = 0.86 (993.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:08:50.716390: step 15690, loss = 0.94 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:08:52.125959: step 15700, loss = 0.89 (908.1 examples/sec; 0.141 sec/batch)
2017-05-09 01:08:53.288384: step 15710, loss = 0.90 (1101.2 examples/sec; 0.116 sec/batch)
2017-05-09 01:08:54.553661: step 15720, loss = 1.00 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:08:55.862145: step 15730, loss = 0.97 (978.2 examples/sec; 0.131 sec/batch)
2017-05-09 01:08:57.174166: step 15740, loss = 0.75 (975.6 examples/sec; 0.131 sec/batch)
2017-05-09 01:08:58.469043: step 15750, loss = 0.92 (988.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:08:59.749898: step 15760, loss = 1.05 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:09:01.032543: step 15770, loss = 0.95 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:09:02.323943: step 15780, loss = 0.97 (991.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:09:03.633371: step 15790, loss = 1.01 (977.5 examples/sec; 0.131 sec/batch)
2017-05-09 01:09:05.021275: step 15800, loss = 1.01 (922.3 examples/sec; 0.139 sec/batch)
2017-05-09 01:09:06.199143: step 15810, loss = 1.03 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-09 01:09:07.500522: step 15820, loss = 0.74 (983.6 examples/sec; 0.130 sec/batch)
2017-05-09 01:09:08.803812: step 15830, loss = 1.02 (982.2 examples/sec; 0.130 sec/batch)
2017-05-09 01:09:10.088566: step 15840, loss = 0.78 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:09:11.369588: step 15850, loss = 0.88 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:09:12.631148: step 15860, loss = 0.79 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:09:13.900072: step 15870, loss = 0.98 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:09:15.207867: step 15880, loss = 0.87 (978.7 examples/sec; 0.131 sec/batch)
2017-05-09 01:09:16.504683: step 15890, loss = 0.97 (987.0 examples/sec; 0.130 sec/batch)
2017-05-09 01:09:17.882285: step 15900, loss = 0.96 (929.1 examples/sec; 0.138 sec/batch)
2017-05-09 01:09:19.063720: step 15910, loss = 0.88 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-09 01:09:20.361612: step 15920, loss = 0.85 (986.2 examples/sec; 0.130 sec/batch)
2017-05-09 01:09:21.663202: step 15930, loss = 0.69 (983.4 examples/sec; 0.130 sec/batch)
2017-05-09 01:09:22.957388: step 15940, loss = 1.01 (989.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:09:24.244686: step 15950, loss = 0.86 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:09:25.510238: step 15960, loss = 0.95 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:09:26.765052: step 15970, loss = 0.75 (1020.1 examples/sec; 0.125 sec/batch)
2017-05-09 01:09:28.049621: step 15980, loss = 0.91 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:09:29.317225: step 15990, loss = 1.05 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:09:30.705648: step 16000, loss = 0.94 (921.9 examples/sec; 0.139 sec/batch)
2017-05-09 01:09:31.888478: step 16010, loss = 0.82 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-09 01:09:33.169204: step 16020, loss = 0.88 (999.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:09:34.428964: step 16030, loss = 0.66 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:09:35.703576: step 16040, loss = 0.91 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:09:36.975942: step 16050, loss = 0.73 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:09:38.267183: step 16060, loss = 0.90 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:09:39.548808: step 16070, loss = 0.78 (998.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:09:40.818610: step 16080, loss = 0.97 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:09:42.087282: step 16090, loss = 0.77 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:09:43.494964: step 16100, loss = 0.95 (909.3 examples/sec; 0.141 sec/batch)
2017-05-09 01:09:44.690710: step 16110, loss = 1.08 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-09 01:09:45.993155: step 16120, loss = 0.75 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:09:47.255918: step 16130, loss = 1.04 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:09:48.537782: step 16140, loss = 1.05 (998.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:09:49.795624: step 16150, loss = 0.86 (1017.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:09:51.072256: step 16160, loss = 0.87 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:09:52.394379: step 16170, loss = 1.07 (968.1 examples/sec; 0.132 sec/batch)
2017-05-09 01:09:53.688573: step 16180, loss = 1.01 (989.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:09:54.970610: step 16190, loss = 0.89 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:09:56.328977: step 16200, loss = 0.76 (942.3 examples/sec; 0.136 sec/batch)
2017-05-09 01:09:57.533130: step 16210, loss = 0.80 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-09 01:09:58.841251: step 16220, loss = 0.93 (978.5 examples/sec; 0.131 sec/batch)
2017-05-09 01:10:00.202201: step 16230, loss = 0.99 (940.5 examples/sec; 0.136 sec/batch)
2017-05-09 01:10:01.482200: step 16240, loss = 0.90 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:10:02.855525: step 16250, loss = 0.79 (932.0 examples/sec; 0.137 sec/batch)
2017-05-09 01:10:04.274246: step 16260, loss = 0.81 (902.2 examples/sec; 0.142 sec/batch)
2017-05-09 01:10:05.677903: step 16270, loss = 0.90 (911.9 examples/sec; 0.140 sec/batch)
2017-05-09 01:10:07.115020: step 16280, loss = 0.85 (890.7 examples/sec; 0.144 sec/batch)
2017-05-09 01:10:08.402749: step 16290, loss = 0.86 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:10:09.757897: step 16300, loss = 1.07 (944.5 examples/sec; 0.136 sec/batch)
2017-05-09 01:10:10.947909: step 16310, loss = 0.77 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-09 01:10:12.247803: step 16320, loss = 1.06 (984.7 examples/sec; 0.130 sec/batch)
2017-05-09 01:10:13.525982: step 16330, loss = 0.75 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:10:14.830846: step 16340, loss = 0.83 (981.0 examples/sec; 0.130 sec/batch)
2017-05-09 01:10:16.130950: step 16350, loss = 0.95 (984.5 examples/sec; 0.130 sec/batch)
2017-05-09 01:10:17.423535: step 16360, loss = 1.03 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:10:18.722127: step 16370, loss = 0.78 (985.7 examples/sec; 0.130 sec/batch)
2017-05-09 01:10:20.017975: step 16380, loss = 0.73 (987.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:10:21.347488: step 16390, loss = 0.86 (962.8 examples/sec; 0.133 sec/batch)
2017-05-09 01:10:22.745348: step 16400, loss = 0.90 (915.7 examples/sec; 0.140 sec/batch)
2017-05-09 01:10:24.024673: step 16410, loss = 0.83 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:10:25.404795: step 16420, loss = 0.92 (927.4 examples/sec; 0.138 sec/batch)
2017-05-09 01:10:26.786374: step 16430, loss = 0.86 (926.5 examples/sec; 0.138 sec/batch)
2017-05-09 01:10:28.157354: step 16440, loss = 0.88 (933.7 examples/sec; 0.137 sec/batch)
2017-05-09 01:10:29.539722: step 16450, loss = 0.96 (925.9 examples/sec; 0.138 sec/batch)
2017-05-09 01:10:30.938082: step 16460, loss = 0.98 (915.4 examples/sec; 0.140 sec/batch)
2017-05-09 01:10:32.321173: step 16470, loss = 0.87 (925.5 examples/sec; 0.138 sec/batch)
2017-05-09 01:10:33.724676: step 16480, loss = 0.84 (912.0 examples/sec; 0.140 sec/batch)
2017-05-09 01:10:35.119863: step 16490, loss = 0.74 (917.4 examples/sec; 0.140 sec/batch)
2017-05-09 01:10:36.611765: step 16500, loss = 0.79 (858.0 examples/sec; 0.149 sec/batch)
2017-05-09 01:10:37.913625: step 16510, loss = 0.86 (983.2 examples/sec; 0.130 sec/batch)
2017-05-09 01:10:39.330690: step 16520, loss = 1.04 (903.3 examples/sec; 0.142 sec/batch)
2017-05-09 01:10:40.755192: step 16530, loss = 0.93 (898.6 examples/sec; 0.142 sec/batch)
2017-05-09 01:10:42.217130: step 16540, loss = 0.87 (875.6 examples/sec; 0.146 sec/batch)
2017-05-09 01:10:43.498028: step 16550, loss = 0.97 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:10:44.771393: step 16560, loss = 1.01 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:10:46.051680: step 16570, loss = 0.90 (999.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:10:47.349512: step 16580, loss = 0.97 (986.3 examples/sec; 0.130 sec/batch)
2017-05-09 01:10:48.641964: step 16590, loss = 1.11 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:10:50.041690: step 16600, loss = 1.02 (914.5 examples/sec; 0.140 sec/batch)
2017-05-09 01:10:51.244253: step 16610, loss = 0.80 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-09 01:10:52.537677: step 16620, loss = 0.88 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:10:53.805577: step 16630, loss = 1.15 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:10:55.131003: step 16640, loss = 0.94 (965.7 examples/sec; 0.133 sec/batch)
2017-05-09 01:10:56.412770: step 16650, loss = 0.85 (998.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:10:57.675349: step 16660, loss = 0.99 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:10:58.941361: step 16670, loss = 0.95 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:11:00.227907: step 16680, loss = 0.84 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:11:01.500494: step 16690, loss = 0.91 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:11:02.872031: step 16700, loss = 0.79 (933.3 examples/sec; 0.137 sec/batch)
2017-05-09 01:11:04.073443: step 16710, loss = 0.74 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-09 01:11:05.347770: step 16720, loss = 0.95 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:11:06.624687: step 16730, loss = 0.95 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:11:07.904621: step 16740, loss = 0.87 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:11:09.217903: step 16750, loss = 1.03 (974.7 examples/sec; 0.131 sec/batch)
2017-05-09 01:11:10.499854: step 16760, loss = 0.88 (998.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:11:11.803083: step 16770, loss = 1.18 (982.2 examples/sec; 0.130 sec/batch)
2017-05-09 01:11:13.097992: step 16780, loss = 1.14 (988.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:11:14.372124: step 16790, loss = 0.93 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:11:15.735973: step 16800, loss = 0.94 (938.5 examples/sec; 0.136 sec/batch)
2017-05-09 01:11:16.941301: step 16810, loss = 0.87 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-09 01:11:18.221169: step 16820, loss = 1.08 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:11:19.495126: step 16830, loss = 0.84 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:11:20.764072: step 16840, loss = 1.00 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:11:22.036972: step 16850, loss = 1.01 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:11:23.298687: step 16860, loss = 0.95 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:11:24.554120: step 16870, loss = 0.97 (1019.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:11:25.83E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 346 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
4094: step 16880, loss = 0.80 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:11:27.108109: step 16890, loss = 1.10 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:11:28.488010: step 16900, loss = 1.06 (927.6 examples/sec; 0.138 sec/batch)
2017-05-09 01:11:29.661050: step 16910, loss = 0.74 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-09 01:11:30.934130: step 16920, loss = 0.75 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:11:32.199304: step 16930, loss = 0.84 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:11:33.475689: step 16940, loss = 0.95 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:11:34.740000: step 16950, loss = 0.89 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:11:36.004549: step 16960, loss = 0.75 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:11:37.294832: step 16970, loss = 0.96 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:11:38.551832: step 16980, loss = 0.78 (1018.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:11:39.816492: step 16990, loss = 0.85 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:11:41.181851: step 17000, loss = 1.05 (937.5 examples/sec; 0.137 sec/batch)
2017-05-09 01:11:42.339604: step 17010, loss = 1.03 (1105.6 examples/sec; 0.116 sec/batch)
2017-05-09 01:11:43.614464: step 17020, loss = 0.81 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:11:44.877681: step 17030, loss = 0.84 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:11:46.122101: step 17040, loss = 1.31 (1028.6 examples/sec; 0.124 sec/batch)
2017-05-09 01:11:47.428941: step 17050, loss = 0.94 (979.5 examples/sec; 0.131 sec/batch)
2017-05-09 01:11:48.703688: step 17060, loss = 1.23 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:11:50.005938: step 17070, loss = 1.00 (982.9 examples/sec; 0.130 sec/batch)
2017-05-09 01:11:51.299770: step 17080, loss = 0.82 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:11:52.566949: step 17090, loss = 0.90 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:11:53.913683: step 17100, loss = 0.82 (950.4 examples/sec; 0.135 sec/batch)
2017-05-09 01:11:55.074974: step 17110, loss = 0.84 (1102.2 examples/sec; 0.116 sec/batch)
2017-05-09 01:11:56.335883: step 17120, loss = 1.17 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:11:57.613156: step 17130, loss = 1.09 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:11:58.926249: step 17140, loss = 0.89 (974.8 examples/sec; 0.131 sec/batch)
2017-05-09 01:12:00.240612: step 17150, loss = 0.91 (973.9 examples/sec; 0.131 sec/batch)
2017-05-09 01:12:01.492985: step 17160, loss = 0.97 (1022.1 examples/sec; 0.125 sec/batch)
2017-05-09 01:12:02.786639: step 17170, loss = 0.90 (989.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:12:04.080726: step 17180, loss = 0.91 (989.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:12:05.374077: step 17190, loss = 0.82 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:12:06.728923: step 17200, loss = 1.08 (944.8 examples/sec; 0.135 sec/batch)
2017-05-09 01:12:07.933132: step 17210, loss = 0.80 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-09 01:12:09.241423: step 17220, loss = 0.92 (978.4 examples/sec; 0.131 sec/batch)
2017-05-09 01:12:10.529022: step 17230, loss = 1.08 (994.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:12:11.822234: step 17240, loss = 0.75 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:12:13.097646: step 17250, loss = 0.97 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:12:14.364534: step 17260, loss = 0.81 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:12:15.647705: step 17270, loss = 0.90 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:12:16.944996: step 17280, loss = 0.68 (986.7 examples/sec; 0.130 sec/batch)
2017-05-09 01:12:18.207884: step 17290, loss = 0.88 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:12:19.589871: step 17300, loss = 0.83 (926.2 examples/sec; 0.138 sec/batch)
2017-05-09 01:12:20.786667: step 17310, loss = 0.70 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-09 01:12:22.082718: step 17320, loss = 0.97 (987.6 examples/sec; 0.130 sec/batch)
2017-05-09 01:12:23.370903: step 17330, loss = 1.30 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:12:24.643000: step 17340, loss = 0.74 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:12:25.911016: step 17350, loss = 0.91 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:12:27.175096: step 17360, loss = 0.83 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:12:28.447557: step 17370, loss = 0.83 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:12:29.753341: step 17380, loss = 0.83 (980.3 examples/sec; 0.131 sec/batch)
2017-05-09 01:12:31.056821: step 17390, loss = 0.97 (982.0 examples/sec; 0.130 sec/batch)
2017-05-09 01:12:32.422264: step 17400, loss = 0.95 (937.4 examples/sec; 0.137 sec/batch)
2017-05-09 01:12:33.610061: step 17410, loss = 0.99 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-09 01:12:34.912735: step 17420, loss = 0.78 (982.6 examples/sec; 0.130 sec/batch)
2017-05-09 01:12:36.191995: step 17430, loss = 0.82 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:12:37.454696: step 17440, loss = 0.95 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:12:38.736524: step 17450, loss = 0.89 (998.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:12:40.023888: step 17460, loss = 0.84 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:12:41.309675: step 17470, loss = 0.85 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:12:42.580255: step 17480, loss = 0.79 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:12:43.854676: step 17490, loss = 0.91 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:12:45.232170: step 17500, loss = 0.87 (929.2 examples/sec; 0.138 sec/batch)
2017-05-09 01:12:46.453089: step 17510, loss = 1.15 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-09 01:12:47.727824: step 17520, loss = 0.92 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:12:49.004594: step 17530, loss = 0.86 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:12:50.257990: step 17540, loss = 0.77 (1021.2 examples/sec; 0.125 sec/batch)
2017-05-09 01:12:51.520720: step 17550, loss = 0.78 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:12:52.809388: step 17560, loss = 0.89 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:12:54.072491: step 17570, loss = 0.78 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:12:55.346936: step 17580, loss = 0.78 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:12:56.608914: step 17590, loss = 0.82 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:12:57.963167: step 17600, loss = 0.93 (945.2 examples/sec; 0.135 sec/batch)
2017-05-09 01:12:59.129851: step 17610, loss = 0.84 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-09 01:13:00.402465: step 17620, loss = 0.95 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:13:01.717093: step 17630, loss = 1.02 (973.7 examples/sec; 0.131 sec/batch)
2017-05-09 01:13:03.022444: step 17640, loss = 1.14 (980.6 examples/sec; 0.131 sec/batch)
2017-05-09 01:13:04.318991: step 17650, loss = 0.89 (987.2 examples/sec; 0.130 sec/batch)
2017-05-09 01:13:05.601085: step 17660, loss = 0.89 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:13:06.868843: step 17670, loss = 0.82 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:13:08.175687: step 17680, loss = 0.82 (979.5 examples/sec; 0.131 sec/batch)
2017-05-09 01:13:09.472241: step 17690, loss = 0.89 (987.2 examples/sec; 0.130 sec/batch)
2017-05-09 01:13:10.827203: step 17700, loss = 0.91 (944.7 examples/sec; 0.135 sec/batch)
2017-05-09 01:13:12.026516: step 17710, loss = 0.75 (1067.3 examples/sec; 0.120 sec/batch)
2017-05-09 01:13:13.291210: step 17720, loss = 0.90 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:13:14.559910: step 17730, loss = 0.94 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:13:15.820350: step 17740, loss = 0.95 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:13:17.088380: step 17750, loss = 0.79 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:13:18.352847: step 17760, loss = 0.83 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:13:19.639839: step 17770, loss = 0.80 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:13:20.919361: step 17780, loss = 0.77 (1000.4 exaE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 366 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
mples/sec; 0.128 sec/batch)
2017-05-09 01:13:22.192586: step 17790, loss = 0.88 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:13:23.576900: step 17800, loss = 0.75 (924.6 examples/sec; 0.138 sec/batch)
2017-05-09 01:13:24.756632: step 17810, loss = 0.74 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-09 01:13:26.047077: step 17820, loss = 1.00 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:13:27.349581: step 17830, loss = 0.90 (982.7 examples/sec; 0.130 sec/batch)
2017-05-09 01:13:28.645460: step 17840, loss = 0.75 (987.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:13:29.924770: step 17850, loss = 1.04 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:13:31.214835: step 17860, loss = 1.02 (992.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:13:32.491983: step 17870, loss = 0.96 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:13:33.768590: step 17880, loss = 0.86 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:13:35.042465: step 17890, loss = 0.78 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:13:36.406515: step 17900, loss = 0.72 (938.4 examples/sec; 0.136 sec/batch)
2017-05-09 01:13:37.586514: step 17910, loss = 1.00 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-09 01:13:38.838880: step 17920, loss = 0.93 (1022.1 examples/sec; 0.125 sec/batch)
2017-05-09 01:13:40.104695: step 17930, loss = 0.96 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:13:41.374998: step 17940, loss = 1.10 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:13:42.666472: step 17950, loss = 0.85 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:13:43.965310: step 17960, loss = 0.95 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 01:13:45.236547: step 17970, loss = 0.90 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:13:46.495413: step 17980, loss = 1.14 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:13:47.768248: step 17990, loss = 1.10 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:13:49.168921: step 18000, loss = 0.90 (913.8 examples/sec; 0.140 sec/batch)
2017-05-09 01:13:50.390765: step 18010, loss = 0.92 (1047.6 examples/sec; 0.122 sec/batch)
2017-05-09 01:13:51.647818: step 18020, loss = 0.98 (1018.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:13:52.944117: step 18030, loss = 0.89 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 01:13:54.262848: step 18040, loss = 0.94 (970.6 examples/sec; 0.132 sec/batch)
2017-05-09 01:13:55.549949: step 18050, loss = 0.82 (994.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:13:56.829204: step 18060, loss = 0.96 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:13:58.086659: step 18070, loss = 0.91 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:13:59.372955: step 18080, loss = 1.13 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:14:00.697646: step 18090, loss = 0.84 (966.3 examples/sec; 0.132 sec/batch)
2017-05-09 01:14:02.053909: step 18100, loss = 0.85 (943.8 examples/sec; 0.136 sec/batch)
2017-05-09 01:14:03.283291: step 18110, loss = 0.85 (1041.2 examples/sec; 0.123 sec/batch)
2017-05-09 01:14:04.571178: step 18120, loss = 0.95 (993.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:14:05.846958: step 18130, loss = 1.08 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:14:07.146889: step 18140, loss = 0.86 (984.7 examples/sec; 0.130 sec/batch)
2017-05-09 01:14:08.447003: step 18150, loss = 0.83 (984.5 examples/sec; 0.130 sec/batch)
2017-05-09 01:14:09.722413: step 18160, loss = 1.04 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:14:11.029496: step 18170, loss = 0.78 (979.3 examples/sec; 0.131 sec/batch)
2017-05-09 01:14:12.314483: step 18180, loss = 0.76 (996.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:14:13.586815: step 18190, loss = 0.93 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:14:14.956205: step 18200, loss = 0.96 (934.7 examples/sec; 0.137 sec/batch)
2017-05-09 01:14:16.134847: step 18210, loss = 0.76 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-09 01:14:17.425811: step 18220, loss = 0.87 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:14:18.669338: step 18230, loss = 0.82 (1029.3 examples/sec; 0.124 sec/batch)
2017-05-09 01:14:19.968423: step 18240, loss = 1.23 (985.3 examples/sec; 0.130 sec/batch)
2017-05-09 01:14:21.282759: step 18250, loss = 0.94 (973.9 examples/sec; 0.131 sec/batch)
2017-05-09 01:14:22.575992: step 18260, loss = 0.89 (989.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:14:23.858720: step 18270, loss = 0.92 (997.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:14:25.130376: step 18280, loss = 0.92 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:14:26.418152: step 18290, loss = 0.85 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:14:27.802246: step 18300, loss = 0.89 (924.8 examples/sec; 0.138 sec/batch)
2017-05-09 01:14:28.989375: step 18310, loss = 0.88 (1078.2 examples/sec; 0.119 sec/batch)
2017-05-09 01:14:30.291411: step 18320, loss = 0.87 (983.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:14:31.557034: step 18330, loss = 0.99 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:14:32.844421: step 18340, loss = 0.88 (994.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:14:34.126479: step 18350, loss = 0.81 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:14:35.420365: step 18360, loss = 0.99 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:14:36.766997: step 18370, loss = 0.77 (950.5 examples/sec; 0.135 sec/batch)
2017-05-09 01:14:38.059927: step 18380, loss = 0.89 (990.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:14:39.366774: step 18390, loss = 0.66 (979.5 examples/sec; 0.131 sec/batch)
2017-05-09 01:14:40.719552: step 18400, loss = 0.84 (946.2 examples/sec; 0.135 sec/batch)
2017-05-09 01:14:41.922179: step 18410, loss = 0.93 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-09 01:14:43.212217: step 18420, loss = 0.80 (992.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:14:44.474599: step 18430, loss = 0.71 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:14:45.752662: step 18440, loss = 0.98 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:14:47.016203: step 18450, loss = 0.85 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:14:48.303024: step 18460, loss = 0.98 (994.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:14:49.596855: step 18470, loss = 0.84 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:14:50.858846: step 18480, loss = 1.00 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:14:52.142751: step 18490, loss = 0.83 (996.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:14:53.537783: step 18500, loss = 0.86 (917.5 examples/sec; 0.140 sec/batch)
2017-05-09 01:14:54.701478: step 18510, loss = 0.90 (1099.9 examples/sec; 0.116 sec/batch)
2017-05-09 01:14:55.978754: step 18520, loss = 0.81 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:14:57.256162: step 18530, loss = 0.74 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:14:58.563889: step 18540, loss = 0.84 (978.8 examples/sec; 0.131 sec/batch)
2017-05-09 01:14:59.823258: step 18550, loss = 0.87 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:15:01.093539: step 18560, loss = 0.77 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:15:02.371934: step 18570, loss = 0.85 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:15:03.649271: step 18580, loss = 0.89 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:15:04.925005: step 18590, loss = 0.93 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:15:06.296271: step 18600, loss = 0.95 (933.4 examples/sec; 0.137 sec/batch)
2017-05-09 01:15:07.455040: step 18610, loss = 0.89 (1104.6 examples/sec; 0.116 sec/batch)
2017-05-09 01:15:08.748881: step 18620, loss = 0.95 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:15:10.005206: step 18630, loss = 0.76 (1018.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:15:11.284816: step 18640, loss = 0.98 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:15:12.544787: step 18650, loss = 0.90 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:15:13.796270: step 18660, loss = 0.78 (1022.8 examples/sec; 0.125 sec/batch)
2017-05-09 01:15:15.068064: step 18670, loss = 0.92 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:15:16.367397: step 18680, loss = 1.03 (985.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:15:17E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 387 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
.625080: step 18690, loss = 0.93 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:15:19.014089: step 18700, loss = 0.82 (921.5 examples/sec; 0.139 sec/batch)
2017-05-09 01:15:20.197745: step 18710, loss = 0.94 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-09 01:15:21.487300: step 18720, loss = 0.95 (992.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:15:22.767021: step 18730, loss = 0.93 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:15:24.048003: step 18740, loss = 0.80 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:15:25.362132: step 18750, loss = 0.82 (974.0 examples/sec; 0.131 sec/batch)
2017-05-09 01:15:26.653481: step 18760, loss = 0.96 (991.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:15:27.970431: step 18770, loss = 0.98 (971.9 examples/sec; 0.132 sec/batch)
2017-05-09 01:15:29.252580: step 18780, loss = 0.90 (998.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:15:30.530279: step 18790, loss = 0.96 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:15:31.903485: step 18800, loss = 0.82 (932.1 examples/sec; 0.137 sec/batch)
2017-05-09 01:15:33.070281: step 18810, loss = 0.98 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-09 01:15:34.345611: step 18820, loss = 0.71 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:15:35.617185: step 18830, loss = 1.18 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:15:36.892723: step 18840, loss = 0.96 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:15:38.159100: step 18850, loss = 0.89 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:15:39.429436: step 18860, loss = 0.78 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:15:40.702654: step 18870, loss = 0.79 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:15:41.997704: step 18880, loss = 0.95 (988.4 examples/sec; 0.130 sec/batch)
2017-05-09 01:15:43.312097: step 18890, loss = 1.03 (973.8 examples/sec; 0.131 sec/batch)
2017-05-09 01:15:44.700017: step 18900, loss = 1.07 (922.2 examples/sec; 0.139 sec/batch)
2017-05-09 01:15:45.867467: step 18910, loss = 0.96 (1096.4 examples/sec; 0.117 sec/batch)
2017-05-09 01:15:47.130713: step 18920, loss = 0.89 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:15:48.411913: step 18930, loss = 0.86 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:15:49.692078: step 18940, loss = 0.91 (999.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:15:50.951389: step 18950, loss = 0.86 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:15:52.244869: step 18960, loss = 0.85 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:15:53.501062: step 18970, loss = 0.84 (1018.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:15:54.791500: step 18980, loss = 0.96 (991.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:15:56.053766: step 18990, loss = 0.85 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:15:57.408444: step 19000, loss = 0.86 (944.9 examples/sec; 0.135 sec/batch)
2017-05-09 01:15:58.578121: step 19010, loss = 0.90 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-09 01:15:59.859980: step 19020, loss = 0.84 (998.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:16:01.158433: step 19030, loss = 0.96 (985.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:16:02.440892: step 19040, loss = 0.84 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:16:03.718200: step 19050, loss = 0.68 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:16:04.999940: step 19060, loss = 0.99 (998.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:16:06.306574: step 19070, loss = 0.93 (979.6 examples/sec; 0.131 sec/batch)
2017-05-09 01:16:07.614479: step 19080, loss = 0.89 (978.7 examples/sec; 0.131 sec/batch)
2017-05-09 01:16:08.889686: step 19090, loss = 1.05 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:16:10.247826: step 19100, loss = 1.08 (942.5 examples/sec; 0.136 sec/batch)
2017-05-09 01:16:11.431558: step 19110, loss = 0.89 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-09 01:16:12.717721: step 19120, loss = 0.88 (995.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:16:14.012868: step 19130, loss = 0.87 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 01:16:15.320969: step 19140, loss = 0.91 (978.5 examples/sec; 0.131 sec/batch)
2017-05-09 01:16:16.614789: step 19150, loss = 0.74 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:16:17.897741: step 19160, loss = 0.77 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:16:19.217147: step 19170, loss = 0.76 (970.1 examples/sec; 0.132 sec/batch)
2017-05-09 01:16:20.504114: step 19180, loss = 0.81 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:16:21.800631: step 19190, loss = 0.95 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 01:16:23.167015: step 19200, loss = 0.74 (936.8 examples/sec; 0.137 sec/batch)
2017-05-09 01:16:24.377559: step 19210, loss = 0.95 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-09 01:16:25.665568: step 19220, loss = 1.04 (993.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:16:26.963069: step 19230, loss = 0.90 (986.5 examples/sec; 0.130 sec/batch)
2017-05-09 01:16:28.238200: step 19240, loss = 0.97 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:16:29.502878: step 19250, loss = 0.90 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:16:30.763696: step 19260, loss = 0.79 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:16:32.053512: step 19270, loss = 0.81 (992.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:16:33.325379: step 19280, loss = 0.85 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:16:34.597090: step 19290, loss = 0.82 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:16:35.964309: step 19300, loss = 1.01 (936.2 examples/sec; 0.137 sec/batch)
2017-05-09 01:16:37.148574: step 19310, loss = 0.85 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-09 01:16:38.429844: step 19320, loss = 0.86 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:16:39.699844: step 19330, loss = 0.86 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:16:40.971400: step 19340, loss = 0.93 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:16:42.233086: step 19350, loss = 0.77 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:16:43.486185: step 19360, loss = 0.78 (1021.7 examples/sec; 0.125 sec/batch)
2017-05-09 01:16:44.762397: step 19370, loss = 0.77 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:16:46.062047: step 19380, loss = 1.02 (984.9 examples/sec; 0.130 sec/batch)
2017-05-09 01:16:47.333758: step 19390, loss = 0.88 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:16:48.703825: step 19400, loss = 0.84 (934.3 examples/sec; 0.137 sec/batch)
2017-05-09 01:16:49.909383: step 19410, loss = 1.01 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-09 01:16:51.199684: step 19420, loss = 0.78 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:16:52.475969: step 19430, loss = 1.01 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:16:53.770637: step 19440, loss = 0.81 (988.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:16:55.080393: step 19450, loss = 0.89 (977.3 examples/sec; 0.131 sec/batch)
2017-05-09 01:16:56.369644: step 19460, loss = 0.82 (992.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:16:57.648269: step 19470, loss = 0.81 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:16:58.925960: step 19480, loss = 0.83 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:17:00.190928: step 19490, loss = 0.89 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:17:01.649214: step 19500, loss = 1.03 (877.7 examples/sec; 0.146 sec/batch)
2017-05-09 01:17:02.857523: step 19510, loss = 0.87 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-09 01:17:04.180980: step 19520, loss = 0.69 (967.2 examples/sec; 0.132 sec/batch)
2017-05-09 01:17:05.452284: step 19530, loss = 0.73 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:17:06.724684: step 19540, loss = 1.04 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:17:08.008004: step 19550, loss = 1.04 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:17:09.298124: step 19560, loss = 0.74 (992.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:17:10.584924: step 19570, loss = 0.90 (994.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:17:11.892435: step 19580, loss = 1.16 (979.0 examples/sec; 0.131 sec/batch)
2017-05-09 01:17:13.173037: step 19590, loss = 0.86 (999.5 examples/seE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 407 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
c; 0.128 sec/batch)
2017-05-09 01:17:14.550424: step 19600, loss = 1.00 (929.3 examples/sec; 0.138 sec/batch)
2017-05-09 01:17:15.734085: step 19610, loss = 1.08 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-09 01:17:17.036278: step 19620, loss = 0.87 (983.0 examples/sec; 0.130 sec/batch)
2017-05-09 01:17:18.324406: step 19630, loss = 0.99 (993.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:17:19.614617: step 19640, loss = 0.86 (992.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:17:20.932157: step 19650, loss = 0.94 (971.5 examples/sec; 0.132 sec/batch)
2017-05-09 01:17:22.210760: step 19660, loss = 1.24 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:17:23.500207: step 19670, loss = 0.97 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:17:24.761637: step 19680, loss = 0.91 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:17:26.018508: step 19690, loss = 0.77 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:17:27.423872: step 19700, loss = 1.07 (910.8 examples/sec; 0.141 sec/batch)
2017-05-09 01:17:28.631820: step 19710, loss = 0.72 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-09 01:17:29.909402: step 19720, loss = 0.98 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:17:31.173409: step 19730, loss = 0.97 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:17:32.457644: step 19740, loss = 0.89 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:17:33.760098: step 19750, loss = 0.78 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:17:35.059523: step 19760, loss = 0.85 (985.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:17:36.366104: step 19770, loss = 0.96 (979.7 examples/sec; 0.131 sec/batch)
2017-05-09 01:17:37.649264: step 19780, loss = 0.96 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:17:38.912894: step 19790, loss = 0.84 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:17:40.283090: step 19800, loss = 0.80 (934.2 examples/sec; 0.137 sec/batch)
2017-05-09 01:17:41.441589: step 19810, loss = 0.76 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-09 01:17:42.715687: step 19820, loss = 0.81 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:17:43.982959: step 19830, loss = 0.98 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:17:45.257218: step 19840, loss = 0.94 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:17:46.546818: step 19850, loss = 0.73 (992.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:17:47.807864: step 19860, loss = 0.89 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:17:49.064955: step 19870, loss = 0.82 (1018.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:17:50.331090: step 19880, loss = 0.95 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:17:51.640290: step 19890, loss = 0.92 (977.7 examples/sec; 0.131 sec/batch)
2017-05-09 01:17:53.035592: step 19900, loss = 0.79 (917.4 examples/sec; 0.140 sec/batch)
2017-05-09 01:17:54.221664: step 19910, loss = 1.00 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-09 01:17:55.503524: step 19920, loss = 1.04 (998.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:17:56.802388: step 19930, loss = 0.82 (985.5 examples/sec; 0.130 sec/batch)
2017-05-09 01:17:58.074843: step 19940, loss = 0.89 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:17:59.384498: step 19950, loss = 0.81 (977.3 examples/sec; 0.131 sec/batch)
2017-05-09 01:18:00.642533: step 19960, loss = 0.82 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:18:01.914671: step 19970, loss = 0.92 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:18:03.184922: step 19980, loss = 0.79 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:18:04.464186: step 19990, loss = 0.94 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:18:05.832568: step 20000, loss = 0.79 (935.4 examples/sec; 0.137 sec/batch)
2017-05-09 01:18:07.016990: step 20010, loss = 0.77 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-09 01:18:08.308420: step 20020, loss = 0.85 (991.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:18:09.604021: step 20030, loss = 1.06 (988.0 examples/sec; 0.130 sec/batch)
2017-05-09 01:18:10.900298: step 20040, loss = 0.93 (987.4 examples/sec; 0.130 sec/batch)
2017-05-09 01:18:12.199634: step 20050, loss = 0.96 (985.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:18:13.497703: step 20060, loss = 0.93 (986.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:18:14.783104: step 20070, loss = 0.74 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:18:16.063480: step 20080, loss = 0.91 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:18:17.333045: step 20090, loss = 0.88 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:18:18.702644: step 20100, loss = 0.89 (934.6 examples/sec; 0.137 sec/batch)
2017-05-09 01:18:19.881693: step 20110, loss = 0.83 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-09 01:18:21.158708: step 20120, loss = 0.91 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:18:22.423383: step 20130, loss = 0.84 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:18:23.728946: step 20140, loss = 0.89 (980.4 examples/sec; 0.131 sec/batch)
2017-05-09 01:18:25.047237: step 20150, loss = 1.01 (971.0 examples/sec; 0.132 sec/batch)
2017-05-09 01:18:26.306624: step 20160, loss = 0.90 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:18:27.581754: step 20170, loss = 0.89 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:18:28.873470: step 20180, loss = 0.85 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:18:30.152989: step 20190, loss = 0.91 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:18:31.522235: step 20200, loss = 0.79 (934.8 examples/sec; 0.137 sec/batch)
2017-05-09 01:18:32.715053: step 20210, loss = 0.95 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-09 01:18:34.008564: step 20220, loss = 0.71 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:18:35.319624: step 20230, loss = 1.02 (976.3 examples/sec; 0.131 sec/batch)
2017-05-09 01:18:36.590067: step 20240, loss = 0.88 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:18:37.865822: step 20250, loss = 0.92 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:18:39.131101: step 20260, loss = 0.89 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:18:40.422843: step 20270, loss = 0.79 (990.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:18:41.693283: step 20280, loss = 0.80 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:18:42.976282: step 20290, loss = 0.78 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:18:44.352730: step 20300, loss = 0.81 (929.9 examples/sec; 0.138 sec/batch)
2017-05-09 01:18:45.519598: step 20310, loss = 0.70 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-09 01:18:46.801762: step 20320, loss = 0.82 (998.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:18:48.098405: step 20330, loss = 1.00 (987.2 examples/sec; 0.130 sec/batch)
2017-05-09 01:18:49.404966: step 20340, loss = 0.84 (979.7 examples/sec; 0.131 sec/batch)
2017-05-09 01:18:50.668948: step 20350, loss = 0.92 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:18:51.935700: step 20360, loss = 0.76 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:18:53.225089: step 20370, loss = 0.90 (992.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:18:54.515413: step 20380, loss = 0.92 (992.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:18:55.813000: step 20390, loss = 0.82 (986.4 examples/sec; 0.130 sec/batch)
2017-05-09 01:18:57.183668: step 20400, loss = 0.93 (933.9 examples/sec; 0.137 sec/batch)
2017-05-09 01:18:58.354940: step 20410, loss = 0.79 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-09 01:18:59.626793: step 20420, loss = 0.88 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:19:00.905885: step 20430, loss = 0.79 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:19:02.192863: step 20440, loss = 0.86 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:19:03.450269: step 20450, loss = 1.22 (1018.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:19:04.750879: step 20460, loss = 0.96 (984.2 examples/sec; 0.130 sec/batch)
2017-05-09 01:19:06.049373: step 20470, loss = 0.86 (985.7 examples/sec; 0.130 sec/batch)
2017-05-09 01:19:07.340110: step 20480, loss = 0.82 (991.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:19:08.615867: step 20490, loss = 0.90 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:19:09.981484: stE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 427 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
ep 20500, loss = 0.71 (937.3 examples/sec; 0.137 sec/batch)
2017-05-09 01:19:11.159610: step 20510, loss = 0.89 (1086.5 examples/sec; 0.118 sec/batch)
2017-05-09 01:19:12.410481: step 20520, loss = 0.98 (1023.3 examples/sec; 0.125 sec/batch)
2017-05-09 01:19:13.656819: step 20530, loss = 0.85 (1027.0 examples/sec; 0.125 sec/batch)
2017-05-09 01:19:14.954969: step 20540, loss = 1.30 (986.0 examples/sec; 0.130 sec/batch)
2017-05-09 01:19:16.247555: step 20550, loss = 0.93 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:19:17.544682: step 20560, loss = 0.86 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:19:18.823070: step 20570, loss = 0.99 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:19:20.059483: step 20580, loss = 0.84 (1035.3 examples/sec; 0.124 sec/batch)
2017-05-09 01:19:21.318524: step 20590, loss = 0.81 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:19:22.678800: step 20600, loss = 0.80 (941.0 examples/sec; 0.136 sec/batch)
2017-05-09 01:19:23.873360: step 20610, loss = 0.82 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-09 01:19:25.181998: step 20620, loss = 0.97 (978.1 examples/sec; 0.131 sec/batch)
2017-05-09 01:19:26.477708: step 20630, loss = 0.73 (987.9 examples/sec; 0.130 sec/batch)
2017-05-09 01:19:27.746028: step 20640, loss = 0.74 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:19:29.061485: step 20650, loss = 0.80 (973.0 examples/sec; 0.132 sec/batch)
2017-05-09 01:19:30.331065: step 20660, loss = 0.75 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:19:31.624902: step 20670, loss = 0.78 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:19:32.912620: step 20680, loss = 0.96 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:19:34.205206: step 20690, loss = 0.80 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:19:35.569753: step 20700, loss = 0.83 (938.0 examples/sec; 0.136 sec/batch)
2017-05-09 01:19:36.762582: step 20710, loss = 0.86 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-09 01:19:38.043459: step 20720, loss = 0.85 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:19:39.327707: step 20730, loss = 0.81 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:19:40.628124: step 20740, loss = 0.93 (984.3 examples/sec; 0.130 sec/batch)
2017-05-09 01:19:41.898952: step 20750, loss = 1.00 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:19:43.175624: step 20760, loss = 0.99 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:19:44.455193: step 20770, loss = 0.78 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:19:45.724535: step 20780, loss = 1.15 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:19:47.042717: step 20790, loss = 0.71 (971.0 examples/sec; 0.132 sec/batch)
2017-05-09 01:19:48.402438: step 20800, loss = 0.86 (941.4 examples/sec; 0.136 sec/batch)
2017-05-09 01:19:49.597800: step 20810, loss = 0.82 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-09 01:19:50.864555: step 20820, loss = 0.80 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:19:52.143029: step 20830, loss = 0.71 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:19:53.420269: step 20840, loss = 0.78 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:19:54.696020: step 20850, loss = 0.67 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:19:55.978438: step 20860, loss = 0.91 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:19:57.264976: step 20870, loss = 0.79 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:19:58.566303: step 20880, loss = 0.84 (983.6 examples/sec; 0.130 sec/batch)
2017-05-09 01:19:59.844197: step 20890, loss = 1.07 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:20:01.216719: step 20900, loss = 0.92 (932.6 examples/sec; 0.137 sec/batch)
2017-05-09 01:20:02.407459: step 20910, loss = 0.83 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-09 01:20:03.686513: step 20920, loss = 0.70 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:20:04.954715: step 20930, loss = 0.89 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:20:06.221164: step 20940, loss = 0.97 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:20:07.506597: step 20950, loss = 0.74 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:20:08.826578: step 20960, loss = 0.83 (969.7 examples/sec; 0.132 sec/batch)
2017-05-09 01:20:10.092770: step 20970, loss = 1.03 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:20:11.382841: step 20980, loss = 0.76 (992.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:20:12.659065: step 20990, loss = 0.89 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:20:14.027868: step 21000, loss = 0.95 (935.1 examples/sec; 0.137 sec/batch)
2017-05-09 01:20:15.198209: step 21010, loss = 0.94 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-09 01:20:16.473818: step 21020, loss = 0.76 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:20:17.720834: step 21030, loss = 0.88 (1026.5 examples/sec; 0.125 sec/batch)
2017-05-09 01:20:19.030746: step 21040, loss = 0.87 (977.2 examples/sec; 0.131 sec/batch)
2017-05-09 01:20:20.313897: step 21050, loss = 1.02 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:20:21.589201: step 21060, loss = 1.03 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:20:22.863591: step 21070, loss = 0.95 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:20:24.126975: step 21080, loss = 0.92 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:20:25.405959: step 21090, loss = 0.96 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:20:26.792512: step 21100, loss = 1.01 (923.2 examples/sec; 0.139 sec/batch)
2017-05-09 01:20:27.988578: step 21110, loss = 0.85 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-09 01:20:29.272869: step 21120, loss = 0.90 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:20:30.517176: step 21130, loss = 0.98 (1028.7 examples/sec; 0.124 sec/batch)
2017-05-09 01:20:31.786146: step 21140, loss = 1.00 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:20:33.045133: step 21150, loss = 0.76 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:20:34.314044: step 21160, loss = 0.86 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:20:35.577970: step 21170, loss = 0.88 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:20:36.855247: step 21180, loss = 0.82 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:20:38.131656: step 21190, loss = 0.82 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:20:39.503750: step 21200, loss = 1.00 (932.9 examples/sec; 0.137 sec/batch)
2017-05-09 01:20:40.689768: step 21210, loss = 0.81 (1079.2 examples/sec; 0.119 sec/batch)
2017-05-09 01:20:41.983581: step 21220, loss = 0.88 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:20:43.284744: step 21230, loss = 1.03 (983.7 examples/sec; 0.130 sec/batch)
2017-05-09 01:20:44.581418: step 21240, loss = 0.77 (987.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:20:45.870041: step 21250, loss = 0.81 (993.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:20:47.157788: step 21260, loss = 0.79 (994.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:20:48.428043: step 21270, loss = 0.87 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:20:49.686800: step 21280, loss = 0.81 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:20:50.962949: step 21290, loss = 0.84 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:20:52.335975: step 21300, loss = 0.77 (932.2 examples/sec; 0.137 sec/batch)
2017-05-09 01:20:53.495455: step 21310, loss = 0.93 (1103.9 examples/sec; 0.116 sec/batch)
2017-05-09 01:20:54.759767: step 21320, loss = 0.83 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:20:56.037441: step 21330, loss = 1.06 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:20:57.315047: step 21340, loss = 0.86 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:20:58.580363: step 21350, loss = 0.71 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:20:59.853452: step 21360, loss = 0.97 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:21:01.161704: step 21370, loss = 0.90 (978.4 examples/sec; 0.131 sec/batch)
2017-05-09 01:21:02.438073: step 21380, loss = 0.78 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:21:03.742778: step 21390, loss = 0.81 (981.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:21:05.144168: step 21400, loss = 0.90 (913.4 examples/sec; 0.140 sec/batch)
2017-05-09 01:21:06.337941: step 21410, loss = 1.01 (1072.2 examples/sec; 0.119 sec/batch)
2017-05-09 01:21:07.651726: step 21420, loss = 0.89 (974.3 examples/sec; 0.131 sec/batch)
2017-05-09 01:21:08.918382: step 21430, loss = 0.86 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:21:10.185787: step 21440, loss = 0.75 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:21:11.463607: step 21450, loss = 0.80 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:21:12.743006: step 21460, loss = 0.84 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:21:14.012480: step 21470, loss = 0.71 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:21:15.324206: step 21480, loss = 0.90 (975.8 examples/sec; 0.131 sec/batch)
2017-05-09 01:21:16.632417: step 21490, loss = 0.93 (978.4 examples/sec; 0.131 sec/batch)
2017-05-09 01:21:18.001042: step 21500, loss = 1.24 (935.2 examples/sec; 0.137 sec/batch)
2017-05-09 01:21:19.205052: step 21510, loss = 0.88 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-09 01:21:20.481565: step 21520, loss = 0.76 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:21:21.759346: step 21530, loss = 0.77 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:21:23.022231: step 21540, loss = 1.20 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:21:24.305970: step 21550, loss = 0.69 (997.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:21:25.587201: step 21560, loss = 0.96 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:21:26.908456: step 21570, loss = 0.69 (968.8 examples/sec; 0.132 sec/batch)
2017-05-09 01:21:28.170242: step 21580, loss = 0.97 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:21:29.471556: step 21590, loss = 0.85 (983.6 examples/sec; 0.130 sec/batch)
2017-05-09 01:21:30.834552: step 21600, loss = 0.97 (939.1 examples/sec; 0.136 sec/batch)
2017-05-09 01:21:32.005027: step 21610, loss = 1.03 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-09 01:21:33.296999: step 21620, loss = 0.95 (990.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:21:34.558093: step 21630, loss = 0.83 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:21:35.830769: step 21640, loss = 0.87 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:21:37.110144: step 21650, loss = 1.03 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:21:38.391450: step 21660, loss = 1.01 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:21:39.685650: step 21670, loss = 0.78 (989.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:21:40.966025: step 21680, loss = 0.75 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:21:42.224598: step 21690, loss = 1.08 (1017.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:21:43.628672: step 21700, loss = 0.70 (911.6 examples/sec; 0.140 sec/batch)
2017-05-09 01:21:44.826104: step 21710, loss = 0.73 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-09 01:21:46.110733: step 21720, loss = 0.96 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:21:47.425943: step 21730, loss = 0.97 (973.2 examples/sec; 0.132 sec/batch)
2017-05-09 01:21:48.696458: step 21740, loss = 0.90 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:21:49.958522: step 21750, loss = 0.76 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:21:51.219646: step 21760, loss = 0.68 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:21:52.486958: step 21770, loss = 0.87 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:21:53.759217: step 21780, loss = 0.82 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:21:55.020123: step 21790, loss = 0.79 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:21:56.382594: step 21800, loss = 0.82 (939.5 examples/sec; 0.136 sec/batch)
2017-05-09 01:21:57.551950: step 21810, loss = 0.77 (1094.6 examples/sec; 0.117 sec/batch)
2017-05-09 01:21:58.812671: step 21820, loss = 0.87 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:22:00.088784: step 21830, loss = 0.78 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:22:01.359643: step 21840, loss = 0.90 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:22:02.616713: step 21850, loss = 1.00 (1018.2 examples/sec; 0.126 sec/batchE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 447 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
)
2017-05-09 01:22:03.894187: step 21860, loss = 0.67 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:22:05.187671: step 21870, loss = 0.95 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:22:06.452122: step 21880, loss = 0.72 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:22:07.712974: step 21890, loss = 0.86 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:22:09.090202: step 21900, loss = 0.97 (929.4 examples/sec; 0.138 sec/batch)
2017-05-09 01:22:10.287525: step 21910, loss = 0.90 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-09 01:22:11.577455: step 21920, loss = 0.87 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:22:12.853967: step 21930, loss = 0.90 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:22:14.122637: step 21940, loss = 0.98 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:22:15.404352: step 21950, loss = 0.72 (998.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:22:16.715262: step 21960, loss = 1.16 (976.4 examples/sec; 0.131 sec/batch)
2017-05-09 01:22:18.001990: step 21970, loss = 0.72 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:22:19.268192: step 21980, loss = 0.73 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:22:20.552246: step 21990, loss = 0.87 (996.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:22:21.948660: step 22000, loss = 1.03 (916.6 examples/sec; 0.140 sec/batch)
2017-05-09 01:22:23.154138: step 22010, loss = 0.90 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-09 01:22:24.440076: step 22020, loss = 0.97 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:22:25.706864: step 22030, loss = 0.83 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:22:26.982163: step 22040, loss = 0.90 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:22:28.283795: step 22050, loss = 1.06 (983.4 examples/sec; 0.130 sec/batch)
2017-05-09 01:22:29.537253: step 22060, loss = 0.88 (1021.2 examples/sec; 0.125 sec/batch)
2017-05-09 01:22:30.834735: step 22070, loss = 1.10 (986.5 examples/sec; 0.130 sec/batch)
2017-05-09 01:22:32.116205: step 22080, loss = 0.92 (998.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:22:33.388366: step 22090, loss = 0.92 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:22:34.780593: step 22100, loss = 0.99 (919.4 examples/sec; 0.139 sec/batch)
2017-05-09 01:22:35.950786: step 22110, loss = 0.91 (1093.8 examples/sec; 0.117 sec/batch)
2017-05-09 01:22:37.244137: step 22120, loss = 0.85 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:22:38.534932: step 22130, loss = 1.17 (991.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:22:39.795475: step 22140, loss = 0.79 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:22:41.088064: step 22150, loss = 0.82 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:22:42.364864: step 22160, loss = 0.68 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:22:43.648951: step 22170, loss = 0.87 (996.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:22:44.957266: step 22180, loss = 0.93 (978.4 examples/sec; 0.131 sec/batch)
2017-05-09 01:22:46.255641: step 22190, loss = 1.05 (985.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:22:47.624724: step 22200, loss = 0.71 (934.9 examples/sec; 0.137 sec/batch)
2017-05-09 01:22:48.797535: step 22210, loss = 0.98 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-09 01:22:50.057156: step 22220, loss = 0.97 (1016.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:22:51.351033: step 22230, loss = 0.80 (989.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:22:52.653409: step 22240, loss = 0.82 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:22:53.905176: step 22250, loss = 0.76 (1022.6 examples/sec; 0.125 sec/batch)
2017-05-09 01:22:55.174547: step 22260, loss = 0.87 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:22:56.437893: step 22270, loss = 0.93 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:22:57.701538: step 22280, loss = 1.02 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:22:58.966006: step 22290, loss = 1.05 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:23:00.353877: step 22300, loss = 0.92 (922.3 examples/sec; 0.139 sec/batch)
2017-05-09 01:23:01.555165: step 22310, loss = 0.73 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-09 01:23:02.859157: step 22320, loss = 0.78 (981.6 examples/sec; 0.130 sec/batch)
2017-05-09 01:23:04.138224: step 22330, loss = 0.86 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:23:05.416821: step 22340, loss = 0.72 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:23:06.703169: step 22350, loss = 0.94 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:23:07.965692: step 22360, loss = 0.92 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:23:09.226230: step 22370, loss = 0.98 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:23:10.472281: step 22380, loss = 0.89 (1027.2 examples/sec; 0.125 sec/batch)
2017-05-09 01:23:11.736925: step 22390, loss = 0.96 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:23:13.094928: step 22400, loss = 0.82 (942.6 examples/sec; 0.136 sec/batch)
2017-05-09 01:23:14.285883: step 22410, loss = 0.99 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-09 01:23:15.548758: step 22420, loss = 0.92 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:23:16.811803: step 22430, loss = 0.87 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:23:18.076801: step 22440, loss = 0.93 (1011.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:23:19.334813: step 22450, loss = 0.69 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:23:20.616909: step 22460, loss = 0.72 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:23:21.894775: step 22470, loss = 0.86 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:23:23.155690: step 22480, loss = 0.94 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:23:24.448088: step 22490, loss = 0.71 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:23:25.800351: step 22500, loss = 0.84 (946.6 examples/sec; 0.135 sec/batch)
2017-05-09 01:23:27.018242: step 22510, loss = 1.02 (1051.0 examples/sec; 0.122 sec/batch)
2017-05-09 01:23:28.324245: step 22520, loss = 0.87 (980.1 examples/sec; 0.131 sec/batch)
2017-05-09 01:23:29.593750: step 22530, loss = 0.80 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:23:30.855635: step 22540, loss = 0.80 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:23:32.119377: step 22550, loss = 0.90 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:23:33.389481: step 22560, loss = 0.98 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:23:34.666155: step 22570, loss = 0.72 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:23:35.955013: step 22580, loss = 0.84 (993.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:23:37.260447: step 22590, loss = 0.98 (980.5 examples/sec; 0.131 sec/batch)
2017-05-09 01:23:38.639160: step 22600, loss = 0.79 (928.4 examples/sec; 0.138 sec/batch)
2017-05-09 01:23:39.829718: step 22610, loss = 1.03 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-09 01:23:41.125412: step 22620, loss = 0.92 (987.9 examples/sec; 0.130 sec/batch)
2017-05-09 01:23:42.408870: step 22630, loss = 0.95 (997.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:23:43.682754: step 22640, loss = 0.82 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:23:44.955003: step 22650, loss = 0.73 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:23:46.238020: step 22660, loss = 0.73 (997.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:23:47.526878: step 22670, loss = 0.87 (993.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:23:48.819298: step 22680, loss = 1.15 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:23:50.118261: step 22690, loss = 0.99 (985.4 examples/sec; 0.130 sec/batch)
2017-05-09 01:23:51.485449: step 22700, loss = 0.83 (936.2 examples/sec; 0.137 sec/batch)
2017-05-09 01:23:52.668307: step 22710, loss = 0.94 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-09 01:23:53.923256: step 22720, loss = 0.86 (1020.0 examples/sec; 0.125 sec/batch)
2017-05-09 01:23:55.188100: step 22730, loss = 0.91 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:23:56.468425: step 22740, loss = 0.73 (999.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:23:57.726258: step 22750, loss = 0.77 (1017.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:23:58.994179: step 22760, lE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 467 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
oss = 0.94 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:24:00.267206: step 22770, loss = 0.92 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:24:01.532054: step 22780, loss = 0.95 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:24:02.813069: step 22790, loss = 0.91 (999.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:24:04.213957: step 22800, loss = 0.87 (913.6 examples/sec; 0.140 sec/batch)
2017-05-09 01:24:05.395850: step 22810, loss = 0.79 (1083.0 examples/sec; 0.118 sec/batch)
2017-05-09 01:24:06.693544: step 22820, loss = 0.88 (986.4 examples/sec; 0.130 sec/batch)
2017-05-09 01:24:07.987055: step 22830, loss = 0.91 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:24:09.283670: step 22840, loss = 0.81 (987.2 examples/sec; 0.130 sec/batch)
2017-05-09 01:24:10.541063: step 22850, loss = 0.98 (1018.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:24:11.824707: step 22860, loss = 0.87 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:24:13.148470: step 22870, loss = 0.95 (966.9 examples/sec; 0.132 sec/batch)
2017-05-09 01:24:14.439577: step 22880, loss = 1.09 (991.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:24:15.743983: step 22890, loss = 1.06 (981.3 examples/sec; 0.130 sec/batch)
2017-05-09 01:24:17.101928: step 22900, loss = 0.79 (942.6 examples/sec; 0.136 sec/batch)
2017-05-09 01:24:18.289681: step 22910, loss = 0.74 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-09 01:24:19.560415: step 22920, loss = 0.96 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:24:20.871086: step 22930, loss = 0.77 (976.6 examples/sec; 0.131 sec/batch)
2017-05-09 01:24:22.142044: step 22940, loss = 0.92 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:24:23.472005: step 22950, loss = 0.72 (962.4 examples/sec; 0.133 sec/batch)
2017-05-09 01:24:24.761623: step 22960, loss = 0.85 (992.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:24:26.021496: step 22970, loss = 0.89 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:24:27.313935: step 22980, loss = 0.80 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:24:28.591923: step 22990, loss = 0.83 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:24:29.965485: step 23000, loss = 0.70 (931.9 examples/sec; 0.137 sec/batch)
2017-05-09 01:24:31.185089: step 23010, loss = 0.98 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-09 01:24:32.470825: step 23020, loss = 0.98 (995.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:24:33.734474: step 23030, loss = 0.86 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:24:35.017140: step 23040, loss = 0.86 (997.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:24:36.277597: step 23050, loss = 0.92 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:24:37.554508: step 23060, loss = 0.87 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:24:38.819347: step 23070, loss = 0.87 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:24:40.106580: step 23080, loss = 0.85 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:24:41.381562: step 23090, loss = 0.78 (1003.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:24:42.776293: step 23100, loss = 0.80 (917.7 examples/sec; 0.139 sec/batch)
2017-05-09 01:24:43.930888: step 23110, loss = 0.74 (1108.6 examples/sec; 0.115 sec/batch)
2017-05-09 01:24:45.208466: step 23120, loss = 0.93 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:24:46.462176: step 23130, loss = 0.94 (1021.0 examples/sec; 0.125 sec/batch)
2017-05-09 01:24:47.739296: step 23140, loss = 0.88 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:24:49.016967: step 23150, loss = 0.83 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:24:50.329478: step 23160, loss = 0.92 (975.2 examples/sec; 0.131 sec/batch)
2017-05-09 01:24:51.622162: step 23170, loss = 0.95 (990.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:24:52.891351: step 23180, loss = 0.84 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:24:54.159097: step 23190, loss = 0.67 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:24:55.535911: step 23200, loss = 0.94 (929.7 examples/sec; 0.138 sec/batch)
2017-05-09 01:24:56.719529: step 23210, loss = 0.87 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-09 01:24:58.037764: step 23220, loss = 0.78 (971.0 examples/sec; 0.132 sec/batch)
2017-05-09 01:24:59.279552: step 23230, loss = 0.86 (1030.8 examples/sec; 0.124 sec/batch)
2017-05-09 01:25:00.596353: step 23240, loss = 0.86 (972.0 examples/sec; 0.132 sec/batch)
2017-05-09 01:25:01.916363: step 23250, loss = 0.81 (969.7 examples/sec; 0.132 sec/batch)
2017-05-09 01:25:03.204523: step 23260, loss = 0.88 (993.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:25:04.468136: step 23270, loss = 0.92 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:25:05.731023: step 23280, loss = 0.84 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:25:07.018253: step 23290, loss = 0.79 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:25:08.409030: step 23300, loss = 0.86 (920.4 examples/sec; 0.139 sec/batch)
2017-05-09 01:25:09.604498: step 23310, loss = 0.80 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-09 01:25:10.919957: step 23320, loss = 0.92 (973.0 examples/sec; 0.132 sec/batch)
2017-05-09 01:25:12.185069: step 23330, loss = 0.89 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:25:13.454265: step 23340, loss = 0.74 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:25:14.742533: step 23350, loss = 1.00 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:25:16.012865: step 23360, loss = 0.73 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:25:17.309282: step 23370, loss = 0.71 (987.3 examples/sec; 0.130 sec/batch)
2017-05-09 01:25:18.583045: step 23380, loss = 0.80 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:25:19.887237: step 23390, loss = 0.92 (981.5 examples/sec; 0.130 sec/batch)
2017-05-09 01:25:21.258919: step 23400, loss = 0.99 (933.2 examples/sec; 0.137 sec/batch)
2017-05-09 01:25:22.430336: step 23410, loss = 0.79 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-09 01:25:23.710525: step 23420, loss = 0.84 (999.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:25:24.999861: step 23430, loss = 0.86 (992.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:25:26.259661: step 23440, loss = 0.83 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:25:27.556762: step 23450, loss = 0.82 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:25:28.809747: step 23460, loss = 0.85 (1021.6 examples/sec; 0.125 sec/batch)
2017-05-09 01:25:30.101751: step 23470, loss = 0.93 (990.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:25:31.396616: step 23480, loss = 0.83 (988.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:25:32.685405: step 23490, loss = 1.28 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:25:34.081701: step 23500, loss = 0.90 (916.7 examples/sec; 0.140 sec/batch)
2017-05-09 01:25:35.259168: step 23510, loss = 1.03 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-09 01:25:36.593754: step 23520, loss = 0.87 (959.1 examples/sec; 0.133 sec/batch)
2017-05-09 01:25:37.858260: step 23530, loss = 0.72 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:25:39.137981: step 23540, loss = 0.80 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:25:40.433355: step 23550, loss = 0.83 (988.2 examples/sec; 0.130 sec/batch)
2017-05-09 01:25:41.696694: step 23560, loss = 0.92 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:25:42.944300: step 23570, loss = 0.82 (1026.0 examples/sec; 0.125 sec/batch)
2017-05-09 01:25:44.213431: step 23580, loss = 0.90 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:25:45.496421: step 23590, loss = 0.86 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:25:46.872116: step 23600, loss = 0.96 (930.4 examples/sec; 0.138 sec/batch)
2017-05-09 01:25:48.043863: step 23610, loss = 0.87 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-09 01:25:49.321708: step 23620, loss = 0.98 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:25:50.595030: step 23630, loss = 0.82 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:25:51.873398: step 23640, loss = 0.87 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:25:53.170819: step 23650, loss = 0.75 (986.6 examples/sec; 0.130 sec/batch)
2017-05-09 01:25:54.454563: step 23660, loss = 0.74 (997.1 examples/sec; 0.128 sec/batchE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 488 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
)
2017-05-09 01:25:55.727568: step 23670, loss = 0.83 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:25:56.984281: step 23680, loss = 0.89 (1018.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:25:58.240281: step 23690, loss = 0.76 (1019.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:25:59.609980: step 23700, loss = 0.99 (934.5 examples/sec; 0.137 sec/batch)
2017-05-09 01:26:00.782511: step 23710, loss = 0.74 (1091.7 examples/sec; 0.117 sec/batch)
2017-05-09 01:26:02.048557: step 23720, loss = 0.93 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:26:03.329556: step 23730, loss = 0.94 (999.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:26:04.625678: step 23740, loss = 0.98 (987.6 examples/sec; 0.130 sec/batch)
2017-05-09 01:26:05.893052: step 23750, loss = 0.80 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:26:07.167791: step 23760, loss = 0.88 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:26:08.459236: step 23770, loss = 0.85 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:26:09.739291: step 23780, loss = 0.90 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:26:11.026463: step 23790, loss = 0.84 (994.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:26:12.406116: step 23800, loss = 0.75 (927.8 examples/sec; 0.138 sec/batch)
2017-05-09 01:26:13.587672: step 23810, loss = 0.93 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-09 01:26:14.906148: step 23820, loss = 0.66 (970.8 examples/sec; 0.132 sec/batch)
2017-05-09 01:26:16.170782: step 23830, loss = 0.93 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:26:17.444586: step 23840, loss = 0.86 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:26:18.707640: step 23850, loss = 0.93 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:26:19.992962: step 23860, loss = 0.77 (995.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:26:21.267095: step 23870, loss = 0.77 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:26:22.539014: step 23880, loss = 0.88 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:26:23.820247: step 23890, loss = 0.82 (999.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:26:25.188826: step 23900, loss = 0.88 (935.3 examples/sec; 0.137 sec/batch)
2017-05-09 01:26:26.356175: step 23910, loss = 0.77 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-09 01:26:27.618921: step 23920, loss = 0.72 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:26:28.891554: step 23930, loss = 0.87 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:26:30.187625: step 23940, loss = 0.91 (987.6 examples/sec; 0.130 sec/batch)
2017-05-09 01:26:31.452278: step 23950, loss = 0.82 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:26:32.757451: step 23960, loss = 0.90 (980.7 examples/sec; 0.131 sec/batch)
2017-05-09 01:26:34.024571: step 23970, loss = 0.77 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:26:35.339250: step 23980, loss = 0.96 (973.6 examples/sec; 0.131 sec/batch)
2017-05-09 01:26:36.628159: step 23990, loss = 0.92 (993.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:26:38.016146: step 24000, loss = 0.95 (922.2 examples/sec; 0.139 sec/batch)
2017-05-09 01:26:39.202366: step 24010, loss = 0.84 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-09 01:26:40.472856: step 24020, loss = 0.94 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:26:41.756393: step 24030, loss = 0.81 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:26:43.029905: step 24040, loss = 0.80 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:26:44.296395: step 24050, loss = 0.78 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:26:45.587378: step 24060, loss = 0.93 (991.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:26:46.851588: step 24070, loss = 1.05 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:26:48.129229: step 24080, loss = 0.89 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:26:49.381689: step 24090, loss = 0.77 (1022.0 examples/sec; 0.125 sec/batch)
2017-05-09 01:26:50.737589: step 24100, loss = 0.80 (944.0 examples/sec; 0.136 sec/batch)
2017-05-09 01:26:51.926020: step 24110, loss = 0.77 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-09 01:26:53.208282: step 24120, loss = 0.97 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:26:54.473356: step 24130, loss = 0.75 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:26:55.745275: step 24140, loss = 0.75 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:26:57.003245: step 24150, loss = 0.82 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:26:58.277484: step 24160, loss = 0.83 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:26:59.542047: step 24170, loss = 0.89 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:27:00.839900: step 24180, loss = 0.83 (986.2 examples/sec; 0.130 sec/batch)
2017-05-09 01:27:02.116231: step 24190, loss = 1.05 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:27:03.505493: step 24200, loss = 0.84 (921.4 examples/sec; 0.139 sec/batch)
2017-05-09 01:27:04.684284: step 24210, loss = 0.81 (1085.9 examples/sec; 0.118 sec/batch)
2017-05-09 01:27:05.953527: step 24220, loss = 0.91 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:27:07.228726: step 24230, loss = 0.73 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:27:08.510815: step 24240, loss = 0.85 (998.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:27:09.782042: step 24250, loss = 0.83 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:27:11.075522: step 24260, loss = 1.02 (989.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:27:12.374531: step 24270, loss = 0.71 (985.4 examples/sec; 0.130 sec/batch)
2017-05-09 01:27:13.649382: step 24280, loss = 0.87 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:27:14.926577: step 24290, loss = 0.80 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:27:16.302264: step 24300, loss = 1.13 (930.4 examples/sec; 0.138 sec/batch)
2017-05-09 01:27:17.466374: step 24310, loss = 0.91 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-09 01:27:18.738428: step 24320, loss = 0.89 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:27:20.008907: step 24330, loss = 0.76 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:27:21.271291: step 24340, loss = 0.95 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:27:22.552815: step 24350, loss = 0.69 (998.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:27:23.829166: step 24360, loss = 0.80 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:27:25.103884: step 24370, loss = 1.00 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:27:26.359101: step 24380, loss = 0.95 (1019.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:27:27.641960: step 24390, loss = 0.96 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:27:29.053665: step 24400, loss = 1.03 (906.7 examples/sec; 0.141 sec/batch)
2017-05-09 01:27:30.237018: step 24410, loss = 0.89 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-09 01:27:31.516413: step 24420, loss = 0.90 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:27:32.788434: step 24430, loss = 1.07 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:27:34.093760: step 24440, loss = 0.70 (980.6 examples/sec; 0.131 sec/batch)
2017-05-09 01:27:35.357377: step 24450, loss = 0.93 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:27:36.637149: step 24460, loss = 0.90 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:27:37.894739: step 24470, loss = 0.69 (1017.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:27:39.191804: step 24480, loss = 1.06 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:27:40.477017: step 24490, loss = 0.88 (995.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:27:41.823801: step 24500, loss = 0.92 (950.4 examples/sec; 0.135 sec/batch)
2017-05-09 01:27:42.992426: step 24510, loss = 1.10 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-09 01:27:44.259007: step 24520, loss = 0.84 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:27:45.534364: step 24530, loss = 0.71 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:27:46.810389: step 24540, loss = 0.93 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:27:48.069838: step 24550, loss = 0.77 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:27:49.349395: step 24560, loss = 0.87 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:27:50.630884: stepE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 508 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
 24570, loss = 0.75 (998.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:27:51.910178: step 24580, loss = 0.71 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:27:53.171537: step 24590, loss = 0.86 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:27:54.531617: step 24600, loss = 0.79 (941.1 examples/sec; 0.136 sec/batch)
2017-05-09 01:27:55.720172: step 24610, loss = 0.87 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-09 01:27:57.017965: step 24620, loss = 0.75 (986.3 examples/sec; 0.130 sec/batch)
2017-05-09 01:27:58.320404: step 24630, loss = 0.93 (982.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:27:59.610289: step 24640, loss = 0.87 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:28:00.880750: step 24650, loss = 0.82 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:28:02.165188: step 24660, loss = 0.76 (996.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:28:03.442126: step 24670, loss = 0.96 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:28:04.720679: step 24680, loss = 0.84 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:28:05.984609: step 24690, loss = 0.87 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-09 01:28:07.373371: step 24700, loss = 0.64 (921.7 examples/sec; 0.139 sec/batch)
2017-05-09 01:28:08.561301: step 24710, loss = 0.81 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-09 01:28:09.822938: step 24720, loss = 0.84 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:28:11.087946: step 24730, loss = 0.91 (1011.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:28:12.373294: step 24740, loss = 0.98 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:28:13.679197: step 24750, loss = 1.07 (980.2 examples/sec; 0.131 sec/batch)
2017-05-09 01:28:14.978484: step 24760, loss = 0.87 (985.2 examples/sec; 0.130 sec/batch)
2017-05-09 01:28:16.264919: step 24770, loss = 0.85 (995.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:28:17.552439: step 24780, loss = 0.74 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:28:18.814682: step 24790, loss = 0.73 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:28:20.188829: step 24800, loss = 1.11 (931.5 examples/sec; 0.137 sec/batch)
2017-05-09 01:28:21.402356: step 24810, loss = 0.75 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-09 01:28:22.667782: step 24820, loss = 1.01 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:28:23.932822: step 24830, loss = 0.68 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:28:25.202769: step 24840, loss = 0.93 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:28:26.470869: step 24850, loss = 0.79 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:28:27.731404: step 24860, loss = 0.79 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:28:29.019922: step 24870, loss = 0.78 (993.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:28:30.275639: step 24880, loss = 0.73 (1019.3 examples/sec; 0.126 sec/batch)
2017-05-09 01:28:31.564374: step 24890, loss = 0.83 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:28:32.916060: step 24900, loss = 0.79 (947.0 examples/sec; 0.135 sec/batch)
2017-05-09 01:28:34.061452: step 24910, loss = 0.87 (1117.5 examples/sec; 0.115 sec/batch)
2017-05-09 01:28:35.346049: step 24920, loss = 0.67 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:28:36.607908: step 24930, loss = 0.81 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:28:37.892025: step 24940, loss = 0.83 (996.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:28:39.173382: step 24950, loss = 0.97 (998.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:28:40.459307: step 24960, loss = 0.91 (995.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:28:41.734672: step 24970, loss = 0.71 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:28:43.001099: step 24980, loss = 0.99 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:28:44.297885: step 24990, loss = 0.82 (987.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:28:45.676867: step 25000, loss = 0.81 (928.2 examples/sec; 0.138 sec/batch)
2017-05-09 01:28:46.906055: step 25010, loss = 1.00 (1041.3 examples/sec; 0.123 sec/batch)
2017-05-09 01:28:48.184080: step 25020, loss = 0.92 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:28:49.466457: step 25030, loss = 0.84 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:28:50.724423: step 25040, loss = 0.93 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:28:51.992992: step 25050, loss = 0.85 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:28:53.263473: step 25060, loss = 0.86 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:28:54.515599: step 25070, loss = 0.74 (1022.3 examples/sec; 0.125 sec/batch)
2017-05-09 01:28:55.778710: step 25080, loss = 1.05 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:28:57.062290: step 25090, loss = 0.83 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:28:58.413087: step 25100, loss = 0.95 (947.6 examples/sec; 0.135 sec/batch)
2017-05-09 01:28:59.599034: step 25110, loss = 0.92 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-09 01:29:00.871023: step 25120, loss = 0.96 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:29:02.166755: step 25130, loss = 1.01 (987.9 examples/sec; 0.130 sec/batch)
2017-05-09 01:29:03.458266: step 25140, loss = 0.86 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:29:04.750239: step 25150, loss = 0.85 (990.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:29:06.033461: step 25160, loss = 0.78 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:29:07.319973: step 25170, loss = 0.80 (994.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:29:08.589363: step 25180, loss = 0.94 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:29:09.851038: step 25190, loss = 0.77 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:29:11.234542: step 25200, loss = 0.71 (925.2 examples/sec; 0.138 sec/batch)
2017-05-09 01:29:12.412361: step 25210, loss = 0.81 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-09 01:29:13.689020: step 25220, loss = 0.75 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:29:14.959651: step 25230, loss = 0.87 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:29:16.242867: step 25240, loss = 0.71 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:29:17.516382: step 25250, loss = 0.98 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:29:18.808230: step 25260, loss = 0.78 (990.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:29:20.105483: step 25270, loss = 1.27 (986.7 examples/sec; 0.130 sec/batch)
2017-05-09 01:29:21.351805: step 25280, loss = 0.79 (1027.0 examples/sec; 0.125 sec/batch)
2017-05-09 01:29:22.629376: step 25290, loss = 0.90 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:29:24.014551: step 25300, loss = 0.81 (924.1 examples/sec; 0.139 sec/batch)
2017-05-09 01:29:25.224743: step 25310, loss = 0.79 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-09 01:29:26.514421: step 25320, loss = 0.76 (992.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:29:27.817896: step 25330, loss = 0.69 (982.0 examples/sec; 0.130 sec/batch)
2017-05-09 01:29:29.095871: step 25340, loss = 0.88 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:29:30.375039: step 25350, loss = 0.83 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:29:31.647524: step 25360, loss = 0.80 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:29:32.929823: step 25370, loss = 0.79 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:29:34.236054: step 25380, loss = 0.87 (979.9 examples/sec; 0.131 sec/batch)
2017-05-09 01:29:35.542571: step 25390, loss = 0.91 (979.7 examples/sec; 0.131 sec/batch)
2017-05-09 01:29:36.912867: step 25400, loss = 0.86 (934.1 examples/sec; 0.137 sec/batch)
2017-05-09 01:29:38.118150: step 25410, loss = 1.03 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-09 01:29:39.400999: step 25420, loss = 0.83 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:29:40.678475: step 25430, loss = 0.93 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:29:41.968025: step 25440, loss = 0.90 (992.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:29:43.233641: step 25450, loss = 0.75 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:29:44.497063: step 25460, loss = 0.62 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:29:45.758293: step 25470, loss = 0.75 (1014.9 examples/sec; 0.1E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 528 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
26 sec/batch)
2017-05-09 01:29:47.023070: step 25480, loss = 0.76 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:29:48.290577: step 25490, loss = 0.72 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:29:49.660538: step 25500, loss = 0.82 (934.3 examples/sec; 0.137 sec/batch)
2017-05-09 01:29:50.838516: step 25510, loss = 0.96 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-09 01:29:52.093675: step 25520, loss = 0.87 (1019.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:29:53.376245: step 25530, loss = 0.92 (998.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:29:54.659889: step 25540, loss = 0.91 (997.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:29:55.967973: step 25550, loss = 0.79 (978.5 examples/sec; 0.131 sec/batch)
2017-05-09 01:29:57.265741: step 25560, loss = 0.92 (986.3 examples/sec; 0.130 sec/batch)
2017-05-09 01:29:58.571136: step 25570, loss = 0.94 (980.6 examples/sec; 0.131 sec/batch)
2017-05-09 01:29:59.855471: step 25580, loss = 1.05 (996.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:30:01.142141: step 25590, loss = 0.94 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:30:02.518449: step 25600, loss = 0.87 (930.0 examples/sec; 0.138 sec/batch)
2017-05-09 01:30:03.711913: step 25610, loss = 0.83 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-09 01:30:05.011554: step 25620, loss = 1.08 (984.9 examples/sec; 0.130 sec/batch)
2017-05-09 01:30:06.297133: step 25630, loss = 0.81 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:30:07.576518: step 25640, loss = 0.95 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:30:08.891967: step 25650, loss = 0.92 (973.1 examples/sec; 0.132 sec/batch)
2017-05-09 01:30:10.168858: step 25660, loss = 0.77 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:30:11.429273: step 25670, loss = 0.92 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:30:12.737271: step 25680, loss = 0.91 (978.6 examples/sec; 0.131 sec/batch)
2017-05-09 01:30:14.005983: step 25690, loss = 0.90 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:30:15.389113: step 25700, loss = 1.03 (925.4 examples/sec; 0.138 sec/batch)
2017-05-09 01:30:16.554264: step 25710, loss = 0.60 (1098.6 examples/sec; 0.117 sec/batch)
2017-05-09 01:30:17.849081: step 25720, loss = 0.66 (988.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:30:19.127274: step 25730, loss = 0.75 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:30:20.422784: step 25740, loss = 0.85 (988.0 examples/sec; 0.130 sec/batch)
2017-05-09 01:30:21.730593: step 25750, loss = 0.81 (978.7 examples/sec; 0.131 sec/batch)
2017-05-09 01:30:23.017292: step 25760, loss = 0.82 (994.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:30:24.292080: step 25770, loss = 0.76 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:30:25.569858: step 25780, loss = 0.74 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:30:26.832091: step 25790, loss = 1.40 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:30:28.220476: step 25800, loss = 1.18 (921.9 examples/sec; 0.139 sec/batch)
2017-05-09 01:30:29.393248: step 25810, loss = 0.76 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-09 01:30:30.678025: step 25820, loss = 1.02 (996.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:30:31.979640: step 25830, loss = 0.73 (983.4 examples/sec; 0.130 sec/batch)
2017-05-09 01:30:33.254535: step 25840, loss = 0.75 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:30:34.546016: step 25850, loss = 1.02 (991.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:30:35.830198: step 25860, loss = 0.82 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:30:37.076693: step 25870, loss = 0.81 (1026.9 examples/sec; 0.125 sec/batch)
2017-05-09 01:30:38.350725: step 25880, loss = 0.76 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:30:39.630045: step 25890, loss = 0.90 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:30:40.985521: step 25900, loss = 0.83 (944.3 examples/sec; 0.136 sec/batch)
2017-05-09 01:30:42.156934: step 25910, loss = 0.69 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-09 01:30:43.419559: step 25920, loss = 0.95 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-09 01:30:44.713156: step 25930, loss = 0.99 (989.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:30:45.975444: step 25940, loss = 0.93 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:30:47.229684: step 25950, loss = 0.80 (1020.5 examples/sec; 0.125 sec/batch)
2017-05-09 01:30:48.483243: step 25960, loss = 0.88 (1021.1 examples/sec; 0.125 sec/batch)
2017-05-09 01:30:49.736130: step 25970, loss = 0.95 (1021.6 examples/sec; 0.125 sec/batch)
2017-05-09 01:30:51.020306: step 25980, loss = 0.84 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:30:52.302565: step 25990, loss = 0.90 (998.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:30:53.662377: step 26000, loss = 0.72 (941.3 examples/sec; 0.136 sec/batch)
2017-05-09 01:30:54.834149: step 26010, loss = 0.98 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-09 01:30:56.096606: step 26020, loss = 0.88 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:30:57.377100: step 26030, loss = 0.89 (999.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:30:58.652450: step 26040, loss = 0.90 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:30:59.940858: step 26050, loss = 0.84 (993.5 examples/sec; 0.129 sec/batch)
2017-05-09 01:31:01.237637: step 26060, loss = 0.77 (987.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:31:02.532847: step 26070, loss = 0.87 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 01:31:03.788181: step 26080, loss = 0.91 (1019.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:31:05.069390: step 26090, loss = 0.92 (999.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:31:06.433708: step 26100, loss = 0.74 (938.2 examples/sec; 0.136 sec/batch)
2017-05-09 01:31:07.645799: step 26110, loss = 1.12 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-09 01:31:08.925118: step 26120, loss = 0.96 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:31:10.198719: step 26130, loss = 0.75 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:31:11.459211: step 26140, loss = 0.74 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:31:12.751857: step 26150, loss = 0.67 (990.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:31:14.016505: step 26160, loss = 0.79 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:31:15.280206: step 26170, loss = 0.77 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:31:16.538951: step 26180, loss = 0.91 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:31:17.810882: step 26190, loss = 1.03 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:31:19.153800: step 26200, loss = 0.74 (953.1 examples/sec; 0.134 sec/batch)
2017-05-09 01:31:20.338881: step 26210, loss = 0.98 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-09 01:31:21.622055: step 26220, loss = 0.74 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:31:22.889026: step 26230, loss = 0.76 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:31:24.167017: step 26240, loss = 0.80 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:31:25.464176: step 26250, loss = 1.10 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:31:26.768082: step 26260, loss = 1.01 (981.7 examples/sec; 0.130 sec/batch)
2017-05-09 01:31:28.071150: step 26270, loss = 0.92 (982.3 examples/sec; 0.130 sec/batch)
2017-05-09 01:31:29.354037: step 26280, loss = 0.95 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:31:30.632029: step 26290, loss = 0.93 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:31:32.010686: step 26300, loss = 1.00 (928.4 examples/sec; 0.138 sec/batch)
2017-05-09 01:31:33.234137: step 26310, loss = 0.87 (1046.2 examples/sec; 0.122 sec/batch)
2017-05-09 01:31:34.505054: step 26320, loss = 0.73 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:31:35.779430: step 26330, loss = 1.29 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:31:37.068234: step 26340, loss = 0.84 (993.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:31:38.338229: step 26350, loss = 0.81 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:31:39.615249: step 26360, loss = 0.87 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:31:40.867165: step 26370, loss = 0.89 (1022.4 examples/sec; 0.125 sec/batch)
2017-05-09 01:31:42.128061: step 26380, loss = 0.87 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:31:43.423227: step 26390, loss = 0.76 (988.3 examples/sec; 0.130 sec/batch)
2017-05-09 01:31:44.796295: step 26400, loss = 0.82 (932.2 examples/sec; 0.137 sec/batch)
2017-05-09 01:31:45.982626: step 26410, loss = 0.77 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-09 01:31:47.264197: step 26420, loss = 0.80 (998.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:31:48.547351: step 26430, loss = 0.86 (997.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:31:49.817926: step 26440, loss = 0.88 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:31:51.103478: step 26450, loss = 0.84 (995.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:31:52.378505: step 26460, loss = 0.84 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:31:53.655652: step 26470, loss = 0.85 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:31:54.911009: step 26480, loss = 1.00 (1019.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:31:56.159905: step 26490, loss = 0.85 (1024.9 examples/sec; 0.125 sec/batch)
2017-05-09 01:31:57.505783: step 26500, loss = 0.86 (951.1 examples/sec; 0.135 sec/batch)
2017-05-09 01:31:58.697109: step 26510, loss = 0.77 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-09 01:31:59.960696: step 26520, loss = 0.76 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:32:01.245069: step 26530, loss = 0.92 (996.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:32:02.504681: step 26540, loss = 0.76 (1016.2 examples/sec; 0.126 sec/batch)
2017-05-09 01:32:03.784664: step 26550, loss = 0.57 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:32:05.041891: step 26560, loss = 0.89 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:32:06.334499: step 26570, loss = 0.73 (990.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:32:07.625096: step 26580, loss = 1.15 (991.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:32:08.929873: step 26590, loss = 0.69 (981.0 examples/sec; 0.130 sec/batch)
2017-05-09 01:32:10.290851: step 26600, loss = 0.81 (940.5 examples/sec; 0.136 sec/batch)
2017-05-09 01:32:11.495595: step 26610, loss = 0.81 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-09 01:32:12.764409: step 26620, loss = 0.97 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:32:14.051871: step 26630, loss = 0.80 (994.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:32:15.329996: step 26640, loss = 0.76 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:32:16.639193: step 26650, loss = 0.89 (977.7 examples/sec; 0.131 sec/batch)
2017-05-09 01:32:17.921583: step 26660, loss = 0.75 (998.1 examples/sec; 0.128 sec/batch)
2017-05-09 01:32:19.190786: step 26670, loss = 0.82 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-09 01:32:20.470738: step 26680, loss = 0.84 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:32:21.755424: step 26690, loss = 0.84 (996.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:32:23.129000: step 26700, loss = 0.87 (931.9 examples/sec; 0.137 sec/batch)
2017-05-09 01:32:24.359592: step 26710, loss = 1.04 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-09 01:32:25.652016: step 26720, loss = 0.77 (990.4 examples/sec; 0.129 sec/batch)
2017-05-09 01:32:26.945975: step 26730, loss = 0.74 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:32:28.261222: step 26740, loss = 1.12 (973.2 examples/sec; 0.132 sec/batch)
2017-05-09 01:32:29.507174: step 26750, loss = 0.89 (1027.3 examples/sec; 0.125 sec/batch)
2017-05-09 01:32:30.811014: step 26760, loss = 0.83 (981.7 examples/sec; 0.130 sec/batch)
2017-05-09 01:32:32.084930: step 26770, loss = 0.92 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-09 01:32:33.353227: step 26780, loss = 0.96 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:32:34.650391: step 26790, loss = 0.90 (986.8 examples/sec; 0.130 sec/batch)
2017-05-09 01:32:36.036687: step 26800, loss = 0.81 (923.3 examples/sec; 0.139 sec/batch)
2017-05-09 01:32:37.236139: step 26810, loss = 0.95 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-09 01:32:38.503913: step 26820, loss = 0.95 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:32:39.773492: step 26830, loss = 0.72 (1E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 548 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
008.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:32:41.040863: step 26840, loss = 0.79 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:32:42.320239: step 26850, loss = 0.72 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:32:43.585102: step 26860, loss = 0.85 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:32:44.876273: step 26870, loss = 0.72 (991.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:32:46.146874: step 26880, loss = 0.87 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:32:47.470767: step 26890, loss = 0.83 (966.9 examples/sec; 0.132 sec/batch)
2017-05-09 01:32:48.818930: step 26900, loss = 0.85 (949.4 examples/sec; 0.135 sec/batch)
2017-05-09 01:32:50.025492: step 26910, loss = 0.98 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-09 01:32:51.311200: step 26920, loss = 0.87 (995.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:32:52.624955: step 26930, loss = 0.67 (974.3 examples/sec; 0.131 sec/batch)
2017-05-09 01:32:53.882850: step 26940, loss = 0.80 (1017.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:32:55.164752: step 26950, loss = 0.88 (998.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:32:56.456851: step 26960, loss = 0.78 (990.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:32:57.725493: step 26970, loss = 1.07 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:32:59.019499: step 26980, loss = 0.82 (989.2 examples/sec; 0.129 sec/batch)
2017-05-09 01:33:00.293968: step 26990, loss = 0.90 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-09 01:33:01.656554: step 27000, loss = 0.89 (939.4 examples/sec; 0.136 sec/batch)
2017-05-09 01:33:02.836845: step 27010, loss = 0.80 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-09 01:33:04.128492: step 27020, loss = 1.08 (991.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:33:05.410081: step 27030, loss = 0.86 (998.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:33:06.676648: step 27040, loss = 0.86 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:33:07.938940: step 27050, loss = 0.81 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-09 01:33:09.234932: step 27060, loss = 0.83 (987.7 examples/sec; 0.130 sec/batch)
2017-05-09 01:33:10.500569: step 27070, loss = 0.87 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:33:11.772639: step 27080, loss = 0.71 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:33:13.055281: step 27090, loss = 0.76 (997.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:33:14.411006: step 27100, loss = 0.70 (944.1 examples/sec; 0.136 sec/batch)
2017-05-09 01:33:15.587937: step 27110, loss = 0.81 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-09 01:33:16.870729: step 27120, loss = 0.86 (997.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:33:18.149210: step 27130, loss = 0.94 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:33:19.436131: step 27140, loss = 0.85 (994.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:33:20.717913: step 27150, loss = 1.10 (998.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:33:21.995102: step 27160, loss = 0.93 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:33:23.303677: step 27170, loss = 0.76 (978.2 examples/sec; 0.131 sec/batch)
2017-05-09 01:33:24.572580: step 27180, loss = 0.74 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:33:25.835060: step 27190, loss = 0.79 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:33:27.201041: step 27200, loss = 0.86 (937.0 examples/sec; 0.137 sec/batch)
2017-05-09 01:33:28.396249: step 27210, loss = 0.89 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-09 01:33:29.666245: step 27220, loss = 0.72 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-09 01:33:30.941855: step 27230, loss = 0.70 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:33:32.251549: step 27240, loss = 0.84 (977.3 examples/sec; 0.131 sec/batch)
2017-05-09 01:33:33.535746: step 27250, loss = 0.79 (996.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:33:34.842291: step 27260, loss = 0.71 (979.7 examples/sec; 0.131 sec/batch)
2017-05-09 01:33:36.115418: step 27270, loss = 0.69 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-09 01:33:37.374614: step 27280, loss = 0.58 (1016.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:33:38.648289: step 27290, loss = 0.76 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:33:40.025040: step 27300, loss = 0.74 (929.7 examples/sec; 0.138 sec/batch)
2017-05-09 01:33:41.194263: step 27310, loss = 0.88 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-09 01:33:42.467030: step 27320, loss = 0.71 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:33:43.756976: step 27330, loss = 0.74 (992.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:33:45.039614: step 27340, loss = 0.78 (997.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:33:46.327836: step 27350, loss = 0.75 (993.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:33:47.612947: step 27360, loss = 0.90 (996.0 examples/sec; 0.129 sec/batch)
2017-05-09 01:33:48.899268: step 27370, loss = 0.96 (995.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:33:50.160535: step 27380, loss = 0.83 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-09 01:33:51.435301: step 27390, loss = 0.85 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:33:52.808637: step 27400, loss = 0.83 (932.0 examples/sec; 0.137 sec/batch)
2017-05-09 01:33:53.979459: step 27410, loss = 0.79 (1093.2 examples/sec; 0.117 sec/batch)
2017-05-09 01:33:55.257816: step 27420, loss = 0.79 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:33:56.529302: step 27430, loss = 0.75 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:33:57.788474: step 27440, loss = 0.80 (1016.5 examples/sec; 0.126 sec/batch)
2017-05-09 01:33:59.050082: step 27450, loss = 0.66 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:34:00.322152: step 27460, loss = 0.66 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:34:01.594548: step 27470, loss = 0.83 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:34:02.901053: step 27480, loss = 1.16 (979.7 examples/sec; 0.131 sec/batch)
2017-05-09 01:34:04.184446: step 27490, loss = 0.98 (997.4 examples/sec; 0.128 sec/batch)
2017-05-09 01:34:05.574859: step 27500, loss = 0.77 (920.6 examples/sec; 0.139 sec/batch)
2017-05-09 01:34:06.781725: step 27510, loss = 1.10 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-09 01:34:08.059686: step 27520, loss = 0.97 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-09 01:34:09.342619: step 27530, loss = 0.60 (997.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:34:10.611708: step 27540, loss = 0.94 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-09 01:34:11.884063: step 27550, loss = 0.77 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:34:13.177421: step 27560, loss = 0.98 (989.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:34:14.483445: step 27570, loss = 1.09 (980.1 examples/sec; 0.131 sec/batch)
2017-05-09 01:34:15.792421: step 27580, loss = 0.82 (977.9 examples/sec; 0.131 sec/batch)
2017-05-09 01:34:17.093175: step 27590, loss = 0.83 (984.0 examples/sec; 0.130 sec/batch)
2017-05-09 01:34:18.492281: step 27600, loss = 0.69 (914.9 examples/sec; 0.140 sec/batch)
2017-05-09 01:34:19.671519: step 27610, loss = 0.76 (1085.4 examples/sec; 0.118 sec/batch)
2017-05-09 01:34:20.970428: step 27620, loss = 0.92 (985.4 examples/sec; 0.130 sec/batch)
2017-05-09 01:34:22.284513: step 27630, loss = 0.94 (974.1 examples/sec; 0.131 sec/batch)
2017-05-09 01:34:23.604711: step 27640, loss = 0.84 (969.6 examples/sec; 0.132 sec/batch)
2017-05-09 01:34:24.904708: step 27650, loss = 0.72 (984.6 examples/sec; 0.130 sec/batch)
2017-05-09 01:34:26.182211: step 27660, loss = 1.05 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:34:27.455814: step 27670, loss = 0.95 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-09 01:34:28.729365: step 27680, loss = 0.88 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-09 01:34:30.067339: step 27690, loss = 0.77 (956.7 examples/sec; 0.134 sec/batch)
2017-05-09 01:34:31.469897: step 27700, loss = 0.87 (912.6 examples/sec; 0.140 sec/batch)
2017-05-09 01:34:32.730867: step 27710, loss = 0.82 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:34:34.107287: step 27720, loss = 0.91 (930.0 examples/sec; 0.138 sec/batch)
2017-05-09 01:34:35.507117: step 27730, loss = 0.82 (914.4 examples/sec; 0.140 sec/batch)
2017-05-09 E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 568 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
01:34:36.898285: step 27740, loss = 0.91 (920.1 examples/sec; 0.139 sec/batch)
2017-05-09 01:34:38.285485: step 27750, loss = 0.83 (922.7 examples/sec; 0.139 sec/batch)
2017-05-09 01:34:39.658527: step 27760, loss = 0.83 (932.2 examples/sec; 0.137 sec/batch)
2017-05-09 01:34:41.056644: step 27770, loss = 0.78 (915.5 examples/sec; 0.140 sec/batch)
2017-05-09 01:34:42.440424: step 27780, loss = 0.81 (925.0 examples/sec; 0.138 sec/batch)
2017-05-09 01:34:43.833277: step 27790, loss = 0.74 (919.0 examples/sec; 0.139 sec/batch)
2017-05-09 01:34:45.305780: step 27800, loss = 0.89 (869.3 examples/sec; 0.147 sec/batch)
2017-05-09 01:34:46.588475: step 27810, loss = 0.96 (997.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:34:47.961199: step 27820, loss = 0.81 (932.5 examples/sec; 0.137 sec/batch)
2017-05-09 01:34:49.355001: step 27830, loss = 0.87 (918.3 examples/sec; 0.139 sec/batch)
2017-05-09 01:34:50.730640: step 27840, loss = 0.81 (930.5 examples/sec; 0.138 sec/batch)
2017-05-09 01:34:52.095454: step 27850, loss = 0.81 (937.9 examples/sec; 0.136 sec/batch)
2017-05-09 01:34:53.470505: step 27860, loss = 0.80 (930.9 examples/sec; 0.138 sec/batch)
2017-05-09 01:34:54.850771: step 27870, loss = 0.87 (927.4 examples/sec; 0.138 sec/batch)
2017-05-09 01:34:56.134840: step 27880, loss = 0.89 (996.8 examples/sec; 0.128 sec/batch)
2017-05-09 01:34:57.438866: step 27890, loss = 0.73 (981.6 examples/sec; 0.130 sec/batch)
2017-05-09 01:34:58.841679: step 27900, loss = 0.78 (912.5 examples/sec; 0.140 sec/batch)
2017-05-09 01:35:00.030986: step 27910, loss = 0.77 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-09 01:35:01.321190: step 27920, loss = 0.85 (992.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:35:02.606571: step 27930, loss = 0.88 (995.8 examples/sec; 0.129 sec/batch)
2017-05-09 01:35:03.909588: step 27940, loss = 0.85 (982.3 examples/sec; 0.130 sec/batch)
2017-05-09 01:35:05.186159: step 27950, loss = 0.87 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-09 01:35:06.447767: step 27960, loss = 0.88 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-09 01:35:07.732305: step 27970, loss = 0.73 (996.5 examples/sec; 0.128 sec/batch)
2017-05-09 01:35:09.014443: step 27980, loss = 0.78 (998.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:35:10.283987: step 27990, loss = 0.81 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-09 01:35:11.635966: step 28000, loss = 0.72 (946.8 examples/sec; 0.135 sec/batch)
2017-05-09 01:35:12.820513: step 28010, loss = 0.89 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-09 01:35:14.102751: step 28020, loss = 0.82 (998.3 examples/sec; 0.128 sec/batch)
2017-05-09 01:35:15.388045: step 28030, loss = 0.80 (995.9 examples/sec; 0.129 sec/batch)
2017-05-09 01:35:16.715114: step 28040, loss = 0.85 (964.5 examples/sec; 0.133 sec/batch)
2017-05-09 01:35:18.009694: step 28050, loss = 0.98 (988.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:35:19.334934: step 28060, loss = 0.69 (965.9 examples/sec; 0.133 sec/batch)
2017-05-09 01:35:20.683177: step 28070, loss = 0.75 (949.4 examples/sec; 0.135 sec/batch)
2017-05-09 01:35:21.981446: step 28080, loss = 0.78 (985.9 examples/sec; 0.130 sec/batch)
2017-05-09 01:35:23.305037: step 28090, loss = 0.85 (967.1 examples/sec; 0.132 sec/batch)
2017-05-09 01:35:24.715751: step 28100, loss = 1.04 (907.3 examples/sec; 0.141 sec/batch)
2017-05-09 01:35:25.921267: step 28110, loss = 0.85 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-09 01:35:27.222073: step 28120, loss = 1.06 (984.0 examples/sec; 0.130 sec/batch)
2017-05-09 01:35:28.553528: step 28130, loss = 0.98 (961.4 examples/sec; 0.133 sec/batch)
2017-05-09 01:35:29.897659: step 28140, loss = 0.82 (952.3 examples/sec; 0.134 sec/batch)
2017-05-09 01:35:31.214063: step 28150, loss = 1.00 (972.4 examples/sec; 0.132 sec/batch)
2017-05-09 01:35:32.520236: step 28160, loss = 0.87 (980.0 examples/sec; 0.131 sec/batch)
2017-05-09 01:35:33.811009: step 28170, loss = 0.80 (991.6 examples/sec; 0.129 sec/batch)
2017-05-09 01:35:35.130988: step 28180, loss = 0.86 (969.7 examples/sec; 0.132 sec/batch)
2017-05-09 01:35:36.443396: step 28190, loss = 0.99 (975.3 examples/sec; 0.131 sec/batch)
2017-05-09 01:35:37.837243: step 28200, loss = 1.07 (918.3 examples/sec; 0.139 sec/batch)
2017-05-09 01:35:39.077860: step 28210, loss = 0.71 (1031.7 examples/sec; 0.124 sec/batch)
2017-05-09 01:35:40.411569: step 28220, loss = 0.73 (959.7 examples/sec; 0.133 sec/batch)
2017-05-09 01:35:41.717578: step 28230, loss = 0.71 (980.1 examples/sec; 0.131 sec/batch)
2017-05-09 01:35:43.029295: step 28240, loss = 0.95 (975.8 examples/sec; 0.131 sec/batch)
2017-05-09 01:35:44.346203: step 28250, loss = 0.77 (972.0 examples/sec; 0.132 sec/batch)
2017-05-09 01:35:45.638991: step 28260, loss = 0.80 (990.1 examples/sec; 0.129 sec/batch)
2017-05-09 01:35:46.936985: step 28270, loss = 0.93 (986.1 examples/sec; 0.130 sec/batch)
2017-05-09 01:35:48.232678: step 28280, loss = 0.76 (987.9 examples/sec; 0.130 sec/batch)
2017-05-09 01:35:49.533960: step 28290, loss = 0.85 (983.6 examples/sec; 0.130 sec/batch)
2017-05-09 01:35:50.911091: step 28300, loss = 0.86 (929.5 examples/sec; 0.138 sec/batch)
2017-05-09 01:35:52.174507: step 28310, loss = 0.86 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-09 01:35:53.481377: step 28320, loss = 0.99 (979.4 examples/sec; 0.131 sec/batch)
2017-05-09 01:35:54.816219: step 28330, loss = 0.84 (958.9 examples/sec; 0.133 sec/batch)
2017-05-09 01:35:56.102330: step 28340, loss = 0.76 (995.3 examples/sec; 0.129 sec/batch)
2017-05-09 01:35:57.386111: step 28350, loss = 0.96 (997.0 examples/sec; 0.128 sec/batch)
2017-05-09 01:35:58.649199: step 28360, loss = 0.82 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-09 01:35:59.919454: step 28370, loss = 0.89 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-09 01:36:01.255835: step 28380, loss = 0.71 (957.8 examples/sec; 0.134 sec/batch)
2017-05-09 01:36:02.534741: step 28390, loss = 0.90 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-09 01:36:03.912152: step 28400, loss = 0.85 (929.3 examples/sec; 0.138 sec/batch)
2017-05-09 01:36:05.075360: step 28410, loss = 0.85 (1100.4 examples/sec; 0.116 sec/batch)
2017-05-09 01:36:06.362121: step 28420, loss = 0.86 (994.7 examples/sec; 0.129 sec/batch)
2017-05-09 01:36:07.639244: step 28430, loss = 1.21 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-09 01:36:08.964817: step 28440, loss = 0.83 (965.6 examples/sec; 0.133 sec/batch)
2017-05-09 01:36:10.298566: step 28450, loss = 0.78 (959.7 examples/sec; 0.133 sec/batch)
2017-05-09 01:36:11.772804: step 28460, loss = 0.71 (868.2 examples/sec; 0.147 sec/batch)
2017-05-09 01:36:13.216713: step 28470, loss = 0.65 (886.5 examples/sec; 0.144 sec/batch)
2017-05-09 01:36:14.655421: step 28480, loss = 0.81 (889.7 examples/sec; 0.144 sec/batch)
2017-05-09 01:36:16.135946: step 28490, loss = 0.77 (864.6 examples/sec; 0.148 sec/batch)
2017-05-09 01:36:17.675714: step 28500, loss = 0.84 (831.3 examples/sec; 0.154 sec/batch)
2017-05-09 01:36:19.083358: step 28510, loss = 1.03 (909.3 examples/sec; 0.141 sec/batch)
2017-05-09 01:36:20.550166: step 28520, loss = 0.84 (872.6 examples/sec; 0.147 sec/batch)
2017-05-09 01:36:22.043668: step 28530, loss = 0.74 (857.0 examples/sec; 0.149 sec/batch)
2017-05-09 01:36:23.518182: step 28540, loss = 0.87 (868.1 examples/sec; 0.147 sec/batch)
2017-05-09 01:36:25.007289: step 28550, loss = 0.79 (859.6 examples/sec; 0.149 sec/batch)
2017-05-09 01:36:26.478610: step 28560, loss = 0.88 (870.0 examples/sec; 0.147 sec/batch)
2017-05-09 01:36:27.945398: step 28570, loss = 0.91 (872.7 examples/sec; 0.147 sec/batch)
2017-05-09 01:36:29.412097: step 28580, loss = 0.75 (872.7 examples/sec; 0.147 sec/batch)
2017-05-09 01:36:30.867092: step 28590, loss = 0.88 (879.7 examples/sec; 0.145 sec/batch)
2017-05-09 01:36:32.396414: step 28600, loss = 0.84 (837.0 examples/sec; 0.153 sec/batch)
2017-05-09 01:36:33.749957: step 28610, loss = 0.67 (945.7 examples/sec; 0.135 sec/batch)
2017-05-09 01:36:35.198590: step 28620, loss = 0.85 (883.6 examples/sec; 0.145 sec/batch)
2017-05-09 01:36:36.654738: step 28630, loss = 0.93 (879.0 examples/sec; 0.146 sec/batch)
2017-05-09 01:36:38.145244: step 28640, loss = 0.88 (858.8 examples/sec; 0.149 sec/batch)
E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 587 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
2017-05-09 01:36:39.627475: step 28650, loss = 0.98 (863.6 examples/sec; 0.148 sec/batch)
2017-05-09 01:36:41.122474: step 28660, loss = 0.67 (856.2 examples/sec; 0.150 sec/batch)
2017-05-09 01:36:42.626362: step 28670, loss = 0.68 (851.2 examples/sec; 0.150 sec/batch)
2017-05-09 01:36:44.121708: step 28680, loss = 0.83 (856.0 examples/sec; 0.150 sec/batch)
2017-05-09 01:36:45.601349: step 28690, loss = 0.76 (865.1 examples/sec; 0.148 sec/batch)
2017-05-09 01:36:47.197504: step 28700, loss = 1.12 (801.9 examples/sec; 0.160 sec/batch)
2017-05-09 01:36:48.591509: step 28710, loss = 0.95 (918.2 examples/sec; 0.139 sec/batch)
2017-05-09 01:36:50.067951: step 28720, loss = 0.76 (867.0 examples/sec; 0.148 sec/batch)
2017-05-09 01:36:51.563219: step 28730, loss = 0.88 (856.0 examples/sec; 0.150 sec/batch)
2017-05-09 01:36:53.047941: step 28740, loss = 0.88 (862.1 examples/sec; 0.148 sec/batch)
2017-05-09 01:36:54.550737: step 28750, loss = 0.96 (851.7 examples/sec; 0.150 sec/batch)
2017-05-09 01:36:56.025884: step 28760, loss = 0.95 (867.7 examples/sec; 0.148 sec/batch)
2017-05-09 01:36:57.489962: step 28770, loss = 0.87 (874.3 examples/sec; 0.146 sec/batch)
2017-05-09 01:36:58.992646: step 28780, loss = 0.78 (851.8 examples/sec; 0.150 sec/batch)
2017-05-09 01:37:00.468275: step 28790, loss = 0.88 (867.4 examples/sec; 0.148 sec/batch)
2017-05-09 01:37:02.031859: step 28800, loss = 1.00 (818.6 examples/sec; 0.156 sec/batch)
2017-05-09 01:37:03.419343: step 28810, loss = 0.81 (922.5 examples/sec; 0.139 sec/batch)
2017-05-09 01:37:04.936379: step 28820, loss = 0.81 (843.7 examples/sec; 0.152 sec/batch)
2017-05-09 01:37:06.434378: step 28830, loss = 0.82 (854.5 examples/sec; 0.150 sec/batch)
2017-05-09 01:37:07.905422: step 28840, loss = 0.81 (870.1 examples/sec; 0.147 sec/batch)
2017-05-09 01:37:09.405157: step 28850, loss = 0.73 (853.5 examples/sec; 0.150 sec/batch)
2017-05-09 01:37:10.873395: step 28860, loss = 0.82 (871.8 examples/sec; 0.147 sec/batch)
2017-05-09 01:37:12.353997: step 28870, loss = 0.79 (864.5 examples/sec; 0.148 sec/batch)
2017-05-09 01:37:13.813944: step 28880, loss = 0.99 (876.7 examples/sec; 0.146 sec/batch)
2017-05-09 01:37:15.313743: step 28890, loss = 0.78 (853.4 examples/sec; 0.150 sec/batch)
2017-05-09 01:37:16.837459: step 28900, loss = 0.80 (840.1 examples/sec; 0.152 sec/batch)
2017-05-09 01:37:18.224067: step 28910, loss = 0.73 (923.1 examples/sec; 0.139 sec/batch)
2017-05-09 01:37:19.718930: step 28920, loss = 0.78 (856.3 examples/sec; 0.149 sec/batch)
2017-05-09 01:37:21.192518: step 28930, loss = 1.11 (868.6 examples/sec; 0.147 sec/batch)
2017-05-09 01:37:22.649693: step 28940, loss = 0.71 (878.4 examples/sec; 0.146 sec/batch)
2017-05-09 01:37:24.155675: step 28950, loss = 0.67 (849.9 examples/sec; 0.151 sec/batch)
2017-05-09 01:37:25.623991: step 28960, loss = 0.65 (871.7 examples/sec; 0.147 sec/batch)
2017-05-09 01:37:27.079585: step 28970, loss = 0.76 (879.4 examples/sec; 0.146 sec/batch)
2017-05-09 01:37:28.531532: step 28980, loss = 0.71 (881.6 examples/sec; 0.145 sec/batch)
2017-05-09 01:37:30.006276: step 28990, loss = 1.11 (867.9 examples/sec; 0.147 sec/batch)
2017-05-09 01:37:31.601440: step 29000, loss = 1.13 (802.4 examples/sec; 0.160 sec/batch)
2017-05-09 01:37:33.001644: step 29010, loss = 0.87 (914.2 examples/sec; 0.140 sec/batch)
2017-05-09 01:37:34.476563: step 29020, loss = 0.78 (867.8 examples/sec; 0.147 sec/batch)
2017-05-09 01:37:35.959756: step 29030, loss = 0.86 (863.0 examples/sec; 0.148 sec/batch)
2017-05-09 01:37:37.402206: step 29040, loss = 0.88 (887.4 examples/sec; 0.144 sec/batch)
2017-05-09 01:37:38.884752: step 29050, loss = 1.08 (863.4 examples/sec; 0.148 sec/batch)
2017-05-09 01:37:40.349704: step 29060, loss = 0.77 (873.7 examples/sec; 0.146 sec/batch)
2017-05-09 01:37:41.788709: step 29070, loss = 0.78 (889.5 examples/sec; 0.144 sec/batch)
2017-05-09 01:37:43.259645: step 29080, loss = 0.76 (870.2 examples/sec; 0.147 sec/batch)
2017-05-09 01:37:44.698521: step 29090, loss = 0.85 (889.6 examples/sec; 0.144 sec/batch)
2017-05-09 01:37:46.240823: step 29100, loss = 0.75 (829.9 examples/sec; 0.154 sec/batch)
2017-05-09 01:37:47.619176: step 29110, loss = 0.77 (928.6 examples/sec; 0.138 sec/batch)
2017-05-09 01:37:49.094656: step 29120, loss = 1.02 (867.5 examples/sec; 0.148 sec/batch)
2017-05-09 01:37:50.571436: step 29130, loss = 0.77 (866.7 examples/sec; 0.148 sec/batch)
2017-05-09 01:37:52.048532: step 29140, loss = 0.81 (866.6 examples/sec; 0.148 sec/batch)
2017-05-09 01:37:53.525424: step 29150, loss = 0.87 (866.7 examples/sec; 0.148 sec/batch)
2017-05-09 01:37:55.034639: step 29160, loss = 0.65 (848.1 examples/sec; 0.151 sec/batch)
2017-05-09 01:37:56.523235: step 29170, loss = 0.79 (859.9 examples/sec; 0.149 sec/batch)
2017-05-09 01:37:57.999958: step 29180, loss = 0.77 (866.8 examples/sec; 0.148 sec/batch)
2017-05-09 01:37:59.474373: step 29190, loss = 0.90 (868.2 examples/sec; 0.147 sec/batch)
2017-05-09 01:38:01.059782: step 29200, loss = 0.91 (807.3 examples/sec; 0.159 sec/batch)
2017-05-09 01:38:02.413667: step 29210, loss = 0.73 (945.4 examples/sec; 0.135 sec/batch)
2017-05-09 01:38:03.902258: step 29220, loss = 0.74 (859.9 examples/sec; 0.149 sec/batch)
2017-05-09 01:38:05.387472: step 29230, loss = 0.60 (861.8 examples/sec; 0.149 sec/batch)
2017-05-09 01:38:06.861072: step 29240, loss = 0.70 (868.6 examples/sec; 0.147 sec/batch)
2017-05-09 01:38:08.341713: step 29250, loss = 0.88 (864.5 examples/sec; 0.148 sec/batch)
2017-05-09 01:38:09.834443: step 29260, loss = 0.99 (857.5 examples/sec; 0.149 sec/batch)
2017-05-09 01:38:11.290959: step 29270, loss = 0.69 (878.8 examples/sec; 0.146 sec/batch)
2017-05-09 01:38:12.787995: step 29280, loss = 0.81 (855.0 examples/sec; 0.150 sec/batch)
2017-05-09 01:38:14.263622: step 29290, loss = 0.74 (867.4 examples/sec; 0.148 sec/batch)
2017-05-09 01:38:15.865015: step 29300, loss = 0.84 (799.3 examples/sec; 0.160 sec/batch)
2017-05-09 01:38:17.239499: step 29310, loss = 0.86 (931.3 examples/sec; 0.137 sec/batch)
2017-05-09 01:38:18.720705: step 29320, loss = 0.75 (864.2 examples/sec; 0.148 sec/batch)
2017-05-09 01:38:20.190649: step 29330, loss = 0.79 (870.8 examples/sec; 0.147 sec/batch)
2017-05-09 01:38:21.652360: step 29340, loss = 0.92 (875.7 examples/sec; 0.146 sec/batch)
2017-05-09 01:38:23.104551: step 29350, loss = 0.78 (881.4 examples/sec; 0.145 sec/batch)
2017-05-09 01:38:24.563164: step 29360, loss = 0.77 (877.5 examples/sec; 0.146 sec/batch)
2017-05-09 01:38:26.068880: step 29370, loss = 0.69 (850.1 examples/sec; 0.151 sec/batch)
2017-05-09 01:38:27.611221: step 29380, loss = 0.71 (829.9 examples/sec; 0.154 sec/batch)
2017-05-09 01:38:29.083660: step 29390, loss = 0.80 (869.3 examples/sec; 0.147 sec/batch)
2017-05-09 01:38:30.667751: step 29400, loss = 0.77 (808.0 examples/sec; 0.158 sec/batch)
2017-05-09 01:38:32.065428: step 29410, loss = 0.81 (915.8 examples/sec; 0.140 sec/batch)
2017-05-09 01:38:33.545543: step 29420, loss = 0.91 (864.8 examples/sec; 0.148 sec/batch)
2017-05-09 01:38:35.050154: step 29430, loss = 0.96 (850.7 examples/sec; 0.150 sec/batch)
2017-05-09 01:38:36.595307: step 29440, loss = 0.96 (828.4 examples/sec; 0.155 sec/batch)
2017-05-09 01:38:38.094822: step 29450, loss = 0.85 (853.6 examples/sec; 0.150 sec/batch)
2017-05-09 01:38:39.564121: step 29460, loss = 0.90 (871.2 examples/sec; 0.147 sec/batch)
2017-05-09 01:38:41.042986: step 29470, loss = 0.94 (865.5 examples/sec; 0.148 sec/batch)
2017-05-09 01:38:42.515512: step 29480, loss = 0.90 (869.3 examples/sec; 0.147 sec/batch)
2017-05-09 01:38:43.996777: step 29490, loss = 0.79 (864.1 examples/sec; 0.148 sec/batch)
2017-05-09 01:38:45.609406: step 29500, loss = 0.94 (793.7 examples/sec; 0.161 sec/batch)
2017-05-09 01:38:46.983682: step 29510, loss = 0.84 (931.4 examples/sec; 0.137 sec/batch)
2017-05-09 01:38:48.512500: step 29520, loss = 0.81 (837.2 examples/sec; 0.153 sec/batch)
2017-05-09 01:38:49.990177: step 29530, loss = 0.75 (866.2 examples/sec; 0.148 sec/batch)
2017-05-09 01:38:51.508648: step 29540, loss = 0.71 (843.0 examples/sec; 0.152 sec/batch)
2017-05-09 01:38:53.004502: step 29550, loss = 0.95 (855.7 examples/sec; 0.150 sec/batch)
20E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 605 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
17-05-09 01:38:54.502341: step 29560, loss = 0.67 (854.6 examples/sec; 0.150 sec/batch)
2017-05-09 01:38:55.980153: step 29570, loss = 0.86 (866.1 examples/sec; 0.148 sec/batch)
2017-05-09 01:38:57.463694: step 29580, loss = 0.70 (862.8 examples/sec; 0.148 sec/batch)
2017-05-09 01:38:58.936017: step 29590, loss = 0.78 (869.4 examples/sec; 0.147 sec/batch)
2017-05-09 01:39:00.553475: step 29600, loss = 1.03 (791.4 examples/sec; 0.162 sec/batch)
2017-05-09 01:39:01.960710: step 29610, loss = 0.83 (909.6 examples/sec; 0.141 sec/batch)
2017-05-09 01:39:03.450834: step 29620, loss = 0.84 (859.0 examples/sec; 0.149 sec/batch)
2017-05-09 01:39:04.912860: step 29630, loss = 0.87 (875.5 examples/sec; 0.146 sec/batch)
2017-05-09 01:39:06.382519: step 29640, loss = 0.81 (870.9 examples/sec; 0.147 sec/batch)
2017-05-09 01:39:07.864065: step 29650, loss = 0.90 (864.0 examples/sec; 0.148 sec/batch)
2017-05-09 01:39:09.344671: step 29660, loss = 0.83 (864.5 examples/sec; 0.148 sec/batch)
2017-05-09 01:39:10.814365: step 29670, loss = 0.88 (871.0 examples/sec; 0.147 sec/batch)
2017-05-09 01:39:12.286123: step 29680, loss = 0.69 (869.7 examples/sec; 0.147 sec/batch)
2017-05-09 01:39:13.776677: step 29690, loss = 0.87 (858.7 examples/sec; 0.149 sec/batch)
2017-05-09 01:39:15.356398: step 29700, loss = 0.82 (810.3 examples/sec; 0.158 sec/batch)
2017-05-09 01:39:16.755341: step 29710, loss = 0.83 (915.0 examples/sec; 0.140 sec/batch)
2017-05-09 01:39:18.244922: step 29720, loss = 0.92 (859.3 examples/sec; 0.149 sec/batch)
2017-05-09 01:39:19.743667: step 29730, loss = 0.93 (854.1 examples/sec; 0.150 sec/batch)
2017-05-09 01:39:21.233827: step 29740, loss = 0.74 (859.0 examples/sec; 0.149 sec/batch)
2017-05-09 01:39:22.743152: step 29750, loss = 0.78 (848.1 examples/sec; 0.151 sec/batch)
2017-05-09 01:39:24.262625: step 29760, loss = 1.29 (842.4 examples/sec; 0.152 sec/batch)
2017-05-09 01:39:25.766968: step 29770, loss = 1.26 (850.9 examples/sec; 0.150 sec/batch)
2017-05-09 01:39:27.246265: step 29780, loss = 0.78 (865.3 examples/sec; 0.148 sec/batch)
2017-05-09 01:39:28.735178: step 29790, loss = 0.89 (859.7 examples/sec; 0.149 sec/batch)
2017-05-09 01:39:30.327745: step 29800, loss = 0.81 (803.7 examples/sec; 0.159 sec/batch)
2017-05-09 01:39:31.735231: step 29810, loss = 0.69 (909.4 examples/sec; 0.141 sec/batch)
2017-05-09 01:39:33.248442: step 29820, loss = 0.76 (845.9 examples/sec; 0.151 sec/batch)
2017-05-09 01:39:34.737348: step 29830, loss = 0.72 (859.7 examples/sec; 0.149 sec/batch)
2017-05-09 01:39:36.218290: step 29840, loss = 0.83 (864.3 examples/sec; 0.148 sec/batch)
2017-05-09 01:39:37.693561: step 29850, loss = 0.85 (867.6 examples/sec; 0.148 sec/batch)
2017-05-09 01:39:39.183936: step 29860, loss = 0.82 (858.8 examples/sec; 0.149 sec/batch)
2017-05-09 01:39:40.667234: step 29870, loss = 0.73 (862.9 examples/sec; 0.148 sec/batch)
2017-05-09 01:39:42.141530: step 29880, loss = 0.72 (868.2 examples/sec; 0.147 sec/batch)
2017-05-09 01:39:43.629508: step 29890, loss = 0.87 (860.2 examples/sec; 0.149 sec/batch)
2017-05-09 01:39:45.187594: step 29900, loss = 0.75 (821.5 examples/sec; 0.156 sec/batch)
2017-05-09 01:39:46.569143: step 29910, loss = 1.06 (926.5 examples/sec; 0.138 sec/batch)
2017-05-09 01:39:48.059781: step 29920, loss = 1.01 (858.7 examples/sec; 0.149 sec/batch)
2017-05-09 01:39:49.584160: step 29930, loss = 0.80 (839.7 examples/sec; 0.152 sec/batch)
2017-05-09 01:39:51.078424: step 29940, loss = 0.72 (856.6 examples/sec; 0.149 sec/batch)
2017-05-09 01:39:52.566277: step 29950, loss = 0.89 (860.3 examples/sec; 0.149 sec/batch)
2017-05-09 01:39:54.081922: step 29960, loss = 0.82 (844.5 examples/sec; 0.152 sec/batch)
2017-05-09 01:39:55.582508: step 29970, loss = 0.88 (853.0 examples/sec; 0.150 sec/batch)
2017-05-09 01:39:57.097755: step 29980, loss = 0.81 (844.7 examples/sec; 0.152 sec/batch)
2017-05-09 01:39:58.602615: step 29990, loss = 0.74 (850.6 examples/sec; 0.150 sec/batch)
2017-05-09 01:40:00.190640: step 30000, loss = 0.88 (806.0 examples/sec; 0.159 sec/batch)
2017-05-09 01:40:01.568548: step 30010, loss = 0.84 (928.9 examples/sec; 0.138 sec/batch)
2017-05-09 01:40:03.035253: step 30020, loss = 0.89 (872.7 examples/sec; 0.147 sec/batch)
2017-05-09 01:40:04.537905: step 30030, loss = 0.73 (851.8 examples/sec; 0.150 sec/batch)
2017-05-09 01:40:06.008376: step 30040, loss = 0.78 (870.5 examples/sec; 0.147 sec/batch)
2017-05-09 01:40:07.537577: step 30050, loss = 0.92 (837.0 examples/sec; 0.153 sec/batch)
2017-05-09 01:40:09.041596: step 30060, loss = 0.74 (851.1 examples/sec; 0.150 sec/batch)
2017-05-09 01:40:10.504626: step 30070, loss = 0.75 (874.9 examples/sec; 0.146 sec/batch)
2017-05-09 01:40:12.016727: step 30080, loss = 0.74 (846.5 examples/sec; 0.151 sec/batch)
2017-05-09 01:40:13.526302: step 30090, loss = 0.82 (847.9 examples/sec; 0.151 sec/batch)
2017-05-09 01:40:15.112122: step 30100, loss = 0.82 (807.2 examples/sec; 0.159 sec/batch)
2017-05-09 01:40:16.547833: step 30110, loss = 0.71 (891.5 examples/sec; 0.144 sec/batch)
2017-05-09 01:40:18.024053: step 30120, loss = 0.89 (867.1 examples/sec; 0.148 sec/batch)
2017-05-09 01:40:19.516113: step 30130, loss = 0.93 (857.9 examples/sec; 0.149 sec/batch)
2017-05-09 01:40:20.999876: step 30140, loss = 0.80 (862.7 examples/sec; 0.148 sec/batch)
2017-05-09 01:40:22.511786: step 30150, loss = 0.78 (846.6 examples/sec; 0.151 sec/batch)
2017-05-09 01:40:24.001532: step 30160, loss = 0.94 (859.2 examples/sec; 0.149 sec/batch)
2017-05-09 01:40:25.486005: step 30170, loss = 0.75 (862.3 examples/sec; 0.148 sec/batch)
2017-05-09 01:40:26.975876: step 30180, loss = 0.93 (859.1 examples/sec; 0.149 sec/batch)
2017-05-09 01:40:28.481454: step 30190, loss = 0.74 (850.2 examples/sec; 0.151 sec/batch)
2017-05-09 01:40:30.069701: step 30200, loss = 0.76 (805.9 examples/sec; 0.159 sec/batch)
2017-05-09 01:40:31.506084: step 30210, loss = 0.94 (891.1 examples/sec; 0.144 sec/batch)
2017-05-09 01:40:33.007236: step 30220, loss = 0.90 (852.7 examples/sec; 0.150 sec/batch)
2017-05-09 01:40:34.499863: step 30230, loss = 0.78 (857.5 examples/sec; 0.149 sec/batch)
2017-05-09 01:40:35.990288: step 30240, loss = 0.99 (858.8 examples/sec; 0.149 sec/batch)
2017-05-09 01:40:37.459025: step 30250, loss = 0.96 (871.5 examples/sec; 0.147 sec/batch)
2017-05-09 01:40:38.945688: step 30260, loss = 1.09 (861.0 examples/sec; 0.149 sec/batch)
2017-05-09 01:40:40.443572: step 30270, loss = 0.74 (854.5 examples/sec; 0.150 sec/batch)
2017-05-09 01:40:41.936652: step 30280, loss = 0.79 (857.3 examples/sec; 0.149 sec/batch)
2017-05-09 01:40:43.422597: step 30290, loss = 0.82 (861.4 examples/sec; 0.149 sec/batch)
2017-05-09 01:40:45.013090: step 30300, loss = 0.80 (804.8 examples/sec; 0.159 sec/batch)
2017-05-09 01:40:46.424118: step 30310, loss = 0.91 (907.1 examples/sec; 0.141 sec/batch)
2017-05-09 01:40:47.923349: step 30320, loss = 1.17 (853.8 examples/sec; 0.150 sec/batch)
2017-05-09 01:40:49.424144: step 30330, loss = 0.89 (852.9 examples/sec; 0.150 sec/batch)
2017-05-09 01:40:50.925227: step 30340, loss = 0.83 (852.7 examples/sec; 0.150 sec/batch)
2017-05-09 01:40:52.457295: step 30350, loss = 0.80 (835.5 examples/sec; 0.153 sec/batch)
2017-05-09 01:40:53.973482: step 30360, loss = 0.91 (844.2 examples/sec; 0.152 sec/batch)
2017-05-09 01:40:55.454106: step 30370, loss = 1.29 (864.5 examples/sec; 0.148 sec/batch)
2017-05-09 01:40:56.958185: step 30380, loss = 0.76 (851.0 examples/sec; 0.150 sec/batch)
2017-05-09 01:40:58.447151: step 30390, loss = 0.85 (859.7 examples/sec; 0.149 sec/batch)
2017-05-09 01:41:00.049414: step 30400, loss = 0.93 (798.9 examples/sec; 0.160 sec/batch)
2017-05-09 01:41:01.461158: step 30410, loss = 0.88 (906.7 examples/sec; 0.141 sec/batch)
2017-05-09 01:41:02.944444: step 30420, loss = 0.85 (863.0 examples/sec; 0.148 sec/batch)
2017-05-09 01:41:04.428487: step 30430, loss = 0.76 (862.5 examples/sec; 0.148 sec/batch)
2017-05-09 01:41:05.921904: step 30440, loss = 1.08 (857.1 examples/sec; 0.149 sec/batch)
2017-05-09 01:41:07.413528: step 30450, loss = 0.85 (858.1 examples/sec; 0.149 sec/batch)
2017-05-09 01:41:08.941025: step 30460, loss = 0.99 (838.0 examples/sec; 0.153 sec/batch)
2017E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 623 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
-05-09 01:41:10.420763: step 30470, loss = 1.02 (865.0 examples/sec; 0.148 sec/batch)
2017-05-09 01:41:11.920255: step 30480, loss = 0.95 (853.6 examples/sec; 0.150 sec/batch)
2017-05-09 01:41:13.416994: step 30490, loss = 1.02 (855.2 examples/sec; 0.150 sec/batch)
2017-05-09 01:41:15.029196: step 30500, loss = 0.81 (793.9 examples/sec; 0.161 sec/batch)
2017-05-09 01:41:16.465034: step 30510, loss = 0.77 (891.5 examples/sec; 0.144 sec/batch)
2017-05-09 01:41:17.983728: step 30520, loss = 0.76 (842.8 examples/sec; 0.152 sec/batch)
2017-05-09 01:41:19.495475: step 30530, loss = 1.09 (846.7 examples/sec; 0.151 sec/batch)
2017-05-09 01:41:21.018479: step 30540, loss = 0.82 (840.4 examples/sec; 0.152 sec/batch)
2017-05-09 01:41:22.493245: step 30550, loss = 0.72 (867.9 examples/sec; 0.147 sec/batch)
2017-05-09 01:41:24.021867: step 30560, loss = 0.82 (837.4 examples/sec; 0.153 sec/batch)
2017-05-09 01:41:25.540266: step 30570, loss = 0.74 (843.0 examples/sec; 0.152 sec/batch)
2017-05-09 01:41:27.053751: step 30580, loss = 0.77 (845.7 examples/sec; 0.151 sec/batch)
2017-05-09 01:41:28.585551: step 30590, loss = 0.76 (835.6 examples/sec; 0.153 sec/batch)
2017-05-09 01:41:30.190656: step 30600, loss = 0.85 (797.5 examples/sec; 0.161 sec/batch)
2017-05-09 01:41:31.598580: step 30610, loss = 1.01 (909.1 examples/sec; 0.141 sec/batch)
2017-05-09 01:41:33.129653: step 30620, loss = 0.72 (836.0 examples/sec; 0.153 sec/batch)
2017-05-09 01:41:34.628351: step 30630, loss = 0.80 (854.1 examples/sec; 0.150 sec/batch)
2017-05-09 01:41:36.157061: step 30640, loss = 0.90 (837.3 examples/sec; 0.153 sec/batch)
2017-05-09 01:41:37.676677: step 30650, loss = 0.99 (842.3 examples/sec; 0.152 sec/batch)
2017-05-09 01:41:39.177398: step 30660, loss = 0.69 (852.9 examples/sec; 0.150 sec/batch)
2017-05-09 01:41:40.671571: step 30670, loss = 0.78 (856.7 examples/sec; 0.149 sec/batch)
2017-05-09 01:41:42.186172: step 30680, loss = 0.82 (845.1 examples/sec; 0.151 sec/batch)
2017-05-09 01:41:43.679245: step 30690, loss = 0.79 (857.3 examples/sec; 0.149 sec/batch)
2017-05-09 01:41:45.274898: step 30700, loss = 0.71 (802.2 examples/sec; 0.160 sec/batch)
2017-05-09 01:41:46.693570: step 30710, loss = 0.94 (902.3 examples/sec; 0.142 sec/batch)
2017-05-09 01:41:48.241357: step 30720, loss = 0.88 (827.0 examples/sec; 0.155 sec/batch)
2017-05-09 01:41:49.753730: step 30730, loss = 0.71 (846.3 examples/sec; 0.151 sec/batch)
2017-05-09 01:41:51.267821: step 30740, loss = 0.81 (845.4 examples/sec; 0.151 sec/batch)
2017-05-09 01:41:52.768356: step 30750, loss = 0.98 (853.1 examples/sec; 0.150 sec/batch)
2017-05-09 01:41:54.295954: step 30760, loss = 0.82 (837.9 examples/sec; 0.153 sec/batch)
2017-05-09 01:41:55.822864: step 30770, loss = 0.98 (838.3 examples/sec; 0.153 sec/batch)
2017-05-09 01:41:57.332787: step 30780, loss = 0.96 (847.7 examples/sec; 0.151 sec/batch)
2017-05-09 01:41:58.859621: step 30790, loss = 0.88 (838.3 examples/sec; 0.153 sec/batch)
2017-05-09 01:42:00.426940: step 30800, loss = 0.86 (816.7 examples/sec; 0.157 sec/batch)
2017-05-09 01:42:01.853619: step 30810, loss = 0.80 (897.2 examples/sec; 0.143 sec/batch)
2017-05-09 01:42:03.358385: step 30820, loss = 0.77 (850.6 examples/sec; 0.150 sec/batch)
2017-05-09 01:42:04.889175: step 30830, loss = 0.68 (836.2 examples/sec; 0.153 sec/batch)
2017-05-09 01:42:06.405306: step 30840, loss = 0.96 (844.3 examples/sec; 0.152 sec/batch)
2017-05-09 01:42:07.918177: step 30850, loss = 0.76 (846.1 examples/sec; 0.151 sec/batch)
2017-05-09 01:42:09.426592: step 30860, loss = 0.71 (848.6 examples/sec; 0.151 sec/batch)
2017-05-09 01:42:10.963756: step 30870, loss = 1.01 (832.7 examples/sec; 0.154 sec/batch)
2017-05-09 01:42:12.471465: step 30880, loss = 0.80 (849.0 examples/sec; 0.151 sec/batch)
2017-05-09 01:42:13.977995: step 30890, loss = 0.85 (849.6 examples/sec; 0.151 sec/batch)
2017-05-09 01:42:15.605812: step 30900, loss = 0.96 (786.3 examples/sec; 0.163 sec/batch)
2017-05-09 01:42:17.034254: step 30910, loss = 0.81 (896.1 examples/sec; 0.143 sec/batch)
2017-05-09 01:42:18.537739: step 30920, loss = 0.78 (851.4 examples/sec; 0.150 sec/batch)
2017-05-09 01:42:20.076778: step 30930, loss = 0.75 (831.7 examples/sec; 0.154 sec/batch)
2017-05-09 01:42:21.575239: step 30940, loss = 0.93 (854.2 examples/sec; 0.150 sec/batch)
2017-05-09 01:42:23.092010: step 30950, loss = 0.78 (843.9 examples/sec; 0.152 sec/batch)
2017-05-09 01:42:24.610279: step 30960, loss = 0.77 (843.1 examples/sec; 0.152 sec/batch)
2017-05-09 01:42:26.099803: step 30970, loss = 0.92 (859.3 examples/sec; 0.149 sec/batch)
2017-05-09 01:42:27.601723: step 30980, loss = 0.98 (852.2 examples/sec; 0.150 sec/batch)
2017-05-09 01:42:29.140584: step 30990, loss = 0.99 (831.8 examples/sec; 0.154 sec/batch)
2017-05-09 01:42:30.754905: step 31000, loss = 0.84 (792.9 examples/sec; 0.161 sec/batch)
2017-05-09 01:42:32.174593: step 31010, loss = 0.94 (901.6 examples/sec; 0.142 sec/batch)
2017-05-09 01:42:33.717216: step 31020, loss = 0.76 (829.8 examples/sec; 0.154 sec/batch)
2017-05-09 01:42:35.242959: step 31030, loss = 0.77 (838.9 examples/sec; 0.153 sec/batch)
2017-05-09 01:42:36.788043: step 31040, loss = 0.89 (828.4 examples/sec; 0.155 sec/batch)
2017-05-09 01:42:38.310629: step 31050, loss = 0.84 (840.7 examples/sec; 0.152 sec/batch)
2017-05-09 01:42:39.856113: step 31060, loss = 1.07 (828.2 examples/sec; 0.155 sec/batch)
2017-05-09 01:42:41.397689: step 31070, loss = 0.91 (830.3 examples/sec; 0.154 sec/batch)
2017-05-09 01:42:42.906922: step 31080, loss = 0.82 (848.1 examples/sec; 0.151 sec/batch)
2017-05-09 01:42:44.437147: step 31090, loss = 0.86 (836.5 examples/sec; 0.153 sec/batch)
2017-05-09 01:42:46.042917: step 31100, loss = 0.70 (797.1 examples/sec; 0.161 sec/batch)
2017-05-09 01:42:47.494773: step 31110, loss = 0.76 (881.6 examples/sec; 0.145 sec/batch)
2017-05-09 01:42:49.001479: step 31120, loss = 0.79 (849.5 examples/sec; 0.151 sec/batch)
2017-05-09 01:42:50.551041: step 31130, loss = 0.87 (826.0 examples/sec; 0.155 sec/batch)
2017-05-09 01:42:52.081337: step 31140, loss = 0.77 (836.5 examples/sec; 0.153 sec/batch)
2017-05-09 01:42:53.591607: step 31150, loss = 0.77 (847.5 examples/sec; 0.151 sec/batch)
2017-05-09 01:42:55.115721: step 31160, loss = 0.74 (839.8 examples/sec; 0.152 sec/batch)
2017-05-09 01:42:56.646173: step 31170, loss = 0.63 (836.4 examples/sec; 0.153 sec/batch)
2017-05-09 01:42:58.155096: step 31180, loss = 0.74 (848.3 examples/sec; 0.151 sec/batch)
2017-05-09 01:42:59.666833: step 31190, loss = 0.81 (846.7 examples/sec; 0.151 sec/batch)
2017-05-09 01:43:01.299139: step 31200, loss = 0.63 (784.2 examples/sec; 0.163 sec/batch)
2017-05-09 01:43:02.706854: step 31210, loss = 0.84 (909.3 examples/sec; 0.141 sec/batch)
2017-05-09 01:43:04.269173: step 31220, loss = 0.89 (819.3 examples/sec; 0.156 sec/batch)
2017-05-09 01:43:05.792334: step 31230, loss = 0.76 (840.4 examples/sec; 0.152 sec/batch)
2017-05-09 01:43:07.303620: step 31240, loss = 0.80 (847.0 examples/sec; 0.151 sec/batch)
2017-05-09 01:43:08.840131: step 31250, loss = 0.75 (833.1 examples/sec; 0.154 sec/batch)
2017-05-09 01:43:10.362445: step 31260, loss = 0.68 (840.8 examples/sec; 0.152 sec/batch)
2017-05-09 01:43:11.904434: step 31270, loss = 0.90 (830.1 examples/sec; 0.154 sec/batch)
2017-05-09 01:43:13.410540: step 31280, loss = 0.81 (849.9 examples/sec; 0.151 sec/batch)
2017-05-09 01:43:14.950072: step 31290, loss = 0.80 (831.4 examples/sec; 0.154 sec/batch)
2017-05-09 01:43:16.587572: step 31300, loss = 1.04 (781.7 examples/sec; 0.164 sec/batch)
2017-05-09 01:43:18.000706: step 31310, loss = 0.73 (905.8 examples/sec; 0.141 sec/batch)
2017-05-09 01:43:19.522997: step 31320, loss = 0.81 (840.8 examples/sec; 0.152 sec/batch)
2017-05-09 01:43:21.053621: step 31330, loss = 0.79 (836.3 examples/sec; 0.153 sec/batch)
2017-05-09 01:43:22.581963: step 31340, loss = 1.01 (837.5 examples/sec; 0.153 sec/batch)
2017-05-09 01:43:24.120386: step 31350, loss = 0.72 (832.0 examples/sec; 0.154 sec/batch)
2017-05-09 01:43:25.674210: step 31360, loss = 0.70 (823.8 examples/sec; 0.155 sec/batch)
2017-05-09 01:43:27.215983: step 31370, loss = 0.68 (830.2 examples/sec; 0.154 sec/batch)
2017-0E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 639 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
5-09 01:43:28.729563: step 31380, loss = 1.04 (845.7 examples/sec; 0.151 sec/batch)
2017-05-09 01:43:30.243277: step 31390, loss = 0.89 (845.6 examples/sec; 0.151 sec/batch)
2017-05-09 01:43:31.865050: step 31400, loss = 0.79 (789.3 examples/sec; 0.162 sec/batch)
2017-05-09 01:43:33.316491: step 31410, loss = 0.76 (881.9 examples/sec; 0.145 sec/batch)
2017-05-09 01:43:34.896207: step 31420, loss = 0.65 (810.3 examples/sec; 0.158 sec/batch)
2017-05-09 01:43:36.419374: step 31430, loss = 0.79 (840.4 examples/sec; 0.152 sec/batch)
2017-05-09 01:43:37.942679: step 31440, loss = 0.65 (840.3 examples/sec; 0.152 sec/batch)
2017-05-09 01:43:39.501230: step 31450, loss = 0.78 (821.3 examples/sec; 0.156 sec/batch)
2017-05-09 01:43:41.051572: step 31460, loss = 0.78 (825.6 examples/sec; 0.155 sec/batch)
2017-05-09 01:43:42.555823: step 31470, loss = 0.73 (850.9 examples/sec; 0.150 sec/batch)
2017-05-09 01:43:44.097994: step 31480, loss = 0.79 (830.0 examples/sec; 0.154 sec/batch)
2017-05-09 01:43:45.617471: step 31490, loss = 0.81 (842.4 examples/sec; 0.152 sec/batch)
2017-05-09 01:43:47.229716: step 31500, loss = 0.71 (793.9 examples/sec; 0.161 sec/batch)
2017-05-09 01:43:48.694618: step 31510, loss = 0.92 (873.8 examples/sec; 0.146 sec/batch)
2017-05-09 01:43:50.219411: step 31520, loss = 0.94 (839.5 examples/sec; 0.152 sec/batch)
2017-05-09 01:43:51.758369: step 31530, loss = 0.72 (831.7 examples/sec; 0.154 sec/batch)
2017-05-09 01:43:53.338789: step 31540, loss = 0.82 (809.9 examples/sec; 0.158 sec/batch)
2017-05-09 01:43:54.869304: step 31550, loss = 1.24 (836.3 examples/sec; 0.153 sec/batch)
2017-05-09 01:43:56.411615: step 31560, loss = 0.95 (829.9 examples/sec; 0.154 sec/batch)
2017-05-09 01:43:57.942515: step 31570, loss = 0.82 (836.1 examples/sec; 0.153 sec/batch)
2017-05-09 01:43:59.494992: step 31580, loss = 0.86 (824.5 examples/sec; 0.155 sec/batch)
2017-05-09 01:44:01.043764: step 31590, loss = 0.79 (826.5 examples/sec; 0.155 sec/batch)
2017-05-09 01:44:02.650543: step 31600, loss = 0.99 (796.6 examples/sec; 0.161 sec/batch)
2017-05-09 01:44:04.070291: step 31610, loss = 0.85 (901.6 examples/sec; 0.142 sec/batch)
2017-05-09 01:44:05.624417: step 31620, loss = 1.01 (823.6 examples/sec; 0.155 sec/batch)
2017-05-09 01:44:07.183231: step 31630, loss = 0.87 (821.1 examples/sec; 0.156 sec/batch)
2017-05-09 01:44:08.706953: step 31640, loss = 0.93 (840.0 examples/sec; 0.152 sec/batch)
2017-05-09 01:44:10.244013: step 31650, loss = 0.71 (832.8 examples/sec; 0.154 sec/batch)
2017-05-09 01:44:11.772647: step 31660, loss = 0.84 (837.3 examples/sec; 0.153 sec/batch)
2017-05-09 01:44:13.315605: step 31670, loss = 0.84 (829.6 examples/sec; 0.154 sec/batch)
2017-05-09 01:44:14.851217: step 31680, loss = 0.68 (833.5 examples/sec; 0.154 sec/batch)
2017-05-09 01:44:16.379560: step 31690, loss = 0.73 (837.5 examples/sec; 0.153 sec/batch)
2017-05-09 01:44:18.004781: step 31700, loss = 0.92 (787.6 examples/sec; 0.163 sec/batch)
2017-05-09 01:44:19.456115: step 31710, loss = 0.78 (881.9 examples/sec; 0.145 sec/batch)
2017-05-09 01:44:20.969657: step 31720, loss = 0.85 (845.7 examples/sec; 0.151 sec/batch)
2017-05-09 01:44:22.491446: step 31730, loss = 0.99 (841.1 examples/sec; 0.152 sec/batch)
2017-05-09 01:44:24.038776: step 31740, loss = 0.65 (827.2 examples/sec; 0.155 sec/batch)
2017-05-09 01:44:25.601117: step 31750, loss = 0.79 (819.3 examples/sec; 0.156 sec/batch)
2017-05-09 01:44:27.128612: step 31760, loss = 0.85 (838.0 examples/sec; 0.153 sec/batch)
2017-05-09 01:44:28.678171: step 31770, loss = 0.93 (826.0 examples/sec; 0.155 sec/batch)
2017-05-09 01:44:30.199290: step 31780, loss = 0.96 (841.5 examples/sec; 0.152 sec/batch)
2017-05-09 01:44:31.714658: step 31790, loss = 0.82 (844.7 examples/sec; 0.152 sec/batch)
2017-05-09 01:44:33.346696: step 31800, loss = 0.82 (784.3 examples/sec; 0.163 sec/batch)
2017-05-09 01:44:34.786039: step 31810, loss = 0.72 (889.3 examples/sec; 0.144 sec/batch)
2017-05-09 01:44:36.328749: step 31820, loss = 0.79 (829.7 examples/sec; 0.154 sec/batch)
2017-05-09 01:44:37.848829: step 31830, loss = 0.62 E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 656 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
(842.1 examples/sec; 0.152 sec/batch)
2017-05-09 01:44:39.390972: step 31840, loss = 0.71 (830.0 examples/sec; 0.154 sec/batch)
2017-05-09 01:44:40.927426: step 31850, loss = 0.73 (833.1 examples/sec; 0.154 sec/batch)
2017-05-09 01:44:42.471277: step 31860, loss = 0.92 (829.1 examples/sec; 0.154 sec/batch)
2017-05-09 01:44:44.022419: step 31870, loss = 0.95 (825.2 examples/sec; 0.155 sec/batch)
2017-05-09 01:44:45.581880: step 31880, loss = 0.77 (820.8 examples/sec; 0.156 sec/batch)
2017-05-09 01:44:47.101151: step 31890, loss = 0.82 (842.5 examples/sec; 0.152 sec/batch)
2017-05-09 01:44:48.768369: step 31900, loss = 0.81 (767.7 examples/sec; 0.167 sec/batch)
2017-05-09 01:44:50.196756: step 31910, loss = 0.76 (896.1 examples/sec; 0.143 sec/batch)
2017-05-09 01:44:51.717845: step 31920, loss = 0.79 (841.5 examples/sec; 0.152 sec/batch)
2017-05-09 01:44:53.269787: step 31930, loss = 0.67 (824.8 examples/sec; 0.155 sec/batch)
2017-05-09 01:44:54.839624: step 31940, loss = 0.86 (815.4 examples/sec; 0.157 sec/batch)
2017-05-09 01:44:56.393795: step 31950, loss = 0.85 (823.6 examples/sec; 0.155 sec/batch)
2017-05-09 01:44:58.008291: step 31960, loss = 0.89 (792.8 examples/sec; 0.161 sec/batch)
2017-05-09 01:44:59.585799: step 31970, loss = 0.72 (811.4 examples/sec; 0.158 sec/batch)
2017-05-09 01:45:01.140969: step 31980, loss = 0.81 (823.1 examples/sec; 0.156 sec/batch)
2017-05-09 01:45:02.704618: step 31990, loss = 0.78 (818.6 examples/sec; 0.156 sec/batch)
2017-05-09 01:45:04.322599: step 32000, loss = 0.80 (791.1 examples/sec; 0.162 sec/batch)
2017-05-09 01:45:05.764913: step 32010, loss = 0.87 (887.5 examples/sec; 0.144 sec/batch)
2017-05-09 01:45:07.314837: step 32020, loss = 0.90 (825.8 examples/sec; 0.155 sec/batch)
2017-05-09 01:45:08.859152: step 32030, loss = 0.95 (828.8 examples/sec; 0.154 sec/batch)
2017-05-09 01:45:10.422510: step 32040, loss = 0.70 (818.8 examples/sec; 0.156 sec/batch)
2017-05-09 01:45:11.956209: step 32050, loss = 0.84 (834.6 examples/sec; 0.153 sec/batch)
2017-05-09 01:45:13.523557: step 32060, loss = 0.77 (816.7 examples/sec; 0.157 sec/batch)
2017-05-09 01:45:15.070595: step 32070, loss = 0.71 (827.4 examples/sec; 0.155 sec/batch)
2017-05-09 01:45:16.609650: step 32080, loss = 0.74 (831.7 examples/sec; 0.154 sec/batch)
2017-05-09 01:45:18.131672: step 32090, loss = 0.73 (841.0 examples/sec; 0.152 sec/batch)
2017-05-09 01:45:19.783219: step 32100, loss = 0.82 (775.0 examples/sec; 0.165 sec/batch)
2017-05-09 01:45:21.229699: step 32110, loss = 0.87 (884.9 examples/sec; 0.145 sec/batch)
2017-05-09 01:45:22.781888: step 32120, loss = 0.74 (824.6 examples/sec; 0.155 sec/batch)
2017-05-09 01:45:24.336041: step 32130, loss = 1.03 (823.6 examples/sec; 0.155 sec/batch)
2017-05-09 01:45:25.864500: step 32140, loss = 0.69 (837.5 examples/sec; 0.153 sec/batch)
2017-05-09 01:45:27.441162: step 32150, loss = 0.83 (811.8 examples/sec; 0.158 sec/batch)
2017-05-09 01:45:28.994729: step 32160, loss = 0.86 (823.9 examples/sec; 0.155 sec/batch)
2017-05-09 01:45:30.554184: step 32170, loss = 1.26 (820.8 examples/sec; 0.156 sec/batch)
2017-05-09 01:45:32.113718: step 32180, loss = 0.79 (820.8 examples/sec; 0.156 sec/batch)
2017-05-09 01:45:33.668648: step 32190, loss = 0.92 (823.2 examples/sec; 0.155 sec/batch)
2017-05-09 01:45:35.309445: step 32200, loss = 0.84 (780.1 examples/sec; 0.164 sec/batch)
2017-05-09 01:45:36.763856: step 32210, loss = 1.00 (880.1 examples/sec; 0.145 sec/batch)
2017-05-09 01:45:38.312465: step 32220, loss = 0.82 (826.5 examples/sec; 0.155 sec/batch)
2017-05-09 01:45:39.860953: step 32230, loss = 0.68 (826.6 examples/sec; 0.155 sec/batch)
2017-05-09 01:45:41.417775: step 32240, loss = 1.08 (822.2 examples/sec; 0.156 sec/batch)
2017-05-09 01:45:42.965205: step 32250, loss = 0.74 (827.2 examples/sec; 0.155 sec/batch)
2017-05-09 01:45:44.500999: step 32260, loss = 0.82 (833.4 examples/sec; 0.154 sec/batch)
2017-05-09 01:45:46.062020: step 32270, loss = 0.77 (820.0 examples/sec; 0.156 sec/batch)
2017-05-09 01:45:47.626663: step 32280, loss = 0.87 (818.1 examples/sec; 0.156 sec/batch)
2017-05-09 01:45:49.200946: step 32290, loss = 1.00 (813.1 examples/sec; 0.157 sec/batch)
2017-05-09 01:45:50.837134: step 32300, loss = 0.73 (782.3 examples/sec; 0.164 sec/batch)
2017-05-09 01:45:52.294407: step 32310, loss = 0.80 (878.4 examples/sec; 0.146 sec/batch)
2017-05-09 01:45:53.842261: step 32320, loss = 0.80 (827.0 examples/sec; 0.155 sec/batch)
2017-05-09 01:45:55.373114: step 32330, loss = 0.81 (836.1 examples/sec; 0.153 sec/batch)
2017-05-09 01:45:56.925240: step 32340, loss = 0.79 (824.7 examples/sec; 0.155 sec/batch)
2017-05-09 01:45:58.481009: step 32350, loss = 0.80 (822.7 examples/sec; 0.156 sec/batch)
2017-05-09 01:46:00.051545: step 32360, loss = 0.81 (815.0 examples/sec; 0.157 sec/batch)
2017-05-09 01:46:01.580138: step 32370, loss = 0.92 (837.4 examples/sec; 0.153 sec/batch)
2017-05-09 01:46:03.097248: step 32380, loss = 1.06 (843.7 examples/sec; 0.152 sec/batch)
2017-05-09 01:46:04.643554: step 32390, loss = 0.67 (827.8 examples/sec; 0.155 sec/batch)
2017-05-09 01:46:06.257764: step 32400, loss = 0.68 (793.0 examples/sec; 0.161 sec/batch)
2017-05-09 01:46:07.738308: step 32410, loss = 0.63 (864.5 examples/sec; 0.148 sec/batch)
2017-05-09 01:46:09.291383: step 32420, loss = 0.83 (824.2 examples/sec; 0.155 sec/batch)
2017-05-09 01:46:10.848252: step 32430, loss = 0.84 (822.2 examples/sec; 0.156 sec/batch)
2017-05-09 01:46:12.398570: step 32440, loss = 0.80 (825.6 examples/sec; 0.155 sec/batch)
2017-05-09 01:46:13.935651: step 32450, loss = 0.80 (832.7 examples/sec; 0.154 sec/batch)
2017-05-09 01:46:15.491889: step 32460, loss = 0.72 (822.5 examples/sec; 0.156 sec/batch)
2017-05-09 01:46:17.051878: step 32470, loss = 1.02 (820.5 examples/sec; 0.156 sec/batch)
2017-05-09 01:46:18.611428: step 32480, loss = 0.74 (820.7 examples/sec; 0.156 sec/batch)
2017-05-09 01:46:20.188712: step 32490, loss = 0.78 (811.5 examples/sec; 0.158 sec/batch)
2017-05-09 01:46:21.811013: step 32500, loss = 0.61 (789.0 examples/sec; 0.162 sec/batch)
2017-05-09 01:46:23.293423: step 32510, loss = 0.80 (863.5 examples/sec; 0.148 sec/batch)
2017-05-09 01:46:24.846543: step 32520, loss = 0.75 (824.1 examples/sec; 0.155 sec/batch)
2017-05-09 01:46:26.396189: step 32530, loss = 0.89 (826.0 examples/sec; 0.155 sec/batch)
2017-05-09 01:46:27.957493: step 32540, loss = 0.79 (819.8 examples/sec; 0.156 sec/batch)
2017-05-09 01:46:29.541447: step 32550, loss = 0.95 (808.1 examples/sec; 0.158 sec/batch)
2017-05-09 01:46:31.083035: step 32560, loss = 0.86 (830.3 examples/sec; 0.154 sec/batch)
2017-05-09 01:46:32.657139: step 32570, loss = 0.63 (813.2 examples/sec; 0.157 sec/batch)
2017-05-09 01:46:34.217888: step 32580, loss = 0.85 (820.1 examples/sec; 0.156 sec/batch)
2017-05-09 01:46:35.754189: step 32590, loss = 0.86 (833.2 examples/sec; 0.154 sec/batch)
2017-05-09 01:46:37.385727: step 32600, loss = 0.79 (784.5 examples/sec; 0.163 sec/batch)
2017-05-09 01:46:38.824771: step 32610, loss = 0.69 (889.5 examples/sec; 0.144 sec/batch)
2017-05-09 01:46:40.353949: step 32620, loss = 0.76 (837.0 examples/sec; 0.153 sec/batch)
2017-05-09 01:46:41.923634: step 32630, loss = 0.82 (815.4 examples/sec; 0.157 sec/batch)
2017-05-09 01:46:43.506926: step 32640, loss = 0.68 (808.4 examples/sec; 0.158 sec/batch)
2017-05-09 01:46:45.055119: step 32650, loss = 0.71 (826.8 examples/sec; 0.155 sec/batch)
2017-05-09 01:46:46.596200: step 32660, loss = 0.77 (830.6 examples/sec; 0.154 sec/batch)
2017-05-09 01:46:48.159793: step 32670, loss = 0.86 (818.6 examples/sec; 0.156 sec/batch)
2017-05-09 01:46:49.719856: step 32680, loss = 0.74 (820.5 examples/sec; 0.156 sec/batch)
2017-05-09 01:46:51.288725: step 32690, loss = 0.86 (815.9 examples/sec; 0.157 sec/batch)
2017-05-09 01:46:52.946483: step 32700, loss = 0.85 (772.1 examples/sec; 0.166 sec/batch)
2017-05-09 01:46:54.435757: step 32710, loss = 1.07 (859.5 examples/sec; 0.149 sec/batch)
2017-05-09 01:46:55.986275: step 32720, loss = 0.95 (825.5 examples/sec; 0.155 sec/batch)
2017-05-09 01:46:57.539108: step 32730, loss = 0.88 (824.3 examples/sec; 0.155 sec/batch)
2017-05-09 01:46:59.069299: step 32740, loss = 0.76 (8E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 672 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
36.5 examples/sec; 0.153 sec/batch)
2017-05-09 01:47:00.659796: step 32750, loss = 0.83 (804.8 examples/sec; 0.159 sec/batch)
2017-05-09 01:47:02.223523: step 32760, loss = 1.01 (818.6 examples/sec; 0.156 sec/batch)
2017-05-09 01:47:03.807995: step 32770, loss = 0.91 (807.8 examples/sec; 0.158 sec/batch)
2017-05-09 01:47:05.396478: step 32780, loss = 0.77 (805.8 examples/sec; 0.159 sec/batch)
2017-05-09 01:47:06.923884: step 32790, loss = 0.79 (838.0 examples/sec; 0.153 sec/batch)
2017-05-09 01:47:08.578385: step 32800, loss = 0.83 (773.6 examples/sec; 0.165 sec/batch)
2017-05-09 01:47:10.041087: step 32810, loss = 0.83 (875.1 examples/sec; 0.146 sec/batch)
2017-05-09 01:47:11.567369: step 32820, loss = 1.06 (838.7 examples/sec; 0.153 sec/batch)
2017-05-09 01:47:13.125576: step 32830, loss = 0.89 (821.4 examples/sec; 0.156 sec/batch)
2017-05-09 01:47:14.688439: step 32840, loss = 0.94 (819.0 examples/sec; 0.156 sec/batch)
2017-05-09 01:47:16.237637: step 32850, loss = 0.96 (826.2 examples/sec; 0.155 sec/batch)
2017-05-09 01:47:17.789975: step 32860, loss = 0.73 (824.6 examples/sec; 0.155 sec/batch)
2017-05-09 01:47:19.355982: step 32870, loss = 0.89 (817.4 examples/sec; 0.157 sec/batch)
2017-05-09 01:47:20.900362: step 32880, loss = 0.83 (828.8 examples/sec; 0.154 sec/batch)
2017-05-09 01:47:22.440091: step 32890, loss = 0.75 (831.3 examples/sec; 0.154 sec/batch)
2017-05-09 01:47:24.095036: step 32900, loss = 0.94 (773.4 examples/sec; 0.165 sec/batch)
2017-05-09 01:47:25.580992: step 32910, loss = 0.78 (861.4 examples/sec; 0.149 sec/batch)
2017-05-09 01:47:27.116907: step 32920, loss = 0.91 (833.4 examples/sec; 0.154 sec/batch)
2017-05-09 01:47:28.706771: step 32930, loss = 0.81 (805.1 examples/sec; 0.159 sec/batch)
2017-05-09 01:47:30.249865: step 32940, loss = 0.90 (829.5 examples/sec; 0.154 sec/batch)
2017-05-09 01:47:31.773894: step 32950, loss = 0.83 (839.9 examples/sec; 0.152 sec/batch)
2017-05-09 01:47:33.295638: step 32960, loss = 0.67 (841.1 examples/sec; 0.152 sec/batch)
2017-05-09 01:47:34.855990: step 32970, loss = 0.98 (820.3 examples/sec; 0.156 sec/batch)
2017-05-09 01:47:36.403412: step 32980, loss = 0.71 (827.2 examples/sec; 0.155 sec/batch)
2017-05-09 01:47:37.969379: step 32990, loss = 0.99 (817.4 examples/sec; 0.157 sec/batch)
2017-05-09 01:47:39.612393: step 33000, loss = 0.84 (779.1 examples/sec; 0.164 sec/batch)
2017-05-09 01:47:41.086544: step 33010, loss = 0.84 (868.3 examples/sec; 0.147 sec/batch)
2017-05-09 01:47:42.644044: step 33020, loss = 0.80 (821.8 examples/sec; 0.156 sec/batch)
2017-05-09 01:47:44.231844: step 33030, loss = 0.92 (806.1 examples/sec; 0.159 sec/batch)
2017-05-09 01:47:45.805454: step 33040, loss = 0.83 (813.4 examples/sec; 0.157 sec/batch)
2017-05-09 01:47:47.353401: step 33050, loss = 1.01 (826.9 examples/sec; 0.155 sec/batch)
2017-05-09 01:47:48.899338: step 33060, loss = 0.67 (828.0 examples/sec; 0.155 sec/batch)
2017-05-09 01:47:50.464920: step 33070, loss = 0.75 (817.6 examples/sec; 0.157 sec/batch)
2017-05-09 01:47:52.025658: step 33080, loss = 0.70 (820.1 examples/sec; 0.156 sec/batch)
2017-05-09 01:47:53.583742: step 33090, loss = 0.77 (821.5 examples/sec; 0.156 sec/batch)
2017-05-09 01:47:55.243154: step 33100, loss = 0.85 (771.4 examples/sec; 0.166 sec/batch)
2017-05-09 01:47:56.696854: step 33110, loss = 0.77 (880.5 examples/sec; 0.145 sec/batch)
2017-05-09 01:47:58.296965: step 33120, loss = 0.81 (799.9 examples/sec; 0.160 sec/batch)
2017-05-09 01:47:59.839960: step 33130, loss = 0.72 (829.6 examples/sec; 0.154 sec/batch)
2017-05-09 01:48:01.393887: step 33140, loss = 0.87 (823.7 examples/sec; 0.155 sec/batch)
2017-05-09 01:48:02.952839: step 33150, loss = 0.93 (821.1 examples/sec; 0.156 sec/batch)
2017-05-09 01:48:04.534285: step 33160, loss = 0.86 (809.4 examples/sec; 0.158 sec/batch)
2017-05-09 01:48:06.118209: step 33170, loss = 0.72 (808.1 examples/sec; 0.158 sec/batch)
2017-05-09 01:48:07.682205: step 33180, loss = 0.83 (818.4 examples/sec; 0.156 sec/batch)
2017-05-09 01:48:09.212171: step 33190, loss = 0.93 (836.6 examples/sec; 0.153 sec/batch)
2017-05-09 01:48:10.873693: step 33200, loss = 0.83 (770.4 examples/sec; 0.166 sec/batch)
2017-05-09 01:48:12.331612: step 33210, loss = 0.71 (878.0 examples/sec; 0.146 sec/batch)
2017-05-09 01:48:13.891846: step 33220, loss = 0.79 (820.4 examples/sec; 0.156 sec/batch)
2017-05-09 01:48:15.486394: step 33230, loss = 0.96 (802.7 examples/sec; 0.159 sec/batch)
2017-05-09 01:48:17.051726: step 33240, loss = 0.99 (817.7 examples/sec; 0.157 sec/batch)
2017-05-09 01:48:18.635884: step 33250, loss = 0.86 (808.0 examples/sec; 0.158 sec/batch)
2017-05-09 01:48:20.243283: step 33260, loss = 0.90 (796.3 examples/sec; 0.161 sec/batch)
2017-05-09 01:48:21.806840: step 33270, loss = 0.91 (818.6 examples/sec; 0.156 sec/batch)
2017-05-09 01:48:23.384693: step 33280, loss = 0.77 (811.2 examples/sec; 0.158 sec/batch)
2017-05-09 01:48:24.941511: step 33290, loss = 0.76 (822.2 examples/sec; 0.156 sec/batch)
2017-05-09 01:48:26.597563: step 33300, loss = 0.93 (772.9 examples/sec; 0.166 sec/batch)
2017-05-09 01:48:28.070266: step 33310, loss = 0.91 (869.2 examples/sec; 0.147 sec/batch)
2017-05-09 01:48:29.636461: step 33320, loss = 0.90 (817.3 examples/sec; 0.157 sec/batch)
2017-05-09 01:48:31.211357: step 33330, loss = 0.85 (812.8 examples/sec; 0.157 sec/batch)
2017-05-09 01:48:32.787727: step 33340, loss = 0.88 (812.0 examples/sec; 0.158 sec/batch)
2017-05-09 01:48:34.340972: step 33350, loss = 0.92 (824.1 examples/sec; 0.155 sec/batch)
2017-05-09 01:48:35.926724: step 33360, loss = 1.01 (807.2 examples/sec; 0.159 sec/batch)
2017-05-09 01:48:37.499757: step 33370, loss = 0.85 (813.7 examples/sec; 0.157 sec/batch)
2017-05-09 01:48:39.113675: step 33380, loss = 0.88 (793.1 examples/sec; 0.161 sec/batch)
2017-05-09 01:48:40.680000: step 33390, loss = 0.84 (817.2 examples/sec; 0.157 sec/batch)
2017-05-09 01:48:42.383998: step 33400, loss = 0.71 (751.2 examples/sec; 0.170 sec/batch)
2017-05-09 01:48:43.852993: step 33410, loss = 0.81 (871.3 examples/sec; 0.147 sec/batch)
2017-05-09 01:48:45.384110: step 33420, loss = 0.85 (836.0 examples/sec; 0.153 sec/batch)
2017-05-09 01:48:46.929119: step 33430, loss = 0.85 (828.5 examples/sec; 0.155 sec/batch)
2017-05-09 01:48:48.490223: step 33440, loss = 0.76 (819.9 examples/sec; 0.156 sec/batch)
2017-05-09 01:48:50.044297: step 33450, loss = 0.74 (823.6 examples/sec; 0.155 sec/batch)
2017-05-09 01:48:51.616432: step 33460, loss = 0.83 (814.2 examples/sec; 0.157 sec/batch)
2017-05-09 01:48:53.152610: step 33470, loss = 0.86 (833.2 examples/sec; 0.154 sec/batch)
2017-05-09 01:48:54.708209: step 33480, loss = 0.82 (822.8 examples/sec; 0.156 sec/batch)
2017-05-09 01:48:56.256843: step 33490, loss = 0.66 (826.5 examples/sec; 0.155 sec/batch)
2017-05-09 01:48:57.898762: step 33500, loss = 0.94 (779.6 examples/sec; 0.164 sec/batch)
2017-05-09 01:48:59.392256: step 33510, loss = 0.78 (857.1 examples/sec; 0.149 sec/batch)
2017-05-09 01:49:00.928361: step 33520, loss = 0.80 (833.3 examples/sec; 0.154 sec/batch)
2017-05-09 01:49:02.532461: step 33530, loss = 0.78 (798.0 examples/sec; 0.160 sec/batch)
2017-05-09 01:49:04.088820: step 33540, loss = 0.90 (822.4 examples/sec; 0.156 sec/batch)
2017-05-09 01:49:05.620276: step 33550, loss = 0.73 (835.8 examples/sec; 0.153 sec/batch)
2017-05-09 01:49:07.189643: step 33560, loss = 0.82 (815.6 examples/sec; 0.157 sec/batch)
2017-05-09 01:49:08.746064: step 33570, loss = 0.80 (822.4 examples/sec; 0.156 sec/batch)
2017-05-09 01:49:10.292874: step 33580, loss = 0.80 (827.5 examples/sec; 0.155 sec/batch)
2017-05-09 01:49:11.890633: step 33590, loss = 0.70 (801.1 examples/sec; 0.160 sec/batch)
2017-05-09 01:49:13.538374: step 33600, loss = 0.81 (776.8 examples/sec; 0.165 sec/batch)
2017-05-09 01:49:15.009136: step 33610, loss = 0.94 (870.3 examples/sec; 0.147 sec/batch)
2017-05-09 01:49:16.598393: step 33620, loss = 0.98 (805.4 examples/sec; 0.159 sec/batch)
2017-05-09 01:49:18.179786: step 33630, loss = 0.96 (809.4 examples/sec; 0.158 sec/batch)
2017-05-09 01:49:19.755931: step 33640, loss = 0.63 (812.1 examples/sec; 0.158 sec/batch)
2017-05-09 01:49:21.313071: step 33650, loss = 0.72 (822E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 688 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
.0 examples/sec; 0.156 sec/batch)
2017-05-09 01:49:22.882655: step 33660, loss = 0.78 (815.5 examples/sec; 0.157 sec/batch)
2017-05-09 01:49:24.431660: step 33670, loss = 0.82 (826.3 examples/sec; 0.155 sec/batch)
2017-05-09 01:49:25.985295: step 33680, loss = 0.82 (823.9 examples/sec; 0.155 sec/batch)
2017-05-09 01:49:27.559607: step 33690, loss = 1.02 (813.1 examples/sec; 0.157 sec/batch)
2017-05-09 01:49:29.235950: step 33700, loss = 0.75 (763.6 examples/sec; 0.168 sec/batch)
2017-05-09 01:49:30.721102: step 33710, loss = 0.74 (861.9 examples/sec; 0.149 sec/batch)
2017-05-09 01:49:32.292092: step 33720, loss = 0.70 (814.8 examples/sec; 0.157 sec/batch)
2017-05-09 01:49:33.838430: step 33730, loss = 0.78 (827.8 examples/sec; 0.155 sec/batch)
2017-05-09 01:49:35.415974: step 33740, loss = 1.03 (811.4 examples/sec; 0.158 sec/batch)
2017-05-09 01:49:36.991044: step 33750, loss = 0.93 (812.7 examples/sec; 0.158 sec/batch)
2017-05-09 01:49:38.529718: step 33760, loss = 0.87 (831.9 examples/sec; 0.154 sec/batch)
2017-05-09 01:49:40.093733: step 33770, loss = 0.68 (818.4 examples/sec; 0.156 sec/batch)
2017-05-09 01:49:41.619176: step 33780, loss = 0.82 (839.1 examples/sec; 0.153 sec/batch)
2017-05-09 01:49:43.167393: step 33790, loss = 0.72 (826.8 examples/sec; 0.155 sec/batch)
2017-05-09 01:49:44.800927: step 33800, loss = 0.91 (783.6 examples/sec; 0.163 sec/batch)
2017-05-09 01:49:46.229893: step 33810, loss = 0.95 (895.8 examples/sec; 0.143 sec/batch)
2017-05-09 01:49:47.778352: step 33820, loss = 0.92 (826.7 examples/sec; 0.155 sec/batch)
2017-05-09 01:49:49.277816: step 33830, loss = 0.71 (853.6 examples/sec; 0.150 sec/batch)
2017-05-09 01:49:50.841961: step 33840, loss = 0.76 (818.3 examples/sec; 0.156 sec/batch)
2017-05-09 01:49:52.387146: step 33850, loss = 0.79 (828.4 examples/sec; 0.155 sec/batch)
2017-05-09 01:49:53.961093: step 33860, loss = 0.89 (813.2 examples/sec; 0.157 sec/batch)
2017-05-09 01:49:55.528760: step 33870, loss = 0.92 (816.5 examples/sec; 0.157 sec/batch)
2017-05-09 01:49:57.085996: step 33880, loss = 0.90 (822.0 examples/sec; 0.156 sec/batch)
2017-05-09 01:49:58.646341: step 33890, loss = 0.81 (820.3 examples/sec; 0.156 sec/batch)
2017-05-09 01:50:00.283624: step 33900, loss = 0.99 (781.8 examples/sec; 0.164 sec/batch)
2017-05-09 01:50:01.748919: step 33910, loss = 0.70 (873.6 examples/sec; 0.147 sec/batch)
2017-05-09 01:50:03.335669: step 33920, loss = 0.74 (806.7 examples/sec; 0.159 sec/batch)
2017-05-09 01:50:04.917878: step 33930, loss = 0.93 (809.0 examples/sec; 0.158 sec/batch)
2017-05-09 01:50:06.489219: step 33940, loss = 0.85 (814.6 examples/sec; 0.157 sec/batch)
2017-05-09 01:50:08.049401: step 33950, loss = 0.90 (820.4 examples/sec; 0.156 sec/batch)
2017-05-09 01:50:09.592726: step 33960, loss = 0.85 (829.4 examples/sec; 0.154 sec/batch)
2017-05-09 01:50:11.143648: step 33970, loss = 0.99 (825.3 examples/sec; 0.155 sec/batch)
2017-05-09 01:50:12.680748: step 33980, loss = 0.88 (832.7 examples/sec; 0.154 sec/batch)
2017-05-09 01:50:14.231197: step 33990, loss = 0.71 (825.6 examples/sec; 0.155 sec/batch)
2017-05-09 01:50:15.866024: step 34000, loss = 0.97 (783.0 examples/sec; 0.163 sec/batch)
2017-05-09 01:50:17.309943: step 34010, loss = 0.88 (886.5 examples/sec; 0.144 sec/batch)
2017-05-09 01:50:18.883906: step 34020, loss = 0.81 (813.2 examples/sec; 0.157 sec/batch)
2017-05-09 01:50:20.447366: step 34030, loss = 0.91 (818.7 examples/sec; 0.156 sec/batch)
2017-05-09 01:50:22.003953: step 34040, loss = 1.00 (822.3 examples/sec; 0.156 sec/batch)
2017-05-09 01:50:23.595610: step 34050, loss = 0.84 (804.2 examples/sec; 0.159 sec/batch)
2017-05-09 01:50:25.120267: step 34060, loss = 0.79 (839.5 examples/sec; 0.152 sec/batch)
2017-05-09 01:50:26.668264: step 34070, loss = 0.68 (826.9 examples/sec; 0.155 sec/batch)
2017-05-09 01:50:28.206535: step 34080, loss = 0.85 (832.1 examples/sec; 0.154 sec/batch)
2017-05-09 01:50:29.723770: step 34090, loss = 0.74 (843.6 examples/sec; 0.152 sec/batch)
2017-05-09 01:50:31.342207: step 34100, loss = 0.80 (790.9 examples/sec; 0.162 sec/batch)
2017-05-09 01:50:32.744149: step 34110, loss = 0.74 (913.0 examples/sec; 0.140 sec/batch)
2017-05-09 01:50:34.306777: step 34120, loss = 0.84 (819.1 examples/sec; 0.156 sec/batch)
2017-05-09 01:50:35.846689: step 34130, loss = 1.03 (831.2 examples/sec; 0.154 sec/batch)
2017-05-09 01:50:37.383273: step 34140, loss = 0.71 (833.0 examples/sec; 0.154 sec/batch)
2017-05-09 01:50:38.929558: step 34150, loss = 0.76 (827.8 examples/sec; 0.155 sec/batch)
2017-05-09 01:50:40.491957: step 34160, loss = 0.70 (819.2 examples/sec; 0.156 sec/batch)
2017-05-09 01:50:42.016702: step 34170, loss = 0.92 (839.5 examples/sec; 0.152 sec/batch)
2017-05-09 01:50:43.543639: step 34180, loss = 0.85 (838.3 examples/sec; 0.153 sec/batch)
2017-05-09 01:50:45.075070: step 34190, loss = 0.76 (835.8 examples/sec; 0.153 sec/batch)
2017-05-09 01:50:46.691458: step 34200, loss = 0.80 (791.9 examples/sec; 0.162 sec/batch)
2017-05-09 01:50:48.131950: step 34210, loss = 0.87 (888.6 examples/sec; 0.144 sec/batch)
2017-05-09 01:50:49.681291: step 34220, loss = 0.63 (826.2 examples/sec; 0.155 sec/batch)
2017-05-09 01:50:51.258536: step 34230, loss = 0.83 (811.5 examples/sec; 0.158 sec/batch)
2017-05-09 01:50:52.783118: step 34240, loss = 0.87 (839.6 examples/sec; 0.152 sec/batch)
2017-05-09 01:50:54.341310: step 34250, loss = 0.82 (821.5 examples/sec; 0.156 sec/batch)
2017-05-09 01:50:55.883108: step 34260, loss = 0.93 (830.2 examples/sec; 0.154 sec/batch)
2017-05-09 01:50:57.429359: step 34270, loss = 0.77 (827.8 examples/sec; 0.155 sec/batch)
2017-05-09 01:50:58.988046: step 34280, loss = 0.81 (821.2 examples/sec; 0.156 sec/batch)
2017-05-09 01:51:00.522855: step 34290, loss = 0.75 (834.0 examples/sec; 0.153 sec/batch)
2017-05-09 01:51:02.147940: step 34300, loss = 0.71 (787.7 examples/sec; 0.163 sec/batch)
2017-05-09 01:51:03.599065: step 34310, loss = 0.84 (882.1 examples/sec; 0.145 sec/batch)
2017-05-09 01:51:05.130247: step 34320, loss = 0.81 (835.9 examples/sec; 0.153 sec/batch)
2017-05-09 01:51:06.672005: step 34330, loss = 0.76 (830.2 examples/sec; 0.154 sec/batch)
2017-05-09 01:51:08.196142: step 34340, loss = 0.72 (839.8 examples/sec; 0.152 sec/batch)
2017-05-09 01:51:09.701206: step 34350, loss = 0.82 (850.5 examples/sec; 0.151 sec/batch)
2017-05-09 01:51:11.231790: step 34360, loss = 0.77 (836.3 examples/sec; 0.153 sec/batch)
2017-05-09 01:51:12.775389: step 34370, loss = 0.79 (829.2 examples/sec; 0.154 sec/batch)
2017-05-09 01:51:14.317601: step 34380, loss = 0.62 (830.0 examples/sec; 0.154 sec/batch)
2017-05-09 01:51:15.859223: step 34390, loss = 0.99 (830.3 examples/sec; 0.154 sec/batch)
2017-05-09 01:51:17.500429: step 34400, loss = 0.75 (779.9 examples/sec; 0.164 sec/batch)
2017-05-09 01:51:18.917689: step 34410, loss = 0.68 (903.1 examples/sec; 0.142 sec/batch)
2017-05-09 01:51:20.427701: step 34420, loss = 0.66 (847.7 examples/sec; 0.151 sec/batch)
2017-05-09 01:51:21.931705: step 34430, loss = 0.79 (851.1 examples/sec; 0.150 sec/batch)
2017-05-09 01:51:23.465867: step 34440, loss = 0.75 (834.3 examples/sec; 0.153 sec/batch)
2017-05-09 01:51:25.028783: step 34450, loss = 0.97 (819.0 examples/sec; 0.156 sec/batch)
2017-05-09 01:51:26.589070: step 34460, loss = 0.94 (820.4 examples/sec; 0.156 sec/batch)
2017-05-09 01:51:28.126765: step 34470, loss = 0.65 (832.4 examples/sec; 0.154 sec/batch)
2017-05-09 01:51:29.657021: step 34480, loss = 0.95 (836.5 examples/sec; 0.153 sec/batch)
2017-05-09 01:51:31.173410: step 34490, loss = 0.71 (844.1 examples/sec; 0.152 sec/batch)
2017-05-09 01:51:32.852499: step 34500, loss = 0.79 (762.3 examples/sec; 0.168 sec/batch)
2017-05-09 01:51:34.307082: step 34510, loss = 0.78 (880.0 examples/sec; 0.145 sec/batch)
2017-05-09 01:51:35.858384: step 34520, loss = 0.75 (825.1 examples/sec; 0.155 sec/batch)
2017-05-09 01:51:37.396823: step 34530, loss = 0.95 (832.0 examples/sec; 0.154 sec/batch)
2017-05-09 01:51:38.927742: step 34540, loss = 1.06 (836.1 examples/sec; 0.153 sec/batch)
2017-05-09 01:51:40.481095: step 34550, loss = 0.98 (824.0 examples/sec; 0.155 sec/batch)
2017-05-09 01:51:42.034422: step 34560, loss = 0.82 (824.0E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 704 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
 examples/sec; 0.155 sec/batch)
2017-05-09 01:51:43.566683: step 34570, loss = 0.74 (835.4 examples/sec; 0.153 sec/batch)
2017-05-09 01:51:45.110985: step 34580, loss = 0.92 (828.9 examples/sec; 0.154 sec/batch)
2017-05-09 01:51:46.637586: step 34590, loss = 0.89 (838.5 examples/sec; 0.153 sec/batch)
2017-05-09 01:51:48.285254: step 34600, loss = 0.68 (776.9 examples/sec; 0.165 sec/batch)
2017-05-09 01:51:49.723577: step 34610, loss = 0.80 (889.9 examples/sec; 0.144 sec/batch)
2017-05-09 01:51:51.235821: step 34620, loss = 0.70 (846.4 examples/sec; 0.151 sec/batch)
2017-05-09 01:51:52.759162: step 34630, loss = 0.84 (840.3 examples/sec; 0.152 sec/batch)
2017-05-09 01:51:54.260635: step 34640, loss = 0.71 (852.5 examples/sec; 0.150 sec/batch)
2017-05-09 01:51:55.766676: step 34650, loss = 0.79 (849.9 examples/sec; 0.151 sec/batch)
2017-05-09 01:51:57.320775: step 34660, loss = 0.81 (823.6 examples/sec; 0.155 sec/batch)
2017-05-09 01:51:58.875731: step 34670, loss = 0.84 (823.2 examples/sec; 0.155 sec/batch)
2017-05-09 01:52:00.435084: step 34680, loss = 0.79 (820.9 examples/sec; 0.156 sec/batch)
2017-05-09 01:52:01.953669: step 34690, loss = 1.06 (842.9 examples/sec; 0.152 sec/batch)
2017-05-09 01:52:03.590775: step 34700, loss = 0.82 (781.9 examples/sec; 0.164 sec/batch)
2017-05-09 01:52:05.026169: step 34710, loss = 0.70 (891.7 examples/sec; 0.144 sec/batch)
2017-05-09 01:52:06.547297: step 34720, loss = 0.92 (841.5 examples/sec; 0.152 sec/batch)
2017-05-09 01:52:08.079765: step 34730, loss = 0.84 (835.2 examples/sec; 0.153 sec/batch)
2017-05-09 01:52:09.582113: step 34740, loss = 0.76 (852.0 examples/sec; 0.150 sec/batch)
2017-05-09 01:52:11.115501: step 34750, loss = 0.77 (834.8 examples/sec; 0.153 sec/batch)
2017-05-09 01:52:12.627990: step 34760, loss = 0.84 (846.3 examples/sec; 0.151 sec/batch)
2017-05-09 01:52:14.162088: step 34770, loss = 0.96 (834.4 examples/sec; 0.153 sec/batch)
2017-05-09 01:52:15.685023: step 34780, loss = 0.66 (840.5 examples/sec; 0.152 sec/batch)
2017-05-09 01:52:17.195779: step 34790, loss = 0.87 (847.3 examples/sec; 0.151 sec/batch)
2017-05-09 01:52:18.847414: step 34800, loss = 0.80 (775.0 examples/sec; 0.165 sec/batch)
2017-05-09 01:52:20.307072: step 34810, loss = 0.90 (876.9 examples/sec; 0.146 sec/batch)
2017-05-09 01:52:21.852254: step 34820, loss = 0.76 (828.4 examples/sec; 0.155 sec/batch)
2017-05-09 01:52:23.428542: step 34830, loss = 0.77 (812.0 examples/sec; 0.158 sec/batch)
2017-05-09 01:52:24.953379: step 34840, loss = 1.02 (839.4 examples/sec; 0.152 sec/batch)
2017-05-09 01:52:26.517224: step 34850, loss = 0.70 (818.5 examples/sec; 0.156 sec/batch)
2017-05-09 01:52:28.058955: step 34860, loss = 0.70 (830.2 examples/sec; 0.154 sec/batch)
2017-05-09 01:52:29.592228: step 34870, loss = 0.78 (834.8 examples/sec; 0.153 sec/batch)
2017-05-09 01:52:31.144112: step 34880, loss = 0.78 (824.8 examples/sec; 0.155 sec/batch)
2017-05-09 01:52:32.683917: step 34890, loss = 0.82 (831.3 examples/sec; 0.154 sec/batch)
2017-05-09 01:52:34.316652: step 34900, loss = 0.94 (784.0 examples/sec; 0.163 sec/batch)
2017-05-09 01:52:35.781588: step 34910, loss = 0.77 (873.8 examples/sec; 0.146 sec/batch)
2017-05-09 01:52:37.337103: step 34920, loss = 1.01 (822.9 examples/sec; 0.156 sec/batch)
2017-05-09 01:52:38.905181: step 34930, loss = 1.02 (816.3 examples/sec; 0.157 sec/batch)
2017-05-09 01:52:40.439381: step 34940, loss = 0.80 (834.3 examples/sec; 0.153 sec/batch)
2017-05-09 01:52:41.967183: step 34950, loss = 1.09 (837.8 examples/sec; 0.153 sec/batch)
2017-05-09 01:52:43.538133: step 34960, loss = 0.99 (814.8 examples/sec; 0.157 sec/batch)
2017-05-09 01:52:45.081307: step 34970, loss = 0.73 (829.5 examples/sec; 0.154 sec/batch)
2017-05-09 01:52:46.605821: step 34980, loss = 0.84 (839.6 examples/sec; 0.152 sec/batch)
2017-05-09 01:52:48.144846: step 34990, loss = 0.66 (831.7 examples/sec; 0.154 sec/batch)
2017-05-09 01:52:49.789120: step 35000, loss = 0.83 (778.5 examples/sec; 0.164 sec/batch)
2017-05-09 01:52:51.265479: step 35010, loss = 0.88 (867.0 examples/sec; 0.148 sec/batch)
2017-05-09 01:E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 720 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
52:52.865128: step 35020, loss = 0.90 (800.2 examples/sec; 0.160 sec/batch)
2017-05-09 01:52:54.416264: step 35030, loss = 0.62 (825.2 examples/sec; 0.155 sec/batch)
2017-05-09 01:52:55.958815: step 35040, loss = 0.72 (829.8 examples/sec; 0.154 sec/batch)
2017-05-09 01:52:57.478516: step 35050, loss = 0.72 (842.3 examples/sec; 0.152 sec/batch)
2017-05-09 01:52:58.996638: step 35060, loss = 0.58 (843.1 examples/sec; 0.152 sec/batch)
2017-05-09 01:53:00.539862: step 35070, loss = 0.85 (829.4 examples/sec; 0.154 sec/batch)
2017-05-09 01:53:02.087166: step 35080, loss = 0.82 (827.2 examples/sec; 0.155 sec/batch)
2017-05-09 01:53:03.618239: step 35090, loss = 1.06 (836.0 examples/sec; 0.153 sec/batch)
2017-05-09 01:53:05.251767: step 35100, loss = 0.83 (783.6 examples/sec; 0.163 sec/batch)
2017-05-09 01:53:06.701038: step 35110, loss = 0.74 (883.2 examples/sec; 0.145 sec/batch)
2017-05-09 01:53:08.227656: step 35120, loss = 0.78 (838.5 examples/sec; 0.153 sec/batch)
2017-05-09 01:53:09.799667: step 35130, loss = 0.91 (814.2 examples/sec; 0.157 sec/batch)
2017-05-09 01:53:11.350251: step 35140, loss = 0.80 (825.5 examples/sec; 0.155 sec/batch)
2017-05-09 01:53:12.941750: step 35150, loss = 0.86 (804.3 examples/sec; 0.159 sec/batch)
2017-05-09 01:53:14.486199: step 35160, loss = 0.98 (828.8 examples/sec; 0.154 sec/batch)
2017-05-09 01:53:16.010230: step 35170, loss = 0.79 (839.9 examples/sec; 0.152 sec/batch)
2017-05-09 01:53:17.560090: step 35180, loss = 0.84 (825.9 examples/sec; 0.155 sec/batch)
2017-05-09 01:53:19.134229: step 35190, loss = 0.81 (813.1 examples/sec; 0.157 sec/batch)
2017-05-09 01:53:20.782708: step 35200, loss = 0.92 (776.5 examples/sec; 0.165 sec/batch)
2017-05-09 01:53:22.260727: step 35210, loss = 0.78 (866.0 examples/sec; 0.148 sec/batch)
2017-05-09 01:53:23.852855: step 35220, loss = 0.82 (803.9 examples/sec; 0.159 sec/batch)
2017-05-09 01:53:25.385063: step 35230, loss = 0.82 (835.4 examples/sec; 0.153 sec/batch)
2017-05-09 01:53:26.917275: step 35240, loss = 0.93 (835.4 examples/sec; 0.153 sec/batch)
2017-05-09 01:53:28.449661: step 35250, loss = 0.65 (835.3 examples/sec; 0.153 sec/batch)
2017-05-09 01:53:30.009673: step 35260, loss = 0.84 (820.5 examples/sec; 0.156 sec/batch)
2017-05-09 01:53:31.514081: step 35270, loss = 0.91 (850.8 examples/sec; 0.150 sec/batch)
2017-05-09 01:53:33.057728: step 35280, loss = 0.78 (829.2 examples/sec; 0.154 sec/batch)
2017-05-09 01:53:34.597822: step 35290, loss = 0.74 (831.1 examples/sec; 0.154 sec/batch)
2017-05-09 01:53:36.198859: step 35300, loss = 0.96 (799.5 examples/sec; 0.160 sec/batch)
2017-05-09 01:53:37.629025: step 35310, loss = 1.17 (895.0 examples/sec; 0.143 sec/batch)
2017-05-09 01:53:39.133121: step 35320, loss = 0.88 (851.0 examples/sec; 0.150 sec/batch)
2017-05-09 01:53:40.643627: step 35330, loss = 0.96 (847.4 examples/sec; 0.151 sec/batch)
2017-05-09 01:53:42.165782: step 35340, loss = 0.74 (840.9 examples/sec; 0.152 sec/batch)
2017-05-09 01:53:43.714893: step 35350, loss = 0.68 (826.3 examples/sec; 0.155 sec/batch)
2017-05-09 01:53:45.250995: step 35360, loss = 0.88 (833.3 examples/sec; 0.154 sec/batch)
2017-05-09 01:53:46.767229: step 35370, loss = 0.79 (844.2 examples/sec; 0.152 sec/batch)
2017-05-09 01:53:48.323142: step 35380, loss = 0.89 (822.7 examples/sec; 0.156 sec/batch)
2017-05-09 01:53:49.850206: step 35390, loss = 0.89 (838.2 examples/sec; 0.153 sec/batch)
2017-05-09 01:53:51.496332: step 35400, loss = 0.87 (777.6 examples/sec; 0.165 sec/batch)
2017-05-09 01:53:52.949612: step 35410, loss = 0.88 (880.8 examples/sec; 0.145 sec/batch)
2017-05-09 01:53:54.472002: step 35420, loss = 0.72 (840.8 examples/sec; 0.152 sec/batch)
2017-05-09 01:53:56.007555: step 35430, loss = 0.87 (833.6 examples/sec; 0.154 sec/batch)
2017-05-09 01:53:57.549381: step 35440, loss = 0.87 (830.2 examples/sec; 0.154 sec/batch)
2017-05-09 01:53:59.076158: step 35450, loss = 0.78 (838.4 examples/sec; 0.153 sec/batch)
2017-05-09 01:54:00.620137: step 35460, loss = 0.79 (829.0 examples/sec; 0.154 sec/batch)
2017-05-09 01:54:02.200439: step 35470, loss = 0.93 (810.0 examples/sec; 0.158 sec/batch)
2017-05-09 01:54:03.755502: step 35480, loss = 0.91 (823.1 examples/sec; 0.156 sec/batch)
2017-05-09 01:54:05.309840: step 35490, loss = 0.77 (823.5 examples/sec; 0.155 sec/batch)
2017-05-09 01:54:06.967182: step 35500, loss = 0.71 (772.3 examples/sec; 0.166 sec/batch)
2017-05-09 01:54:08.409288: step 35510, loss = 0.98 (887.6 examples/sec; 0.144 sec/batch)
2017-05-09 01:54:09.979540: step 35520, loss = 0.89 (815.2 examples/sec; 0.157 sec/batch)
2017-05-09 01:54:11.552456: step 35530, loss = 0.83 (813.8 examples/sec; 0.157 sec/batch)
2017-05-09 01:54:13.111729: step 35540, loss = 0.71 (820.9 examples/sec; 0.156 sec/batch)
2017-05-09 01:54:14.648384: step 35550, loss = 0.74 (833.0 examples/sec; 0.154 sec/batch)
2017-05-09 01:54:16.216931: step 35560, loss = 0.89 (816.0 examples/sec; 0.157 sec/batch)
2017-05-09 01:54:17.752667: step 35570, loss = 0.80 (833.5 examples/sec; 0.154 sec/batch)
2017-05-09 01:54:19.334786: step 35580, loss = 1.01 (809.0 examples/sec; 0.158 sec/batch)
2017-05-09 01:54:20.891542: step 35590, loss = 0.91 (822.2 examples/sec; 0.156 sec/batch)
2017-05-09 01:54:22.561479: step 35600, loss = 0.74 (766.5 examples/sec; 0.167 sec/batch)
2017-05-09 01:54:24.070953: step 35610, loss = 0.86 (848.0 examples/sec; 0.151 sec/batch)
2017-05-09 01:54:25.623048: step 35620, loss = 0.74 (824.7 examples/sec; 0.155 sec/batch)
2017-05-09 01:54:27.208523: step 35630, loss = 0.83 (807.3 examples/sec; 0.159 sec/batch)
2017-05-09 01:54:28.740704: step 35640, loss = 0.78 (835.4 examples/sec; 0.153 sec/batch)
2017-05-09 01:54:30.292917: step 35650, loss = 0.73 (824.6 examples/sec; 0.155 sec/batch)
2017-05-09 01:54:31.859789: step 35660, loss = 0.78 (816.9 examples/sec; 0.157 sec/batch)
2017-05-09 01:54:33.399970: step 35670, loss = 1.02 (831.1 examples/sec; 0.154 sec/batch)
2017-05-09 01:54:34.963388: step 35680, loss = 0.77 (818.7 examples/sec; 0.156 sec/batch)
2017-05-09 01:54:36.545125: step 35690, loss = 0.95 (809.2 examples/sec; 0.158 sec/batch)
2017-05-09 01:54:38.223201: step 35700, loss = 0.95 (762.8 examples/sec; 0.168 sec/batch)
2017-05-09 01:54:39.699308: step 35710, loss = 0.90 (867.1 examples/sec; 0.148 sec/batch)
2017-05-09 01:54:41.244881: step 35720, loss = 0.84 (828.2 examples/sec; 0.155 sec/batch)
2017-05-09 01:54:42.795724: step 35730, loss = 0.79 (825.3 examples/sec; 0.155 sec/batch)
2017-05-09 01:54:44.334020: step 35740, loss = 0.75 (832.1 examples/sec; 0.154 sec/batch)
2017-05-09 01:54:45.863421: step 35750, loss = 0.75 (836.9 examples/sec; 0.153 sec/batch)
2017-05-09 01:54:47.412060: step 35760, loss = 0.84 (826.5 examples/sec; 0.155 sec/batch)
2017-05-09 01:54:48.943268: step 35770, loss = 0.88 (835.9 examples/sec; 0.153 sec/batch)
2017-05-09 01:54:50.484905: step 35780, loss = 0.77 (830.3 examples/sec; 0.154 sec/batch)
2017-05-09 01:54:52.087845: step 35790, loss = 0.65 (798.5 examples/sec; 0.160 sec/batch)
2017-05-09 01:54:53.724987: step 35800, loss = 0.80 (781.9 examples/sec; 0.164 sec/batch)
2017-05-09 01:54:55.172675: step 35810, loss = 0.74 (884.2 examples/sec; 0.145 sec/batch)
2017-05-09 01:54:56.724454: step 35820, loss = 1.03 (824.9 examples/sec; 0.155 sec/batch)
2017-05-09 01:54:58.317750: step 35830, loss = 0.82 (803.4 examples/sec; 0.159 sec/batch)
2017-05-09 01:54:59.873050: step 35840, loss = 0.92 (823.0 examples/sec; 0.156 sec/batch)
2017-05-09 01:55:01.432608: step 35850, loss = 0.70 (820.7 examples/sec; 0.156 sec/batch)
2017-05-09 01:55:03.028891: step 35860, loss = 0.79 (801.9 examples/sec; 0.160 sec/batch)
2017-05-09 01:55:04.554403: step 35870, loss = 0.71 (839.1 examples/sec; 0.153 sec/batch)
2017-05-09 01:55:06.066633: step 35880, loss = 0.72 (846.4 examples/sec; 0.151 sec/batch)
2017-05-09 01:55:07.607790: step 35890, loss = 0.69 (830.6 examples/sec; 0.154 sec/batch)
2017-05-09 01:55:09.246565: step 35900, loss = 0.72 (781.1 examples/sec; 0.164 sec/batch)
2017-05-09 01:55:10.701155: step 35910, loss = 1.05 (880.0 examples/sec; 0.145 sec/batch)
2017-05-09 01:55:12.222428: step 35920, loss = 0.59 (841.4 examples/sec; 0.152 sec/batch)
2017-05-09 01:55E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 737 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
:13.776450: step 35930, loss = 0.67 (823.7 examples/sec; 0.155 sec/batch)
2017-05-09 01:55:15.341884: step 35940, loss = 0.69 (817.7 examples/sec; 0.157 sec/batch)
2017-05-09 01:55:16.898029: step 35950, loss = 0.84 (822.5 examples/sec; 0.156 sec/batch)
2017-05-09 01:55:18.454373: step 35960, loss = 0.94 (822.4 examples/sec; 0.156 sec/batch)
2017-05-09 01:55:20.013840: step 35970, loss = 0.90 (820.8 examples/sec; 0.156 sec/batch)
2017-05-09 01:55:21.547094: step 35980, loss = 0.74 (834.8 examples/sec; 0.153 sec/batch)
2017-05-09 01:55:23.099610: step 35990, loss = 0.92 (824.5 examples/sec; 0.155 sec/batch)
2017-05-09 01:55:24.737794: step 36000, loss = 0.83 (781.4 examples/sec; 0.164 sec/batch)
2017-05-09 01:55:26.218360: step 36010, loss = 1.00 (864.5 examples/sec; 0.148 sec/batch)
2017-05-09 01:55:27.771421: step 36020, loss = 0.74 (824.2 examples/sec; 0.155 sec/batch)
2017-05-09 01:55:29.297801: step 36030, loss = 0.89 (838.6 examples/sec; 0.153 sec/batch)
2017-05-09 01:55:30.830564: step 36040, loss = 0.98 (835.1 examples/sec; 0.153 sec/batch)
2017-05-09 01:55:32.390414: step 36050, loss = 0.74 (820.6 examples/sec; 0.156 sec/batch)
2017-05-09 01:55:33.928116: step 36060, loss = 1.03 (832.4 examples/sec; 0.154 sec/batch)
2017-05-09 01:55:35.494770: step 36070, loss = 0.79 (817.0 examples/sec; 0.157 sec/batch)
2017-05-09 01:55:37.009717: step 36080, loss = 0.72 (844.9 examples/sec; 0.151 sec/batch)
2017-05-09 01:55:38.562492: step 36090, loss = 0.77 (824.3 examples/sec; 0.155 sec/batch)
2017-05-09 01:55:40.184300: step 36100, loss = 0.84 (789.2 examples/sec; 0.162 sec/batch)
2017-05-09 01:55:41.606673: step 36110, loss = 0.87 (899.9 examples/sec; 0.142 sec/batch)
2017-05-09 01:55:43.149146: step 36120, loss = 0.66 (829.8 examples/sec; 0.154 sec/batch)
2017-05-09 01:55:44.723749: step 36130, loss = 1.04 (812.9 examples/sec; 0.157 sec/batch)
2017-05-09 01:55:46.283967: step 36140, loss = 0.77 (820.4 examples/sec; 0.156 sec/batch)
2017-05-09 01:55:47.837269: step 36150, loss = 0.88 (824.1 examples/sec; 0.155 sec/batch)
2017-05-09 01:55:49.360978: step 36160, loss = 0.76 (840.0 examples/sec; 0.152 sec/batch)
2017-05-09 01:55:50.909520: step 36170, loss = 0.76 (826.6 examples/sec; 0.155 sec/batch)
2017-05-09 01:55:52.468247: step 36180, loss = 0.90 (821.2 examples/sec; 0.156 sec/batch)
2017-05-09 01:55:54.002556: step 36190, loss = 0.69 (834.3 examples/sec; 0.153 sec/batch)
2017-05-09 01:55:55.637959: step 36200, loss = 0.76 (782.7 examples/sec; 0.164 sec/batch)
2017-05-09 01:55:57.070703: step 36210, loss = 0.75 (893.4 examples/sec; 0.143 sec/batch)
2017-05-09 01:55:58.605602: step 36220, loss = 0.93 (833.9 examples/sec; 0.153 sec/batch)
2017-05-09 01:56:00.148623: step 36230, loss = 0.84 (829.5 examples/sec; 0.154 sec/batch)
2017-05-09 01:56:01.677121: step 36240, loss = 0.83 (837.4 examples/sec; 0.153 sec/batch)
2017-05-09 01:56:03.214555: step 36250, loss = 0.67 (832.6 examples/sec; 0.154 sec/batch)
2017-05-09 01:56:04.746222: step 36260, loss = 0.67 (835.7 examples/sec; 0.153 sec/batch)
2017-05-09 01:56:06.255535: step 36270, loss = 0.85 (848.1 examples/sec; 0.151 sec/batch)
2017-05-09 01:56:07.777878: step 36280, loss = 0.77 (840.8 examples/sec; 0.152 sec/batch)
2017-05-09 01:56:09.333929: step 36290, loss = 0.92 (822.6 examples/sec; 0.156 sec/batch)
2017-05-09 01:56:10.993407: step 36300, loss = 0.99 (771.3 examples/sec; 0.166 sec/batch)
2017-05-09 01:56:12.486819: step 36310, loss = 0.73 (857.1 examples/sec; 0.149 sec/batch)
2017-05-09 01:56:14.019568: step 36320, loss = 0.81 (835.1 examples/sec; 0.153 sec/batch)
2017-05-09 01:56:15.566800: step 36330, loss = 0.71 (827.3 examples/sec; 0.155 sec/batch)
2017-05-09 01:56:17.098212: step 36340, loss = 0.91 (835.8 examples/sec; 0.153 sec/batch)
2017-05-09 01:56:18.590293: step 36350, loss = 0.75 (857.9 examples/sec; 0.149 sec/batch)
2017-05-09 01:56:20.112597: step 36360, loss = 0.72 (840.8 examples/sec; 0.152 sec/batch)
2017-05-09 01:56:21.620218: step 36370, loss = 0.89 (849.0 examples/sec; 0.151 sec/batch)
2017-05-09 01:56:23.147048: step 36380, loss = 1.11 (838.3 examples/sec; 0.153 sec/batch)
2017-05-09 01:56:24.652423: step 36390, loss = 0.98 (850.3 examples/sec; 0.151 sec/batch)
2017-05-09 01:56:26.238378: step 36400, loss = 0.92 (807.1 examples/sec; 0.159 sec/batch)
2017-05-09 01:56:27.656650: step 36410, loss = 0.76 (902.5 examples/sec; 0.142 sec/batch)
2017-05-09 01:56:29.161660: step 36420, loss = 0.76 (850.5 examples/sec; 0.151 sec/batch)
2017-05-09 01:56:30.668409: step 36430, loss = 0.83 (849.5 examples/sec; 0.151 sec/batch)
2017-05-09 01:56:32.203140: step 36440, loss = 0.82 (834.0 examples/sec; 0.153 sec/batch)
2017-05-09 01:56:33.702448: step 36450, loss = 0.73 (853.7 examples/sec; 0.150 sec/batch)
2017-05-09 01:56:35.229567: step 36460, loss = 0.89 (838.2 examples/sec; 0.153 sec/batch)
2017-05-09 01:56:36.745492: step 36470, loss = 0.84 (844.4 examples/sec; 0.152 sec/batch)
2017-05-09 01:56:38.273034: step 36480, loss = 0.75 (837.9 examples/sec; 0.153 sec/batch)
2017-05-09 01:56:39.784698: step 36490, loss = 0.68 (846.8 examples/sec; 0.151 sec/batch)
2017-05-09 01:56:41.377662: step 36500, loss = 0.74 (803.5 examples/sec; 0.159 sec/batch)
2017-05-09 01:56:42.799815: step 36510, loss = 0.87 (900.1 examples/sec; 0.142 sec/batch)
2017-05-09 01:56:44.330369: step 36520, loss = 0.82 (836.3 examples/sec; 0.153 sec/batch)
2017-05-09 01:56:45.826170: step 36530, loss = 0.78 (855.7 examples/sec; 0.150 sec/batch)
2017-05-09 01:56:47.360813: step 36540, loss = 0.91 (834.1 examples/sec; 0.153 sec/batch)
2017-05-09 01:56:48.886117: step 36550, loss = 0.78 (839.2 examples/sec; 0.153 sec/batch)
2017-05-09 01:56:50.406402: step 36560, loss = 0.85 (841.9 examples/sec; 0.152 sec/batch)
2017-05-09 01:56:51.927093: step 36570, loss = 1.04 (841.7 examples/sec; 0.152 sec/batch)
2017-05-09 01:56:53.456538: step 36580, loss = 1.00 (836.9 examples/sec; 0.153 sec/batch)
2017-05-09 01:56:54.974574: step 36590, loss = 0.88 (843.2 examples/sec; 0.152 sec/batch)
2017-05-09 01:56:56.606711: step 36600, loss = 0.83 (784.2 examples/sec; 0.163 sec/batch)
2017-05-09 01:56:58.042063: step 36610, loss = 0.77 (891.8 examples/sec; 0.144 sec/batch)
2017-05-09 01:56:59.571644: step 36620, loss = 0.98 (836.8 examples/sec; 0.153 sec/batch)
2017-05-09 01:57:01.090713: step 36630, loss = 0.86 (842.6 examples/sec; 0.152 sec/batch)
2017-05-09 01:57:02.596255: step 36640, loss = 0.68 (850.2 examples/sec; 0.151 sec/batch)
2017-05-09 01:57:04.104940: step 36650, loss = 0.91 (848.4 examples/sec; 0.151 sec/batch)
2017-05-09 01:57:05.640417: step 36660, loss = 0.74 (833.6 examples/sec; 0.154 sec/batch)
2017-05-09 01:57:07.187571: step 36670, loss = 0.79 (827.3 examples/sec; 0.155 sec/batch)
2017-05-09 01:57:08.725822: step 36680, loss = 0.64 (832.1 examples/sec; 0.154 sec/batch)
2017-05-09 01:57:10.272203: step 36690, loss = 0.88 (827.7 examples/sec; 0.155 sec/batch)
2017-05-09 01:57:11.889044: step 36700, loss = 0.72 (791.7 examples/sec; 0.162 sec/batch)
2017-05-09 01:57:13.330972: step 36710, loss = 1.08 (887.7 examples/sec; 0.144 sec/batch)
2017-05-09 01:57:14.876656: step 36720, loss = 0.72 (828.1 examples/sec; 0.155 sec/batch)
2017-05-09 01:57:16.438037: step 36730, loss = 0.81 (819.8 examples/sec; 0.156 sec/batch)
2017-05-09 01:57:17.964586: step 36740, loss = 0.76 (838.5 examples/sec; 0.153 sec/batch)
2017-05-09 01:57:19.524372: step 36750, loss = 0.75 (820.6 examples/sec; 0.156 sec/batch)
2017-05-09 01:57:21.078539: step 36760, loss = 0.82 (823.6 examples/sec; 0.155 sec/batch)
2017-05-09 01:57:22.641824: step 36770, loss = 0.69 (818.8 examples/sec; 0.156 sec/batch)
2017-05-09 01:57:24.162913: step 36780, loss = 0.79 (841.5 examples/sec; 0.152 sec/batch)
2017-05-09 01:57:25.749767: step 36790, loss = 1.05 (806.6 examples/sec; 0.159 sec/batch)
2017-05-09 01:57:27.414832: step 36800, loss = 0.93 (768.7 examples/sec; 0.167 sec/batch)
2017-05-09 01:57:28.846836: step 36810, loss = 0.84 (893.9 examples/sec; 0.143 sec/batch)
2017-05-09 01:57:30.380107: step 36820, loss = 0.81 (834.8 examples/sec; 0.153 sec/batch)
2017-05-09 01:57:31.912040: step 36830, loss = 0.67 (835.5 examples/sec; 0.153 sec/batch)
2017-05-09 01:57:3E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 753 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
3.462130: step 36840, loss = 0.77 (825.8 examples/sec; 0.155 sec/batch)
2017-05-09 01:57:35.038523: step 36850, loss = 0.92 (812.0 examples/sec; 0.158 sec/batch)
2017-05-09 01:57:36.580808: step 36860, loss = 0.71 (829.9 examples/sec; 0.154 sec/batch)
2017-05-09 01:57:38.130441: step 36870, loss = 0.83 (826.0 examples/sec; 0.155 sec/batch)
2017-05-09 01:57:39.646660: step 36880, loss = 0.75 (844.2 examples/sec; 0.152 sec/batch)
2017-05-09 01:57:41.149259: step 36890, loss = 0.87 (851.9 examples/sec; 0.150 sec/batch)
2017-05-09 01:57:42.770652: step 36900, loss = 0.67 (789.4 examples/sec; 0.162 sec/batch)
2017-05-09 01:57:44.210936: step 36910, loss = 1.00 (888.7 examples/sec; 0.144 sec/batch)
2017-05-09 01:57:45.732095: step 36920, loss = 0.97 (841.5 examples/sec; 0.152 sec/batch)
2017-05-09 01:57:47.256258: step 36930, loss = 0.86 (839.8 examples/sec; 0.152 sec/batch)
2017-05-09 01:57:48.767962: step 36940, loss = 0.82 (846.7 examples/sec; 0.151 sec/batch)
2017-05-09 01:57:50.286307: step 36950, loss = 0.72 (843.0 examples/sec; 0.152 sec/batch)
2017-05-09 01:57:51.825435: step 36960, loss = 0.81 (831.6 examples/sec; 0.154 sec/batch)
2017-05-09 01:57:53.328850: step 36970, loss = 0.81 (851.4 examples/sec; 0.150 sec/batch)
2017-05-09 01:57:54.837249: step 36980, loss = 1.04 (848.6 examples/sec; 0.151 sec/batch)
2017-05-09 01:57:56.338364: step 36990, loss = 1.00 (852.7 examples/sec; 0.150 sec/batch)
2017-05-09 01:57:57.940045: step 37000, loss = 0.74 (799.2 examples/sec; 0.160 sec/batch)
2017-05-09 01:57:59.343126: step 37010, loss = 0.84 (912.3 examples/sec; 0.140 sec/batch)
2017-05-09 01:58:00.860047: step 37020, loss = 0.95 (843.8 examples/sec; 0.152 sec/batch)
2017-05-09 01:58:02.405945: step 37030, loss = 0.89 (828.0 examples/sec; 0.155 sec/batch)
2017-05-09 01:58:03.949947: step 37040, loss = 0.75 (829.0 examples/sec; 0.154 sec/batch)
2017-05-09 01:58:05.508887: step 37050, loss = 0.71 (821.1 examples/sec; 0.156 sec/batch)
2017-05-09 01:58:07.092237: step 37060, loss = 0.92 (808.4 examples/sec; 0.158 sec/batch)
2017-05-09 01:58:08.625425: step 37070, loss = 1.16 (834.9 examples/sec; 0.153 sec/batch)
2017-05-09 01:58:10.159371: step 37080, loss = 0.87 (834.5 examples/sec; 0.153 sec/batch)
2017-05-09 01:58:11.705874: step 37090, loss = 0.73 (827.6 examples/sec; 0.155 sec/batch)
2017-05-09 01:58:13.338577: step 37100, loss = 0.84 (784.0 examples/sec; 0.163 sec/batch)
2017-05-09 01:58:14.826204: step 37110, loss = 0.81 (860.4 examples/sec; 0.149 sec/batch)
2017-05-09 01:58:16.373542: step 37120, loss = 0.86 (827.2 examples/sec; 0.155 sec/batch)
2017-05-09 01:58:17.929574: step 37130, loss = 0.73 (822.6 examples/sec; 0.156 sec/batch)
2017-05-09 01:58:19.476370: step 37140, loss = 0.74 (827.5 examples/sec; 0.155 sec/batch)
2017-05-09 01:58:21.013451: step 37150, loss = 0.89 (832.7 examples/sec; 0.154 sec/batch)
2017-05-09 01:58:22.579586: step 37160, loss = 0.81 (817.3 examples/sec; 0.157 sec/batch)
2017-05-09 01:58:24.136407: step 37170, loss = 0.88 (822.2 examples/sec; 0.156 sec/batch)
2017-05-09 01:58:25.676908: step 37180, loss = 0.94 (830.9 examples/sec; 0.154 sec/batch)
2017-05-09 01:58:27.191510: step 37190, loss = 0.98 (845.1 examples/sec; 0.151 sec/batch)
2017-05-09 01:58:28.909045: step 37200, loss = 0.76 (745.3 examples/sec; 0.172 sec/batch)
2017-05-09 01:58:30.347823: step 37210, loss = 0.88 (889.6 examples/sec; 0.144 sec/batch)
2017-05-09 01:58:31.899205: step 37220, loss = 0.83 (825.1 examples/sec; 0.155 sec/batch)
2017-05-09 01:58:33.441689: step 37230, loss = 0.78 (829.8 examples/sec; 0.154 sec/batch)
2017-05-09 01:58:34.980650: step 37240, loss = 0.82 (831.7 examples/sec; 0.154 sec/batch)
2017-05-09 01:58:36.509883: step 37250, loss = 0.69 (837.0 examples/sec; 0.153 sec/batch)
2017-05-09 01:58:38.028293: step 37260, loss = 0.71 (843.0 examples/sec; 0.152 sec/batch)
2017-05-09 01:58:39.563181: step 37270, loss = 0.66 (833.9 examples/sec; 0.153 sec/batch)
2017-05-09 01:58:41.137151: step 37280, loss = 0.76 (813.2 examples/sec; 0.157 sec/batch)
2017-05-09 01:58:42.704446: step 37290, loss = 0.88 (816.7 examples/sec; 0.157 sec/batch)
2017-05-09 01:58:44.348010: step 37300, loss = 0.68 (778.8 examples/sec; 0.164 sec/batch)
2017-05-09 01:58:45.792110: step 37310, loss = 1.21 (886.4 examples/sec; 0.144 sec/batch)
2017-05-09 01:58:47.344958: step 37320, loss = 0.81 (824.3 examples/sec; 0.155 sec/batch)
2017-05-09 01:58:48.874073: step 37330, loss = 0.78 (837.1 examples/sec; 0.153 sec/batch)
2017-05-09 01:58:50.412134: step 37340, loss = 0.92 (832.2 examples/sec; 0.154 sec/batch)
2017-05-09 01:58:51.959512: step 37350, loss = 1.20 (827.2 examples/sec; 0.155 sec/batch)
2017-05-09 01:58:53.560251: step 37360, loss = 1.01 (799.6 examples/sec; 0.160 sec/batch)
2017-05-09 01:58:55.154204: step 37370, loss = 0.95 (803.0 examples/sec; 0.159 sec/batch)
2017-05-09 01:58:56.712702: step 37380, loss = 0.68 (821.3 examples/sec; 0.156 sec/batch)
2017-05-09 01:58:58.244258: step 37390, loss = 0.92 (835.8 examples/sec; 0.153 sec/batch)
2017-05-09 01:58:59.851071: step 37400, loss = 0.91 (796.6 examples/sec; 0.161 sec/batch)
2017-05-09 01:59:01.280424: step 37410, loss = 1.02 (895.5 examples/sec; 0.143 sec/batch)
2017-05-09 01:59:02.814571: step 37420, loss = 0.88 (834.3 examples/sec; 0.153 sec/batch)
2017-05-09 01:59:04.378049: step 37430, loss = 0.87 (818.7 examples/sec; 0.156 sec/batch)
2017-05-09 01:59:05.912282: step 37440, loss = 0.80 (834.3 examples/sec; 0.153 sec/batch)
2017-05-09 01:59:07.439530: step 37450, loss = 1.15 (838.1 examples/sec; 0.153 sec/batch)
2017-05-09 01:59:09.013210: step 37460, loss = 0.76 (813.4 examples/sec; 0.157 sec/batch)
2017-05-09 01:59:10.555529: step 37470, loss = 0.83 (829.9 examples/sec; 0.154 sec/batch)
2017-05-09 01:59:12.089841: step 37480, loss = 0.87 (834.2 examples/sec; 0.153 sec/batch)
2017-05-09 01:59:13.609583: step 37490, loss = 0.78 (842.2 examples/sec; 0.152 sec/batch)
2017-05-09 01:59:15.251028: step 37500, loss = 0.76 (779.8 examples/sec; 0.164 sec/batch)
2017-05-09 01:59:16.689860: step 37510, loss = 0.77 (889.6 examples/sec; 0.144 sec/batch)
2017-05-09 01:59:18.210473: step 37520, loss = 0.82 (841.8 examples/sec; 0.152 sec/batch)
2017-05-09 01:59:19.756618: step 37530, loss = 1.07 (827.9 examples/sec; 0.155 sec/batch)
2017-05-09 01:59:21.284548: step 37540, loss = 0.76 (837.7 examples/sec; 0.153 sec/batch)
2017-05-09 01:59:22.832267: step 37550, loss = 0.94 (827.0 examples/sec; 0.155 sec/batch)
2017-05-09 01:59:24.356737: step 37560, loss = 0.76 (839.6 examples/sec; 0.152 sec/batch)
2017-05-09 01:59:25.883006: step 37570, loss = 0.87 (838.6 examples/sec; 0.153 sec/batch)
2017-05-09 01:59:27.403549: step 37580, loss = 0.85 (841.8 examples/sec; 0.152 sec/batch)
2017-05-09 01:59:28.951372: step 37590, loss = 0.68 (827.0 examples/sec; 0.155 sec/batch)
2017-05-09 01:59:30.605354: step 37600, loss = 0.78 (773.9 examples/sec; 0.165 sec/batch)
2017-05-09 01:59:32.055897: step 37610, loss = 0.81 (882.4 examples/sec; 0.145 sec/batch)
2017-05-09 01:59:33.588509: step 37620, loss = 0.73 (835.2 examples/sec; 0.153 sec/batch)
2017-05-09 01:59:35.120594: step 37630, loss = 0.84 (835.5 examples/sec; 0.153 sec/batch)
2017-05-09 01:59:36.638426: step 37640, loss = 0.80 (843.3 examples/sec; 0.152 sec/batch)
2017-05-09 01:59:38.180563: step 37650, loss = 0.78 (830.0 examples/sec; 0.154 sec/batch)
2017-05-09 01:59:39.721124: step 37660, loss = 0.72 (830.9 examples/sec; 0.154 sec/batch)
2017-05-09 01:59:41.248849: step 37670, loss = 0.82 (837.8 examples/sec; 0.153 sec/batch)
2017-05-09 01:59:42.781611: step 37680, loss = 0.82 (835.1 examples/sec; 0.153 sec/batch)
2017-05-09 01:59:44.313059: step 37690, loss = 0.65 (835.8 examples/sec; 0.153 sec/batch)
2017-05-09 01:59:45.943516: step 37700, loss = 0.78 (785.1 examples/sec; 0.163 sec/batch)
2017-05-09 01:59:47.428154: step 37710, loss = 0.73 (862.2 examples/sec; 0.148 sec/batch)
2017-05-09 01:59:49.009909: step 37720, loss = 0.95 (809.2 examples/sec; 0.158 sec/batch)
2017-05-09 01:59:50.531365: step 37730, loss = 0.82 (841.3 examples/sec; 0.152 sec/batch)
2017-05-09 01:59:52.074994: step 37740, loss = 0.73 (829.2 examples/sec; 0.154 sec/batch)
2017-05-09 01:59:53.E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 769 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
636197: step 37750, loss = 0.78 (819.9 examples/sec; 0.156 sec/batch)
2017-05-09 01:59:55.190088: step 37760, loss = 0.84 (823.7 examples/sec; 0.155 sec/batch)
2017-05-09 01:59:56.755192: step 37770, loss = 1.01 (817.8 examples/sec; 0.157 sec/batch)
2017-05-09 01:59:58.273735: step 37780, loss = 0.80 (842.9 examples/sec; 0.152 sec/batch)
2017-05-09 01:59:59.812220: step 37790, loss = 0.77 (832.0 examples/sec; 0.154 sec/batch)
2017-05-09 02:00:01.425161: step 37800, loss = 0.74 (793.6 examples/sec; 0.161 sec/batch)
2017-05-09 02:00:02.906539: step 37810, loss = 0.85 (864.1 examples/sec; 0.148 sec/batch)
2017-05-09 02:00:04.432492: step 37820, loss = 0.68 (838.8 examples/sec; 0.153 sec/batch)
2017-05-09 02:00:05.996568: step 37830, loss = 0.83 (818.4 examples/sec; 0.156 sec/batch)
2017-05-09 02:00:07.531844: step 37840, loss = 0.91 (833.7 examples/sec; 0.154 sec/batch)
2017-05-09 02:00:09.106179: step 37850, loss = 0.79 (813.0 examples/sec; 0.157 sec/batch)
2017-05-09 02:00:10.643021: step 37860, loss = 0.82 (832.9 examples/sec; 0.154 sec/batch)
2017-05-09 02:00:12.171378: step 37870, loss = 0.82 (837.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:00:13.704604: step 37880, loss = 1.18 (834.8 examples/sec; 0.153 sec/batch)
2017-05-09 02:00:15.240419: step 37890, loss = 0.98 (833.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:00:16.889664: step 37900, loss = 0.77 (776.1 examples/sec; 0.165 sec/batch)
2017-05-09 02:00:18.338676: step 37910, loss = 0.73 (883.4 examples/sec; 0.145 sec/batch)
2017-05-09 02:00:19.878293: step 37920, loss = 0.99 (831.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:00:21.455487: step 37930, loss = 0.95 (811.6 examples/sec; 0.158 sec/batch)
2017-05-09 02:00:23.000066: step 37940, loss = 0.86 (828.7 examples/sec; 0.154 sec/batch)
2017-05-09 02:00:24.510717: step 37950, loss = 0.75 (847.3 examples/sec; 0.151 sec/batch)
2017-05-09 02:00:26.074229: step 37960, loss = 0.73 (818.7 examples/sec; 0.156 sec/batch)
2017-05-09 02:00:27.623708: step 37970, loss = 0.86 (826.1 examples/sec; 0.155 sec/batch)
2017-05-09 02:00:29.180009: step 37980, loss = 0.71 (822.5 examples/sec; 0.156 sec/batch)
2017-05-09 02:00:30.705756: step 37990, loss = 0.81 (838.9 examples/sec; 0.153 sec/batch)
2017-05-09 02:00:32.349456: step 38000, loss = 0.77 (778.7 examples/sec; 0.164 sec/batch)
2017-05-09 02:00:33.807621: step 38010, loss = 0.66 (877.8 examples/sec; 0.146 sec/batch)
2017-05-09 02:00:35.389362: step 38020, loss = 0.94 (809.3 examples/sec; 0.158 sec/batch)
2017-05-09 02:00:36.956779: step 38030, loss = 0.68 (816.6 examples/sec; 0.157 sec/batch)
2017-05-09 02:00:38.509184: step 38040, loss = 0.64 (824.5 examples/sec; 0.155 sec/batch)
2017-05-09 02:00:40.052115: step 38050, loss = 0.88 (829.6 examples/sec; 0.154 sec/batch)
2017-05-09 02:00:41.621660: step 38060, loss = 0.91 (815.5 examples/sec; 0.157 sec/batch)
2017-05-09 02:00:43.163009: step 38070, loss = 0.78 (830.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:00:44.690751: step 38080, loss = 0.85 (837.8 examples/sec; 0.153 sec/batch)
2017-05-09 02:00:46.228896: step 38090, loss = 0.76 (832.2 examples/sec; 0.154 sec/batch)
2017-05-09 02:00:47.846280: step 38100, loss = 0.86 (791.4 examples/sec; 0.162 sec/batch)
2017-05-09 02:00:49.286510: step 38110, loss = 0.86 (888.7 examples/sec; 0.144 sec/batch)
2017-05-09 02:00:50.816492: step 38120, loss = 0.85 (836.6 examples/sec; 0.153 sec/batch)
2017-05-09 02:00:52.324514: step 38130, loss = 0.88 (848.8 examples/sec; 0.151 sec/batch)
2017-05-09 02:00:53.843021: step 38140, loss = 0.84 (842.9 examples/sec; 0.152 sec/batch)
2017-05-09 02:00:55.360069: step 38150, loss = 0.79 (843.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:00:56.911779: step 38160, loss = 0.82 (824.9 examples/sec; 0.155 sec/batch)
2017-05-09 02:00:58.485639: step 38170, loss = 0.65 (813.3 examples/sec; 0.157 sec/batch)
2017-05-09 02:01:00.038115: step 38180, loss = 1.07 (824.5 examples/sec; 0.155 sec/batch)
2017-05-09 02:01:01.566889: step 38190, loss = 0.83 (837.3 examples/sec; 0.153 sec/batch)
2017-05-09 02:01:03.238942: step 38200, loss = 0.82 (765.5 exampleE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 785 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
s/sec; 0.167 sec/batch)
2017-05-09 02:01:04.667474: step 38210, loss = 0.71 (896.0 examples/sec; 0.143 sec/batch)
2017-05-09 02:01:06.186408: step 38220, loss = 1.06 (842.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:01:07.761428: step 38230, loss = 0.92 (812.7 examples/sec; 0.158 sec/batch)
2017-05-09 02:01:09.296260: step 38240, loss = 0.82 (834.0 examples/sec; 0.153 sec/batch)
2017-05-09 02:01:10.862653: step 38250, loss = 0.87 (817.2 examples/sec; 0.157 sec/batch)
2017-05-09 02:01:12.444096: step 38260, loss = 0.79 (809.4 examples/sec; 0.158 sec/batch)
2017-05-09 02:01:14.000614: step 38270, loss = 0.73 (822.4 examples/sec; 0.156 sec/batch)
2017-05-09 02:01:15.514955: step 38280, loss = 0.74 (845.3 examples/sec; 0.151 sec/batch)
2017-05-09 02:01:17.039563: step 38290, loss = 0.72 (839.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:01:18.682025: step 38300, loss = 0.73 (779.3 examples/sec; 0.164 sec/batch)
2017-05-09 02:01:20.144639: step 38310, loss = 0.83 (875.1 examples/sec; 0.146 sec/batch)
2017-05-09 02:01:21.682540: step 38320, loss = 0.76 (832.3 examples/sec; 0.154 sec/batch)
2017-05-09 02:01:23.226729: step 38330, loss = 0.84 (828.9 examples/sec; 0.154 sec/batch)
2017-05-09 02:01:24.764595: step 38340, loss = 0.91 (832.3 examples/sec; 0.154 sec/batch)
2017-05-09 02:01:26.317631: step 38350, loss = 0.70 (824.2 examples/sec; 0.155 sec/batch)
2017-05-09 02:01:27.834991: step 38360, loss = 0.83 (843.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:01:29.451423: step 38370, loss = 0.89 (791.9 examples/sec; 0.162 sec/batch)
2017-05-09 02:01:30.958770: step 38380, loss = 0.86 (849.2 examples/sec; 0.151 sec/batch)
2017-05-09 02:01:32.474642: step 38390, loss = 0.85 (844.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:01:34.106657: step 38400, loss = 0.91 (784.3 examples/sec; 0.163 sec/batch)
2017-05-09 02:01:35.560827: step 38410, loss = 0.84 (880.2 examples/sec; 0.145 sec/batch)
2017-05-09 02:01:37.130578: step 38420, loss = 0.81 (815.4 examples/sec; 0.157 sec/batch)
2017-05-09 02:01:38.672267: step 38430, loss = 0.98 (830.3 examples/sec; 0.154 sec/batch)
2017-05-09 02:01:40.213693: step 38440, loss = 0.69 (830.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:01:41.739161: step 38450, loss = 0.79 (839.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:01:43.261567: step 38460, loss = 0.71 (840.8 examples/sec; 0.152 sec/batch)
2017-05-09 02:01:44.854606: step 38470, loss = 0.71 (803.5 examples/sec; 0.159 sec/batch)
2017-05-09 02:01:46.382008: step 38480, loss = 0.95 (838.0 examples/sec; 0.153 sec/batch)
2017-05-09 02:01:47.950063: step 38490, loss = 0.83 (816.3 examples/sec; 0.157 sec/batch)
2017-05-09 02:01:49.579567: step 38500, loss = 0.89 (785.5 examples/sec; 0.163 sec/batch)
2017-05-09 02:01:51.026551: step 38510, loss = 0.67 (884.6 examples/sec; 0.145 sec/batch)
2017-05-09 02:01:52.615374: step 38520, loss = 0.97 (805.6 examples/sec; 0.159 sec/batch)
2017-05-09 02:01:54.173080: step 38530, loss = 0.77 (821.7 examples/sec; 0.156 sec/batch)
2017-05-09 02:01:55.712776: step 38540, loss = 0.91 (831.3 examples/sec; 0.154 sec/batch)
2017-05-09 02:01:57.199648: step 38550, loss = 0.70 (860.9 examples/sec; 0.149 sec/batch)
2017-05-09 02:01:58.714494: step 38560, loss = 0.69 (845.0 examples/sec; 0.151 sec/batch)
2017-05-09 02:02:00.190677: step 38570, loss = 0.98 (867.1 examples/sec; 0.148 sec/batch)
2017-05-09 02:02:01.729485: step 38580, loss = 1.14 (831.8 examples/sec; 0.154 sec/batch)
2017-05-09 02:02:03.243476: step 38590, loss = 0.74 (845.4 examples/sec; 0.151 sec/batch)
2017-05-09 02:02:04.810761: step 38600, loss = 0.72 (816.7 examples/sec; 0.157 sec/batch)
2017-05-09 02:02:06.212636: step 38610, loss = 0.83 (913.1 examples/sec; 0.140 sec/batch)
2017-05-09 02:02:07.729768: step 38620, loss = 0.77 (843.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:02:09.228438: step 38630, loss = 0.79 (854.1 examples/sec; 0.150 sec/batch)
2017-05-09 02:02:10.718770: step 38640, loss = 0.94 (858.9 examples/sec; 0.149 sec/batch)
2017-05-09 02:02:12.231773: step 38650, loss = 0.73 (846.0 examples/sec; 0.151 sec/batch)
2017-05-09 02:02:13.736394: step 38660, loss = 0.82 (850.7 examples/sec; 0.150 sec/batch)
2017-05-09 02:02:15.252222: step 38670, loss = 0.76 (844.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:02:16.760015: step 38680, loss = 0.77 (848.9 examples/sec; 0.151 sec/batch)
2017-05-09 02:02:18.257155: step 38690, loss = 0.86 (855.0 examples/sec; 0.150 sec/batch)
2017-05-09 02:02:19.830130: step 38700, loss = 0.69 (813.7 examples/sec; 0.157 sec/batch)
2017-05-09 02:02:21.253890: step 38710, loss = 0.65 (899.0 examples/sec; 0.142 sec/batch)
2017-05-09 02:02:22.745265: step 38720, loss = 0.79 (858.3 examples/sec; 0.149 sec/batch)
2017-05-09 02:02:24.244926: step 38730, loss = 0.76 (853.5 examples/sec; 0.150 sec/batch)
2017-05-09 02:02:25.743220: step 38740, loss = 0.93 (854.3 examples/sec; 0.150 sec/batch)
2017-05-09 02:02:27.282708: step 38750, loss = 0.91 (831.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:02:28.801470: step 38760, loss = 0.73 (842.8 examples/sec; 0.152 sec/batch)
2017-05-09 02:02:30.301661: step 38770, loss = 0.87 (853.2 examples/sec; 0.150 sec/batch)
2017-05-09 02:02:31.800535: step 38780, loss = 0.81 (854.0 examples/sec; 0.150 sec/batch)
2017-05-09 02:02:33.299543: step 38790, loss = 0.92 (853.9 examples/sec; 0.150 sec/batch)
2017-05-09 02:02:34.906497: step 38800, loss = 1.02 (796.5 examples/sec; 0.161 sec/batch)
2017-05-09 02:02:36.302140: step 38810, loss = 0.86 (917.1 examples/sec; 0.140 sec/batch)
2017-05-09 02:02:37.794120: step 38820, loss = 0.73 (857.9 examples/sec; 0.149 sec/batch)
2017-05-09 02:02:39.289253: step 38830, loss = 0.97 (856.1 examples/sec; 0.150 sec/batch)
2017-05-09 02:02:40.791034: step 38840, loss = 0.81 (852.3 examples/sec; 0.150 sec/batch)
2017-05-09 02:02:42.276514: step 38850, loss = 0.83 (861.7 examples/sec; 0.149 sec/batch)
2017-05-09 02:02:43.780057: step 38860, loss = 0.73 (851.3 examples/sec; 0.150 sec/batch)
2017-05-09 02:02:45.292976: step 38870, loss = 0.73 (846.0 examples/sec; 0.151 sec/batch)
2017-05-09 02:02:46.776105: step 38880, loss = 0.88 (863.0 examples/sec; 0.148 sec/batch)
2017-05-09 02:02:48.271657: step 38890, loss = 0.83 (855.9 examples/sec; 0.150 sec/batch)
2017-05-09 02:02:49.857194: step 38900, loss = 0.63 (807.3 examples/sec; 0.159 sec/batch)
2017-05-09 02:02:51.270819: step 38910, loss = 0.73 (905.5 examples/sec; 0.141 sec/batch)
2017-05-09 02:02:52.774043: step 38920, loss = 0.89 (851.5 examples/sec; 0.150 sec/batch)
2017-05-09 02:02:54.293208: step 38930, loss = 0.85 (842.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:02:55.817598: step 38940, loss = 0.80 (839.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:02:57.325662: step 38950, loss = 0.80 (848.8 examples/sec; 0.151 sec/batch)
2017-05-09 02:02:58.804143: step 38960, loss = 0.81 (865.8 examples/sec; 0.148 sec/batch)
2017-05-09 02:03:00.290311: step 38970, loss = 0.81 (861.3 examples/sec; 0.149 sec/batch)
2017-05-09 02:03:01.788622: step 38980, loss = 0.75 (854.3 examples/sec; 0.150 sec/batch)
2017-05-09 02:03:03.294992: step 38990, loss = 0.82 (849.7 examples/sec; 0.151 sec/batch)
2017-05-09 02:03:04.874448: step 39000, loss = 0.88 (810.4 examples/sec; 0.158 sec/batch)
2017-05-09 02:03:06.277939: step 39010, loss = 0.92 (912.0 examples/sec; 0.140 sec/batch)
2017-05-09 02:03:07.795345: step 39020, loss = 0.88 (843.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:03:09.309736: step 39030, loss = 0.72 (845.2 examples/sec; 0.151 sec/batch)
2017-05-09 02:03:10.791175: step 39040, loss = 0.80 (864.0 examples/sec; 0.148 sec/batch)
2017-05-09 02:03:12.292688: step 39050, loss = 0.71 (852.5 examples/sec; 0.150 sec/batch)
2017-05-09 02:03:13.771456: step 39060, loss = 0.65 (865.6 examples/sec; 0.148 sec/batch)
2017-05-09 02:03:15.259881: step 39070, loss = 0.71 (860.0 examples/sec; 0.149 sec/batch)
2017-05-09 02:03:16.771064: step 39080, loss = 0.82 (847.0 examples/sec; 0.151 sec/batch)
2017-05-09 02:03:18.288128: step 39090, loss = 0.74 (843.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:03:19.859184: step 39100, loss = 0.82 (814.7 examples/sec; 0.157 sec/batch)
2017-05-09 02:03:21.278250: step 39110, loss = 0.78 (902.0 examples/E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 801 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
sec; 0.142 sec/batch)
2017-05-09 02:03:22.755983: step 39120, loss = 0.75 (866.2 examples/sec; 0.148 sec/batch)
2017-05-09 02:03:24.261538: step 39130, loss = 0.75 (850.2 examples/sec; 0.151 sec/batch)
2017-05-09 02:03:25.758621: step 39140, loss = 0.79 (855.0 examples/sec; 0.150 sec/batch)
2017-05-09 02:03:27.267542: step 39150, loss = 0.76 (848.3 examples/sec; 0.151 sec/batch)
2017-05-09 02:03:28.835224: step 39160, loss = 0.78 (816.5 examples/sec; 0.157 sec/batch)
2017-05-09 02:03:30.392818: step 39170, loss = 0.96 (821.8 examples/sec; 0.156 sec/batch)
2017-05-09 02:03:31.938677: step 39180, loss = 0.70 (828.0 examples/sec; 0.155 sec/batch)
2017-05-09 02:03:33.469927: step 39190, loss = 0.91 (835.9 examples/sec; 0.153 sec/batch)
2017-05-09 02:03:35.078451: step 39200, loss = 0.84 (795.8 examples/sec; 0.161 sec/batch)
2017-05-09 02:03:36.516703: step 39210, loss = 0.82 (890.0 examples/sec; 0.144 sec/batch)
2017-05-09 02:03:38.038976: step 39220, loss = 0.81 (840.8 examples/sec; 0.152 sec/batch)
2017-05-09 02:03:39.600672: step 39230, loss = 0.73 (819.6 examples/sec; 0.156 sec/batch)
2017-05-09 02:03:41.143608: step 39240, loss = 0.83 (829.6 examples/sec; 0.154 sec/batch)
2017-05-09 02:03:42.697928: step 39250, loss = 0.76 (823.5 examples/sec; 0.155 sec/batch)
2017-05-09 02:03:44.206697: step 39260, loss = 0.85 (848.4 examples/sec; 0.151 sec/batch)
2017-05-09 02:03:45.768080: step 39270, loss = 0.90 (819.8 examples/sec; 0.156 sec/batch)
2017-05-09 02:03:47.368921: step 39280, loss = 0.73 (799.6 examples/sec; 0.160 sec/batch)
2017-05-09 02:03:48.922912: step 39290, loss = 0.73 (823.7 examples/sec; 0.155 sec/batch)
2017-05-09 02:03:50.575443: step 39300, loss = 0.74 (774.6 examples/sec; 0.165 sec/batch)
2017-05-09 02:03:52.021009: step 39310, loss = 1.00 (885.5 examples/sec; 0.145 sec/batch)
2017-05-09 02:03:53.585114: step 39320, loss = 0.83 (818.4 examples/sec; 0.156 sec/batch)
2017-05-09 02:03:55.122241: step 39330, loss = 0.86 (832.7 examples/sec; 0.154 sec/batch)
2017-05-09 02:03:56.702173: step 39340, loss = 0.92 (810.2 examples/sec; 0.158 sec/batch)
2017-05-09 02:03:58.251922: step 39350, loss = 0.81 (825.9 examples/sec; 0.155 sec/batch)
2017-05-09 02:03:59.822265: step 39360, loss = 1.07 (815.1 examples/sec; 0.157 sec/batch)
2017-05-09 02:04:01.362441: step 39370, loss = 0.80 (831.1 examples/sec; 0.154 sec/batch)
2017-05-09 02:04:02.907285: step 39380, loss = 0.70 (828.6 examples/sec; 0.154 sec/batch)
2017-05-09 02:04:04.485291: step 39390, loss = 0.90 (811.1 examples/sec; 0.158 sec/batch)
2017-05-09 02:04:06.090295: step 39400, loss = 0.82 (797.5 examples/sec; 0.160 sec/batch)
2017-05-09 02:04:07.521900: step 39410, loss = 0.72 (894.1 examples/sec; 0.143 sec/batch)
2017-05-09 02:04:09.068988: step 39420, loss = 1.02 (827.4 examples/sec; 0.155 sec/batch)
2017-05-09 02:04:10.639632: step 39430, loss = 0.93 (814.9 examples/sec; 0.157 sec/batch)
2017-05-09 02:04:12.198914: step 39440, loss = 0.99 (820.9 examples/sec; 0.156 sec/batch)
2017-05-09 02:04:13.739731: step 39450, loss = 0.87 (830.7 examples/sec; 0.154 sec/batch)
2017-05-09 02:04:15.263562: step 39460, loss = 0.91 (840.0 examples/sec; 0.152 sec/batch)
2017-05-09 02:04:16.815038: step 39470, loss = 0.77 (825.0 examples/sec; 0.155 sec/batch)
2017-05-09 02:04:18.352253: step 39480, loss = 0.74 (832.7 examples/sec; 0.154 sec/batch)
2017-05-09 02:04:19.864838: step 39490, loss = 0.78 (846.2 examples/sec; 0.151 sec/batch)
2017-05-09 02:04:21.473644: step 39500, loss = 0.83 (795.6 examples/sec; 0.161 sec/batch)
2017-05-09 02:04:22.875439: step 39510, loss = 1.00 (913.1 examples/sec; 0.140 sec/batch)
2017-05-09 02:04:24.395862: step 39520, loss = 0.77 (841.9 examples/sec; 0.152 sec/batch)
2017-05-09 02:04:25.936160: step 39530, loss = 0.76 (831.0 examples/sec; 0.154 sec/batch)
2017-05-09 02:04:27.480754: step 39540, loss = 0.94 (828.7 examples/sec; 0.154 sec/batch)
2017-05-09 02:04:29.037529: step 39550, loss = 0.84 (822.2 examples/sec; 0.156 sec/batch)
2017-05-09 02:04:30.574794: step 39560, loss = 0.84 (832.6 examples/sec; 0.154 sec/batch)
2017-05-09 02:04:32.141189: step 39570, loss = 0.84 (817.2 examples/sec; 0.157 sec/batch)
2017-05-09 02:04:33.665191: step 39580, loss = 0.93 (839.9 examples/sec; 0.152 sec/batch)
2017-05-09 02:04:35.207488: step 39590, loss = 0.82 (829.9 examples/sec; 0.154 sec/batch)
2017-05-09 02:04:36.845144: step 39600, loss = 0.72 (781.6 examples/sec; 0.164 sec/batch)
2017-05-09 02:04:38.289378: step 39610, loss = 0.86 (886.3 examples/sec; 0.144 sec/batch)
2017-05-09 02:04:39.784880: step 39620, loss = 0.86 (855.9 examples/sec; 0.150 sec/batch)
2017-05-09 02:04:41.295767: step 39630, loss = 0.74 (847.2 examples/sec; 0.151 sec/batch)
2017-05-09 02:04:42.824407: step 39640, loss = 0.70 (837.4 examples/sec; 0.153 sec/batch)
2017-05-09 02:04:44.357006: step 39650, loss = 0.89 (835.2 examples/sec; 0.153 sec/batch)
2017-05-09 02:04:45.873468: step 39660, loss = 0.75 (844.1 examples/sec; 0.152 sec/batch)
2017-05-09 02:04:47.428703: step 39670, loss = 0.75 (823.0 examples/sec; 0.156 sec/batch)
2017-05-09 02:04:48.946099: step 39680, loss = 0.68 (843.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:04:50.463145: step 39690, loss = 0.93 (843.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:04:52.102657: step 39700, loss = 0.76 (780.7 examples/sec; 0.164 sec/batch)
2017-05-09 02:04:53.552439: step 39710, loss = 0.89 (882.9 examples/sec; 0.145 sec/batch)
2017-05-09 02:04:55.103612: step 39720, loss = 0.98 (825.2 examples/sec; 0.155 sec/batch)
2017-05-09 02:04:56.626956: step 39730, loss = 0.89 (840.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:04:58.150373: step 39740, loss = 0.79 (840.2 examples/sec; 0.152 sec/batch)
2017-05-09 02:04:59.667367: step 39750, loss = 0.93 (843.8 examples/sec; 0.152 sec/batch)
2017-05-09 02:05:01.198171: step 39760, loss = 0.80 (836.2 examples/sec; 0.153 sec/batch)
2017-05-09 02:05:02.759357: step 39770, loss = 0.73 (819.9 examples/sec; 0.156 sec/batch)
2017-05-09 02:05:04.311088: step 39780, loss = 0.66 (824.9 examples/sec; 0.155 sec/batch)
2017-05-09 02:05:05.825602: step 39790, loss = 0.69 (845.2 examples/sec; 0.151 sec/batch)
2017-05-09 02:05:07.436789: step 39800, loss = 0.79 (794.4 examples/sec; 0.161 sec/batch)
2017-05-09 02:05:08.886108: step 39810, loss = 0.80 (883.2 examples/sec; 0.145 sec/batch)
2017-05-09 02:05:10.417007: step 39820, loss = 0.74 (836.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:05:11.964092: step 39830, loss = 0.67 (827.4 examples/sec; 0.155 sec/batch)
2017-05-09 02:05:13.475364: step 39840, loss = 0.95 (847.0 examples/sec; 0.151 sec/batch)
2017-05-09 02:05:15.015575: step 39850, loss = 0.89 (831.1 examples/sec; 0.154 sec/batch)
2017-05-09 02:05:16.541960: step 39860, loss = 0.85 (838.6 examples/sec; 0.153 sec/batch)
2017-05-09 02:05:18.114050: step 39870, loss = 0.79 (814.2 examples/sec; 0.157 sec/batch)
2017-05-09 02:05:19.670499: step 39880, loss = 0.80 (822.4 examples/sec; 0.156 sec/batch)
2017-05-09 02:05:21.215781: step 39890, loss = 0.73 (828.3 examples/sec; 0.155 sec/batch)
2017-05-09 02:05:22.851459: step 39900, loss = 1.01 (782.5 examples/sec; 0.164 sec/batch)
2017-05-09 02:05:24.275705: step 39910, loss = 0.94 (898.7 examples/sec; 0.142 sec/batch)
2017-05-09 02:05:25.802670: step 39920, loss = 0.75 (838.3 examples/sec; 0.153 sec/batch)
2017-05-09 02:05:27.353839: step 39930, loss = 0.94 (825.2 examples/sec; 0.155 sec/batch)
2017-05-09 02:05:28.887814: step 39940, loss = 0.90 (834.4 examples/sec; 0.153 sec/batch)
2017-05-09 02:05:30.425682: step 39950, loss = 0.94 (832.3 examples/sec; 0.154 sec/batch)
2017-05-09 02:05:31.990275: step 39960, loss = 0.73 (818.1 examples/sec; 0.156 sec/batch)
2017-05-09 02:05:33.545056: step 39970, loss = 0.83 (823.3 examples/sec; 0.155 sec/batch)
2017-05-09 02:05:35.065462: step 39980, loss = 0.85 (841.9 examples/sec; 0.152 sec/batch)
2017-05-09 02:05:36.645177: step 39990, loss = 0.85 (810.3 examples/sec; 0.158 sec/batch)
2017-05-09 02:05:38.281733: step 40000, loss = 0.77 (782.1 examples/sec; 0.164 sec/batch)
2017-05-09 02:05:39.702593: step 40010, loss = 0.81 (900.9 examples/sec; 0.142 sec/batch)
2017-05-09 02:05:41.186177: step 40020, loss = 0.93 (862.8 examples/seE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 818 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
c; 0.148 sec/batch)
2017-05-09 02:05:42.735391: step 40030, loss = 0.76 (826.2 examples/sec; 0.155 sec/batch)
2017-05-09 02:05:44.253160: step 40040, loss = 0.89 (843.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:05:45.779965: step 40050, loss = 0.79 (838.3 examples/sec; 0.153 sec/batch)
2017-05-09 02:05:47.309954: step 40060, loss = 0.74 (836.6 examples/sec; 0.153 sec/batch)
2017-05-09 02:05:48.857804: step 40070, loss = 0.74 (827.0 examples/sec; 0.155 sec/batch)
2017-05-09 02:05:50.388720: step 40080, loss = 0.75 (836.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:05:51.934614: step 40090, loss = 0.80 (828.0 examples/sec; 0.155 sec/batch)
2017-05-09 02:05:53.558459: step 40100, loss = 1.06 (788.2 examples/sec; 0.162 sec/batch)
2017-05-09 02:05:55.037120: step 40110, loss = 0.78 (865.7 examples/sec; 0.148 sec/batch)
2017-05-09 02:05:56.569403: step 40120, loss = 0.82 (835.3 examples/sec; 0.153 sec/batch)
2017-05-09 02:05:58.104621: step 40130, loss = 0.62 (833.8 examples/sec; 0.154 sec/batch)
2017-05-09 02:05:59.630138: step 40140, loss = 0.80 (839.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:06:01.205367: step 40150, loss = 0.79 (812.6 examples/sec; 0.158 sec/batch)
2017-05-09 02:06:02.745831: step 40160, loss = 0.83 (830.9 examples/sec; 0.154 sec/batch)
2017-05-09 02:06:04.306796: step 40170, loss = 0.73 (820.0 examples/sec; 0.156 sec/batch)
2017-05-09 02:06:05.900484: step 40180, loss = 0.80 (803.2 examples/sec; 0.159 sec/batch)
2017-05-09 02:06:07.464095: step 40190, loss = 0.58 (818.6 examples/sec; 0.156 sec/batch)
2017-05-09 02:06:09.112570: step 40200, loss = 0.73 (776.5 examples/sec; 0.165 sec/batch)
2017-05-09 02:06:10.541045: step 40210, loss = 0.58 (896.1 examples/sec; 0.143 sec/batch)
2017-05-09 02:06:12.070649: step 40220, loss = 0.80 (836.8 examples/sec; 0.153 sec/batch)
2017-05-09 02:06:13.602953: step 40230, loss = 0.70 (835.3 examples/sec; 0.153 sec/batch)
2017-05-09 02:06:15.154079: step 40240, loss = 0.89 (825.2 examples/sec; 0.155 sec/batch)
2017-05-09 02:06:16.673117: step 40250, loss = 0.70 (842.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:06:18.198916: step 40260, loss = 0.76 (838.9 examples/sec; 0.153 sec/batch)
2017-05-09 02:06:19.780117: step 40270, loss = 0.82 (809.5 examples/sec; 0.158 sec/batch)
2017-05-09 02:06:21.311705: step 40280, loss = 0.91 (835.7 examples/sec; 0.153 sec/batch)
2017-05-09 02:06:22.877720: step 40290, loss = 0.78 (817.4 examples/sec; 0.157 sec/batch)
2017-05-09 02:06:24.513457: step 40300, loss = 0.65 (782.5 examples/sec; 0.164 sec/batch)
2017-05-09 02:06:25.969301: step 40310, loss = 0.83 (879.2 examples/sec; 0.146 sec/batch)
2017-05-09 02:06:27.505498: step 40320, loss = 0.71 (833.2 examples/sec; 0.154 sec/batch)
2017-05-09 02:06:29.009172: step 40330, loss = 0.74 (851.3 examples/sec; 0.150 sec/batch)
2017-05-09 02:06:30.564122: step 40340, loss = 0.79 (823.2 examples/sec; 0.155 sec/batch)
2017-05-09 02:06:32.095673: step 40350, loss = 0.75 (835.7 examples/sec; 0.153 sec/batch)
2017-05-09 02:06:33.630511: step 40360, loss = 0.68 (834.0 examples/sec; 0.153 sec/batch)
2017-05-09 02:06:35.158237: step 40370, loss = 0.73 (837.8 examples/sec; 0.153 sec/batch)
2017-05-09 02:06:36.716387: step 40380, loss = 0.75 (821.5 examples/sec; 0.156 sec/batch)
2017-05-09 02:06:38.246487: step 40390, loss = 0.81 (836.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:06:39.874671: step 40400, loss = 0.68 (786.2 examples/sec; 0.163 sec/batch)
2017-05-09 02:06:41.320931: step 40410, loss = 0.69 (885.0 examples/sec; 0.145 sec/batch)
2017-05-09 02:06:42.873041: step 40420, loss = 0.88 (824.7 examples/sec; 0.155 sec/batch)
2017-05-09 02:06:44.408116: step 40430, loss = 0.78 (833.8 examples/sec; 0.154 sec/batch)
2017-05-09 02:06:45.956440: step 40440, loss = 0.99 (826.7 examples/sec; 0.155 sec/batch)
2017-05-09 02:06:47.521127: step 40450, loss = 0.85 (818.1 examples/sec; 0.156 sec/batch)
2017-05-09 02:06:49.053751: step 40460, loss = 0.91 (835.2 examples/sec; 0.153 sec/batch)
2017-05-09 02:06:50.605802: step 40470, loss = 0.87 (824.7 examples/sec; 0.155 sec/batch)
2017-05-09 02:06:52.140457: step 40480, loss = 0.86 (834.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:06:53.684510: step 40490, loss = 0.86 (829.0 examples/sec; 0.154 sec/batch)
2017-05-09 02:06:55.311447: step 40500, loss = 0.96 (786.8 examples/sec; 0.163 sec/batch)
2017-05-09 02:06:56.763952: step 40510, loss = 0.73 (881.2 examples/sec; 0.145 sec/batch)
2017-05-09 02:06:58.301660: step 40520, loss = 0.71 (832.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:06:59.844800: step 40530, loss = 0.97 (829.5 examples/sec; 0.154 sec/batch)
2017-05-09 02:07:01.369466: step 40540, loss = 1.01 (839.5 examples/sec; 0.152 sec/batch)
2017-05-09 02:07:02.867880: step 40550, loss = 0.92 (854.2 examples/sec; 0.150 sec/batch)
2017-05-09 02:07:04.381076: step 40560, loss = 0.70 (845.9 examples/sec; 0.151 sec/batch)
2017-05-09 02:07:05.964247: step 40570, loss = 0.84 (808.5 examples/sec; 0.158 sec/batch)
2017-05-09 02:07:07.513166: step 40580, loss = 0.78 (826.4 examples/sec; 0.155 sec/batch)
2017-05-09 02:07:09.053044: step 40590, loss = 0.72 (831.2 examples/sec; 0.154 sec/batch)
2017-05-09 02:07:10.704637: step 40600, loss = 0.83 (775.0 examples/sec; 0.165 sec/batch)
2017-05-09 02:07:12.096879: step 40610, loss = 0.72 (919.4 examples/sec; 0.139 sec/batch)
2017-05-09 02:07:13.606088: step 40620, loss = 0.95 (848.1 examples/sec; 0.151 sec/batch)
2017-05-09 02:07:15.122545: step 40630, loss = 0.79 (844.1 examples/sec; 0.152 sec/batch)
2017-05-09 02:07:16.606424: step 40640, loss = 0.81 (862.6 examples/sec; 0.148 sec/batch)
2017-05-09 02:07:18.087235: step 40650, loss = 0.83 (864.4 examples/sec; 0.148 sec/batch)
2017-05-09 02:07:19.608838: step 40660, loss = 0.88 (841.2 examples/sec; 0.152 sec/batch)
2017-05-09 02:07:21.100684: step 40670, loss = 1.02 (858.0 examples/sec; 0.149 sec/batch)
2017-05-09 02:07:22.625745: step 40680, loss = 0.89 (839.3 examples/sec; 0.153 sec/batch)
2017-05-09 02:07:24.205219: step 40690, loss = 0.75 (810.4 examples/sec; 0.158 sec/batch)
2017-05-09 02:07:25.792191: step 40700, loss = 0.87 (806.6 examples/sec; 0.159 sec/batch)
2017-05-09 02:07:27.208892: step 40710, loss = 0.75 (903.5 examples/sec; 0.142 sec/batch)
2017-05-09 02:07:28.731627: step 40720, loss = 0.86 (840.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:07:30.236414: step 40730, loss = 0.72 (850.6 examples/sec; 0.150 sec/batch)
2017-05-09 02:07:31.735484: step 40740, loss = 0.89 (853.9 examples/sec; 0.150 sec/batch)
2017-05-09 02:07:33.268358: step 40750, loss = 0.90 (835.0 examples/sec; 0.153 sec/batch)
2017-05-09 02:07:34.774730: step 40760, loss = 0.79 (849.7 examples/sec; 0.151 sec/batch)
2017-05-09 02:07:36.328837: step 40770, loss = 0.72 (823.6 examples/sec; 0.155 sec/batch)
2017-05-09 02:07:37.850801: step 40780, loss = 0.78 (841.0 examples/sec; 0.152 sec/batch)
2017-05-09 02:07:39.373910: step 40790, loss = 0.75 (840.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:07:40.987079: step 40800, loss = 0.70 (793.5 examples/sec; 0.161 sec/batch)
2017-05-09 02:07:42.439592: step 40810, loss = 0.82 (881.2 examples/sec; 0.145 sec/batch)
2017-05-09 02:07:43.953612: step 40820, loss = 0.76 (845.4 examples/sec; 0.151 sec/batch)
2017-05-09 02:07:45.487522: step 40830, loss = 0.80 (834.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:07:46.994844: step 40840, loss = 0.82 (849.2 examples/sec; 0.151 sec/batch)
2017-05-09 02:07:48.511719: step 40850, loss = 0.86 (843.8 examples/sec; 0.152 sec/batch)
2017-05-09 02:07:50.002724: step 40860, loss = 0.82 (858.5 examples/sec; 0.149 sec/batch)
2017-05-09 02:07:51.540574: step 40870, loss = 0.94 (832.3 examples/sec; 0.154 sec/batch)
2017-05-09 02:07:53.056074: step 40880, loss = 1.02 (844.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:07:54.554130: step 40890, loss = 0.94 (854.4 examples/sec; 0.150 sec/batch)
2017-05-09 02:07:56.142206: step 40900, loss = 0.75 (806.0 examples/sec; 0.159 sec/batch)
2017-05-09 02:07:57.548934: step 40910, loss = 0.83 (909.9 examples/sec; 0.141 sec/batch)
2017-05-09 02:07:59.037155: step 40920, loss = 0.80 (860.1 examples/sec; 0.149 sec/batch)
2017-05-09 02:08:00.537245: step 40930, loss = 0.77 (853.3 examples/sec;E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 834 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
 0.150 sec/batch)
2017-05-09 02:08:02.007103: step 40940, loss = 0.75 (870.8 examples/sec; 0.147 sec/batch)
2017-05-09 02:08:03.504835: step 40950, loss = 0.89 (854.6 examples/sec; 0.150 sec/batch)
2017-05-09 02:08:05.028098: step 40960, loss = 0.74 (840.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:08:06.553281: step 40970, loss = 0.71 (839.3 examples/sec; 0.153 sec/batch)
2017-05-09 02:08:08.106843: step 40980, loss = 0.75 (823.9 examples/sec; 0.155 sec/batch)
2017-05-09 02:08:09.643045: step 40990, loss = 0.88 (833.2 examples/sec; 0.154 sec/batch)
2017-05-09 02:08:11.240900: step 41000, loss = 0.82 (801.1 examples/sec; 0.160 sec/batch)
2017-05-09 02:08:12.654305: step 41010, loss = 0.66 (905.6 examples/sec; 0.141 sec/batch)
2017-05-09 02:08:14.166958: step 41020, loss = 0.79 (846.2 examples/sec; 0.151 sec/batch)
2017-05-09 02:08:15.680738: step 41030, loss = 0.68 (845.6 examples/sec; 0.151 sec/batch)
2017-05-09 02:08:17.197363: step 41040, loss = 0.73 (844.0 examples/sec; 0.152 sec/batch)
2017-05-09 02:08:18.709275: step 41050, loss = 0.68 (846.6 examples/sec; 0.151 sec/batch)
2017-05-09 02:08:20.202198: step 41060, loss = 0.87 (857.4 examples/sec; 0.149 sec/batch)
2017-05-09 02:08:21.710062: step 41070, loss = 0.99 (848.9 examples/sec; 0.151 sec/batch)
2017-05-09 02:08:23.245586: step 41080, loss = 0.77 (833.6 examples/sec; 0.154 sec/batch)
2017-05-09 02:08:24.792035: step 41090, loss = 0.62 (827.7 examples/sec; 0.155 sec/batch)
2017-05-09 02:08:26.432847: step 41100, loss = 0.88 (780.1 examples/sec; 0.164 sec/batch)
2017-05-09 02:08:27.880726: step 41110, loss = 0.94 (884.1 examples/sec; 0.145 sec/batch)
2017-05-09 02:08:29.377896: step 41120, loss = 0.71 (854.9 examples/sec; 0.150 sec/batch)
2017-05-09 02:08:30.871545: step 41130, loss = 0.79 (857.0 examples/sec; 0.149 sec/batch)
2017-05-09 02:08:32.439718: step 41140, loss = 0.93 (816.2 examples/sec; 0.157 sec/batch)
2017-05-09 02:08:33.969750: step 41150, loss = 0.80 (836.6 examples/sec; 0.153 sec/batch)
2017-05-09 02:08:35.491678: step 41160, loss = 0.83 (841.0 examples/sec; 0.152 sec/batch)
2017-05-09 02:08:37.038877: step 41170, loss = 0.80 (827.3 examples/sec; 0.155 sec/batch)
2017-05-09 02:08:38.560636: step 41180, loss = 0.70 (841.1 examples/sec; 0.152 sec/batch)
2017-05-09 02:08:40.070857: step 41190, loss = 0.79 (847.6 examples/sec; 0.151 sec/batch)
2017-05-09 02:08:41.686814: step 41200, loss = 0.95 (792.1 examples/sec; 0.162 sec/batch)
2017-05-09 02:08:43.110579: step 41210, loss = 0.88 (899.0 examples/sec; 0.142 sec/batch)
2017-05-09 02:08:44.636056: step 41220, loss = 0.74 (839.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:08:46.172107: step 41230, loss = 0.95 (833.3 examples/sec; 0.154 sec/batch)
2017-05-09 02:08:47.668456: step 41240, loss = 0.68 (855.4 examples/sec; 0.150 sec/batch)
2017-05-09 02:08:49.207842: step 41250, loss = 0.82 (831.5 examples/sec; 0.154 sec/batch)
2017-05-09 02:08:50.724086: step 41260, loss = 0.78 (844.2 examples/sec; 0.152 sec/batch)
2017-05-09 02:08:52.257372: step 41270, loss = 0.79 (834.8 examples/sec; 0.153 sec/batch)
2017-05-09 02:08:53.765218: step 41280, loss = 0.70 (848.9 examples/sec; 0.151 sec/batch)
2017-05-09 02:08:55.325389: step 41290, loss = 0.80 (820.4 examples/sec; 0.156 sec/batch)
2017-05-09 02:08:56.954186: step 41300, loss = 0.87 (785.9 examples/sec; 0.163 sec/batch)
2017-05-09 02:08:58.382270: step 41310, loss = 0.78 (896.3 examples/sec; 0.143 sec/batch)
2017-05-09 02:08:59.947359: step 41320, loss = 0.85 (817.9 examples/sec; 0.157 sec/batch)
2017-05-09 02:09:01.490419: step 41330, loss = 0.89 (829.5 examples/sec; 0.154 sec/batch)
2017-05-09 02:09:03.026657: step 41340, loss = 0.79 (833.2 examples/sec; 0.154 sec/batch)
2017-05-09 02:09:04.596841: step 41350, loss = 0.78 (815.2 examples/sec; 0.157 sec/batch)
2017-05-09 02:09:06.124435: step 41360, loss = 0.75 (837.9 examples/sec; 0.153 sec/batch)
2017-05-09 02:09:07.659728: step 41370, loss = 0.84 (833.7 examples/sec; 0.154 sec/batch)
2017-05-09 02:09:09.174706: step 41380, loss = 0.86 (844.9 examples/sec; 0.151 sec/batch)
2017-05-09 02:09:10.659560: E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 850 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
step 41390, loss = 0.85 (862.0 examples/sec; 0.148 sec/batch)
2017-05-09 02:09:12.265835: step 41400, loss = 0.77 (796.9 examples/sec; 0.161 sec/batch)
2017-05-09 02:09:13.712748: step 41410, loss = 0.80 (884.6 examples/sec; 0.145 sec/batch)
2017-05-09 02:09:15.223805: step 41420, loss = 0.91 (847.1 examples/sec; 0.151 sec/batch)
2017-05-09 02:09:16.752365: step 41430, loss = 0.88 (837.4 examples/sec; 0.153 sec/batch)
2017-05-09 02:09:18.250597: step 41440, loss = 1.02 (854.3 examples/sec; 0.150 sec/batch)
2017-05-09 02:09:19.753993: step 41450, loss = 0.98 (851.4 examples/sec; 0.150 sec/batch)
2017-05-09 02:09:21.278306: step 41460, loss = 0.69 (839.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:09:22.797695: step 41470, loss = 0.75 (842.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:09:24.351119: step 41480, loss = 1.14 (824.0 examples/sec; 0.155 sec/batch)
2017-05-09 02:09:25.909276: step 41490, loss = 1.11 (821.5 examples/sec; 0.156 sec/batch)
2017-05-09 02:09:27.527909: step 41500, loss = 0.85 (790.8 examples/sec; 0.162 sec/batch)
2017-05-09 02:09:28.950237: step 41510, loss = 0.79 (899.9 examples/sec; 0.142 sec/batch)
2017-05-09 02:09:30.499025: step 41520, loss = 0.63 (826.5 examples/sec; 0.155 sec/batch)
2017-05-09 02:09:32.035142: step 41530, loss = 0.84 (833.3 examples/sec; 0.154 sec/batch)
2017-05-09 02:09:33.559140: step 41540, loss = 0.78 (839.9 examples/sec; 0.152 sec/batch)
2017-05-09 02:09:35.068575: step 41550, loss = 0.77 (848.0 examples/sec; 0.151 sec/batch)
2017-05-09 02:09:36.578492: step 41560, loss = 0.71 (847.7 examples/sec; 0.151 sec/batch)
2017-05-09 02:09:38.106442: step 41570, loss = 0.87 (837.7 examples/sec; 0.153 sec/batch)
2017-05-09 02:09:39.621848: step 41580, loss = 0.74 (844.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:09:41.137961: step 41590, loss = 0.86 (844.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:09:42.757565: step 41600, loss = 1.07 (790.3 examples/sec; 0.162 sec/batch)
2017-05-09 02:09:44.211057: step 41610, loss = 0.80 (880.6 examples/sec; 0.145 sec/batch)
2017-05-09 02:09:45.736302: step 41620, loss = 0.68 (839.2 examples/sec; 0.153 sec/batch)
2017-05-09 02:09:47.262481: step 41630, loss = 1.20 (838.7 examples/sec; 0.153 sec/batch)
2017-05-09 02:09:48.828707: step 41640, loss = 0.75 (817.3 examples/sec; 0.157 sec/batch)
2017-05-09 02:09:50.362849: step 41650, loss = 0.81 (834.3 examples/sec; 0.153 sec/batch)
2017-05-09 02:09:51.885760: step 41660, loss = 0.83 (840.5 examples/sec; 0.152 sec/batch)
2017-05-09 02:09:53.389701: step 41670, loss = 0.75 (851.1 examples/sec; 0.150 sec/batch)
2017-05-09 02:09:54.937495: step 41680, loss = 0.76 (827.0 examples/sec; 0.155 sec/batch)
2017-05-09 02:09:56.462247: step 41690, loss = 0.75 (839.5 examples/sec; 0.152 sec/batch)
2017-05-09 02:09:58.059988: step 41700, loss = 0.74 (801.1 examples/sec; 0.160 sec/batch)
2017-05-09 02:09:59.492862: step 41710, loss = 0.76 (893.3 examples/sec; 0.143 sec/batch)
2017-05-09 02:10:01.014389: step 41720, loss = 0.73 (841.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:10:02.560458: step 41730, loss = 0.98 (827.9 examples/sec; 0.155 sec/batch)
2017-05-09 02:10:04.100506: step 41740, loss = 0.82 (831.1 examples/sec; 0.154 sec/batch)
2017-05-09 02:10:05.634052: step 41750, loss = 0.71 (834.7 examples/sec; 0.153 sec/batch)
2017-05-09 02:10:07.196352: step 41760, loss = 0.83 (819.3 examples/sec; 0.156 sec/batch)
2017-05-09 02:10:08.727678: step 41770, loss = 0.84 (835.9 examples/sec; 0.153 sec/batch)
2017-05-09 02:10:10.250462: step 41780, loss = 0.90 (840.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:10:11.735625: step 41790, loss = 1.07 (861.9 examples/sec; 0.149 sec/batch)
2017-05-09 02:10:13.357032: step 41800, loss = 0.97 (789.4 examples/sec; 0.162 sec/batch)
2017-05-09 02:10:14.764539: step 41810, loss = 0.67 (909.4 examples/sec; 0.141 sec/batch)
2017-05-09 02:10:16.249246: step 41820, loss = 0.77 (862.1 examples/sec; 0.148 sec/batch)
2017-05-09 02:10:17.779016: step 41830, loss = 0.93 (836.7 examples/sec; 0.153 sec/batch)
2017-05-09 02:10:19.275217: step 41840, loss = 0.75 (855.5 examples/sec; 0.150 sec/batch)
2017-05-09 02:10:20.795949: step 41850, loss = 0.88 (841.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:10:22.339624: step 41860, loss = 0.75 (829.2 examples/sec; 0.154 sec/batch)
2017-05-09 02:10:23.845110: step 41870, loss = 0.79 (850.2 examples/sec; 0.151 sec/batch)
2017-05-09 02:10:25.372222: step 41880, loss = 0.82 (838.2 examples/sec; 0.153 sec/batch)
2017-05-09 02:10:26.879202: step 41890, loss = 1.02 (849.4 examples/sec; 0.151 sec/batch)
2017-05-09 02:10:28.483835: step 41900, loss = 0.75 (797.7 examples/sec; 0.160 sec/batch)
2017-05-09 02:10:29.889410: step 41910, loss = 0.85 (910.7 examples/sec; 0.141 sec/batch)
2017-05-09 02:10:31.389235: step 41920, loss = 0.82 (853.4 examples/sec; 0.150 sec/batch)
2017-05-09 02:10:32.885024: step 41930, loss = 0.74 (855.7 examples/sec; 0.150 sec/batch)
2017-05-09 02:10:34.379177: step 41940, loss = 0.83 (856.7 examples/sec; 0.149 sec/batch)
2017-05-09 02:10:35.862063: step 41950, loss = 0.65 (863.2 examples/sec; 0.148 sec/batch)
2017-05-09 02:10:37.368433: step 41960, loss = 0.92 (849.7 examples/sec; 0.151 sec/batch)
2017-05-09 02:10:38.879721: step 41970, loss = 0.76 (847.0 examples/sec; 0.151 sec/batch)
2017-05-09 02:10:40.361250: step 41980, loss = 0.69 (864.0 examples/sec; 0.148 sec/batch)
2017-05-09 02:10:41.842958: step 41990, loss = 0.79 (863.9 examples/sec; 0.148 sec/batch)
2017-05-09 02:10:43.407119: step 42000, loss = 0.88 (818.3 examples/sec; 0.156 sec/batch)
2017-05-09 02:10:44.810149: step 42010, loss = 0.67 (912.3 examples/sec; 0.140 sec/batch)
2017-05-09 02:10:46.289712: step 42020, loss = 0.62 (865.1 examples/sec; 0.148 sec/batch)
2017-05-09 02:10:47.789084: step 42030, loss = 0.79 (853.7 examples/sec; 0.150 sec/batch)
2017-05-09 02:10:49.275168: step 42040, loss = 0.65 (861.3 examples/sec; 0.149 sec/batch)
2017-05-09 02:10:50.775606: step 42050, loss = 0.77 (853.1 examples/sec; 0.150 sec/batch)
2017-05-09 02:10:52.274843: step 42060, loss = 0.75 (853.8 examples/sec; 0.150 sec/batch)
2017-05-09 02:10:53.775910: step 42070, loss = 0.86 (852.7 examples/sec; 0.150 sec/batch)
2017-05-09 02:10:55.283981: step 42080, loss = 0.82 (848.8 examples/sec; 0.151 sec/batch)
2017-05-09 02:10:56.786343: step 42090, loss = 0.79 (852.0 examples/sec; 0.150 sec/batch)
2017-05-09 02:10:58.381338: step 42100, loss = 0.89 (802.5 examples/sec; 0.159 sec/batch)
2017-05-09 02:10:59.814467: step 42110, loss = 0.84 (893.1 examples/sec; 0.143 sec/batch)
2017-05-09 02:11:01.309825: step 42120, loss = 1.00 (856.0 examples/sec; 0.150 sec/batch)
2017-05-09 02:11:02.782733: step 42130, loss = 0.80 (869.0 examples/sec; 0.147 sec/batch)
2017-05-09 02:11:04.274126: step 42140, loss = 0.63 (858.3 examples/sec; 0.149 sec/batch)
2017-05-09 02:11:05.776065: step 42150, loss = 1.03 (852.2 examples/sec; 0.150 sec/batch)
2017-05-09 02:11:07.267516: step 42160, loss = 0.76 (858.2 examples/sec; 0.149 sec/batch)
2017-05-09 02:11:08.777619: step 42170, loss = 0.86 (847.6 examples/sec; 0.151 sec/batch)
2017-05-09 02:11:10.264504: step 42180, loss = 0.94 (860.9 examples/sec; 0.149 sec/batch)
2017-05-09 02:11:11.758486: step 42190, loss = 0.93 (856.8 examples/sec; 0.149 sec/batch)
2017-05-09 02:11:13.362063: step 42200, loss = 0.70 (798.2 examples/sec; 0.160 sec/batch)
2017-05-09 02:11:14.764822: step 42210, loss = 0.82 (912.5 examples/sec; 0.140 sec/batch)
2017-05-09 02:11:16.263007: step 42220, loss = 0.80 (854.4 examples/sec; 0.150 sec/batch)
2017-05-09 02:11:17.758746: step 42230, loss = 0.60 (855.8 examples/sec; 0.150 sec/batch)
2017-05-09 02:11:19.253572: step 42240, loss = 0.79 (856.3 examples/sec; 0.149 sec/batch)
2017-05-09 02:11:20.721999: step 42250, loss = 0.91 (871.7 examples/sec; 0.147 sec/batch)
2017-05-09 02:11:22.207606: step 42260, loss = 0.76 (861.6 examples/sec; 0.149 sec/batch)
2017-05-09 02:11:23.722294: step 42270, loss = 0.70 (845.1 examples/sec; 0.151 sec/batch)
2017-05-09 02:11:25.235527: step 42280, loss = 0.81 (845.9 examples/sec; 0.151 sec/batch)
2017-05-09 02:11:26.740250: step 42290, loss = 0.76 (850.6 examples/sec; 0.150 sec/batch)
2017-05-09 02:11:28.393110: stE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 866 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
ep 42300, loss = 0.76 (774.4 examples/sec; 0.165 sec/batch)
2017-05-09 02:11:29.819546: step 42310, loss = 0.69 (897.3 examples/sec; 0.143 sec/batch)
2017-05-09 02:11:31.326224: step 42320, loss = 0.82 (849.6 examples/sec; 0.151 sec/batch)
2017-05-09 02:11:32.857581: step 42330, loss = 0.76 (835.9 examples/sec; 0.153 sec/batch)
2017-05-09 02:11:34.364187: step 42340, loss = 0.78 (849.6 examples/sec; 0.151 sec/batch)
2017-05-09 02:11:35.882170: step 42350, loss = 0.86 (843.2 examples/sec; 0.152 sec/batch)
2017-05-09 02:11:37.405160: step 42360, loss = 0.75 (840.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:11:38.955379: step 42370, loss = 0.87 (825.7 examples/sec; 0.155 sec/batch)
2017-05-09 02:11:40.512916: step 42380, loss = 0.82 (821.8 examples/sec; 0.156 sec/batch)
2017-05-09 02:11:42.022915: step 42390, loss = 0.87 (847.7 examples/sec; 0.151 sec/batch)
2017-05-09 02:11:43.640649: step 42400, loss = 0.85 (791.2 examples/sec; 0.162 sec/batch)
2017-05-09 02:11:45.084185: step 42410, loss = 0.98 (886.7 examples/sec; 0.144 sec/batch)
2017-05-09 02:11:46.576942: step 42420, loss = 0.62 (857.5 examples/sec; 0.149 sec/batch)
2017-05-09 02:11:48.092792: step 42430, loss = 0.88 (844.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:11:49.599567: step 42440, loss = 0.73 (849.5 examples/sec; 0.151 sec/batch)
2017-05-09 02:11:51.118803: step 42450, loss = 1.04 (842.5 examples/sec; 0.152 sec/batch)
2017-05-09 02:11:52.643890: step 42460, loss = 0.70 (839.3 examples/sec; 0.153 sec/batch)
2017-05-09 02:11:54.158369: step 42470, loss = 0.75 (845.2 examples/sec; 0.151 sec/batch)
2017-05-09 02:11:55.698566: step 42480, loss = 0.92 (831.1 examples/sec; 0.154 sec/batch)
2017-05-09 02:11:57.210942: step 42490, loss = 1.17 (846.4 examples/sec; 0.151 sec/batch)
2017-05-09 02:11:58.837770: step 42500, loss = 0.78 (786.8 examples/sec; 0.163 sec/batch)
2017-05-09 02:12:00.249593: step 42510, loss = 0.85 (906.6 examples/sec; 0.141 sec/batch)
2017-05-09 02:12:01.779554: step 42520, loss = 0.77 (836.6 examples/sec; 0.153 sec/batch)
2017-05-09 02:12:03.339798: step 42530, loss = 0.91 (820.4 examples/sec; 0.156 sec/batch)
2017-05-09 02:12:04.845921: step 42540, loss = 0.85 (849.9 examples/sec; 0.151 sec/batch)
2017-05-09 02:12:06.361760: step 42550, loss = 0.76 (844.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:12:07.876969: step 42560, loss = 0.81 (844.8 examples/sec; 0.152 sec/batch)
2017-05-09 02:12:09.422660: step 42570, loss = 0.77 (828.1 examples/sec; 0.155 sec/batch)
2017-05-09 02:12:10.943307: step 42580, loss = 0.81 (841.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:12:12.471029: step 42590, loss = 0.65 (837.8 examples/sec; 0.153 sec/batch)
2017-05-09 02:12:14.083084: step 42600, loss = 0.71 (794.0 examples/sec; 0.161 sec/batch)
2017-05-09 02:12:15.490881: step 42610, loss = 0.75 (909.2 examples/sec; 0.141 sec/batch)
2017-05-09 02:12:17.030773: step 42620, loss = 0.70 (831.2 examples/sec; 0.154 sec/batch)
2017-05-09 02:12:18.572250: step 42630, loss = 0.68 (830.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:12:20.119635: step 42640, loss = 0.93 (827.2 examples/sec; 0.155 sec/batch)
2017-05-09 02:12:21.622873: step 42650, loss = 0.71 (851.5 examples/sec; 0.150 sec/batch)
2017-05-09 02:12:23.169293: step 42660, loss = 0.97 (827.7 examples/sec; 0.155 sec/batch)
2017-05-09 02:12:24.716091: step 42670, loss = 0.72 (827.5 examples/sec; 0.155 sec/batch)
2017-05-09 02:12:26.235555: step 42680, loss = 0.82 (842.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:12:27.736640: step 42690, loss = 0.69 (852.7 examples/sec; 0.150 sec/batch)
2017-05-09 02:12:29.348288: step 42700, loss = 0.89 (794.2 examples/sec; 0.161 sec/batch)
2017-05-09 02:12:30.806689: step 42710, loss = 0.78 (877.7 examples/sec; 0.146 sec/batch)
2017-05-09 02:12:32.339089: step 42720, loss = 0.85 (835.3 examples/sec; 0.153 sec/batch)
2017-05-09 02:12:33.856389: step 42730, loss = 0.74 (843.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:12:35.379750: step 42740, loss = 0.95 (840.2 examples/sec; 0.152 sec/batch)
2017-05-09 02:12:36.893612: step 42750, loss = 0.85 (845.5 examples/sec; 0.151 sec/batch)
2017-05-09 02:12:38.402066: step 42760, loss = 0.85 (848.6 examples/sec; 0.151 sec/batch)
2017-05-09 02:12:39.929146: step 42770, loss = 0.88 (838.2 examples/sec; 0.153 sec/batch)
2017-05-09 02:12:41.430482: step 42780, loss = 0.80 (852.6 examples/sec; 0.150 sec/batch)
2017-05-09 02:12:42.937713: step 42790, loss = 0.90 (849.2 examples/sec; 0.151 sec/batch)
2017-05-09 02:12:44.581424: step 42800, loss = 0.82 (778.7 examples/sec; 0.164 sec/batch)
2017-05-09 02:12:45.991082: step 42810, loss = 0.76 (908.0 examples/sec; 0.141 sec/batch)
2017-05-09 02:12:47.516769: step 42820, loss = 0.84 (839.0 examples/sec; 0.153 sec/batch)
2017-05-09 02:12:49.052628: step 42830, loss = 0.92 (833.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:12:50.547393: step 42840, loss = 0.73 (856.3 examples/sec; 0.149 sec/batch)
2017-05-09 02:12:52.057875: step 42850, loss = 0.72 (847.4 examples/sec; 0.151 sec/batch)
2017-05-09 02:12:53.574242: step 42860, loss = 0.71 (844.1 examples/sec; 0.152 sec/batch)
2017-05-09 02:12:55.108530: step 42870, loss = 0.78 (834.3 examples/sec; 0.153 sec/batch)
2017-05-09 02:12:56.647749: step 42880, loss = 0.63 (831.6 examples/sec; 0.154 sec/batch)
2017-05-09 02:12:58.162593: step 42890, loss = 0.65 (845.0 examples/sec; 0.151 sec/batch)
2017-05-09 02:12:59.779718: step 42900, loss = 0.67 (791.5 examples/sec; 0.162 sec/batch)
2017-05-09 02:13:01.217873: step 42910, loss = 0.80 (890.0 examples/sec; 0.144 sec/batch)
2017-05-09 02:13:02.771767: step 42920, loss = 0.84 (823.7 examples/sec; 0.155 sec/batch)
2017-05-09 02:13:04.314609: step 42930, loss = 0.90 (829.6 examples/sec; 0.154 sec/batch)
2017-05-09 02:13:05.871756: step 42940, loss = 0.75 (822.0 examples/sec; 0.156 sec/batch)
2017-05-09 02:13:07.373158: step 42950, loss = 0.81 (852.5 examples/sec; 0.150 sec/batch)
2017-05-09 02:13:08.908212: step 42960, loss = 0.95 (833.9 examples/sec; 0.154 sec/batch)
2017-05-09 02:13:10.438912: step 42970, loss = 0.74 (836.2 examples/sec; 0.153 sec/batch)
2017-05-09 02:13:11.941286: step 42980, loss = 0.82 (852.0 examples/sec; 0.150 sec/batch)
2017-05-09 02:13:13.451242: step 42990, loss = 0.70 (847.7 examples/sec; 0.151 sec/batch)
2017-05-09 02:13:15.069127: step 43000, loss = 0.67 (791.2 examples/sec; 0.162 sec/batch)
2017-05-09 02:13:16.492745: step 43010, loss = 0.90 (899.1 examples/sec; 0.142 sec/batch)
2017-05-09 02:13:18.016651: step 43020, loss = 1.01 (839.9 examples/sec; 0.152 sec/batch)
2017-05-09 02:13:19.543903: step 43030, loss = 0.83 (838.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:13:21.057233: step 43040, loss = 0.82 (845.8 examples/sec; 0.151 sec/batch)
2017-05-09 02:13:22.571834: step 43050, loss = 0.84 (845.1 examples/sec; 0.151 sec/batch)
2017-05-09 02:13:24.077878: step 43060, loss = 0.71 (849.9 examples/sec; 0.151 sec/batch)
2017-05-09 02:13:25.618414: step 43070, loss = 0.86 (830.9 examples/sec; 0.154 sec/batch)
2017-05-09 02:13:27.177633: step 43080, loss = 0.86 (820.9 examples/sec; 0.156 sec/batch)
2017-05-09 02:13:28.691777: step 43090, loss = 0.72 (845.4 examples/sec; 0.151 sec/batch)
2017-05-09 02:13:30.296505: step 43100, loss = 0.69 (797.6 examples/sec; 0.160 sec/batch)
2017-05-09 02:13:31.715295: step 43110, loss = 0.77 (902.2 examples/sec; 0.142 sec/batch)
2017-05-09 02:13:33.231081: step 43120, loss = 0.77 (844.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:13:34.749088: step 43130, loss = 0.67 (843.2 examples/sec; 0.152 sec/batch)
2017-05-09 02:13:36.262283: step 43140, loss = 1.00 (845.9 examples/sec; 0.151 sec/batch)
2017-05-09 02:13:37.777565: step 43150, loss = 0.87 (844.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:13:39.301525: step 43160, loss = 0.63 (839.9 examples/sec; 0.152 sec/batch)
2017-05-09 02:13:40.791200: step 43170, loss = 0.63 (859.2 examples/sec; 0.149 sec/batch)
2017-05-09 02:13:42.314862: step 43180, loss = 0.93 (840.1 examples/sec; 0.152 sec/batch)
2017-05-09 02:13:43.878965: step 43190, loss = 0.74 (818.4 examples/sec; 0.156 sec/batch)
2017-05-09 02:13:45.460173: step 43200, loss = 0.77 (809.5 examples/sec; 0.158 sec/batch)
2017-05-09 02:13:46.895256: stepE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 882 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
 43210, loss = 0.74 (891.9 examples/sec; 0.144 sec/batch)
2017-05-09 02:13:48.407725: step 43220, loss = 0.73 (846.3 examples/sec; 0.151 sec/batch)
2017-05-09 02:13:49.937138: step 43230, loss = 0.80 (836.9 examples/sec; 0.153 sec/batch)
2017-05-09 02:13:51.458100: step 43240, loss = 0.76 (841.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:13:52.956809: step 43250, loss = 0.71 (854.1 examples/sec; 0.150 sec/batch)
2017-05-09 02:13:54.484199: step 43260, loss = 0.75 (838.0 examples/sec; 0.153 sec/batch)
2017-05-09 02:13:55.997004: step 43270, loss = 0.80 (846.1 examples/sec; 0.151 sec/batch)
2017-05-09 02:13:57.534783: step 43280, loss = 0.91 (832.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:13:59.070516: step 43290, loss = 0.73 (833.5 examples/sec; 0.154 sec/batch)
2017-05-09 02:14:00.709768: step 43300, loss = 0.76 (780.8 examples/sec; 0.164 sec/batch)
2017-05-09 02:14:02.164245: step 43310, loss = 0.76 (880.0 examples/sec; 0.145 sec/batch)
2017-05-09 02:14:03.705857: step 43320, loss = 0.78 (830.3 examples/sec; 0.154 sec/batch)
2017-05-09 02:14:05.244900: step 43330, loss = 0.81 (831.7 examples/sec; 0.154 sec/batch)
2017-05-09 02:14:06.775513: step 43340, loss = 0.71 (836.3 examples/sec; 0.153 sec/batch)
2017-05-09 02:14:08.277991: step 43350, loss = 0.93 (851.9 examples/sec; 0.150 sec/batch)
2017-05-09 02:14:09.807192: step 43360, loss = 1.01 (837.0 examples/sec; 0.153 sec/batch)
2017-05-09 02:14:11.355301: step 43370, loss = 0.84 (826.8 examples/sec; 0.155 sec/batch)
2017-05-09 02:14:12.878566: step 43380, loss = 0.66 (840.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:14:14.395038: step 43390, loss = 0.89 (844.1 examples/sec; 0.152 sec/batch)
2017-05-09 02:14:16.030175: step 43400, loss = 0.82 (782.8 examples/sec; 0.164 sec/batch)
2017-05-09 02:14:17.452874: step 43410, loss = 0.76 (899.7 examples/sec; 0.142 sec/batch)
2017-05-09 02:14:18.993724: step 43420, loss = 0.78 (830.7 examples/sec; 0.154 sec/batch)
2017-05-09 02:14:20.494673: step 43430, loss = 0.91 (852.8 examples/sec; 0.150 sec/batch)
2017-05-09 02:14:22.000949: step 43440, loss = 0.81 (849.8 examples/sec; 0.151 sec/batch)
2017-05-09 02:14:23.522613: step 43450, loss = 0.69 (841.2 examples/sec; 0.152 sec/batch)
2017-05-09 02:14:25.045862: step 43460, loss = 0.91 (840.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:14:26.581736: step 43470, loss = 0.85 (833.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:14:28.092164: step 43480, loss = 0.82 (847.4 examples/sec; 0.151 sec/batch)
2017-05-09 02:14:29.599248: step 43490, loss = 0.72 (849.3 examples/sec; 0.151 sec/batch)
2017-05-09 02:14:31.208486: step 43500, loss = 0.71 (795.4 examples/sec; 0.161 sec/batch)
2017-05-09 02:14:32.631798: step 43510, loss = 0.74 (899.3 examples/sec; 0.142 sec/batch)
2017-05-09 02:14:34.178036: step 43520, loss = 0.88 (827.8 examples/sec; 0.155 sec/batch)
2017-05-09 02:14:35.741353: step 43530, loss = 0.93 (818.8 examples/sec; 0.156 sec/batch)
2017-05-09 02:14:37.272142: step 43540, loss = 0.87 (836.2 examples/sec; 0.153 sec/batch)
2017-05-09 02:14:38.817383: step 43550, loss = 0.98 (828.4 examples/sec; 0.155 sec/batch)
2017-05-09 02:14:40.330756: step 43560, loss = 0.77 (845.8 examples/sec; 0.151 sec/batch)
2017-05-09 02:14:41.807504: step 43570, loss = 0.80 (866.8 examples/sec; 0.148 sec/batch)
2017-05-09 02:14:43.326962: step 43580, loss = 0.75 (842.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:14:44.868979: step 43590, loss = 0.83 (830.1 examples/sec; 0.154 sec/batch)
2017-05-09 02:14:46.520626: step 43600, loss = 0.83 (775.0 examples/sec; 0.165 sec/batch)
2017-05-09 02:14:47.993825: step 43610, loss = 0.68 (868.9 examples/sec; 0.147 sec/batch)
2017-05-09 02:14:49.507119: step 43620, loss = 0.90 (845.8 examples/sec; 0.151 sec/batch)
2017-05-09 02:14:50.974576: step 43630, loss = 0.79 (872.3 examples/sec; 0.147 sec/batch)
2017-05-09 02:14:52.497480: step 43640, loss = 1.04 (840.5 examples/sec; 0.152 sec/batch)
2017-05-09 02:14:54.024779: step 43650, loss = 1.05 (838.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:14:55.572693: step 43660, loss = 0.71 (826.9 examples/sec; 0.155 sec/batch)
2017-05-09 02:14:57.102430: step 43670, loss = 0.80 (836.7 examples/sec; 0.153 sec/batch)
2017-05-09 02:14:58.621857: step 43680, loss = 0.76 (842.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:15:00.132578: step 43690, loss = 0.79 (847.3 examples/sec; 0.151 sec/batch)
2017-05-09 02:15:01.755841: step 43700, loss = 0.80 (788.5 examples/sec; 0.162 sec/batch)
2017-05-09 02:15:03.258155: step 43710, loss = 0.71 (852.0 examples/sec; 0.150 sec/batch)
2017-05-09 02:15:04.769821: step 43720, loss = 0.60 (846.8 examples/sec; 0.151 sec/batch)
2017-05-09 02:15:06.294459: step 43730, loss = 0.71 (839.5 examples/sec; 0.152 sec/batch)
2017-05-09 02:15:07.840017: step 43740, loss = 0.70 (828.2 examples/sec; 0.155 sec/batch)
2017-05-09 02:15:09.359431: step 43750, loss = 0.65 (842.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:15:10.919917: step 43760, loss = 0.96 (820.3 examples/sec; 0.156 sec/batch)
2017-05-09 02:15:12.454878: step 43770, loss = 0.80 (833.9 examples/sec; 0.153 sec/batch)
2017-05-09 02:15:13.963437: step 43780, loss = 0.79 (848.5 examples/sec; 0.151 sec/batch)
2017-05-09 02:15:15.512946: step 43790, loss = 0.67 (826.1 examples/sec; 0.155 sec/batch)
2017-05-09 02:15:17.119805: step 43800, loss = 0.79 (796.6 examples/sec; 0.161 sec/batch)
2017-05-09 02:15:18.520257: step 43810, loss = 0.80 (914.0 examples/sec; 0.140 sec/batch)
2017-05-09 02:15:20.030767: step 43820, loss = 0.88 (847.4 examples/sec; 0.151 sec/batch)
2017-05-09 02:15:21.536825: step 43830, loss = 0.75 (849.9 examples/sec; 0.151 sec/batch)
2017-05-09 02:15:23.065965: step 43840, loss = 0.88 (837.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:15:24.571617: step 43850, loss = 0.79 (850.1 examples/sec; 0.151 sec/batch)
2017-05-09 02:15:26.090295: step 43860, loss = 0.83 (842.8 examples/sec; 0.152 sec/batch)
2017-05-09 02:15:27.616006: step 43870, loss = 0.86 (839.0 examples/sec; 0.153 sec/batch)
2017-05-09 02:15:29.131632: step 43880, loss = 0.69 (844.5 examples/sec; 0.152 sec/batch)
2017-05-09 02:15:30.640050: step 43890, loss = 0.79 (848.6 examples/sec; 0.151 sec/batch)
2017-05-09 02:15:32.263911: step 43900, loss = 0.98 (788.2 examples/sec; 0.162 sec/batch)
2017-05-09 02:15:33.676365: step 43910, loss = 0.89 (906.2 examples/sec; 0.141 sec/batch)
2017-05-09 02:15:35.191014: step 43920, loss = 0.73 (845.1 examples/sec; 0.151 sec/batch)
2017-05-09 02:15:36.707174: step 43930, loss = 0.73 (844.2 examples/sec; 0.152 sec/batch)
2017-05-09 02:15:38.245138: step 43940, loss = 0.88 (832.3 examples/sec; 0.154 sec/batch)
2017-05-09 02:15:39.777761: step 43950, loss = 0.80 (835.2 examples/sec; 0.153 sec/batch)
2017-05-09 02:15:41.266011: step 43960, loss = 0.61 (860.1 examples/sec; 0.149 sec/batch)
2017-05-09 02:15:42.784181: step 43970, loss = 0.78 (843.1 examples/sec; 0.152 sec/batch)
2017-05-09 02:15:44.295146: step 43980, loss = 1.02 (847.1 examples/sec; 0.151 sec/batch)
2017-05-09 02:15:45.825109: step 43990, loss = 0.83 (836.6 examples/sec; 0.153 sec/batch)
2017-05-09 02:15:47.433349: step 44000, loss = 1.01 (795.9 examples/sec; 0.161 sec/batch)
2017-05-09 02:15:48.875547: step 44010, loss = 0.85 (887.5 examples/sec; 0.144 sec/batch)
2017-05-09 02:15:50.412516: step 44020, loss = 0.80 (832.8 examples/sec; 0.154 sec/batch)
2017-05-09 02:15:51.962720: step 44030, loss = 0.86 (825.7 examples/sec; 0.155 sec/batch)
2017-05-09 02:15:53.508023: step 44040, loss = 0.85 (828.3 examples/sec; 0.155 sec/batch)
2017-05-09 02:15:55.024663: step 44050, loss = 0.88 (844.0 examples/sec; 0.152 sec/batch)
2017-05-09 02:15:56.504221: step 44060, loss = 0.84 (865.1 examples/sec; 0.148 sec/batch)
2017-05-09 02:15:58.000312: step 44070, loss = 0.80 (855.6 examples/sec; 0.150 sec/batch)
2017-05-09 02:15:59.524706: step 44080, loss = 0.82 (839.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:16:01.056583: step 44090, loss = 0.76 (835.6 examples/sec; 0.153 sec/batch)
2017-05-09 02:16:02.655361: step 44100, loss = 0.96 (800.6 examples/sec; 0.160 sec/batch)
2017-05-09 02:16:04.069026: step 44110, loss = 0.82 (905.4 examples/sec; 0.141 sec/batch)
2017-05-09 02:16:05.583573: step 4E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 899 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
4120, loss = 0.85 (845.1 examples/sec; 0.151 sec/batch)
2017-05-09 02:16:07.095692: step 44130, loss = 0.75 (846.5 examples/sec; 0.151 sec/batch)
2017-05-09 02:16:08.640787: step 44140, loss = 0.81 (828.4 examples/sec; 0.155 sec/batch)
2017-05-09 02:16:10.155833: step 44150, loss = 0.71 (844.9 examples/sec; 0.152 sec/batch)
2017-05-09 02:16:11.672132: step 44160, loss = 0.89 (844.2 examples/sec; 0.152 sec/batch)
2017-05-09 02:16:13.174203: step 44170, loss = 0.68 (852.2 examples/sec; 0.150 sec/batch)
2017-05-09 02:16:14.678204: step 44180, loss = 0.84 (851.1 examples/sec; 0.150 sec/batch)
2017-05-09 02:16:16.140943: step 44190, loss = 0.67 (875.1 examples/sec; 0.146 sec/batch)
2017-05-09 02:16:17.777055: step 44200, loss = 0.81 (782.3 examples/sec; 0.164 sec/batch)
2017-05-09 02:16:19.215139: step 44210, loss = 0.64 (890.1 examples/sec; 0.144 sec/batch)
2017-05-09 02:16:20.712450: step 44220, loss = 0.88 (854.9 examples/sec; 0.150 sec/batch)
2017-05-09 02:16:22.208887: step 44230, loss = 0.76 (855.4 examples/sec; 0.150 sec/batch)
2017-05-09 02:16:23.724906: step 44240, loss = 0.87 (844.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:16:25.229507: step 44250, loss = 0.98 (850.7 examples/sec; 0.150 sec/batch)
2017-05-09 02:16:26.742021: step 44260, loss = 0.68 (846.3 examples/sec; 0.151 sec/batch)
2017-05-09 02:16:28.244365: step 44270, loss = 0.79 (852.0 examples/sec; 0.150 sec/batch)
2017-05-09 02:16:29.728792: step 44280, loss = 0.84 (862.3 examples/sec; 0.148 sec/batch)
2017-05-09 02:16:31.222569: step 44290, loss = 0.80 (856.9 examples/sec; 0.149 sec/batch)
2017-05-09 02:16:32.807167: step 44300, loss = 0.93 (807.8 examples/sec; 0.158 sec/batch)
2017-05-09 02:16:34.202032: step 44310, loss = 0.97 (917.6 examples/sec; 0.139 sec/batch)
2017-05-09 02:16:35.720366: step 44320, loss = 0.65 (843.0 examples/sec; 0.152 sec/batch)
2017-05-09 02:16:37.230641: step 44330, loss = 0.70 (847.5 examples/sec; 0.151 sec/batch)
2017-05-09 02:16:38.715630: step 44340, loss = 0.89 (862.0 examples/sec; 0.148 sec/batch)
2017-05-09 02:16:40.228430: step 44350, loss = 0.79 (846.1 examples/sec; 0.151 sec/batch)
2017-05-09 02:16:41.727355: step 44360, loss = 0.77 (854.0 examples/sec; 0.150 sec/batch)
2017-05-09 02:16:43.225140: step 44370, loss = 0.81 (854.6 examples/sec; 0.150 sec/batch)
2017-05-09 02:16:44.737404: step 44380, loss = 0.71 (846.4 examples/sec; 0.151 sec/batch)
2017-05-09 02:16:46.255226: step 44390, loss = 0.85 (843.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:16:47.895212: step 44400, loss = 0.77 (780.5 examples/sec; 0.164 sec/batch)
2017-05-09 02:16:49.317726: step 44410, loss = 0.79 (899.8 examples/sec; 0.142 sec/batch)
2017-05-09 02:16:50.821298: step 44420, loss = 0.80 (851.3 examples/sec; 0.150 sec/batch)
2017-05-09 02:16:52.302035: step 44430, loss = 0.69 (864.4 examples/sec; 0.148 sec/batch)
2017-05-09 02:16:53.766471: step 44440, loss = 0.92 (874.1 examples/sec; 0.146 sec/batch)
2017-05-09 02:16:55.241828: step 44450, loss = 0.86 (867.6 examples/sec; 0.148 sec/batch)
2017-05-09 02:16:56.730028: step 44460, loss = 0.80 (860.1 examples/sec; 0.149 sec/batch)
2017-05-09 02:16:58.236759: step 44470, loss = 0.85 (849.5 examples/sec; 0.151 sec/batch)
2017-05-09 02:16:59.742176: step 44480, loss = 0.85 (850.3 examples/sec; 0.151 sec/batch)
2017-05-09 02:17:01.226660: step 44490, loss = 0.79 (862.3 examples/sec; 0.148 sec/batch)
2017-05-09 02:17:02.812040: step 44500, loss = 0.87 (807.4 examples/sec; 0.159 sec/batch)
2017-05-09 02:17:04.224699: step 44510, loss = 0.82 (906.1 examples/sec; 0.141 sec/batch)
2017-05-09 02:17:05.704425: step 44520, loss = 0.98 (865.0 examples/sec; 0.148 sec/batch)
2017-05-09 02:17:07.218453: step 44530, loss = 0.72 (845.4 examples/sec; 0.151 sec/batch)
2017-05-09 02:17:08.732990: step 44540, loss = 0.82 (845.2 examples/sec; 0.151 sec/batch)
2017-05-09 02:17:10.266070: step 44550, loss = 0.96 (834.9 examples/sec; 0.153 sec/batch)
2017-05-09 02:17:11.776936: step 44560, loss = 0.68 (847.2 examples/sec; 0.151 sec/batch)
2017-05-09 02:17:13.276309: step 44570, loss = 0.82 (853.7 examples/sec; 0.150 sE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 915 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
ec/batch)
2017-05-09 02:17:14.767652: step 44580, loss = 0.86 (858.3 examples/sec; 0.149 sec/batch)
2017-05-09 02:17:16.254840: step 44590, loss = 0.75 (860.7 examples/sec; 0.149 sec/batch)
2017-05-09 02:17:17.851681: step 44600, loss = 0.80 (801.6 examples/sec; 0.160 sec/batch)
2017-05-09 02:17:19.287153: step 44610, loss = 0.76 (891.7 examples/sec; 0.144 sec/batch)
2017-05-09 02:17:20.799843: step 44620, loss = 0.75 (846.2 examples/sec; 0.151 sec/batch)
2017-05-09 02:17:22.299866: step 44630, loss = 0.98 (853.3 examples/sec; 0.150 sec/batch)
2017-05-09 02:17:23.789782: step 44640, loss = 0.93 (859.1 examples/sec; 0.149 sec/batch)
2017-05-09 02:17:25.306523: step 44650, loss = 0.94 (843.9 examples/sec; 0.152 sec/batch)
2017-05-09 02:17:26.834682: step 44660, loss = 0.71 (837.6 examples/sec; 0.153 sec/batch)
2017-05-09 02:17:28.325226: step 44670, loss = 0.67 (858.8 examples/sec; 0.149 sec/batch)
2017-05-09 02:17:29.827671: step 44680, loss = 0.86 (851.9 examples/sec; 0.150 sec/batch)
2017-05-09 02:17:31.350348: step 44690, loss = 0.88 (840.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:17:32.939690: step 44700, loss = 0.79 (805.4 examples/sec; 0.159 sec/batch)
2017-05-09 02:17:34.349986: step 44710, loss = 0.80 (907.6 examples/sec; 0.141 sec/batch)
2017-05-09 02:17:35.888872: step 44720, loss = 0.76 (831.8 examples/sec; 0.154 sec/batch)
2017-05-09 02:17:37.412878: step 44730, loss = 0.86 (839.9 examples/sec; 0.152 sec/batch)
2017-05-09 02:17:38.915737: step 44740, loss = 0.77 (851.7 examples/sec; 0.150 sec/batch)
2017-05-09 02:17:40.407437: step 44750, loss = 0.75 (858.1 examples/sec; 0.149 sec/batch)
2017-05-09 02:17:41.896143: step 44760, loss = 0.86 (859.8 examples/sec; 0.149 sec/batch)
2017-05-09 02:17:43.415629: step 44770, loss = 0.90 (842.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:17:44.911599: step 44780, loss = 0.67 (855.6 examples/sec; 0.150 sec/batch)
2017-05-09 02:17:46.406518: step 44790, loss = 0.93 (856.2 examples/sec; 0.149 sec/batch)
2017-05-09 02:17:47.989303: step 44800, loss = 0.90 (808.7 examples/sec; 0.158 sec/batch)
2017-05-09 02:17:49.401246: step 44810, loss = 0.82 (906.5 examples/sec; 0.141 sec/batch)
2017-05-09 02:17:50.881507: step 44820, loss = 0.79 (864.7 examples/sec; 0.148 sec/batch)
2017-05-09 02:17:52.396234: step 44830, loss = 0.97 (845.0 examples/sec; 0.151 sec/batch)
2017-05-09 02:17:53.878232: step 44840, loss = 0.79 (863.7 examples/sec; 0.148 sec/batch)
2017-05-09 02:17:55.373420: step 44850, loss = 0.87 (856.1 examples/sec; 0.150 sec/batch)
2017-05-09 02:17:56.869079: step 44860, loss = 0.85 (855.8 examples/sec; 0.150 sec/batch)
2017-05-09 02:17:58.357590: step 44870, loss = 0.80 (859.9 examples/sec; 0.149 sec/batch)
2017-05-09 02:17:59.843890: step 44880, loss = 0.76 (861.2 examples/sec; 0.149 sec/batch)
2017-05-09 02:18:01.361410: step 44890, loss = 0.71 (843.5 examples/sec; 0.152 sec/batch)
2017-05-09 02:18:02.970635: step 44900, loss = 0.95 (795.4 examples/sec; 0.161 sec/batch)
2017-05-09 02:18:04.392287: step 44910, loss = 0.86 (900.3 examples/sec; 0.142 sec/batch)
2017-05-09 02:18:05.891637: step 44920, loss = 0.85 (853.7 examples/sec; 0.150 sec/batch)
2017-05-09 02:18:07.411837: step 44930, loss = 0.81 (842.0 examples/sec; 0.152 sec/batch)
2017-05-09 02:18:08.964672: step 44940, loss = 0.90 (824.3 examples/sec; 0.155 sec/batch)
2017-05-09 02:18:10.472494: step 44950, loss = 0.73 (848.9 examples/sec; 0.151 sec/batch)
2017-05-09 02:18:11.981843: step 44960, loss = 0.71 (848.1 examples/sec; 0.151 sec/batch)
2017-05-09 02:18:13.515956: step 44970, loss = 0.90 (834.4 examples/sec; 0.153 sec/batch)
2017-05-09 02:18:15.033873: step 44980, loss = 0.73 (843.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:18:16.565667: step 44990, loss = 0.66 (835.6 examples/sec; 0.153 sec/batch)
2017-05-09 02:18:18.164242: step 45000, loss = 0.90 (800.7 examples/sec; 0.160 sec/batch)
2017-05-09 02:18:19.619884: step 45010, loss = 0.73 (879.3 examples/sec; 0.146 sec/batch)
2017-05-09 02:18:21.156548: step 45020, loss = 0.94 (833.0 examples/sec; 0.154 sec/batch)
2017-05-09 02:18:22.693380: step 45030, loss = 0.79 (832.9 examples/sec; 0.154 sec/batch)
2017-05-09 02:18:24.204756: step 45040, loss = 0.94 (846.9 examples/sec; 0.151 sec/batch)
2017-05-09 02:18:25.730786: step 45050, loss = 0.80 (838.8 examples/sec; 0.153 sec/batch)
2017-05-09 02:18:27.308866: step 45060, loss = 0.81 (811.1 examples/sec; 0.158 sec/batch)
2017-05-09 02:18:28.836932: step 45070, loss = 0.67 (837.7 examples/sec; 0.153 sec/batch)
2017-05-09 02:18:30.344176: step 45080, loss = 0.93 (849.2 examples/sec; 0.151 sec/batch)
2017-05-09 02:18:31.897999: step 45090, loss = 0.88 (823.8 examples/sec; 0.155 sec/batch)
2017-05-09 02:18:33.511266: step 45100, loss = 0.79 (793.4 examples/sec; 0.161 sec/batch)
2017-05-09 02:18:34.956914: step 45110, loss = 0.79 (885.4 examples/sec; 0.145 sec/batch)
2017-05-09 02:18:36.490687: step 45120, loss = 0.85 (834.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:18:38.000025: step 45130, loss = 0.79 (848.1 examples/sec; 0.151 sec/batch)
2017-05-09 02:18:39.533863: step 45140, loss = 0.69 (834.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:18:41.077772: step 45150, loss = 0.68 (829.1 examples/sec; 0.154 sec/batch)
2017-05-09 02:18:42.613144: step 45160, loss = 0.79 (833.7 examples/sec; 0.154 sec/batch)
2017-05-09 02:18:44.150789: step 45170, loss = 0.76 (832.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:18:45.663647: step 45180, loss = 0.81 (846.1 examples/sec; 0.151 sec/batch)
2017-05-09 02:18:47.187360: step 45190, loss = 0.80 (840.0 examples/sec; 0.152 sec/batch)
2017-05-09 02:18:48.821527: step 45200, loss = 0.83 (783.3 examples/sec; 0.163 sec/batch)
2017-05-09 02:18:50.252554: step 45210, loss = 0.84 (894.5 examples/sec; 0.143 sec/batch)
2017-05-09 02:18:51.779005: step 45220, loss = 0.84 (838.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:18:53.320444: step 45230, loss = 0.81 (830.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:18:54.851598: step 45240, loss = 0.75 (836.0 examples/sec; 0.153 sec/batch)
2017-05-09 02:18:56.358810: step 45250, loss = 0.76 (849.2 examples/sec; 0.151 sec/batch)
2017-05-09 02:18:57.896800: step 45260, loss = 0.71 (832.3 examples/sec; 0.154 sec/batch)
2017-05-09 02:18:59.418020: step 45270, loss = 0.79 (841.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:19:00.918000: step 45280, loss = 0.73 (853.3 examples/sec; 0.150 sec/batch)
2017-05-09 02:19:02.446166: step 45290, loss = 0.63 (837.6 examples/sec; 0.153 sec/batch)
2017-05-09 02:19:04.092725: step 45300, loss = 0.88 (777.4 examples/sec; 0.165 sec/batch)
2017-05-09 02:19:05.491240: step 45310, loss = 0.66 (915.3 examples/sec; 0.140 sec/batch)
2017-05-09 02:19:06.978352: step 45320, loss = 1.02 (860.7 examples/sec; 0.149 sec/batch)
2017-05-09 02:19:08.522890: step 45330, loss = 0.75 (828.7 examples/sec; 0.154 sec/batch)
2017-05-09 02:19:10.047389: step 45340, loss = 0.72 (839.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:19:11.576800: step 45350, loss = 1.17 (836.9 examples/sec; 0.153 sec/batch)
2017-05-09 02:19:13.191497: step 45360, loss = 0.76 (792.7 examples/sec; 0.161 sec/batch)
2017-05-09 02:19:14.720604: step 45370, loss = 0.77 (837.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:19:16.272156: step 45380, loss = 0.80 (825.0 examples/sec; 0.155 sec/batch)
2017-05-09 02:19:17.845512: step 45390, loss = 0.86 (813.5 examples/sec; 0.157 sec/batch)
2017-05-09 02:19:19.462919: step 45400, loss = 0.86 (791.4 examples/sec; 0.162 sec/batch)
2017-05-09 02:19:20.911879: step 45410, loss = 0.69 (883.4 examples/sec; 0.145 sec/batch)
2017-05-09 02:19:22.461800: step 45420, loss = 0.67 (825.8 examples/sec; 0.155 sec/batch)
2017-05-09 02:19:24.015117: step 45430, loss = 0.79 (824.0 examples/sec; 0.155 sec/batch)
2017-05-09 02:19:25.550264: step 45440, loss = 0.81 (833.8 examples/sec; 0.154 sec/batch)
2017-05-09 02:19:27.082969: step 45450, loss = 0.67 (835.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:19:28.640817: step 45460, loss = 0.92 (821.6 examples/sec; 0.156 sec/batch)
2017-05-09 02:19:30.219474: step 45470, loss = 0.78 (810.8 examples/sec; 0.158 sec/batch)
2017-05-09 02:19:31.796388: step 45480, loss = 0.75 (811.7 examples/sec; 0.158 secE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 931 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
/batch)
2017-05-09 02:19:33.330158: step 45490, loss = 0.72 (834.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:19:34.977877: step 45500, loss = 0.82 (776.8 examples/sec; 0.165 sec/batch)
2017-05-09 02:19:36.410189: step 45510, loss = 0.82 (893.7 examples/sec; 0.143 sec/batch)
2017-05-09 02:19:37.939413: step 45520, loss = 0.92 (837.0 examples/sec; 0.153 sec/batch)
2017-05-09 02:19:39.489904: step 45530, loss = 0.92 (825.5 examples/sec; 0.155 sec/batch)
2017-05-09 02:19:41.018676: step 45540, loss = 0.71 (837.3 examples/sec; 0.153 sec/batch)
2017-05-09 02:19:42.530367: step 45550, loss = 0.89 (846.8 examples/sec; 0.151 sec/batch)
2017-05-09 02:19:44.057309: step 45560, loss = 0.82 (838.3 examples/sec; 0.153 sec/batch)
2017-05-09 02:19:45.576473: step 45570, loss = 0.92 (842.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:19:47.088783: step 45580, loss = 0.75 (846.4 examples/sec; 0.151 sec/batch)
2017-05-09 02:19:48.618913: step 45590, loss = 0.84 (836.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:19:50.233298: step 45600, loss = 1.00 (792.9 examples/sec; 0.161 sec/batch)
2017-05-09 02:19:51.671520: step 45610, loss = 0.63 (890.0 examples/sec; 0.144 sec/batch)
2017-05-09 02:19:53.166858: step 45620, loss = 0.78 (856.0 examples/sec; 0.150 sec/batch)
2017-05-09 02:19:54.661272: step 45630, loss = 0.74 (856.5 examples/sec; 0.149 sec/batch)
2017-05-09 02:19:56.163070: step 45640, loss = 0.83 (852.3 examples/sec; 0.150 sec/batch)
2017-05-09 02:19:57.658413: step 45650, loss = 0.73 (856.0 examples/sec; 0.150 sec/batch)
2017-05-09 02:19:59.168253: step 45660, loss = 0.95 (847.8 examples/sec; 0.151 sec/batch)
2017-05-09 02:20:00.667294: step 45670, loss = 0.80 (853.9 examples/sec; 0.150 sec/batch)
2017-05-09 02:20:02.170483: step 45680, loss = 0.88 (851.5 examples/sec; 0.150 sec/batch)
2017-05-09 02:20:03.693622: step 45690, loss = 0.83 (840.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:20:05.287452: step 45700, loss = 0.80 (803.1 examples/sec; 0.159 sec/batch)
2017-05-09 02:20:06.676370: step 45710, loss = 0.82 (921.6 examples/sec; 0.139 sec/batch)
2017-05-09 02:20:08.174649: step 45720, loss = 0.71 (854.3 examples/sec; 0.150 sec/batch)
2017-05-09 02:20:09.690482: step 45730, loss = 0.71 (844.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:20:11.228261: step 45740, loss = 0.73 (832.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:20:12.764508: step 45750, loss = 0.87 (833.2 examples/sec; 0.154 sec/batch)
2017-05-09 02:20:14.267982: step 45760, loss = 0.76 (851.4 examples/sec; 0.150 sec/batch)
2017-05-09 02:20:15.814745: step 45770, loss = 0.83 (827.5 examples/sec; 0.155 sec/batch)
2017-05-09 02:20:17.423123: step 45780, loss = 0.68 (795.8 examples/sec; 0.161 sec/batch)
2017-05-09 02:20:18.957681: step 45790, loss = 0.75 (834.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:20:20.602477: step 45800, loss = 0.79 (778.2 examples/sec; 0.164 sec/batch)
2017-05-09 02:20:22.046404: step 45810, loss = 0.71 (886.5 examples/sec; 0.144 sec/batch)
2017-05-09 02:20:23.547467: step 45820, loss = 0.70 (852.7 examples/sec; 0.150 sec/batch)
2017-05-09 02:20:25.092485: step 45830, loss = 0.94 (828.5 examples/sec; 0.155 sec/batch)
2017-05-09 02:20:26.610727: step 45840, loss = 1.04 (843.1 examples/sec; 0.152 sec/batch)
2017-05-09 02:20:28.191107: step 45850, loss = 0.68 (809.9 examples/sec; 0.158 sec/batch)
2017-05-09 02:20:29.780954: step 45860, loss = 0.92 (805.1 examples/sec; 0.159 sec/batch)
2017-05-09 02:20:31.325399: step 45870, loss = 0.87 (828.8 examples/sec; 0.154 sec/batch)
2017-05-09 02:20:32.863186: step 45880, loss = 0.91 (832.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:20:34.418455: step 45890, loss = 0.77 (823.0 examples/sec; 0.156 sec/batch)
2017-05-09 02:20:36.055205: step 45900, loss = 0.90 (782.0 examples/sec; 0.164 sec/batch)
2017-05-09 02:20:37.513122: step 45910, loss = 1.07 (878.0 examples/sec; 0.146 sec/batch)
2017-05-09 02:20:39.070665: step 45920, loss = 0.75 (821.8 examples/sec; 0.156 sec/batch)
2017-05-09 02:20:40.611427: step 45930, loss = 1.01 (830.8 examples/sec; 0.154 sec/batch)
2017-05-09 02:20:42.150902: step 45940, loss = 0.79 (831.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:20:43.681434: step 45950, loss = 0.71 (836.3 examples/sec; 0.153 sec/batch)
2017-05-09 02:20:45.181393: step 45960, loss = 0.77 (853.4 examples/sec; 0.150 sec/batch)
2017-05-09 02:20:46.698789: step 45970, loss = 0.68 (843.5 examples/sec; 0.152 sec/batch)
2017-05-09 02:20:48.263343: step 45980, loss = 0.68 (818.1 examples/sec; 0.156 sec/batch)
2017-05-09 02:20:49.827858: step 45990, loss = 0.75 (818.1 examples/sec; 0.156 sec/batch)
2017-05-09 02:20:51.482558: step 46000, loss = 0.75 (773.6 examples/sec; 0.165 sec/batch)
2017-05-09 02:20:52.956381: step 46010, loss = 0.76 (868.5 examples/sec; 0.147 sec/batch)
2017-05-09 02:20:54.458179: step 46020, loss = 0.72 (852.3 examples/sec; 0.150 sec/batch)
2017-05-09 02:20:55.999901: step 46030, loss = 0.84 (830.2 examples/sec; 0.154 sec/batch)
2017-05-09 02:20:57.516449: step 46040, loss = 0.98 (844.0 examples/sec; 0.152 sec/batch)
2017-05-09 02:20:59.049398: step 46050, loss = 0.79 (835.0 examples/sec; 0.153 sec/batch)
2017-05-09 02:21:00.582629: step 46060, loss = 0.62 (834.8 examples/sec; 0.153 sec/batch)
2017-05-09 02:21:02.080694: step 46070, loss = 0.75 (854.4 examples/sec; 0.150 sec/batch)
2017-05-09 02:21:03.591212: step 46080, loss = 0.72 (847.4 examples/sec; 0.151 sec/batch)
2017-05-09 02:21:05.138580: step 46090, loss = 0.66 (827.2 examples/sec; 0.155 sec/batch)
2017-05-09 02:21:06.797941: step 46100, loss = 0.79 (771.4 examples/sec; 0.166 sec/batch)
2017-05-09 02:21:08.248667: step 46110, loss = 0.81 (882.3 examples/sec; 0.145 sec/batch)
2017-05-09 02:21:09.753067: step 46120, loss = 0.88 (850.8 examples/sec; 0.150 sec/batch)
2017-05-09 02:21:11.307651: step 46130, loss = 0.99 (823.4 examples/sec; 0.155 sec/batch)
2017-05-09 02:21:12.828175: step 46140, loss = 0.77 (841.8 examples/sec; 0.152 sec/batch)
2017-05-09 02:21:14.373254: step 46150, loss = 0.71 (828.4 examples/sec; 0.155 sec/batch)
2017-05-09 02:21:15.908019: step 46160, loss = 0.79 (834.0 examples/sec; 0.153 sec/batch)
2017-05-09 02:21:17.417441: step 46170, loss = 0.84 (848.0 examples/sec; 0.151 sec/batch)
2017-05-09 02:21:18.971404: step 46180, loss = 0.74 (823.7 examples/sec; 0.155 sec/batch)
2017-05-09 02:21:20.531912: step 46190, loss = 0.78 (820.2 examples/sec; 0.156 sec/batch)
2017-05-09 02:21:22.171912: step 46200, loss = 0.67 (780.5 examples/sec; 0.164 sec/batch)
2017-05-09 02:21:23.607265: step 46210, loss = 0.68 (891.8 examples/sec; 0.144 sec/batch)
2017-05-09 02:21:25.161285: step 46220, loss = 0.91 (823.7 examples/sec; 0.155 sec/batch)
2017-05-09 02:21:26.660229: step 46230, loss = 0.92 (853.9 examples/sec; 0.150 sec/batch)
2017-05-09 02:21:28.189727: step 46240, loss = 0.75 (836.9 examples/sec; 0.153 sec/batch)
2017-05-09 02:21:29.719131: step 46250, loss = 0.82 (836.9 examples/sec; 0.153 sec/batch)
2017-05-09 02:21:31.252198: step 46260, loss = 0.90 (834.9 examples/sec; 0.153 sec/batch)
2017-05-09 02:21:32.796613: step 46270, loss = 0.94 (828.8 examples/sec; 0.154 sec/batch)
2017-05-09 02:21:34.368294: step 46280, loss = 0.74 (814.4 examples/sec; 0.157 sec/batch)
2017-05-09 02:21:35.883481: step 46290, loss = 0.85 (844.8 examples/sec; 0.152 sec/batch)
2017-05-09 02:21:37.505823: step 46300, loss = 0.72 (789.0 examples/sec; 0.162 sec/batch)
2017-05-09 02:21:38.911449: step 46310, loss = 0.75 (910.6 examples/sec; 0.141 sec/batch)
2017-05-09 02:21:40.482542: step 46320, loss = 0.82 (814.7 examples/sec; 0.157 sec/batch)
2017-05-09 02:21:42.009622: step 46330, loss = 0.64 (838.2 examples/sec; 0.153 sec/batch)
2017-05-09 02:21:43.531831: step 46340, loss = 0.81 (840.9 examples/sec; 0.152 sec/batch)
2017-05-09 02:21:45.049485: step 46350, loss = 0.76 (843.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:21:46.586519: step 46360, loss = 0.79 (832.8 examples/sec; 0.154 sec/batch)
2017-05-09 02:21:48.161154: step 46370, loss = 0.86 (812.9 examples/sec; 0.157 sec/batch)
2017-05-09 02:21:49.679004: step 46380, loss = 0.63 (843.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:21:51.219260: step 46390, loss = 0.57 (831.0 examples/sec; 0.154 sec/bE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 947 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
atch)
2017-05-09 02:21:52.826589: step 46400, loss = 0.92 (796.4 examples/sec; 0.161 sec/batch)
2017-05-09 02:21:54.269264: step 46410, loss = 0.87 (887.2 examples/sec; 0.144 sec/batch)
2017-05-09 02:21:55.815941: step 46420, loss = 0.81 (827.6 examples/sec; 0.155 sec/batch)
2017-05-09 02:21:57.342357: step 46430, loss = 0.83 (838.6 examples/sec; 0.153 sec/batch)
2017-05-09 02:21:58.915046: step 46440, loss = 0.79 (813.9 examples/sec; 0.157 sec/batch)
2017-05-09 02:22:00.431287: step 46450, loss = 1.02 (844.2 examples/sec; 0.152 sec/batch)
2017-05-09 02:22:01.934010: step 46460, loss = 0.69 (851.8 examples/sec; 0.150 sec/batch)
2017-05-09 02:22:03.474870: step 46470, loss = 0.68 (830.7 examples/sec; 0.154 sec/batch)
2017-05-09 02:22:05.001537: step 46480, loss = 0.89 (838.4 examples/sec; 0.153 sec/batch)
2017-05-09 02:22:06.534549: step 46490, loss = 0.86 (835.0 examples/sec; 0.153 sec/batch)
2017-05-09 02:22:08.168766: step 46500, loss = 0.90 (783.3 examples/sec; 0.163 sec/batch)
2017-05-09 02:22:09.618534: step 46510, loss = 0.76 (882.9 examples/sec; 0.145 sec/batch)
2017-05-09 02:22:11.209908: step 46520, loss = 0.70 (804.3 examples/sec; 0.159 sec/batch)
2017-05-09 02:22:12.770545: step 46530, loss = 0.87 (820.2 examples/sec; 0.156 sec/batch)
2017-05-09 02:22:14.334303: step 46540, loss = 0.75 (818.5 examples/sec; 0.156 sec/batch)
2017-05-09 02:22:15.861557: step 46550, loss = 0.79 (838.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:22:17.438826: step 46560, loss = 0.82 (811.5 examples/sec; 0.158 sec/batch)
2017-05-09 02:22:19.017910: step 46570, loss = 0.71 (810.6 examples/sec; 0.158 sec/batch)
2017-05-09 02:22:20.554515: step 46580, loss = 0.75 (833.0 examples/sec; 0.154 sec/batch)
2017-05-09 02:22:22.084498: step 46590, loss = 0.88 (836.6 examples/sec; 0.153 sec/batch)
2017-05-09 02:22:23.718505: step 46600, loss = 0.76 (783.4 examples/sec; 0.163 sec/batch)
2017-05-09 02:22:25.202096: step 46610, loss = 0.71 (862.8 examples/sec; 0.148 sec/batch)
2017-05-09 02:22:26.733715: step 46620, loss = 1.18 (835.7 examples/sec; 0.153 sec/batch)
2017-05-09 02:22:28.290144: step 46630, loss = 0.94 (822.4 examples/sec; 0.156 sec/batch)
2017-05-09 02:22:29.828852: step 46640, loss = 0.69 (831.9 examples/sec; 0.154 sec/batch)
2017-05-09 02:22:31.337431: step 46650, loss = 0.81 (848.5 examples/sec; 0.151 sec/batch)
2017-05-09 02:22:32.902865: step 46660, loss = 0.76 (817.7 examples/sec; 0.157 sec/batch)
2017-05-09 02:22:34.466986: step 46670, loss = 0.76 (818.4 examples/sec; 0.156 sec/batch)
2017-05-09 02:22:36.044052: step 46680, loss = 0.76 (811.6 examples/sec; 0.158 sec/batch)
2017-05-09 02:22:37.588413: step 46690, loss = 1.01 (828.8 examples/sec; 0.154 sec/batch)
2017-05-09 02:22:39.242948: step 46700, loss = 0.75 (773.6 examples/sec; 0.165 sec/batch)
2017-05-09 02:22:40.699205: step 46710, loss = 0.79 (879.0 examples/sec; 0.146 sec/batch)
2017-05-09 02:22:42.227117: step 46720, loss = 0.84 (837.7 examples/sec; 0.153 sec/batch)
2017-05-09 02:22:43.781149: step 46730, loss = 0.67 (823.7 examples/sec; 0.155 sec/batch)
2017-05-09 02:22:45.343839: step 46740, loss = 0.66 (819.1 examples/sec; 0.156 sec/batch)
2017-05-09 02:22:46.904359: step 46750, loss = 0.77 (820.2 examples/sec; 0.156 sec/batch)
2017-05-09 02:22:48.421546: step 46760, loss = 0.77 (843.7 examples/sec; 0.152 sec/batch)
2017-05-09 02:22:49.946098: step 46770, loss = 0.82 (839.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:22:51.481097: step 46780, loss = 0.85 (833.9 examples/sec; 0.154 sec/batch)
2017-05-09 02:22:53.021210: step 46790, loss = 0.91 (831.1 examples/sec; 0.154 sec/batch)
2017-05-09 02:22:54.637993: step 46800, loss = 0.87 (791.7 examples/sec; 0.162 sec/batch)
2017-05-09 02:22:56.073983: step 46810, loss = 0.65 (891.4 examples/sec; 0.144 sec/batch)
2017-05-09 02:22:57.589614: step 46820, loss = 0.91 (844.5 examples/sec; 0.152 sec/batch)
2017-05-09 02:22:59.104217: step 46830, loss = 0.67 (845.1 examples/sec; 0.151 sec/batch)
2017-05-09 02:23:00.631876: step 46840, loss = 0.72 (837.9 examples/sec; 0.153 sec/batch)
2017-05-09 02:23:02.174298: step 46850, loss = 0.64 (829.9 examples/sec; 0.154 sec/batch)
2017-05-09 02:23:03.702501: step 46860, loss = 0.93 (837.6 examples/sec; 0.153 sec/batch)
2017-05-09 02:23:05.252180: step 46870, loss = 0.87 (826.0 examples/sec; 0.155 sec/batch)
2017-05-09 02:23:06.795141: step 46880, loss = 0.92 (829.6 examples/sec; 0.154 sec/batch)
2017-05-09 02:23:08.319959: step 46890, loss = 0.69 (839.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:23:09.950367: step 46900, loss = 0.71 (785.1 examples/sec; 0.163 sec/batch)
2017-05-09 02:23:11.413006: step 46910, loss = 0.81 (875.1 examples/sec; 0.146 sec/batch)
2017-05-09 02:23:12.938918: step 46920, loss = 0.71 (838.8 examples/sec; 0.153 sec/batch)
2017-05-09 02:23:14.487425: step 46930, loss = 0.74 (826.6 examples/sec; 0.155 sec/batch)
2017-05-09 02:23:16.025519: step 46940, loss = 0.67 (832.2 examples/sec; 0.154 sec/batch)
2017-05-09 02:23:17.553567: step 46950, loss = 0.90 (837.7 examples/sec; 0.153 sec/batch)
2017-05-09 02:23:19.137176: step 46960, loss = 0.74 (808.3 examples/sec; 0.158 sec/batch)
2017-05-09 02:23:20.672437: step 46970, loss = 1.03 (833.7 examples/sec; 0.154 sec/batch)
2017-05-09 02:23:22.202764: step 46980, loss = 1.01 (836.4 examples/sec; 0.153 sec/batch)
2017-05-09 02:23:23.754291: step 46990, loss = 0.83 (825.0 examples/sec; 0.155 sec/batch)
2017-05-09 02:23:25.401945: step 47000, loss = 0.72 (776.9 examples/sec; 0.165 sec/batch)
2017-05-09 02:23:26.841142: step 47010, loss = 0.72 (889.4 examples/sec; 0.144 sec/batch)
2017-05-09 02:23:28.365663: step 47020, loss = 0.78 (839.6 examples/sec; 0.152 sec/batch)
2017-05-09 02:23:29.909386: step 47030, loss = 0.89 (829.2 examples/sec; 0.154 sec/batch)
2017-05-09 02:23:31.451238: step 47040, loss = 0.77 (830.2 examples/sec; 0.154 sec/batch)
2017-05-09 02:23:32.989184: step 47050, loss = 0.92 (832.3 examples/sec; 0.154 sec/batch)
2017-05-09 02:23:34.523655: step 47060, loss = 0.93 (834.2 examples/sec; 0.153 sec/batch)
2017-05-09 02:23:36.051818: step 47070, loss = 0.70 (837.6 examples/sec; 0.153 sec/batch)
2017-05-09 02:23:37.577157: step 47080, loss = 0.72 (839.2 examples/sec; 0.153 sec/batch)
2017-05-09 02:23:39.149462: step 47090, loss = 0.98 (814.1 examples/sec; 0.157 sec/batch)
2017-05-09 02:23:40.787845: step 47100, loss = 0.74 (781.3 examples/sec; 0.164 sec/batch)
2017-05-09 02:23:42.247711: step 47110, loss = 0.72 (876.8 examples/sec; 0.146 sec/batch)
2017-05-09 02:23:43.827553: step 47120, loss = 0.79 (810.2 examples/sec; 0.158 sec/batch)
2017-05-09 02:23:45.349666: step 47130, loss = 0.87 (840.9 examples/sec; 0.152 sec/batch)
2017-05-09 02:23:46.866355: step 47140, loss = 0.76 (844.0 examples/sec; 0.152 sec/batch)
2017-05-09 02:23:48.396430: step 47150, loss = 0.86 (836.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:23:49.958937: step 47160, loss = 0.81 (819.2 examples/sec; 0.156 sec/batch)
2017-05-09 02:23:51.496527: step 47170, loss = 0.81 (832.5 examples/sec; 0.154 sec/batch)
2017-05-09 02:23:53.041018: step 47180, loss = 0.75 (828.8 examples/sec; 0.154 sec/batch)
2017-05-09 02:23:54.569099: step 47190, loss = 0.72 (837.6 examples/sec; 0.153 sec/batch)
2017-05-09 02:23:56.216486: step 47200, loss = 0.95 (777.0 examples/sec; 0.165 sec/batch)
2017-05-09 02:23:57.668575: step 47210, loss = 0.87 (881.5 examples/sec; 0.145 sec/batch)
2017-05-09 02:23:59.184533: step 47220, loss = 0.90 (844.4 examples/sec; 0.152 sec/batch)
2017-05-09 02:24:00.714433: step 47230, loss = 0.78 (836.7 examples/sec; 0.153 sec/batch)
2017-05-09 02:24:02.236812: step 47240, loss = 0.73 (840.8 examples/sec; 0.152 sec/batch)
2017-05-09 02:24:03.748516: step 47250, loss = 0.84 (846.7 examples/sec; 0.151 sec/batch)
2017-05-09 02:24:05.290964: step 47260, loss = 0.89 (829.8 examples/sec; 0.154 sec/batch)
2017-05-09 02:24:06.821811: step 47270, loss = 0.82 (836.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:24:08.376585: step 47280, loss = 0.92 (823.3 examples/sec; 0.155 sec/batch)
2017-05-09 02:24:09.945648: step 47290, loss = 0.80 (815.8 examples/sec; 0.157 sec/batch)
2017-05-09 02:24:11.554712: step 47300, loss = 0.70 (795.5 examples/sec; 0.161 sec/batE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 963 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
ch)
2017-05-09 02:24:13.007518: step 47310, loss = 0.77 (881.1 examples/sec; 0.145 sec/batch)
2017-05-09 02:24:14.511788: step 47320, loss = 0.92 (850.9 examples/sec; 0.150 sec/batch)
2017-05-09 02:24:16.084739: step 47330, loss = 1.03 (813.8 examples/sec; 0.157 sec/batch)
2017-05-09 02:24:17.613147: step 47340, loss = 0.77 (837.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:24:19.140409: step 47350, loss = 0.73 (838.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:24:20.683847: step 47360, loss = 0.76 (829.3 examples/sec; 0.154 sec/batch)
2017-05-09 02:24:22.199884: step 47370, loss = 0.92 (844.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:24:23.739515: step 47380, loss = 0.92 (831.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:24:25.275077: step 47390, loss = 0.88 (833.6 examples/sec; 0.154 sec/batch)
2017-05-09 02:24:26.927054: step 47400, loss = 0.71 (774.8 examples/sec; 0.165 sec/batch)
2017-05-09 02:24:28.361488: step 47410, loss = 0.89 (892.3 examples/sec; 0.143 sec/batch)
2017-05-09 02:24:29.892563: step 47420, loss = 0.69 (836.0 examples/sec; 0.153 sec/batch)
2017-05-09 02:24:31.427157: step 47430, loss = 0.84 (834.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:24:32.953370: step 47440, loss = 0.86 (838.7 examples/sec; 0.153 sec/batch)
2017-05-09 02:24:34.516041: step 47450, loss = 0.72 (819.1 examples/sec; 0.156 sec/batch)
2017-05-09 02:24:36.036516: step 47460, loss = 0.65 (841.8 examples/sec; 0.152 sec/batch)
2017-05-09 02:24:37.577502: step 47470, loss = 0.82 (830.6 examples/sec; 0.154 sec/batch)
2017-05-09 02:24:39.134598: step 47480, loss = 1.01 (822.0 examples/sec; 0.156 sec/batch)
2017-05-09 02:24:40.671585: step 47490, loss = 0.86 (832.8 examples/sec; 0.154 sec/batch)
2017-05-09 02:24:42.318973: step 47500, loss = 0.77 (777.0 examples/sec; 0.165 sec/batch)
2017-05-09 02:24:43.761490: step 47510, loss = 0.71 (887.3 examples/sec; 0.144 sec/batch)
2017-05-09 02:24:45.280012: step 47520, loss = 0.60 (842.9 examples/sec; 0.152 sec/batch)
2017-05-09 02:24:46.841931: step 47530, loss = 1.02 (819.5 examples/sec; 0.156 sec/batch)
2017-05-09 02:24:48.371372: step 47540, loss = 0.65 (836.9 examples/sec; 0.153 sec/batch)
2017-05-09 02:24:49.938941: step 47550, loss = 0.81 (816.6 examples/sec; 0.157 sec/batch)
2017-05-09 02:24:51.488545: step 47560, loss = 0.92 (826.0 examples/sec; 0.155 sec/batch)
2017-05-09 02:24:53.029979: step 47570, loss = 0.93 (830.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:24:54.593543: step 47580, loss = 1.00 (818.6 examples/sec; 0.156 sec/batch)
2017-05-09 02:24:56.157433: step 47590, loss = 0.87 (818.5 examples/sec; 0.156 sec/batch)
2017-05-09 02:24:57.783739: step 47600, loss = 0.79 (787.1 examples/sec; 0.163 sec/batch)
2017-05-09 02:24:59.263195: step 47610, loss = 0.82 (865.2 examples/sec; 0.148 sec/batch)
2017-05-09 02:25:00.808894: step 47620, loss = 0.80 (828.1 examples/sec; 0.155 sec/batch)
2017-05-09 02:25:02.394221: step 47630, loss = 0.76 (807.4 examples/sec; 0.159 sec/batch)
2017-05-09 02:25:03.979299: step 47640, loss = 0.93 (807.5 examples/sec; 0.159 sec/batch)
2017-05-09 02:25:05.550397: step 47650, loss = 0.73 (814.7 examples/sec; 0.157 sec/batch)
2017-05-09 02:25:07.070063: step 47660, loss = 0.76 (842.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:25:08.628560: step 47670, loss = 0.69 (821.3 examples/sec; 0.156 sec/batch)
2017-05-09 02:25:10.175491: step 47680, loss = 0.76 (827.4 examples/sec; 0.155 sec/batch)
2017-05-09 02:25:11.694091: step 47690, loss = 0.69 (842.9 examples/sec; 0.152 sec/batch)
2017-05-09 02:25:13.352498: step 47700, loss = 0.73 (771.8 examples/sec; 0.166 sec/batch)
2017-05-09 02:25:14.780963: step 47710, loss = 0.67 (896.1 examples/sec; 0.143 sec/batch)
2017-05-09 02:25:16.330664: step 47720, loss = 0.70 (826.0 examples/sec; 0.155 sec/batch)
2017-05-09 02:25:17.856798: step 47730, loss = 0.86 (838.7 examples/sec; 0.153 sec/batch)
2017-05-09 02:25:19.416024: step 47740, loss = 0.64 (820.9 examples/sec; 0.156 sec/batch)
2017-05-09 02:25:20.968599: step 47750, loss = 0.79 (824.4 examples/sec; 0.155 sec/batch)
2017-05-09 02:25:22.505600: step 47760, loE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 980 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
ss = 0.78 (832.8 examples/sec; 0.154 sec/batch)
2017-05-09 02:25:24.044611: step 47770, loss = 0.81 (831.7 examples/sec; 0.154 sec/batch)
2017-05-09 02:25:25.557752: step 47780, loss = 0.67 (845.9 examples/sec; 0.151 sec/batch)
2017-05-09 02:25:27.120046: step 47790, loss = 0.73 (819.3 examples/sec; 0.156 sec/batch)
2017-05-09 02:25:28.753770: step 47800, loss = 0.89 (783.5 examples/sec; 0.163 sec/batch)
2017-05-09 02:25:30.214590: step 47810, loss = 0.76 (876.2 examples/sec; 0.146 sec/batch)
2017-05-09 02:25:31.754602: step 47820, loss = 0.88 (831.2 examples/sec; 0.154 sec/batch)
2017-05-09 02:25:33.325857: step 47830, loss = 0.97 (814.6 examples/sec; 0.157 sec/batch)
2017-05-09 02:25:34.884291: step 47840, loss = 0.85 (821.3 examples/sec; 0.156 sec/batch)
2017-05-09 02:25:36.441820: step 47850, loss = 0.77 (821.8 examples/sec; 0.156 sec/batch)
2017-05-09 02:25:37.990424: step 47860, loss = 0.73 (826.6 examples/sec; 0.155 sec/batch)
2017-05-09 02:25:39.548648: step 47870, loss = 0.83 (821.4 examples/sec; 0.156 sec/batch)
2017-05-09 02:25:41.098769: step 47880, loss = 1.13 (825.7 examples/sec; 0.155 sec/batch)
2017-05-09 02:25:42.624426: step 47890, loss = 0.77 (839.0 examples/sec; 0.153 sec/batch)
2017-05-09 02:25:44.273199: step 47900, loss = 0.80 (776.3 examples/sec; 0.165 sec/batch)
2017-05-09 02:25:45.712006: step 47910, loss = 0.72 (889.6 examples/sec; 0.144 sec/batch)
2017-05-09 02:25:47.260175: step 47920, loss = 0.95 (826.8 examples/sec; 0.155 sec/batch)
2017-05-09 02:25:48.821081: step 47930, loss = 0.88 (820.0 examples/sec; 0.156 sec/batch)
2017-05-09 02:25:50.325071: step 47940, loss = 0.73 (851.1 examples/sec; 0.150 sec/batch)
2017-05-09 02:25:51.869857: step 47950, loss = 0.64 (828.6 examples/sec; 0.154 sec/batch)
2017-05-09 02:25:53.373353: step 47960, loss = 0.97 (851.4 examples/sec; 0.150 sec/batch)
2017-05-09 02:25:54.888978: step 47970, loss = 0.87 (844.5 examples/sec; 0.152 sec/batch)
2017-05-09 02:25:56.401783: step 47980, loss = 0.74 (846.1 examples/sec; 0.151 sec/batch)
2017-05-09 02:25:57.973808: step 47990, loss = 0.63 (814.2 examples/sec; 0.157 sec/batch)
2017-05-09 02:25:59.589894: step 48000, loss = 0.69 (792.0 examples/sec; 0.162 sec/batch)
2017-05-09 02:26:01.018166: step 48010, loss = 0.70 (896.2 examples/sec; 0.143 sec/batch)
2017-05-09 02:26:02.577124: step 48020, loss = 0.67 (821.1 examples/sec; 0.156 sec/batch)
2017-05-09 02:26:04.067580: step 48030, loss = 0.95 (858.8 examples/sec; 0.149 sec/batch)
2017-05-09 02:26:05.592283: step 48040, loss = 0.87 (839.5 examples/sec; 0.152 sec/batch)
2017-05-09 02:26:07.155645: step 48050, loss = 0.75 (818.7 examples/sec; 0.156 sec/batch)
2017-05-09 02:26:08.704941: step 48060, loss = 0.86 (826.2 examples/sec; 0.155 sec/batch)
2017-05-09 02:26:10.256879: step 48070, loss = 0.77 (824.8 examples/sec; 0.155 sec/batch)
2017-05-09 02:26:11.788445: step 48080, loss = 0.77 (835.7 examples/sec; 0.153 sec/batch)
2017-05-09 02:26:13.322966: step 48090, loss = 0.76 (834.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:26:14.969138: step 48100, loss = 0.76 (777.6 examples/sec; 0.165 sec/batch)
2017-05-09 02:26:16.449358: step 48110, loss = 0.91 (864.7 examples/sec; 0.148 sec/batch)
2017-05-09 02:26:18.035532: step 48120, loss = 0.78 (807.0 examples/sec; 0.159 sec/batch)
2017-05-09 02:26:19.563828: step 48130, loss = 0.69 (837.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:26:21.122005: step 48140, loss = 0.91 (821.5 examples/sec; 0.156 sec/batch)
2017-05-09 02:26:22.681230: step 48150, loss = 0.92 (820.9 examples/sec; 0.156 sec/batch)
2017-05-09 02:26:24.228782: step 48160, loss = 0.74 (827.1 examples/sec; 0.155 sec/batch)
2017-05-09 02:26:25.815952: step 48170, loss = 0.67 (806.5 examples/sec; 0.159 sec/batch)
2017-05-09 02:26:27.368497: step 48180, loss = 0.76 (824.5 examples/sec; 0.155 sec/batch)
2017-05-09 02:26:28.907128: step 48190, loss = 0.81 (831.9 examples/sec; 0.154 sec/batch)
2017-05-09 02:26:30.530373: step 48200, loss = 0.76 (788.5 examples/sec; 0.162 sec/batch)
2017-05-09 02:26:31.962061: step 48210, loss = 0.63 (894.0 examples/sec; 0.143 sec/batch)
2017-05-09 02:26:33.514857: step 48220, loss = 0.71 (824.3 examples/sec; 0.155 sec/batch)
2017-05-09 02:26:35.056079: step 48230, loss = 0.78 (830.5 examples/sec; 0.154 sec/batch)
2017-05-09 02:26:36.608731: step 48240, loss = 1.00 (824.4 examples/sec; 0.155 sec/batch)
2017-05-09 02:26:38.162236: step 48250, loss = 0.76 (823.9 examples/sec; 0.155 sec/batch)
2017-05-09 02:26:39.729939: step 48260, loss = 0.87 (816.5 examples/sec; 0.157 sec/batch)
2017-05-09 02:26:41.298982: step 48270, loss = 0.86 (815.8 examples/sec; 0.157 sec/batch)
2017-05-09 02:26:42.866731: step 48280, loss = 0.74 (816.5 examples/sec; 0.157 sec/batch)
2017-05-09 02:26:44.435160: step 48290, loss = 0.78 (816.1 examples/sec; 0.157 sec/batch)
2017-05-09 02:26:46.077189: step 48300, loss = 0.63 (779.5 examples/sec; 0.164 sec/batch)
2017-05-09 02:26:47.538740: step 48310, loss = 0.73 (875.8 examples/sec; 0.146 sec/batch)
2017-05-09 02:26:49.079905: step 48320, loss = 0.79 (830.5 examples/sec; 0.154 sec/batch)
2017-05-09 02:26:50.632208: step 48330, loss = 0.62 (824.6 examples/sec; 0.155 sec/batch)
2017-05-09 02:26:52.165401: step 48340, loss = 0.90 (834.9 examples/sec; 0.153 sec/batch)
2017-05-09 02:26:53.723176: step 48350, loss = 0.80 (821.7 examples/sec; 0.156 sec/batch)
2017-05-09 02:26:55.277378: step 48360, loss = 0.74 (823.6 examples/sec; 0.155 sec/batch)
2017-05-09 02:26:56.857467: step 48370, loss = 1.10 (810.1 examples/sec; 0.158 sec/batch)
2017-05-09 02:26:58.397763: step 48380, loss = 0.68 (831.0 examples/sec; 0.154 sec/batch)
2017-05-09 02:26:59.948745: step 48390, loss = 0.90 (825.3 examples/sec; 0.155 sec/batch)
2017-05-09 02:27:01.613854: step 48400, loss = 0.75 (768.7 examples/sec; 0.167 sec/batch)
2017-05-09 02:27:03.078964: step 48410, loss = 0.91 (873.7 examples/sec; 0.147 sec/batch)
2017-05-09 02:27:04.635435: step 48420, loss = 0.71 (822.4 examples/sec; 0.156 sec/batch)
2017-05-09 02:27:06.162719: step 48430, loss = 1.05 (838.1 examples/sec; 0.153 sec/batch)
2017-05-09 02:27:07.703666: step 48440, loss = 0.76 (830.7 examples/sec; 0.154 sec/batch)
2017-05-09 02:27:09.284257: step 48450, loss = 1.09 (809.8 examples/sec; 0.158 sec/batch)
2017-05-09 02:27:10.839768: step 48460, loss = 0.96 (822.9 examples/sec; 0.156 sec/batch)
2017-05-09 02:27:12.384611: step 48470, loss = 0.66 (828.6 examples/sec; 0.154 sec/batch)
2017-05-09 02:27:13.922590: step 48480, loss = 0.82 (832.3 examples/sec; 0.154 sec/batch)
2017-05-09 02:27:15.472826: step 48490, loss = 0.65 (825.7 examples/sec; 0.155 sec/batch)
2017-05-09 02:27:17.107869: step 48500, loss = 0.67 (782.9 examples/sec; 0.164 sec/batch)
2017-05-09 02:27:18.556882: step 48510, loss = 0.82 (883.4 examples/sec; 0.145 sec/batch)
2017-05-09 02:27:20.134076: step 48520, loss = 0.78 (811.6 examples/sec; 0.158 sec/batch)
2017-05-09 02:27:21.682302: step 48530, loss = 0.69 (826.7 examples/sec; 0.155 sec/batch)
2017-05-09 02:27:23.212148: step 48540, loss = 0.85 (836.7 examples/sec; 0.153 sec/batch)
2017-05-09 02:27:24.744461: step 48550, loss = 0.80 (835.3 examples/sec; 0.153 sec/batch)
2017-05-09 02:27:26.284086: step 48560, loss = 0.79 (831.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:27:27.842554: step 48570, loss = 0.82 (821.3 examples/sec; 0.156 sec/batch)
2017-05-09 02:27:29.397050: step 48580, loss = 1.11 (823.4 examples/sec; 0.155 sec/batch)
2017-05-09 02:27:30.956285: step 48590, loss = 0.78 (820.9 examples/sec; 0.156 sec/batch)
2017-05-09 02:27:32.588976: step 48600, loss = 0.80 (784.0 examples/sec; 0.163 sec/batch)
2017-05-09 02:27:34.025340: step 48610, loss = 0.90 (891.2 examples/sec; 0.144 sec/batch)
2017-05-09 02:27:35.578996: step 48620, loss = 0.75 (823.8 examples/sec; 0.155 sec/batch)
2017-05-09 02:27:37.135563: step 48630, loss = 0.68 (822.3 examples/sec; 0.156 sec/batch)
2017-05-09 02:27:38.652593: step 48640, loss = 0.83 (843.8 examples/sec; 0.152 sec/batch)
2017-05-09 02:27:40.202428: step 48650, loss = 0.98 (825.9 examples/sec; 0.155 sec/batch)
2017-05-09 02:27:41.756260: step 48660, loss = 0.82 (823.8 examples/sec; 0.155 sec/batch)
2017-05-09 02:27:43.309475: step 48670, lossE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 996 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
 = 0.95 (824.1 examples/sec; 0.155 sec/batch)
2017-05-09 02:27:44.870540: step 48680, loss = 0.86 (819.9 examples/sec; 0.156 sec/batch)
2017-05-09 02:27:46.431054: step 48690, loss = 0.89 (820.2 examples/sec; 0.156 sec/batch)
2017-05-09 02:27:48.051204: step 48700, loss = 0.70 (790.1 examples/sec; 0.162 sec/batch)
2017-05-09 02:27:49.486134: step 48710, loss = 0.77 (892.0 examples/sec; 0.143 sec/batch)
2017-05-09 02:27:51.030850: step 48720, loss = 0.67 (828.6 examples/sec; 0.154 sec/batch)
2017-05-09 02:27:52.581548: step 48730, loss = 0.83 (825.4 examples/sec; 0.155 sec/batch)
2017-05-09 02:27:54.133960: step 48740, loss = 0.81 (824.5 examples/sec; 0.155 sec/batch)
2017-05-09 02:27:55.695804: step 48750, loss = 0.77 (819.5 examples/sec; 0.156 sec/batch)
2017-05-09 02:27:57.251754: step 48760, loss = 0.96 (822.7 examples/sec; 0.156 sec/batch)
2017-05-09 02:27:58.799229: step 48770, loss = 0.85 (827.2 examples/sec; 0.155 sec/batch)
2017-05-09 02:28:00.351527: step 48780, loss = 0.90 (824.6 examples/sec; 0.155 sec/batch)
2017-05-09 02:28:01.926392: step 48790, loss = 0.78 (812.8 examples/sec; 0.157 sec/batch)
2017-05-09 02:28:03.625228: step 48800, loss = 0.84 (753.5 examples/sec; 0.170 sec/batch)
2017-05-09 02:28:05.104537: step 48810, loss = 0.84 (865.3 examples/sec; 0.148 sec/batch)
2017-05-09 02:28:06.664498: step 48820, loss = 0.94 (820.5 examples/sec; 0.156 sec/batch)
2017-05-09 02:28:08.225918: step 48830, loss = 0.83 (819.8 examples/sec; 0.156 sec/batch)
2017-05-09 02:28:09.797180: step 48840, loss = 0.86 (814.6 examples/sec; 0.157 sec/batch)
2017-05-09 02:28:11.345593: step 48850, loss = 0.84 (826.7 examples/sec; 0.155 sec/batch)
2017-05-09 02:28:12.880020: step 48860, loss = 0.65 (834.2 examples/sec; 0.153 sec/batch)
2017-05-09 02:28:14.400270: step 48870, loss = 0.73 (842.0 examples/sec; 0.152 sec/batch)
2017-05-09 02:28:15.935259: step 48880, loss = 0.76 (833.9 examples/sec; 0.153 sec/batch)
2017-05-09 02:28:17.476207: step 48890, loss = 0.80 (830.7 examples/sec; 0.154 sec/batch)
2017-05-09 02:28:19.104278: step 48900, loss = 0.93 (786.2 examples/sec; 0.163 sec/batch)
2017-05-09 02:28:20.582923: step 48910, loss = 0.76 (865.7 examples/sec; 0.148 sec/batch)
2017-05-09 02:28:22.150539: step 48920, loss = 0.73 (816.5 examples/sec; 0.157 sec/batch)
2017-05-09 02:28:23.688211: step 48930, loss = 0.81 (832.4 examples/sec; 0.154 sec/batch)
2017-05-09 02:28:25.275744: step 48940, loss = 0.86 (806.3 examples/sec; 0.159 sec/batch)
2017-05-09 02:28:26.811350: step 48950, loss = 0.81 (833.6 examples/sec; 0.154 sec/batch)
2017-05-09 02:28:28.373699: step 48960, loss = 0.77 (819.3 examples/sec; 0.156 sec/batch)
2017-05-09 02:28:29.909402: step 48970, loss = 0.78 (833.5 examples/sec; 0.154 sec/batch)
2017-05-09 02:28:31.463165: step 48980, loss = 0.68 (823.8 examples/sec; 0.155 sec/batch)
2017-05-09 02:28:32.979158: step 48990, loss = 0.91 (844.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:28:34.590609: step 49000, loss = 0.87 (794.3 examples/sec; 0.161 sec/batch)
2017-05-09 02:28:36.031934: step 49010, loss = 0.93 (888.1 examples/sec; 0.144 sec/batch)
2017-05-09 02:28:37.565883: step 49020, loss = 0.90 (834.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:28:39.085522: step 49030, loss = 0.84 (842.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:28:40.641421: step 49040, loss = 0.83 (822.7 examples/sec; 0.156 sec/batch)
2017-05-09 02:28:42.172859: step 49050, loss = 0.99 (835.8 examples/sec; 0.153 sec/batch)
2017-05-09 02:28:43.788397: step 49060, loss = 0.80 (792.3 examples/sec; 0.162 sec/batch)
2017-05-09 02:28:45.327078: step 49070, loss = 0.83 (831.9 examples/sec; 0.154 sec/batch)
2017-05-09 02:28:46.902238: step 49080, loss = 0.72 (812.6 examples/sec; 0.158 sec/batch)
2017-05-09 02:28:48.445652: step 49090, loss = 0.70 (829.3 examples/sec; 0.154 sec/batch)
2017-05-09 02:28:50.073539: step 49100, loss = 0.80 (786.3 examples/sec; 0.163 sec/batch)
2017-05-09 02:28:51.535467: step 49110, loss = 0.67 (875.6 examples/sec; 0.146 sec/batch)
2017-05-09 02:28:53.090472: step 49120, loss = 0.75 (823.1 examples/sec; 0.156 sec/batch)
2017-05-09 02:28:54.618851: step 49130, loss = 0.75 (837.5 examples/sec; 0.153 sec/batch)
2017-05-09 02:28:56.171943: step 49140, loss = 0.85 (824.2 examples/sec; 0.155 sec/batch)
2017-05-09 02:28:57.705427: step 49150, loss = 0.71 (834.7 examples/sec; 0.153 sec/batch)
2017-05-09 02:28:59.226430: step 49160, loss = 0.76 (841.5 examples/sec; 0.152 sec/batch)
2017-05-09 02:29:00.775174: step 49170, loss = 0.68 (826.5 examples/sec; 0.155 sec/batch)
2017-05-09 02:29:02.376717: step 49180, loss = 1.00 (799.2 examples/sec; 0.160 sec/batch)
2017-05-09 02:29:03.918041: step 49190, loss = 0.95 (830.5 examples/sec; 0.154 sec/batch)
2017-05-09 02:29:05.533990: step 49200, loss = 0.82 (792.1 examples/sec; 0.162 sec/batch)
2017-05-09 02:29:06.980304: step 49210, loss = 0.74 (885.0 examples/sec; 0.145 sec/batch)
2017-05-09 02:29:08.545277: step 49220, loss = 0.83 (817.9 examples/sec; 0.156 sec/batch)
2017-05-09 02:29:10.096126: step 49230, loss = 0.66 (825.4 examples/sec; 0.155 sec/batch)
2017-05-09 02:29:11.655112: step 49240, loss = 0.71 (821.0 examples/sec; 0.156 sec/batch)
2017-05-09 02:29:13.213368: step 49250, loss = 0.75 (821.4 examples/sec; 0.156 sec/batch)
2017-05-09 02:29:14.730200: step 49260, loss = 0.67 (843.9 examples/sec; 0.152 sec/batch)
2017-05-09 02:29:16.294826: step 49270, loss = 0.80 (818.1 examples/sec; 0.156 sec/batch)
2017-05-09 02:29:17.849677: step 49280, loss = 0.89 (823.2 examples/sec; 0.155 sec/batch)
2017-05-09 02:29:19.450293: step 49290, loss = 0.79 (799.7 examples/sec; 0.160 sec/batch)
2017-05-09 02:29:21.086550: step 49300, loss = 0.72 (782.3 examples/sec; 0.164 sec/batch)
2017-05-09 02:29:22.514883: step 49310, loss = 0.71 (896.1 examples/sec; 0.143 sec/batch)
2017-05-09 02:29:24.110465: step 49320, loss = 0.85 (802.2 examples/sec; 0.160 sec/batch)
2017-05-09 02:29:25.679814: step 49330, loss = 0.87 (815.6 examples/sec; 0.157 sec/batch)
2017-05-09 02:29:27.256020: step 49340, loss = 0.66 (812.1 examples/sec; 0.158 sec/batch)
2017-05-09 02:29:28.836200: step 49350, loss = 1.10 (810.0 examples/sec; 0.158 sec/batch)
2017-05-09 02:29:30.376889: step 49360, loss = 0.61 (830.8 examples/sec; 0.154 sec/batch)
2017-05-09 02:29:31.928308: step 49370, loss = 0.87 (825.1 examples/sec; 0.155 sec/batch)
2017-05-09 02:29:33.495426: step 49380, loss = 0.73 (816.8 examples/sec; 0.157 sec/batch)
2017-05-09 02:29:35.038398: step 49390, loss = 0.69 (829.6 examples/sec; 0.154 sec/batch)
2017-05-09 02:29:36.700335: step 49400, loss = 0.81 (770.2 examples/sec; 0.166 sec/batch)
2017-05-09 02:29:38.164370: step 49410, loss = 0.83 (874.3 examples/sec; 0.146 sec/batch)
2017-05-09 02:29:39.701990: step 49420, loss = 0.72 (832.5 examples/sec; 0.154 sec/batch)
2017-05-09 02:29:41.234565: step 49430, loss = 0.88 (835.2 examples/sec; 0.153 sec/batch)
2017-05-09 02:29:42.756049: step 49440, loss = 0.75 (841.3 examples/sec; 0.152 sec/batch)
2017-05-09 02:29:44.286660: step 49450, loss = 0.79 (836.3 examples/sec; 0.153 sec/batch)
2017-05-09 02:29:45.838213: step 49460, loss = 0.91 (825.0 examples/sec; 0.155 sec/batch)
2017-05-09 02:29:47.394830: step 49470, loss = 0.81 (822.3 examples/sec; 0.156 sec/batch)
2017-05-09 02:29:48.938452: step 49480, loss = 0.72 (829.2 examples/sec; 0.154 sec/batch)
2017-05-09 02:29:50.474486: step 49490, loss = 0.79 (833.3 examples/sec; 0.154 sec/batch)
2017-05-09 02:29:52.100788: step 49500, loss = 0.81 (787.1 examples/sec; 0.163 sec/batch)
2017-05-09 02:29:53.558398: step 49510, loss = 0.78 (878.2 examples/sec; 0.146 sec/batch)
2017-05-09 02:29:55.104381: step 49520, loss = 0.80 (828.0 examples/sec; 0.155 sec/batch)
2017-05-09 02:29:56.679295: step 49530, loss = 0.73 (812.7 examples/sec; 0.157 sec/batch)
2017-05-09 02:29:58.188555: step 49540, loss = 0.88 (848.1 examples/sec; 0.151 sec/batch)
2017-05-09 02:29:59.729840: step 49550, loss = 0.82 (830.6 examples/sec; 0.154 sec/batch)
2017-05-09 02:30:01.281291: step 49560, loss = 1.35 (825.0 examples/sec; 0.155 sec/batch)
2017-05-09 02:30:02.819759: step 49570, loss = 0.78 (832.0 examples/sec; 0.154 sec/batch)
2017-05-09 02:30:04.387308: step 49580, loss =E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1012 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1017 events to /tmp/cifar10_train/events.out.tfevents.1494304480.GHC42.GHC.ANDREW.CMU.EDU
 0.77 (816.6 examples/sec; 0.157 sec/batch)
2017-05-09 02:30:05.924180: step 49590, loss = 0.93 (832.9 examples/sec; 0.154 sec/batch)
2017-05-09 02:30:07.587186: step 49600, loss = 0.75 (769.7 examples/sec; 0.166 sec/batch)
2017-05-09 02:30:09.037971: step 49610, loss = 0.99 (882.3 examples/sec; 0.145 sec/batch)
2017-05-09 02:30:10.625062: step 49620, loss = 0.85 (806.5 examples/sec; 0.159 sec/batch)
2017-05-09 02:30:12.200646: step 49630, loss = 0.87 (812.4 examples/sec; 0.158 sec/batch)
2017-05-09 02:30:13.758014: step 49640, loss = 0.86 (821.9 examples/sec; 0.156 sec/batch)
2017-05-09 02:30:15.300455: step 49650, loss = 0.77 (829.9 examples/sec; 0.154 sec/batch)
2017-05-09 02:30:16.886489: step 49660, loss = 0.83 (807.0 examples/sec; 0.159 sec/batch)
2017-05-09 02:30:18.436294: step 49670, loss = 1.05 (825.9 examples/sec; 0.155 sec/batch)
2017-05-09 02:30:20.023946: step 49680, loss = 0.93 (806.2 examples/sec; 0.159 sec/batch)
2017-05-09 02:30:21.558250: step 49690, loss = 0.65 (834.3 examples/sec; 0.153 sec/batch)
2017-05-09 02:30:23.199193: step 49700, loss = 0.80 (780.0 examples/sec; 0.164 sec/batch)
2017-05-09 02:30:24.692678: step 49710, loss = 0.87 (857.1 examples/sec; 0.149 sec/batch)
2017-05-09 02:30:26.265083: step 49720, loss = 0.96 (814.0 examples/sec; 0.157 sec/batch)
2017-05-09 02:30:27.829575: step 49730, loss = 0.97 (818.2 examples/sec; 0.156 sec/batch)
2017-05-09 02:30:29.413340: step 49740, loss = 0.77 (808.2 examples/sec; 0.158 sec/batch)
2017-05-09 02:30:30.952055: step 49750, loss = 0.74 (831.9 examples/sec; 0.154 sec/batch)
2017-05-09 02:30:32.561905: step 49760, loss = 0.79 (795.1 examples/sec; 0.161 sec/batch)
2017-05-09 02:30:34.085858: step 49770, loss = 0.78 (839.9 examples/sec; 0.152 sec/batch)
2017-05-09 02:30:35.680964: step 49780, loss = 0.95 (802.5 examples/sec; 0.160 sec/batch)
2017-05-09 02:30:37.226950: step 49790, loss = 0.62 (828.0 examples/sec; 0.155 sec/batch)
2017-05-09 02:30:38.872740: step 49800, loss = 0.73 (777.7 examples/sec; 0.165 sec/batch)
2017-05-09 02:30:40.312171: step 49810, loss = 1.04 (889.2 examples/sec; 0.144 sec/batch)
2017-05-09 02:30:41.870899: step 49820, loss = 0.69 (821.2 examples/sec; 0.156 sec/batch)
2017-05-09 02:30:43.431293: step 49830, loss = 0.86 (820.3 examples/sec; 0.156 sec/batch)
2017-05-09 02:30:44.971089: step 49840, loss = 0.72 (831.3 examples/sec; 0.154 sec/batch)
2017-05-09 02:30:46.521886: step 49850, loss = 0.94 (825.4 examples/sec; 0.155 sec/batch)
2017-05-09 02:30:48.025997: step 49860, loss = 0.81 (851.0 examples/sec; 0.150 sec/batch)
2017-05-09 02:30:49.031845: step 49870, loss = 0.80 (1272.6 examples/sec; 0.101 sec/batch)
2017-05-09 02:30:50.038963: step 49880, loss = 0.74 (1270.9 examples/sec; 0.101 sec/batch)
2017-05-09 02:30:51.079128: step 49890, loss = 0.73 (1230.6 examples/sec; 0.104 sec/batch)
2017-05-09 02:30:52.177965: step 49900, loss = 0.65 (1164.9 examples/sec; 0.110 sec/batch)
2017-05-09 02:30:53.104004: step 49910, loss = 0.74 (1382.2 examples/sec; 0.093 sec/batch)
2017-05-09 02:30:54.126756: step 49920, loss = 0.66 (1251.5 examples/sec; 0.102 sec/batch)
2017-05-09 02:30:55.147602: step 49930, loss = 0.90 (1253.9 examples/sec; 0.102 sec/batch)
2017-05-09 02:30:56.183692: step 49940, loss = 0.74 (1235.4 examples/sec; 0.104 sec/batch)
2017-05-09 02:30:57.206310: step 49950, loss = 0.68 (1251.7 examples/sec; 0.102 sec/batch)
2017-05-09 02:30:58.208728: step 49960, loss = 0.76 (1276.9 examples/sec; 0.100 sec/batch)
2017-05-09 02:30:59.237310: step 49970, loss = 0.70 (1244.4 examples/sec; 0.103 sec/batch)
2017-05-09 02:31:00.254888: step 49980, loss = 0.63 (1257.9 examples/sec; 0.102 sec/batch)
2017-05-09 02:31:01.271185: step 49990, loss = 0.72 (1259.5 examples/sec; 0.102 sec/batch)
2017-05-09 02:31:02.395849: step 50000, loss = 0.80 (1138.1 examples/sec; 0.112 sec/batch)
--- 6982.15119815 seconds ---
