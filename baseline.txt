Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-05-05 17:26:05.207220: step 0, loss = 4.68 (98.5 examples/sec; 1.299 sec/batch)
2017-05-05 17:26:05.841447: step 10, loss = 4.65 (2018.2 examples/sec; 0.063 sec/batch)
2017-05-05 17:26:06.615704: step 20, loss = 4.64 (1653.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:07.382971: step 30, loss = 4.57 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:08.147477: step 40, loss = 4.49 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:26:08.921517: step 50, loss = 4.47 (1653.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:09.695230: step 60, loss = 4.39 (1654.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:10.467827: step 70, loss = 4.46 (1656.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:11.235134: step 80, loss = 4.37 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:11.997625: step 90, loss = 4.37 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:26:12.767951: step 100, loss = 4.24 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:13.539691: step 110, loss = 4.21 (1658.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:14.315875: step 120, loss = 4.26 (1649.1 examples/sec; 0.078 sec/batch)
2017-05-05 17:26:15.090880: step 130, loss = 4.21 (1651.6 examples/sec; 0.078 sec/batch)
2017-05-05 17:26:15.850058: step 140, loss = 4.46 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:26:16.619771: step 150, loss = 4.09 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:17.389822: step 160, loss = 4.17 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:18.161013: step 170, loss = 4.38 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:18.930886: step 180, loss = 4.19 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:19.688962: step 190, loss = 4.02 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:26:20.482954: step 200, loss = 4.09 (1612.1 examples/sec; 0.079 sec/batch)
2017-05-05 17:26:21.243830: step 210, loss = 4.40 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:26:22.020530: step 220, loss = 4.08 (1648.0 examples/sec; 0.078 sec/batch)
2017-05-05 17:26:22.791744: step 230, loss = 3.95 (1659.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:23.556132: step 240, loss = 4.44 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:26:24.324244: step 250, loss = 4.03 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:25.103026: step 260, loss = 4.03 (1643.6 examples/sec; 0.078 sec/batch)
2017-05-05 17:26:25.877034: step 270, loss = 3.94 (1653.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:26.649587: step 280, loss = 4.09 (1656.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:27.422400: step 290, loss = 3.79 (1656.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:28.196973: step 300, loss = 3.95 (1652.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:28.971644: step 310, loss = 4.65 (1652.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:29.748614: step 320, loss = 3.93 (1647.4 examples/sec; 0.078 sec/batch)
2017-05-05 17:26:30.523440: step 330, loss = 3.95 (1652.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:31.292913: step 340, loss = 3.83 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:32.063640: step 350, loss = 3.99 (1660.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:32.842988: step 360, loss = 3.85 (1642.4 examples/sec; 0.078 sec/batch)
2017-05-05 17:26:33.617390: step 370, loss = 3.85 (1652.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:34.393267: step 380, loss = 3.84 (1649.7 examples/sec; 0.078 sec/batch)
2017-05-05 17:26:35.167398: step 390, loss = 3.83 (1653.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:35.933805: step 400, loss = 3.80 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:36.708182: step 410, loss = 3.77 (1652.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:37.487838: step 420, loss = 3.77 (1641.7 examples/sec; 0.078 sec/batch)
2017-05-05 17:26:38.260997: step 430, loss = 3.67 (1655.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:39.038948: step 440, loss = 3.74 (1645.4 examples/sec; 0.078 sec/batch)
2017-05-05 17:26:39.801464: step 450, loss = 3.74 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:26:40.573338: step 460, loss = 3.73 (1658.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:41.344016: step 470, loss = 3.62 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:42.120869: step 480, loss = 3.88 (1647.7 examples/sec; 0.078 sec/batch)
2017-05-05 17:26:42.895396: step 490, loss = 3.86 (1652.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:43.659751: step 500, loss = 3.76 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:26:44.432887: step 510, loss = 3.70 (1655.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:45.208685: step 520, loss = 3.61 (1649.9 examples/sec; 0.078 sec/batch)
2017-05-05 17:26:45.986007: step 530, loss = 3.65 (1646.7 examples/sec; 0.078 sec/batch)
2017-05-05 17:26:46.761080: step 540, loss = 3.71 (1651.4 examples/sec; 0.078 sec/batch)
2017-05-05 17:26:47.524049: step 550, loss = 3.80 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:26:48.298765: step 560, loss = 3.65 (1652.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:49.070433: step 570, loss = 3.44 (1658.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:49.847230: step 580, loss = 3.81 (1647.8 examples/sec; 0.078 sec/batch)
2017-05-05 17:26:50.617919: step 590, loss = 3.51 (1660.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:51.393789: step 600, loss = 3.68 (1649.8 examples/sec; 0.078 sec/batch)
2017-05-05 17:26:52.165723: step 610, loss = 3.61 (1658.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:52.939454: step 620, loss = 3.67 (1654.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:53.711900: step 630, loss = 3.64 (1657.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:54.491498: step 640, loss = 3.63 (1641.9 examples/sec; 0.078 sec/batch)
2017-05-05 17:26:55.264883: step 650, loss = 3.47 (1655.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:56.030648: step 660, loss = 3.49 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:56.803427: step 670, loss = 3.57 (1656.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:57.584140: step 680, loss = 3.46 (1639.5 examples/sec; 0.078 sec/batch)
2017-05-05 17:26:58.358090: step 690, loss = 3.56 (1653.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:26:59.141338: step 700, loss = 3.54 (1634.2 examples/sec; 0.078 sec/batch)
2017-05-05 17:26:59.906150: step 710, loss = 3.69 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:27:00.677928: step 720, loss = 3.31 (1658.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:01.444637: step 730, loss = 3.29 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:02.222280: step 740, loss = 3.45 (1646.0 examples/sec; 0.078 sec/batch)
2017-05-05 17:27:02.994256: step 750, loss = 3.50 (1658.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:03.769368: step 760, loss = 3.49 (1651.4 examples/sec; 0.078 sec/batch)
2017-05-05 17:27:04.543693: step 770, loss = 3.42 (1653.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:05.318530: step 780, loss = 3.45 (1652.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:06.089410: step 790, loss = 3.28 (1660.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:06.866581: step 800, loss = 3.33 (1647.0 examples/sec; 0.078 sec/batch)
2017-05-05 17:27:07.635270: step 810, loss = 3.37 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:08.405531: step 820, loss = 3.65 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:09.178930: step 830, loss = 3.36 (1655.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:09.957151: step 840, loss = 3.34 (1644.8 examples/sec; 0.078 sec/batch)
2017-05-05 17:27:10.728576: step 850, loss = 3.43 (1659.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:11.497132: step 860, loss = 3.25 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:12.267402: step 870, loss = 3.29 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:13.047329: step 880, loss = 3.38 (1641.2 examples/sec; 0.078 sec/batch)
2017-05-05 17:27:13.820095: step 890, loss = 3.12 (1656.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:14.595446: step 900, loss = 3.33 (1650.9 examples/sec; 0.078 sec/batch)
2017-05-05 17:27:15.361159: step 910, loss = 3.35 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:16.126956: step 920, loss = 3.25 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:16.899138: step 930, loss = 3.24 (1657.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:17.675382: step 940, loss = 3.32 (1649.0 examples/sec; 0.078 sec/batch)
2017-05-05 17:27:18.454654: step 950, loss = 3.09 (1642.6 examples/sec; 0.078 sec/batch)
2017-05-05 17:27:19.226121: step 960, loss = 3.14 (1659.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:19.993317: step 970, loss = 3.19 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:20.764135: step 980, loss = 3.09 (1660.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:21.650427: step 990, loss = 3.31 (1444.2 examples/sec; 0.089 sec/batch)
2017-05-05 17:27:22.315810: step 1000, loss = 3.22 (1923.7 examples/sec; 0.067 sec/batch)
2017-05-05 17:27:23.090318: step 1010, loss = 3.27 (1652.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:23.859088: step 1020, loss = 3.13 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:24.627117: step 1030, loss = 3.26 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:25.404864: step 1040, loss = 3.16 (1645.8 examples/sec; 0.078 sec/batch)
2017-05-05 17:27:26.170531: step 1050, loss = 3.17 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:26.944021: step 1060, loss = 3.08 (1654.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:27.702990: step 1070, loss = 3.04 (1686.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:27:28.486095: step 1080, loss = 3.17 (1634.5 examples/sec; 0.078 sec/batch)
2017-05-05 17:27:29.260060: step 1090, loss = 3.15 (1653.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:30.041355: step 1100, loss = 2.88 (1638.3 examples/sec; 0.078 sec/batch)
2017-05-05 17:27:30.812495: step 1110, loss = 2.92 (1659.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:31.572899: step 1120, loss = 3.00 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:27:32.349261: step 1130, loss = 3.27 (1648.7 examples/sec; 0.078 sec/batch)
2017-05-05 17:27:33.118322: step 1140, loss = 2.92 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:33.898247: step 1150, loss = 3.07 (1641.2 examples/sec; 0.078 sec/batch)
2017-05-05 17:27:34.664079: step 1160, loss = 3.02 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:35.434728: step 1170, loss = 2.84 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:36.205440: step 1180, loss = 2.97 (1660.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:36.986126: step 1190, loss = 2.97 (1639.6 examples/sec; 0.078 sec/batch)
2017-05-05 17:27:37.752530: step 1200, loss = 3.18 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:38.522559: step 1210, loss = 3.03 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:39.296933: step 1220, loss = 3.08 (1652.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:40.062965: step 1230, loss = 3.26 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:40.829929: step 1240, loss = 2.89 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:41.600060: step 1250, loss = 3.46 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:42.377854: step 1260, loss = 2.80 (1645.7 examples/sec; 0.078 sec/batch)
2017-05-05 17:27:43.152909: step 1270, loss = 2.95 (1651.5 examples/sec; 0.078 sec/batch)
2017-05-05 17:27:43.920587: step 1280, loss = 2.77 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:44.699731: step 1290, loss = 2.98 (1642.8 examples/sec; 0.078 sec/batch)
2017-05-05 17:27:45.470392: step 1300, loss = 2.95 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:46.242278: step 1310, loss = 2.88 (1658.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:47.012742: step 1320, loss = 3.12 (1661.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:47.779100: step 1330, loss = 2.79 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:48.551758: step 1340, loss = 3.04 (1656.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:49.326250: step 1350, loss = 2.87 (1652.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:50.105426: step 1360, loss = 2.73 (1642.8 examples/sec; 0.078 sec/batch)
2017-05-05 17:27:50.870309: step 1370, loss = 2.72 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:27:51.637964: step 1380, loss = 2.94 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:52.417138: step 1390, loss = 3.00 (1642.8 examples/sec; 0.078 sec/batch)
2017-05-05 17:27:53.193942: step 1400, loss = 3.28 (1647.8 examples/sec; 0.078 sec/batch)
2017-05-05 17:27:53.971155: step 1410, loss = 2.87 (1646.9 examples/sec; 0.078 sec/batch)
2017-05-05 17:27:54.754769: step 1420, loss = 2.69 (1633.5 examples/sec; 0.078 sec/batch)
2017-05-05 17:27:55.518160: step 1430, loss = 2.74 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:27:56.286651: step 1440, loss = 2.74 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:57.056099: step 1450, loss = 2.65 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:57.830871: step 1460, loss = 2.91 (1652.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:27:58.607912: step 1470, loss = 2.83 (1647.3 examples/sec; 0.078 sec/batch)
2017-05-05 17:27:59.376704: step 1480, loss = 2.89 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:00.152041: step 1490, loss = 2.72 (1650.9 examples/sec; 0.078 sec/batch)
2017-05-05 17:28:00.922861: step 1500, loss = 2.79 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:01.693800: step 1510, loss = 2.86 (1660.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:02.498020: step 1520, loss = 2.90 (1591.6 examples/sec; 0.080 sec/batch)
2017-05-05 17:28:03.267783: step 1530, loss = 2.87 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:04.031198: step 1540, loss = 2.82 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:28:04.801680: step 1550, loss = 2.60 (1661.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:05.569741: step 1560, loss = 2.85 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:06.345509: step 1570, loss = 2.58 (1650.0 examples/sec; 0.078 sec/batch)
2017-05-05 17:28:07.117013: step 1580, loss = 2.70 (1659.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:07.887705: step 1590, loss = 2.74 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:08.664757: step 1600, loss = 2.63 (1647.2 examples/sec; 0.078 sec/batch)
2017-05-05 17:28:09.440616: step 1610, loss = 2.91 (1649.8 examples/sec; 0.078 sec/batch)
2017-05-05 17:28:10.214032: step 1620, loss = 2.91 (1655.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:10.980785: step 1630, loss = 2.75 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:11.741901: step 1640, loss = 2.75 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:28:12.518450: step 1650, loss = 2.53 (1648.3 examples/sec; 0.078 sec/batch)
2017-05-05 17:28:13.290915: step 1660, loss = 2.67 (1657.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:14.068693: step 1670, loss = 2.62 (1645.7 examples/sec; 0.078 sec/batch)
2017-05-05 17:28:14.839287: step 1680, loss = 2.58 (1661.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:15.614094: step 1690, loss = 2.77 (1652.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:16.390983: step 1700, loss = 2.62 (1647.6 examples/sec; 0.078 sec/batch)
2017-05-05 17:28:17.164890: step 1710, loss = 2.64 (1653.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:17.942248: step 1720, loss = 2.58 (1646.6 examples/sec; 0.078 sec/batch)
2017-05-05 17:28:18.717111: step 1730, loss = 2.65 (1651.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:19.481333: step 1740, loss = 2.51 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:28:20.251586: step 1750, loss = 2.60 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:21.026135: step 1760, loss = 2.49 (1652.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:21.806287: step 1770, loss = 2.50 (1640.7 examples/sec; 0.078 sec/batch)
2017-05-05 17:28:22.571075: step 1780, loss = 2.60 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:28:23.345612: step 1790, loss = 2.59 (1652.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:24.110709: step 1800, loss = 2.63 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:24.883142: step 1810, loss = 2.43 (1657.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:25.644962: step 1820, loss = 2.50 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:28:26.418856: step 1830, loss = 2.48 (1654.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:27.192080: step 1840, loss = 2.50 (1655.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:27.956867: step 1850, loss = 2.61 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:28:28.729152: step 1860, loss = 2.68 (1657.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:29.506078: step 1870, loss = 2.58 (1647.5 examples/sec; 0.078 sec/batch)
2017-05-05 17:28:30.281535: step 1880, loss = 2.56 (1650.6 examples/sec; 0.078 sec/batch)
2017-05-05 17:28:31.054222: step 1890, loss = 2.42 (1656.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:31.819839: step 1900, loss = 2.63 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:32.592261: step 1910, loss = 2.52 (1657.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:33.362514: step 1920, loss = 2.36 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:34.135096: step 1930, loss = 2.41 (1656.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:34.909857: step 1940, loss = 2.46 (1652.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:35.675995: step 1950, loss = 2.48 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:36.449265: step 1960, loss = 2.51 (1655.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:37.224564: step 1970, loss = 2.49 (1651.0 examples/sec; 0.078 sec/batch)
2017-05-05 17:28:38.111266: step 1980, loss = 2.44 (1443.5 examples/sec; 0.089 sec/batch)
2017-05-05 17:28:38.779188: step 1990, loss = 2.50 (1916.4 examples/sec; 0.067 sec/batch)
2017-05-05 17:28:39.555893: step 2000, loss = 2.42 (1648.0 examples/sec; 0.078 sec/batch)
2017-05-05 17:28:40.327776: step 2010, loss = 2.37 (1658.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:41.091272: step 2020, loss = 2.61 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:28:41.860236: step 2030, loss = 2.41 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:42.635348: step 2040, loss = 2.32 (1651.4 examples/sec; 0.078 sec/batch)
2017-05-05 17:28:43.404274: step 2050, loss = 2.47 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:44.174655: step 2060, loss = 2.44 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:44.948795: step 2070, loss = 2.38 (1653.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:45.712733: step 2080, loss = 2.43 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:28:46.493531: step 2090, loss = 2.34 (1639.3 examples/sec; 0.078 sec/batch)
2017-05-05 17:28:47.261297: step 2100, loss = 2.46 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:48.029115: step 2110, loss = 2.51 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:48.805244: step 2120, loss = 2.23 (1649.2 examples/sec; 0.078 sec/batch)
2017-05-05 17:28:49.575721: step 2130, loss = 2.34 (1661.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:50.349837: step 2140, loss = 2.36 (1653.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:51.119746: step 2150, loss = 2.39 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:51.882628: step 2160, loss = 2.41 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:28:52.653623: step 2170, loss = 2.41 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:53.427444: step 2180, loss = 2.18 (1654.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:54.204713: step 2190, loss = 2.33 (1646.8 examples/sec; 0.078 sec/batch)
2017-05-05 17:28:54.975971: step 2200, loss = 2.37 (1659.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:55.742210: step 2210, loss = 2.47 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:56.518186: step 2220, loss = 2.59 (1649.5 examples/sec; 0.078 sec/batch)
2017-05-05 17:28:57.291040: step 2230, loss = 2.43 (1656.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:28:58.066496: step 2240, loss = 2.45 (1650.7 examples/sec; 0.078 sec/batch)
2017-05-05 17:28:58.843168: step 2250, loss = 2.45 (1648.0 examples/sec; 0.078 sec/batch)
2017-05-05 17:28:59.610182: step 2260, loss = 2.33 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:00.379253: step 2270, loss = 2.23 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:01.149522: step 2280, loss = 2.18 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:01.918885: step 2290, loss = 2.27 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:02.701004: step 2300, loss = 2.27 (1636.6 examples/sec; 0.078 sec/batch)
2017-05-05 17:29:03.466825: step 2310, loss = 2.20 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:04.237757: step 2320, loss = 2.32 (1660.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:05.006187: step 2330, loss = 2.22 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:05.773341: step 2340, loss = 2.32 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:06.551207: step 2350, loss = 2.12 (1645.5 examples/sec; 0.078 sec/batch)
2017-05-05 17:29:07.318034: step 2360, loss = 2.47 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:08.083175: step 2370, loss = 2.30 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:08.862427: step 2380, loss = 2.12 (1642.6 examples/sec; 0.078 sec/batch)
2017-05-05 17:29:09.633429: step 2390, loss = 2.10 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:10.404538: step 2400, loss = 2.22 (1659.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:11.171435: step 2410, loss = 2.12 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:11.937547: step 2420, loss = 2.36 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:12.711091: step 2430, loss = 2.21 (1654.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:13.481864: step 2440, loss = 2.35 (1660.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:14.252341: step 2450, loss = 2.17 (1661.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:15.024354: step 2460, loss = 2.24 (1658.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:15.787162: step 2470, loss = 2.07 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:29:16.557601: step 2480, loss = 2.26 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:17.335850: step 2490, loss = 2.16 (1644.7 examples/sec; 0.078 sec/batch)
2017-05-05 17:29:18.112514: step 2500, loss = 2.04 (1648.1 examples/sec; 0.078 sec/batch)
2017-05-05 17:29:18.888948: step 2510, loss = 2.01 (1648.6 examples/sec; 0.078 sec/batch)
2017-05-05 17:29:19.652541: step 2520, loss = 2.13 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:29:20.427800: step 2530, loss = 2.12 (1651.0 examples/sec; 0.078 sec/batch)
2017-05-05 17:29:21.196195: step 2540, loss = 2.14 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:21.970151: step 2550, loss = 2.29 (1653.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:22.742785: step 2560, loss = 2.24 (1656.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:23.520169: step 2570, loss = 1.99 (1646.6 examples/sec; 0.078 sec/batch)
2017-05-05 17:29:24.291811: step 2580, loss = 1.86 (1658.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:25.065441: step 2590, loss = 2.07 (1654.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:25.835602: step 2600, loss = 1.97 (1662.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:26.612510: step 2610, loss = 2.08 (1647.5 examples/sec; 0.078 sec/batch)
2017-05-05 17:29:27.384449: step 2620, loss = 1.98 (1658.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:28.155622: step 2630, loss = 1.95 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:28.934479: step 2640, loss = 2.14 (1643.4 examples/sec; 0.078 sec/batch)
2017-05-05 17:29:29.712749: step 2650, loss = 2.12 (1644.7 examples/sec; 0.078 sec/batch)
2017-05-05 17:29:30.489223: step 2660, loss = 2.02 (1648.5 examples/sec; 0.078 sec/batch)
2017-05-05 17:29:31.260756: step 2670, loss = 2.12 (1659.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:32.026055: step 2680, loss = 2.10 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:32.797630: step 2690, loss = 1.99 (1658.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:33.577465: step 2700, loss = 2.23 (1641.4 examples/sec; 0.078 sec/batch)
2017-05-05 17:29:34.350424: step 2710, loss = 2.09 (1656.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:35.129247: step 2720, loss = 2.42 (1643.5 examples/sec; 0.078 sec/batch)
2017-05-05 17:29:35.894653: step 2730, loss = 2.08 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:36.674858: step 2740, loss = 2.01 (1640.6 examples/sec; 0.078 sec/batch)
2017-05-05 17:29:37.450173: step 2750, loss = 1.97 (1651.0 examples/sec; 0.078 sec/batch)
2017-05-05 17:29:38.228562: step 2760, loss = 2.05 (1644.4 examples/sec; 0.078 sec/batch)
2017-05-05 17:29:39.002058: step 2770, loss = 2.05 (1654.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:39.768902: step 2780, loss = 2.08 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:40.553398: step 2790, loss = 2.20 (1631.6 examples/sec; 0.078 sec/batch)
2017-05-05 17:29:41.337504: step 2800, loss = 1.92 (1632.5 examples/sec; 0.078 sec/batch)
2017-05-05 17:29:42.120313: step 2810, loss = 2.00 (1635.1 examples/sec; 0.078 sec/batch)
2017-05-05 17:29:42.892702: step 2820, loss = 2.04 (1657.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:43.655185: step 2830, loss = 2.15 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:29:44.425990: step 2840, loss = 2.10 (1660.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:45.206052: step 2850, loss = 2.06 (1640.9 examples/sec; 0.078 sec/batch)
2017-05-05 17:29:45.961744: step 2860, loss = 1.74 (1693.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:29:46.744102: step 2870, loss = 1.95 (1636.1 examples/sec; 0.078 sec/batch)
2017-05-05 17:29:47.504799: step 2880, loss = 2.04 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:29:48.275393: step 2890, loss = 2.20 (1661.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:49.046714: step 2900, loss = 1.87 (1659.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:49.828004: step 2910, loss = 1.95 (1638.3 examples/sec; 0.078 sec/batch)
2017-05-05 17:29:50.594402: step 2920, loss = 2.13 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:51.360482: step 2930, loss = 2.07 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:52.131567: step 2940, loss = 2.18 (1660.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:52.899791: step 2950, loss = 2.10 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:53.670813: step 2960, loss = 1.86 (1660.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:54.551841: step 2970, loss = 1.92 (1452.8 examples/sec; 0.088 sec/batch)
2017-05-05 17:29:55.218507: step 2980, loss = 1.91 (1920.0 examples/sec; 0.067 sec/batch)
2017-05-05 17:29:55.989338: step 2990, loss = 1.90 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:29:56.770586: step 3000, loss = 2.17 (1638.4 examples/sec; 0.078 sec/batch)
2017-05-05 17:29:57.557933: step 3010, loss = 1.90 (1625.7 examples/sec; 0.079 sec/batch)
2017-05-05 17:29:58.352138: step 3020, loss = 2.02 (1611.7 examples/sec; 0.079 sec/batch)
2017-05-05 17:29:59.154865: step 3030, loss = 2.20 (1594.6 examples/sec; 0.080 sec/batch)
2017-05-05 17:29:59.997696: step 3040, loss = 1.93 (1518.7 examples/sec; 0.084 sec/batch)
2017-05-05 17:30:00.825741: step 3050, loss = 1.86 (1545.8 examples/sec; 0.083 sec/batch)
2017-05-05 17:30:01.637463: step 3060, loss = 1.90 (1576.9 examples/sec; 0.081 sec/batch)
2017-05-05 17:30:02.485027: step 3070, loss = 1.94 (1510.2 examples/sec; 0.085 sec/batch)
2017-05-05 17:30:03.319337: step 3080, loss = 1.99 (1534.2 examples/sec; 0.083 sec/batch)
2017-05-05 17:30:04.143196: step 3090, loss = 1.78 (1553.7 examples/sec; 0.082 sec/batch)
2017-05-05 17:30:04.986292: step 3100, loss = 2.05 (1518.2 examples/sec; 0.084 sec/batch)
2017-05-05 17:30:05.821105: step 3110, loss = 1.89 (1533.3 examples/sec; 0.083 sec/batch)
2017-05-05 17:30:06.671112: step 3120, loss = 2.07 (1505.9 examples/sec; 0.085 sec/batch)
2017-05-05 17:30:07.550897: step 3130, loss = 1.89 (1454.9 examples/sec; 0.088 sec/batch)
2017-05-05 17:30:08.397281: step 3140, loss = 1.88 (1512.3 examples/sec; 0.085 sec/batch)
2017-05-05 17:30:09.277953: step 3150, loss = 1.89 (1453.4 examples/sec; 0.088 sec/batch)
2017-05-05 17:30:10.059516: step 3160, loss = 1.87 (1637.7 examples/sec; 0.078 sec/batch)
2017-05-05 17:30:10.833161: step 3170, loss = 1.99 (1654.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:11.595560: step 3180, loss = 1.99 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:30:12.365870: step 3190, loss = 1.95 (1661.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:13.144784: step 3200, loss = 1.84 (1643.3 examples/sec; 0.078 sec/batch)
2017-05-05 17:30:13.917543: step 3210, loss = 1.94 (1656.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:14.690653: step 3220, loss = 1.75 (1655.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:15.454176: step 3230, loss = 1.85 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:30:16.222532: step 3240, loss = 1.96 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:16.993873: step 3250, loss = 1.78 (1659.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:17.758244: step 3260, loss = 1.81 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:30:18.530070: step 3270, loss = 1.73 (1658.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:19.300615: step 3280, loss = 1.76 (1661.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:20.065870: step 3290, loss = 1.94 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:20.833099: step 3300, loss = 1.88 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:21.611863: step 3310, loss = 2.01 (1643.6 examples/sec; 0.078 sec/batch)
2017-05-05 17:30:22.382433: step 3320, loss = 1.79 (1661.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:23.158212: step 3330, loss = 1.66 (1650.0 examples/sec; 0.078 sec/batch)
2017-05-05 17:30:23.923424: step 3340, loss = 1.94 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:24.697823: step 3350, loss = 2.12 (1652.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:25.472246: step 3360, loss = 1.83 (1652.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:26.242389: step 3370, loss = 1.88 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:27.015154: step 3380, loss = 1.91 (1656.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:27.778228: step 3390, loss = 1.85 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:30:28.547739: step 3400, loss = 1.66 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:29.319325: step 3410, loss = 1.92 (1658.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:30.094348: step 3420, loss = 1.72 (1651.6 examples/sec; 0.078 sec/batch)
2017-05-05 17:30:30.868080: step 3430, loss = 1.84 (1654.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:31.638781: step 3440, loss = 1.83 (1660.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:32.412517: step 3450, loss = 1.85 (1654.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:33.181525: step 3460, loss = 1.68 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:33.946604: step 3470, loss = 1.74 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:34.721332: step 3480, loss = 1.77 (1652.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:35.489888: step 3490, loss = 1.80 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:36.259964: step 3500, loss = 1.98 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:37.032975: step 3510, loss = 1.51 (1655.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:37.803335: step 3520, loss = 1.74 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:38.576624: step 3530, loss = 1.72 (1655.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:39.346171: step 3540, loss = 1.75 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:40.114583: step 3550, loss = 1.73 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:40.879033: step 3560, loss = 1.71 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:30:41.650088: step 3570, loss = 1.75 (1660.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:42.421287: step 3580, loss = 1.69 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:43.193320: step 3590, loss = 1.63 (1658.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:43.951063: step 3600, loss = 1.71 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:30:44.721672: step 3610, loss = 1.67 (1661.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:45.490527: step 3620, loss = 1.75 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:46.262640: step 3630, loss = 1.92 (1657.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:47.032573: step 3640, loss = 1.76 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:47.792664: step 3650, loss = 1.82 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:30:48.559229: step 3660, loss = 1.56 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:49.325327: step 3670, loss = 1.55 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:50.096578: step 3680, loss = 1.53 (1659.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:50.863725: step 3690, loss = 1.72 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:51.623674: step 3700, loss = 1.75 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:30:52.394671: step 3710, loss = 1.71 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:53.161793: step 3720, loss = 1.55 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:53.933496: step 3730, loss = 1.85 (1658.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:54.696262: step 3740, loss = 1.63 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:30:55.459486: step 3750, loss = 1.61 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:30:56.230218: step 3760, loss = 1.80 (1660.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:57.002223: step 3770, loss = 1.82 (1658.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:57.771899: step 3780, loss = 1.62 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:58.538568: step 3790, loss = 1.72 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:30:59.306465: step 3800, loss = 1.63 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:00.065268: step 3810, loss = 1.66 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:31:00.831794: step 3820, loss = 1.58 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:01.594662: step 3830, loss = 1.73 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:31:02.364265: step 3840, loss = 1.64 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:03.131895: step 3850, loss = 1.61 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:03.889841: step 3860, loss = 1.65 (1688.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:31:04.659627: step 3870, loss = 1.58 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:05.429792: step 3880, loss = 1.65 (1662.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:06.192672: step 3890, loss = 1.65 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:31:06.958982: step 3900, loss = 1.77 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:07.715083: step 3910, loss = 1.75 (1692.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:31:08.484786: step 3920, loss = 1.85 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:09.253058: step 3930, loss = 1.49 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:10.018427: step 3940, loss = 1.53 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:10.779459: step 3950, loss = 1.83 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:31:11.645279: step 3960, loss = 1.53 (1478.4 examples/sec; 0.087 sec/batch)
2017-05-05 17:31:12.310571: step 3970, loss = 1.55 (1923.9 examples/sec; 0.067 sec/batch)
2017-05-05 17:31:13.079489: step 3980, loss = 1.64 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:13.849732: step 3990, loss = 1.66 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:14.616447: step 4000, loss = 1.53 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:15.385332: step 4010, loss = 1.60 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:16.147537: step 4020, loss = 1.64 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:31:16.915067: step 4030, loss = 1.63 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:17.678435: step 4040, loss = 1.57 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:31:18.452305: step 4050, loss = 1.59 (1654.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:19.222904: step 4060, loss = 1.44 (1661.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:19.978763: step 4070, loss = 1.68 (1693.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:31:20.746250: step 4080, loss = 1.77 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:21.515395: step 4090, loss = 1.63 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:22.281209: step 4100, loss = 1.71 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:23.049554: step 4110, loss = 1.64 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:23.811652: step 4120, loss = 1.65 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:31:24.583099: step 4130, loss = 1.64 (1659.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:25.352994: step 4140, loss = 1.56 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:26.123033: step 4150, loss = 1.61 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:26.897094: step 4160, loss = 1.48 (1653.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:27.655927: step 4170, loss = 1.46 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:31:28.425852: step 4180, loss = 1.53 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:29.194778: step 4190, loss = 1.64 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:29.967830: step 4200, loss = 1.75 (1655.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:30.726136: step 4210, loss = 1.61 (1688.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:31:31.490828: step 4220, loss = 1.65 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:31:32.258010: step 4230, loss = 1.57 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:33.024099: step 4240, loss = 1.62 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:33.798044: step 4250, loss = 1.68 (1653.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:34.567333: step 4260, loss = 1.54 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:35.332116: step 4270, loss = 1.56 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:31:36.096538: step 4280, loss = 1.37 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:31:36.865796: step 4290, loss = 1.50 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:37.643525: step 4300, loss = 1.70 (1645.8 examples/sec; 0.078 sec/batch)
2017-05-05 17:31:38.411243: step 4310, loss = 1.59 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:39.172258: step 4320, loss = 1.66 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:31:39.937648: step 4330, loss = 1.86 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:40.706273: step 4340, loss = 1.53 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:41.466676: step 4350, loss = 1.68 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:31:42.238341: step 4360, loss = 1.38 (1658.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:43.008932: step 4370, loss = 1.65 (1661.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:43.765921: step 4380, loss = 1.52 (1690.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:31:44.534938: step 4390, loss = 1.49 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:45.304764: step 4400, loss = 1.57 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:46.072438: step 4410, loss = 1.63 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:46.839945: step 4420, loss = 1.43 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:47.604773: step 4430, loss = 1.70 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:31:48.384671: step 4440, loss = 1.74 (1641.2 examples/sec; 0.078 sec/batch)
2017-05-05 17:31:49.157926: step 4450, loss = 1.56 (1655.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:49.920270: step 4460, loss = 1.39 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:31:50.685788: step 4470, loss = 1.63 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:51.464939: step 4480, loss = 1.56 (1642.8 examples/sec; 0.078 sec/batch)
2017-05-05 17:31:52.249503: step 4490, loss = 1.59 (1631.5 examples/sec; 0.078 sec/batch)
2017-05-05 17:31:53.030505: step 4500, loss = 1.47 (1638.9 examples/sec; 0.078 sec/batch)
2017-05-05 17:31:53.810947: step 4510, loss = 1.55 (1640.1 examples/sec; 0.078 sec/batch)
2017-05-05 17:31:54.595278: step 4520, loss = 1.42 (1632.0 examples/sec; 0.078 sec/batch)
2017-05-05 17:31:55.386655: step 4530, loss = 1.37 (1617.4 examples/sec; 0.079 sec/batch)
2017-05-05 17:31:56.263146: step 4540, loss = 1.50 (1460.4 examples/sec; 0.088 sec/batch)
2017-05-05 17:31:57.027573: step 4550, loss = 1.51 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:31:57.796679: step 4560, loss = 1.60 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:58.565371: step 4570, loss = 1.58 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:31:59.331890: step 4580, loss = 1.59 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:00.090834: step 4590, loss = 1.53 (1686.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:32:00.852408: step 4600, loss = 1.69 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:32:01.624281: step 4610, loss = 1.48 (1658.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:02.396225: step 4620, loss = 1.23 (1658.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:03.163880: step 4630, loss = 1.53 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:03.919159: step 4640, loss = 1.43 (1694.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:32:04.692240: step 4650, loss = 1.53 (1655.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:05.457490: step 4660, loss = 1.55 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:06.227285: step 4670, loss = 1.49 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:06.998387: step 4680, loss = 1.63 (1660.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:07.759682: step 4690, loss = 1.83 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:32:08.528787: step 4700, loss = 1.44 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:09.293222: step 4710, loss = 1.52 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:32:10.061014: step 4720, loss = 1.52 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:10.823059: step 4730, loss = 1.30 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:32:11.588710: step 4740, loss = 1.34 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:12.359140: step 4750, loss = 1.44 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:13.129849: step 4760, loss = 1.41 (1660.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:13.896723: step 4770, loss = 1.43 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:14.670496: step 4780, loss = 1.57 (1654.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:15.425694: step 4790, loss = 1.39 (1694.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:32:16.188386: step 4800, loss = 1.36 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:32:16.955340: step 4810, loss = 1.41 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:17.725172: step 4820, loss = 1.44 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:18.496905: step 4830, loss = 1.59 (1658.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:19.270280: step 4840, loss = 1.40 (1655.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:20.029985: step 4850, loss = 1.36 (1684.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:32:20.788526: step 4860, loss = 1.58 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:32:21.551888: step 4870, loss = 1.49 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:32:22.320171: step 4880, loss = 1.47 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:23.086090: step 4890, loss = 1.48 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:23.843348: step 4900, loss = 1.21 (1690.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:32:24.618159: step 4910, loss = 1.27 (1652.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:25.382906: step 4920, loss = 1.48 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:32:26.149059: step 4930, loss = 1.50 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:26.915329: step 4940, loss = 1.32 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:27.783191: step 4950, loss = 1.36 (1474.9 examples/sec; 0.087 sec/batch)
2017-05-05 17:32:28.448610: step 4960, loss = 1.33 (1923.6 examples/sec; 0.067 sec/batch)
2017-05-05 17:32:29.216028: step 4970, loss = 1.31 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:29.982697: step 4980, loss = 1.29 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:30.750522: step 4990, loss = 1.41 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:31.519516: step 5000, loss = 1.54 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:32.286070: step 5010, loss = 1.52 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:33.058535: step 5020, loss = 1.34 (1657.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:33.826806: step 5030, loss = 1.61 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:34.596505: step 5040, loss = 1.34 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:35.362693: step 5050, loss = 1.62 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:36.122454: step 5060, loss = 1.31 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:32:36.892999: step 5070, loss = 1.41 (1661.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:37.661709: step 5080, loss = 1.48 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:38.431842: step 5090, loss = 1.43 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:39.199176: step 5100, loss = 1.43 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:39.958466: step 5110, loss = 1.50 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:32:40.720855: step 5120, loss = 1.34 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:32:41.485332: step 5130, loss = 1.51 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:32:42.256910: step 5140, loss = 1.30 (1658.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:43.027142: step 5150, loss = 1.55 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:43.788179: step 5160, loss = 1.41 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:32:44.555278: step 5170, loss = 1.46 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:45.328391: step 5180, loss = 1.49 (1655.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:46.099177: step 5190, loss = 1.54 (1660.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:46.865144: step 5200, loss = 1.51 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:47.627218: step 5210, loss = 1.40 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:32:48.393610: step 5220, loss = 1.45 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:49.154551: step 5230, loss = 1.38 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:32:49.924431: step 5240, loss = 1.53 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:50.690458: step 5250, loss = 1.53 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:51.455126: step 5260, loss = 1.28 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:32:52.220012: step 5270, loss = 1.26 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:32:52.995157: step 5280, loss = 1.36 (1651.3 examples/sec; 0.078 sec/batch)
2017-05-05 17:32:53.763955: step 5290, loss = 1.32 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:54.537210: step 5300, loss = 1.59 (1655.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:55.310700: step 5310, loss = 1.24 (1654.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:56.067529: step 5320, loss = 1.38 (1691.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:32:56.837196: step 5330, loss = 1.36 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:57.607286: step 5340, loss = 1.22 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:58.386892: step 5350, loss = 1.25 (1641.9 examples/sec; 0.078 sec/batch)
2017-05-05 17:32:59.154950: step 5360, loss = 1.27 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:32:59.914882: step 5370, loss = 1.40 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:33:00.682388: step 5380, loss = 1.30 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:01.445284: step 5390, loss = 1.40 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:33:02.211313: step 5400, loss = 1.25 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:02.979509: step 5410, loss = 1.37 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:03.743632: step 5420, loss = 1.40 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:33:04.513410: step 5430, loss = 1.30 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:05.279016: step 5440, loss = 1.19 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:06.038781: step 5450, loss = 1.26 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:33:06.809134: step 5460, loss = 1.15 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:07.571961: step 5470, loss = 1.55 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:33:08.334100: step 5480, loss = 1.30 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:33:09.103890: step 5490, loss = 1.21 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:09.874447: step 5500, loss = 1.81 (1661.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:10.629311: step 5510, loss = 1.21 (1695.7 examples/sec; 0.075 sec/batch)
2017-05-05 17:33:11.395455: step 5520, loss = 1.16 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:12.161659: step 5530, loss = 1.30 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:12.928008: step 5540, loss = 1.35 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:13.693658: step 5550, loss = 1.27 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:14.470891: step 5560, loss = 1.32 (1646.9 examples/sec; 0.078 sec/batch)
2017-05-05 17:33:15.235628: step 5570, loss = 1.35 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:33:15.994115: step 5580, loss = 1.21 (1687.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:33:16.758404: step 5590, loss = 1.41 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:33:17.532006: step 5600, loss = 1.34 (1654.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:18.304519: step 5610, loss = 1.19 (1656.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:19.073475: step 5620, loss = 1.18 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:19.841219: step 5630, loss = 1.59 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:20.610201: step 5640, loss = 1.43 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:21.375786: step 5650, loss = 1.21 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:22.140697: step 5660, loss = 1.24 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:33:22.904707: step 5670, loss = 1.37 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:33:23.666862: step 5680, loss = 1.24 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:33:24.435734: step 5690, loss = 1.25 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:25.206838: step 5700, loss = 1.34 (1659.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:25.965704: step 5710, loss = 1.46 (1686.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:33:26.734137: step 5720, loss = 1.20 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:27.497439: step 5730, loss = 1.35 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:33:28.271536: step 5740, loss = 1.28 (1653.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:29.035978: step 5750, loss = 1.17 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:33:29.805046: step 5760, loss = 1.31 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:30.574616: step 5770, loss = 1.32 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:31.335086: step 5780, loss = 1.31 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:33:32.096469: step 5790, loss = 1.31 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:33:32.866777: step 5800, loss = 1.46 (1661.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:33.639481: step 5810, loss = 1.55 (1656.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:34.405017: step 5820, loss = 1.31 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:35.170004: step 5830, loss = 1.28 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:33:35.931001: step 5840, loss = 1.25 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:33:36.699135: step 5850, loss = 1.44 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:37.473708: step 5860, loss = 1.38 (1652.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:38.255319: step 5870, loss = 1.31 (1637.7 examples/sec; 0.078 sec/batch)
2017-05-05 17:33:39.017342: step 5880, loss = 1.40 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:33:39.775850: step 5890, loss = 1.22 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:33:40.543111: step 5900, loss = 1.13 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:41.323599: step 5910, loss = 1.32 (1640.0 examples/sec; 0.078 sec/batch)
2017-05-05 17:33:42.082343: step 5920, loss = 1.23 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:33:42.840634: step 5930, loss = 1.08 (1688.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:33:43.700808: step 5940, loss = 1.23 (1488.1 examples/sec; 0.086 sec/batch)
2017-05-05 17:33:44.367206: step 5950, loss = 1.43 (1920.8 examples/sec; 0.067 sec/batch)
2017-05-05 17:33:45.134852: step 5960, loss = 1.38 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:45.907593: step 5970, loss = 1.42 (1656.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:46.678635: step 5980, loss = 1.32 (1660.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:47.444794: step 5990, loss = 1.31 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:48.206546: step 6000, loss = 1.15 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:33:48.967465: step 6010, loss = 1.25 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:33:49.750502: step 6020, loss = 1.24 (1634.6 examples/sec; 0.078 sec/batch)
2017-05-05 17:33:50.519321: step 6030, loss = 1.29 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:51.280978: step 6040, loss = 1.43 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:33:52.043000: step 6050, loss = 1.21 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:33:52.815173: step 6060, loss = 1.17 (1657.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:53.584825: step 6070, loss = 1.31 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:54.350083: step 6080, loss = 1.32 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:55.119991: step 6090, loss = 1.26 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:55.880614: step 6100, loss = 1.25 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:33:56.652499: step 6110, loss = 1.04 (1658.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:57.415464: step 6120, loss = 1.15 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:33:58.180067: step 6130, loss = 1.39 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:33:58.949852: step 6140, loss = 1.34 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:33:59.711418: step 6150, loss = 1.18 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:34:00.476759: step 6160, loss = 1.13 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:01.246957: step 6170, loss = 1.41 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:02.009805: step 6180, loss = 1.05 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:34:02.777828: step 6190, loss = 1.30 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:03.543385: step 6200, loss = 1.39 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:04.313054: step 6210, loss = 1.17 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:05.081120: step 6220, loss = 1.24 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:05.847828: step 6230, loss = 1.20 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:06.617593: step 6240, loss = 1.21 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:07.384024: step 6250, loss = 1.18 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:08.142841: step 6260, loss = 1.21 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:34:08.907585: step 6270, loss = 1.35 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:34:09.676845: step 6280, loss = 1.23 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:10.448918: step 6290, loss = 1.19 (1657.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:11.214831: step 6300, loss = 1.25 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:11.970423: step 6310, loss = 1.19 (1694.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:34:12.737269: step 6320, loss = 1.26 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:13.507215: step 6330, loss = 1.29 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:14.284320: step 6340, loss = 1.17 (1647.2 examples/sec; 0.078 sec/batch)
2017-05-05 17:34:15.055382: step 6350, loss = 1.15 (1660.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:15.814214: step 6360, loss = 1.26 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:34:16.582037: step 6370, loss = 1.36 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:17.357080: step 6380, loss = 1.35 (1651.5 examples/sec; 0.078 sec/batch)
2017-05-05 17:34:18.121539: step 6390, loss = 1.37 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:34:18.889142: step 6400, loss = 1.13 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:19.647147: step 6410, loss = 1.15 (1688.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:34:20.413423: step 6420, loss = 1.20 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:21.182631: step 6430, loss = 1.27 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:21.955871: step 6440, loss = 1.28 (1655.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:22.725867: step 6450, loss = 1.19 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:23.488986: step 6460, loss = 1.34 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:34:24.260254: step 6470, loss = 1.34 (1659.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:25.030658: step 6480, loss = 1.16 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:25.794865: step 6490, loss = 1.31 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:34:26.565475: step 6500, loss = 1.26 (1661.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:27.332293: step 6510, loss = 1.17 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:28.097710: step 6520, loss = 1.20 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:28.865430: step 6530, loss = 1.19 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:29.637435: step 6540, loss = 1.15 (1658.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:30.406453: step 6550, loss = 1.39 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:31.168855: step 6560, loss = 1.17 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:34:31.926189: step 6570, loss = 1.30 (1690.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:34:32.701164: step 6580, loss = 1.18 (1651.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:33.473021: step 6590, loss = 1.20 (1658.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:34.242691: step 6600, loss = 1.26 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:35.008210: step 6610, loss = 1.19 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:35.772020: step 6620, loss = 1.19 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:34:36.539901: step 6630, loss = 1.18 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:37.306699: step 6640, loss = 1.29 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:38.078854: step 6650, loss = 1.15 (1657.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:38.844557: step 6660, loss = 1.12 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:39.602671: step 6670, loss = 1.07 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:34:40.373750: step 6680, loss = 1.18 (1660.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:41.132979: step 6690, loss = 1.15 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:34:41.901609: step 6700, loss = 1.24 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:42.677374: step 6710, loss = 1.15 (1650.0 examples/sec; 0.078 sec/batch)
2017-05-05 17:34:43.441382: step 6720, loss = 1.16 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:34:44.205030: step 6730, loss = 1.07 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:34:44.975838: step 6740, loss = 1.18 (1660.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:45.736148: step 6750, loss = 1.17 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:34:46.501232: step 6760, loss = 1.20 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:47.269927: step 6770, loss = 1.42 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:48.027709: step 6780, loss = 1.16 (1689.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:34:48.797574: step 6790, loss = 1.05 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:49.565610: step 6800, loss = 1.24 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:50.335322: step 6810, loss = 1.27 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:51.096503: step 6820, loss = 1.27 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:34:51.857385: step 6830, loss = 1.36 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:34:52.621929: step 6840, loss = 1.20 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:34:53.392828: step 6850, loss = 1.00 (1660.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:54.164693: step 6860, loss = 1.23 (1658.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:54.929211: step 6870, loss = 1.18 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:34:55.683532: step 6880, loss = 1.04 (1696.9 examples/sec; 0.075 sec/batch)
2017-05-05 17:34:56.451298: step 6890, loss = 1.24 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:57.219570: step 6900, loss = 1.15 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:57.989421: step 6910, loss = 1.25 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:58.759539: step 6920, loss = 1.16 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:34:59.634732: step 6930, loss = 1.09 (1462.5 examples/sec; 0.088 sec/batch)
2017-05-05 17:35:00.288385: step 6940, loss = 1.07 (1958.2 examples/sec; 0.065 sec/batch)
2017-05-05 17:35:01.048754: step 6950, loss = 1.11 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:35:01.821272: step 6960, loss = 1.24 (1656.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:02.593075: step 6970, loss = 1.27 (1658.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:03.362629: step 6980, loss = 1.19 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:04.134398: step 6990, loss = 1.15 (1658.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:04.904543: step 7000, loss = 1.13 (1662.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:05.663709: step 7010, loss = 1.14 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:35:06.433510: step 7020, loss = 1.18 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:07.203308: step 7030, loss = 1.32 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:07.967662: step 7040, loss = 1.25 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:35:08.735163: step 7050, loss = 1.32 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:09.502663: step 7060, loss = 1.00 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:10.267717: step 7070, loss = 1.09 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:11.032108: step 7080, loss = 1.12 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:35:11.794564: step 7090, loss = 1.49 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:35:12.566191: step 7100, loss = 1.01 (1658.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:13.338203: step 7110, loss = 0.96 (1658.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:14.112930: step 7120, loss = 1.18 (1652.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:14.888691: step 7130, loss = 1.22 (1650.0 examples/sec; 0.078 sec/batch)
2017-05-05 17:35:15.650572: step 7140, loss = 1.03 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:35:16.412039: step 7150, loss = 0.99 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:35:17.183485: step 7160, loss = 1.33 (1659.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:17.956518: step 7170, loss = 1.05 (1655.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:18.723176: step 7180, loss = 1.17 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:19.480329: step 7190, loss = 1.12 (1690.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:35:20.246527: step 7200, loss = 1.24 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:21.013800: step 7210, loss = 1.17 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:21.782189: step 7220, loss = 1.14 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:22.557196: step 7230, loss = 1.36 (1651.6 examples/sec; 0.078 sec/batch)
2017-05-05 17:35:23.323207: step 7240, loss = 1.17 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:24.084636: step 7250, loss = 1.05 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:35:24.855976: step 7260, loss = 1.14 (1659.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:25.622431: step 7270, loss = 1.07 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:26.393715: step 7280, loss = 0.94 (1659.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:27.161588: step 7290, loss = 1.14 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:27.921501: step 7300, loss = 1.12 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:35:28.691913: step 7310, loss = 1.12 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:29.460675: step 7320, loss = 1.07 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:30.230529: step 7330, loss = 1.14 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:30.989974: step 7340, loss = 1.25 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:35:31.752049: step 7350, loss = 1.21 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:35:32.519065: step 7360, loss = 1.13 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:33.291100: step 7370, loss = 1.09 (1657.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:34.060741: step 7380, loss = 1.01 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:34.827457: step 7390, loss = 0.97 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:35.585213: step 7400, loss = 1.11 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:35:36.351556: step 7410, loss = 1.23 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:37.118922: step 7420, loss = 1.11 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:37.884640: step 7430, loss = 1.12 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:38.647938: step 7440, loss = 1.08 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:35:39.417159: step 7450, loss = 1.17 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:40.177140: step 7460, loss = 1.36 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:35:40.944695: step 7470, loss = 1.12 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:41.709313: step 7480, loss = 1.16 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:35:42.476775: step 7490, loss = 1.16 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:43.247617: step 7500, loss = 1.07 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:44.003255: step 7510, loss = 1.06 (1693.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:35:44.768799: step 7520, loss = 1.20 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:45.543046: step 7530, loss = 1.18 (1653.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:46.310928: step 7540, loss = 1.14 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:47.080552: step 7550, loss = 0.96 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:47.838066: step 7560, loss = 1.18 (1689.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:35:48.611200: step 7570, loss = 1.13 (1655.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:49.376024: step 7580, loss = 1.20 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:35:50.150265: step 7590, loss = 1.16 (1653.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:50.912791: step 7600, loss = 1.30 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:35:51.677204: step 7610, loss = 1.17 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:35:52.452219: step 7620, loss = 1.16 (1651.6 examples/sec; 0.078 sec/batch)
2017-05-05 17:35:53.217143: step 7630, loss = 1.06 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:35:53.985716: step 7640, loss = 0.96 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:54.752692: step 7650, loss = 1.08 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:55.514537: step 7660, loss = 1.02 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:35:56.279769: step 7670, loss = 1.00 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:57.046872: step 7680, loss = 1.18 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:57.807852: step 7690, loss = 1.05 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:35:58.574492: step 7700, loss = 1.04 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:35:59.342289: step 7710, loss = 1.21 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:00.103165: step 7720, loss = 0.86 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:36:00.863500: step 7730, loss = 1.00 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:36:01.626728: step 7740, loss = 1.15 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:36:02.394818: step 7750, loss = 1.27 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:03.165162: step 7760, loss = 1.04 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:03.926308: step 7770, loss = 1.08 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:36:04.694329: step 7780, loss = 1.00 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:05.492191: step 7790, loss = 1.16 (1604.3 examples/sec; 0.080 sec/batch)
2017-05-05 17:36:06.259703: step 7800, loss = 1.18 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:07.028201: step 7810, loss = 1.19 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:07.783347: step 7820, loss = 1.23 (1695.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:36:08.551284: step 7830, loss = 1.16 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:09.323209: step 7840, loss = 1.24 (1658.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:10.094599: step 7850, loss = 0.94 (1659.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:10.862929: step 7860, loss = 0.93 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:11.623520: step 7870, loss = 1.21 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:36:12.396703: step 7880, loss = 0.99 (1655.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:13.164945: step 7890, loss = 1.26 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:13.936403: step 7900, loss = 1.02 (1659.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:14.709057: step 7910, loss = 1.09 (1656.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:15.572710: step 7920, loss = 1.11 (1482.1 examples/sec; 0.086 sec/batch)
2017-05-05 17:36:16.245112: step 7930, loss = 1.30 (1903.6 examples/sec; 0.067 sec/batch)
2017-05-05 17:36:17.014522: step 7940, loss = 1.22 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:17.782581: step 7950, loss = 1.14 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:18.556620: step 7960, loss = 0.97 (1653.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:19.323744: step 7970, loss = 1.22 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:20.080808: step 7980, loss = 0.98 (1690.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:36:20.847217: step 7990, loss = 1.45 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:21.608424: step 8000, loss = 1.11 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:36:22.373851: step 8010, loss = 0.98 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:23.142162: step 8020, loss = 1.17 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:23.903544: step 8030, loss = 1.08 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:36:24.677281: step 8040, loss = 1.17 (1654.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:25.444302: step 8050, loss = 1.09 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:26.202816: step 8060, loss = 1.08 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:36:26.969589: step 8070, loss = 1.10 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:27.725690: step 8080, loss = 1.25 (1692.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:36:28.491732: step 8090, loss = 1.19 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:29.265717: step 8100, loss = 1.17 (1653.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:30.037585: step 8110, loss = 1.29 (1658.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:30.804814: step 8120, loss = 1.18 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:31.568487: step 8130, loss = 0.94 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:36:32.336857: step 8140, loss = 0.97 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:33.101448: step 8150, loss = 1.08 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:36:33.865313: step 8160, loss = 1.28 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:36:34.631475: step 8170, loss = 1.00 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:35.399800: step 8180, loss = 1.05 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:36.159336: step 8190, loss = 1.07 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:36:36.931928: step 8200, loss = 1.09 (1656.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:37.703675: step 8210, loss = 0.98 (1658.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:38.472635: step 8220, loss = 1.01 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:39.239035: step 8230, loss = 0.98 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:39.991307: step 8240, loss = 1.05 (1701.5 examples/sec; 0.075 sec/batch)
2017-05-05 17:36:40.762460: step 8250, loss = 0.97 (1659.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:41.522985: step 8260, loss = 1.17 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:36:42.286596: step 8270, loss = 1.10 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:36:43.054010: step 8280, loss = 1.16 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:43.811322: step 8290, loss = 1.00 (1690.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:36:44.580981: step 8300, loss = 1.05 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:45.356108: step 8310, loss = 1.17 (1651.3 examples/sec; 0.078 sec/batch)
2017-05-05 17:36:46.122262: step 8320, loss = 1.03 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:46.889195: step 8330, loss = 0.99 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:47.645854: step 8340, loss = 1.14 (1691.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:36:48.413886: step 8350, loss = 1.05 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:49.181726: step 8360, loss = 1.19 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:49.941756: step 8370, loss = 1.04 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:36:50.704932: step 8380, loss = 1.02 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:36:51.466951: step 8390, loss = 0.96 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:36:52.235399: step 8400, loss = 1.13 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:53.000213: step 8410, loss = 1.00 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:36:53.767217: step 8420, loss = 0.98 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:54.534664: step 8430, loss = 1.16 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:55.301501: step 8440, loss = 0.87 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:56.060208: step 8450, loss = 1.12 (1687.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:36:56.827255: step 8460, loss = 1.04 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:57.592126: step 8470, loss = 1.13 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:36:58.354963: step 8480, loss = 0.92 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:36:59.122487: step 8490, loss = 1.08 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:36:59.882342: step 8500, loss = 1.04 (1684.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:00.642697: step 8510, loss = 1.18 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:01.404079: step 8520, loss = 1.12 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:02.179703: step 8530, loss = 0.94 (1650.3 examples/sec; 0.078 sec/batch)
2017-05-05 17:37:02.951880: step 8540, loss = 1.07 (1657.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:03.704301: step 8550, loss = 1.25 (1701.2 examples/sec; 0.075 sec/batch)
2017-05-05 17:37:04.473596: step 8560, loss = 0.93 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:05.243776: step 8570, loss = 1.10 (1662.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:06.006015: step 8580, loss = 1.05 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:06.770247: step 8590, loss = 1.02 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:07.528063: step 8600, loss = 1.05 (1689.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:08.292520: step 8610, loss = 1.08 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:09.062913: step 8620, loss = 0.96 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:09.827206: step 8630, loss = 0.99 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:10.591310: step 8640, loss = 1.31 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:11.350007: step 8650, loss = 1.08 (1687.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:12.111125: step 8660, loss = 1.12 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:12.881408: step 8670, loss = 1.02 (1661.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:13.649052: step 8680, loss = 0.90 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:14.415130: step 8690, loss = 1.03 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:15.188438: step 8700, loss = 1.24 (1655.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:15.959732: step 8710, loss = 1.00 (1659.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:16.727819: step 8720, loss = 1.20 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:17.491254: step 8730, loss = 1.07 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:18.255701: step 8740, loss = 1.06 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:19.023983: step 8750, loss = 1.17 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:19.779848: step 8760, loss = 1.22 (1693.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:20.559171: step 8770, loss = 0.96 (1642.4 examples/sec; 0.078 sec/batch)
2017-05-05 17:37:21.315375: step 8780, loss = 0.97 (1692.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:22.074776: step 8790, loss = 1.22 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:22.841718: step 8800, loss = 0.93 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:23.594736: step 8810, loss = 1.02 (1699.8 examples/sec; 0.075 sec/batch)
2017-05-05 17:37:24.362293: step 8820, loss = 0.82 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:25.128088: step 8830, loss = 1.12 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:25.890317: step 8840, loss = 1.02 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:26.657456: step 8850, loss = 1.03 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:27.415735: step 8860, loss = 0.91 (1688.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:28.179388: step 8870, loss = 0.96 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:28.948738: step 8880, loss = 1.05 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:29.717220: step 8890, loss = 1.13 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:30.487432: step 8900, loss = 1.05 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:31.363293: step 8910, loss = 1.15 (1461.4 examples/sec; 0.088 sec/batch)
2017-05-05 17:37:32.023168: step 8920, loss = 1.06 (1939.8 examples/sec; 0.066 sec/batch)
2017-05-05 17:37:32.794190: step 8930, loss = 1.06 (1660.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:33.556839: step 8940, loss = 1.31 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:34.318627: step 8950, loss = 1.10 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:35.083672: step 8960, loss = 0.90 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:35.836058: step 8970, loss = 0.78 (1701.2 examples/sec; 0.075 sec/batch)
2017-05-05 17:37:36.606178: step 8980, loss = 1.00 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:37.377724: step 8990, loss = 1.04 (1659.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:38.144459: step 9000, loss = 1.08 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:38.910598: step 9010, loss = 1.04 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:39.668812: step 9020, loss = 1.12 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:40.441384: step 9030, loss = 1.02 (1656.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:41.201327: step 9040, loss = 0.84 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:41.965089: step 9050, loss = 1.06 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:42.732143: step 9060, loss = 1.17 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:43.481243: step 9070, loss = 0.96 (1708.7 examples/sec; 0.075 sec/batch)
2017-05-05 17:37:44.241758: step 9080, loss = 0.93 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:45.016992: step 9090, loss = 1.21 (1651.1 examples/sec; 0.078 sec/batch)
2017-05-05 17:37:45.779273: step 9100, loss = 1.20 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:46.559152: step 9110, loss = 0.98 (1641.3 examples/sec; 0.078 sec/batch)
2017-05-05 17:37:47.330751: step 9120, loss = 1.15 (1658.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:48.097806: step 9130, loss = 0.93 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:48.865445: step 9140, loss = 0.89 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:49.630030: step 9150, loss = 1.03 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:50.398994: step 9160, loss = 1.22 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:51.162145: step 9170, loss = 0.99 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:51.922849: step 9180, loss = 1.19 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:52.685787: step 9190, loss = 1.05 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:53.456893: step 9200, loss = 1.10 (1660.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:54.220842: step 9210, loss = 1.03 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:54.988672: step 9220, loss = 1.06 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:55.740590: step 9230, loss = 1.06 (1702.3 examples/sec; 0.075 sec/batch)
2017-05-05 17:37:56.507750: step 9240, loss = 1.03 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:57.281484: step 9250, loss = 0.80 (1654.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:37:58.046043: step 9260, loss = 0.82 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:58.809973: step 9270, loss = 1.13 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:37:59.567216: step 9280, loss = 0.91 (1690.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:00.329425: step 9290, loss = 1.04 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:01.092801: step 9300, loss = 1.01 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:01.860152: step 9310, loss = 1.14 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:02.629549: step 9320, loss = 0.93 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:03.392982: step 9330, loss = 1.17 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:04.152588: step 9340, loss = 1.06 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:04.921418: step 9350, loss = 1.02 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:05.683966: step 9360, loss = 1.02 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:06.452544: step 9370, loss = 1.11 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:07.214874: step 9380, loss = 1.11 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:07.970632: step 9390, loss = 1.02 (1693.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:08.736658: step 9400, loss = 1.07 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:09.511612: step 9410, loss = 1.13 (1651.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:10.279691: step 9420, loss = 1.09 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:11.052490: step 9430, loss = 1.03 (1656.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:11.813481: step 9440, loss = 1.17 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:12.582285: step 9450, loss = 1.22 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:13.344149: step 9460, loss = 1.06 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:14.112649: step 9470, loss = 1.09 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:14.881394: step 9480, loss = 1.01 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:15.638281: step 9490, loss = 0.97 (1691.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:16.402711: step 9500, loss = 0.94 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:17.169595: step 9510, loss = 0.98 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:17.936298: step 9520, loss = 1.01 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:18.699989: step 9530, loss = 0.97 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:19.463299: step 9540, loss = 0.93 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:20.225451: step 9550, loss = 1.18 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:20.986813: step 9560, loss = 1.12 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:21.758114: step 9570, loss = 1.15 (1659.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:22.530243: step 9580, loss = 1.10 (1657.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:23.293500: step 9590, loss = 0.86 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:24.056863: step 9600, loss = 1.13 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:24.819506: step 9610, loss = 1.19 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:25.590054: step 9620, loss = 0.95 (1661.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:26.357956: step 9630, loss = 0.94 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:27.126863: step 9640, loss = 0.96 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:27.882838: step 9650, loss = 1.07 (1693.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:28.649092: step 9660, loss = 0.95 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:29.414524: step 9670, loss = 1.12 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:30.191034: step 9680, loss = 0.93 (1648.4 examples/sec; 0.078 sec/batch)
2017-05-05 17:38:30.956228: step 9690, loss = 1.01 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:31.715996: step 9700, loss = 0.85 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:32.486052: step 9710, loss = 0.86 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:33.258572: step 9720, loss = 1.14 (1656.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:34.023890: step 9730, loss = 1.08 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:34.790094: step 9740, loss = 0.93 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:35.545869: step 9750, loss = 1.08 (1693.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:36.306618: step 9760, loss = 1.12 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:37.073130: step 9770, loss = 1.22 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:37.840283: step 9780, loss = 1.15 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:38.611725: step 9790, loss = 0.99 (1659.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:39.382329: step 9800, loss = 1.10 (1661.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:40.140765: step 9810, loss = 0.77 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:40.908553: step 9820, loss = 0.98 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:41.666849: step 9830, loss = 1.05 (1688.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:42.433626: step 9840, loss = 1.00 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:43.195553: step 9850, loss = 0.90 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:43.951878: step 9860, loss = 1.12 (1692.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:44.721014: step 9870, loss = 1.06 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:45.488095: step 9880, loss = 1.00 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:46.256494: step 9890, loss = 0.84 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:47.120261: step 9900, loss = 0.86 (1481.9 examples/sec; 0.086 sec/batch)
2017-05-05 17:38:47.778391: step 9910, loss = 0.93 (1944.9 examples/sec; 0.066 sec/batch)
2017-05-05 17:38:48.541694: step 9920, loss = 1.03 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:49.310431: step 9930, loss = 1.18 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:50.075997: step 9940, loss = 1.09 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:50.864227: step 9950, loss = 0.94 (1623.9 examples/sec; 0.079 sec/batch)
2017-05-05 17:38:51.623628: step 9960, loss = 1.17 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:52.391644: step 9970, loss = 1.03 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:53.151576: step 9980, loss = 0.87 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:53.913966: step 9990, loss = 1.14 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:54.683479: step 10000, loss = 1.13 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:38:55.444856: step 10010, loss = 0.96 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:56.204698: step 10020, loss = 1.18 (1684.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:56.969614: step 10030, loss = 1.06 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:57.732001: step 10040, loss = 1.03 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:58.494387: step 10050, loss = 0.93 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:38:59.260010: step 10060, loss = 1.14 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:00.021330: step 10070, loss = 0.98 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:00.785907: step 10080, loss = 0.85 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:01.546258: step 10090, loss = 1.04 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:02.317181: step 10100, loss = 0.90 (1660.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:03.083099: step 10110, loss = 0.94 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:03.841830: step 10120, loss = 1.26 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:04.609789: step 10130, loss = 0.92 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:05.376119: step 10140, loss = 0.99 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:06.146596: step 10150, loss = 1.12 (1661.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:06.914721: step 10160, loss = 0.94 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:07.673802: step 10170, loss = 1.14 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:08.443312: step 10180, loss = 1.26 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:09.205293: step 10190, loss = 0.85 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:09.973182: step 10200, loss = 1.13 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:10.753103: step 10210, loss = 1.08 (1641.2 examples/sec; 0.078 sec/batch)
2017-05-05 17:39:11.515275: step 10220, loss = 0.93 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:12.273673: step 10230, loss = 1.11 (1687.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:13.035805: step 10240, loss = 1.06 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:13.796012: step 10250, loss = 0.95 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:14.562868: step 10260, loss = 1.16 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:15.323858: step 10270, loss = 0.88 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:16.085214: step 10280, loss = 0.91 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:16.854241: step 10290, loss = 1.02 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:17.614161: step 10300, loss = 0.93 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:18.385550: step 10310, loss = 1.15 (1659.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:19.153481: step 10320, loss = 0.83 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:19.912003: step 10330, loss = 1.07 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:20.677915: step 10340, loss = 1.02 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:21.436051: step 10350, loss = 1.00 (1688.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:22.201089: step 10360, loss = 1.11 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:22.968881: step 10370, loss = 1.07 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:23.728662: step 10380, loss = 0.87 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:24.494954: step 10390, loss = 0.92 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:25.258435: step 10400, loss = 0.96 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:26.030542: step 10410, loss = 1.07 (1657.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:26.790163: step 10420, loss = 1.03 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:27.552140: step 10430, loss = 0.90 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:28.321238: step 10440, loss = 0.92 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:29.088448: step 10450, loss = 1.20 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:29.862809: step 10460, loss = 1.01 (1653.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:30.627328: step 10470, loss = 0.89 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:31.389109: step 10480, loss = 0.91 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:32.145528: step 10490, loss = 1.07 (1692.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:32.923882: step 10500, loss = 0.98 (1644.5 examples/sec; 0.078 sec/batch)
2017-05-05 17:39:33.692613: step 10510, loss = 1.14 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:34.458429: step 10520, loss = 0.95 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:35.222917: step 10530, loss = 1.04 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:35.980256: step 10540, loss = 1.03 (1690.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:36.746686: step 10550, loss = 1.01 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:37.510447: step 10560, loss = 0.86 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:38.271199: step 10570, loss = 1.08 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:39.040772: step 10580, loss = 1.07 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:39.796585: step 10590, loss = 0.95 (1693.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:40.564398: step 10600, loss = 1.16 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:41.327946: step 10610, loss = 0.88 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:42.100406: step 10620, loss = 1.07 (1657.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:42.864436: step 10630, loss = 1.10 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:43.624544: step 10640, loss = 1.23 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:44.385969: step 10650, loss = 0.91 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:45.155146: step 10660, loss = 0.87 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:45.920917: step 10670, loss = 0.91 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:46.682308: step 10680, loss = 0.85 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:47.443529: step 10690, loss = 0.83 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:48.209752: step 10700, loss = 1.06 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:48.979119: step 10710, loss = 0.93 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:49.745880: step 10720, loss = 1.14 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:50.514388: step 10730, loss = 0.87 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:51.281476: step 10740, loss = 0.92 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:52.033676: step 10750, loss = 1.10 (1701.7 examples/sec; 0.075 sec/batch)
2017-05-05 17:39:52.799249: step 10760, loss = 0.92 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:53.566801: step 10770, loss = 1.05 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:54.333473: step 10780, loss = 0.86 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:55.098796: step 10790, loss = 1.13 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:55.856940: step 10800, loss = 0.86 (1688.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:56.622396: step 10810, loss = 0.92 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:57.388576: step 10820, loss = 0.89 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:58.154532: step 10830, loss = 0.95 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:39:58.916259: step 10840, loss = 1.09 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:39:59.670477: step 10850, loss = 0.88 (1697.1 examples/sec; 0.075 sec/batch)
2017-05-05 17:40:00.437244: step 10860, loss = 0.89 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:01.202992: step 10870, loss = 1.03 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:01.975834: step 10880, loss = 0.99 (1656.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:02.854476: step 10890, loss = 1.06 (1456.8 examples/sec; 0.088 sec/batch)
2017-05-05 17:40:03.508228: step 10900, loss = 1.07 (1957.9 examples/sec; 0.065 sec/batch)
2017-05-05 17:40:04.273903: step 10910, loss = 0.95 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:05.031986: step 10920, loss = 1.17 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:05.799990: step 10930, loss = 1.04 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:06.564244: step 10940, loss = 0.89 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:07.329642: step 10950, loss = 0.90 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:08.092263: step 10960, loss = 0.93 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:08.854301: step 10970, loss = 1.06 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:09.621548: step 10980, loss = 0.81 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:10.382085: step 10990, loss = 0.96 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:11.144440: step 11000, loss = 1.03 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:11.899305: step 11010, loss = 1.00 (1695.7 examples/sec; 0.075 sec/batch)
2017-05-05 17:40:12.664552: step 11020, loss = 0.93 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:13.428129: step 11030, loss = 1.02 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:14.194889: step 11040, loss = 1.18 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:14.959973: step 11050, loss = 0.99 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:15.712563: step 11060, loss = 1.02 (1700.8 examples/sec; 0.075 sec/batch)
2017-05-05 17:40:16.480171: step 11070, loss = 1.06 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:17.243880: step 11080, loss = 0.86 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:18.014014: step 11090, loss = 0.90 (1662.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:18.776234: step 11100, loss = 0.85 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:19.539811: step 11110, loss = 1.04 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:20.305535: step 11120, loss = 0.90 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:21.065908: step 11130, loss = 1.10 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:21.832189: step 11140, loss = 0.87 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:22.596078: step 11150, loss = 1.12 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:23.359998: step 11160, loss = 1.15 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:24.121573: step 11170, loss = 1.07 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:24.893593: step 11180, loss = 0.78 (1658.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:25.651667: step 11190, loss = 0.95 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:26.421239: step 11200, loss = 0.92 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:27.188516: step 11210, loss = 0.93 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:27.948392: step 11220, loss = 0.95 (1684.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:28.710561: step 11230, loss = 1.00 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:29.476554: step 11240, loss = 0.76 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:30.242302: step 11250, loss = 0.89 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:31.003285: step 11260, loss = 1.00 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:31.755945: step 11270, loss = 1.30 (1700.6 examples/sec; 0.075 sec/batch)
2017-05-05 17:40:32.523893: step 11280, loss = 0.96 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:33.290925: step 11290, loss = 1.16 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:34.064323: step 11300, loss = 1.11 (1655.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:34.835987: step 11310, loss = 1.04 (1658.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:35.592676: step 11320, loss = 1.10 (1691.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:36.357990: step 11330, loss = 0.98 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:37.130599: step 11340, loss = 0.95 (1656.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:37.898405: step 11350, loss = 0.90 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:38.659046: step 11360, loss = 0.90 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:39.419994: step 11370, loss = 0.98 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:40.181516: step 11380, loss = 0.96 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:40.948724: step 11390, loss = 0.87 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:41.709211: step 11400, loss = 1.10 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:42.472882: step 11410, loss = 0.93 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:43.237373: step 11420, loss = 0.87 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:43.992465: step 11430, loss = 0.76 (1695.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:44.761778: step 11440, loss = 1.02 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:45.530062: step 11450, loss = 1.05 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:46.297617: step 11460, loss = 0.83 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:47.065930: step 11470, loss = 1.05 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:47.820614: step 11480, loss = 0.80 (1696.1 examples/sec; 0.075 sec/batch)
2017-05-05 17:40:48.588487: step 11490, loss = 0.99 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:49.357025: step 11500, loss = 0.87 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:50.120278: step 11510, loss = 0.96 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:50.884906: step 11520, loss = 0.99 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:51.644938: step 11530, loss = 1.04 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:52.408672: step 11540, loss = 0.94 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:53.171259: step 11550, loss = 0.91 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:53.938640: step 11560, loss = 0.83 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:54.705886: step 11570, loss = 0.97 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:55.466088: step 11580, loss = 0.85 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:56.229307: step 11590, loss = 0.83 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:56.990246: step 11600, loss = 0.99 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:57.752780: step 11610, loss = 1.06 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:40:58.519814: step 11620, loss = 0.88 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:40:59.279852: step 11630, loss = 1.00 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:00.038112: step 11640, loss = 1.10 (1688.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:00.807311: step 11650, loss = 0.94 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:01.570273: step 11660, loss = 1.10 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:02.336760: step 11670, loss = 1.03 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:03.105955: step 11680, loss = 0.88 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:03.867684: step 11690, loss = 0.93 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:04.629608: step 11700, loss = 1.00 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:05.386708: step 11710, loss = 1.03 (1690.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:06.152236: step 11720, loss = 0.99 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:06.917848: step 11730, loss = 1.11 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:07.671937: step 11740, loss = 0.89 (1697.4 examples/sec; 0.075 sec/batch)
2017-05-05 17:41:08.441201: step 11750, loss = 0.91 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:09.209886: step 11760, loss = 0.81 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:09.972479: step 11770, loss = 0.83 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:10.738296: step 11780, loss = 0.95 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:11.493804: step 11790, loss = 1.06 (1694.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:12.253868: step 11800, loss = 0.93 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:13.019068: step 11810, loss = 0.97 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:13.780822: step 11820, loss = 1.07 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:14.543525: step 11830, loss = 0.99 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:15.314260: step 11840, loss = 0.94 (1660.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:16.075352: step 11850, loss = 0.90 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:16.833182: step 11860, loss = 1.04 (1689.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:17.603452: step 11870, loss = 0.84 (1661.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:18.471614: step 11880, loss = 0.99 (1474.4 examples/sec; 0.087 sec/batch)
2017-05-05 17:41:19.136182: step 11890, loss = 0.91 (1926.1 examples/sec; 0.066 sec/batch)
2017-05-05 17:41:19.890641: step 11900, loss = 0.92 (1696.6 examples/sec; 0.075 sec/batch)
2017-05-05 17:41:20.659099: step 11910, loss = 0.89 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:21.422322: step 11920, loss = 1.03 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:22.179514: step 11930, loss = 1.03 (1690.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:22.945447: step 11940, loss = 0.90 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:23.702646: step 11950, loss = 1.06 (1690.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:24.468757: step 11960, loss = 0.87 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:25.232828: step 11970, loss = 0.90 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:25.994658: step 11980, loss = 0.89 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:26.760590: step 11990, loss = 1.11 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:27.522094: step 12000, loss = 0.79 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:28.286119: step 12010, loss = 1.04 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:29.050777: step 12020, loss = 0.96 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:29.816753: step 12030, loss = 1.01 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:30.590569: step 12040, loss = 1.19 (1654.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:31.353198: step 12050, loss = 1.02 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:32.113803: step 12060, loss = 0.93 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:32.878287: step 12070, loss = 0.98 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:33.642323: step 12080, loss = 0.95 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:34.408167: step 12090, loss = 0.84 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:35.183849: step 12100, loss = 1.11 (1650.2 examples/sec; 0.078 sec/batch)
2017-05-05 17:41:35.937237: step 12110, loss = 1.06 (1699.0 examples/sec; 0.075 sec/batch)
2017-05-05 17:41:36.696313: step 12120, loss = 0.99 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:37.463594: step 12130, loss = 0.99 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:38.232576: step 12140, loss = 1.01 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:38.999517: step 12150, loss = 1.00 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:39.768112: step 12160, loss = 0.90 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:40.538269: step 12170, loss = 1.11 (1662.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:41.311145: step 12180, loss = 1.03 (1656.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:42.090683: step 12190, loss = 0.93 (1642.0 examples/sec; 0.078 sec/batch)
2017-05-05 17:41:42.877182: step 12200, loss = 0.80 (1627.5 examples/sec; 0.079 sec/batch)
2017-05-05 17:41:43.642063: step 12210, loss = 0.93 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:44.410091: step 12220, loss = 0.94 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:45.184002: step 12230, loss = 0.92 (1653.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:45.942572: step 12240, loss = 0.85 (1687.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:46.706490: step 12250, loss = 0.88 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:47.468815: step 12260, loss = 0.91 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:48.232658: step 12270, loss = 1.15 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:49.001001: step 12280, loss = 0.92 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:49.767643: step 12290, loss = 1.25 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:50.536088: step 12300, loss = 0.93 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:51.304952: step 12310, loss = 1.03 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:52.064022: step 12320, loss = 0.92 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:52.828523: step 12330, loss = 1.04 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:53.595499: step 12340, loss = 1.09 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:54.364652: step 12350, loss = 1.10 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:55.132736: step 12360, loss = 0.83 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:55.894119: step 12370, loss = 0.93 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:56.661085: step 12380, loss = 0.93 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:57.428732: step 12390, loss = 0.78 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:58.197561: step 12400, loss = 0.91 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:41:58.955439: step 12410, loss = 1.05 (1688.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:41:59.713312: step 12420, loss = 0.93 (1688.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:00.480243: step 12430, loss = 0.88 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:01.246970: step 12440, loss = 1.01 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:02.006665: step 12450, loss = 0.89 (1684.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:02.771090: step 12460, loss = 0.93 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:03.527527: step 12470, loss = 0.70 (1692.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:04.296776: step 12480, loss = 0.95 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:05.062136: step 12490, loss = 1.00 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:05.827851: step 12500, loss = 0.99 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:06.590064: step 12510, loss = 1.04 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:07.350955: step 12520, loss = 1.00 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:08.109171: step 12530, loss = 0.79 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:08.879062: step 12540, loss = 0.91 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:09.641418: step 12550, loss = 0.87 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:10.415249: step 12560, loss = 1.10 (1654.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:11.182240: step 12570, loss = 1.02 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:11.940420: step 12580, loss = 0.98 (1688.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:12.712025: step 12590, loss = 0.96 (1658.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:13.489967: step 12600, loss = 1.04 (1645.4 examples/sec; 0.078 sec/batch)
2017-05-05 17:42:14.251224: step 12610, loss = 1.04 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:15.017518: step 12620, loss = 0.83 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:15.779186: step 12630, loss = 0.98 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:16.537263: step 12640, loss = 1.04 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:17.306903: step 12650, loss = 0.96 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:18.070060: step 12660, loss = 0.92 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:18.835545: step 12670, loss = 1.08 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:19.596606: step 12680, loss = 1.14 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:20.358549: step 12690, loss = 0.84 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:21.127928: step 12700, loss = 0.99 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:21.887184: step 12710, loss = 0.95 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:22.650799: step 12720, loss = 0.90 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:23.410935: step 12730, loss = 0.80 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:24.175678: step 12740, loss = 0.88 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:24.934885: step 12750, loss = 0.97 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:25.701737: step 12760, loss = 0.99 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:26.465563: step 12770, loss = 0.91 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:27.234180: step 12780, loss = 0.98 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:27.994872: step 12790, loss = 1.17 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:28.754106: step 12800, loss = 0.87 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:29.522520: step 12810, loss = 0.96 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:30.295782: step 12820, loss = 1.01 (1655.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:31.060125: step 12830, loss = 0.81 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:31.813296: step 12840, loss = 0.95 (1699.5 examples/sec; 0.075 sec/batch)
2017-05-05 17:42:32.581326: step 12850, loss = 1.04 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:33.347388: step 12860, loss = 1.04 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:34.208510: step 12870, loss = 1.00 (1486.4 examples/sec; 0.086 sec/batch)
2017-05-05 17:42:34.875776: step 12880, loss = 1.09 (1918.3 examples/sec; 0.067 sec/batch)
2017-05-05 17:42:35.642577: step 12890, loss = 0.90 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:36.406503: step 12900, loss = 0.99 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:37.167822: step 12910, loss = 0.85 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:37.936463: step 12920, loss = 1.01 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:38.701678: step 12930, loss = 0.88 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:39.457033: step 12940, loss = 0.98 (1694.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:40.217644: step 12950, loss = 0.91 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:40.979028: step 12960, loss = 0.87 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:41.741803: step 12970, loss = 0.87 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:42.512026: step 12980, loss = 0.80 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:43.273105: step 12990, loss = 1.00 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:44.032045: step 13000, loss = 0.93 (1686.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:44.796978: step 13010, loss = 1.02 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:45.568145: step 13020, loss = 0.90 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:46.333860: step 13030, loss = 0.94 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:47.099155: step 13040, loss = 1.04 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:47.854694: step 13050, loss = 1.01 (1694.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:48.623886: step 13060, loss = 0.90 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:49.391221: step 13070, loss = 1.04 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:50.153080: step 13080, loss = 1.11 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:50.912241: step 13090, loss = 0.94 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:51.672014: step 13100, loss = 0.91 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:52.439187: step 13110, loss = 0.87 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:53.217220: step 13120, loss = 0.80 (1645.2 examples/sec; 0.078 sec/batch)
2017-05-05 17:42:53.983516: step 13130, loss = 1.00 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:54.751459: step 13140, loss = 0.86 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:55.511956: step 13150, loss = 0.91 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:56.269250: step 13160, loss = 1.08 (1690.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:42:57.037215: step 13170, loss = 0.96 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:57.805428: step 13180, loss = 1.01 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:58.578203: step 13190, loss = 1.02 (1656.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:42:59.338020: step 13200, loss = 0.95 (1684.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:00.091655: step 13210, loss = 0.94 (1698.4 examples/sec; 0.075 sec/batch)
2017-05-05 17:43:00.858800: step 13220, loss = 1.04 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:43:01.619392: step 13230, loss = 0.85 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:02.392326: step 13240, loss = 0.93 (1656.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:43:03.156295: step 13250, loss = 0.94 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:03.920375: step 13260, loss = 0.99 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:04.692128: step 13270, loss = 0.78 (1658.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:43:05.457475: step 13280, loss = 0.97 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:43:06.223655: step 13290, loss = 0.85 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:43:06.984452: step 13300, loss = 0.88 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:07.740302: step 13310, loss = 1.08 (1693.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:08.512665: step 13320, loss = 0.92 (1657.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:43:09.281972: step 13330, loss = 1.03 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:43:10.046697: step 13340, loss = 0.86 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:10.812219: step 13350, loss = 0.76 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:43:11.562794: step 13360, loss = 0.84 (1705.3 examples/sec; 0.075 sec/batch)
2017-05-05 17:43:12.325110: step 13370, loss = 1.06 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:13.105874: step 13380, loss = 1.00 (1639.4 examples/sec; 0.078 sec/batch)
2017-05-05 17:43:13.867407: step 13390, loss = 1.05 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:14.635901: step 13400, loss = 1.07 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:43:15.400416: step 13410, loss = 1.00 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:16.162302: step 13420, loss = 0.95 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:16.927028: step 13430, loss = 0.93 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:17.692876: step 13440, loss = 0.87 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:43:18.464081: step 13450, loss = 0.90 (1659.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:43:19.233477: step 13460, loss = 0.95 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:43:19.990853: step 13470, loss = 0.75 (1690.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:20.748887: step 13480, loss = 0.86 (1688.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:21.517975: step 13490, loss = 1.01 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:43:22.281789: step 13500, loss = 1.10 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:23.042445: step 13510, loss = 0.94 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:23.797990: step 13520, loss = 0.99 (1694.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:24.562192: step 13530, loss = 0.77 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:25.329054: step 13540, loss = 0.70 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:43:26.089520: step 13550, loss = 0.84 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:26.850819: step 13560, loss = 0.88 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:27.607946: step 13570, loss = 0.84 (1690.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:28.372826: step 13580, loss = 0.92 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:29.134525: step 13590, loss = 1.05 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:29.903948: step 13600, loss = 1.06 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:43:30.669774: step 13610, loss = 0.80 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:43:31.432065: step 13620, loss = 1.02 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:32.190801: step 13630, loss = 0.88 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:32.956669: step 13640, loss = 0.83 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:43:33.725255: step 13650, loss = 0.90 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:43:34.490423: step 13660, loss = 0.99 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:43:35.264228: step 13670, loss = 0.84 (1654.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:43:36.020994: step 13680, loss = 1.20 (1691.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:36.785203: step 13690, loss = 0.88 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:37.549840: step 13700, loss = 1.08 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:38.314613: step 13710, loss = 1.03 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:39.082469: step 13720, loss = 0.86 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:43:39.843146: step 13730, loss = 0.89 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:40.614383: step 13740, loss = 1.04 (1659.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:43:41.382063: step 13750, loss = 1.07 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:43:42.144668: step 13760, loss = 1.19 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:42.910239: step 13770, loss = 0.79 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:43:43.663550: step 13780, loss = 0.79 (1699.2 examples/sec; 0.075 sec/batch)
2017-05-05 17:43:44.429396: step 13790, loss = 0.89 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:43:45.188975: step 13800, loss = 0.86 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:45.951026: step 13810, loss = 1.04 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:46.715520: step 13820, loss = 0.97 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:47.470503: step 13830, loss = 0.88 (1695.4 examples/sec; 0.075 sec/batch)
2017-05-05 17:43:48.232107: step 13840, loss = 0.86 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:48.998799: step 13850, loss = 0.87 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:43:49.863566: step 13860, loss = 0.85 (1480.2 examples/sec; 0.086 sec/batch)
2017-05-05 17:43:50.530825: step 13870, loss = 0.92 (1918.3 examples/sec; 0.067 sec/batch)
2017-05-05 17:43:51.291607: step 13880, loss = 1.11 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:52.047576: step 13890, loss = 0.82 (1693.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:52.813987: step 13900, loss = 1.01 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:43:53.576736: step 13910, loss = 1.02 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:54.342278: step 13920, loss = 0.86 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:43:55.114703: step 13930, loss = 0.89 (1657.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:43:55.868753: step 13940, loss = 0.89 (1697.5 examples/sec; 0.075 sec/batch)
2017-05-05 17:43:56.633150: step 13950, loss = 0.81 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:57.394859: step 13960, loss = 0.84 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:58.154519: step 13970, loss = 0.95 (1685.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:43:58.923343: step 13980, loss = 0.96 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:43:59.682760: step 13990, loss = 0.88 (1685.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:00.450939: step 14000, loss = 0.96 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:01.220968: step 14010, loss = 0.86 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:01.977800: step 14020, loss = 0.79 (1691.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:02.744744: step 14030, loss = 0.81 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:03.507975: step 14040, loss = 0.91 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:04.273138: step 14050, loss = 1.19 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:05.042289: step 14060, loss = 0.92 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:05.810836: step 14070, loss = 0.97 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:06.576476: step 14080, loss = 0.91 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:07.335328: step 14090, loss = 0.82 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:08.091457: step 14100, loss = 0.90 (1692.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:08.855692: step 14110, loss = 0.99 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:09.627304: step 14120, loss = 0.84 (1658.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:10.399057: step 14130, loss = 0.84 (1658.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:11.170691: step 14140, loss = 0.80 (1658.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:11.929071: step 14150, loss = 0.86 (1687.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:12.693869: step 14160, loss = 0.99 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:13.463526: step 14170, loss = 1.02 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:14.229864: step 14180, loss = 0.92 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:14.998565: step 14190, loss = 0.92 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:15.752902: step 14200, loss = 0.92 (1696.8 examples/sec; 0.075 sec/batch)
2017-05-05 17:44:16.516126: step 14210, loss = 0.98 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:17.280535: step 14220, loss = 0.85 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:18.043106: step 14230, loss = 1.05 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:18.816461: step 14240, loss = 0.83 (1655.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:19.574230: step 14250, loss = 1.07 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:20.339986: step 14260, loss = 1.05 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:21.102699: step 14270, loss = 0.81 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:21.864667: step 14280, loss = 0.95 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:22.630389: step 14290, loss = 0.92 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:23.397784: step 14300, loss = 0.98 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:24.156379: step 14310, loss = 0.96 (1687.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:24.920956: step 14320, loss = 0.83 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:25.690520: step 14330, loss = 0.79 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:26.451544: step 14340, loss = 0.90 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:27.221202: step 14350, loss = 0.79 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:27.974473: step 14360, loss = 0.98 (1699.3 examples/sec; 0.075 sec/batch)
2017-05-05 17:44:28.737522: step 14370, loss = 0.97 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:29.501548: step 14380, loss = 1.00 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:30.275025: step 14390, loss = 0.81 (1654.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:31.034406: step 14400, loss = 0.91 (1685.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:31.783347: step 14410, loss = 0.84 (1709.1 examples/sec; 0.075 sec/batch)
2017-05-05 17:44:32.549228: step 14420, loss = 0.99 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:33.316370: step 14430, loss = 0.88 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:34.077550: step 14440, loss = 0.76 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:34.839683: step 14450, loss = 1.01 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:35.594414: step 14460, loss = 0.74 (1696.0 examples/sec; 0.075 sec/batch)
2017-05-05 17:44:36.361547: step 14470, loss = 1.14 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:37.120302: step 14480, loss = 0.91 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:37.887337: step 14490, loss = 0.96 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:38.648419: step 14500, loss = 0.90 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:39.407995: step 14510, loss = 0.92 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:40.171491: step 14520, loss = 0.83 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:40.936046: step 14530, loss = 0.88 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:41.694887: step 14540, loss = 0.95 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:42.465584: step 14550, loss = 0.92 (1660.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:43.230616: step 14560, loss = 0.88 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:43.986054: step 14570, loss = 1.09 (1694.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:44.752117: step 14580, loss = 0.73 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:45.523290: step 14590, loss = 0.83 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:46.284423: step 14600, loss = 1.00 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:47.048891: step 14610, loss = 1.25 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:47.799210: step 14620, loss = 0.83 (1705.9 examples/sec; 0.075 sec/batch)
2017-05-05 17:44:48.574204: step 14630, loss = 0.99 (1651.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:49.337285: step 14640, loss = 0.86 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:50.109690: step 14650, loss = 0.87 (1657.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:50.879196: step 14660, loss = 0.88 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:51.643014: step 14670, loss = 0.93 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:52.410192: step 14680, loss = 0.80 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:53.177016: step 14690, loss = 1.07 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:53.947798: step 14700, loss = 1.06 (1660.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:54.713910: step 14710, loss = 0.83 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:55.490148: step 14720, loss = 0.91 (1649.0 examples/sec; 0.078 sec/batch)
2017-05-05 17:44:56.254461: step 14730, loss = 1.00 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:57.018316: step 14740, loss = 1.04 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:57.783971: step 14750, loss = 0.99 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:44:58.545263: step 14760, loss = 0.95 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:44:59.309899: step 14770, loss = 0.98 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:45:00.070447: step 14780, loss = 0.82 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:45:00.834949: step 14790, loss = 0.97 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:45:01.598896: step 14800, loss = 0.93 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:45:02.374851: step 14810, loss = 0.94 (1649.6 examples/sec; 0.078 sec/batch)
2017-05-05 17:45:03.135618: step 14820, loss = 0.81 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:45:03.897150: step 14830, loss = 0.97 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:45:04.668009: step 14840, loss = 0.81 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:05.536473: step 14850, loss = 0.88 (1473.9 examples/sec; 0.087 sec/batch)
2017-05-05 17:45:06.202956: step 14860, loss = 1.01 (1920.5 examples/sec; 0.067 sec/batch)
2017-05-05 17:45:06.970339: step 14870, loss = 0.91 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:07.725036: step 14880, loss = 0.87 (1696.0 examples/sec; 0.075 sec/batch)
2017-05-05 17:45:08.491860: step 14890, loss = 1.06 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:09.263196: step 14900, loss = 0.83 (1659.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:10.033653: step 14910, loss = 0.82 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:10.796811: step 14920, loss = 0.92 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:45:11.555250: step 14930, loss = 0.89 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:45:12.325438: step 14940, loss = 0.85 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:13.091418: step 14950, loss = 1.05 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:13.857320: step 14960, loss = 0.90 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:14.623623: step 14970, loss = 1.09 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:15.391578: step 14980, loss = 0.93 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:16.153588: step 14990, loss = 0.97 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:45:16.912908: step 15000, loss = 0.82 (1685.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:45:17.685103: step 15010, loss = 0.79 (1657.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:18.455493: step 15020, loss = 0.91 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:19.227978: step 15030, loss = 1.00 (1657.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:19.980555: step 15040, loss = 0.79 (1700.8 examples/sec; 0.075 sec/batch)
2017-05-05 17:45:20.751665: step 15050, loss = 1.03 (1659.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:21.520283: step 15060, loss = 0.87 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:22.281683: step 15070, loss = 0.77 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:45:23.049580: step 15080, loss = 1.03 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:23.804214: step 15090, loss = 0.87 (1696.2 examples/sec; 0.075 sec/batch)
2017-05-05 17:45:24.565943: step 15100, loss = 0.85 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:45:25.337177: step 15110, loss = 1.08 (1659.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:26.105612: step 15120, loss = 0.79 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:26.868972: step 15130, loss = 0.89 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:45:27.626699: step 15140, loss = 1.15 (1689.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:45:28.394436: step 15150, loss = 1.01 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:29.161448: step 15160, loss = 1.04 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:29.932878: step 15170, loss = 0.95 (1659.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:30.697023: step 15180, loss = 0.72 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:45:31.457344: step 15190, loss = 1.00 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:45:32.222665: step 15200, loss = 0.97 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:32.986498: step 15210, loss = 0.76 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:45:33.757980: step 15220, loss = 0.94 (1659.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:34.524092: step 15230, loss = 0.84 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:35.287628: step 15240, loss = 0.83 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:45:36.048241: step 15250, loss = 0.87 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:45:36.815711: step 15260, loss = 0.85 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:37.583983: step 15270, loss = 0.91 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:38.351955: step 15280, loss = 0.90 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:39.112383: step 15290, loss = 0.95 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:45:39.871069: step 15300, loss = 0.94 (1687.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:45:40.632295: step 15310, loss = 0.92 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:45:41.403171: step 15320, loss = 0.93 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:42.169684: step 15330, loss = 1.05 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:42.938865: step 15340, loss = 0.91 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:43.693620: step 15350, loss = 1.10 (1695.9 examples/sec; 0.075 sec/batch)
2017-05-05 17:45:44.463627: step 15360, loss = 0.98 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:45.230400: step 15370, loss = 0.84 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:45.997531: step 15380, loss = 0.96 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:46.759968: step 15390, loss = 0.98 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:45:47.519405: step 15400, loss = 0.88 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:45:48.287655: step 15410, loss = 0.80 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:49.052753: step 15420, loss = 0.93 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:49.824970: step 15430, loss = 0.91 (1657.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:50.593570: step 15440, loss = 0.87 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:51.354901: step 15450, loss = 0.88 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:45:52.116974: step 15460, loss = 0.99 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:45:52.880782: step 15470, loss = 0.96 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:45:53.655015: step 15480, loss = 1.01 (1653.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:54.423953: step 15490, loss = 1.04 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:55.179492: step 15500, loss = 0.86 (1694.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:45:55.939738: step 15510, loss = 0.85 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:45:56.711289: step 15520, loss = 0.88 (1659.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:57.479520: step 15530, loss = 1.09 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:58.246151: step 15540, loss = 0.95 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:45:59.007023: step 15550, loss = 0.97 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:45:59.766223: step 15560, loss = 0.91 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:00.534983: step 15570, loss = 1.00 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:01.302572: step 15580, loss = 0.95 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:02.063935: step 15590, loss = 0.90 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:02.829583: step 15600, loss = 0.86 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:03.584857: step 15610, loss = 0.94 (1694.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:04.348658: step 15620, loss = 0.81 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:05.116191: step 15630, loss = 0.88 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:05.916928: step 15640, loss = 0.91 (1598.5 examples/sec; 0.080 sec/batch)
2017-05-05 17:46:06.686408: step 15650, loss = 1.00 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:07.441367: step 15660, loss = 1.02 (1695.4 examples/sec; 0.075 sec/batch)
2017-05-05 17:46:08.205931: step 15670, loss = 0.85 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:08.972397: step 15680, loss = 1.05 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:09.736864: step 15690, loss = 0.88 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:10.506170: step 15700, loss = 1.02 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:11.275472: step 15710, loss = 0.94 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:12.036775: step 15720, loss = 0.82 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:12.797184: step 15730, loss = 0.94 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:13.564579: step 15740, loss = 1.10 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:14.332456: step 15750, loss = 0.96 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:15.101801: step 15760, loss = 0.91 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:15.862571: step 15770, loss = 0.89 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:16.622330: step 15780, loss = 0.88 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:17.382062: step 15790, loss = 0.90 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:18.151186: step 15800, loss = 0.96 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:18.913483: step 15810, loss = 0.89 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:19.668504: step 15820, loss = 0.82 (1695.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:20.438945: step 15830, loss = 0.89 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:21.325828: step 15840, loss = 0.69 (1443.2 examples/sec; 0.089 sec/batch)
2017-05-05 17:46:21.972315: step 15850, loss = 1.10 (1979.9 examples/sec; 0.065 sec/batch)
2017-05-05 17:46:22.742924: step 15860, loss = 0.82 (1661.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:23.500329: step 15870, loss = 0.75 (1690.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:24.261751: step 15880, loss = 0.97 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:25.030243: step 15890, loss = 0.87 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:25.795968: step 15900, loss = 0.92 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:26.558056: step 15910, loss = 1.13 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:27.322396: step 15920, loss = 0.72 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:28.083134: step 15930, loss = 0.93 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:28.854448: step 15940, loss = 0.97 (1659.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:29.616313: step 15950, loss = 0.86 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:30.383535: step 15960, loss = 0.98 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:31.151852: step 15970, loss = 0.77 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:31.905000: step 15980, loss = 0.91 (1699.6 examples/sec; 0.075 sec/batch)
2017-05-05 17:46:32.668324: step 15990, loss = 1.04 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:33.439549: step 16000, loss = 0.85 (1659.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:34.210508: step 16010, loss = 0.87 (1660.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:34.970601: step 16020, loss = 0.76 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:35.728515: step 16030, loss = 0.98 (1688.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:36.494897: step 16040, loss = 0.79 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:37.258403: step 16050, loss = 0.74 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:38.018962: step 16060, loss = 0.73 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:38.786219: step 16070, loss = 0.91 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:39.543258: step 16080, loss = 0.82 (1690.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:40.306384: step 16090, loss = 0.88 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:41.076483: step 16100, loss = 0.93 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:41.842440: step 16110, loss = 0.91 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:42.612002: step 16120, loss = 0.97 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:43.376188: step 16130, loss = 0.88 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:44.139545: step 16140, loss = 0.68 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:44.899240: step 16150, loss = 0.81 (1684.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:45.668601: step 16160, loss = 0.83 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:46.436612: step 16170, loss = 0.81 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:47.201405: step 16180, loss = 0.98 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:47.956198: step 16190, loss = 0.80 (1695.8 examples/sec; 0.075 sec/batch)
2017-05-05 17:46:48.728074: step 16200, loss = 1.05 (1658.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:49.493279: step 16210, loss = 0.92 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:50.262692: step 16220, loss = 0.83 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:51.033941: step 16230, loss = 0.95 (1659.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:51.795881: step 16240, loss = 0.93 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:52.567899: step 16250, loss = 0.85 (1658.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:53.332865: step 16260, loss = 0.92 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:54.104236: step 16270, loss = 0.92 (1659.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:54.875007: step 16280, loss = 0.98 (1660.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:55.638133: step 16290, loss = 0.83 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:56.409146: step 16300, loss = 0.93 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:57.175812: step 16310, loss = 0.90 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:57.946957: step 16320, loss = 0.88 (1659.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:46:58.708242: step 16330, loss = 0.94 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:46:59.475024: step 16340, loss = 0.93 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:00.245587: step 16350, loss = 0.83 (1661.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:01.015840: step 16360, loss = 1.02 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:01.779515: step 16370, loss = 0.81 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:47:02.549078: step 16380, loss = 1.13 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:03.320968: step 16390, loss = 1.08 (1658.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:04.086413: step 16400, loss = 1.05 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:04.859447: step 16410, loss = 0.80 (1655.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:05.625034: step 16420, loss = 0.78 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:06.394364: step 16430, loss = 0.89 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:07.160853: step 16440, loss = 0.96 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:07.920468: step 16450, loss = 0.91 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:47:08.689324: step 16460, loss = 0.76 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:09.449862: step 16470, loss = 0.93 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:47:10.213181: step 16480, loss = 1.00 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:47:10.972880: step 16490, loss = 1.19 (1684.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:47:11.732660: step 16500, loss = 0.83 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:47:12.495473: step 16510, loss = 0.82 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:47:13.271625: step 16520, loss = 0.95 (1649.2 examples/sec; 0.078 sec/batch)
2017-05-05 17:47:14.037058: step 16530, loss = 0.93 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:14.793530: step 16540, loss = 0.97 (1692.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:47:15.554553: step 16550, loss = 0.91 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:47:16.318943: step 16560, loss = 1.01 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:47:17.085693: step 16570, loss = 0.77 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:17.853136: step 16580, loss = 1.04 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:18.617300: step 16590, loss = 1.05 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:47:19.377447: step 16600, loss = 1.00 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:47:20.135884: step 16610, loss = 0.97 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:47:20.903988: step 16620, loss = 0.88 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:21.662913: step 16630, loss = 1.04 (1686.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:47:22.428856: step 16640, loss = 0.92 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:23.197240: step 16650, loss = 0.94 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:23.948568: step 16660, loss = 0.85 (1703.6 examples/sec; 0.075 sec/batch)
2017-05-05 17:47:24.715943: step 16670, loss = 0.90 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:25.486248: step 16680, loss = 1.07 (1661.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:26.255017: step 16690, loss = 0.80 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:27.022065: step 16700, loss = 0.83 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:27.774784: step 16710, loss = 1.02 (1700.5 examples/sec; 0.075 sec/batch)
2017-05-05 17:47:28.541189: step 16720, loss = 0.92 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:29.311158: step 16730, loss = 0.69 (1662.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:30.078036: step 16740, loss = 0.87 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:30.839414: step 16750, loss = 0.99 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:47:31.593163: step 16760, loss = 0.80 (1698.2 examples/sec; 0.075 sec/batch)
2017-05-05 17:47:32.358860: step 16770, loss = 0.93 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:33.123473: step 16780, loss = 0.97 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:47:33.888203: step 16790, loss = 0.95 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:47:34.653354: step 16800, loss = 1.13 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:35.415179: step 16810, loss = 0.79 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:47:36.173673: step 16820, loss = 0.91 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:47:37.040623: step 16830, loss = 0.86 (1476.4 examples/sec; 0.087 sec/batch)
2017-05-05 17:47:37.717515: step 16840, loss = 0.70 (1891.0 examples/sec; 0.068 sec/batch)
2017-05-05 17:47:38.483646: step 16850, loss = 1.08 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:39.247425: step 16860, loss = 1.03 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:47:40.005041: step 16870, loss = 0.97 (1689.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:47:40.770810: step 16880, loss = 1.01 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:41.530706: step 16890, loss = 0.91 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:47:42.302161: step 16900, loss = 0.84 (1659.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:43.072838: step 16910, loss = 0.80 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:43.828064: step 16920, loss = 0.90 (1694.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:47:44.594815: step 16930, loss = 0.99 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:45.364579: step 16940, loss = 0.92 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:46.129894: step 16950, loss = 0.84 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:46.898324: step 16960, loss = 0.84 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:47.652048: step 16970, loss = 0.95 (1698.2 examples/sec; 0.075 sec/batch)
2017-05-05 17:47:48.423025: step 16980, loss = 0.84 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:49.185270: step 16990, loss = 0.87 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:47:49.952018: step 17000, loss = 0.67 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:50.725094: step 17010, loss = 0.94 (1655.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:51.486236: step 17020, loss = 0.81 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:47:52.254813: step 17030, loss = 0.79 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:53.021979: step 17040, loss = 1.01 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:53.783181: step 17050, loss = 0.94 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:47:54.551497: step 17060, loss = 0.73 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:55.319166: step 17070, loss = 0.85 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:56.077892: step 17080, loss = 0.98 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:47:56.847982: step 17090, loss = 0.90 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:57.620981: step 17100, loss = 0.96 (1655.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:58.390976: step 17110, loss = 1.00 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:59.164055: step 17120, loss = 0.77 (1655.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:47:59.916573: step 17130, loss = 1.04 (1701.0 examples/sec; 0.075 sec/batch)
2017-05-05 17:48:00.688401: step 17140, loss = 1.03 (1658.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:01.448599: step 17150, loss = 0.66 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:02.216567: step 17160, loss = 0.62 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:02.980115: step 17170, loss = 0.84 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:03.736634: step 17180, loss = 0.89 (1692.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:04.501879: step 17190, loss = 0.84 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:05.272723: step 17200, loss = 0.95 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:06.040783: step 17210, loss = 0.74 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:06.802696: step 17220, loss = 0.93 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:07.556491: step 17230, loss = 0.82 (1698.1 examples/sec; 0.075 sec/batch)
2017-05-05 17:48:08.325094: step 17240, loss = 1.04 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:09.086128: step 17250, loss = 0.95 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:09.856502: step 17260, loss = 0.83 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:10.623200: step 17270, loss = 0.84 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:11.391810: step 17280, loss = 0.84 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:12.159709: step 17290, loss = 0.80 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:12.917814: step 17300, loss = 0.97 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:13.684969: step 17310, loss = 0.91 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:14.457551: step 17320, loss = 0.82 (1656.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:15.222217: step 17330, loss = 0.81 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:15.980026: step 17340, loss = 1.00 (1689.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:16.742584: step 17350, loss = 0.92 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:17.510525: step 17360, loss = 0.84 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:18.282042: step 17370, loss = 0.63 (1659.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:19.044759: step 17380, loss = 0.86 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:19.798213: step 17390, loss = 0.78 (1698.8 examples/sec; 0.075 sec/batch)
2017-05-05 17:48:20.565041: step 17400, loss = 1.12 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:21.321410: step 17410, loss = 0.76 (1692.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:22.087771: step 17420, loss = 1.02 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:22.862380: step 17430, loss = 0.75 (1652.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:23.615000: step 17440, loss = 0.78 (1700.7 examples/sec; 0.075 sec/batch)
2017-05-05 17:48:24.380204: step 17450, loss = 1.03 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:25.147819: step 17460, loss = 1.01 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:25.919018: step 17470, loss = 1.26 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:26.691780: step 17480, loss = 0.78 (1656.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:27.455636: step 17490, loss = 1.11 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:28.221637: step 17500, loss = 0.95 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:28.988760: step 17510, loss = 1.08 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:29.755457: step 17520, loss = 0.85 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:30.520282: step 17530, loss = 0.80 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:31.292328: step 17540, loss = 0.81 (1657.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:32.055745: step 17550, loss = 0.84 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:32.817681: step 17560, loss = 1.08 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:33.591504: step 17570, loss = 0.80 (1654.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:34.351827: step 17580, loss = 0.88 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:35.113484: step 17590, loss = 0.74 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:35.870244: step 17600, loss = 0.94 (1691.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:36.635014: step 17610, loss = 0.83 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:37.397998: step 17620, loss = 0.87 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:38.169192: step 17630, loss = 0.85 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:38.934825: step 17640, loss = 0.70 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:39.690744: step 17650, loss = 0.88 (1693.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:40.460405: step 17660, loss = 0.90 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:41.223766: step 17670, loss = 0.78 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:41.993165: step 17680, loss = 0.91 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:42.758184: step 17690, loss = 0.87 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:43.518705: step 17700, loss = 0.97 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:44.280916: step 17710, loss = 0.79 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:45.047275: step 17720, loss = 0.87 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:45.810471: step 17730, loss = 0.76 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:46.571269: step 17740, loss = 0.88 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:47.336217: step 17750, loss = 1.06 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:48.098602: step 17760, loss = 0.93 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:48.860904: step 17770, loss = 0.79 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:49.624511: step 17780, loss = 0.79 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:50.395488: step 17790, loss = 0.88 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:51.159912: step 17800, loss = 1.10 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:51.915084: step 17810, loss = 0.95 (1695.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:52.783653: step 17820, loss = 0.99 (1473.7 examples/sec; 0.087 sec/batch)
2017-05-05 17:48:53.453337: step 17830, loss = 0.95 (1911.3 examples/sec; 0.067 sec/batch)
2017-05-05 17:48:54.218066: step 17840, loss = 0.84 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:54.977962: step 17850, loss = 0.82 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:55.736087: step 17860, loss = 0.93 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:56.500572: step 17870, loss = 0.89 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:57.267883: step 17880, loss = 0.93 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:48:58.032729: step 17890, loss = 0.99 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:58.793724: step 17900, loss = 0.90 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:48:59.551459: step 17910, loss = 1.05 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:00.315139: step 17920, loss = 0.92 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:01.077658: step 17930, loss = 0.84 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:01.836408: step 17940, loss = 0.97 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:02.606063: step 17950, loss = 0.85 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:03.373258: step 17960, loss = 0.94 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:04.132323: step 17970, loss = 0.65 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:04.900671: step 17980, loss = 0.93 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:05.667725: step 17990, loss = 0.97 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:06.437446: step 18000, loss = 0.81 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:07.204314: step 18010, loss = 1.03 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:07.968427: step 18020, loss = 0.81 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:08.731536: step 18030, loss = 0.94 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:09.498423: step 18040, loss = 0.90 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:10.261216: step 18050, loss = 0.81 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:11.028160: step 18060, loss = 0.80 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:11.781477: step 18070, loss = 1.02 (1699.2 examples/sec; 0.075 sec/batch)
2017-05-05 17:49:12.548981: step 18080, loss = 1.00 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:13.310102: step 18090, loss = 0.75 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:14.073804: step 18100, loss = 1.04 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:14.837551: step 18110, loss = 0.91 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:15.593300: step 18120, loss = 0.89 (1693.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:16.360064: step 18130, loss = 0.93 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:17.127695: step 18140, loss = 0.92 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:17.893762: step 18150, loss = 0.83 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:18.659307: step 18160, loss = 0.79 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:19.420399: step 18170, loss = 1.09 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:20.185198: step 18180, loss = 0.89 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:20.955647: step 18190, loss = 0.86 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:21.719061: step 18200, loss = 0.76 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:22.485345: step 18210, loss = 0.84 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:23.254124: step 18220, loss = 0.91 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:24.011148: step 18230, loss = 0.83 (1690.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:24.777964: step 18240, loss = 0.95 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:25.547207: step 18250, loss = 0.88 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:26.310769: step 18260, loss = 0.88 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:27.073897: step 18270, loss = 0.95 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:27.830674: step 18280, loss = 0.99 (1691.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:28.599855: step 18290, loss = 0.87 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:29.371240: step 18300, loss = 0.94 (1659.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:30.130666: step 18310, loss = 1.05 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:30.895537: step 18320, loss = 0.92 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:31.649411: step 18330, loss = 0.90 (1697.9 examples/sec; 0.075 sec/batch)
2017-05-05 17:49:32.419544: step 18340, loss = 1.03 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:33.184865: step 18350, loss = 1.01 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:33.944728: step 18360, loss = 0.79 (1684.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:34.712207: step 18370, loss = 0.95 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:35.477598: step 18380, loss = 0.69 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:36.241959: step 18390, loss = 0.83 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:36.999297: step 18400, loss = 0.97 (1690.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:37.770955: step 18410, loss = 0.91 (1658.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:38.534972: step 18420, loss = 0.84 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:39.303673: step 18430, loss = 0.99 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:40.063107: step 18440, loss = 0.70 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:40.831545: step 18450, loss = 0.89 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:41.587885: step 18460, loss = 0.79 (1692.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:42.352943: step 18470, loss = 0.95 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:43.118723: step 18480, loss = 0.99 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:43.881101: step 18490, loss = 0.88 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:44.643432: step 18500, loss = 0.89 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:45.407843: step 18510, loss = 0.80 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:46.170935: step 18520, loss = 0.90 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:46.923292: step 18530, loss = 1.03 (1701.3 examples/sec; 0.075 sec/batch)
2017-05-05 17:49:47.696099: step 18540, loss = 0.83 (1656.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:48.464705: step 18550, loss = 0.78 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:49.224967: step 18560, loss = 0.76 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:49.990034: step 18570, loss = 0.83 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:50.753506: step 18580, loss = 0.93 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:51.511341: step 18590, loss = 1.07 (1689.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:52.272849: step 18600, loss = 1.10 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:53.036124: step 18610, loss = 0.89 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:53.798972: step 18620, loss = 0.84 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:54.563574: step 18630, loss = 1.22 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:55.325909: step 18640, loss = 0.75 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:56.097611: step 18650, loss = 0.97 (1658.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:56.856233: step 18660, loss = 0.88 (1687.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:49:57.622303: step 18670, loss = 0.89 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:58.388724: step 18680, loss = 0.87 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:59.154422: step 18690, loss = 0.87 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:49:59.909052: step 18700, loss = 0.79 (1696.2 examples/sec; 0.075 sec/batch)
2017-05-05 17:50:00.673378: step 18710, loss = 0.97 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:01.439094: step 18720, loss = 0.93 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:02.213852: step 18730, loss = 0.81 (1652.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:02.979630: step 18740, loss = 0.74 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:03.737795: step 18750, loss = 0.99 (1688.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:04.506733: step 18760, loss = 0.74 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:05.272286: step 18770, loss = 0.97 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:06.031685: step 18780, loss = 0.75 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:06.789952: step 18790, loss = 0.74 (1688.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:07.549420: step 18800, loss = 0.97 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:08.426535: step 18810, loss = 0.78 (1459.3 examples/sec; 0.088 sec/batch)
2017-05-05 17:50:09.094723: step 18820, loss = 0.81 (1915.6 examples/sec; 0.067 sec/batch)
2017-05-05 17:50:09.858819: step 18830, loss = 0.87 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:10.628996: step 18840, loss = 0.87 (1662.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:11.387015: step 18850, loss = 0.81 (1688.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:12.148941: step 18860, loss = 0.98 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:12.919800: step 18870, loss = 0.97 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:13.681988: step 18880, loss = 0.75 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:14.448674: step 18890, loss = 1.00 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:15.213613: step 18900, loss = 0.79 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:15.975553: step 18910, loss = 0.79 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:16.735944: step 18920, loss = 0.86 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:17.508991: step 18930, loss = 0.94 (1655.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:18.268734: step 18940, loss = 0.94 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:19.028154: step 18950, loss = 0.83 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:19.789210: step 18960, loss = 0.82 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:20.556871: step 18970, loss = 0.64 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:21.318332: step 18980, loss = 0.91 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:22.080663: step 18990, loss = 1.14 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:22.849695: step 19000, loss = 0.73 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:23.610749: step 19010, loss = 1.03 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:24.379592: step 19020, loss = 0.87 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:25.141986: step 19030, loss = 0.97 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:25.908397: step 19040, loss = 0.92 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:26.674724: step 19050, loss = 0.97 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:27.435773: step 19060, loss = 0.86 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:28.199425: step 19070, loss = 0.97 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:28.965888: step 19080, loss = 0.89 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:29.726652: step 19090, loss = 0.88 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:30.493623: step 19100, loss = 1.01 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:31.259225: step 19110, loss = 0.76 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:32.013229: step 19120, loss = 0.72 (1697.6 examples/sec; 0.075 sec/batch)
2017-05-05 17:50:32.778513: step 19130, loss = 0.87 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:33.546545: step 19140, loss = 0.94 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:34.313458: step 19150, loss = 0.92 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:35.080833: step 19160, loss = 0.81 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:35.838011: step 19170, loss = 0.93 (1690.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:36.603768: step 19180, loss = 0.78 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:37.365688: step 19190, loss = 0.80 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:38.132757: step 19200, loss = 1.06 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:38.903111: step 19210, loss = 0.82 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:39.656658: step 19220, loss = 0.87 (1698.6 examples/sec; 0.075 sec/batch)
2017-05-05 17:50:40.419130: step 19230, loss = 0.98 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:41.180295: step 19240, loss = 0.88 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:41.940174: step 19250, loss = 0.99 (1684.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:42.709496: step 19260, loss = 0.95 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:43.467254: step 19270, loss = 1.09 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:44.230771: step 19280, loss = 0.98 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:44.994984: step 19290, loss = 1.05 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:45.758486: step 19300, loss = 0.79 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:46.518407: step 19310, loss = 0.87 (1684.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:47.285400: step 19320, loss = 0.90 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:48.045512: step 19330, loss = 0.94 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:48.811958: step 19340, loss = 0.93 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:49.584443: step 19350, loss = 1.04 (1657.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:50.359108: step 19360, loss = 0.86 (1652.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:51.134042: step 19370, loss = 0.73 (1651.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:51.890867: step 19380, loss = 0.90 (1691.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:52.658380: step 19390, loss = 0.89 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:53.427963: step 19400, loss = 0.86 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:54.197162: step 19410, loss = 0.85 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:54.959546: step 19420, loss = 0.83 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:55.717563: step 19430, loss = 0.91 (1688.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:56.484135: step 19440, loss = 0.67 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:57.250843: step 19450, loss = 1.05 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:58.017937: step 19460, loss = 1.05 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:50:58.782572: step 19470, loss = 0.85 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:50:59.537445: step 19480, loss = 0.63 (1695.7 examples/sec; 0.075 sec/batch)
2017-05-05 17:51:00.304493: step 19490, loss = 0.96 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:01.062599: step 19500, loss = 0.80 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:01.822187: step 19510, loss = 0.94 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:02.587630: step 19520, loss = 0.79 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:03.347568: step 19530, loss = 1.12 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:04.108940: step 19540, loss = 0.88 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:04.873312: step 19550, loss = 0.97 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:05.638476: step 19560, loss = 0.93 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:06.406043: step 19570, loss = 0.92 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:07.174079: step 19580, loss = 0.95 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:07.932460: step 19590, loss = 0.89 (1687.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:08.691490: step 19600, loss = 0.89 (1686.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:09.459703: step 19610, loss = 0.87 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:10.235952: step 19620, loss = 0.83 (1649.0 examples/sec; 0.078 sec/batch)
2017-05-05 17:51:10.991246: step 19630, loss = 0.93 (1694.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:11.750564: step 19640, loss = 0.82 (1685.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:12.514494: step 19650, loss = 0.86 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:13.285605: step 19660, loss = 0.91 (1659.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:14.055524: step 19670, loss = 1.11 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:14.825480: step 19680, loss = 0.93 (1662.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:15.585071: step 19690, loss = 0.97 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:16.352704: step 19700, loss = 0.75 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:17.121221: step 19710, loss = 1.06 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:17.884801: step 19720, loss = 0.95 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:18.652584: step 19730, loss = 0.97 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:19.417960: step 19740, loss = 0.80 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:20.183973: step 19750, loss = 0.96 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:20.949173: step 19760, loss = 1.06 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:21.713020: step 19770, loss = 0.84 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:22.481008: step 19780, loss = 0.92 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:23.243651: step 19790, loss = 0.84 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:24.102535: step 19800, loss = 0.89 (1490.3 examples/sec; 0.086 sec/batch)
2017-05-05 17:51:24.765396: step 19810, loss = 0.86 (1931.1 examples/sec; 0.066 sec/batch)
2017-05-05 17:51:25.535126: step 19820, loss = 1.03 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:26.301704: step 19830, loss = 0.96 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:27.060110: step 19840, loss = 0.88 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:27.816858: step 19850, loss = 0.84 (1691.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:28.584655: step 19860, loss = 0.75 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:29.353923: step 19870, loss = 1.02 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:30.112005: step 19880, loss = 0.86 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:30.879442: step 19890, loss = 0.90 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:31.637187: step 19900, loss = 0.82 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:32.401936: step 19910, loss = 1.12 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:33.171949: step 19920, loss = 0.81 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:33.939924: step 19930, loss = 0.83 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:34.717401: step 19940, loss = 1.02 (1646.3 examples/sec; 0.078 sec/batch)
2017-05-05 17:51:35.477445: step 19950, loss = 0.91 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:36.242456: step 19960, loss = 0.81 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:37.005197: step 19970, loss = 0.94 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:37.776686: step 19980, loss = 0.76 (1659.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:38.540283: step 19990, loss = 0.94 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:39.305277: step 20000, loss = 1.06 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:40.057873: step 20010, loss = 1.05 (1700.8 examples/sec; 0.075 sec/batch)
2017-05-05 17:51:40.821296: step 20020, loss = 0.89 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:41.584176: step 20030, loss = 0.84 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:42.360783: step 20040, loss = 0.93 (1648.2 examples/sec; 0.078 sec/batch)
2017-05-05 17:51:43.121271: step 20050, loss = 0.77 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:43.882652: step 20060, loss = 0.87 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:44.645080: step 20070, loss = 0.78 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:45.416587: step 20080, loss = 0.91 (1659.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:46.186033: step 20090, loss = 0.75 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:46.954207: step 20100, loss = 0.96 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:47.711211: step 20110, loss = 0.71 (1690.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:48.486914: step 20120, loss = 0.86 (1650.1 examples/sec; 0.078 sec/batch)
2017-05-05 17:51:49.248530: step 20130, loss = 1.01 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:50.011575: step 20140, loss = 1.03 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:50.774872: step 20150, loss = 0.70 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:51.528060: step 20160, loss = 0.84 (1699.4 examples/sec; 0.075 sec/batch)
2017-05-05 17:51:52.292777: step 20170, loss = 1.08 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:53.051267: step 20180, loss = 1.00 (1687.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:53.818464: step 20190, loss = 0.83 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:54.587891: step 20200, loss = 1.00 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:55.348978: step 20210, loss = 0.83 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:51:56.114449: step 20220, loss = 0.82 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:56.884322: step 20230, loss = 0.96 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:57.653638: step 20240, loss = 0.91 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:58.422885: step 20250, loss = 0.93 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:59.192324: step 20260, loss = 0.90 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:51:59.958090: step 20270, loss = 0.89 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:00.724213: step 20280, loss = 1.01 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:01.484097: step 20290, loss = 0.96 (1684.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:52:02.248160: step 20300, loss = 0.90 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:52:03.014655: step 20310, loss = 0.84 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:03.770338: step 20320, loss = 0.81 (1693.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:52:04.540335: step 20330, loss = 0.90 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:05.309660: step 20340, loss = 0.80 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:06.071418: step 20350, loss = 0.94 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:52:06.838938: step 20360, loss = 1.06 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:07.594393: step 20370, loss = 0.64 (1694.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:52:08.355814: step 20380, loss = 0.86 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:52:09.119133: step 20390, loss = 0.84 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:52:09.884917: step 20400, loss = 0.81 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:10.656615: step 20410, loss = 0.85 (1658.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:11.426927: step 20420, loss = 0.94 (1661.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:12.187009: step 20430, loss = 1.07 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:52:12.954783: step 20440, loss = 0.93 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:13.722659: step 20450, loss = 0.90 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:14.487244: step 20460, loss = 0.98 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:52:15.252068: step 20470, loss = 0.98 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:52:16.025351: step 20480, loss = 0.83 (1655.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:16.785130: step 20490, loss = 0.96 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:52:17.553912: step 20500, loss = 0.73 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:18.320739: step 20510, loss = 0.87 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:19.085834: step 20520, loss = 0.89 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:19.842464: step 20530, loss = 0.89 (1691.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:52:20.611286: step 20540, loss = 1.02 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:21.378275: step 20550, loss = 0.94 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:22.147752: step 20560, loss = 0.89 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:22.916864: step 20570, loss = 0.90 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:23.674538: step 20580, loss = 0.89 (1689.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:52:24.444204: step 20590, loss = 0.83 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:25.213497: step 20600, loss = 0.94 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:25.974255: step 20610, loss = 0.82 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:52:26.739886: step 20620, loss = 0.82 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:27.499543: step 20630, loss = 0.83 (1685.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:52:28.259972: step 20640, loss = 0.94 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:52:29.024284: step 20650, loss = 0.88 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:52:29.786464: step 20660, loss = 0.89 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:52:30.556170: step 20670, loss = 1.04 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:31.322116: step 20680, loss = 0.92 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:32.074937: step 20690, loss = 0.73 (1700.3 examples/sec; 0.075 sec/batch)
2017-05-05 17:52:32.838073: step 20700, loss = 0.91 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:52:33.597891: step 20710, loss = 0.98 (1684.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:52:34.365965: step 20720, loss = 0.93 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:35.134833: step 20730, loss = 0.91 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:35.884335: step 20740, loss = 0.95 (1707.8 examples/sec; 0.075 sec/batch)
2017-05-05 17:52:36.650018: step 20750, loss = 0.79 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:37.416036: step 20760, loss = 0.94 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:38.177280: step 20770, loss = 0.76 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:52:38.944184: step 20780, loss = 1.03 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:39.810898: step 20790, loss = 0.94 (1476.8 examples/sec; 0.087 sec/batch)
2017-05-05 17:52:40.473703: step 20800, loss = 0.80 (1931.2 examples/sec; 0.066 sec/batch)
2017-05-05 17:52:41.236204: step 20810, loss = 0.82 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:52:41.995510: step 20820, loss = 0.99 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:52:42.765578: step 20830, loss = 0.89 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:43.523689: step 20840, loss = 0.79 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:52:44.287898: step 20850, loss = 0.92 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:52:45.057528: step 20860, loss = 0.74 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:45.823394: step 20870, loss = 0.68 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:46.587025: step 20880, loss = 0.84 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:52:47.357612: step 20890, loss = 1.07 (1661.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:48.121319: step 20900, loss = 0.78 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:52:48.878038: step 20910, loss = 0.86 (1691.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:52:49.644415: step 20920, loss = 1.02 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:50.410994: step 20930, loss = 0.91 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:51.176945: step 20940, loss = 0.88 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:51.936017: step 20950, loss = 0.94 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:52:52.701951: step 20960, loss = 0.97 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:53.474708: step 20970, loss = 0.87 (1656.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:54.239935: step 20980, loss = 0.94 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:55.005686: step 20990, loss = 0.68 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:55.763134: step 21000, loss = 0.84 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:52:56.523634: step 21010, loss = 0.71 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:52:57.292068: step 21020, loss = 0.75 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:58.053548: step 21030, loss = 1.02 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:52:58.823810: step 21040, loss = 0.82 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:52:59.586565: step 21050, loss = 0.88 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:00.357316: step 21060, loss = 0.91 (1660.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:01.126267: step 21070, loss = 0.99 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:01.889136: step 21080, loss = 0.88 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:02.666677: step 21090, loss = 0.96 (1646.2 examples/sec; 0.078 sec/batch)
2017-05-05 17:53:03.430851: step 21100, loss = 0.73 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:04.191764: step 21110, loss = 0.96 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:04.953661: step 21120, loss = 0.84 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:05.717727: step 21130, loss = 0.84 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:06.481866: step 21140, loss = 0.79 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:07.248059: step 21150, loss = 0.79 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:08.005266: step 21160, loss = 0.95 (1690.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:08.771919: step 21170, loss = 0.99 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:09.541755: step 21180, loss = 0.84 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:10.316049: step 21190, loss = 0.93 (1653.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:11.083233: step 21200, loss = 0.90 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:11.841296: step 21210, loss = 0.99 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:12.607726: step 21220, loss = 0.78 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:13.375793: step 21230, loss = 0.88 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:14.140244: step 21240, loss = 0.76 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:14.902452: step 21250, loss = 0.79 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:15.663437: step 21260, loss = 0.93 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:16.424246: step 21270, loss = 0.89 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:17.188567: step 21280, loss = 0.84 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:17.961578: step 21290, loss = 0.85 (1655.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:18.733607: step 21300, loss = 0.84 (1658.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:19.489884: step 21310, loss = 0.92 (1692.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:20.250721: step 21320, loss = 0.94 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:21.016014: step 21330, loss = 1.10 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:21.768879: step 21340, loss = 0.98 (1700.2 examples/sec; 0.075 sec/batch)
2017-05-05 17:53:22.537735: step 21350, loss = 0.73 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:23.304462: step 21360, loss = 0.89 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:24.058414: step 21370, loss = 0.96 (1697.7 examples/sec; 0.075 sec/batch)
2017-05-05 17:53:24.819348: step 21380, loss = 0.93 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:25.591739: step 21390, loss = 0.90 (1657.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:26.356263: step 21400, loss = 0.84 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:27.122404: step 21410, loss = 0.88 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:27.879636: step 21420, loss = 0.83 (1690.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:28.646003: step 21430, loss = 0.87 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:29.411892: step 21440, loss = 1.11 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:30.179089: step 21450, loss = 0.86 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:30.939837: step 21460, loss = 0.80 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:31.699671: step 21470, loss = 0.89 (1684.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:32.465499: step 21480, loss = 0.90 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:33.227050: step 21490, loss = 0.97 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:33.988689: step 21500, loss = 0.83 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:34.757785: step 21510, loss = 0.87 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:35.517073: step 21520, loss = 0.92 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:36.281421: step 21530, loss = 0.78 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:37.046094: step 21540, loss = 0.73 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:37.808442: step 21550, loss = 0.90 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:38.583859: step 21560, loss = 0.70 (1650.7 examples/sec; 0.078 sec/batch)
2017-05-05 17:53:39.349519: step 21570, loss = 1.10 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:40.104579: step 21580, loss = 0.81 (1695.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:40.877712: step 21590, loss = 0.79 (1655.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:41.646251: step 21600, loss = 0.92 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:42.408547: step 21610, loss = 0.98 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:43.174354: step 21620, loss = 0.82 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:43.933427: step 21630, loss = 0.78 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:44.703450: step 21640, loss = 0.85 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:45.470488: step 21650, loss = 0.95 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:46.245946: step 21660, loss = 0.79 (1650.6 examples/sec; 0.078 sec/batch)
2017-05-05 17:53:47.011283: step 21670, loss = 1.03 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:47.775429: step 21680, loss = 0.79 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:48.553944: step 21690, loss = 0.82 (1644.2 examples/sec; 0.078 sec/batch)
2017-05-05 17:53:49.319537: step 21700, loss = 0.88 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:50.088553: step 21710, loss = 0.90 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:50.856806: step 21720, loss = 0.92 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:51.615781: step 21730, loss = 0.71 (1686.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:52.377468: step 21740, loss = 0.83 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:53.143106: step 21750, loss = 0.97 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:53.911944: step 21760, loss = 1.00 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:54.683085: step 21770, loss = 0.80 (1659.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:55.550631: step 21780, loss = 0.88 (1475.4 examples/sec; 0.087 sec/batch)
2017-05-05 17:53:56.208783: step 21790, loss = 0.88 (1944.8 examples/sec; 0.066 sec/batch)
2017-05-05 17:53:56.972320: step 21800, loss = 0.83 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:53:57.739952: step 21810, loss = 0.83 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:58.513162: step 21820, loss = 0.75 (1655.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:53:59.276050: step 21830, loss = 0.89 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:00.036073: step 21840, loss = 0.87 (1684.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:00.799533: step 21850, loss = 0.80 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:01.563757: step 21860, loss = 0.89 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:02.331269: step 21870, loss = 0.90 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:03.097222: step 21880, loss = 0.76 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:03.857086: step 21890, loss = 1.12 (1684.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:04.623569: step 21900, loss = 0.77 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:05.393801: step 21910, loss = 0.80 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:06.160112: step 21920, loss = 1.02 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:06.926786: step 21930, loss = 0.83 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:07.691994: step 21940, loss = 0.78 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:08.452965: step 21950, loss = 0.77 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:09.217686: step 21960, loss = 0.79 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:09.982388: step 21970, loss = 0.94 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:10.754660: step 21980, loss = 0.93 (1657.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:11.507385: step 21990, loss = 0.70 (1700.5 examples/sec; 0.075 sec/batch)
2017-05-05 17:54:12.269861: step 22000, loss = 0.79 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:13.036246: step 22010, loss = 0.89 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:13.803399: step 22020, loss = 0.79 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:14.573191: step 22030, loss = 0.93 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:15.343590: step 22040, loss = 0.98 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:16.101578: step 22050, loss = 0.95 (1688.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:16.861324: step 22060, loss = 0.86 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:17.626599: step 22070, loss = 0.87 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:18.392922: step 22080, loss = 0.87 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:19.158165: step 22090, loss = 0.87 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:19.909568: step 22100, loss = 1.11 (1703.5 examples/sec; 0.075 sec/batch)
2017-05-05 17:54:20.675219: step 22110, loss = 0.94 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:21.442877: step 22120, loss = 0.76 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:22.206809: step 22130, loss = 0.96 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:22.969855: step 22140, loss = 0.89 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:23.733439: step 22150, loss = 0.87 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:24.499162: step 22160, loss = 0.79 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:25.267812: step 22170, loss = 0.79 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:26.048120: step 22180, loss = 1.02 (1640.4 examples/sec; 0.078 sec/batch)
2017-05-05 17:54:26.813184: step 22190, loss = 0.81 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:27.573556: step 22200, loss = 0.94 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:28.336011: step 22210, loss = 0.98 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:29.100302: step 22220, loss = 0.84 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:29.863454: step 22230, loss = 0.81 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:30.633878: step 22240, loss = 0.73 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:31.391904: step 22250, loss = 0.86 (1688.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:32.147567: step 22260, loss = 0.97 (1693.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:32.913602: step 22270, loss = 1.04 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:33.684855: step 22280, loss = 0.83 (1659.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:34.453481: step 22290, loss = 1.11 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:35.223608: step 22300, loss = 0.92 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:35.976589: step 22310, loss = 0.90 (1699.9 examples/sec; 0.075 sec/batch)
2017-05-05 17:54:36.746825: step 22320, loss = 0.79 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:37.507830: step 22330, loss = 0.87 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:38.275880: step 22340, loss = 0.82 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:39.043267: step 22350, loss = 0.93 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:39.794551: step 22360, loss = 0.84 (1703.7 examples/sec; 0.075 sec/batch)
2017-05-05 17:54:40.560225: step 22370, loss = 1.02 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:41.332340: step 22380, loss = 0.79 (1657.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:42.097707: step 22390, loss = 0.81 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:42.858482: step 22400, loss = 0.78 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:43.613146: step 22410, loss = 0.90 (1696.1 examples/sec; 0.075 sec/batch)
2017-05-05 17:54:44.379710: step 22420, loss = 0.78 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:45.148070: step 22430, loss = 0.83 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:45.907927: step 22440, loss = 0.93 (1684.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:46.667545: step 22450, loss = 0.88 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:47.422962: step 22460, loss = 0.74 (1694.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:48.177993: step 22470, loss = 0.99 (1695.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:48.942745: step 22480, loss = 1.00 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:49.708452: step 22490, loss = 0.83 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:50.476981: step 22500, loss = 0.70 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:51.245900: step 22510, loss = 0.88 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:51.999539: step 22520, loss = 0.94 (1698.5 examples/sec; 0.075 sec/batch)
2017-05-05 17:54:52.762212: step 22530, loss = 0.74 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:53.527190: step 22540, loss = 0.79 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:54.294924: step 22550, loss = 0.93 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:54:55.057306: step 22560, loss = 1.14 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:55.808170: step 22570, loss = 0.93 (1704.7 examples/sec; 0.075 sec/batch)
2017-05-05 17:54:56.584757: step 22580, loss = 1.00 (1648.2 examples/sec; 0.078 sec/batch)
2017-05-05 17:54:57.346742: step 22590, loss = 0.92 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:58.108862: step 22600, loss = 0.95 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:58.873580: step 22610, loss = 1.01 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:54:59.636834: step 22620, loss = 1.00 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:00.401217: step 22630, loss = 0.98 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:01.167180: step 22640, loss = 0.83 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:01.931438: step 22650, loss = 0.80 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:02.715276: step 22660, loss = 1.14 (1633.0 examples/sec; 0.078 sec/batch)
2017-05-05 17:55:03.482968: step 22670, loss = 0.74 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:04.244985: step 22680, loss = 0.90 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:05.020785: step 22690, loss = 0.87 (1649.9 examples/sec; 0.078 sec/batch)
2017-05-05 17:55:05.792773: step 22700, loss = 0.87 (1658.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:06.548609: step 22710, loss = 0.84 (1693.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:07.318143: step 22720, loss = 0.82 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:08.077713: step 22730, loss = 0.76 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:08.845387: step 22740, loss = 1.08 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:09.613080: step 22750, loss = 0.98 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:10.381752: step 22760, loss = 0.80 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:11.242627: step 22770, loss = 0.78 (1486.9 examples/sec; 0.086 sec/batch)
2017-05-05 17:55:11.904465: step 22780, loss = 0.84 (1934.0 examples/sec; 0.066 sec/batch)
2017-05-05 17:55:12.675764: step 22790, loss = 0.84 (1659.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:13.449016: step 22800, loss = 0.85 (1655.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:14.207051: step 22810, loss = 0.77 (1688.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:14.967111: step 22820, loss = 0.90 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:15.722820: step 22830, loss = 0.94 (1693.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:16.485276: step 22840, loss = 0.99 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:17.253367: step 22850, loss = 0.97 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:18.020650: step 22860, loss = 0.87 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:18.784539: step 22870, loss = 0.99 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:19.545066: step 22880, loss = 0.83 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:20.303281: step 22890, loss = 0.83 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:21.069081: step 22900, loss = 0.81 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:21.835428: step 22910, loss = 0.75 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:22.600889: step 22920, loss = 0.96 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:23.362030: step 22930, loss = 0.88 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:24.125267: step 22940, loss = 0.89 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:24.892532: step 22950, loss = 0.81 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:25.651334: step 22960, loss = 0.93 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:26.417078: step 22970, loss = 1.04 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:27.187013: step 22980, loss = 0.78 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:27.944308: step 22990, loss = 0.86 (1690.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:28.704755: step 23000, loss = 0.88 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:29.469637: step 23010, loss = 0.67 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:30.237517: step 23020, loss = 0.87 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:31.007582: step 23030, loss = 0.97 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:31.759853: step 23040, loss = 0.89 (1701.5 examples/sec; 0.075 sec/batch)
2017-05-05 17:55:32.534918: step 23050, loss = 0.83 (1651.5 examples/sec; 0.078 sec/batch)
2017-05-05 17:55:33.305919: step 23060, loss = 0.89 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:34.074021: step 23070, loss = 0.81 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:34.843269: step 23080, loss = 0.97 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:35.599780: step 23090, loss = 0.71 (1692.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:36.367192: step 23100, loss = 0.99 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:37.129580: step 23110, loss = 1.07 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:37.893731: step 23120, loss = 0.92 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:38.659537: step 23130, loss = 0.92 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:39.423657: step 23140, loss = 0.71 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:40.183282: step 23150, loss = 0.87 (1685.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:40.948149: step 23160, loss = 0.77 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:41.715030: step 23170, loss = 0.92 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:42.476603: step 23180, loss = 0.82 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:43.244022: step 23190, loss = 0.99 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:43.999812: step 23200, loss = 0.79 (1693.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:44.761544: step 23210, loss = 1.15 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:45.532761: step 23220, loss = 0.97 (1659.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:46.300192: step 23230, loss = 0.92 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:47.063769: step 23240, loss = 0.94 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:47.821296: step 23250, loss = 0.90 (1689.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:48.590491: step 23260, loss = 0.90 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:49.357818: step 23270, loss = 1.01 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:50.124958: step 23280, loss = 0.89 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:50.888720: step 23290, loss = 0.82 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:51.648251: step 23300, loss = 0.97 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:52.415492: step 23310, loss = 0.77 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:53.186646: step 23320, loss = 0.77 (1659.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:53.953830: step 23330, loss = 0.80 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:54.716593: step 23340, loss = 0.97 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:55.472156: step 23350, loss = 0.75 (1694.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:56.237707: step 23360, loss = 0.83 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:57.003409: step 23370, loss = 0.80 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:57.768148: step 23380, loss = 0.88 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:55:58.534505: step 23390, loss = 0.77 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:55:59.298876: step 23400, loss = 0.70 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:00.053547: step 23410, loss = 0.75 (1696.1 examples/sec; 0.075 sec/batch)
2017-05-05 17:56:00.818667: step 23420, loss = 0.69 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:01.581165: step 23430, loss = 0.75 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:02.345052: step 23440, loss = 0.78 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:03.104441: step 23450, loss = 0.87 (1685.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:03.858960: step 23460, loss = 1.02 (1696.4 examples/sec; 0.075 sec/batch)
2017-05-05 17:56:04.615652: step 23470, loss = 0.92 (1691.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:05.427261: step 23480, loss = 0.96 (1577.1 examples/sec; 0.081 sec/batch)
2017-05-05 17:56:06.188327: step 23490, loss = 0.77 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:06.949620: step 23500, loss = 1.02 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:07.706437: step 23510, loss = 0.82 (1691.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:08.472556: step 23520, loss = 0.86 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:09.238636: step 23530, loss = 0.66 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:10.007275: step 23540, loss = 0.86 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:10.770813: step 23550, loss = 0.96 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:11.531481: step 23560, loss = 0.76 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:12.296740: step 23570, loss = 0.82 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:13.058962: step 23580, loss = 0.88 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:13.825177: step 23590, loss = 0.89 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:14.598589: step 23600, loss = 0.79 (1655.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:15.361949: step 23610, loss = 0.83 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:16.121020: step 23620, loss = 0.83 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:16.880622: step 23630, loss = 0.86 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:17.650497: step 23640, loss = 0.97 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:18.412463: step 23650, loss = 0.90 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:19.186371: step 23660, loss = 0.89 (1653.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:19.939129: step 23670, loss = 0.88 (1700.4 examples/sec; 0.075 sec/batch)
2017-05-05 17:56:20.705801: step 23680, loss = 0.80 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:21.463736: step 23690, loss = 0.77 (1688.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:22.229398: step 23700, loss = 0.89 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:22.989975: step 23710, loss = 0.76 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:23.743477: step 23720, loss = 0.72 (1698.7 examples/sec; 0.075 sec/batch)
2017-05-05 17:56:24.507644: step 23730, loss = 0.94 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:25.274068: step 23740, loss = 0.87 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:26.038996: step 23750, loss = 0.89 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:26.924643: step 23760, loss = 0.83 (1445.3 examples/sec; 0.089 sec/batch)
2017-05-05 17:56:27.562755: step 23770, loss = 0.86 (2005.9 examples/sec; 0.064 sec/batch)
2017-05-05 17:56:28.331331: step 23780, loss = 0.85 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:29.097963: step 23790, loss = 0.74 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:29.864599: step 23800, loss = 1.08 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:30.631002: step 23810, loss = 0.91 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:31.389527: step 23820, loss = 0.96 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:32.148678: step 23830, loss = 0.91 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:32.914167: step 23840, loss = 0.80 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:33.680431: step 23850, loss = 0.78 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:34.446933: step 23860, loss = 0.77 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:35.212210: step 23870, loss = 0.78 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:35.968885: step 23880, loss = 0.94 (1691.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:36.736015: step 23890, loss = 0.94 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:37.501764: step 23900, loss = 0.86 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:38.267404: step 23910, loss = 1.04 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:39.037281: step 23920, loss = 0.81 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:39.801684: step 23930, loss = 1.03 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:40.568782: step 23940, loss = 0.79 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:41.335218: step 23950, loss = 0.69 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:42.103085: step 23960, loss = 0.90 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:42.872041: step 23970, loss = 0.83 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:43.628932: step 23980, loss = 0.76 (1691.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:44.394928: step 23990, loss = 0.81 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:45.156711: step 24000, loss = 0.91 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:45.924541: step 24010, loss = 0.71 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:46.686653: step 24020, loss = 1.01 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:47.450747: step 24030, loss = 0.87 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:48.205919: step 24040, loss = 0.99 (1695.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:48.966674: step 24050, loss = 0.74 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:49.736692: step 24060, loss = 0.87 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:50.502204: step 24070, loss = 0.97 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:51.268499: step 24080, loss = 0.74 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:52.018562: step 24090, loss = 0.83 (1706.5 examples/sec; 0.075 sec/batch)
2017-05-05 17:56:52.782751: step 24100, loss = 0.75 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:53.545849: step 24110, loss = 0.80 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:54.309718: step 24120, loss = 0.84 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:55.078640: step 24130, loss = 0.96 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:55.835768: step 24140, loss = 0.73 (1690.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:56.599018: step 24150, loss = 0.92 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:56:57.366480: step 24160, loss = 0.70 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:58.145336: step 24170, loss = 0.78 (1643.4 examples/sec; 0.078 sec/batch)
2017-05-05 17:56:58.913393: step 24180, loss = 0.85 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:56:59.667308: step 24190, loss = 0.91 (1697.8 examples/sec; 0.075 sec/batch)
2017-05-05 17:57:00.435286: step 24200, loss = 1.02 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:57:01.196390: step 24210, loss = 0.81 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:01.953614: step 24220, loss = 0.76 (1690.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:02.724550: step 24230, loss = 0.85 (1660.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:57:03.489147: step 24240, loss = 0.85 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:04.250881: step 24250, loss = 0.87 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:05.023706: step 24260, loss = 0.90 (1656.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:57:05.791405: step 24270, loss = 0.87 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:57:06.550508: step 24280, loss = 0.80 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:07.315225: step 24290, loss = 0.80 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:08.067247: step 24300, loss = 0.87 (1702.1 examples/sec; 0.075 sec/batch)
2017-05-05 17:57:08.831891: step 24310, loss = 1.12 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:09.602246: step 24320, loss = 0.87 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:57:10.373244: step 24330, loss = 0.89 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:57:11.133835: step 24340, loss = 0.95 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:11.898485: step 24350, loss = 0.82 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:12.665794: step 24360, loss = 0.86 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:57:13.431652: step 24370, loss = 0.90 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:57:14.201847: step 24380, loss = 0.99 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:57:14.961036: step 24390, loss = 0.83 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:15.721818: step 24400, loss = 0.86 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:16.489928: step 24410, loss = 0.79 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:57:17.250632: step 24420, loss = 0.93 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:18.021508: step 24430, loss = 0.83 (1660.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:57:18.791635: step 24440, loss = 0.98 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:57:19.551329: step 24450, loss = 0.79 (1684.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:20.321727: step 24460, loss = 0.71 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:57:21.090748: step 24470, loss = 1.00 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:57:21.847799: step 24480, loss = 0.88 (1690.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:22.614010: step 24490, loss = 0.84 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:57:23.382377: step 24500, loss = 0.83 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:57:24.141192: step 24510, loss = 1.09 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:24.906514: step 24520, loss = 0.79 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:57:25.680407: step 24530, loss = 0.80 (1654.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:57:26.444505: step 24540, loss = 0.82 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:27.208922: step 24550, loss = 0.93 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:27.973276: step 24560, loss = 1.04 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:28.739161: step 24570, loss = 0.79 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:57:29.498763: step 24580, loss = 0.86 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:30.261381: step 24590, loss = 0.99 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:31.032753: step 24600, loss = 0.82 (1659.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:57:31.791768: step 24610, loss = 0.76 (1686.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:32.555863: step 24620, loss = 0.99 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:33.323213: step 24630, loss = 0.92 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:57:34.094022: step 24640, loss = 0.80 (1660.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:57:34.858878: step 24650, loss = 0.92 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:35.618020: step 24660, loss = 0.83 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:36.381536: step 24670, loss = 0.91 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:37.143622: step 24680, loss = 0.79 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:37.907435: step 24690, loss = 0.84 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:38.678252: step 24700, loss = 0.82 (1660.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:57:39.434977: step 24710, loss = 0.93 (1691.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:40.200877: step 24720, loss = 0.89 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:57:40.959646: step 24730, loss = 0.67 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:41.722142: step 24740, loss = 0.84 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:42.589565: step 24750, loss = 0.77 (1475.6 examples/sec; 0.087 sec/batch)
2017-05-05 17:57:43.253555: step 24760, loss = 0.84 (1927.7 examples/sec; 0.066 sec/batch)
2017-05-05 17:57:44.014436: step 24770, loss = 0.86 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:44.781454: step 24780, loss = 0.82 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:57:45.541776: step 24790, loss = 0.87 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:46.306558: step 24800, loss = 0.81 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:47.071525: step 24810, loss = 0.99 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:47.827137: step 24820, loss = 0.99 (1694.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:48.597099: step 24830, loss = 0.92 (1662.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:57:49.364353: step 24840, loss = 0.82 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:57:50.127834: step 24850, loss = 1.05 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:50.892786: step 24860, loss = 0.91 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:51.649113: step 24870, loss = 0.80 (1692.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:52.411544: step 24880, loss = 0.73 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:53.179144: step 24890, loss = 0.96 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:57:53.943483: step 24900, loss = 0.94 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:54.710591: step 24910, loss = 1.04 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:57:55.474884: step 24920, loss = 0.85 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:56.241482: step 24930, loss = 0.90 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:57:57.004450: step 24940, loss = 0.83 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:57:57.772530: step 24950, loss = 0.71 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:57:58.537576: step 24960, loss = 0.84 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:57:59.303927: step 24970, loss = 0.82 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:58:00.058167: step 24980, loss = 0.80 (1697.1 examples/sec; 0.075 sec/batch)
2017-05-05 17:58:00.825475: step 24990, loss = 0.95 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:58:01.621716: step 25000, loss = 0.97 (1607.6 examples/sec; 0.080 sec/batch)
2017-05-05 17:58:02.384921: step 25010, loss = 0.75 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:03.156001: step 25020, loss = 0.97 (1660.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:58:03.908223: step 25030, loss = 0.78 (1701.6 examples/sec; 0.075 sec/batch)
2017-05-05 17:58:04.669594: step 25040, loss = 0.78 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:05.437270: step 25050, loss = 0.88 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:58:06.202442: step 25060, loss = 0.69 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:58:06.970154: step 25070, loss = 0.96 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:58:07.726349: step 25080, loss = 0.91 (1692.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:08.490258: step 25090, loss = 0.69 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:09.253874: step 25100, loss = 0.80 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:10.015635: step 25110, loss = 0.76 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:10.775686: step 25120, loss = 1.04 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:11.538758: step 25130, loss = 0.81 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:12.304350: step 25140, loss = 0.81 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:58:13.071346: step 25150, loss = 0.92 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:58:13.838826: step 25160, loss = 0.82 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:58:14.602749: step 25170, loss = 0.83 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:15.362214: step 25180, loss = 0.78 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:16.120437: step 25190, loss = 0.72 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:16.885495: step 25200, loss = 0.92 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:58:17.648356: step 25210, loss = 0.91 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:18.411041: step 25220, loss = 0.90 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:19.176617: step 25230, loss = 0.86 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:58:19.928912: step 25240, loss = 0.93 (1701.5 examples/sec; 0.075 sec/batch)
2017-05-05 17:58:20.693838: step 25250, loss = 0.76 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:21.454511: step 25260, loss = 0.85 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:22.225456: step 25270, loss = 0.84 (1660.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:58:22.989852: step 25280, loss = 1.03 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:23.746265: step 25290, loss = 0.86 (1692.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:24.514839: step 25300, loss = 0.81 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:58:25.277689: step 25310, loss = 0.92 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:26.033654: step 25320, loss = 0.82 (1693.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:26.798884: step 25330, loss = 0.76 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:58:27.554468: step 25340, loss = 0.90 (1694.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:28.321858: step 25350, loss = 0.92 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:58:29.086093: step 25360, loss = 0.82 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:29.850896: step 25370, loss = 0.86 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:30.611482: step 25380, loss = 0.96 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:31.368280: step 25390, loss = 0.81 (1691.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:32.136975: step 25400, loss = 1.04 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:58:32.904273: step 25410, loss = 0.81 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:58:33.671198: step 25420, loss = 0.89 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:58:34.434738: step 25430, loss = 0.85 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:35.199226: step 25440, loss = 1.05 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:35.950448: step 25450, loss = 0.79 (1703.9 examples/sec; 0.075 sec/batch)
2017-05-05 17:58:36.713585: step 25460, loss = 0.81 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:37.481724: step 25470, loss = 0.85 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:58:38.249688: step 25480, loss = 0.83 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:58:39.018569: step 25490, loss = 0.85 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:58:39.774480: step 25500, loss = 1.03 (1693.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:40.544772: step 25510, loss = 0.94 (1661.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:58:41.306461: step 25520, loss = 0.95 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:42.073836: step 25530, loss = 0.84 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:58:42.848951: step 25540, loss = 0.78 (1651.4 examples/sec; 0.078 sec/batch)
2017-05-05 17:58:43.610081: step 25550, loss = 0.89 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:44.378099: step 25560, loss = 0.90 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:58:45.141874: step 25570, loss = 0.84 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:45.906652: step 25580, loss = 0.84 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:46.670569: step 25590, loss = 0.93 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:47.432323: step 25600, loss = 0.69 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:48.188787: step 25610, loss = 0.89 (1692.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:48.957174: step 25620, loss = 0.90 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:58:49.721115: step 25630, loss = 0.83 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:50.484797: step 25640, loss = 1.10 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:51.248731: step 25650, loss = 0.85 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:52.003657: step 25660, loss = 0.86 (1695.5 examples/sec; 0.075 sec/batch)
2017-05-05 17:58:52.772958: step 25670, loss = 0.83 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:58:53.539025: step 25680, loss = 0.82 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:58:54.306221: step 25690, loss = 0.83 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:58:55.072272: step 25700, loss = 0.82 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:58:55.831120: step 25710, loss = 0.69 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:56.597329: step 25720, loss = 1.04 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:58:57.361212: step 25730, loss = 0.90 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:58:58.229044: step 25740, loss = 0.82 (1474.9 examples/sec; 0.087 sec/batch)
2017-05-05 17:58:58.892514: step 25750, loss = 0.88 (1929.2 examples/sec; 0.066 sec/batch)
2017-05-05 17:58:59.650568: step 25760, loss = 0.79 (1688.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:00.415063: step 25770, loss = 0.73 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:01.175944: step 25780, loss = 0.84 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:01.941417: step 25790, loss = 0.75 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 17:59:02.710189: step 25800, loss = 0.69 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:59:03.468509: step 25810, loss = 0.87 (1687.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:04.229228: step 25820, loss = 0.94 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:04.997208: step 25830, loss = 0.86 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:59:05.760287: step 25840, loss = 0.84 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:06.525441: step 25850, loss = 0.81 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:59:07.290235: step 25860, loss = 1.00 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:08.052586: step 25870, loss = 0.75 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:08.816680: step 25880, loss = 0.83 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:09.582950: step 25890, loss = 0.84 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:59:10.348100: step 25900, loss = 0.83 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:59:11.112410: step 25910, loss = 0.92 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:11.869698: step 25920, loss = 0.77 (1690.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:12.636652: step 25930, loss = 0.74 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:59:13.414713: step 25940, loss = 0.97 (1645.1 examples/sec; 0.078 sec/batch)
2017-05-05 17:59:14.175408: step 25950, loss = 0.92 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:14.943296: step 25960, loss = 0.93 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:59:15.706737: step 25970, loss = 0.72 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:16.470517: step 25980, loss = 0.94 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:17.240308: step 25990, loss = 0.85 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:59:18.003931: step 26000, loss = 0.98 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:18.770984: step 26010, loss = 1.22 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:59:19.529253: step 26020, loss = 0.80 (1688.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:20.286549: step 26030, loss = 0.84 (1690.2 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:21.043003: step 26040, loss = 0.90 (1692.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:21.799788: step 26050, loss = 0.78 (1691.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:22.567574: step 26060, loss = 0.84 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 17:59:23.335557: step 26070, loss = 1.03 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:59:24.094518: step 26080, loss = 0.90 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:24.858107: step 26090, loss = 0.86 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:25.618846: step 26100, loss = 0.76 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:26.381781: step 26110, loss = 0.71 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:27.143815: step 26120, loss = 0.86 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:27.897890: step 26130, loss = 0.86 (1697.4 examples/sec; 0.075 sec/batch)
2017-05-05 17:59:28.667283: step 26140, loss = 1.05 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 17:59:29.432256: step 26150, loss = 0.88 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:30.201004: step 26160, loss = 0.70 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:59:30.961091: step 26170, loss = 0.72 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:31.716967: step 26180, loss = 1.08 (1693.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:32.481790: step 26190, loss = 1.01 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:33.250810: step 26200, loss = 1.05 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 17:59:34.017470: step 26210, loss = 0.77 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:59:34.786679: step 26220, loss = 0.79 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:59:35.541416: step 26230, loss = 1.02 (1696.0 examples/sec; 0.075 sec/batch)
2017-05-05 17:59:36.299653: step 26240, loss = 0.94 (1688.1 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:37.066471: step 26250, loss = 1.05 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:59:37.843749: step 26260, loss = 0.78 (1646.8 examples/sec; 0.078 sec/batch)
2017-05-05 17:59:38.609619: step 26270, loss = 0.87 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:59:39.371818: step 26280, loss = 0.78 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:40.131481: step 26290, loss = 0.85 (1685.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:40.888022: step 26300, loss = 0.78 (1691.9 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:41.657710: step 26310, loss = 0.76 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 17:59:42.419490: step 26320, loss = 0.93 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:43.185536: step 26330, loss = 0.94 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:59:43.938618: step 26340, loss = 0.84 (1699.7 examples/sec; 0.075 sec/batch)
2017-05-05 17:59:44.710460: step 26350, loss = 0.90 (1658.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:59:45.471543: step 26360, loss = 0.79 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:46.230170: step 26370, loss = 0.69 (1687.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:46.997401: step 26380, loss = 0.95 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:59:47.757155: step 26390, loss = 0.84 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:48.529435: step 26400, loss = 0.74 (1657.4 examples/sec; 0.077 sec/batch)
2017-05-05 17:59:49.290895: step 26410, loss = 0.79 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:50.058614: step 26420, loss = 0.90 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:59:50.813228: step 26430, loss = 0.75 (1696.2 examples/sec; 0.075 sec/batch)
2017-05-05 17:59:51.569378: step 26440, loss = 0.91 (1692.8 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:52.335560: step 26450, loss = 0.87 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:59:53.096437: step 26460, loss = 0.78 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:53.863667: step 26470, loss = 0.90 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 17:59:54.631591: step 26480, loss = 0.83 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:59:55.389225: step 26490, loss = 0.95 (1689.5 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:56.149964: step 26500, loss = 0.88 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:56.913424: step 26510, loss = 1.03 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 17:59:57.684218: step 26520, loss = 0.98 (1660.6 examples/sec; 0.077 sec/batch)
2017-05-05 17:59:58.453550: step 26530, loss = 0.96 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 17:59:59.226048: step 26540, loss = 0.91 (1656.9 examples/sec; 0.077 sec/batch)
2017-05-05 17:59:59.983852: step 26550, loss = 0.92 (1689.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:00.735856: step 26560, loss = 0.79 (1702.1 examples/sec; 0.075 sec/batch)
2017-05-05 18:00:01.498786: step 26570, loss = 0.70 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:02.280447: step 26580, loss = 0.90 (1637.5 examples/sec; 0.078 sec/batch)
2017-05-05 18:00:03.043138: step 26590, loss = 0.90 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:03.800306: step 26600, loss = 0.90 (1690.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:04.569719: step 26610, loss = 0.78 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:00:05.331519: step 26620, loss = 0.78 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:06.093759: step 26630, loss = 0.83 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:06.857730: step 26640, loss = 0.91 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:07.617834: step 26650, loss = 0.77 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:08.384089: step 26660, loss = 0.81 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:00:09.153949: step 26670, loss = 0.76 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:00:09.920460: step 26680, loss = 0.91 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:00:10.684903: step 26690, loss = 0.83 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:11.445839: step 26700, loss = 0.91 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:12.204055: step 26710, loss = 0.83 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:12.970256: step 26720, loss = 0.79 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:00:13.837553: step 26730, loss = 0.89 (1475.8 examples/sec; 0.087 sec/batch)
2017-05-05 18:00:14.499490: step 26740, loss = 0.77 (1933.7 examples/sec; 0.066 sec/batch)
2017-05-05 18:00:15.263406: step 26750, loss = 0.87 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:16.009062: step 26760, loss = 0.83 (1716.6 examples/sec; 0.075 sec/batch)
2017-05-05 18:00:16.777353: step 26770, loss = 0.86 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:00:17.538995: step 26780, loss = 0.84 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:18.312179: step 26790, loss = 0.77 (1655.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:00:19.078645: step 26800, loss = 0.78 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:00:19.828769: step 26810, loss = 0.84 (1706.4 examples/sec; 0.075 sec/batch)
2017-05-05 18:00:20.592763: step 26820, loss = 0.97 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:21.360985: step 26830, loss = 0.80 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:00:22.127609: step 26840, loss = 0.86 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:00:22.897857: step 26850, loss = 0.64 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:00:23.658178: step 26860, loss = 0.75 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:24.420921: step 26870, loss = 0.95 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:25.181002: step 26880, loss = 0.98 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:25.937801: step 26890, loss = 0.79 (1691.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:26.706118: step 26900, loss = 0.88 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:00:27.462928: step 26910, loss = 0.85 (1691.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:28.226350: step 26920, loss = 0.85 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:28.984153: step 26930, loss = 0.75 (1689.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:29.751741: step 26940, loss = 0.79 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:00:30.513066: step 26950, loss = 0.89 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:31.274488: step 26960, loss = 0.77 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:32.028115: step 26970, loss = 0.82 (1698.5 examples/sec; 0.075 sec/batch)
2017-05-05 18:00:32.793130: step 26980, loss = 0.64 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:00:33.558912: step 26990, loss = 0.83 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:00:34.330125: step 27000, loss = 0.99 (1659.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:00:35.095634: step 27010, loss = 0.86 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:00:35.848481: step 27020, loss = 0.69 (1700.2 examples/sec; 0.075 sec/batch)
2017-05-05 18:00:36.609128: step 27030, loss = 0.82 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:37.378781: step 27040, loss = 0.90 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:00:38.143297: step 27050, loss = 0.89 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:38.904132: step 27060, loss = 0.96 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:39.664330: step 27070, loss = 0.90 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:40.425263: step 27080, loss = 0.89 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:41.182753: step 27090, loss = 0.83 (1689.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:41.949617: step 27100, loss = 0.84 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:00:42.710822: step 27110, loss = 0.90 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:43.467478: step 27120, loss = 0.82 (1691.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:44.233040: step 27130, loss = 0.94 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:00:45.000802: step 27140, loss = 0.86 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:00:45.762955: step 27150, loss = 0.79 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:46.523619: step 27160, loss = 0.79 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:47.288330: step 27170, loss = 0.77 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:48.045144: step 27180, loss = 0.83 (1691.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:48.812533: step 27190, loss = 0.90 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:00:49.576802: step 27200, loss = 0.67 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:50.340468: step 27210, loss = 0.83 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:51.102435: step 27220, loss = 0.96 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:51.854670: step 27230, loss = 0.92 (1701.6 examples/sec; 0.075 sec/batch)
2017-05-05 18:00:52.629147: step 27240, loss = 1.00 (1652.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:00:53.396319: step 27250, loss = 0.82 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:00:54.174055: step 27260, loss = 0.78 (1645.8 examples/sec; 0.078 sec/batch)
2017-05-05 18:00:54.938194: step 27270, loss = 0.79 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:00:55.692571: step 27280, loss = 0.89 (1696.8 examples/sec; 0.075 sec/batch)
2017-05-05 18:00:56.458198: step 27290, loss = 1.01 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:00:57.225697: step 27300, loss = 0.88 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:00:57.992699: step 27310, loss = 0.94 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:00:58.764755: step 27320, loss = 1.00 (1657.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:00:59.522591: step 27330, loss = 0.81 (1689.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:00.289481: step 27340, loss = 0.91 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:01.059909: step 27350, loss = 0.86 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:01.818618: step 27360, loss = 0.84 (1687.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:02.594891: step 27370, loss = 0.75 (1648.9 examples/sec; 0.078 sec/batch)
2017-05-05 18:01:03.360599: step 27380, loss = 0.97 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:04.120206: step 27390, loss = 0.77 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:04.888574: step 27400, loss = 0.75 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:05.656414: step 27410, loss = 0.87 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:06.417069: step 27420, loss = 0.75 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:07.183302: step 27430, loss = 1.02 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:07.946614: step 27440, loss = 1.20 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:08.713575: step 27450, loss = 0.75 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:09.478798: step 27460, loss = 0.95 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:10.244119: step 27470, loss = 0.81 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:11.005283: step 27480, loss = 0.81 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:11.766457: step 27490, loss = 0.79 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:12.537812: step 27500, loss = 0.72 (1659.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:13.300416: step 27510, loss = 0.93 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:14.069986: step 27520, loss = 0.91 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:14.836167: step 27530, loss = 0.77 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:15.591963: step 27540, loss = 0.71 (1693.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:16.363193: step 27550, loss = 0.96 (1659.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:17.125347: step 27560, loss = 0.87 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:17.890071: step 27570, loss = 0.75 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:18.651861: step 27580, loss = 1.01 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:19.416482: step 27590, loss = 0.92 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:20.177547: step 27600, loss = 0.77 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:20.932851: step 27610, loss = 0.84 (1694.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:21.699550: step 27620, loss = 0.71 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:22.470639: step 27630, loss = 0.69 (1660.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:23.232194: step 27640, loss = 0.92 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:23.989725: step 27650, loss = 0.93 (1689.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:24.752955: step 27660, loss = 0.73 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:25.516902: step 27670, loss = 0.96 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:26.284172: step 27680, loss = 1.06 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:27.049207: step 27690, loss = 0.77 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:27.811223: step 27700, loss = 0.91 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:28.578465: step 27710, loss = 0.96 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:29.435776: step 27720, loss = 0.77 (1493.0 examples/sec; 0.086 sec/batch)
2017-05-05 18:01:30.101743: step 27730, loss = 0.85 (1922.0 examples/sec; 0.067 sec/batch)
2017-05-05 18:01:30.859392: step 27740, loss = 0.89 (1689.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:31.618643: step 27750, loss = 0.79 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:32.381093: step 27760, loss = 0.86 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:33.143588: step 27770, loss = 0.78 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:33.909443: step 27780, loss = 0.78 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:34.679208: step 27790, loss = 0.91 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:35.443135: step 27800, loss = 0.70 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:36.201545: step 27810, loss = 0.85 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:36.972069: step 27820, loss = 0.81 (1661.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:37.741529: step 27830, loss = 0.77 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:38.512260: step 27840, loss = 0.78 (1660.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:39.278678: step 27850, loss = 0.70 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:40.037027: step 27860, loss = 0.86 (1687.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:40.794252: step 27870, loss = 0.98 (1690.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:41.563254: step 27880, loss = 0.81 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:42.328511: step 27890, loss = 0.63 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:43.094920: step 27900, loss = 0.84 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:43.848460: step 27910, loss = 0.92 (1698.7 examples/sec; 0.075 sec/batch)
2017-05-05 18:01:44.623768: step 27920, loss = 1.00 (1651.0 examples/sec; 0.078 sec/batch)
2017-05-05 18:01:45.388471: step 27930, loss = 0.68 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:46.145440: step 27940, loss = 0.80 (1691.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:46.914664: step 27950, loss = 0.82 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:47.673134: step 27960, loss = 0.82 (1687.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:48.436636: step 27970, loss = 0.82 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:49.203638: step 27980, loss = 0.79 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:49.967253: step 27990, loss = 0.85 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:50.730612: step 28000, loss = 0.71 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:51.489513: step 28010, loss = 0.79 (1686.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:52.255668: step 28020, loss = 0.92 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:53.019726: step 28030, loss = 0.66 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:53.783956: step 28040, loss = 1.04 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:54.547646: step 28050, loss = 0.84 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:55.311999: step 28060, loss = 0.85 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:56.067388: step 28070, loss = 0.71 (1694.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:56.830998: step 28080, loss = 0.88 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:01:57.601244: step 28090, loss = 0.66 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:58.366434: step 28100, loss = 0.98 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:59.132464: step 28110, loss = 0.88 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:01:59.898306: step 28120, loss = 0.89 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:00.670617: step 28130, loss = 0.79 (1657.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:01.430800: step 28140, loss = 0.88 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:02.195233: step 28150, loss = 0.84 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:02.960474: step 28160, loss = 0.81 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:03.711899: step 28170, loss = 0.87 (1703.4 examples/sec; 0.075 sec/batch)
2017-05-05 18:02:04.482770: step 28180, loss = 0.74 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:05.249283: step 28190, loss = 0.87 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:06.010722: step 28200, loss = 0.75 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:06.772505: step 28210, loss = 0.79 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:07.531772: step 28220, loss = 0.91 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:08.295255: step 28230, loss = 0.84 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:09.066882: step 28240, loss = 0.86 (1658.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:09.838262: step 28250, loss = 0.67 (1659.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:10.595676: step 28260, loss = 0.84 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:11.355733: step 28270, loss = 0.85 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:12.116862: step 28280, loss = 0.77 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:12.882383: step 28290, loss = 0.85 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:13.646906: step 28300, loss = 0.76 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:14.411516: step 28310, loss = 0.81 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:15.178715: step 28320, loss = 0.84 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:15.936481: step 28330, loss = 0.75 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:16.705755: step 28340, loss = 0.90 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:17.473546: step 28350, loss = 0.77 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:18.244054: step 28360, loss = 0.83 (1661.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:19.013715: step 28370, loss = 0.73 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:19.770846: step 28380, loss = 0.94 (1690.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:20.536725: step 28390, loss = 0.96 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:21.304244: step 28400, loss = 0.80 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:22.065289: step 28410, loss = 0.97 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:22.836710: step 28420, loss = 0.86 (1659.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:23.595694: step 28430, loss = 0.79 (1686.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:24.362948: step 28440, loss = 0.88 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:25.125588: step 28450, loss = 0.78 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:25.887768: step 28460, loss = 0.85 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:26.650707: step 28470, loss = 0.88 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:27.426951: step 28480, loss = 1.00 (1649.0 examples/sec; 0.078 sec/batch)
2017-05-05 18:02:28.188135: step 28490, loss = 0.91 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:28.957107: step 28500, loss = 0.79 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:29.728981: step 28510, loss = 0.78 (1658.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:30.490784: step 28520, loss = 0.98 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:31.253326: step 28530, loss = 0.79 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:32.008692: step 28540, loss = 0.87 (1694.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:32.768895: step 28550, loss = 0.90 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:33.537373: step 28560, loss = 0.78 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:34.311466: step 28570, loss = 0.97 (1653.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:35.071003: step 28580, loss = 0.76 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:35.825313: step 28590, loss = 0.72 (1696.9 examples/sec; 0.075 sec/batch)
2017-05-05 18:02:36.588387: step 28600, loss = 0.87 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:37.355203: step 28610, loss = 0.73 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:38.124834: step 28620, loss = 0.85 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:38.893507: step 28630, loss = 0.86 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:39.647314: step 28640, loss = 0.79 (1698.0 examples/sec; 0.075 sec/batch)
2017-05-05 18:02:40.409836: step 28650, loss = 0.99 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:41.171218: step 28660, loss = 0.94 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:41.930306: step 28670, loss = 0.75 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:42.696230: step 28680, loss = 0.80 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:43.457884: step 28690, loss = 0.78 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:44.219550: step 28700, loss = 0.89 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:45.077783: step 28710, loss = 0.90 (1491.4 examples/sec; 0.086 sec/batch)
2017-05-05 18:02:45.742280: step 28720, loss = 0.87 (1926.3 examples/sec; 0.066 sec/batch)
2017-05-05 18:02:46.511905: step 28730, loss = 0.91 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:47.273310: step 28740, loss = 0.85 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:48.036842: step 28750, loss = 0.97 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:48.798614: step 28760, loss = 0.80 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:49.566790: step 28770, loss = 0.86 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:50.340256: step 28780, loss = 0.78 (1654.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:51.103744: step 28790, loss = 0.71 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:51.858073: step 28800, loss = 0.99 (1696.9 examples/sec; 0.075 sec/batch)
2017-05-05 18:02:52.624293: step 28810, loss = 0.83 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:53.394496: step 28820, loss = 0.83 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:54.153677: step 28830, loss = 0.79 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:54.919001: step 28840, loss = 0.75 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:55.673132: step 28850, loss = 0.75 (1697.3 examples/sec; 0.075 sec/batch)
2017-05-05 18:02:56.441470: step 28860, loss = 0.87 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:57.204238: step 28870, loss = 0.84 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:02:57.975537: step 28880, loss = 0.93 (1659.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:58.744109: step 28890, loss = 0.86 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:02:59.499991: step 28900, loss = 0.84 (1693.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:00.268860: step 28910, loss = 0.83 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:03:01.028634: step 28920, loss = 0.77 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:01.795972: step 28930, loss = 0.75 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:03:02.571453: step 28940, loss = 0.97 (1650.6 examples/sec; 0.078 sec/batch)
2017-05-05 18:03:03.336263: step 28950, loss = 0.97 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:04.096382: step 28960, loss = 0.86 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:04.858752: step 28970, loss = 0.98 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:05.627026: step 28980, loss = 1.10 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:03:06.402641: step 28990, loss = 0.88 (1650.3 examples/sec; 0.078 sec/batch)
2017-05-05 18:03:07.166818: step 29000, loss = 0.92 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:07.924160: step 29010, loss = 0.77 (1690.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:08.687745: step 29020, loss = 0.72 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:09.458317: step 29030, loss = 0.97 (1661.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:03:10.223798: step 29040, loss = 0.76 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:03:10.985634: step 29050, loss = 0.80 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:11.740809: step 29060, loss = 0.79 (1695.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:12.508155: step 29070, loss = 0.67 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:03:13.273203: step 29080, loss = 0.90 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:03:14.039877: step 29090, loss = 0.79 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:03:14.802051: step 29100, loss = 0.74 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:15.572044: step 29110, loss = 0.91 (1662.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:03:16.334916: step 29120, loss = 0.86 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:17.093375: step 29130, loss = 0.71 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:17.865122: step 29140, loss = 0.91 (1658.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:03:18.629096: step 29150, loss = 0.91 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:19.395965: step 29160, loss = 0.79 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:03:20.156257: step 29170, loss = 0.92 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:20.917721: step 29180, loss = 0.81 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:21.679544: step 29190, loss = 0.88 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:22.446549: step 29200, loss = 0.87 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:03:23.210321: step 29210, loss = 0.84 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:23.969490: step 29220, loss = 0.78 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:24.734671: step 29230, loss = 0.91 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:03:25.497891: step 29240, loss = 0.83 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:26.267464: step 29250, loss = 0.94 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:03:27.030591: step 29260, loss = 0.70 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:27.791052: step 29270, loss = 0.70 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:28.556261: step 29280, loss = 0.76 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:03:29.317841: step 29290, loss = 0.71 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:30.085913: step 29300, loss = 0.87 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:03:30.848401: step 29310, loss = 0.93 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:31.606468: step 29320, loss = 0.80 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:32.372756: step 29330, loss = 1.13 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:03:33.139497: step 29340, loss = 0.84 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:03:33.904343: step 29350, loss = 0.88 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:34.668025: step 29360, loss = 0.86 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:35.428854: step 29370, loss = 0.66 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:36.189865: step 29380, loss = 0.88 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:36.955174: step 29390, loss = 1.07 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:03:37.716818: step 29400, loss = 0.70 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:38.486475: step 29410, loss = 0.83 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:03:39.257501: step 29420, loss = 0.89 (1660.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:03:40.015011: step 29430, loss = 0.76 (1689.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:40.775286: step 29440, loss = 0.93 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:41.542954: step 29450, loss = 0.86 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:03:42.313973: step 29460, loss = 0.79 (1660.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:03:43.075994: step 29470, loss = 0.80 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:43.835810: step 29480, loss = 0.83 (1684.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:44.606732: step 29490, loss = 0.85 (1660.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:03:45.377406: step 29500, loss = 0.88 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:03:46.144781: step 29510, loss = 0.80 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:03:46.912380: step 29520, loss = 0.94 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:03:47.676798: step 29530, loss = 0.69 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:48.439055: step 29540, loss = 0.84 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:49.208021: step 29550, loss = 0.84 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:03:49.985571: step 29560, loss = 0.78 (1646.2 examples/sec; 0.078 sec/batch)
2017-05-05 18:03:50.745119: step 29570, loss = 0.92 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:51.502069: step 29580, loss = 0.74 (1691.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:52.264256: step 29590, loss = 0.86 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:53.025255: step 29600, loss = 1.05 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:53.794665: step 29610, loss = 0.96 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:03:54.558212: step 29620, loss = 0.94 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:55.319671: step 29630, loss = 0.87 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:56.075491: step 29640, loss = 0.95 (1693.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:56.840040: step 29650, loss = 0.75 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:57.599846: step 29660, loss = 0.81 (1684.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:58.363951: step 29670, loss = 0.89 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:59.124758: step 29680, loss = 0.87 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:03:59.874980: step 29690, loss = 0.80 (1706.2 examples/sec; 0.075 sec/batch)
2017-05-05 18:04:00.736838: step 29700, loss = 0.92 (1485.2 examples/sec; 0.086 sec/batch)
2017-05-05 18:04:01.407796: step 29710, loss = 1.02 (1907.7 examples/sec; 0.067 sec/batch)
2017-05-05 18:04:02.172323: step 29720, loss = 0.83 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:02.940837: step 29730, loss = 0.95 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:04:03.690963: step 29740, loss = 0.92 (1706.4 examples/sec; 0.075 sec/batch)
2017-05-05 18:04:04.457049: step 29750, loss = 0.81 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:04:05.219741: step 29760, loss = 0.78 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:05.974625: step 29770, loss = 0.88 (1695.6 examples/sec; 0.075 sec/batch)
2017-05-05 18:04:06.734873: step 29780, loss = 0.87 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:07.492071: step 29790, loss = 0.77 (1690.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:08.260033: step 29800, loss = 0.65 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:04:09.019090: step 29810, loss = 0.85 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:09.784962: step 29820, loss = 0.94 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:04:10.550567: step 29830, loss = 0.78 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:04:11.312961: step 29840, loss = 0.86 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:12.069398: step 29850, loss = 0.82 (1692.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:12.836627: step 29860, loss = 0.85 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:04:13.599995: step 29870, loss = 0.77 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:14.369716: step 29880, loss = 0.77 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:04:15.133759: step 29890, loss = 0.90 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:15.896241: step 29900, loss = 0.98 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:16.669568: step 29910, loss = 0.77 (1655.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:04:17.432036: step 29920, loss = 0.96 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:18.196969: step 29930, loss = 0.89 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:18.963088: step 29940, loss = 0.95 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:04:19.721425: step 29950, loss = 0.90 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:20.480476: step 29960, loss = 0.94 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:21.245622: step 29970, loss = 0.85 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:04:22.008187: step 29980, loss = 0.95 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:22.774176: step 29990, loss = 1.08 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:04:23.539101: step 30000, loss = 1.00 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:24.301792: step 30010, loss = 0.89 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:25.065159: step 30020, loss = 0.89 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:25.823071: step 30030, loss = 0.78 (1688.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:26.588901: step 30040, loss = 0.87 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:04:27.352905: step 30050, loss = 0.88 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:28.116245: step 30060, loss = 0.91 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:28.882664: step 30070, loss = 0.88 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:04:29.653503: step 30080, loss = 0.88 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:04:30.418954: step 30090, loss = 0.75 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:04:31.182214: step 30100, loss = 0.78 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:31.932416: step 30110, loss = 0.64 (1706.2 examples/sec; 0.075 sec/batch)
2017-05-05 18:04:32.696837: step 30120, loss = 0.89 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:33.465076: step 30130, loss = 0.70 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:04:34.225659: step 30140, loss = 0.89 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:34.987524: step 30150, loss = 0.99 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:35.745029: step 30160, loss = 0.75 (1689.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:36.507026: step 30170, loss = 1.11 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:37.272946: step 30180, loss = 0.87 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:04:38.034542: step 30190, loss = 0.84 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:38.802539: step 30200, loss = 0.69 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:04:39.566606: step 30210, loss = 0.83 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:40.330549: step 30220, loss = 0.68 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:41.088703: step 30230, loss = 0.81 (1688.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:41.852899: step 30240, loss = 0.76 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:42.615643: step 30250, loss = 0.76 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:43.382269: step 30260, loss = 0.99 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:04:44.138721: step 30270, loss = 0.73 (1692.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:44.901233: step 30280, loss = 0.92 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:45.669940: step 30290, loss = 0.90 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:04:46.436840: step 30300, loss = 0.96 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:04:47.207880: step 30310, loss = 0.73 (1660.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:04:47.962391: step 30320, loss = 0.84 (1696.5 examples/sec; 0.075 sec/batch)
2017-05-05 18:04:48.735350: step 30330, loss = 1.01 (1655.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:04:49.499811: step 30340, loss = 1.01 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:50.266951: step 30350, loss = 1.00 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:04:51.030324: step 30360, loss = 0.87 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:51.783351: step 30370, loss = 0.72 (1699.8 examples/sec; 0.075 sec/batch)
2017-05-05 18:04:52.551207: step 30380, loss = 0.68 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:04:53.314999: step 30390, loss = 0.81 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:54.083155: step 30400, loss = 0.95 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:04:54.852979: step 30410, loss = 0.70 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:04:55.608964: step 30420, loss = 0.82 (1693.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:56.367468: step 30430, loss = 0.92 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:57.130096: step 30440, loss = 0.86 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:04:57.895618: step 30450, loss = 0.86 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:04:58.660721: step 30460, loss = 0.98 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:04:59.425282: step 30470, loss = 0.86 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:05:00.190658: step 30480, loss = 0.99 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:00.944398: step 30490, loss = 0.79 (1698.2 examples/sec; 0.075 sec/batch)
2017-05-05 18:05:01.709691: step 30500, loss = 0.86 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:02.478332: step 30510, loss = 0.85 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:03.243826: step 30520, loss = 0.84 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:04.000786: step 30530, loss = 0.94 (1691.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:05:04.770414: step 30540, loss = 0.96 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:05.532834: step 30550, loss = 0.81 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:05:06.296681: step 30560, loss = 0.91 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:05:07.061758: step 30570, loss = 0.89 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:07.816560: step 30580, loss = 0.81 (1695.8 examples/sec; 0.075 sec/batch)
2017-05-05 18:05:08.586406: step 30590, loss = 0.88 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:09.355058: step 30600, loss = 0.76 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:10.134567: step 30610, loss = 0.78 (1642.1 examples/sec; 0.078 sec/batch)
2017-05-05 18:05:10.902322: step 30620, loss = 0.95 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:11.660038: step 30630, loss = 0.85 (1689.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:05:12.431624: step 30640, loss = 0.86 (1658.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:13.201997: step 30650, loss = 0.75 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:13.969782: step 30660, loss = 0.78 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:14.736775: step 30670, loss = 0.84 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:15.490775: step 30680, loss = 0.89 (1697.6 examples/sec; 0.075 sec/batch)
2017-05-05 18:05:16.349572: step 30690, loss = 1.00 (1490.5 examples/sec; 0.086 sec/batch)
2017-05-05 18:05:17.018735: step 30700, loss = 0.91 (1912.8 examples/sec; 0.067 sec/batch)
2017-05-05 18:05:17.788696: step 30710, loss = 0.95 (1662.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:18.558242: step 30720, loss = 0.82 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:19.321090: step 30730, loss = 0.91 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:05:20.081762: step 30740, loss = 0.92 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:05:20.842384: step 30750, loss = 0.83 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:05:21.600653: step 30760, loss = 0.90 (1688.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:05:22.363327: step 30770, loss = 0.80 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:05:23.128730: step 30780, loss = 0.85 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:23.886271: step 30790, loss = 0.70 (1689.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:05:24.656036: step 30800, loss = 0.87 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:25.423734: step 30810, loss = 0.77 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:26.184957: step 30820, loss = 0.89 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:05:26.949766: step 30830, loss = 0.90 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:05:27.703523: step 30840, loss = 0.76 (1698.1 examples/sec; 0.075 sec/batch)
2017-05-05 18:05:28.472897: step 30850, loss = 0.86 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:29.244217: step 30860, loss = 0.79 (1659.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:30.010121: step 30870, loss = 0.82 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:30.775040: step 30880, loss = 0.73 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:05:31.537488: step 30890, loss = 0.86 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:05:32.298831: step 30900, loss = 0.86 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:05:33.059673: step 30910, loss = 0.94 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:05:33.826755: step 30920, loss = 0.87 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:34.597883: step 30930, loss = 0.89 (1659.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:35.365296: step 30940, loss = 0.84 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:36.117336: step 30950, loss = 0.89 (1702.0 examples/sec; 0.075 sec/batch)
2017-05-05 18:05:36.884159: step 30960, loss = 0.63 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:37.653930: step 30970, loss = 1.07 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:38.419874: step 30980, loss = 0.90 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:39.182517: step 30990, loss = 0.87 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:05:39.941509: step 31000, loss = 0.82 (1686.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:05:40.699870: step 31010, loss = 0.84 (1687.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:05:41.465191: step 31020, loss = 0.73 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:42.228291: step 31030, loss = 0.90 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:05:42.993502: step 31040, loss = 0.89 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:43.754891: step 31050, loss = 0.93 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:05:44.515699: step 31060, loss = 0.93 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:05:45.276960: step 31070, loss = 0.74 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:05:46.034461: step 31080, loss = 0.72 (1689.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:05:46.809722: step 31090, loss = 1.02 (1651.1 examples/sec; 0.078 sec/batch)
2017-05-05 18:05:47.580945: step 31100, loss = 0.84 (1659.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:48.343252: step 31110, loss = 0.94 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:05:49.111925: step 31120, loss = 0.93 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:49.876703: step 31130, loss = 0.65 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:05:50.634505: step 31140, loss = 0.91 (1689.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:05:51.391328: step 31150, loss = 0.90 (1691.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:05:52.150344: step 31160, loss = 0.84 (1686.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:05:52.915603: step 31170, loss = 1.04 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:53.687093: step 31180, loss = 0.75 (1659.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:54.451590: step 31190, loss = 0.87 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:05:55.219895: step 31200, loss = 0.88 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:55.969731: step 31210, loss = 0.77 (1707.0 examples/sec; 0.075 sec/batch)
2017-05-05 18:05:56.746772: step 31220, loss = 0.80 (1647.3 examples/sec; 0.078 sec/batch)
2017-05-05 18:05:57.512723: step 31230, loss = 0.71 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:58.282942: step 31240, loss = 0.93 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:05:59.045013: step 31250, loss = 0.71 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:05:59.801120: step 31260, loss = 0.73 (1692.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:00.562103: step 31270, loss = 0.90 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:01.326012: step 31280, loss = 0.85 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:02.091229: step 31290, loss = 0.80 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:02.861017: step 31300, loss = 0.93 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:03.620380: step 31310, loss = 0.62 (1685.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:04.388106: step 31320, loss = 0.91 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:05.141027: step 31330, loss = 0.73 (1700.0 examples/sec; 0.075 sec/batch)
2017-05-05 18:06:05.936183: step 31340, loss = 0.85 (1609.8 examples/sec; 0.080 sec/batch)
2017-05-05 18:06:06.697869: step 31350, loss = 0.95 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:07.460016: step 31360, loss = 0.77 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:08.221752: step 31370, loss = 0.87 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:08.986243: step 31380, loss = 0.77 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:09.757116: step 31390, loss = 0.94 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:10.523796: step 31400, loss = 0.79 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:11.288914: step 31410, loss = 0.81 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:12.060169: step 31420, loss = 0.92 (1659.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:12.820927: step 31430, loss = 0.85 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:13.583165: step 31440, loss = 0.75 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:14.351701: step 31450, loss = 0.72 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:15.120341: step 31460, loss = 0.88 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:15.880044: step 31470, loss = 0.81 (1684.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:16.645633: step 31480, loss = 0.87 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:17.408098: step 31490, loss = 0.83 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:18.179476: step 31500, loss = 0.85 (1659.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:18.945390: step 31510, loss = 0.98 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:19.695504: step 31520, loss = 0.70 (1706.3 examples/sec; 0.075 sec/batch)
2017-05-05 18:06:20.468059: step 31530, loss = 0.80 (1656.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:21.236730: step 31540, loss = 0.90 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:22.001217: step 31550, loss = 0.82 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:22.765572: step 31560, loss = 0.82 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:23.527649: step 31570, loss = 0.83 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:24.290383: step 31580, loss = 0.92 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:25.055134: step 31590, loss = 0.74 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:25.823391: step 31600, loss = 0.86 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:26.590335: step 31610, loss = 1.15 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:27.360487: step 31620, loss = 0.88 (1662.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:28.122449: step 31630, loss = 0.64 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:28.886858: step 31640, loss = 0.79 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:29.656306: step 31650, loss = 0.73 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:30.422271: step 31660, loss = 0.88 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:31.187578: step 31670, loss = 0.80 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:32.062469: step 31680, loss = 0.81 (1463.0 examples/sec; 0.087 sec/batch)
2017-05-05 18:06:32.712647: step 31690, loss = 0.84 (1968.7 examples/sec; 0.065 sec/batch)
2017-05-05 18:06:33.480000: step 31700, loss = 0.89 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:34.245472: step 31710, loss = 0.85 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:35.015948: step 31720, loss = 0.92 (1661.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:35.778158: step 31730, loss = 0.80 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:36.545938: step 31740, loss = 0.73 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:37.321442: step 31750, loss = 1.08 (1650.5 examples/sec; 0.078 sec/batch)
2017-05-05 18:06:38.099553: step 31760, loss = 0.73 (1645.0 examples/sec; 0.078 sec/batch)
2017-05-05 18:06:38.868599: step 31770, loss = 1.00 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:39.628110: step 31780, loss = 0.92 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:40.398729: step 31790, loss = 0.84 (1661.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:41.162441: step 31800, loss = 0.86 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:41.926275: step 31810, loss = 0.75 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:42.693875: step 31820, loss = 0.76 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:43.450497: step 31830, loss = 0.80 (1691.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:44.212814: step 31840, loss = 0.87 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:44.981778: step 31850, loss = 1.01 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:45.741632: step 31860, loss = 0.83 (1684.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:46.503583: step 31870, loss = 0.74 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:47.271216: step 31880, loss = 0.93 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:48.026422: step 31890, loss = 0.74 (1694.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:48.793091: step 31900, loss = 0.81 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:49.560830: step 31910, loss = 0.90 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:50.327076: step 31920, loss = 0.90 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:51.085339: step 31930, loss = 0.95 (1688.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:51.841721: step 31940, loss = 0.80 (1692.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:52.610350: step 31950, loss = 0.64 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:53.367010: step 31960, loss = 0.85 (1691.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:54.129420: step 31970, loss = 0.84 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:54.902338: step 31980, loss = 0.72 (1656.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:55.655857: step 31990, loss = 0.93 (1698.7 examples/sec; 0.075 sec/batch)
2017-05-05 18:06:56.416775: step 32000, loss = 0.86 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:57.187246: step 32010, loss = 0.98 (1661.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:06:57.950839: step 32020, loss = 0.79 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:58.712977: step 32030, loss = 0.95 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:06:59.470460: step 32040, loss = 0.82 (1689.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:00.237318: step 32050, loss = 0.72 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:00.994972: step 32060, loss = 0.91 (1689.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:01.763498: step 32070, loss = 1.00 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:02.534309: step 32080, loss = 0.95 (1660.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:03.295329: step 32090, loss = 0.88 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:04.048487: step 32100, loss = 1.07 (1699.5 examples/sec; 0.075 sec/batch)
2017-05-05 18:07:04.814740: step 32110, loss = 0.71 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:05.581194: step 32120, loss = 0.73 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:06.337177: step 32130, loss = 0.88 (1693.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:07.107569: step 32140, loss = 0.71 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:07.857612: step 32150, loss = 0.81 (1706.6 examples/sec; 0.075 sec/batch)
2017-05-05 18:07:08.624790: step 32160, loss = 0.85 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:09.399210: step 32170, loss = 0.82 (1652.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:10.162645: step 32180, loss = 1.07 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:10.919813: step 32190, loss = 0.85 (1690.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:11.675318: step 32200, loss = 0.87 (1694.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:12.440688: step 32210, loss = 0.90 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:13.202500: step 32220, loss = 0.83 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:13.972576: step 32230, loss = 0.83 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:14.728138: step 32240, loss = 0.81 (1694.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:15.488643: step 32250, loss = 0.83 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:16.246776: step 32260, loss = 0.90 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:17.011454: step 32270, loss = 0.89 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:17.773376: step 32280, loss = 0.79 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:18.537917: step 32290, loss = 0.98 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:19.304133: step 32300, loss = 0.88 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:20.058642: step 32310, loss = 1.08 (1696.5 examples/sec; 0.075 sec/batch)
2017-05-05 18:07:20.820477: step 32320, loss = 0.88 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:21.586749: step 32330, loss = 0.76 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:22.353830: step 32340, loss = 1.07 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:23.115096: step 32350, loss = 0.89 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:23.871720: step 32360, loss = 0.81 (1691.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:24.636515: step 32370, loss = 0.79 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:25.400311: step 32380, loss = 0.79 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:26.158606: step 32390, loss = 0.78 (1688.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:26.929477: step 32400, loss = 0.76 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:27.687352: step 32410, loss = 0.87 (1688.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:28.457681: step 32420, loss = 0.80 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:29.218945: step 32430, loss = 0.78 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:29.984529: step 32440, loss = 0.90 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:30.747738: step 32450, loss = 0.88 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:31.505004: step 32460, loss = 0.76 (1690.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:32.268040: step 32470, loss = 0.88 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:33.033530: step 32480, loss = 0.84 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:33.794188: step 32490, loss = 0.73 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:34.557990: step 32500, loss = 0.94 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:35.316483: step 32510, loss = 0.66 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:36.070489: step 32520, loss = 0.65 (1697.6 examples/sec; 0.075 sec/batch)
2017-05-05 18:07:36.833013: step 32530, loss = 0.69 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:37.600199: step 32540, loss = 0.92 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:38.367236: step 32550, loss = 1.03 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:39.138638: step 32560, loss = 0.94 (1659.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:39.902428: step 32570, loss = 0.73 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:40.665852: step 32580, loss = 0.76 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:41.436504: step 32590, loss = 0.87 (1661.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:42.192130: step 32600, loss = 0.91 (1694.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:42.956470: step 32610, loss = 0.75 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:43.710419: step 32620, loss = 0.84 (1697.7 examples/sec; 0.075 sec/batch)
2017-05-05 18:07:44.476071: step 32630, loss = 0.91 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:45.242274: step 32640, loss = 0.78 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:46.010833: step 32650, loss = 0.90 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:46.778094: step 32660, loss = 0.91 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:47.635909: step 32670, loss = 0.79 (1492.2 examples/sec; 0.086 sec/batch)
2017-05-05 18:07:48.302886: step 32680, loss = 0.93 (1919.1 examples/sec; 0.067 sec/batch)
2017-05-05 18:07:49.067084: step 32690, loss = 0.86 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:49.838372: step 32700, loss = 0.90 (1659.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:50.600176: step 32710, loss = 0.90 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:51.365278: step 32720, loss = 1.00 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:52.119876: step 32730, loss = 0.79 (1696.3 examples/sec; 0.075 sec/batch)
2017-05-05 18:07:52.890740: step 32740, loss = 0.81 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:53.656105: step 32750, loss = 0.98 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:54.421975: step 32760, loss = 0.94 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:55.188390: step 32770, loss = 0.86 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:55.938967: step 32780, loss = 0.81 (1705.4 examples/sec; 0.075 sec/batch)
2017-05-05 18:07:56.706092: step 32790, loss = 0.90 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:57.470454: step 32800, loss = 0.81 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:07:58.241126: step 32810, loss = 0.96 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:59.010230: step 32820, loss = 0.88 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:07:59.764756: step 32830, loss = 0.78 (1696.4 examples/sec; 0.075 sec/batch)
2017-05-05 18:08:00.531244: step 32840, loss = 0.92 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:08:01.290879: step 32850, loss = 0.88 (1685.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:02.058264: step 32860, loss = 0.78 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:08:02.824226: step 32870, loss = 0.91 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:08:03.582651: step 32880, loss = 0.85 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:04.350662: step 32890, loss = 0.72 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:08:05.117333: step 32900, loss = 0.71 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:08:05.877149: step 32910, loss = 0.81 (1684.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:06.639694: step 32920, loss = 0.86 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:07.399778: step 32930, loss = 0.70 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:08.164698: step 32940, loss = 0.67 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:08.933207: step 32950, loss = 0.94 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:08:09.696748: step 32960, loss = 0.76 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:10.454813: step 32970, loss = 0.83 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:11.211895: step 32980, loss = 0.83 (1690.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:11.967727: step 32990, loss = 0.96 (1693.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:12.734957: step 33000, loss = 0.96 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:08:13.500099: step 33010, loss = 0.79 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:08:14.269227: step 33020, loss = 0.74 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:08:15.043473: step 33030, loss = 0.80 (1653.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:08:15.802739: step 33040, loss = 0.91 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:16.572292: step 33050, loss = 0.87 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:08:17.345049: step 33060, loss = 0.79 (1656.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:08:18.109099: step 33070, loss = 0.85 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:18.882536: step 33080, loss = 0.72 (1655.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:08:19.638228: step 33090, loss = 0.81 (1693.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:20.398712: step 33100, loss = 0.81 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:21.162530: step 33110, loss = 0.84 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:21.922810: step 33120, loss = 0.93 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:22.693148: step 33130, loss = 0.94 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:08:23.454485: step 33140, loss = 0.87 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:24.224134: step 33150, loss = 0.75 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:08:24.983315: step 33160, loss = 0.76 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:25.747585: step 33170, loss = 0.91 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:26.520333: step 33180, loss = 0.79 (1656.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:08:27.284402: step 33190, loss = 0.86 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:28.049092: step 33200, loss = 0.88 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:28.814396: step 33210, loss = 1.00 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:08:29.577970: step 33220, loss = 0.87 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:30.345475: step 33230, loss = 0.87 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:08:31.100963: step 33240, loss = 0.80 (1694.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:31.855012: step 33250, loss = 0.78 (1697.5 examples/sec; 0.075 sec/batch)
2017-05-05 18:08:32.626898: step 33260, loss = 0.81 (1658.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:08:33.395531: step 33270, loss = 0.82 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:08:34.169649: step 33280, loss = 0.94 (1653.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:08:34.929016: step 33290, loss = 0.85 (1685.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:35.683943: step 33300, loss = 0.69 (1695.5 examples/sec; 0.075 sec/batch)
2017-05-05 18:08:36.447120: step 33310, loss = 0.84 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:37.209271: step 33320, loss = 0.77 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:37.973940: step 33330, loss = 0.83 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:38.742948: step 33340, loss = 0.74 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:08:39.501171: step 33350, loss = 0.93 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:40.262255: step 33360, loss = 0.77 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:41.020664: step 33370, loss = 0.78 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:41.789192: step 33380, loss = 0.83 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:08:42.552383: step 33390, loss = 0.95 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:43.315811: step 33400, loss = 0.86 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:44.072352: step 33410, loss = 0.80 (1691.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:44.833140: step 33420, loss = 0.89 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:45.595407: step 33430, loss = 0.89 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:46.356646: step 33440, loss = 0.73 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:47.117139: step 33450, loss = 0.76 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:47.871250: step 33460, loss = 0.82 (1697.4 examples/sec; 0.075 sec/batch)
2017-05-05 18:08:48.638496: step 33470, loss = 0.71 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:08:49.406916: step 33480, loss = 0.80 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:08:50.167040: step 33490, loss = 0.72 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:50.923660: step 33500, loss = 0.88 (1691.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:51.677314: step 33510, loss = 0.92 (1698.4 examples/sec; 0.075 sec/batch)
2017-05-05 18:08:52.438880: step 33520, loss = 0.98 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:53.204857: step 33530, loss = 0.79 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:08:53.972003: step 33540, loss = 0.83 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:08:54.738665: step 33550, loss = 0.84 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:08:55.498729: step 33560, loss = 0.68 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:56.258174: step 33570, loss = 1.19 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:57.020005: step 33580, loss = 0.82 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:57.786606: step 33590, loss = 0.91 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:08:58.545448: step 33600, loss = 0.80 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:08:59.310748: step 33610, loss = 0.79 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:09:00.068825: step 33620, loss = 1.08 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:00.826912: step 33630, loss = 0.95 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:01.591328: step 33640, loss = 0.80 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:02.356923: step 33650, loss = 0.78 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:09:03.213595: step 33660, loss = 0.87 (1494.2 examples/sec; 0.086 sec/batch)
2017-05-05 18:09:03.872954: step 33670, loss = 0.84 (1941.3 examples/sec; 0.066 sec/batch)
2017-05-05 18:09:04.635564: step 33680, loss = 0.75 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:05.399774: step 33690, loss = 0.85 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:06.159688: step 33700, loss = 0.88 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:06.922323: step 33710, loss = 0.85 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:07.679917: step 33720, loss = 0.85 (1689.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:08.441153: step 33730, loss = 0.86 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:09.206156: step 33740, loss = 0.72 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:09.968158: step 33750, loss = 0.93 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:10.729260: step 33760, loss = 0.82 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:11.489981: step 33770, loss = 0.84 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:12.249439: step 33780, loss = 0.85 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:13.013951: step 33790, loss = 0.76 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:13.778117: step 33800, loss = 0.90 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:14.538845: step 33810, loss = 0.87 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:15.301787: step 33820, loss = 0.90 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:16.058869: step 33830, loss = 0.76 (1690.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:16.827262: step 33840, loss = 0.96 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:09:17.591740: step 33850, loss = 0.87 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:18.361404: step 33860, loss = 0.83 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:09:19.125431: step 33870, loss = 0.95 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:19.878856: step 33880, loss = 0.86 (1698.9 examples/sec; 0.075 sec/batch)
2017-05-05 18:09:20.640391: step 33890, loss = 0.79 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:21.412984: step 33900, loss = 0.82 (1656.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:09:22.172395: step 33910, loss = 0.88 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:22.936564: step 33920, loss = 0.93 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:23.686956: step 33930, loss = 0.80 (1705.8 examples/sec; 0.075 sec/batch)
2017-05-05 18:09:24.456413: step 33940, loss = 0.74 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:09:25.220215: step 33950, loss = 0.79 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:25.973236: step 33960, loss = 0.63 (1699.8 examples/sec; 0.075 sec/batch)
2017-05-05 18:09:26.745662: step 33970, loss = 0.96 (1657.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:09:27.505441: step 33980, loss = 0.71 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:28.264145: step 33990, loss = 0.87 (1687.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:29.038661: step 34000, loss = 0.78 (1652.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:09:29.807840: step 34010, loss = 0.70 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:09:30.568929: step 34020, loss = 0.90 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:31.330237: step 34030, loss = 0.94 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:32.090143: step 34040, loss = 0.77 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:32.856291: step 34050, loss = 0.81 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:09:33.626675: step 34060, loss = 0.75 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:09:34.394705: step 34070, loss = 0.77 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:09:35.163621: step 34080, loss = 0.96 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:09:35.915387: step 34090, loss = 0.81 (1702.7 examples/sec; 0.075 sec/batch)
2017-05-05 18:09:36.681640: step 34100, loss = 0.93 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:09:37.442577: step 34110, loss = 0.86 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:38.204667: step 34120, loss = 0.82 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:38.969595: step 34130, loss = 0.73 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:39.724824: step 34140, loss = 0.91 (1694.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:40.486068: step 34150, loss = 0.76 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:41.251611: step 34160, loss = 0.76 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:09:42.013781: step 34170, loss = 0.79 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:42.780460: step 34180, loss = 0.90 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:09:43.535454: step 34190, loss = 0.80 (1695.4 examples/sec; 0.075 sec/batch)
2017-05-05 18:09:44.294216: step 34200, loss = 0.84 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:45.070179: step 34210, loss = 0.82 (1649.6 examples/sec; 0.078 sec/batch)
2017-05-05 18:09:45.824773: step 34220, loss = 0.72 (1696.3 examples/sec; 0.075 sec/batch)
2017-05-05 18:09:46.595521: step 34230, loss = 0.84 (1660.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:09:47.362944: step 34240, loss = 0.90 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:09:48.119497: step 34250, loss = 0.67 (1691.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:48.883470: step 34260, loss = 0.71 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:49.652913: step 34270, loss = 1.01 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:09:50.418961: step 34280, loss = 0.62 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:09:51.179192: step 34290, loss = 0.77 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:51.938211: step 34300, loss = 0.81 (1686.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:52.707934: step 34310, loss = 0.96 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:09:53.478129: step 34320, loss = 0.75 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:09:54.248386: step 34330, loss = 0.91 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:09:55.018597: step 34340, loss = 0.67 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:09:55.777783: step 34350, loss = 0.80 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:09:56.548022: step 34360, loss = 0.91 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:09:57.314678: step 34370, loss = 0.81 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:09:58.080278: step 34380, loss = 0.78 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:09:58.845547: step 34390, loss = 0.90 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:09:59.610599: step 34400, loss = 0.85 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:10:00.379472: step 34410, loss = 0.79 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:10:01.145695: step 34420, loss = 0.95 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:10:01.908855: step 34430, loss = 0.86 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:02.684012: step 34440, loss = 0.61 (1651.3 examples/sec; 0.078 sec/batch)
2017-05-05 18:10:03.444566: step 34450, loss = 0.74 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:04.205468: step 34460, loss = 0.86 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:04.968601: step 34470, loss = 0.87 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:05.730285: step 34480, loss = 0.91 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:06.493280: step 34490, loss = 1.04 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:07.254145: step 34500, loss = 0.85 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:08.015600: step 34510, loss = 0.64 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:08.776049: step 34520, loss = 0.74 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:09.546216: step 34530, loss = 1.15 (1662.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:10:10.308687: step 34540, loss = 0.72 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:11.070532: step 34550, loss = 0.96 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:11.829267: step 34560, loss = 0.87 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:12.596165: step 34570, loss = 0.91 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:10:13.369305: step 34580, loss = 0.84 (1655.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:10:14.138420: step 34590, loss = 0.71 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:10:14.906956: step 34600, loss = 0.92 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:10:15.665379: step 34610, loss = 0.77 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:16.433828: step 34620, loss = 0.71 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:10:17.198572: step 34630, loss = 0.80 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:17.973442: step 34640, loss = 0.84 (1651.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:10:18.835407: step 34650, loss = 0.92 (1485.0 examples/sec; 0.086 sec/batch)
2017-05-05 18:10:19.497469: step 34660, loss = 0.69 (1933.3 examples/sec; 0.066 sec/batch)
2017-05-05 18:10:20.264526: step 34670, loss = 0.74 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:10:21.022742: step 34680, loss = 0.87 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:21.790480: step 34690, loss = 0.73 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:10:22.557544: step 34700, loss = 0.85 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:10:23.327007: step 34710, loss = 0.84 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:10:24.083925: step 34720, loss = 0.90 (1691.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:24.850391: step 34730, loss = 0.81 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:10:25.616980: step 34740, loss = 0.76 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:10:26.371226: step 34750, loss = 0.79 (1697.0 examples/sec; 0.075 sec/batch)
2017-05-05 18:10:27.139708: step 34760, loss = 1.02 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:10:27.894417: step 34770, loss = 1.00 (1696.0 examples/sec; 0.075 sec/batch)
2017-05-05 18:10:28.661492: step 34780, loss = 1.21 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:10:29.433449: step 34790, loss = 0.79 (1658.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:10:30.193383: step 34800, loss = 0.77 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:30.947955: step 34810, loss = 0.77 (1696.3 examples/sec; 0.075 sec/batch)
2017-05-05 18:10:31.701731: step 34820, loss = 0.84 (1698.1 examples/sec; 0.075 sec/batch)
2017-05-05 18:10:32.468304: step 34830, loss = 0.85 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:10:33.233850: step 34840, loss = 0.90 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:10:34.001387: step 34850, loss = 0.97 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:10:34.762664: step 34860, loss = 1.01 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:35.523563: step 34870, loss = 0.86 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:36.281347: step 34880, loss = 0.84 (1689.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:37.045999: step 34890, loss = 0.82 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:37.809875: step 34900, loss = 0.77 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:38.577590: step 34910, loss = 0.83 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:10:39.337640: step 34920, loss = 0.63 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:40.095392: step 34930, loss = 0.80 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:40.856499: step 34940, loss = 0.77 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:41.616394: step 34950, loss = 0.90 (1684.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:42.392674: step 34960, loss = 0.90 (1648.9 examples/sec; 0.078 sec/batch)
2017-05-05 18:10:43.155299: step 34970, loss = 0.84 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:43.911800: step 34980, loss = 0.97 (1692.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:44.680023: step 34990, loss = 1.00 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:10:45.446284: step 35000, loss = 0.83 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:10:46.210141: step 35010, loss = 0.73 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:46.972591: step 35020, loss = 0.89 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:47.730525: step 35030, loss = 0.78 (1688.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:48.503626: step 35040, loss = 0.78 (1655.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:10:49.267560: step 35050, loss = 0.87 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:50.033416: step 35060, loss = 0.78 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:10:50.797315: step 35070, loss = 0.76 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:51.559126: step 35080, loss = 0.76 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:52.324034: step 35090, loss = 0.74 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:53.086687: step 35100, loss = 0.87 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:53.851349: step 35110, loss = 0.74 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:54.620225: step 35120, loss = 0.65 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:10:55.379262: step 35130, loss = 0.77 (1686.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:56.130231: step 35140, loss = 0.95 (1704.5 examples/sec; 0.075 sec/batch)
2017-05-05 18:10:56.898101: step 35150, loss = 0.84 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:10:57.662294: step 35160, loss = 0.76 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:10:58.435520: step 35170, loss = 0.86 (1655.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:10:59.202269: step 35180, loss = 0.87 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:10:59.954284: step 35190, loss = 0.65 (1702.1 examples/sec; 0.075 sec/batch)
2017-05-05 18:11:00.718314: step 35200, loss = 0.71 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:01.475026: step 35210, loss = 0.93 (1691.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:02.239450: step 35220, loss = 0.78 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:03.001618: step 35230, loss = 0.85 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:03.763090: step 35240, loss = 0.81 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:04.529754: step 35250, loss = 1.03 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:11:05.293542: step 35260, loss = 0.82 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:06.053658: step 35270, loss = 0.75 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:06.832164: step 35280, loss = 0.85 (1644.2 examples/sec; 0.078 sec/batch)
2017-05-05 18:11:07.589903: step 35290, loss = 0.79 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:08.357266: step 35300, loss = 0.70 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:11:09.123596: step 35310, loss = 0.77 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:11:09.889055: step 35320, loss = 1.01 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:11:10.653289: step 35330, loss = 0.84 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:11.416987: step 35340, loss = 0.90 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:12.169753: step 35350, loss = 0.79 (1700.4 examples/sec; 0.075 sec/batch)
2017-05-05 18:11:12.931242: step 35360, loss = 0.89 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:13.705541: step 35370, loss = 0.69 (1653.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:11:14.473352: step 35380, loss = 0.90 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:11:15.238428: step 35390, loss = 0.83 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:11:15.988745: step 35400, loss = 0.75 (1706.0 examples/sec; 0.075 sec/batch)
2017-05-05 18:11:16.758959: step 35410, loss = 0.67 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:11:17.536624: step 35420, loss = 0.79 (1646.0 examples/sec; 0.078 sec/batch)
2017-05-05 18:11:18.301210: step 35430, loss = 0.92 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:19.066879: step 35440, loss = 0.82 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:11:19.823702: step 35450, loss = 0.70 (1691.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:20.582606: step 35460, loss = 0.93 (1686.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:21.338335: step 35470, loss = 0.80 (1693.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:22.105108: step 35480, loss = 0.87 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:11:22.869529: step 35490, loss = 0.79 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:23.630747: step 35500, loss = 0.99 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:24.392717: step 35510, loss = 0.74 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:25.157280: step 35520, loss = 0.89 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:25.911796: step 35530, loss = 0.82 (1696.5 examples/sec; 0.075 sec/batch)
2017-05-05 18:11:26.678891: step 35540, loss = 0.69 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:11:27.445326: step 35550, loss = 0.67 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:11:28.206758: step 35560, loss = 0.87 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:28.969249: step 35570, loss = 0.84 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:29.733103: step 35580, loss = 0.86 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:30.494202: step 35590, loss = 0.76 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:31.249133: step 35600, loss = 0.83 (1695.5 examples/sec; 0.075 sec/batch)
2017-05-05 18:11:31.999966: step 35610, loss = 0.91 (1704.8 examples/sec; 0.075 sec/batch)
2017-05-05 18:11:32.759494: step 35620, loss = 0.92 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:33.540943: step 35630, loss = 0.75 (1638.0 examples/sec; 0.078 sec/batch)
2017-05-05 18:11:34.424287: step 35640, loss = 0.84 (1449.0 examples/sec; 0.088 sec/batch)
2017-05-05 18:11:35.088115: step 35650, loss = 0.69 (1928.2 examples/sec; 0.066 sec/batch)
2017-05-05 18:11:35.838335: step 35660, loss = 0.86 (1706.2 examples/sec; 0.075 sec/batch)
2017-05-05 18:11:36.607712: step 35670, loss = 1.10 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:11:37.379080: step 35680, loss = 0.83 (1659.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:11:38.145202: step 35690, loss = 1.03 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:11:38.909758: step 35700, loss = 0.94 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:39.664923: step 35710, loss = 0.81 (1695.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:40.428584: step 35720, loss = 0.81 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:41.186513: step 35730, loss = 0.75 (1688.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:41.953718: step 35740, loss = 0.88 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:11:42.720039: step 35750, loss = 0.82 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:11:43.479821: step 35760, loss = 1.00 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:44.243340: step 35770, loss = 0.92 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:45.012503: step 35780, loss = 0.80 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:11:45.774917: step 35790, loss = 0.79 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:46.533642: step 35800, loss = 0.92 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:47.301897: step 35810, loss = 0.73 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:11:48.060673: step 35820, loss = 0.75 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:48.824074: step 35830, loss = 0.73 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:49.590490: step 35840, loss = 0.82 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:11:50.346350: step 35850, loss = 0.87 (1693.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:51.110429: step 35860, loss = 0.71 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:51.864518: step 35870, loss = 0.73 (1697.4 examples/sec; 0.075 sec/batch)
2017-05-05 18:11:52.625590: step 35880, loss = 0.79 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:53.393211: step 35890, loss = 0.76 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:11:54.156547: step 35900, loss = 0.84 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:54.916293: step 35910, loss = 0.84 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:55.674547: step 35920, loss = 0.74 (1688.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:56.437579: step 35930, loss = 0.97 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:57.203123: step 35940, loss = 0.91 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:11:57.966154: step 35950, loss = 0.89 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:11:58.731562: step 35960, loss = 0.94 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:11:59.492569: step 35970, loss = 0.85 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:00.258658: step 35980, loss = 0.89 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:12:01.014311: step 35990, loss = 0.79 (1693.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:01.782802: step 36000, loss = 0.84 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:12:02.546106: step 36010, loss = 0.92 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:03.311589: step 36020, loss = 0.90 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:12:04.065121: step 36030, loss = 0.78 (1698.7 examples/sec; 0.075 sec/batch)
2017-05-05 18:12:04.840828: step 36040, loss = 0.82 (1650.1 examples/sec; 0.078 sec/batch)
2017-05-05 18:12:05.599999: step 36050, loss = 0.91 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:06.381282: step 36060, loss = 0.87 (1638.3 examples/sec; 0.078 sec/batch)
2017-05-05 18:12:07.144991: step 36070, loss = 0.90 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:07.899176: step 36080, loss = 0.80 (1697.2 examples/sec; 0.075 sec/batch)
2017-05-05 18:12:08.660710: step 36090, loss = 0.93 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:09.430778: step 36100, loss = 0.87 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:12:10.194665: step 36110, loss = 0.87 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:10.953167: step 36120, loss = 0.68 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:11.712194: step 36130, loss = 0.96 (1686.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:12.475029: step 36140, loss = 0.95 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:13.242042: step 36150, loss = 0.86 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:12:14.006761: step 36160, loss = 0.85 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:14.769307: step 36170, loss = 0.77 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:15.530189: step 36180, loss = 0.76 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:16.286245: step 36190, loss = 0.74 (1693.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:17.047800: step 36200, loss = 0.82 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:17.818521: step 36210, loss = 0.88 (1660.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:12:18.585120: step 36220, loss = 0.84 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:12:19.350234: step 36230, loss = 0.92 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:12:20.107663: step 36240, loss = 0.75 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:20.863746: step 36250, loss = 0.98 (1692.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:21.639338: step 36260, loss = 0.92 (1650.4 examples/sec; 0.078 sec/batch)
2017-05-05 18:12:22.414303: step 36270, loss = 0.75 (1651.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:12:23.186334: step 36280, loss = 0.98 (1658.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:12:23.943406: step 36290, loss = 0.79 (1690.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:24.710428: step 36300, loss = 1.00 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:12:25.474774: step 36310, loss = 0.88 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:26.234376: step 36320, loss = 1.01 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:26.998932: step 36330, loss = 0.90 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:27.753535: step 36340, loss = 0.86 (1696.3 examples/sec; 0.075 sec/batch)
2017-05-05 18:12:28.518235: step 36350, loss = 0.66 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:29.283838: step 36360, loss = 0.86 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:12:30.051937: step 36370, loss = 0.82 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:12:30.809272: step 36380, loss = 0.96 (1690.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:31.566040: step 36390, loss = 0.93 (1691.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:32.333841: step 36400, loss = 0.99 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:12:33.092615: step 36410, loss = 0.72 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:33.860756: step 36420, loss = 0.74 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:12:34.622898: step 36430, loss = 0.84 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:35.390823: step 36440, loss = 0.82 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:12:36.149581: step 36450, loss = 0.84 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:36.919529: step 36460, loss = 0.84 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:12:37.698604: step 36470, loss = 0.78 (1643.0 examples/sec; 0.078 sec/batch)
2017-05-05 18:12:38.461672: step 36480, loss = 0.97 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:39.230498: step 36490, loss = 0.92 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:12:39.997636: step 36500, loss = 0.78 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:12:40.767293: step 36510, loss = 0.76 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:12:41.532702: step 36520, loss = 0.90 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:12:42.293483: step 36530, loss = 0.94 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:43.059239: step 36540, loss = 0.84 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:12:43.815425: step 36550, loss = 0.74 (1692.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:44.589130: step 36560, loss = 0.75 (1654.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:12:45.352777: step 36570, loss = 0.83 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:46.115512: step 36580, loss = 0.84 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:46.882565: step 36590, loss = 0.98 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:12:47.642303: step 36600, loss = 0.77 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:48.414425: step 36610, loss = 0.88 (1657.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:12:49.175473: step 36620, loss = 0.86 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:50.045648: step 36630, loss = 0.83 (1471.0 examples/sec; 0.087 sec/batch)
2017-05-05 18:12:50.701183: step 36640, loss = 0.92 (1952.6 examples/sec; 0.066 sec/batch)
2017-05-05 18:12:51.457238: step 36650, loss = 0.79 (1693.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:52.220085: step 36660, loss = 0.86 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:52.985128: step 36670, loss = 0.80 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:12:53.750266: step 36680, loss = 0.87 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:12:54.526054: step 36690, loss = 0.87 (1649.9 examples/sec; 0.078 sec/batch)
2017-05-05 18:12:55.291804: step 36700, loss = 0.74 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:12:56.056124: step 36710, loss = 0.79 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:56.824633: step 36720, loss = 0.94 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:12:57.597094: step 36730, loss = 0.94 (1657.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:12:58.363564: step 36740, loss = 0.88 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:12:59.123397: step 36750, loss = 0.88 (1684.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:12:59.883680: step 36760, loss = 0.75 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:00.642746: step 36770, loss = 0.78 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:01.405129: step 36780, loss = 0.81 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:02.173576: step 36790, loss = 0.79 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:02.945150: step 36800, loss = 0.81 (1659.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:03.707850: step 36810, loss = 1.00 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:04.469521: step 36820, loss = 0.91 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:05.237258: step 36830, loss = 0.72 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:05.994526: step 36840, loss = 0.97 (1690.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:06.762198: step 36850, loss = 0.84 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:07.524161: step 36860, loss = 0.86 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:08.286804: step 36870, loss = 0.84 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:09.052048: step 36880, loss = 1.00 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:09.818399: step 36890, loss = 0.69 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:10.580293: step 36900, loss = 0.81 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:11.339187: step 36910, loss = 0.87 (1686.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:12.099764: step 36920, loss = 0.92 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:12.864384: step 36930, loss = 0.84 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:13.635600: step 36940, loss = 0.72 (1659.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:14.401239: step 36950, loss = 0.95 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:15.165442: step 36960, loss = 0.84 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:15.925002: step 36970, loss = 0.81 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:16.687388: step 36980, loss = 0.75 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:17.454920: step 36990, loss = 0.82 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:18.220329: step 37000, loss = 0.81 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:18.991535: step 37010, loss = 0.85 (1659.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:19.747329: step 37020, loss = 0.77 (1693.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:20.515485: step 37030, loss = 0.90 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:21.284880: step 37040, loss = 0.82 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:22.051541: step 37050, loss = 0.97 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:22.819538: step 37060, loss = 0.84 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:23.573263: step 37070, loss = 0.87 (1698.2 examples/sec; 0.075 sec/batch)
2017-05-05 18:13:24.338835: step 37080, loss = 0.83 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:25.101757: step 37090, loss = 0.93 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:25.851440: step 37100, loss = 0.88 (1707.4 examples/sec; 0.075 sec/batch)
2017-05-05 18:13:26.615914: step 37110, loss = 1.06 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:27.379928: step 37120, loss = 0.79 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:28.141775: step 37130, loss = 0.89 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:28.900564: step 37140, loss = 0.83 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:29.659850: step 37150, loss = 0.78 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:30.424751: step 37160, loss = 0.79 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:31.186644: step 37170, loss = 0.94 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:31.940390: step 37180, loss = 0.77 (1698.3 examples/sec; 0.075 sec/batch)
2017-05-05 18:13:32.707961: step 37190, loss = 0.77 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:33.475015: step 37200, loss = 0.67 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:34.243333: step 37210, loss = 0.78 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:35.005424: step 37220, loss = 0.83 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:35.765795: step 37230, loss = 0.70 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:36.537757: step 37240, loss = 0.97 (1658.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:37.311187: step 37250, loss = 0.86 (1655.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:38.072448: step 37260, loss = 0.75 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:38.843537: step 37270, loss = 0.78 (1660.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:39.600939: step 37280, loss = 0.83 (1690.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:40.362315: step 37290, loss = 0.93 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:41.124810: step 37300, loss = 0.86 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:41.885447: step 37310, loss = 0.83 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:42.657196: step 37320, loss = 0.79 (1658.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:43.414959: step 37330, loss = 0.78 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:44.187486: step 37340, loss = 0.80 (1656.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:44.948614: step 37350, loss = 0.74 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:45.714301: step 37360, loss = 0.80 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:46.482057: step 37370, loss = 0.71 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:47.251194: step 37380, loss = 0.79 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:48.011714: step 37390, loss = 0.79 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:48.779264: step 37400, loss = 0.76 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:49.545253: step 37410, loss = 0.80 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:50.309172: step 37420, loss = 1.06 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:51.067633: step 37430, loss = 0.74 (1687.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:51.821704: step 37440, loss = 0.73 (1697.4 examples/sec; 0.075 sec/batch)
2017-05-05 18:13:52.593813: step 37450, loss = 0.74 (1657.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:53.356402: step 37460, loss = 0.77 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:54.135648: step 37470, loss = 0.70 (1642.6 examples/sec; 0.078 sec/batch)
2017-05-05 18:13:54.896176: step 37480, loss = 0.93 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:55.657291: step 37490, loss = 0.77 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:56.421814: step 37500, loss = 0.78 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:13:57.191181: step 37510, loss = 0.87 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:57.957445: step 37520, loss = 0.74 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:58.725426: step 37530, loss = 0.80 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:13:59.485367: step 37540, loss = 0.80 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:00.248564: step 37550, loss = 0.66 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:01.007716: step 37560, loss = 0.76 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:01.771783: step 37570, loss = 0.92 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:02.541485: step 37580, loss = 0.88 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:14:03.302243: step 37590, loss = 0.92 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:04.066605: step 37600, loss = 0.87 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:04.827852: step 37610, loss = 0.77 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:05.714460: step 37620, loss = 0.95 (1443.7 examples/sec; 0.089 sec/batch)
2017-05-05 18:14:06.377832: step 37630, loss = 0.84 (1929.6 examples/sec; 0.066 sec/batch)
2017-05-05 18:14:07.140489: step 37640, loss = 0.90 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:07.897557: step 37650, loss = 0.92 (1690.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:08.663134: step 37660, loss = 0.85 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:14:09.429681: step 37670, loss = 0.77 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:14:10.196303: step 37680, loss = 0.79 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:14:10.959544: step 37690, loss = 0.57 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:11.714797: step 37700, loss = 0.81 (1694.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:12.479523: step 37710, loss = 0.66 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:13.255822: step 37720, loss = 0.91 (1648.9 examples/sec; 0.078 sec/batch)
2017-05-05 18:14:14.021443: step 37730, loss = 0.84 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:14:14.788217: step 37740, loss = 0.63 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:14:15.546822: step 37750, loss = 0.78 (1687.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:16.310129: step 37760, loss = 0.68 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:17.073718: step 37770, loss = 0.78 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:17.841404: step 37780, loss = 0.84 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:14:18.606684: step 37790, loss = 0.74 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:14:19.372731: step 37800, loss = 0.91 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:14:20.133870: step 37810, loss = 0.78 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:20.904081: step 37820, loss = 0.71 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:14:21.654006: step 37830, loss = 0.86 (1706.8 examples/sec; 0.075 sec/batch)
2017-05-05 18:14:22.421778: step 37840, loss = 0.67 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:14:23.186856: step 37850, loss = 0.76 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:14:23.950115: step 37860, loss = 0.76 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:24.718011: step 37870, loss = 0.76 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:14:25.475333: step 37880, loss = 0.91 (1690.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:26.240952: step 37890, loss = 0.73 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:14:27.006531: step 37900, loss = 0.75 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:14:27.759425: step 37910, loss = 0.70 (1700.1 examples/sec; 0.075 sec/batch)
2017-05-05 18:14:28.522014: step 37920, loss = 0.80 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:29.280748: step 37930, loss = 0.85 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:30.045990: step 37940, loss = 0.68 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:14:30.809533: step 37950, loss = 0.75 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:31.565513: step 37960, loss = 0.89 (1693.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:32.330955: step 37970, loss = 0.89 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:14:33.095422: step 37980, loss = 0.84 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:33.863769: step 37990, loss = 0.77 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:14:34.623288: step 38000, loss = 0.96 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:35.405958: step 38010, loss = 0.73 (1635.4 examples/sec; 0.078 sec/batch)
2017-05-05 18:14:36.161964: step 38020, loss = 0.71 (1693.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:36.937497: step 38030, loss = 0.92 (1650.5 examples/sec; 0.078 sec/batch)
2017-05-05 18:14:37.708756: step 38040, loss = 0.74 (1659.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:14:38.480666: step 38050, loss = 0.77 (1658.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:14:39.246753: step 38060, loss = 0.80 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:14:40.004273: step 38070, loss = 0.79 (1689.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:40.763823: step 38080, loss = 0.73 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:41.528637: step 38090, loss = 0.78 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:42.297457: step 38100, loss = 0.92 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:14:43.059245: step 38110, loss = 0.68 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:43.816848: step 38120, loss = 0.87 (1689.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:44.585801: step 38130, loss = 0.80 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:14:45.350607: step 38140, loss = 0.72 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:46.104797: step 38150, loss = 0.68 (1697.2 examples/sec; 0.075 sec/batch)
2017-05-05 18:14:46.875558: step 38160, loss = 0.78 (1660.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:14:47.629947: step 38170, loss = 0.83 (1696.7 examples/sec; 0.075 sec/batch)
2017-05-05 18:14:48.393617: step 38180, loss = 0.87 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:49.160657: step 38190, loss = 0.68 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:14:49.928452: step 38200, loss = 0.83 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:14:50.690774: step 38210, loss = 0.77 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:51.457222: step 38220, loss = 0.97 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:14:52.218654: step 38230, loss = 0.76 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:52.983299: step 38240, loss = 1.00 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:53.750786: step 38250, loss = 1.05 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:14:54.513225: step 38260, loss = 0.85 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:55.281211: step 38270, loss = 0.69 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:14:56.036689: step 38280, loss = 0.91 (1694.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:56.800491: step 38290, loss = 0.93 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:57.564133: step 38300, loss = 0.86 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:14:58.334587: step 38310, loss = 0.73 (1661.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:14:59.100512: step 38320, loss = 0.73 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:14:59.856278: step 38330, loss = 0.80 (1693.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:00.622195: step 38340, loss = 0.94 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:15:01.391784: step 38350, loss = 0.88 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:15:02.154324: step 38360, loss = 0.73 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:02.918529: step 38370, loss = 0.82 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:03.679115: step 38380, loss = 0.96 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:04.446818: step 38390, loss = 0.77 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:15:05.204750: step 38400, loss = 0.82 (1688.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:05.965416: step 38410, loss = 0.90 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:06.736437: step 38420, loss = 0.94 (1660.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:15:07.501062: step 38430, loss = 0.85 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:08.264833: step 38440, loss = 0.66 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:09.027946: step 38450, loss = 0.96 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:09.788458: step 38460, loss = 0.75 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:10.552121: step 38470, loss = 0.88 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:11.310559: step 38480, loss = 0.74 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:12.067530: step 38490, loss = 0.89 (1690.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:12.837293: step 38500, loss = 0.84 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:15:13.605997: step 38510, loss = 0.75 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:15:14.372301: step 38520, loss = 0.81 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:15:15.142407: step 38530, loss = 0.88 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:15:15.887543: step 38540, loss = 0.86 (1717.8 examples/sec; 0.075 sec/batch)
2017-05-05 18:15:16.650395: step 38550, loss = 0.84 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:17.416391: step 38560, loss = 0.85 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:15:18.180883: step 38570, loss = 0.78 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:18.952165: step 38580, loss = 0.83 (1659.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:15:19.698498: step 38590, loss = 0.90 (1715.1 examples/sec; 0.075 sec/batch)
2017-05-05 18:15:20.465344: step 38600, loss = 0.69 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:15:21.341184: step 38610, loss = 0.88 (1461.5 examples/sec; 0.088 sec/batch)
2017-05-05 18:15:22.001084: step 38620, loss = 0.85 (1939.7 examples/sec; 0.066 sec/batch)
2017-05-05 18:15:22.766715: step 38630, loss = 0.72 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:15:23.529196: step 38640, loss = 0.95 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:24.292179: step 38650, loss = 0.69 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:25.053756: step 38660, loss = 0.94 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:25.811435: step 38670, loss = 0.99 (1689.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:26.575884: step 38680, loss = 0.78 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:27.338601: step 38690, loss = 0.87 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:28.087664: step 38700, loss = 0.79 (1708.8 examples/sec; 0.075 sec/batch)
2017-05-05 18:15:28.848732: step 38710, loss = 0.66 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:29.608462: step 38720, loss = 0.93 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:30.378988: step 38730, loss = 0.71 (1661.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:15:31.138437: step 38740, loss = 0.77 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:31.887288: step 38750, loss = 1.01 (1709.3 examples/sec; 0.075 sec/batch)
2017-05-05 18:15:32.650230: step 38760, loss = 0.73 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:33.415104: step 38770, loss = 1.04 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:34.177672: step 38780, loss = 0.88 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:34.940279: step 38790, loss = 0.76 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:35.700237: step 38800, loss = 0.83 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:36.462038: step 38810, loss = 0.96 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:37.228682: step 38820, loss = 0.94 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:15:37.995071: step 38830, loss = 0.75 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:15:38.761041: step 38840, loss = 0.93 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:15:39.516350: step 38850, loss = 0.95 (1694.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:40.279931: step 38860, loss = 0.83 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:41.033237: step 38870, loss = 0.82 (1699.2 examples/sec; 0.075 sec/batch)
2017-05-05 18:15:41.795327: step 38880, loss = 0.88 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:42.562777: step 38890, loss = 0.76 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:15:43.325420: step 38900, loss = 0.72 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:44.076582: step 38910, loss = 0.77 (1704.0 examples/sec; 0.075 sec/batch)
2017-05-05 18:15:44.842500: step 38920, loss = 0.70 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:15:45.603534: step 38930, loss = 0.74 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:46.368959: step 38940, loss = 0.92 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:15:47.133678: step 38950, loss = 0.78 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:47.892065: step 38960, loss = 0.81 (1687.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:48.659718: step 38970, loss = 0.89 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:15:49.424493: step 38980, loss = 0.98 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:50.192476: step 38990, loss = 0.92 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:15:50.952977: step 39000, loss = 0.78 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:51.709835: step 39010, loss = 0.61 (1691.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:52.476342: step 39020, loss = 0.71 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:15:53.244229: step 39030, loss = 0.94 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:15:54.014186: step 39040, loss = 0.82 (1662.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:15:54.782065: step 39050, loss = 0.78 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:15:55.543728: step 39060, loss = 0.87 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:56.301081: step 39070, loss = 0.68 (1690.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:15:57.071183: step 39080, loss = 0.84 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:15:57.837561: step 39090, loss = 0.81 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:15:58.611981: step 39100, loss = 0.80 (1652.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:15:59.370503: step 39110, loss = 0.74 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:00.124404: step 39120, loss = 0.96 (1697.8 examples/sec; 0.075 sec/batch)
2017-05-05 18:16:00.887206: step 39130, loss = 0.80 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:01.662614: step 39140, loss = 0.92 (1650.8 examples/sec; 0.078 sec/batch)
2017-05-05 18:16:02.434726: step 39150, loss = 0.91 (1657.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:16:03.208509: step 39160, loss = 0.78 (1654.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:16:03.968314: step 39170, loss = 0.95 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:04.732774: step 39180, loss = 0.68 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:05.534780: step 39190, loss = 0.93 (1596.0 examples/sec; 0.080 sec/batch)
2017-05-05 18:16:06.303284: step 39200, loss = 0.83 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:16:07.065036: step 39210, loss = 1.10 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:07.823145: step 39220, loss = 0.79 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:08.588130: step 39230, loss = 0.84 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:09.352599: step 39240, loss = 0.77 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:10.116668: step 39250, loss = 0.79 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:10.876260: step 39260, loss = 0.68 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:11.633051: step 39270, loss = 0.78 (1691.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:12.403122: step 39280, loss = 0.68 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:16:13.166460: step 39290, loss = 0.67 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:13.930827: step 39300, loss = 0.80 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:14.702671: step 39310, loss = 0.77 (1658.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:16:15.463266: step 39320, loss = 1.01 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:16.224196: step 39330, loss = 0.76 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:16.992930: step 39340, loss = 0.76 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:16:17.764859: step 39350, loss = 0.96 (1658.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:16:18.528345: step 39360, loss = 0.73 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:19.291538: step 39370, loss = 0.80 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:20.054882: step 39380, loss = 0.83 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:20.809588: step 39390, loss = 0.69 (1696.0 examples/sec; 0.075 sec/batch)
2017-05-05 18:16:21.570253: step 39400, loss = 0.75 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:22.346914: step 39410, loss = 0.84 (1648.1 examples/sec; 0.078 sec/batch)
2017-05-05 18:16:23.108795: step 39420, loss = 0.76 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:23.864676: step 39430, loss = 0.70 (1693.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:24.705606: step 39440, loss = 0.82 (1522.1 examples/sec; 0.084 sec/batch)
2017-05-05 18:16:25.466870: step 39450, loss = 0.73 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:26.245699: step 39460, loss = 0.69 (1643.5 examples/sec; 0.078 sec/batch)
2017-05-05 18:16:27.092338: step 39470, loss = 0.82 (1511.9 examples/sec; 0.085 sec/batch)
2017-05-05 18:16:27.851235: step 39480, loss = 0.85 (1686.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:28.686527: step 39490, loss = 0.76 (1532.4 examples/sec; 0.084 sec/batch)
2017-05-05 18:16:29.453567: step 39500, loss = 0.83 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:16:30.218207: step 39510, loss = 0.84 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:30.979266: step 39520, loss = 0.88 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:31.739334: step 39530, loss = 0.72 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:32.512629: step 39540, loss = 0.94 (1655.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:16:33.276857: step 39550, loss = 0.77 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:34.043225: step 39560, loss = 0.87 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:16:34.810749: step 39570, loss = 0.97 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:16:35.577248: step 39580, loss = 0.94 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:16:36.338188: step 39590, loss = 0.90 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:37.212325: step 39600, loss = 0.78 (1464.3 examples/sec; 0.087 sec/batch)
2017-05-05 18:16:37.870237: step 39610, loss = 0.78 (1945.5 examples/sec; 0.066 sec/batch)
2017-05-05 18:16:38.637877: step 39620, loss = 0.88 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:16:39.401728: step 39630, loss = 0.83 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:40.160242: step 39640, loss = 0.72 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:40.923762: step 39650, loss = 0.76 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:41.689344: step 39660, loss = 0.93 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:16:42.456835: step 39670, loss = 1.13 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:16:43.221191: step 39680, loss = 0.84 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:43.974245: step 39690, loss = 0.90 (1699.7 examples/sec; 0.075 sec/batch)
2017-05-05 18:16:44.735644: step 39700, loss = 0.80 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:45.507933: step 39710, loss = 0.79 (1657.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:16:46.267600: step 39720, loss = 0.85 (1685.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:47.033262: step 39730, loss = 0.96 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:16:47.788777: step 39740, loss = 0.81 (1694.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:48.554881: step 39750, loss = 0.78 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:16:49.320896: step 39760, loss = 0.84 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:16:50.086928: step 39770, loss = 0.84 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:16:50.841652: step 39780, loss = 0.80 (1696.0 examples/sec; 0.075 sec/batch)
2017-05-05 18:16:51.593765: step 39790, loss = 0.72 (1701.9 examples/sec; 0.075 sec/batch)
2017-05-05 18:16:52.356845: step 39800, loss = 0.81 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:53.121977: step 39810, loss = 0.82 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:16:53.888235: step 39820, loss = 0.77 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:16:54.661482: step 39830, loss = 0.72 (1655.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:16:55.425751: step 39840, loss = 0.86 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:56.184314: step 39850, loss = 0.96 (1687.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:56.956376: step 39860, loss = 0.83 (1657.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:16:57.724603: step 39870, loss = 0.76 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:16:58.488043: step 39880, loss = 0.74 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:16:59.252673: step 39890, loss = 0.75 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:00.005165: step 39900, loss = 0.73 (1701.0 examples/sec; 0.075 sec/batch)
2017-05-05 18:17:00.762454: step 39910, loss = 0.85 (1690.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:01.524487: step 39920, loss = 0.83 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:02.290522: step 39930, loss = 0.97 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:03.058830: step 39940, loss = 0.76 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:03.811705: step 39950, loss = 0.80 (1700.2 examples/sec; 0.075 sec/batch)
2017-05-05 18:17:04.587002: step 39960, loss = 0.85 (1651.0 examples/sec; 0.078 sec/batch)
2017-05-05 18:17:05.352332: step 39970, loss = 0.86 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:06.119437: step 39980, loss = 0.80 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:06.879808: step 39990, loss = 0.69 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:07.641136: step 40000, loss = 0.69 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:08.409300: step 40010, loss = 0.73 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:09.175920: step 40020, loss = 0.93 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:09.945339: step 40030, loss = 0.97 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:10.714523: step 40040, loss = 0.86 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:11.476843: step 40050, loss = 0.85 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:12.237717: step 40060, loss = 0.97 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:13.003494: step 40070, loss = 0.74 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:13.767281: step 40080, loss = 0.79 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:14.535579: step 40090, loss = 0.83 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:15.300873: step 40100, loss = 0.84 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:16.055616: step 40110, loss = 0.94 (1695.9 examples/sec; 0.075 sec/batch)
2017-05-05 18:17:16.825758: step 40120, loss = 0.73 (1662.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:17.587023: step 40130, loss = 0.78 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:18.357487: step 40140, loss = 0.84 (1661.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:19.130060: step 40150, loss = 0.92 (1656.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:19.880297: step 40160, loss = 0.78 (1706.1 examples/sec; 0.075 sec/batch)
2017-05-05 18:17:20.642833: step 40170, loss = 0.81 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:21.407161: step 40180, loss = 0.77 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:22.171502: step 40190, loss = 0.87 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:22.943452: step 40200, loss = 0.80 (1658.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:23.701966: step 40210, loss = 0.76 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:24.479019: step 40220, loss = 0.89 (1647.2 examples/sec; 0.078 sec/batch)
2017-05-05 18:17:25.242907: step 40230, loss = 0.76 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:26.007189: step 40240, loss = 0.77 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:26.773759: step 40250, loss = 0.87 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:27.528782: step 40260, loss = 0.75 (1695.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:28.294131: step 40270, loss = 0.74 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:29.059390: step 40280, loss = 0.83 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:29.826496: step 40290, loss = 0.71 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:30.587509: step 40300, loss = 0.79 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:31.345728: step 40310, loss = 0.78 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:32.104113: step 40320, loss = 0.77 (1687.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:32.869120: step 40330, loss = 0.76 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:33.631037: step 40340, loss = 0.62 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:34.394196: step 40350, loss = 0.84 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:35.161935: step 40360, loss = 0.79 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:35.925215: step 40370, loss = 1.04 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:36.690395: step 40380, loss = 0.84 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:37.463383: step 40390, loss = 0.96 (1655.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:38.229631: step 40400, loss = 0.85 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:38.992455: step 40410, loss = 0.73 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:39.749437: step 40420, loss = 0.65 (1690.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:40.522557: step 40430, loss = 0.88 (1655.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:41.282409: step 40440, loss = 1.03 (1684.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:42.048314: step 40450, loss = 0.85 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:42.811870: step 40460, loss = 0.93 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:43.569995: step 40470, loss = 0.84 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:44.330232: step 40480, loss = 0.87 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:45.098807: step 40490, loss = 0.82 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:45.854076: step 40500, loss = 0.84 (1694.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:46.622474: step 40510, loss = 0.84 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:47.388285: step 40520, loss = 0.78 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:48.153931: step 40530, loss = 0.69 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:48.911290: step 40540, loss = 0.74 (1690.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:49.678592: step 40550, loss = 0.98 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:50.438797: step 40560, loss = 0.81 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:51.198541: step 40570, loss = 0.84 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:51.955774: step 40580, loss = 0.88 (1690.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:52.823138: step 40590, loss = 1.01 (1475.7 examples/sec; 0.087 sec/batch)
2017-05-05 18:17:53.495008: step 40600, loss = 0.82 (1905.1 examples/sec; 0.067 sec/batch)
2017-05-05 18:17:54.263160: step 40610, loss = 0.82 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:55.032467: step 40620, loss = 0.68 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:55.788944: step 40630, loss = 0.86 (1692.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:17:56.554805: step 40640, loss = 0.77 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:57.320345: step 40650, loss = 0.65 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:58.085471: step 40660, loss = 0.83 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:58.852940: step 40670, loss = 0.83 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:17:59.620414: step 40680, loss = 0.93 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:00.387049: step 40690, loss = 0.94 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:01.148023: step 40700, loss = 0.78 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:18:01.912010: step 40710, loss = 0.92 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:18:02.682809: step 40720, loss = 0.93 (1660.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:03.449293: step 40730, loss = 0.79 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:04.215646: step 40740, loss = 0.91 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:04.978484: step 40750, loss = 0.93 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:18:05.746666: step 40760, loss = 0.74 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:06.514838: step 40770, loss = 0.91 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:07.291007: step 40780, loss = 0.74 (1649.1 examples/sec; 0.078 sec/batch)
2017-05-05 18:18:08.047229: step 40790, loss = 0.69 (1692.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:18:08.815615: step 40800, loss = 0.83 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:09.586613: step 40810, loss = 0.78 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:10.355568: step 40820, loss = 0.74 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:11.119336: step 40830, loss = 0.71 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:18:11.878859: step 40840, loss = 1.05 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:18:12.652655: step 40850, loss = 0.85 (1654.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:13.427330: step 40860, loss = 0.90 (1652.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:14.197627: step 40870, loss = 0.71 (1661.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:14.966045: step 40880, loss = 0.74 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:15.721349: step 40890, loss = 0.76 (1694.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:18:16.488186: step 40900, loss = 0.81 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:17.264663: step 40910, loss = 0.71 (1648.5 examples/sec; 0.078 sec/batch)
2017-05-05 18:18:18.031247: step 40920, loss = 0.89 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:18.803311: step 40930, loss = 0.88 (1657.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:19.562606: step 40940, loss = 0.95 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:18:20.333062: step 40950, loss = 0.86 (1661.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:21.096913: step 40960, loss = 1.10 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:18:21.865926: step 40970, loss = 0.93 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:22.641182: step 40980, loss = 0.78 (1651.1 examples/sec; 0.078 sec/batch)
2017-05-05 18:18:23.410526: step 40990, loss = 0.79 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:24.175558: step 41000, loss = 0.82 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:24.947351: step 41010, loss = 0.83 (1658.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:25.699750: step 41020, loss = 0.83 (1701.2 examples/sec; 0.075 sec/batch)
2017-05-05 18:18:26.467072: step 41030, loss = 0.70 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:27.233796: step 41040, loss = 0.91 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:27.997812: step 41050, loss = 0.70 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:18:28.758247: step 41060, loss = 0.89 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:18:29.530721: step 41070, loss = 0.73 (1657.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:30.288852: step 41080, loss = 0.74 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:18:31.050843: step 41090, loss = 0.79 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:18:31.804564: step 41100, loss = 0.83 (1698.2 examples/sec; 0.075 sec/batch)
2017-05-05 18:18:32.568687: step 41110, loss = 0.74 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:18:33.335837: step 41120, loss = 0.81 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:34.101847: step 41130, loss = 0.91 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:34.863103: step 41140, loss = 0.70 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:18:35.621012: step 41150, loss = 0.61 (1688.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:18:36.390844: step 41160, loss = 0.86 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:37.151539: step 41170, loss = 0.87 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:18:37.918505: step 41180, loss = 0.90 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:38.685120: step 41190, loss = 0.82 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:39.453614: step 41200, loss = 0.83 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:40.217641: step 41210, loss = 0.73 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:18:40.976760: step 41220, loss = 0.85 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:18:41.742123: step 41230, loss = 0.84 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:42.506425: step 41240, loss = 0.88 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:18:43.277558: step 41250, loss = 0.83 (1659.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:44.033175: step 41260, loss = 0.82 (1694.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:18:44.796285: step 41270, loss = 0.76 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:18:45.557100: step 41280, loss = 0.78 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:18:46.321253: step 41290, loss = 0.72 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:18:47.088708: step 41300, loss = 0.75 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:47.848244: step 41310, loss = 0.76 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:18:48.618416: step 41320, loss = 0.71 (1662.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:49.381287: step 41330, loss = 0.95 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:18:50.157287: step 41340, loss = 0.79 (1649.5 examples/sec; 0.078 sec/batch)
2017-05-05 18:18:50.925969: step 41350, loss = 0.88 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:51.680281: step 41360, loss = 0.83 (1696.9 examples/sec; 0.075 sec/batch)
2017-05-05 18:18:52.449006: step 41370, loss = 0.72 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:53.218656: step 41380, loss = 0.80 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:53.993108: step 41390, loss = 0.79 (1652.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:54.756921: step 41400, loss = 0.77 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:18:55.517267: step 41410, loss = 0.82 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:18:56.275298: step 41420, loss = 0.87 (1688.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:18:57.043572: step 41430, loss = 0.76 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:57.807318: step 41440, loss = 0.91 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:18:58.572730: step 41450, loss = 0.84 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:18:59.337147: step 41460, loss = 0.86 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:00.094950: step 41470, loss = 0.84 (1689.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:00.856988: step 41480, loss = 0.73 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:01.621848: step 41490, loss = 0.79 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:02.394683: step 41500, loss = 0.80 (1656.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:19:03.167008: step 41510, loss = 0.77 (1657.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:19:03.924956: step 41520, loss = 0.84 (1688.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:04.692707: step 41530, loss = 0.84 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:19:05.459811: step 41540, loss = 0.83 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:19:06.223971: step 41550, loss = 0.88 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:06.989138: step 41560, loss = 0.87 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:19:07.749277: step 41570, loss = 0.77 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:08.619527: step 41580, loss = 0.91 (1470.9 examples/sec; 0.087 sec/batch)
2017-05-05 18:19:09.287461: step 41590, loss = 0.86 (1916.3 examples/sec; 0.067 sec/batch)
2017-05-05 18:19:10.055574: step 41600, loss = 0.85 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:19:10.818218: step 41610, loss = 0.89 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:11.573470: step 41620, loss = 0.80 (1694.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:12.334111: step 41630, loss = 0.79 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:13.098758: step 41640, loss = 0.75 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:13.863437: step 41650, loss = 0.66 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:14.637950: step 41660, loss = 0.83 (1652.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:19:15.398940: step 41670, loss = 1.02 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:16.150785: step 41680, loss = 0.90 (1702.5 examples/sec; 0.075 sec/batch)
2017-05-05 18:19:16.913256: step 41690, loss = 0.88 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:17.679438: step 41700, loss = 0.88 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:19:18.440485: step 41710, loss = 0.88 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:19.206543: step 41720, loss = 0.83 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:19:19.964595: step 41730, loss = 0.78 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:20.726296: step 41740, loss = 0.76 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:21.491034: step 41750, loss = 0.91 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:22.258712: step 41760, loss = 0.98 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:19:23.020166: step 41770, loss = 0.82 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:23.776656: step 41780, loss = 0.88 (1692.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:24.543234: step 41790, loss = 0.81 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:19:25.314905: step 41800, loss = 0.87 (1658.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:19:26.073987: step 41810, loss = 0.73 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:26.836580: step 41820, loss = 0.74 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:27.591197: step 41830, loss = 0.77 (1696.2 examples/sec; 0.075 sec/batch)
2017-05-05 18:19:28.360978: step 41840, loss = 0.79 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:19:29.132864: step 41850, loss = 0.62 (1658.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:19:29.899510: step 41860, loss = 0.83 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:19:30.657198: step 41870, loss = 0.73 (1689.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:31.416370: step 41880, loss = 0.94 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:32.173544: step 41890, loss = 0.83 (1690.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:32.941462: step 41900, loss = 0.93 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:19:33.700198: step 41910, loss = 0.96 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:34.470865: step 41920, loss = 0.74 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:19:35.230383: step 41930, loss = 0.81 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:35.990296: step 41940, loss = 0.76 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:36.751820: step 41950, loss = 0.83 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:37.521408: step 41960, loss = 0.90 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:19:38.292381: step 41970, loss = 0.77 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:19:39.061741: step 41980, loss = 0.92 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:19:39.819342: step 41990, loss = 0.91 (1689.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:40.580729: step 42000, loss = 0.82 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:41.339657: step 42010, loss = 0.90 (1686.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:42.109425: step 42020, loss = 0.81 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:19:42.870443: step 42030, loss = 0.81 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:43.626136: step 42040, loss = 0.86 (1693.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:44.390027: step 42050, loss = 0.65 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:45.156065: step 42060, loss = 0.76 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:19:45.915963: step 42070, loss = 0.72 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:46.685475: step 42080, loss = 0.80 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:19:47.444659: step 42090, loss = 0.64 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:48.206390: step 42100, loss = 0.95 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:48.974524: step 42110, loss = 0.80 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:19:49.738988: step 42120, loss = 0.87 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:50.500417: step 42130, loss = 0.83 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:51.265685: step 42140, loss = 0.89 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:19:52.023213: step 42150, loss = 0.77 (1689.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:52.788307: step 42160, loss = 0.84 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:19:53.557348: step 42170, loss = 0.68 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:19:54.323589: step 42180, loss = 0.85 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:19:55.096972: step 42190, loss = 0.83 (1655.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:19:55.854413: step 42200, loss = 0.74 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:56.619234: step 42210, loss = 0.76 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:57.377982: step 42220, loss = 0.77 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:58.140036: step 42230, loss = 0.87 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:19:58.915864: step 42240, loss = 0.70 (1649.9 examples/sec; 0.078 sec/batch)
2017-05-05 18:19:59.669852: step 42250, loss = 0.69 (1697.6 examples/sec; 0.075 sec/batch)
2017-05-05 18:20:00.433462: step 42260, loss = 0.78 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:01.190562: step 42270, loss = 0.95 (1690.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:01.966126: step 42280, loss = 0.83 (1650.4 examples/sec; 0.078 sec/batch)
2017-05-05 18:20:02.729419: step 42290, loss = 0.77 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:03.496026: step 42300, loss = 0.84 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:04.260352: step 42310, loss = 0.72 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:05.024219: step 42320, loss = 0.69 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:05.784438: step 42330, loss = 0.81 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:06.551202: step 42340, loss = 0.81 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:07.312288: step 42350, loss = 0.92 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:08.070389: step 42360, loss = 0.68 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:08.840695: step 42370, loss = 0.62 (1661.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:09.610008: step 42380, loss = 0.79 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:10.369827: step 42390, loss = 0.62 (1684.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:11.131491: step 42400, loss = 0.94 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:11.893241: step 42410, loss = 0.98 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:12.652772: step 42420, loss = 0.70 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:13.425971: step 42430, loss = 0.78 (1655.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:14.199267: step 42440, loss = 0.76 (1655.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:14.960537: step 42450, loss = 0.96 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:15.719687: step 42460, loss = 0.77 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:16.494347: step 42470, loss = 0.73 (1652.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:17.260823: step 42480, loss = 0.86 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:18.027616: step 42490, loss = 0.66 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:18.799007: step 42500, loss = 0.77 (1659.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:19.556179: step 42510, loss = 0.86 (1690.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:20.315959: step 42520, loss = 0.83 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:21.080158: step 42530, loss = 0.89 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:21.849613: step 42540, loss = 0.68 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:22.621572: step 42550, loss = 0.75 (1658.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:23.382296: step 42560, loss = 0.83 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:24.249202: step 42570, loss = 0.88 (1476.5 examples/sec; 0.087 sec/batch)
2017-05-05 18:20:24.915937: step 42580, loss = 0.92 (1919.8 examples/sec; 0.067 sec/batch)
2017-05-05 18:20:25.689093: step 42590, loss = 0.79 (1655.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:26.459101: step 42600, loss = 0.89 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:27.229249: step 42610, loss = 0.85 (1662.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:27.986223: step 42620, loss = 0.64 (1691.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:28.749649: step 42630, loss = 0.77 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:29.518027: step 42640, loss = 0.79 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:30.284998: step 42650, loss = 0.67 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:31.047779: step 42660, loss = 0.95 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:31.802959: step 42670, loss = 0.99 (1694.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:32.567853: step 42680, loss = 1.01 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:33.330037: step 42690, loss = 0.80 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:34.093675: step 42700, loss = 0.74 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:34.859593: step 42710, loss = 0.81 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:35.613509: step 42720, loss = 0.86 (1697.8 examples/sec; 0.075 sec/batch)
2017-05-05 18:20:36.379459: step 42730, loss = 0.76 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:37.141749: step 42740, loss = 0.96 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:37.905724: step 42750, loss = 0.87 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:38.670150: step 42760, loss = 0.74 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:39.436917: step 42770, loss = 0.79 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:40.201585: step 42780, loss = 0.79 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:40.960226: step 42790, loss = 0.72 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:41.723959: step 42800, loss = 0.87 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:42.489621: step 42810, loss = 0.79 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:43.260488: step 42820, loss = 0.79 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:44.017235: step 42830, loss = 0.78 (1691.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:44.773278: step 42840, loss = 0.69 (1693.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:45.533864: step 42850, loss = 0.71 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:46.300932: step 42860, loss = 0.96 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:47.063448: step 42870, loss = 0.84 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:47.820067: step 42880, loss = 0.88 (1691.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:48.588063: step 42890, loss = 0.89 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:49.353510: step 42900, loss = 0.74 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:50.121497: step 42910, loss = 0.68 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:50.880849: step 42920, loss = 0.91 (1685.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:51.634283: step 42930, loss = 0.78 (1698.9 examples/sec; 0.075 sec/batch)
2017-05-05 18:20:52.399988: step 42940, loss = 0.79 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:53.168714: step 42950, loss = 0.74 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:53.936548: step 42960, loss = 0.82 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:54.705329: step 42970, loss = 0.73 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:55.464176: step 42980, loss = 0.78 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:56.228589: step 42990, loss = 0.84 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:20:57.000417: step 43000, loss = 0.81 (1658.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:57.770206: step 43010, loss = 0.77 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:58.539820: step 43020, loss = 0.67 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:20:59.301812: step 43030, loss = 0.83 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:00.058781: step 43040, loss = 0.87 (1691.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:00.814026: step 43050, loss = 0.78 (1694.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:01.576991: step 43060, loss = 0.76 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:02.340146: step 43070, loss = 0.74 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:03.107918: step 43080, loss = 0.79 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:21:03.861490: step 43090, loss = 0.97 (1698.6 examples/sec; 0.075 sec/batch)
2017-05-05 18:21:04.621740: step 43100, loss = 0.86 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:05.383349: step 43110, loss = 0.74 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:06.144117: step 43120, loss = 0.83 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:06.908772: step 43130, loss = 0.79 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:07.667821: step 43140, loss = 0.75 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:08.434584: step 43150, loss = 0.81 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:21:09.203287: step 43160, loss = 0.87 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:21:09.970103: step 43170, loss = 0.91 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:21:10.734742: step 43180, loss = 0.84 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:11.491929: step 43190, loss = 0.91 (1690.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:12.260306: step 43200, loss = 0.63 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:21:13.017421: step 43210, loss = 0.88 (1690.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:13.784950: step 43220, loss = 0.75 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:21:14.550773: step 43230, loss = 0.83 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:21:15.308504: step 43240, loss = 0.76 (1689.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:16.063740: step 43250, loss = 0.95 (1694.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:16.829910: step 43260, loss = 0.92 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:21:17.598948: step 43270, loss = 0.92 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:21:18.363707: step 43280, loss = 0.85 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:19.132091: step 43290, loss = 0.75 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:21:19.891729: step 43300, loss = 0.77 (1685.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:20.656052: step 43310, loss = 0.89 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:21.421381: step 43320, loss = 0.75 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:21:22.185894: step 43330, loss = 0.81 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:22.947221: step 43340, loss = 0.79 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:23.705921: step 43350, loss = 0.70 (1687.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:24.467694: step 43360, loss = 0.84 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:25.233333: step 43370, loss = 0.83 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:21:25.996599: step 43380, loss = 0.76 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:26.767732: step 43390, loss = 0.90 (1659.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:21:27.528964: step 43400, loss = 0.73 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:28.289639: step 43410, loss = 0.80 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:29.061842: step 43420, loss = 0.89 (1657.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:21:29.824119: step 43430, loss = 0.75 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:30.593306: step 43440, loss = 0.84 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:21:31.356795: step 43450, loss = 0.78 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:32.110435: step 43460, loss = 0.81 (1698.4 examples/sec; 0.075 sec/batch)
2017-05-05 18:21:32.876522: step 43470, loss = 0.78 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:21:33.639060: step 43480, loss = 0.80 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:34.403686: step 43490, loss = 0.75 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:35.174670: step 43500, loss = 0.95 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:21:35.927814: step 43510, loss = 0.93 (1699.5 examples/sec; 0.075 sec/batch)
2017-05-05 18:21:36.692748: step 43520, loss = 0.81 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:37.456454: step 43530, loss = 0.92 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:38.228141: step 43540, loss = 0.86 (1658.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:21:38.999289: step 43550, loss = 0.83 (1659.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:21:39.851052: step 43560, loss = 0.67 (1502.8 examples/sec; 0.085 sec/batch)
2017-05-05 18:21:40.526930: step 43570, loss = 0.86 (1893.8 examples/sec; 0.068 sec/batch)
2017-05-05 18:21:41.291560: step 43580, loss = 0.79 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:42.053487: step 43590, loss = 0.70 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:42.822412: step 43600, loss = 0.80 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:21:43.578486: step 43610, loss = 0.75 (1692.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:44.341996: step 43620, loss = 0.92 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:45.112737: step 43630, loss = 0.93 (1660.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:21:45.879972: step 43640, loss = 0.76 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:21:46.643965: step 43650, loss = 0.76 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:47.402695: step 43660, loss = 0.76 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:48.164555: step 43670, loss = 0.85 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:48.927915: step 43680, loss = 0.81 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:49.695858: step 43690, loss = 0.80 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:21:50.462416: step 43700, loss = 0.71 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:21:51.232101: step 43710, loss = 0.79 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:21:51.987214: step 43720, loss = 0.75 (1695.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:52.754832: step 43730, loss = 0.79 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:21:53.518336: step 43740, loss = 0.86 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:54.282502: step 43750, loss = 0.88 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:55.052851: step 43760, loss = 0.83 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:21:55.807554: step 43770, loss = 0.83 (1696.1 examples/sec; 0.075 sec/batch)
2017-05-05 18:21:56.574180: step 43780, loss = 0.82 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:21:57.338249: step 43790, loss = 0.78 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:58.101216: step 43800, loss = 0.89 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:58.863975: step 43810, loss = 0.75 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:21:59.623139: step 43820, loss = 0.83 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:00.389110: step 43830, loss = 0.83 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:22:01.149443: step 43840, loss = 0.88 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:01.913403: step 43850, loss = 0.79 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:02.683960: step 43860, loss = 0.87 (1661.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:22:03.445617: step 43870, loss = 0.79 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:04.203842: step 43880, loss = 0.77 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:04.965157: step 43890, loss = 0.80 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:05.731323: step 43900, loss = 0.74 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:22:06.502607: step 43910, loss = 0.94 (1659.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:22:07.263703: step 43920, loss = 0.86 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:08.024522: step 43930, loss = 0.84 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:08.786515: step 43940, loss = 0.76 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:09.563615: step 43950, loss = 0.75 (1647.2 examples/sec; 0.078 sec/batch)
2017-05-05 18:22:10.333322: step 43960, loss = 0.97 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:22:11.099757: step 43970, loss = 0.72 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:22:11.854036: step 43980, loss = 1.02 (1697.0 examples/sec; 0.075 sec/batch)
2017-05-05 18:22:12.620292: step 43990, loss = 0.67 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:22:13.379963: step 44000, loss = 0.73 (1684.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:14.145742: step 44010, loss = 0.75 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:22:14.906686: step 44020, loss = 0.76 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:15.666600: step 44030, loss = 0.84 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:16.427103: step 44040, loss = 0.82 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:17.193691: step 44050, loss = 1.04 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:22:17.947946: step 44060, loss = 0.83 (1697.0 examples/sec; 0.075 sec/batch)
2017-05-05 18:22:18.711980: step 44070, loss = 0.70 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:19.477631: step 44080, loss = 0.77 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:22:20.240534: step 44090, loss = 0.83 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:21.004173: step 44100, loss = 0.81 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:21.761591: step 44110, loss = 0.76 (1690.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:22.522182: step 44120, loss = 0.84 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:23.292883: step 44130, loss = 0.71 (1660.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:22:24.043975: step 44140, loss = 0.81 (1704.2 examples/sec; 0.075 sec/batch)
2017-05-05 18:22:24.810427: step 44150, loss = 0.81 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:22:25.580464: step 44160, loss = 0.78 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:22:26.339848: step 44170, loss = 0.68 (1685.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:27.105264: step 44180, loss = 1.01 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:22:27.862003: step 44190, loss = 0.96 (1691.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:28.625011: step 44200, loss = 0.73 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:29.382560: step 44210, loss = 1.04 (1689.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:30.152156: step 44220, loss = 0.83 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:22:30.916795: step 44230, loss = 0.68 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:31.670790: step 44240, loss = 0.72 (1697.6 examples/sec; 0.075 sec/batch)
2017-05-05 18:22:32.439140: step 44250, loss = 0.95 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:22:33.204303: step 44260, loss = 0.88 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:22:33.958105: step 44270, loss = 0.79 (1698.0 examples/sec; 0.075 sec/batch)
2017-05-05 18:22:34.721658: step 44280, loss = 0.71 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:35.486194: step 44290, loss = 0.84 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:36.244948: step 44300, loss = 0.76 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:37.008631: step 44310, loss = 0.76 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:37.766007: step 44320, loss = 0.85 (1690.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:38.531253: step 44330, loss = 0.94 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:22:39.297803: step 44340, loss = 0.70 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:22:40.051511: step 44350, loss = 0.72 (1698.3 examples/sec; 0.075 sec/batch)
2017-05-05 18:22:40.816339: step 44360, loss = 0.73 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:41.576299: step 44370, loss = 0.68 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:42.343193: step 44380, loss = 0.78 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:22:43.106448: step 44390, loss = 0.69 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:43.863144: step 44400, loss = 0.78 (1691.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:44.627550: step 44410, loss = 0.61 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:45.396111: step 44420, loss = 0.70 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:22:46.157587: step 44430, loss = 0.89 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:46.923753: step 44440, loss = 0.89 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:22:47.678238: step 44450, loss = 0.69 (1696.5 examples/sec; 0.075 sec/batch)
2017-05-05 18:22:48.439872: step 44460, loss = 0.72 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:49.212530: step 44470, loss = 0.84 (1656.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:22:49.972845: step 44480, loss = 0.91 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:50.738617: step 44490, loss = 0.72 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:22:51.492064: step 44500, loss = 0.71 (1698.8 examples/sec; 0.075 sec/batch)
2017-05-05 18:22:52.254075: step 44510, loss = 1.00 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:53.015338: step 44520, loss = 0.68 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:53.778665: step 44530, loss = 0.68 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:54.545081: step 44540, loss = 1.14 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:22:55.414325: step 44550, loss = 0.66 (1472.5 examples/sec; 0.087 sec/batch)
2017-05-05 18:22:56.067473: step 44560, loss = 0.94 (1959.7 examples/sec; 0.065 sec/batch)
2017-05-05 18:22:56.833630: step 44570, loss = 0.74 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:22:57.594807: step 44580, loss = 0.73 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:58.356700: step 44590, loss = 0.64 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:59.116803: step 44600, loss = 0.89 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:22:59.875218: step 44610, loss = 0.83 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:00.646852: step 44620, loss = 0.73 (1658.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:23:01.408422: step 44630, loss = 0.80 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:02.170269: step 44640, loss = 0.89 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:02.937803: step 44650, loss = 0.75 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:23:03.696578: step 44660, loss = 1.04 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:04.466551: step 44670, loss = 0.80 (1662.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:23:05.228286: step 44680, loss = 0.89 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:05.994115: step 44690, loss = 0.80 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:23:06.763550: step 44700, loss = 0.78 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:23:07.527814: step 44710, loss = 0.83 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:08.290984: step 44720, loss = 0.72 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:09.056503: step 44730, loss = 0.86 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:23:09.820708: step 44740, loss = 0.77 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:10.590631: step 44750, loss = 0.64 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:23:11.348768: step 44760, loss = 0.78 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:12.105514: step 44770, loss = 0.76 (1691.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:12.869285: step 44780, loss = 0.65 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:13.636038: step 44790, loss = 1.03 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:23:14.408418: step 44800, loss = 0.95 (1657.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:23:15.177953: step 44810, loss = 0.72 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:23:15.927081: step 44820, loss = 0.79 (1708.7 examples/sec; 0.075 sec/batch)
2017-05-05 18:23:16.693639: step 44830, loss = 0.81 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:23:17.459429: step 44840, loss = 0.79 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:23:18.224387: step 44850, loss = 0.79 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:18.990926: step 44860, loss = 0.91 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:23:19.743019: step 44870, loss = 0.71 (1701.9 examples/sec; 0.075 sec/batch)
2017-05-05 18:23:20.503915: step 44880, loss = 0.82 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:21.267884: step 44890, loss = 0.94 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:22.028623: step 44900, loss = 0.81 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:22.791600: step 44910, loss = 0.84 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:23.554986: step 44920, loss = 0.66 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:24.315959: step 44930, loss = 0.66 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:25.081389: step 44940, loss = 0.65 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:23:25.845708: step 44950, loss = 0.88 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:26.607682: step 44960, loss = 0.81 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:27.367886: step 44970, loss = 0.80 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:28.125436: step 44980, loss = 0.83 (1689.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:28.889308: step 44990, loss = 0.82 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:29.651996: step 45000, loss = 0.71 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:30.418597: step 45010, loss = 0.76 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:23:31.181881: step 45020, loss = 0.79 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:31.936676: step 45030, loss = 0.76 (1695.8 examples/sec; 0.075 sec/batch)
2017-05-05 18:23:32.699293: step 45040, loss = 0.78 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:33.481485: step 45050, loss = 0.90 (1636.4 examples/sec; 0.078 sec/batch)
2017-05-05 18:23:34.237719: step 45060, loss = 0.66 (1692.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:35.016092: step 45070, loss = 0.87 (1644.4 examples/sec; 0.078 sec/batch)
2017-05-05 18:23:35.768610: step 45080, loss = 0.79 (1701.0 examples/sec; 0.075 sec/batch)
2017-05-05 18:23:36.541260: step 45090, loss = 0.78 (1656.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:23:37.305851: step 45100, loss = 0.86 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:38.066870: step 45110, loss = 0.87 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:38.837602: step 45120, loss = 0.87 (1660.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:23:39.594675: step 45130, loss = 0.78 (1690.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:40.358444: step 45140, loss = 0.94 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:41.130133: step 45150, loss = 0.81 (1658.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:23:41.892756: step 45160, loss = 0.83 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:42.655972: step 45170, loss = 0.80 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:43.421175: step 45180, loss = 0.76 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:23:44.177842: step 45190, loss = 0.81 (1691.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:44.940201: step 45200, loss = 0.86 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:45.706863: step 45210, loss = 0.70 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:23:46.470541: step 45220, loss = 0.89 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:47.235851: step 45230, loss = 0.88 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:23:47.989804: step 45240, loss = 0.86 (1697.7 examples/sec; 0.075 sec/batch)
2017-05-05 18:23:48.759510: step 45250, loss = 0.80 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:23:49.523404: step 45260, loss = 0.71 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:50.293657: step 45270, loss = 0.70 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:23:51.057898: step 45280, loss = 0.84 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:51.817656: step 45290, loss = 0.74 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:52.580697: step 45300, loss = 0.89 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:53.344830: step 45310, loss = 0.61 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:54.105379: step 45320, loss = 0.72 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:54.871598: step 45330, loss = 0.86 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:23:55.631117: step 45340, loss = 0.66 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:56.400548: step 45350, loss = 0.88 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:23:57.168513: step 45360, loss = 0.78 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:23:57.930254: step 45370, loss = 0.64 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:58.693210: step 45380, loss = 0.85 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:23:59.454863: step 45390, loss = 0.84 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:00.214937: step 45400, loss = 0.80 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:00.980121: step 45410, loss = 0.88 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:24:01.739442: step 45420, loss = 0.87 (1685.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:02.509662: step 45430, loss = 0.81 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:24:03.271238: step 45440, loss = 0.85 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:04.024235: step 45450, loss = 0.63 (1699.9 examples/sec; 0.075 sec/batch)
2017-05-05 18:24:04.788890: step 45460, loss = 0.78 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:05.555570: step 45470, loss = 0.67 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:24:06.320055: step 45480, loss = 0.85 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:07.085491: step 45490, loss = 0.76 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:24:07.840781: step 45500, loss = 0.76 (1694.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:08.606582: step 45510, loss = 0.81 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:24:09.369591: step 45520, loss = 0.86 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:10.137737: step 45530, loss = 0.82 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:24:10.996146: step 45540, loss = 0.76 (1491.1 examples/sec; 0.086 sec/batch)
2017-05-05 18:24:11.657377: step 45550, loss = 0.68 (1935.8 examples/sec; 0.066 sec/batch)
2017-05-05 18:24:12.422447: step 45560, loss = 0.77 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:24:13.195838: step 45570, loss = 0.99 (1655.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:24:13.966522: step 45580, loss = 0.72 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:24:14.730814: step 45590, loss = 0.79 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:15.489418: step 45600, loss = 0.88 (1687.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:16.253609: step 45610, loss = 0.71 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:17.012731: step 45620, loss = 0.74 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:17.776163: step 45630, loss = 0.85 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:18.538696: step 45640, loss = 0.73 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:19.305149: step 45650, loss = 0.86 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:24:20.058019: step 45660, loss = 0.89 (1700.2 examples/sec; 0.075 sec/batch)
2017-05-05 18:24:20.819415: step 45670, loss = 0.89 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:21.587661: step 45680, loss = 0.79 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:24:22.349866: step 45690, loss = 0.71 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:23.112086: step 45700, loss = 0.96 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:23.866342: step 45710, loss = 0.89 (1697.0 examples/sec; 0.075 sec/batch)
2017-05-05 18:24:24.633770: step 45720, loss = 0.92 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:24:25.401181: step 45730, loss = 0.91 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:24:26.167732: step 45740, loss = 0.99 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:24:26.933981: step 45750, loss = 0.94 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:24:27.679327: step 45760, loss = 0.99 (1717.3 examples/sec; 0.075 sec/batch)
2017-05-05 18:24:28.444270: step 45770, loss = 0.71 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:29.211334: step 45780, loss = 0.88 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:24:29.974657: step 45790, loss = 0.72 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:30.753395: step 45800, loss = 0.71 (1643.7 examples/sec; 0.078 sec/batch)
2017-05-05 18:24:31.512051: step 45810, loss = 0.79 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:32.276612: step 45820, loss = 0.76 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:33.039622: step 45830, loss = 0.87 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:33.806766: step 45840, loss = 0.77 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:24:34.576460: step 45850, loss = 0.75 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:24:35.337307: step 45860, loss = 1.03 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:36.096677: step 45870, loss = 0.80 (1685.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:36.868220: step 45880, loss = 0.84 (1659.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:24:37.627131: step 45890, loss = 0.84 (1686.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:38.403460: step 45900, loss = 0.78 (1648.8 examples/sec; 0.078 sec/batch)
2017-05-05 18:24:39.175091: step 45910, loss = 0.68 (1658.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:24:39.933801: step 45920, loss = 0.83 (1687.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:40.701518: step 45930, loss = 0.68 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:24:41.469576: step 45940, loss = 0.81 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:24:42.234735: step 45950, loss = 0.68 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:24:43.002866: step 45960, loss = 0.86 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:24:43.757112: step 45970, loss = 0.68 (1697.1 examples/sec; 0.075 sec/batch)
2017-05-05 18:24:44.522256: step 45980, loss = 0.84 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:24:45.288659: step 45990, loss = 0.73 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:24:46.048467: step 46000, loss = 0.80 (1684.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:46.811151: step 46010, loss = 0.88 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:47.570674: step 46020, loss = 0.78 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:48.332542: step 46030, loss = 0.88 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:49.103449: step 46040, loss = 0.71 (1660.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:24:49.865483: step 46050, loss = 0.88 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:50.627583: step 46060, loss = 0.65 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:51.386315: step 46070, loss = 0.88 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:52.143266: step 46080, loss = 0.73 (1691.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:52.914776: step 46090, loss = 0.76 (1659.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:24:53.675689: step 46100, loss = 0.62 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:54.443802: step 46110, loss = 0.69 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:24:55.209121: step 46120, loss = 0.88 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:24:55.960244: step 46130, loss = 0.88 (1704.1 examples/sec; 0.075 sec/batch)
2017-05-05 18:24:56.732855: step 46140, loss = 0.92 (1656.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:24:57.500857: step 46150, loss = 0.80 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:24:58.262808: step 46160, loss = 0.73 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:59.024905: step 46170, loss = 0.92 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:24:59.779879: step 46180, loss = 0.81 (1695.4 examples/sec; 0.075 sec/batch)
2017-05-05 18:25:00.546573: step 46190, loss = 0.87 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:01.314293: step 46200, loss = 0.82 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:02.103419: step 46210, loss = 0.72 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-05 18:25:02.862318: step 46220, loss = 0.71 (1686.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:25:03.614889: step 46230, loss = 0.72 (1700.8 examples/sec; 0.075 sec/batch)
2017-05-05 18:25:04.379693: step 46240, loss = 0.74 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:25:05.144786: step 46250, loss = 0.73 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:05.911867: step 46260, loss = 0.85 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:06.673820: step 46270, loss = 0.98 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:25:07.433423: step 46280, loss = 0.84 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:25:08.192762: step 46290, loss = 0.76 (1685.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:25:08.958451: step 46300, loss = 0.84 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:09.724919: step 46310, loss = 0.84 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:10.489707: step 46320, loss = 0.76 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:25:11.257540: step 46330, loss = 0.77 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:12.011774: step 46340, loss = 0.91 (1697.1 examples/sec; 0.075 sec/batch)
2017-05-05 18:25:12.780805: step 46350, loss = 0.79 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:13.551181: step 46360, loss = 0.78 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:14.319349: step 46370, loss = 0.77 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:15.087756: step 46380, loss = 0.78 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:15.848983: step 46390, loss = 0.69 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:25:16.618549: step 46400, loss = 0.83 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:17.392669: step 46410, loss = 0.74 (1653.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:18.164453: step 46420, loss = 0.86 (1658.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:18.928609: step 46430, loss = 0.76 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:25:19.690610: step 46440, loss = 0.63 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:25:20.460721: step 46450, loss = 0.79 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:21.230212: step 46460, loss = 0.76 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:21.996323: step 46470, loss = 0.83 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:22.757628: step 46480, loss = 0.71 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:25:23.512917: step 46490, loss = 0.87 (1694.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:25:24.278383: step 46500, loss = 0.75 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:25.037474: step 46510, loss = 0.74 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:25:25.799120: step 46520, loss = 0.85 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:25:26.663775: step 46530, loss = 0.76 (1480.4 examples/sec; 0.086 sec/batch)
2017-05-05 18:25:27.333171: step 46540, loss = 0.76 (1912.2 examples/sec; 0.067 sec/batch)
2017-05-05 18:25:28.083977: step 46550, loss = 0.82 (1704.8 examples/sec; 0.075 sec/batch)
2017-05-05 18:25:28.851004: step 46560, loss = 0.81 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:29.613413: step 46570, loss = 0.87 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:25:30.383185: step 46580, loss = 0.74 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:31.147797: step 46590, loss = 0.70 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:25:31.905014: step 46600, loss = 0.77 (1690.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:25:32.663511: step 46610, loss = 0.84 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:25:33.440190: step 46620, loss = 0.65 (1648.0 examples/sec; 0.078 sec/batch)
2017-05-05 18:25:34.210056: step 46630, loss = 0.81 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:34.977590: step 46640, loss = 0.96 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:35.733798: step 46650, loss = 0.76 (1692.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:25:36.499018: step 46660, loss = 0.76 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:37.262405: step 46670, loss = 0.69 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:25:38.028895: step 46680, loss = 0.98 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:38.797190: step 46690, loss = 0.81 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:39.559174: step 46700, loss = 0.92 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:25:40.317440: step 46710, loss = 0.77 (1688.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:25:41.081522: step 46720, loss = 0.75 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:25:41.849368: step 46730, loss = 0.88 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:42.617237: step 46740, loss = 0.79 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:43.381728: step 46750, loss = 0.79 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:25:44.143608: step 46760, loss = 0.92 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:25:44.908019: step 46770, loss = 0.75 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:25:45.673181: step 46780, loss = 0.83 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:46.437608: step 46790, loss = 0.83 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:25:47.202691: step 46800, loss = 0.87 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:47.957015: step 46810, loss = 0.80 (1696.9 examples/sec; 0.075 sec/batch)
2017-05-05 18:25:48.723464: step 46820, loss = 0.85 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:49.486714: step 46830, loss = 0.84 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:25:50.264688: step 46840, loss = 0.73 (1645.3 examples/sec; 0.078 sec/batch)
2017-05-05 18:25:51.025456: step 46850, loss = 0.89 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:25:51.778136: step 46860, loss = 0.75 (1700.6 examples/sec; 0.075 sec/batch)
2017-05-05 18:25:52.539309: step 46870, loss = 0.97 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:25:53.305216: step 46880, loss = 0.81 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:54.073308: step 46890, loss = 0.80 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:54.843628: step 46900, loss = 0.77 (1661.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:55.599814: step 46910, loss = 0.67 (1692.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:25:56.362940: step 46920, loss = 0.77 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:25:57.130847: step 46930, loss = 0.73 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:57.897775: step 46940, loss = 0.86 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:25:58.657545: step 46950, loss = 0.65 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:25:59.423056: step 46960, loss = 0.72 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:00.192786: step 46970, loss = 0.64 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:00.956251: step 46980, loss = 0.72 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:01.713159: step 46990, loss = 0.67 (1691.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:02.477612: step 47000, loss = 0.82 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:03.243631: step 47010, loss = 0.73 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:04.001989: step 47020, loss = 0.67 (1687.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:04.771483: step 47030, loss = 0.73 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:05.565537: step 47040, loss = 0.88 (1612.0 examples/sec; 0.079 sec/batch)
2017-05-05 18:26:06.333494: step 47050, loss = 0.75 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:07.099468: step 47060, loss = 0.81 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:07.852728: step 47070, loss = 0.82 (1699.3 examples/sec; 0.075 sec/batch)
2017-05-05 18:26:08.618652: step 47080, loss = 0.72 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:09.386011: step 47090, loss = 0.96 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:10.154803: step 47100, loss = 0.89 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:10.919269: step 47110, loss = 0.78 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:11.679934: step 47120, loss = 0.72 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:12.454149: step 47130, loss = 0.90 (1653.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:13.217729: step 47140, loss = 0.81 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:13.979478: step 47150, loss = 0.85 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:14.749224: step 47160, loss = 0.76 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:15.508950: step 47170, loss = 0.80 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:16.264560: step 47180, loss = 0.87 (1694.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:17.024413: step 47190, loss = 0.68 (1684.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:17.792620: step 47200, loss = 0.75 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:18.564098: step 47210, loss = 0.87 (1659.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:19.327471: step 47220, loss = 0.94 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:20.080595: step 47230, loss = 0.68 (1699.6 examples/sec; 0.075 sec/batch)
2017-05-05 18:26:20.842794: step 47240, loss = 0.81 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:21.604266: step 47250, loss = 0.76 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:22.375799: step 47260, loss = 0.87 (1659.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:23.139734: step 47270, loss = 0.90 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:23.892670: step 47280, loss = 0.97 (1700.0 examples/sec; 0.075 sec/batch)
2017-05-05 18:26:24.654152: step 47290, loss = 0.81 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:25.416564: step 47300, loss = 0.90 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:26.178732: step 47310, loss = 0.71 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:26.947507: step 47320, loss = 0.86 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:27.701622: step 47330, loss = 0.82 (1697.4 examples/sec; 0.075 sec/batch)
2017-05-05 18:26:28.466520: step 47340, loss = 0.82 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:29.231561: step 47350, loss = 0.86 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:29.994059: step 47360, loss = 0.84 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:30.757247: step 47370, loss = 0.86 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:31.519478: step 47380, loss = 0.71 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:32.277518: step 47390, loss = 0.82 (1688.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:33.032881: step 47400, loss = 0.93 (1694.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:33.792613: step 47410, loss = 0.65 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:34.555931: step 47420, loss = 0.90 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:35.325418: step 47430, loss = 0.93 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:36.084888: step 47440, loss = 0.97 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:36.850757: step 47450, loss = 0.70 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:37.608089: step 47460, loss = 0.94 (1690.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:38.374284: step 47470, loss = 0.77 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:39.141783: step 47480, loss = 0.79 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:39.891724: step 47490, loss = 0.94 (1706.8 examples/sec; 0.075 sec/batch)
2017-05-05 18:26:40.661692: step 47500, loss = 0.74 (1662.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:41.428135: step 47510, loss = 0.78 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:42.289676: step 47520, loss = 0.83 (1485.7 examples/sec; 0.086 sec/batch)
2017-05-05 18:26:42.954499: step 47530, loss = 0.73 (1925.3 examples/sec; 0.066 sec/batch)
2017-05-05 18:26:43.704476: step 47540, loss = 0.81 (1706.7 examples/sec; 0.075 sec/batch)
2017-05-05 18:26:44.475660: step 47550, loss = 1.06 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:45.238958: step 47560, loss = 0.99 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:46.006832: step 47570, loss = 0.64 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:46.771280: step 47580, loss = 0.83 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:47.539714: step 47590, loss = 0.82 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:48.303402: step 47600, loss = 0.86 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:49.064174: step 47610, loss = 0.83 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:49.832786: step 47620, loss = 0.81 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:50.598309: step 47630, loss = 0.73 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:51.360478: step 47640, loss = 0.87 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:52.134726: step 47650, loss = 0.83 (1653.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:52.902521: step 47660, loss = 0.74 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:53.664767: step 47670, loss = 0.66 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:54.428052: step 47680, loss = 0.99 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:55.197096: step 47690, loss = 0.80 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:55.949421: step 47700, loss = 0.71 (1701.4 examples/sec; 0.075 sec/batch)
2017-05-05 18:26:56.720943: step 47710, loss = 0.86 (1659.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:57.484458: step 47720, loss = 0.78 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:26:58.253968: step 47730, loss = 0.66 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:59.024636: step 47740, loss = 0.72 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:26:59.816460: step 47750, loss = 0.72 (1616.5 examples/sec; 0.079 sec/batch)
2017-05-05 18:27:00.599928: step 47760, loss = 0.97 (1633.8 examples/sec; 0.078 sec/batch)
2017-05-05 18:27:01.373793: step 47770, loss = 0.74 (1654.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:02.139771: step 47780, loss = 0.93 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:02.903278: step 47790, loss = 0.85 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:27:03.664462: step 47800, loss = 0.84 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:27:04.427808: step 47810, loss = 0.87 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:27:05.196217: step 47820, loss = 0.71 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:05.961572: step 47830, loss = 0.76 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:06.728496: step 47840, loss = 0.84 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:07.486959: step 47850, loss = 0.77 (1687.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:27:08.249662: step 47860, loss = 0.77 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:27:09.018935: step 47870, loss = 0.78 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:09.780907: step 47880, loss = 0.82 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:27:10.548324: step 47890, loss = 0.87 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:11.315115: step 47900, loss = 0.69 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:12.075642: step 47910, loss = 0.81 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:27:12.841873: step 47920, loss = 0.70 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:13.608933: step 47930, loss = 0.95 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:14.368278: step 47940, loss = 0.78 (1685.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:27:15.137633: step 47950, loss = 0.81 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:15.891249: step 47960, loss = 0.76 (1698.5 examples/sec; 0.075 sec/batch)
2017-05-05 18:27:16.655352: step 47970, loss = 0.69 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:27:17.420311: step 47980, loss = 0.78 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:27:18.189987: step 47990, loss = 0.80 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:18.956502: step 48000, loss = 0.66 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:19.716541: step 48010, loss = 0.66 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:27:20.480326: step 48020, loss = 0.94 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:27:21.252757: step 48030, loss = 0.87 (1657.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:22.020802: step 48040, loss = 0.87 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:22.786832: step 48050, loss = 0.73 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:23.545166: step 48060, loss = 0.59 (1687.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:27:24.303554: step 48070, loss = 0.80 (1687.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:27:25.073466: step 48080, loss = 0.70 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:25.838776: step 48090, loss = 0.65 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:26.600967: step 48100, loss = 0.85 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:27:27.361239: step 48110, loss = 0.77 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:27:28.125977: step 48120, loss = 0.63 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:27:28.888054: step 48130, loss = 0.81 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:27:29.647480: step 48140, loss = 0.76 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:27:30.409774: step 48150, loss = 0.71 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:27:31.171862: step 48160, loss = 0.74 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:27:31.923738: step 48170, loss = 0.75 (1702.4 examples/sec; 0.075 sec/batch)
2017-05-05 18:27:32.690396: step 48180, loss = 0.84 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:33.457828: step 48190, loss = 0.79 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:34.225700: step 48200, loss = 0.88 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:34.997007: step 48210, loss = 0.86 (1659.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:35.754132: step 48220, loss = 0.82 (1690.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:27:36.522870: step 48230, loss = 0.80 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:37.296015: step 48240, loss = 0.86 (1655.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:38.063590: step 48250, loss = 0.81 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:38.832212: step 48260, loss = 0.86 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:39.713437: step 48270, loss = 0.85 (1452.5 examples/sec; 0.088 sec/batch)
2017-05-05 18:27:40.481637: step 48280, loss = 0.84 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:41.251940: step 48290, loss = 0.97 (1661.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:42.016289: step 48300, loss = 0.79 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:27:42.787493: step 48310, loss = 0.79 (1659.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:43.548406: step 48320, loss = 0.66 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:27:44.315601: step 48330, loss = 0.74 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:45.088039: step 48340, loss = 0.77 (1657.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:45.855224: step 48350, loss = 0.64 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:46.717194: step 48360, loss = 0.82 (1485.0 examples/sec; 0.086 sec/batch)
2017-05-05 18:27:47.479489: step 48370, loss = 0.85 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:27:48.248304: step 48380, loss = 0.76 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:49.009342: step 48390, loss = 0.73 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:27:49.777917: step 48400, loss = 0.79 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:50.550063: step 48410, loss = 0.71 (1657.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:51.314552: step 48420, loss = 0.86 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:27:52.071692: step 48430, loss = 0.80 (1690.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:27:52.839224: step 48440, loss = 0.71 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:53.605545: step 48450, loss = 0.64 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:54.380594: step 48460, loss = 0.82 (1651.5 examples/sec; 0.078 sec/batch)
2017-05-05 18:27:55.147740: step 48470, loss = 0.70 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:55.909254: step 48480, loss = 0.97 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:27:56.673791: step 48490, loss = 0.60 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:27:57.439126: step 48500, loss = 0.79 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:27:58.305996: step 48510, loss = 0.73 (1476.6 examples/sec; 0.087 sec/batch)
2017-05-05 18:27:58.975823: step 48520, loss = 0.75 (1910.9 examples/sec; 0.067 sec/batch)
2017-05-05 18:27:59.734574: step 48530, loss = 0.74 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:28:00.507274: step 48540, loss = 0.86 (1656.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:01.267333: step 48550, loss = 0.82 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:28:02.067465: step 48560, loss = 0.79 (1599.7 examples/sec; 0.080 sec/batch)
2017-05-05 18:28:02.831059: step 48570, loss = 0.79 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:28:03.594194: step 48580, loss = 0.78 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:28:04.354858: step 48590, loss = 0.81 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:28:05.124900: step 48600, loss = 0.68 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:05.893582: step 48610, loss = 0.82 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:06.664961: step 48620, loss = 0.76 (1659.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:07.422624: step 48630, loss = 0.79 (1689.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:28:08.194658: step 48640, loss = 0.72 (1658.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:08.967276: step 48650, loss = 0.93 (1656.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:09.734989: step 48660, loss = 1.04 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:10.502736: step 48670, loss = 0.73 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:11.274070: step 48680, loss = 0.89 (1659.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:12.030068: step 48690, loss = 0.77 (1693.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:28:12.802551: step 48700, loss = 0.77 (1657.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:13.577231: step 48710, loss = 0.72 (1652.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:14.341171: step 48720, loss = 0.85 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:28:15.109681: step 48730, loss = 0.77 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:15.863812: step 48740, loss = 0.81 (1697.3 examples/sec; 0.075 sec/batch)
2017-05-05 18:28:16.634674: step 48750, loss = 0.92 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:17.404015: step 48760, loss = 0.67 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:18.177080: step 48770, loss = 0.82 (1655.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:18.947291: step 48780, loss = 0.75 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:19.713896: step 48790, loss = 0.96 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:20.484422: step 48800, loss = 0.81 (1661.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:21.254239: step 48810, loss = 0.72 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:22.015582: step 48820, loss = 0.67 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:28:22.783179: step 48830, loss = 0.88 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:23.546138: step 48840, loss = 0.76 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:28:24.313631: step 48850, loss = 0.91 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:25.077734: step 48860, loss = 0.93 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:28:25.840703: step 48870, loss = 0.63 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:28:26.609378: step 48880, loss = 0.85 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:27.375832: step 48890, loss = 0.77 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:28.137407: step 48900, loss = 0.88 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:28:28.912612: step 48910, loss = 0.76 (1651.2 examples/sec; 0.078 sec/batch)
2017-05-05 18:28:29.681319: step 48920, loss = 0.87 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:30.453159: step 48930, loss = 0.67 (1658.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:31.227420: step 48940, loss = 0.88 (1653.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:31.991214: step 48950, loss = 0.87 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:28:32.758634: step 48960, loss = 0.81 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:33.519008: step 48970, loss = 0.81 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:28:34.284388: step 48980, loss = 0.80 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:35.047203: step 48990, loss = 0.77 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:28:35.807168: step 49000, loss = 0.79 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:28:36.568149: step 49010, loss = 0.82 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:28:37.340691: step 49020, loss = 0.83 (1656.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:38.106169: step 49030, loss = 0.90 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:38.877620: step 49040, loss = 0.75 (1659.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:39.635459: step 49050, loss = 0.92 (1689.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:28:40.408227: step 49060, loss = 0.78 (1656.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:41.178715: step 49070, loss = 0.73 (1661.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:41.942280: step 49080, loss = 0.77 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:28:42.712794: step 49090, loss = 0.83 (1661.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:43.468020: step 49100, loss = 0.86 (1694.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:28:44.238788: step 49110, loss = 0.83 (1660.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:45.005285: step 49120, loss = 0.70 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:45.767902: step 49130, loss = 0.90 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:28:46.531108: step 49140, loss = 0.83 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:28:47.295921: step 49150, loss = 0.78 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:28:48.054103: step 49160, loss = 0.75 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:28:48.823124: step 49170, loss = 0.79 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:49.595247: step 49180, loss = 0.82 (1657.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:50.360773: step 49190, loss = 0.89 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:51.126075: step 49200, loss = 0.82 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:51.887630: step 49210, loss = 0.92 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:28:52.657966: step 49220, loss = 0.75 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:53.423771: step 49230, loss = 0.87 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:54.192804: step 49240, loss = 0.90 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:54.958566: step 49250, loss = 1.00 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:55.717740: step 49260, loss = 0.84 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:28:56.488133: step 49270, loss = 0.88 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:28:57.252905: step 49280, loss = 0.79 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:28:58.015830: step 49290, loss = 0.68 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:28:58.780285: step 49300, loss = 0.78 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:28:59.548099: step 49310, loss = 0.70 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:29:00.309722: step 49320, loss = 0.74 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:01.076311: step 49330, loss = 0.74 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:29:01.841501: step 49340, loss = 0.73 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:29:02.606929: step 49350, loss = 0.67 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:29:03.366975: step 49360, loss = 0.89 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:04.128832: step 49370, loss = 0.73 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:04.891053: step 49380, loss = 0.73 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:05.655058: step 49390, loss = 0.97 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:06.422248: step 49400, loss = 0.93 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:29:07.181802: step 49410, loss = 0.97 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:07.932914: step 49420, loss = 0.74 (1704.1 examples/sec; 0.075 sec/batch)
2017-05-05 18:29:08.691461: step 49430, loss = 0.81 (1687.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:09.460052: step 49440, loss = 0.87 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:29:10.222200: step 49450, loss = 0.77 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:10.990313: step 49460, loss = 0.97 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:29:11.740851: step 49470, loss = 0.78 (1705.5 examples/sec; 0.075 sec/batch)
2017-05-05 18:29:12.497950: step 49480, loss = 0.77 (1690.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:13.261080: step 49490, loss = 0.83 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:14.138167: step 49500, loss = 0.87 (1459.4 examples/sec; 0.088 sec/batch)
2017-05-05 18:29:14.812529: step 49510, loss = 0.93 (1898.1 examples/sec; 0.067 sec/batch)
2017-05-05 18:29:15.571396: step 49520, loss = 0.77 (1686.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:16.332943: step 49530, loss = 0.82 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:17.097216: step 49540, loss = 0.87 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:17.860942: step 49550, loss = 0.78 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:18.625368: step 49560, loss = 0.69 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:19.389821: step 49570, loss = 0.78 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:20.154856: step 49580, loss = 0.70 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:29:20.918039: step 49590, loss = 0.84 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:21.685793: step 49600, loss = 0.78 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:29:22.447950: step 49610, loss = 0.73 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:23.213085: step 49620, loss = 0.97 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:29:23.967967: step 49630, loss = 0.77 (1695.6 examples/sec; 0.075 sec/batch)
2017-05-05 18:29:24.734809: step 49640, loss = 0.87 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:29:25.501387: step 49650, loss = 0.78 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:29:26.266269: step 49660, loss = 0.85 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:27.028761: step 49670, loss = 0.73 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:27.784663: step 49680, loss = 0.75 (1693.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:28.551593: step 49690, loss = 0.79 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:29:29.317991: step 49700, loss = 1.07 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:29:30.083566: step 49710, loss = 0.96 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:29:30.853874: step 49720, loss = 0.83 (1661.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:29:31.617778: step 49730, loss = 0.92 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:32.371569: step 49740, loss = 0.65 (1698.1 examples/sec; 0.075 sec/batch)
2017-05-05 18:29:33.135311: step 49750, loss = 0.87 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:33.900088: step 49760, loss = 0.88 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:34.667149: step 49770, loss = 0.81 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:29:35.421790: step 49780, loss = 0.85 (1696.2 examples/sec; 0.075 sec/batch)
2017-05-05 18:29:36.189452: step 49790, loss = 1.01 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:29:36.951411: step 49800, loss = 0.83 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:37.709841: step 49810, loss = 0.83 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:38.473660: step 49820, loss = 0.67 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:39.240106: step 49830, loss = 0.70 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:29:39.996520: step 49840, loss = 0.80 (1692.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:40.756684: step 49850, loss = 0.71 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:41.522515: step 49860, loss = 0.75 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:29:42.283315: step 49870, loss = 0.95 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:43.051878: step 49880, loss = 0.81 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:29:43.802068: step 49890, loss = 0.73 (1706.2 examples/sec; 0.075 sec/batch)
2017-05-05 18:29:44.570796: step 49900, loss = 0.87 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:29:45.336872: step 49910, loss = 0.80 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:29:46.094805: step 49920, loss = 0.78 (1688.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:46.861832: step 49930, loss = 0.67 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:29:47.611754: step 49940, loss = 0.70 (1706.8 examples/sec; 0.075 sec/batch)
2017-05-05 18:29:48.376660: step 49950, loss = 0.72 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:49.139483: step 49960, loss = 0.86 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:49.899150: step 49970, loss = 0.80 (1684.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:50.666861: step 49980, loss = 0.69 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:29:51.433526: step 49990, loss = 0.89 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:29:52.191962: step 50000, loss = 0.78 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:52.957754: step 50010, loss = 0.80 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:29:53.722378: step 50020, loss = 0.74 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:54.493911: step 50030, loss = 1.00 (1659.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:29:55.255693: step 50040, loss = 0.86 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:56.011160: step 50050, loss = 0.74 (1694.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:56.769353: step 50060, loss = 0.82 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:57.541334: step 50070, loss = 0.69 (1658.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:29:58.305183: step 50080, loss = 0.75 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:29:59.071203: step 50090, loss = 0.69 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:29:59.838199: step 50100, loss = 1.02 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:00.602041: step 50110, loss = 0.71 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:01.362025: step 50120, loss = 0.71 (1684.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:02.121571: step 50130, loss = 0.69 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:02.896824: step 50140, loss = 0.66 (1651.1 examples/sec; 0.078 sec/batch)
2017-05-05 18:30:03.660893: step 50150, loss = 0.87 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:04.427042: step 50160, loss = 0.82 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:05.193769: step 50170, loss = 0.92 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:05.956446: step 50180, loss = 0.79 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:06.725058: step 50190, loss = 0.79 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:07.482941: step 50200, loss = 0.70 (1688.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:08.243402: step 50210, loss = 0.87 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:09.008756: step 50220, loss = 0.74 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:09.771730: step 50230, loss = 0.75 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:10.532256: step 50240, loss = 0.69 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:11.297922: step 50250, loss = 0.81 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:12.055118: step 50260, loss = 0.94 (1690.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:12.821144: step 50270, loss = 0.79 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:13.590284: step 50280, loss = 0.71 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:14.357498: step 50290, loss = 0.76 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:15.122108: step 50300, loss = 0.82 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:15.881418: step 50310, loss = 0.91 (1685.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:16.646372: step 50320, loss = 0.83 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:17.409054: step 50330, loss = 0.76 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:18.176997: step 50340, loss = 0.79 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:18.941351: step 50350, loss = 0.77 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:19.696433: step 50360, loss = 0.85 (1695.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:20.463610: step 50370, loss = 0.83 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:21.225490: step 50380, loss = 0.69 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:21.986605: step 50390, loss = 0.80 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:22.747856: step 50400, loss = 0.71 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:23.512384: step 50410, loss = 0.88 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:24.275497: step 50420, loss = 0.87 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:25.036168: step 50430, loss = 0.76 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:25.799247: step 50440, loss = 0.92 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:26.567527: step 50450, loss = 0.74 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:27.329132: step 50460, loss = 0.83 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:28.085700: step 50470, loss = 0.85 (1691.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:28.853078: step 50480, loss = 0.83 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:29.714723: step 50490, loss = 0.87 (1485.5 examples/sec; 0.086 sec/batch)
2017-05-05 18:30:30.384520: step 50500, loss = 0.82 (1911.0 examples/sec; 0.067 sec/batch)
2017-05-05 18:30:31.150134: step 50510, loss = 1.08 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:31.901627: step 50520, loss = 0.86 (1703.3 examples/sec; 0.075 sec/batch)
2017-05-05 18:30:32.664705: step 50530, loss = 0.87 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:33.433329: step 50540, loss = 0.84 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:34.199401: step 50550, loss = 0.89 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:34.961415: step 50560, loss = 0.68 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:35.719229: step 50570, loss = 0.74 (1688.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:36.485028: step 50580, loss = 0.92 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:37.256416: step 50590, loss = 0.82 (1659.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:38.018714: step 50600, loss = 0.69 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:38.783794: step 50610, loss = 0.84 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:39.555400: step 50620, loss = 0.83 (1658.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:40.322633: step 50630, loss = 0.74 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:41.084982: step 50640, loss = 0.86 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:41.849510: step 50650, loss = 0.77 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:42.611306: step 50660, loss = 0.74 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:43.376603: step 50670, loss = 0.74 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:44.132400: step 50680, loss = 0.80 (1693.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:44.898745: step 50690, loss = 0.82 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:45.665340: step 50700, loss = 0.97 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:46.443098: step 50710, loss = 0.58 (1645.8 examples/sec; 0.078 sec/batch)
2017-05-05 18:30:47.206854: step 50720, loss = 0.61 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:47.961241: step 50730, loss = 0.76 (1696.7 examples/sec; 0.075 sec/batch)
2017-05-05 18:30:48.721210: step 50740, loss = 0.85 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:49.487354: step 50750, loss = 0.92 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:50.253935: step 50760, loss = 0.83 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:51.026262: step 50770, loss = 0.94 (1657.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:51.787644: step 50780, loss = 0.89 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:52.557265: step 50790, loss = 0.91 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:53.329884: step 50800, loss = 0.75 (1656.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:54.093504: step 50810, loss = 0.80 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:54.867844: step 50820, loss = 0.90 (1653.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:55.627174: step 50830, loss = 0.72 (1685.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:56.393091: step 50840, loss = 0.80 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:57.161428: step 50850, loss = 0.82 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:57.925181: step 50860, loss = 0.61 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:30:58.691251: step 50870, loss = 0.61 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:30:59.454436: step 50880, loss = 0.79 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:00.227048: step 50890, loss = 0.78 (1656.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:31:00.997114: step 50900, loss = 0.73 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:31:01.766402: step 50910, loss = 0.79 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:31:02.527128: step 50920, loss = 0.83 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:03.291030: step 50930, loss = 0.85 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:04.062203: step 50940, loss = 0.91 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:31:04.827276: step 50950, loss = 0.87 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:31:05.595575: step 50960, loss = 0.90 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:31:06.362392: step 50970, loss = 0.88 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:31:07.125343: step 50980, loss = 0.95 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:07.890578: step 50990, loss = 0.83 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:31:08.656302: step 51000, loss = 0.84 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:31:09.416113: step 51010, loss = 0.83 (1684.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:10.185156: step 51020, loss = 0.67 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:31:10.947882: step 51030, loss = 0.88 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:11.702247: step 51040, loss = 0.68 (1696.8 examples/sec; 0.075 sec/batch)
2017-05-05 18:31:12.465748: step 51050, loss = 0.70 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:13.235313: step 51060, loss = 0.74 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:31:13.995275: step 51070, loss = 0.83 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:14.764711: step 51080, loss = 0.69 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:31:15.527059: step 51090, loss = 0.86 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:16.289186: step 51100, loss = 0.82 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:17.055210: step 51110, loss = 1.05 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:31:17.822614: step 51120, loss = 0.79 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:31:18.590594: step 51130, loss = 0.84 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:31:19.354200: step 51140, loss = 0.77 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:20.113912: step 51150, loss = 0.68 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:20.872942: step 51160, loss = 0.65 (1686.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:21.636842: step 51170, loss = 0.84 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:22.400103: step 51180, loss = 0.83 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:23.165183: step 51190, loss = 0.69 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:31:23.920636: step 51200, loss = 0.81 (1694.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:24.690978: step 51210, loss = 0.84 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:31:25.461264: step 51220, loss = 0.79 (1661.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:31:26.228979: step 51230, loss = 0.67 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:31:26.992216: step 51240, loss = 0.81 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:27.747080: step 51250, loss = 0.72 (1695.7 examples/sec; 0.075 sec/batch)
2017-05-05 18:31:28.506682: step 51260, loss = 1.10 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:29.271354: step 51270, loss = 0.88 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:30.029562: step 51280, loss = 1.01 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:30.803672: step 51290, loss = 0.78 (1653.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:31:31.556610: step 51300, loss = 0.77 (1700.0 examples/sec; 0.075 sec/batch)
2017-05-05 18:31:32.314912: step 51310, loss = 0.77 (1688.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:33.083825: step 51320, loss = 0.74 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:31:33.842878: step 51330, loss = 0.65 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:34.609109: step 51340, loss = 0.82 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:31:35.370423: step 51350, loss = 0.75 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:36.128215: step 51360, loss = 0.85 (1689.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:36.894514: step 51370, loss = 0.79 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:31:37.655170: step 51380, loss = 0.76 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:38.420441: step 51390, loss = 0.83 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:31:39.187341: step 51400, loss = 0.67 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:31:39.938429: step 51410, loss = 0.77 (1704.2 examples/sec; 0.075 sec/batch)
2017-05-05 18:31:40.702924: step 51420, loss = 0.78 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:41.467543: step 51430, loss = 0.74 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:42.231167: step 51440, loss = 0.79 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:42.989474: step 51450, loss = 0.64 (1688.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:43.743572: step 51460, loss = 0.80 (1697.4 examples/sec; 0.075 sec/batch)
2017-05-05 18:31:44.513685: step 51470, loss = 0.65 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:31:45.407247: step 51480, loss = 0.83 (1432.5 examples/sec; 0.089 sec/batch)
2017-05-05 18:31:46.048512: step 51490, loss = 0.83 (1996.1 examples/sec; 0.064 sec/batch)
2017-05-05 18:31:46.812898: step 51500, loss = 0.65 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:47.567999: step 51510, loss = 0.83 (1695.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:48.326634: step 51520, loss = 0.81 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:49.090265: step 51530, loss = 0.63 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:49.857720: step 51540, loss = 1.00 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:31:50.625731: step 51550, loss = 1.08 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:31:51.384250: step 51560, loss = 0.59 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:52.147239: step 51570, loss = 0.57 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:52.911478: step 51580, loss = 0.86 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:53.677537: step 51590, loss = 0.98 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:31:54.440933: step 51600, loss = 0.73 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:55.202297: step 51610, loss = 0.65 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:55.959413: step 51620, loss = 0.80 (1690.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:56.732014: step 51630, loss = 0.82 (1656.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:31:57.481998: step 51640, loss = 0.85 (1706.7 examples/sec; 0.075 sec/batch)
2017-05-05 18:31:58.246043: step 51650, loss = 0.62 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:31:59.013844: step 51660, loss = 0.83 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:31:59.768862: step 51670, loss = 0.75 (1695.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:00.543487: step 51680, loss = 0.77 (1652.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:32:01.295050: step 51690, loss = 0.78 (1703.1 examples/sec; 0.075 sec/batch)
2017-05-05 18:32:02.058967: step 51700, loss = 0.72 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:02.817661: step 51710, loss = 0.69 (1687.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:03.571249: step 51720, loss = 0.89 (1698.5 examples/sec; 0.075 sec/batch)
2017-05-05 18:32:04.338781: step 51730, loss = 0.98 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:32:05.102221: step 51740, loss = 0.79 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:05.866140: step 51750, loss = 0.76 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:06.631602: step 51760, loss = 0.70 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:32:07.395988: step 51770, loss = 1.04 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:08.158302: step 51780, loss = 0.82 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:08.920035: step 51790, loss = 0.95 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:09.686548: step 51800, loss = 0.69 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:32:10.459087: step 51810, loss = 0.79 (1656.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:32:11.222777: step 51820, loss = 0.90 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:11.978319: step 51830, loss = 0.86 (1694.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:12.739490: step 51840, loss = 0.78 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:13.501829: step 51850, loss = 0.63 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:14.268182: step 51860, loss = 0.72 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:32:15.032010: step 51870, loss = 0.67 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:15.794471: step 51880, loss = 0.76 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:16.560376: step 51890, loss = 0.74 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:32:17.325427: step 51900, loss = 0.74 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:32:18.088243: step 51910, loss = 0.90 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:18.852543: step 51920, loss = 0.73 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:19.611062: step 51930, loss = 0.77 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:20.380055: step 51940, loss = 0.78 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:32:21.148302: step 51950, loss = 0.66 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:32:21.909277: step 51960, loss = 0.63 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:22.672209: step 51970, loss = 0.83 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:23.435280: step 51980, loss = 0.86 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:24.196820: step 51990, loss = 0.82 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:24.961765: step 52000, loss = 0.80 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:25.724157: step 52010, loss = 0.79 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:26.491674: step 52020, loss = 0.87 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:32:27.258263: step 52030, loss = 0.77 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:32:28.018951: step 52040, loss = 0.89 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:28.780978: step 52050, loss = 0.75 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:29.548097: step 52060, loss = 0.80 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:32:30.315014: step 52070, loss = 0.78 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:32:31.085546: step 52080, loss = 0.80 (1661.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:32:31.840054: step 52090, loss = 0.67 (1696.5 examples/sec; 0.075 sec/batch)
2017-05-05 18:32:32.598767: step 52100, loss = 0.77 (1687.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:33.369240: step 52110, loss = 0.90 (1661.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:32:34.135119: step 52120, loss = 0.76 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:32:34.903343: step 52130, loss = 0.91 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:32:35.652045: step 52140, loss = 0.87 (1709.6 examples/sec; 0.075 sec/batch)
2017-05-05 18:32:36.415767: step 52150, loss = 0.82 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:37.178789: step 52160, loss = 0.87 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:37.942259: step 52170, loss = 0.95 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:38.708193: step 52180, loss = 0.78 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:32:39.467110: step 52190, loss = 0.88 (1687.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:40.224340: step 52200, loss = 1.05 (1689.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:40.989884: step 52210, loss = 0.68 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:32:41.746341: step 52220, loss = 0.94 (1692.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:42.506847: step 52230, loss = 0.82 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:43.272909: step 52240, loss = 0.93 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:32:44.029727: step 52250, loss = 0.93 (1691.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:44.795342: step 52260, loss = 0.70 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:32:45.560219: step 52270, loss = 0.77 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:46.328897: step 52280, loss = 0.86 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:32:47.102150: step 52290, loss = 0.71 (1655.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:32:47.854732: step 52300, loss = 0.83 (1700.8 examples/sec; 0.075 sec/batch)
2017-05-05 18:32:48.614177: step 52310, loss = 0.97 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:49.378095: step 52320, loss = 0.90 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:50.146559: step 52330, loss = 0.85 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:32:50.905342: step 52340, loss = 0.78 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:51.664403: step 52350, loss = 0.73 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:52.426261: step 52360, loss = 0.78 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:53.192437: step 52370, loss = 0.66 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:32:53.957240: step 52380, loss = 0.93 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:54.721258: step 52390, loss = 0.75 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:55.483584: step 52400, loss = 0.77 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:56.248471: step 52410, loss = 0.84 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:57.020313: step 52420, loss = 0.80 (1658.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:32:57.780723: step 52430, loss = 0.90 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:32:58.548625: step 52440, loss = 0.87 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:32:59.318972: step 52450, loss = 0.61 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:00.077595: step 52460, loss = 0.76 (1687.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:33:00.939284: step 52470, loss = 0.80 (1485.4 examples/sec; 0.086 sec/batch)
2017-05-05 18:33:01.611398: step 52480, loss = 0.67 (1904.4 examples/sec; 0.067 sec/batch)
2017-05-05 18:33:02.374006: step 52490, loss = 0.80 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:33:03.133012: step 52500, loss = 0.84 (1686.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:33:03.887632: step 52510, loss = 0.80 (1696.2 examples/sec; 0.075 sec/batch)
2017-05-05 18:33:04.656660: step 52520, loss = 0.74 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:05.420539: step 52530, loss = 0.71 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:33:06.187007: step 52540, loss = 0.82 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:06.956501: step 52550, loss = 0.79 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:07.711476: step 52560, loss = 0.74 (1695.4 examples/sec; 0.075 sec/batch)
2017-05-05 18:33:08.485350: step 52570, loss = 0.90 (1654.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:09.249848: step 52580, loss = 0.73 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:33:10.016022: step 52590, loss = 0.94 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:10.787386: step 52600, loss = 0.85 (1659.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:11.545262: step 52610, loss = 0.70 (1688.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:33:12.303327: step 52620, loss = 0.76 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:33:13.075139: step 52630, loss = 0.78 (1658.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:13.833260: step 52640, loss = 0.79 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:33:14.602489: step 52650, loss = 0.92 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:15.369694: step 52660, loss = 0.66 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:16.129978: step 52670, loss = 0.75 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:33:16.894291: step 52680, loss = 0.74 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:33:17.660634: step 52690, loss = 0.77 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:18.431263: step 52700, loss = 0.83 (1661.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:19.199368: step 52710, loss = 0.93 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:19.953356: step 52720, loss = 0.90 (1697.6 examples/sec; 0.075 sec/batch)
2017-05-05 18:33:20.721237: step 52730, loss = 0.67 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:21.487581: step 52740, loss = 0.79 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:22.252348: step 52750, loss = 0.74 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:33:23.016869: step 52760, loss = 0.66 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:33:23.776148: step 52770, loss = 0.74 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:33:24.539682: step 52780, loss = 0.77 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:33:25.307981: step 52790, loss = 0.78 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:26.069865: step 52800, loss = 0.80 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:33:26.838812: step 52810, loss = 0.67 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:27.596228: step 52820, loss = 0.78 (1690.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:33:28.359948: step 52830, loss = 0.91 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:33:29.125438: step 52840, loss = 0.80 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:29.891097: step 52850, loss = 0.77 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:30.657409: step 52860, loss = 0.85 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:31.426196: step 52870, loss = 0.74 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:32.185341: step 52880, loss = 0.88 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:33:32.945047: step 52890, loss = 0.94 (1684.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:33:33.714231: step 52900, loss = 0.80 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:34.482726: step 52910, loss = 0.75 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:35.249431: step 52920, loss = 0.85 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:36.006865: step 52930, loss = 0.91 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:33:36.772586: step 52940, loss = 0.93 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:37.544563: step 52950, loss = 0.80 (1658.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:38.314546: step 52960, loss = 0.66 (1662.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:39.079652: step 52970, loss = 0.83 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:39.837845: step 52980, loss = 0.73 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:33:40.605029: step 52990, loss = 0.64 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:41.362171: step 53000, loss = 0.76 (1690.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:33:42.129300: step 53010, loss = 0.82 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:42.896906: step 53020, loss = 0.79 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:43.657590: step 53030, loss = 0.77 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:33:44.429427: step 53040, loss = 0.66 (1658.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:45.188377: step 53050, loss = 0.90 (1686.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:33:45.952842: step 53060, loss = 0.70 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:33:46.724027: step 53070, loss = 0.72 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:47.491891: step 53080, loss = 0.72 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:48.255079: step 53090, loss = 0.72 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:33:49.018533: step 53100, loss = 0.86 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:33:49.783772: step 53110, loss = 0.79 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:50.551164: step 53120, loss = 0.89 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:51.312714: step 53130, loss = 0.78 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:33:52.066829: step 53140, loss = 0.89 (1697.4 examples/sec; 0.075 sec/batch)
2017-05-05 18:33:52.832379: step 53150, loss = 0.70 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:53.601381: step 53160, loss = 0.86 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:54.373564: step 53170, loss = 0.72 (1657.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:55.139044: step 53180, loss = 0.76 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:55.900507: step 53190, loss = 0.80 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:33:56.662949: step 53200, loss = 0.85 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:33:57.421574: step 53210, loss = 0.99 (1687.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:33:58.189201: step 53220, loss = 0.75 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:33:58.952670: step 53230, loss = 0.77 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:33:59.706902: step 53240, loss = 0.76 (1697.1 examples/sec; 0.075 sec/batch)
2017-05-05 18:34:00.472665: step 53250, loss = 0.83 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:34:01.239738: step 53260, loss = 0.78 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:34:01.998830: step 53270, loss = 0.81 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:02.758206: step 53280, loss = 0.82 (1685.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:03.520236: step 53290, loss = 0.82 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:04.280070: step 53300, loss = 0.85 (1684.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:05.043538: step 53310, loss = 0.78 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:05.807539: step 53320, loss = 0.82 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:06.570492: step 53330, loss = 0.86 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:07.338047: step 53340, loss = 0.82 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:34:08.098573: step 53350, loss = 0.74 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:08.859070: step 53360, loss = 0.74 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:09.625875: step 53370, loss = 0.71 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:34:10.397851: step 53380, loss = 0.86 (1658.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:34:11.159992: step 53390, loss = 0.81 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:11.913435: step 53400, loss = 0.74 (1698.9 examples/sec; 0.075 sec/batch)
2017-05-05 18:34:12.685243: step 53410, loss = 0.67 (1658.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:34:13.454198: step 53420, loss = 0.80 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:34:14.218594: step 53430, loss = 0.75 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:14.979720: step 53440, loss = 0.75 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:15.736212: step 53450, loss = 0.73 (1692.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:16.595471: step 53460, loss = 0.65 (1489.7 examples/sec; 0.086 sec/batch)
2017-05-05 18:34:17.265385: step 53470, loss = 0.74 (1910.7 examples/sec; 0.067 sec/batch)
2017-05-05 18:34:18.030434: step 53480, loss = 0.96 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:34:18.794399: step 53490, loss = 0.80 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:19.553028: step 53500, loss = 0.72 (1687.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:20.313266: step 53510, loss = 0.83 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:21.076099: step 53520, loss = 0.79 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:21.835605: step 53530, loss = 0.83 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:22.602457: step 53540, loss = 0.62 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:34:23.360582: step 53550, loss = 0.84 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:24.121820: step 53560, loss = 0.69 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:24.891559: step 53570, loss = 0.78 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:34:25.651982: step 53580, loss = 0.85 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:26.418587: step 53590, loss = 0.65 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:34:27.179908: step 53600, loss = 0.75 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:27.937902: step 53610, loss = 0.78 (1688.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:28.702544: step 53620, loss = 0.89 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:29.467127: step 53630, loss = 0.78 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:30.230620: step 53640, loss = 0.78 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:30.996911: step 53650, loss = 0.74 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:34:31.753179: step 53660, loss = 0.62 (1692.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:32.523133: step 53670, loss = 0.79 (1662.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:34:33.284704: step 53680, loss = 0.80 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:34.047189: step 53690, loss = 0.84 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:34.815812: step 53700, loss = 0.61 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:34:35.569722: step 53710, loss = 0.75 (1697.8 examples/sec; 0.075 sec/batch)
2017-05-05 18:34:36.334687: step 53720, loss = 0.80 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:37.095562: step 53730, loss = 0.82 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:37.860639: step 53740, loss = 0.93 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:34:38.621910: step 53750, loss = 0.82 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:39.384887: step 53760, loss = 0.79 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:40.150847: step 53770, loss = 0.85 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:34:40.913261: step 53780, loss = 0.74 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:41.676841: step 53790, loss = 0.73 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:42.440904: step 53800, loss = 0.79 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:43.210191: step 53810, loss = 0.86 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:34:43.966200: step 53820, loss = 0.76 (1693.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:44.732701: step 53830, loss = 0.73 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:34:45.506928: step 53840, loss = 0.84 (1653.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:34:46.272800: step 53850, loss = 0.76 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:34:47.042540: step 53860, loss = 0.97 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:34:47.799181: step 53870, loss = 0.85 (1691.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:48.567479: step 53880, loss = 0.74 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:34:49.332215: step 53890, loss = 0.75 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:50.102148: step 53900, loss = 0.76 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:34:50.865265: step 53910, loss = 0.63 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:51.621648: step 53920, loss = 0.81 (1692.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:52.381265: step 53930, loss = 0.79 (1685.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:53.143746: step 53940, loss = 0.67 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:53.909438: step 53950, loss = 0.75 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:34:54.677351: step 53960, loss = 0.83 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:34:55.431988: step 53970, loss = 0.88 (1696.2 examples/sec; 0.075 sec/batch)
2017-05-05 18:34:56.195248: step 53980, loss = 0.80 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:56.957588: step 53990, loss = 0.79 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:57.721264: step 54000, loss = 0.71 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:34:58.488087: step 54010, loss = 0.68 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:34:59.250280: step 54020, loss = 0.78 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:00.004225: step 54030, loss = 0.75 (1697.8 examples/sec; 0.075 sec/batch)
2017-05-05 18:35:00.772520: step 54040, loss = 0.75 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:35:01.537941: step 54050, loss = 0.68 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:35:02.317515: step 54060, loss = 0.92 (1641.9 examples/sec; 0.078 sec/batch)
2017-05-05 18:35:03.075713: step 54070, loss = 0.78 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:03.827062: step 54080, loss = 0.79 (1703.6 examples/sec; 0.075 sec/batch)
2017-05-05 18:35:04.594732: step 54090, loss = 0.88 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:35:05.355970: step 54100, loss = 0.84 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:06.128760: step 54110, loss = 0.73 (1656.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:35:06.892054: step 54120, loss = 0.77 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:07.652290: step 54130, loss = 0.83 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:08.421876: step 54140, loss = 0.80 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:35:09.194194: step 54150, loss = 0.86 (1657.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:35:09.948920: step 54160, loss = 0.91 (1696.0 examples/sec; 0.075 sec/batch)
2017-05-05 18:35:10.711856: step 54170, loss = 0.84 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:11.475391: step 54180, loss = 0.78 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:12.236173: step 54190, loss = 0.84 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:12.997024: step 54200, loss = 0.74 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:13.762258: step 54210, loss = 0.70 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:35:14.531141: step 54220, loss = 0.75 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:35:15.301033: step 54230, loss = 0.88 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:35:16.065139: step 54240, loss = 0.90 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:16.834653: step 54250, loss = 0.76 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:35:17.613265: step 54260, loss = 0.81 (1643.9 examples/sec; 0.078 sec/batch)
2017-05-05 18:35:18.378618: step 54270, loss = 0.88 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:35:19.145759: step 54280, loss = 0.89 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:35:19.901504: step 54290, loss = 0.73 (1693.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:20.671215: step 54300, loss = 0.88 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:35:21.438671: step 54310, loss = 0.76 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:35:22.203061: step 54320, loss = 0.91 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:22.963088: step 54330, loss = 0.84 (1684.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:23.722927: step 54340, loss = 0.80 (1684.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:24.482014: step 54350, loss = 0.72 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:25.247789: step 54360, loss = 0.75 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:35:26.011720: step 54370, loss = 0.80 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:26.785351: step 54380, loss = 0.87 (1654.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:35:27.543690: step 54390, loss = 0.74 (1687.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:28.302528: step 54400, loss = 0.72 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:29.068607: step 54410, loss = 0.80 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:35:29.844233: step 54420, loss = 0.82 (1650.3 examples/sec; 0.078 sec/batch)
2017-05-05 18:35:30.613553: step 54430, loss = 0.74 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:35:31.379332: step 54440, loss = 0.65 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:35:32.228495: step 54450, loss = 0.76 (1507.4 examples/sec; 0.085 sec/batch)
2017-05-05 18:35:32.895907: step 54460, loss = 0.65 (1917.9 examples/sec; 0.067 sec/batch)
2017-05-05 18:35:33.658756: step 54470, loss = 0.70 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:34.433162: step 54480, loss = 0.86 (1652.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:35:35.198557: step 54490, loss = 0.69 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:35:35.951774: step 54500, loss = 0.80 (1699.4 examples/sec; 0.075 sec/batch)
2017-05-05 18:35:36.711820: step 54510, loss = 0.77 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:37.469903: step 54520, loss = 0.86 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:38.232128: step 54530, loss = 0.68 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:38.995069: step 54540, loss = 0.62 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:39.749213: step 54550, loss = 1.03 (1697.3 examples/sec; 0.075 sec/batch)
2017-05-05 18:35:40.516473: step 54560, loss = 0.87 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:35:41.278351: step 54570, loss = 0.87 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:42.042942: step 54580, loss = 0.88 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:42.808225: step 54590, loss = 0.90 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:35:43.567007: step 54600, loss = 0.72 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:44.334199: step 54610, loss = 0.90 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:35:45.098347: step 54620, loss = 0.74 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:45.860629: step 54630, loss = 0.90 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:46.626079: step 54640, loss = 0.82 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:35:47.395700: step 54650, loss = 0.81 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:35:48.151964: step 54660, loss = 0.88 (1692.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:48.918766: step 54670, loss = 0.82 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:35:49.676522: step 54680, loss = 0.79 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:50.439572: step 54690, loss = 0.84 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:51.202906: step 54700, loss = 0.71 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:51.959042: step 54710, loss = 0.74 (1692.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:52.719811: step 54720, loss = 0.82 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:53.486692: step 54730, loss = 0.86 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:35:54.247233: step 54740, loss = 0.96 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:55.007775: step 54750, loss = 0.80 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:55.761766: step 54760, loss = 0.75 (1697.6 examples/sec; 0.075 sec/batch)
2017-05-05 18:35:56.523968: step 54770, loss = 0.74 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:57.285158: step 54780, loss = 0.85 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:58.049915: step 54790, loss = 0.65 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:35:58.822625: step 54800, loss = 0.92 (1656.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:35:59.576908: step 54810, loss = 0.78 (1697.0 examples/sec; 0.075 sec/batch)
2017-05-05 18:36:00.332680: step 54820, loss = 0.76 (1693.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:01.088406: step 54830, loss = 0.79 (1693.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:01.851320: step 54840, loss = 0.64 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:02.615886: step 54850, loss = 0.85 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:03.376862: step 54860, loss = 0.81 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:04.141408: step 54870, loss = 0.76 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:04.903066: step 54880, loss = 0.69 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:05.755413: step 54890, loss = 0.76 (1501.8 examples/sec; 0.085 sec/batch)
2017-05-05 18:36:06.526454: step 54900, loss = 0.75 (1660.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:36:07.287370: step 54910, loss = 0.69 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:08.043154: step 54920, loss = 0.93 (1693.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:08.807267: step 54930, loss = 0.88 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:09.580630: step 54940, loss = 0.82 (1655.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:36:10.350321: step 54950, loss = 0.70 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:36:11.105616: step 54960, loss = 0.91 (1694.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:11.862338: step 54970, loss = 0.79 (1691.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:12.625808: step 54980, loss = 0.87 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:13.393490: step 54990, loss = 0.75 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:36:14.153631: step 55000, loss = 1.00 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:14.928677: step 55010, loss = 0.71 (1651.5 examples/sec; 0.078 sec/batch)
2017-05-05 18:36:15.679564: step 55020, loss = 0.63 (1704.7 examples/sec; 0.075 sec/batch)
2017-05-05 18:36:16.448033: step 55030, loss = 0.79 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:36:17.214637: step 55040, loss = 0.75 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:36:17.980589: step 55050, loss = 0.62 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:36:18.742474: step 55060, loss = 0.87 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:19.497186: step 55070, loss = 0.73 (1696.0 examples/sec; 0.075 sec/batch)
2017-05-05 18:36:20.266339: step 55080, loss = 0.93 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:36:21.027860: step 55090, loss = 0.71 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:21.785613: step 55100, loss = 0.73 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:22.552026: step 55110, loss = 0.89 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:36:23.314606: step 55120, loss = 0.81 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:24.071976: step 55130, loss = 0.82 (1690.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:24.843611: step 55140, loss = 1.06 (1658.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:36:25.610957: step 55150, loss = 0.80 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:36:26.376216: step 55160, loss = 0.79 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:36:27.139738: step 55170, loss = 0.76 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:27.892563: step 55180, loss = 0.74 (1700.3 examples/sec; 0.075 sec/batch)
2017-05-05 18:36:28.660947: step 55190, loss = 0.75 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:36:29.425919: step 55200, loss = 0.83 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:30.192926: step 55210, loss = 0.95 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:36:30.956597: step 55220, loss = 0.81 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:31.712283: step 55230, loss = 0.77 (1693.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:32.475663: step 55240, loss = 0.79 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:33.238916: step 55250, loss = 0.74 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:34.000113: step 55260, loss = 0.69 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:34.769619: step 55270, loss = 0.74 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:36:35.526495: step 55280, loss = 0.91 (1691.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:36.289077: step 55290, loss = 0.93 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:37.049128: step 55300, loss = 0.83 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:37.813913: step 55310, loss = 0.66 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:38.578781: step 55320, loss = 0.67 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:39.340437: step 55330, loss = 0.73 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:40.101452: step 55340, loss = 0.93 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:40.870301: step 55350, loss = 0.92 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:36:41.633121: step 55360, loss = 0.85 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:42.397923: step 55370, loss = 0.86 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:43.163843: step 55380, loss = 0.74 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:36:43.921820: step 55390, loss = 0.82 (1688.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:44.693660: step 55400, loss = 0.82 (1658.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:36:45.463651: step 55410, loss = 0.92 (1662.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:36:46.231385: step 55420, loss = 0.78 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:36:46.998737: step 55430, loss = 0.91 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:36:47.849635: step 55440, loss = 0.82 (1504.3 examples/sec; 0.085 sec/batch)
2017-05-05 18:36:48.513277: step 55450, loss = 0.80 (1928.8 examples/sec; 0.066 sec/batch)
2017-05-05 18:36:49.284757: step 55460, loss = 0.78 (1659.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:36:50.044354: step 55470, loss = 0.76 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:50.815797: step 55480, loss = 1.02 (1659.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:36:51.570486: step 55490, loss = 0.66 (1696.1 examples/sec; 0.075 sec/batch)
2017-05-05 18:36:52.336730: step 55500, loss = 0.93 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:36:53.100018: step 55510, loss = 0.80 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:53.868167: step 55520, loss = 0.67 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:36:54.635725: step 55530, loss = 0.82 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:36:55.393175: step 55540, loss = 0.71 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:56.153304: step 55550, loss = 0.72 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:56.923840: step 55560, loss = 0.81 (1661.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:36:57.688755: step 55570, loss = 0.87 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:58.450720: step 55580, loss = 0.78 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:36:59.216728: step 55590, loss = 0.76 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:36:59.973624: step 55600, loss = 0.96 (1691.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:00.739190: step 55610, loss = 0.84 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:01.506650: step 55620, loss = 0.77 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:02.267499: step 55630, loss = 0.81 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:03.041468: step 55640, loss = 0.82 (1653.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:03.795075: step 55650, loss = 0.71 (1698.5 examples/sec; 0.075 sec/batch)
2017-05-05 18:37:04.562007: step 55660, loss = 0.77 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:05.323939: step 55670, loss = 0.79 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:06.089693: step 55680, loss = 0.71 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:06.866114: step 55690, loss = 0.75 (1648.6 examples/sec; 0.078 sec/batch)
2017-05-05 18:37:07.626386: step 55700, loss = 0.85 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:08.394859: step 55710, loss = 0.67 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:09.160603: step 55720, loss = 0.83 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:09.926300: step 55730, loss = 0.86 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:10.693865: step 55740, loss = 0.82 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:11.459779: step 55750, loss = 0.62 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:12.218303: step 55760, loss = 0.66 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:12.979537: step 55770, loss = 0.59 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:13.749433: step 55780, loss = 0.82 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:14.517638: step 55790, loss = 0.67 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:15.284850: step 55800, loss = 0.68 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:16.041235: step 55810, loss = 0.70 (1692.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:16.805703: step 55820, loss = 0.76 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:17.568255: step 55830, loss = 0.81 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:18.340062: step 55840, loss = 0.80 (1658.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:19.098580: step 55850, loss = 0.79 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:19.857051: step 55860, loss = 0.85 (1687.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:20.623527: step 55870, loss = 0.86 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:21.390958: step 55880, loss = 0.74 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:22.157980: step 55890, loss = 0.74 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:22.919061: step 55900, loss = 0.91 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:23.678536: step 55910, loss = 0.86 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:24.446863: step 55920, loss = 0.70 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:25.215460: step 55930, loss = 0.64 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:25.980881: step 55940, loss = 0.76 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:26.748773: step 55950, loss = 0.94 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:27.502044: step 55960, loss = 0.76 (1699.3 examples/sec; 0.075 sec/batch)
2017-05-05 18:37:28.270875: step 55970, loss = 0.92 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:29.041922: step 55980, loss = 0.71 (1660.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:29.813428: step 55990, loss = 0.70 (1659.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:30.580992: step 56000, loss = 0.81 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:31.342252: step 56010, loss = 0.92 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:32.099109: step 56020, loss = 0.90 (1691.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:32.861492: step 56030, loss = 0.73 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:33.627852: step 56040, loss = 0.73 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:34.397225: step 56050, loss = 0.86 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:35.161971: step 56060, loss = 0.83 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:35.914176: step 56070, loss = 0.82 (1701.7 examples/sec; 0.075 sec/batch)
2017-05-05 18:37:36.680276: step 56080, loss = 0.60 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:37.449120: step 56090, loss = 0.82 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:38.209428: step 56100, loss = 0.74 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:38.971496: step 56110, loss = 0.85 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:39.729652: step 56120, loss = 0.75 (1688.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:40.489097: step 56130, loss = 0.79 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:41.261619: step 56140, loss = 0.78 (1656.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:42.025117: step 56150, loss = 0.75 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:42.787826: step 56160, loss = 0.80 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:43.547700: step 56170, loss = 0.84 (1684.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:44.308774: step 56180, loss = 0.83 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:45.072155: step 56190, loss = 0.70 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:45.838400: step 56200, loss = 0.96 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:46.609332: step 56210, loss = 0.80 (1660.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:47.370819: step 56220, loss = 0.71 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:48.133516: step 56230, loss = 0.70 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:48.902944: step 56240, loss = 0.74 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:49.673497: step 56250, loss = 0.72 (1661.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:50.436379: step 56260, loss = 0.80 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:51.203419: step 56270, loss = 0.67 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:51.959911: step 56280, loss = 0.95 (1692.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:52.720922: step 56290, loss = 0.83 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:53.492934: step 56300, loss = 0.80 (1658.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:54.254501: step 56310, loss = 0.74 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:55.025750: step 56320, loss = 0.82 (1659.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:55.783185: step 56330, loss = 0.68 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:56.545768: step 56340, loss = 0.69 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:57.308643: step 56350, loss = 1.03 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:58.071844: step 56360, loss = 0.89 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:37:58.838499: step 56370, loss = 0.82 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:37:59.597181: step 56380, loss = 0.72 (1687.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:00.369064: step 56390, loss = 0.91 (1658.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:01.139138: step 56400, loss = 0.79 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:01.902121: step 56410, loss = 0.84 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:02.668156: step 56420, loss = 0.80 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:03.524824: step 56430, loss = 0.75 (1494.2 examples/sec; 0.086 sec/batch)
2017-05-05 18:38:04.192422: step 56440, loss = 0.83 (1917.3 examples/sec; 0.067 sec/batch)
2017-05-05 18:38:04.954059: step 56450, loss = 0.97 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:05.724686: step 56460, loss = 0.92 (1661.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:06.491987: step 56470, loss = 0.67 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:07.249331: step 56480, loss = 0.64 (1690.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:08.016351: step 56490, loss = 0.82 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:08.777877: step 56500, loss = 0.99 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:09.542836: step 56510, loss = 0.83 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:10.311494: step 56520, loss = 0.76 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:11.082534: step 56530, loss = 0.79 (1660.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:11.839304: step 56540, loss = 0.70 (1691.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:12.607887: step 56550, loss = 0.73 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:13.367846: step 56560, loss = 0.78 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:14.130109: step 56570, loss = 0.72 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:14.898708: step 56580, loss = 0.84 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:15.653582: step 56590, loss = 0.85 (1695.6 examples/sec; 0.075 sec/batch)
2017-05-05 18:38:16.414880: step 56600, loss = 0.82 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:17.184456: step 56610, loss = 0.78 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:17.944577: step 56620, loss = 0.79 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:18.716073: step 56630, loss = 0.72 (1659.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:19.471505: step 56640, loss = 0.72 (1694.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:20.233284: step 56650, loss = 0.91 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:20.993261: step 56660, loss = 0.83 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:21.755439: step 56670, loss = 0.74 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:22.527838: step 56680, loss = 0.73 (1657.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:23.297350: step 56690, loss = 1.02 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:24.056128: step 56700, loss = 0.80 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:24.823722: step 56710, loss = 0.69 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:25.592068: step 56720, loss = 0.82 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:26.354253: step 56730, loss = 0.89 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:27.118109: step 56740, loss = 0.70 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:27.869249: step 56750, loss = 0.90 (1704.1 examples/sec; 0.075 sec/batch)
2017-05-05 18:38:28.634223: step 56760, loss = 0.85 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:29.403213: step 56770, loss = 0.82 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:30.171891: step 56780, loss = 0.72 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:30.936596: step 56790, loss = 0.79 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:31.692517: step 56800, loss = 0.77 (1693.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:32.454896: step 56810, loss = 0.77 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:33.221586: step 56820, loss = 0.73 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:33.988009: step 56830, loss = 0.67 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:34.756744: step 56840, loss = 0.77 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:35.513161: step 56850, loss = 0.89 (1692.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:36.270191: step 56860, loss = 0.76 (1690.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:37.033872: step 56870, loss = 0.70 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:37.798821: step 56880, loss = 0.73 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:38.565212: step 56890, loss = 0.79 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:39.332483: step 56900, loss = 0.99 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:40.088944: step 56910, loss = 0.87 (1692.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:40.850568: step 56920, loss = 0.63 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:41.608947: step 56930, loss = 0.81 (1687.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:42.381774: step 56940, loss = 0.78 (1656.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:43.146833: step 56950, loss = 0.88 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:43.907914: step 56960, loss = 0.80 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:44.677244: step 56970, loss = 0.67 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:45.441205: step 56980, loss = 0.86 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:46.208083: step 56990, loss = 0.98 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:46.975838: step 57000, loss = 0.75 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:47.730258: step 57010, loss = 0.89 (1696.7 examples/sec; 0.075 sec/batch)
2017-05-05 18:38:48.494313: step 57020, loss = 0.77 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:49.267243: step 57030, loss = 0.74 (1656.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:50.025040: step 57040, loss = 0.89 (1689.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:50.789712: step 57050, loss = 0.76 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:51.550229: step 57060, loss = 0.75 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:52.314119: step 57070, loss = 0.78 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:53.081283: step 57080, loss = 0.77 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:53.847610: step 57090, loss = 0.67 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:54.607881: step 57100, loss = 0.74 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:55.371955: step 57110, loss = 0.76 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:56.129521: step 57120, loss = 0.87 (1689.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:56.898718: step 57130, loss = 0.89 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:57.666640: step 57140, loss = 0.82 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:58.430454: step 57150, loss = 1.04 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:38:59.197483: step 57160, loss = 0.74 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:38:59.949028: step 57170, loss = 0.77 (1703.2 examples/sec; 0.075 sec/batch)
2017-05-05 18:39:00.711135: step 57180, loss = 1.26 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:01.477229: step 57190, loss = 0.79 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:02.237659: step 57200, loss = 0.90 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:03.012227: step 57210, loss = 0.94 (1652.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:03.766115: step 57220, loss = 0.70 (1697.9 examples/sec; 0.075 sec/batch)
2017-05-05 18:39:04.531212: step 57230, loss = 0.76 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:05.303354: step 57240, loss = 0.71 (1657.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:06.072736: step 57250, loss = 0.81 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:06.836631: step 57260, loss = 0.68 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:07.597677: step 57270, loss = 0.71 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:08.362401: step 57280, loss = 0.90 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:09.129995: step 57290, loss = 0.75 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:09.897042: step 57300, loss = 0.82 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:10.666666: step 57310, loss = 0.76 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:11.425238: step 57320, loss = 0.81 (1687.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:12.188470: step 57330, loss = 0.76 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:12.956960: step 57340, loss = 0.75 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:13.720541: step 57350, loss = 0.77 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:14.489270: step 57360, loss = 0.79 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:15.257808: step 57370, loss = 0.65 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:16.016886: step 57380, loss = 0.93 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:16.774283: step 57390, loss = 0.78 (1690.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:17.535568: step 57400, loss = 0.79 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:18.307338: step 57410, loss = 0.97 (1658.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:19.183732: step 57420, loss = 0.99 (1460.5 examples/sec; 0.088 sec/batch)
2017-05-05 18:39:19.829517: step 57430, loss = 0.80 (1982.1 examples/sec; 0.065 sec/batch)
2017-05-05 18:39:20.596657: step 57440, loss = 0.97 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:21.364011: step 57450, loss = 0.86 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:22.123308: step 57460, loss = 0.72 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:22.889766: step 57470, loss = 0.73 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:23.643293: step 57480, loss = 0.73 (1698.7 examples/sec; 0.075 sec/batch)
2017-05-05 18:39:24.408325: step 57490, loss = 0.77 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:25.167152: step 57500, loss = 0.85 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:25.936545: step 57510, loss = 0.69 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:26.695153: step 57520, loss = 0.71 (1687.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:27.459700: step 57530, loss = 0.79 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:28.222738: step 57540, loss = 0.85 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:28.982803: step 57550, loss = 0.75 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:29.754072: step 57560, loss = 0.90 (1659.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:30.522097: step 57570, loss = 0.82 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:31.294259: step 57580, loss = 0.65 (1657.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:32.047604: step 57590, loss = 0.81 (1699.1 examples/sec; 0.075 sec/batch)
2017-05-05 18:39:32.811431: step 57600, loss = 0.81 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:33.592344: step 57610, loss = 0.67 (1639.1 examples/sec; 0.078 sec/batch)
2017-05-05 18:39:34.369599: step 57620, loss = 0.85 (1646.8 examples/sec; 0.078 sec/batch)
2017-05-05 18:39:35.133308: step 57630, loss = 0.88 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:35.891409: step 57640, loss = 0.85 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:36.659952: step 57650, loss = 0.76 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:37.425739: step 57660, loss = 0.88 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:38.197390: step 57670, loss = 0.65 (1658.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:38.965002: step 57680, loss = 0.76 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:39.725428: step 57690, loss = 0.62 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:40.494533: step 57700, loss = 0.79 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:41.259461: step 57710, loss = 0.94 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:42.027619: step 57720, loss = 0.83 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:42.793895: step 57730, loss = 0.76 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:43.556148: step 57740, loss = 0.72 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:44.324141: step 57750, loss = 0.91 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:45.088670: step 57760, loss = 0.86 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:45.857855: step 57770, loss = 0.87 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:46.621007: step 57780, loss = 0.71 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:47.383070: step 57790, loss = 0.84 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:48.142678: step 57800, loss = 0.80 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:48.918180: step 57810, loss = 0.86 (1650.6 examples/sec; 0.078 sec/batch)
2017-05-05 18:39:49.684484: step 57820, loss = 0.89 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:50.453343: step 57830, loss = 0.75 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:51.218491: step 57840, loss = 0.78 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:51.976715: step 57850, loss = 0.68 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:52.740309: step 57860, loss = 0.69 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:53.503775: step 57870, loss = 0.65 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:54.271983: step 57880, loss = 0.74 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:55.031909: step 57890, loss = 0.66 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:55.792645: step 57900, loss = 0.70 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:56.555117: step 57910, loss = 0.66 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:57.322012: step 57920, loss = 0.89 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:58.085317: step 57930, loss = 0.81 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:39:58.851641: step 57940, loss = 0.99 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:39:59.606421: step 57950, loss = 0.74 (1695.9 examples/sec; 0.075 sec/batch)
2017-05-05 18:40:00.373572: step 57960, loss = 0.69 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:01.132445: step 57970, loss = 0.89 (1686.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:01.898209: step 57980, loss = 0.82 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:02.667458: step 57990, loss = 1.04 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:03.430292: step 58000, loss = 0.73 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:04.194531: step 58010, loss = 0.74 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:04.961449: step 58020, loss = 0.72 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:05.725822: step 58030, loss = 1.01 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:06.492957: step 58040, loss = 0.79 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:07.258158: step 58050, loss = 0.74 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:08.012200: step 58060, loss = 0.90 (1697.5 examples/sec; 0.075 sec/batch)
2017-05-05 18:40:08.784862: step 58070, loss = 0.76 (1656.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:09.548799: step 58080, loss = 0.81 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:10.315813: step 58090, loss = 0.91 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:11.078569: step 58100, loss = 0.74 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:11.839116: step 58110, loss = 0.79 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:12.600533: step 58120, loss = 0.71 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:13.366219: step 58130, loss = 0.84 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:14.128449: step 58140, loss = 0.83 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:14.886216: step 58150, loss = 0.77 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:15.643955: step 58160, loss = 0.73 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:16.408810: step 58170, loss = 0.81 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:17.175107: step 58180, loss = 0.98 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:17.941992: step 58190, loss = 0.73 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:18.713338: step 58200, loss = 0.69 (1659.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:19.473553: step 58210, loss = 0.79 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:20.240569: step 58220, loss = 0.66 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:21.000881: step 58230, loss = 0.91 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:21.763972: step 58240, loss = 0.85 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:22.540764: step 58250, loss = 0.74 (1647.8 examples/sec; 0.078 sec/batch)
2017-05-05 18:40:23.304294: step 58260, loss = 0.94 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:24.069990: step 58270, loss = 0.78 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:24.838051: step 58280, loss = 0.73 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:25.612045: step 58290, loss = 0.83 (1653.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:26.372290: step 58300, loss = 0.71 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:27.136707: step 58310, loss = 0.72 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:27.892839: step 58320, loss = 0.83 (1692.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:28.654073: step 58330, loss = 0.70 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:29.425908: step 58340, loss = 0.73 (1658.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:30.195207: step 58350, loss = 0.79 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:30.958457: step 58360, loss = 0.82 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:31.717422: step 58370, loss = 0.77 (1686.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:32.486331: step 58380, loss = 0.68 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:33.256343: step 58390, loss = 0.85 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:34.022835: step 58400, loss = 0.76 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:34.906962: step 58410, loss = 0.96 (1447.8 examples/sec; 0.088 sec/batch)
2017-05-05 18:40:35.545863: step 58420, loss = 0.91 (2003.4 examples/sec; 0.064 sec/batch)
2017-05-05 18:40:36.303990: step 58430, loss = 0.89 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:37.072005: step 58440, loss = 0.82 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:37.837038: step 58450, loss = 0.73 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:38.602871: step 58460, loss = 0.75 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:39.369097: step 58470, loss = 0.81 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:40.126641: step 58480, loss = 0.90 (1689.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:40.898995: step 58490, loss = 0.65 (1657.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:41.660864: step 58500, loss = 0.77 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:42.428376: step 58510, loss = 0.79 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:43.197384: step 58520, loss = 0.85 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:43.950833: step 58530, loss = 0.65 (1698.8 examples/sec; 0.075 sec/batch)
2017-05-05 18:40:44.714238: step 58540, loss = 0.86 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:45.474633: step 58550, loss = 0.68 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:46.245941: step 58560, loss = 0.72 (1659.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:47.012795: step 58570, loss = 0.75 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:47.774810: step 58580, loss = 0.78 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:48.540782: step 58590, loss = 0.75 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:49.305590: step 58600, loss = 0.94 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:50.083282: step 58610, loss = 0.80 (1645.9 examples/sec; 0.078 sec/batch)
2017-05-05 18:40:50.851450: step 58620, loss = 0.64 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:51.614432: step 58630, loss = 0.80 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:52.380990: step 58640, loss = 0.68 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:53.150715: step 58650, loss = 0.77 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:53.916930: step 58660, loss = 0.82 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:54.686953: step 58670, loss = 0.79 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:55.449865: step 58680, loss = 0.81 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:56.212203: step 58690, loss = 0.65 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:56.964264: step 58700, loss = 0.90 (1702.0 examples/sec; 0.075 sec/batch)
2017-05-05 18:40:57.729094: step 58710, loss = 0.71 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:40:58.497306: step 58720, loss = 0.71 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:40:59.263416: step 58730, loss = 0.80 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:41:00.023315: step 58740, loss = 0.97 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:00.782085: step 58750, loss = 0.66 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:01.547290: step 58760, loss = 0.80 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:41:02.311736: step 58770, loss = 0.93 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:03.073473: step 58780, loss = 0.77 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:03.835098: step 58790, loss = 0.65 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:04.612158: step 58800, loss = 0.64 (1647.2 examples/sec; 0.078 sec/batch)
2017-05-05 18:41:05.380052: step 58810, loss = 0.84 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:41:06.143008: step 58820, loss = 0.74 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:06.905872: step 58830, loss = 0.75 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:07.659415: step 58840, loss = 0.77 (1698.6 examples/sec; 0.075 sec/batch)
2017-05-05 18:41:08.419768: step 58850, loss = 0.89 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:09.187671: step 58860, loss = 0.67 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:41:09.954182: step 58870, loss = 0.85 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:41:10.720852: step 58880, loss = 0.97 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:41:11.483456: step 58890, loss = 0.77 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:12.246302: step 58900, loss = 0.74 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:13.017075: step 58910, loss = 0.99 (1660.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:41:13.783316: step 58920, loss = 0.84 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:41:14.546547: step 58930, loss = 0.71 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:15.315862: step 58940, loss = 0.91 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:41:16.074388: step 58950, loss = 0.82 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:16.832632: step 58960, loss = 0.86 (1688.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:17.600517: step 58970, loss = 0.90 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:41:18.367047: step 58980, loss = 0.66 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:41:19.128896: step 58990, loss = 0.88 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:19.884708: step 59000, loss = 0.90 (1693.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:20.653301: step 59010, loss = 0.80 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:41:21.416131: step 59020, loss = 0.68 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:22.180881: step 59030, loss = 0.84 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:22.944287: step 59040, loss = 0.79 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:23.702299: step 59050, loss = 0.89 (1688.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:24.465596: step 59060, loss = 0.80 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:25.231151: step 59070, loss = 1.00 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:41:25.999622: step 59080, loss = 0.74 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:41:26.762230: step 59090, loss = 0.70 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:27.524131: step 59100, loss = 0.87 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:28.284529: step 59110, loss = 0.86 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:29.050269: step 59120, loss = 0.83 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:41:29.805045: step 59130, loss = 0.68 (1695.9 examples/sec; 0.075 sec/batch)
2017-05-05 18:41:30.571496: step 59140, loss = 0.81 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:41:31.333933: step 59150, loss = 0.64 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:32.090901: step 59160, loss = 0.69 (1691.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:32.850823: step 59170, loss = 0.81 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:33.613039: step 59180, loss = 0.57 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:34.377698: step 59190, loss = 0.85 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:35.141463: step 59200, loss = 0.64 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:35.893201: step 59210, loss = 0.82 (1702.7 examples/sec; 0.075 sec/batch)
2017-05-05 18:41:36.659929: step 59220, loss = 0.69 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:41:37.425513: step 59230, loss = 0.83 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:41:38.198758: step 59240, loss = 0.92 (1655.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:41:38.964625: step 59250, loss = 0.80 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:41:39.724238: step 59260, loss = 1.05 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:40.490001: step 59270, loss = 0.87 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:41:41.259841: step 59280, loss = 0.75 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:41:42.029846: step 59290, loss = 0.82 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:41:42.797131: step 59300, loss = 0.90 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:41:43.552577: step 59310, loss = 0.60 (1694.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:44.317606: step 59320, loss = 0.74 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:41:45.082892: step 59330, loss = 0.70 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:41:45.847972: step 59340, loss = 0.69 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:41:46.612345: step 59350, loss = 0.70 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:47.380764: step 59360, loss = 0.83 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:41:48.136279: step 59370, loss = 0.80 (1694.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:48.900873: step 59380, loss = 0.73 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:49.665546: step 59390, loss = 0.91 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:50.537886: step 59400, loss = 0.76 (1467.3 examples/sec; 0.087 sec/batch)
2017-05-05 18:41:51.198540: step 59410, loss = 0.95 (1937.5 examples/sec; 0.066 sec/batch)
2017-05-05 18:41:51.958746: step 59420, loss = 0.79 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:52.720713: step 59430, loss = 0.83 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:53.481427: step 59440, loss = 0.88 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:54.250905: step 59450, loss = 0.73 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:41:55.015743: step 59460, loss = 0.67 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:55.776976: step 59470, loss = 0.91 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:56.536270: step 59480, loss = 0.78 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:57.298887: step 59490, loss = 0.62 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:58.068666: step 59500, loss = 0.75 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:41:58.831650: step 59510, loss = 0.77 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:41:59.582432: step 59520, loss = 0.77 (1704.9 examples/sec; 0.075 sec/batch)
2017-05-05 18:42:00.341632: step 59530, loss = 0.76 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:01.111010: step 59540, loss = 0.69 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:01.874932: step 59550, loss = 0.76 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:02.639969: step 59560, loss = 0.89 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:03.408343: step 59570, loss = 0.78 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:04.175392: step 59580, loss = 0.80 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:04.933791: step 59590, loss = 0.70 (1687.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:05.720231: step 59600, loss = 0.69 (1627.6 examples/sec; 0.079 sec/batch)
2017-05-05 18:42:06.489772: step 59610, loss = 0.72 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:07.263949: step 59620, loss = 0.78 (1653.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:08.016273: step 59630, loss = 0.74 (1701.4 examples/sec; 0.075 sec/batch)
2017-05-05 18:42:08.777990: step 59640, loss = 0.83 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:09.546438: step 59650, loss = 0.89 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:10.311163: step 59660, loss = 0.82 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:11.079958: step 59670, loss = 0.66 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:11.840537: step 59680, loss = 0.73 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:12.611255: step 59690, loss = 0.88 (1660.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:13.375684: step 59700, loss = 0.65 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:14.140175: step 59710, loss = 0.84 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:14.903064: step 59720, loss = 0.94 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:15.658348: step 59730, loss = 0.64 (1694.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:16.417671: step 59740, loss = 0.73 (1685.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:17.178999: step 59750, loss = 0.70 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:17.947387: step 59760, loss = 0.78 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:18.711341: step 59770, loss = 0.83 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:19.475947: step 59780, loss = 0.83 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:20.237474: step 59790, loss = 0.73 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:21.009079: step 59800, loss = 0.69 (1658.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:21.774024: step 59810, loss = 0.74 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:22.542433: step 59820, loss = 0.68 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:23.306266: step 59830, loss = 0.84 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:24.070066: step 59840, loss = 0.72 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:24.834554: step 59850, loss = 0.73 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:25.603184: step 59860, loss = 0.85 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:26.373904: step 59870, loss = 0.67 (1660.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:27.142431: step 59880, loss = 0.71 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:27.902042: step 59890, loss = 0.87 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:28.670813: step 59900, loss = 0.79 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:29.448111: step 59910, loss = 0.75 (1646.7 examples/sec; 0.078 sec/batch)
2017-05-05 18:42:30.217601: step 59920, loss = 0.82 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:30.990244: step 59930, loss = 0.86 (1656.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:31.752490: step 59940, loss = 0.76 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:32.520532: step 59950, loss = 0.73 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:33.284023: step 59960, loss = 0.71 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:34.050350: step 59970, loss = 0.75 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:34.812231: step 59980, loss = 0.70 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:35.575959: step 59990, loss = 0.79 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:36.337344: step 60000, loss = 0.65 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:37.099502: step 60010, loss = 0.79 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:37.864843: step 60020, loss = 0.66 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:38.630604: step 60030, loss = 0.88 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:39.405238: step 60040, loss = 0.74 (1652.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:40.167748: step 60050, loss = 1.01 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:40.927196: step 60060, loss = 0.68 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:41.681483: step 60070, loss = 0.89 (1697.0 examples/sec; 0.075 sec/batch)
2017-05-05 18:42:42.452422: step 60080, loss = 0.77 (1660.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:43.220404: step 60090, loss = 0.69 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:43.974177: step 60100, loss = 0.78 (1698.1 examples/sec; 0.075 sec/batch)
2017-05-05 18:42:44.741143: step 60110, loss = 1.01 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:45.508605: step 60120, loss = 0.79 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:46.290872: step 60130, loss = 0.75 (1636.3 examples/sec; 0.078 sec/batch)
2017-05-05 18:42:47.053285: step 60140, loss = 0.86 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:47.812229: step 60150, loss = 0.83 (1686.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:48.576244: step 60160, loss = 0.80 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:49.337502: step 60170, loss = 0.89 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:50.099533: step 60180, loss = 0.73 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:50.870973: step 60190, loss = 0.87 (1659.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:51.636026: step 60200, loss = 0.79 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:52.401769: step 60210, loss = 0.92 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:53.164086: step 60220, loss = 0.90 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:53.945581: step 60230, loss = 0.84 (1637.9 examples/sec; 0.078 sec/batch)
2017-05-05 18:42:54.714845: step 60240, loss = 0.61 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:55.479689: step 60250, loss = 0.76 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:56.244925: step 60260, loss = 0.74 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:57.005918: step 60270, loss = 0.80 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:42:57.774378: step 60280, loss = 0.72 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:58.544207: step 60290, loss = 0.64 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:42:59.301549: step 60300, loss = 0.65 (1690.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:00.055888: step 60310, loss = 0.83 (1696.9 examples/sec; 0.075 sec/batch)
2017-05-05 18:43:00.823810: step 60320, loss = 0.78 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:01.583484: step 60330, loss = 0.68 (1684.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:02.353048: step 60340, loss = 0.68 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:03.114032: step 60350, loss = 0.68 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:03.870339: step 60360, loss = 0.70 (1692.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:04.628449: step 60370, loss = 0.79 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:05.398011: step 60380, loss = 0.76 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:06.261697: step 60390, loss = 0.58 (1482.0 examples/sec; 0.086 sec/batch)
2017-05-05 18:43:06.934471: step 60400, loss = 0.71 (1902.5 examples/sec; 0.067 sec/batch)
2017-05-05 18:43:07.696456: step 60410, loss = 0.89 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:08.465876: step 60420, loss = 0.70 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:09.229692: step 60430, loss = 0.91 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:09.993186: step 60440, loss = 0.85 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:10.756399: step 60450, loss = 0.68 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:11.521437: step 60460, loss = 0.73 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:12.289280: step 60470, loss = 0.65 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:13.055481: step 60480, loss = 0.83 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:13.827918: step 60490, loss = 0.75 (1657.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:14.599095: step 60500, loss = 0.86 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:15.361094: step 60510, loss = 0.79 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:16.121608: step 60520, loss = 0.75 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:16.884500: step 60530, loss = 0.80 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:17.649142: step 60540, loss = 0.81 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:18.422600: step 60550, loss = 0.89 (1654.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:19.192062: step 60560, loss = 0.83 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:19.948468: step 60570, loss = 0.67 (1692.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:20.716273: step 60580, loss = 0.72 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:21.479707: step 60590, loss = 0.78 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:22.242736: step 60600, loss = 0.70 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:23.009330: step 60610, loss = 0.75 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:23.760566: step 60620, loss = 0.89 (1703.9 examples/sec; 0.075 sec/batch)
2017-05-05 18:43:24.527338: step 60630, loss = 0.77 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:25.294986: step 60640, loss = 0.83 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:26.062498: step 60650, loss = 0.93 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:26.826913: step 60660, loss = 0.73 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:27.586852: step 60670, loss = 0.75 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:28.354440: step 60680, loss = 0.81 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:29.119823: step 60690, loss = 0.76 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:29.885403: step 60700, loss = 0.81 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:30.651171: step 60710, loss = 1.05 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:31.408791: step 60720, loss = 0.64 (1689.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:32.167893: step 60730, loss = 0.70 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:32.924268: step 60740, loss = 0.87 (1692.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:33.689961: step 60750, loss = 0.86 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:34.452976: step 60760, loss = 0.65 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:35.222727: step 60770, loss = 0.72 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:35.973808: step 60780, loss = 0.86 (1704.2 examples/sec; 0.075 sec/batch)
2017-05-05 18:43:36.743079: step 60790, loss = 0.75 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:37.511557: step 60800, loss = 0.76 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:38.271680: step 60810, loss = 0.60 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:39.038849: step 60820, loss = 0.64 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:39.793223: step 60830, loss = 0.79 (1696.8 examples/sec; 0.075 sec/batch)
2017-05-05 18:43:40.562562: step 60840, loss = 0.73 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:41.322186: step 60850, loss = 0.82 (1685.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:42.087297: step 60860, loss = 0.84 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:42.855164: step 60870, loss = 0.89 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:43.615088: step 60880, loss = 0.86 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:44.378061: step 60890, loss = 0.89 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:45.142575: step 60900, loss = 0.83 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:45.909241: step 60910, loss = 0.62 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:46.673748: step 60920, loss = 0.69 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:47.435990: step 60930, loss = 0.70 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:48.200895: step 60940, loss = 0.85 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:48.958244: step 60950, loss = 0.88 (1690.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:49.723674: step 60960, loss = 0.72 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:50.493699: step 60970, loss = 0.81 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:51.253511: step 60980, loss = 0.70 (1684.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:52.014652: step 60990, loss = 0.94 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:52.786230: step 61000, loss = 0.80 (1658.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:53.551416: step 61010, loss = 0.66 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:54.322750: step 61020, loss = 0.78 (1659.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:55.088677: step 61030, loss = 0.90 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:55.849950: step 61040, loss = 0.71 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:56.620667: step 61050, loss = 0.71 (1660.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:57.387973: step 61060, loss = 0.78 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:58.151928: step 61070, loss = 0.75 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:43:58.921477: step 61080, loss = 0.73 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:43:59.675303: step 61090, loss = 0.76 (1698.0 examples/sec; 0.075 sec/batch)
2017-05-05 18:44:00.438944: step 61100, loss = 0.87 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:01.204252: step 61110, loss = 0.87 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:44:01.959831: step 61120, loss = 0.97 (1694.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:02.724896: step 61130, loss = 0.82 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:44:03.484891: step 61140, loss = 0.90 (1684.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:04.247971: step 61150, loss = 0.85 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:05.007968: step 61160, loss = 0.73 (1684.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:05.775248: step 61170, loss = 0.70 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:44:06.539486: step 61180, loss = 0.86 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:07.309498: step 61190, loss = 0.69 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:44:08.070692: step 61200, loss = 0.77 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:08.838137: step 61210, loss = 0.81 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:44:09.609607: step 61220, loss = 0.89 (1659.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:44:10.380268: step 61230, loss = 0.86 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:44:11.150936: step 61240, loss = 0.96 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:44:11.919668: step 61250, loss = 0.69 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:44:12.682849: step 61260, loss = 0.86 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:13.452411: step 61270, loss = 0.85 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:44:14.220484: step 61280, loss = 0.78 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:44:14.988873: step 61290, loss = 0.74 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:44:15.741451: step 61300, loss = 0.74 (1700.8 examples/sec; 0.075 sec/batch)
2017-05-05 18:44:16.503374: step 61310, loss = 0.83 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:17.269085: step 61320, loss = 0.78 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:44:18.033007: step 61330, loss = 0.82 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:18.798452: step 61340, loss = 0.78 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:44:19.562155: step 61350, loss = 0.86 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:20.326700: step 61360, loss = 0.72 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:21.096817: step 61370, loss = 0.93 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:44:21.959428: step 61380, loss = 0.75 (1483.9 examples/sec; 0.086 sec/batch)
2017-05-05 18:44:22.634531: step 61390, loss = 0.72 (1896.0 examples/sec; 0.068 sec/batch)
2017-05-05 18:44:23.399179: step 61400, loss = 0.79 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:24.154519: step 61410, loss = 0.74 (1694.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:24.923725: step 61420, loss = 0.82 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:44:25.688777: step 61430, loss = 0.69 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:44:26.449922: step 61440, loss = 0.77 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:27.220417: step 61450, loss = 0.95 (1661.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:44:27.976981: step 61460, loss = 0.65 (1691.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:28.742192: step 61470, loss = 0.77 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:44:29.507282: step 61480, loss = 0.74 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:44:30.267995: step 61490, loss = 0.68 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:31.033570: step 61500, loss = 0.80 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:44:31.790928: step 61510, loss = 0.73 (1690.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:32.560014: step 61520, loss = 0.62 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:44:33.322126: step 61530, loss = 0.79 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:34.086653: step 61540, loss = 0.75 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:34.856069: step 61550, loss = 0.92 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:44:35.612865: step 61560, loss = 0.69 (1691.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:36.373355: step 61570, loss = 0.74 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:37.138412: step 61580, loss = 0.77 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:44:37.901245: step 61590, loss = 0.86 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:38.678858: step 61600, loss = 0.76 (1646.1 examples/sec; 0.078 sec/batch)
2017-05-05 18:44:39.436031: step 61610, loss = 0.90 (1690.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:40.206239: step 61620, loss = 0.78 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:44:40.965394: step 61630, loss = 0.68 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:41.722993: step 61640, loss = 0.76 (1689.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:42.486588: step 61650, loss = 0.81 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:43.249046: step 61660, loss = 0.81 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:44.002541: step 61670, loss = 0.93 (1698.7 examples/sec; 0.075 sec/batch)
2017-05-05 18:44:44.767944: step 61680, loss = 0.73 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:44:45.532509: step 61690, loss = 0.90 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:46.291279: step 61700, loss = 0.75 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:47.054974: step 61710, loss = 0.71 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:47.808307: step 61720, loss = 0.88 (1699.1 examples/sec; 0.075 sec/batch)
2017-05-05 18:44:48.568882: step 61730, loss = 0.86 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:49.333899: step 61740, loss = 0.78 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:44:50.100886: step 61750, loss = 0.79 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:44:50.860409: step 61760, loss = 0.88 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:51.622344: step 61770, loss = 0.72 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:52.395310: step 61780, loss = 0.70 (1656.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:44:53.151927: step 61790, loss = 0.67 (1691.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:53.923681: step 61800, loss = 0.90 (1658.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:44:54.683956: step 61810, loss = 0.72 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:55.446706: step 61820, loss = 0.76 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:56.208860: step 61830, loss = 0.68 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:56.968283: step 61840, loss = 0.94 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:57.736135: step 61850, loss = 0.98 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:44:58.500264: step 61860, loss = 0.70 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:44:59.266710: step 61870, loss = 0.79 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:00.025895: step 61880, loss = 0.65 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:00.790596: step 61890, loss = 0.92 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:01.565695: step 61900, loss = 0.78 (1651.4 examples/sec; 0.078 sec/batch)
2017-05-05 18:45:02.330341: step 61910, loss = 0.73 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:03.097421: step 61920, loss = 0.74 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:03.854555: step 61930, loss = 0.79 (1690.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:04.613378: step 61940, loss = 0.74 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:05.380687: step 61950, loss = 0.61 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:06.141011: step 61960, loss = 0.79 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:06.904178: step 61970, loss = 0.73 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:07.658341: step 61980, loss = 0.67 (1697.2 examples/sec; 0.075 sec/batch)
2017-05-05 18:45:08.419809: step 61990, loss = 0.75 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:09.185580: step 62000, loss = 0.82 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:09.952701: step 62010, loss = 0.71 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:10.716240: step 62020, loss = 0.81 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:11.475507: step 62030, loss = 0.75 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:12.241416: step 62040, loss = 0.63 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:13.008997: step 62050, loss = 0.87 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:13.774298: step 62060, loss = 0.70 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:14.537760: step 62070, loss = 0.74 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:15.305347: step 62080, loss = 0.58 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:16.062697: step 62090, loss = 0.77 (1690.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:16.823206: step 62100, loss = 0.98 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:17.591306: step 62110, loss = 0.70 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:18.359294: step 62120, loss = 0.81 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:19.123682: step 62130, loss = 0.72 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:19.879919: step 62140, loss = 0.73 (1692.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:20.648666: step 62150, loss = 1.03 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:21.411694: step 62160, loss = 0.59 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:22.184702: step 62170, loss = 0.73 (1655.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:22.958981: step 62180, loss = 0.81 (1653.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:23.715458: step 62190, loss = 0.93 (1692.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:24.485272: step 62200, loss = 0.84 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:25.257509: step 62210, loss = 0.86 (1657.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:26.027042: step 62220, loss = 0.76 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:26.795952: step 62230, loss = 0.81 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:27.554993: step 62240, loss = 0.70 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:28.319637: step 62250, loss = 0.77 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:29.086168: step 62260, loss = 0.67 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:29.845252: step 62270, loss = 0.89 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:30.612459: step 62280, loss = 0.71 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:31.375327: step 62290, loss = 0.88 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:32.133868: step 62300, loss = 0.80 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:32.898926: step 62310, loss = 0.76 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:33.668092: step 62320, loss = 0.58 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:34.436327: step 62330, loss = 0.68 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:35.202711: step 62340, loss = 0.79 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:35.954997: step 62350, loss = 0.68 (1701.5 examples/sec; 0.075 sec/batch)
2017-05-05 18:45:36.718930: step 62360, loss = 0.69 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:37.584295: step 62370, loss = 0.73 (1479.2 examples/sec; 0.087 sec/batch)
2017-05-05 18:45:38.260658: step 62380, loss = 0.76 (1892.5 examples/sec; 0.068 sec/batch)
2017-05-05 18:45:39.023527: step 62390, loss = 0.82 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:39.777210: step 62400, loss = 0.91 (1698.3 examples/sec; 0.075 sec/batch)
2017-05-05 18:45:40.535547: step 62410, loss = 1.05 (1687.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:41.297317: step 62420, loss = 0.81 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:42.051319: step 62430, loss = 0.97 (1697.6 examples/sec; 0.075 sec/batch)
2017-05-05 18:45:42.811552: step 62440, loss = 0.85 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:43.564786: step 62450, loss = 0.83 (1699.3 examples/sec; 0.075 sec/batch)
2017-05-05 18:45:44.330689: step 62460, loss = 0.71 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:45.091187: step 62470, loss = 0.87 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:45.862557: step 62480, loss = 0.75 (1659.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:46.623024: step 62490, loss = 0.70 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:47.395223: step 62500, loss = 0.75 (1657.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:48.157121: step 62510, loss = 0.71 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:48.924476: step 62520, loss = 0.66 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:49.696404: step 62530, loss = 0.68 (1658.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:50.464166: step 62540, loss = 0.76 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:51.231675: step 62550, loss = 0.89 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:51.988655: step 62560, loss = 0.89 (1690.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:52.754225: step 62570, loss = 0.66 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:53.517698: step 62580, loss = 0.82 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:54.288693: step 62590, loss = 0.71 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:55.058010: step 62600, loss = 0.66 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:55.814556: step 62610, loss = 0.73 (1691.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:56.578922: step 62620, loss = 0.78 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:57.344985: step 62630, loss = 0.83 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:58.108210: step 62640, loss = 0.77 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:45:58.876155: step 62650, loss = 0.63 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:45:59.637142: step 62660, loss = 0.83 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:00.399098: step 62670, loss = 0.83 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:01.163734: step 62680, loss = 1.00 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:01.925294: step 62690, loss = 0.88 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:02.691459: step 62700, loss = 0.74 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:46:03.450551: step 62710, loss = 0.66 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:04.215232: step 62720, loss = 0.77 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:05.000118: step 62730, loss = 0.71 (1630.8 examples/sec; 0.078 sec/batch)
2017-05-05 18:46:05.810019: step 62740, loss = 0.79 (1580.4 examples/sec; 0.081 sec/batch)
2017-05-05 18:46:06.576299: step 62750, loss = 0.65 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:46:07.337855: step 62760, loss = 0.93 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:08.103439: step 62770, loss = 0.81 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:46:08.874753: step 62780, loss = 0.74 (1659.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:46:09.642035: step 62790, loss = 0.80 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:46:10.400296: step 62800, loss = 0.95 (1688.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:11.161500: step 62810, loss = 0.72 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:11.924621: step 62820, loss = 0.77 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:12.690708: step 62830, loss = 0.74 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:46:13.463541: step 62840, loss = 0.64 (1656.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:46:14.228479: step 62850, loss = 0.87 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:14.999506: step 62860, loss = 0.83 (1660.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:46:15.749908: step 62870, loss = 0.78 (1705.7 examples/sec; 0.075 sec/batch)
2017-05-05 18:46:16.513417: step 62880, loss = 0.86 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:17.278529: step 62890, loss = 0.81 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:46:18.045857: step 62900, loss = 0.77 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:46:18.806423: step 62910, loss = 0.86 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:19.565882: step 62920, loss = 1.03 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:20.324057: step 62930, loss = 0.76 (1688.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:21.096004: step 62940, loss = 0.72 (1658.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:46:21.850470: step 62950, loss = 0.67 (1696.6 examples/sec; 0.075 sec/batch)
2017-05-05 18:46:22.619061: step 62960, loss = 0.77 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:46:23.383309: step 62970, loss = 0.76 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:24.145251: step 62980, loss = 0.73 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:24.917213: step 62990, loss = 0.70 (1658.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:46:25.682636: step 63000, loss = 0.91 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:46:26.448506: step 63010, loss = 0.94 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:46:27.215088: step 63020, loss = 0.75 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:46:27.965387: step 63030, loss = 1.00 (1706.0 examples/sec; 0.075 sec/batch)
2017-05-05 18:46:28.728955: step 63040, loss = 0.71 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:29.497998: step 63050, loss = 0.92 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:46:30.259885: step 63060, loss = 0.79 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:31.027599: step 63070, loss = 0.80 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:46:31.786157: step 63080, loss = 0.79 (1687.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:32.552411: step 63090, loss = 0.85 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:46:33.320178: step 63100, loss = 0.76 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:46:34.083299: step 63110, loss = 0.98 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:34.849769: step 63120, loss = 0.75 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:46:35.612549: step 63130, loss = 0.76 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:36.371761: step 63140, loss = 0.79 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:37.134437: step 63150, loss = 0.83 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:37.895891: step 63160, loss = 0.80 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:38.667332: step 63170, loss = 0.79 (1659.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:46:39.428349: step 63180, loss = 0.71 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:40.193482: step 63190, loss = 0.74 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:46:40.953757: step 63200, loss = 0.80 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:41.719631: step 63210, loss = 0.78 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:46:42.491020: step 63220, loss = 0.67 (1659.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:46:43.253118: step 63230, loss = 0.80 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:44.008582: step 63240, loss = 0.63 (1694.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:44.772245: step 63250, loss = 0.74 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:45.539202: step 63260, loss = 0.93 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:46:46.304411: step 63270, loss = 0.80 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:46:47.065255: step 63280, loss = 0.78 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:47.816377: step 63290, loss = 0.86 (1704.2 examples/sec; 0.075 sec/batch)
2017-05-05 18:46:48.588606: step 63300, loss = 0.74 (1657.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:46:49.358085: step 63310, loss = 0.66 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:46:50.123041: step 63320, loss = 0.76 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:50.894449: step 63330, loss = 1.07 (1659.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:46:51.649451: step 63340, loss = 0.72 (1695.4 examples/sec; 0.075 sec/batch)
2017-05-05 18:46:52.410786: step 63350, loss = 0.91 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:53.274002: step 63360, loss = 0.68 (1482.8 examples/sec; 0.086 sec/batch)
2017-05-05 18:46:53.945201: step 63370, loss = 0.85 (1907.0 examples/sec; 0.067 sec/batch)
2017-05-05 18:46:54.709303: step 63380, loss = 1.03 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:55.473744: step 63390, loss = 0.75 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:56.235246: step 63400, loss = 0.62 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:56.991115: step 63410, loss = 0.90 (1693.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:46:57.758911: step 63420, loss = 0.96 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:46:58.524287: step 63430, loss = 0.82 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:46:59.289477: step 63440, loss = 0.77 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:00.045972: step 63450, loss = 0.89 (1692.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:00.803276: step 63460, loss = 0.82 (1690.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:01.566437: step 63470, loss = 0.71 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:02.329569: step 63480, loss = 0.80 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:03.099575: step 63490, loss = 0.94 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:03.850987: step 63500, loss = 0.72 (1703.5 examples/sec; 0.075 sec/batch)
2017-05-05 18:47:04.617561: step 63510, loss = 0.65 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:05.384800: step 63520, loss = 0.78 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:06.153569: step 63530, loss = 0.85 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:06.919465: step 63540, loss = 0.81 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:07.679734: step 63550, loss = 0.66 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:08.450109: step 63560, loss = 0.69 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:09.215638: step 63570, loss = 0.84 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:09.981630: step 63580, loss = 0.88 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:10.750690: step 63590, loss = 0.81 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:11.505048: step 63600, loss = 0.69 (1696.8 examples/sec; 0.075 sec/batch)
2017-05-05 18:47:12.269297: step 63610, loss = 0.77 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:13.032798: step 63620, loss = 0.84 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:13.804194: step 63630, loss = 0.75 (1659.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:14.565864: step 63640, loss = 0.66 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:15.333896: step 63650, loss = 0.78 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:16.095834: step 63660, loss = 0.78 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:16.860768: step 63670, loss = 0.76 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:17.624834: step 63680, loss = 0.87 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:18.385877: step 63690, loss = 0.90 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:19.158789: step 63700, loss = 0.75 (1656.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:19.915153: step 63710, loss = 0.70 (1692.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:20.682729: step 63720, loss = 0.55 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:21.450429: step 63730, loss = 0.76 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:22.211550: step 63740, loss = 0.72 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:22.981494: step 63750, loss = 0.84 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:23.740222: step 63760, loss = 0.79 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:24.497346: step 63770, loss = 0.82 (1690.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:25.263137: step 63780, loss = 0.81 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:26.023576: step 63790, loss = 0.85 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:26.792099: step 63800, loss = 0.75 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:27.552930: step 63810, loss = 0.87 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:28.313530: step 63820, loss = 0.75 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:29.086879: step 63830, loss = 0.81 (1655.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:29.850522: step 63840, loss = 0.66 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:30.616191: step 63850, loss = 0.93 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:31.374386: step 63860, loss = 0.69 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:32.134487: step 63870, loss = 0.72 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:32.902529: step 63880, loss = 0.82 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:33.668143: step 63890, loss = 0.93 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:34.437243: step 63900, loss = 0.75 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:35.204521: step 63910, loss = 0.87 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:35.964749: step 63920, loss = 0.63 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:36.730579: step 63930, loss = 0.64 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:37.492873: step 63940, loss = 0.90 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:38.252161: step 63950, loss = 0.86 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:39.019015: step 63960, loss = 0.61 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:39.777200: step 63970, loss = 0.85 (1688.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:40.537459: step 63980, loss = 1.01 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:41.304398: step 63990, loss = 0.73 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:42.066155: step 64000, loss = 0.79 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:42.832009: step 64010, loss = 0.62 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:43.589731: step 64020, loss = 0.69 (1689.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:44.349962: step 64030, loss = 0.71 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:45.115353: step 64040, loss = 0.78 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:45.876462: step 64050, loss = 0.82 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:46.644317: step 64060, loss = 0.73 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:47.403885: step 64070, loss = 0.60 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:48.169426: step 64080, loss = 0.80 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:48.930566: step 64090, loss = 0.72 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:49.696512: step 64100, loss = 0.77 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:50.464612: step 64110, loss = 0.78 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:51.230208: step 64120, loss = 0.78 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:51.989232: step 64130, loss = 0.72 (1686.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:52.749529: step 64140, loss = 0.84 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:53.515032: step 64150, loss = 0.83 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:54.284049: step 64160, loss = 0.68 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:55.046300: step 64170, loss = 0.80 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:55.812464: step 64180, loss = 0.78 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:56.572457: step 64190, loss = 0.81 (1684.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:57.345020: step 64200, loss = 0.78 (1656.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:58.116007: step 64210, loss = 0.62 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:47:58.877115: step 64220, loss = 0.73 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:47:59.637610: step 64230, loss = 0.93 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:00.408256: step 64240, loss = 0.81 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:01.171511: step 64250, loss = 0.81 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:01.936063: step 64260, loss = 0.63 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:02.700017: step 64270, loss = 0.60 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:03.460925: step 64280, loss = 0.86 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:04.216690: step 64290, loss = 0.75 (1693.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:04.981980: step 64300, loss = 0.61 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:05.740062: step 64310, loss = 0.86 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:06.510033: step 64320, loss = 0.77 (1662.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:07.272939: step 64330, loss = 0.82 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:08.038525: step 64340, loss = 0.78 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:08.905107: step 64350, loss = 0.71 (1477.1 examples/sec; 0.087 sec/batch)
2017-05-05 18:48:09.574246: step 64360, loss = 0.83 (1912.9 examples/sec; 0.067 sec/batch)
2017-05-05 18:48:10.337620: step 64370, loss = 0.74 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:11.101751: step 64380, loss = 0.68 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:11.851284: step 64390, loss = 0.88 (1707.7 examples/sec; 0.075 sec/batch)
2017-05-05 18:48:12.621283: step 64400, loss = 0.76 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:13.386011: step 64410, loss = 0.74 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:14.157188: step 64420, loss = 0.78 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:14.925914: step 64430, loss = 0.67 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:15.689605: step 64440, loss = 0.85 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:16.451651: step 64450, loss = 0.85 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:17.221738: step 64460, loss = 0.70 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:17.983379: step 64470, loss = 0.92 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:18.748082: step 64480, loss = 0.80 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:19.506158: step 64490, loss = 0.79 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:20.273463: step 64500, loss = 0.75 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:21.042660: step 64510, loss = 0.72 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:21.800578: step 64520, loss = 0.82 (1688.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:22.566912: step 64530, loss = 0.93 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:23.324573: step 64540, loss = 0.82 (1689.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:24.088336: step 64550, loss = 0.71 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:24.860431: step 64560, loss = 0.76 (1657.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:25.619553: step 64570, loss = 0.96 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:26.385628: step 64580, loss = 0.75 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:27.155144: step 64590, loss = 0.76 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:27.909158: step 64600, loss = 0.81 (1697.6 examples/sec; 0.075 sec/batch)
2017-05-05 18:48:28.676669: step 64610, loss = 0.71 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:29.439081: step 64620, loss = 0.66 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:30.206892: step 64630, loss = 0.79 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:30.973429: step 64640, loss = 0.85 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:31.728797: step 64650, loss = 0.85 (1694.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:32.496392: step 64660, loss = 0.77 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:33.260546: step 64670, loss = 0.67 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:34.030382: step 64680, loss = 0.64 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:34.801560: step 64690, loss = 0.85 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:35.565609: step 64700, loss = 0.69 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:36.333086: step 64710, loss = 0.88 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:37.093796: step 64720, loss = 0.84 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:37.865145: step 64730, loss = 0.81 (1659.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:38.628764: step 64740, loss = 0.79 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:39.390323: step 64750, loss = 0.80 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:40.148416: step 64760, loss = 0.86 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:40.915926: step 64770, loss = 0.69 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:41.678456: step 64780, loss = 0.74 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:42.443625: step 64790, loss = 0.85 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:43.212515: step 64800, loss = 0.69 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:43.967528: step 64810, loss = 0.82 (1695.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:44.733307: step 64820, loss = 0.78 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:45.499080: step 64830, loss = 0.73 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:46.266937: step 64840, loss = 0.75 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:47.031382: step 64850, loss = 0.84 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:47.785593: step 64860, loss = 0.78 (1697.1 examples/sec; 0.075 sec/batch)
2017-05-05 18:48:48.551015: step 64870, loss = 0.86 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:49.328901: step 64880, loss = 0.85 (1645.5 examples/sec; 0.078 sec/batch)
2017-05-05 18:48:50.092122: step 64890, loss = 0.83 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:50.861879: step 64900, loss = 0.74 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:51.612863: step 64910, loss = 0.72 (1704.4 examples/sec; 0.075 sec/batch)
2017-05-05 18:48:52.377197: step 64920, loss = 0.84 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:53.145055: step 64930, loss = 0.86 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:53.905865: step 64940, loss = 0.89 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:54.677001: step 64950, loss = 0.64 (1659.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:55.441255: step 64960, loss = 0.77 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:56.201291: step 64970, loss = 0.80 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:56.967961: step 64980, loss = 0.98 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:57.731715: step 64990, loss = 0.87 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:48:58.506592: step 65000, loss = 0.90 (1651.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:48:59.270968: step 65010, loss = 0.82 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:49:00.025010: step 65020, loss = 0.62 (1697.5 examples/sec; 0.075 sec/batch)
2017-05-05 18:49:00.791518: step 65030, loss = 0.80 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:01.553142: step 65040, loss = 0.95 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:49:02.320926: step 65050, loss = 0.83 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:03.084235: step 65060, loss = 0.74 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:49:03.851106: step 65070, loss = 0.68 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:04.621832: step 65080, loss = 0.78 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:05.388958: step 65090, loss = 0.68 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:06.152744: step 65100, loss = 0.69 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:49:06.919230: step 65110, loss = 0.69 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:07.676683: step 65120, loss = 0.75 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:49:08.446000: step 65130, loss = 0.80 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:09.212269: step 65140, loss = 0.70 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:09.975945: step 65150, loss = 0.72 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:49:10.745388: step 65160, loss = 0.77 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:11.505182: step 65170, loss = 0.87 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:49:12.271853: step 65180, loss = 0.76 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:13.038573: step 65190, loss = 0.93 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:13.802789: step 65200, loss = 0.86 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:49:14.568084: step 65210, loss = 0.71 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:15.335374: step 65220, loss = 0.91 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:16.094106: step 65230, loss = 0.73 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:49:16.848549: step 65240, loss = 0.70 (1696.6 examples/sec; 0.075 sec/batch)
2017-05-05 18:49:17.615917: step 65250, loss = 0.75 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:18.382178: step 65260, loss = 0.98 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:19.145079: step 65270, loss = 0.80 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:49:19.902735: step 65280, loss = 0.82 (1689.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:49:20.670529: step 65290, loss = 0.62 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:21.440968: step 65300, loss = 0.69 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:22.201783: step 65310, loss = 0.83 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:49:22.967847: step 65320, loss = 0.71 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:23.723107: step 65330, loss = 0.77 (1694.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:49:24.591634: step 65340, loss = 0.89 (1473.8 examples/sec; 0.087 sec/batch)
2017-05-05 18:49:25.256211: step 65350, loss = 0.74 (1926.0 examples/sec; 0.066 sec/batch)
2017-05-05 18:49:26.027090: step 65360, loss = 0.81 (1660.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:26.789706: step 65370, loss = 0.79 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:49:27.555186: step 65380, loss = 0.79 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:28.323028: step 65390, loss = 0.86 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:29.093756: step 65400, loss = 0.97 (1660.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:29.860467: step 65410, loss = 0.89 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:30.627440: step 65420, loss = 0.87 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:31.395432: step 65430, loss = 0.88 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:32.154461: step 65440, loss = 0.71 (1686.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:49:32.927815: step 65450, loss = 0.94 (1655.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:33.699234: step 65460, loss = 0.71 (1659.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:34.468523: step 65470, loss = 0.68 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:35.238984: step 65480, loss = 0.79 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:35.993318: step 65490, loss = 0.74 (1696.9 examples/sec; 0.075 sec/batch)
2017-05-05 18:49:36.754479: step 65500, loss = 0.65 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:49:37.523989: step 65510, loss = 0.78 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:38.289709: step 65520, loss = 0.71 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:39.051136: step 65530, loss = 0.74 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:49:39.815740: step 65540, loss = 0.82 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:49:40.581043: step 65550, loss = 0.74 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:41.349037: step 65560, loss = 0.59 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:42.104398: step 65570, loss = 0.89 (1694.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:49:42.872215: step 65580, loss = 0.74 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:43.626749: step 65590, loss = 0.78 (1696.4 examples/sec; 0.075 sec/batch)
2017-05-05 18:49:44.395694: step 65600, loss = 0.69 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:45.158162: step 65610, loss = 0.68 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:49:45.927187: step 65620, loss = 0.84 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:46.694593: step 65630, loss = 0.87 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:47.453696: step 65640, loss = 0.74 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:49:48.220147: step 65650, loss = 0.86 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:48.989525: step 65660, loss = 0.83 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:49.761012: step 65670, loss = 0.75 (1659.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:50.521409: step 65680, loss = 0.80 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:49:51.291684: step 65690, loss = 0.77 (1661.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:52.047114: step 65700, loss = 0.76 (1694.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:49:52.813671: step 65710, loss = 0.80 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:53.585518: step 65720, loss = 0.69 (1658.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:54.348248: step 65730, loss = 0.84 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:49:55.111920: step 65740, loss = 0.83 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:49:55.866656: step 65750, loss = 0.88 (1696.0 examples/sec; 0.075 sec/batch)
2017-05-05 18:49:56.626839: step 65760, loss = 0.84 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:49:57.395409: step 65770, loss = 0.89 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:58.154023: step 65780, loss = 0.88 (1687.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:49:58.926391: step 65790, loss = 0.69 (1657.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:49:59.676690: step 65800, loss = 0.72 (1706.0 examples/sec; 0.075 sec/batch)
2017-05-05 18:50:00.437336: step 65810, loss = 0.81 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:01.197879: step 65820, loss = 0.71 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:01.974999: step 65830, loss = 0.69 (1647.1 examples/sec; 0.078 sec/batch)
2017-05-05 18:50:02.736906: step 65840, loss = 0.77 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:03.498954: step 65850, loss = 0.77 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:04.263030: step 65860, loss = 0.71 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:05.024154: step 65870, loss = 0.79 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:05.789882: step 65880, loss = 0.80 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:06.553296: step 65890, loss = 0.73 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:07.320210: step 65900, loss = 0.76 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:08.079312: step 65910, loss = 0.77 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:08.850323: step 65920, loss = 0.83 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:09.621164: step 65930, loss = 0.58 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:10.395860: step 65940, loss = 0.77 (1652.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:11.159088: step 65950, loss = 0.80 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:11.915695: step 65960, loss = 0.74 (1691.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:12.685944: step 65970, loss = 0.95 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:13.451422: step 65980, loss = 0.72 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:14.216479: step 65990, loss = 0.63 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:14.978807: step 66000, loss = 0.79 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:15.737112: step 66010, loss = 0.61 (1688.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:16.502492: step 66020, loss = 0.53 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:17.266189: step 66030, loss = 0.84 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:18.034909: step 66040, loss = 0.97 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:18.802896: step 66050, loss = 0.68 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:19.560656: step 66060, loss = 0.83 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:20.327681: step 66070, loss = 0.79 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:21.090421: step 66080, loss = 0.73 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:21.856803: step 66090, loss = 0.75 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:22.628248: step 66100, loss = 0.74 (1659.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:23.392767: step 66110, loss = 0.87 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:24.156131: step 66120, loss = 0.74 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:24.925723: step 66130, loss = 0.62 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:25.697656: step 66140, loss = 0.80 (1658.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:26.462185: step 66150, loss = 0.89 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:27.221706: step 66160, loss = 0.73 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:27.981097: step 66170, loss = 0.69 (1685.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:28.739735: step 66180, loss = 0.71 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:29.506728: step 66190, loss = 0.80 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:30.273105: step 66200, loss = 0.73 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:31.036976: step 66210, loss = 0.72 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:31.797872: step 66220, loss = 0.78 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:32.566927: step 66230, loss = 0.80 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:33.330461: step 66240, loss = 0.77 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:34.098128: step 66250, loss = 1.02 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:34.865632: step 66260, loss = 0.78 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:35.624179: step 66270, loss = 0.82 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:36.388331: step 66280, loss = 0.80 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:37.156244: step 66290, loss = 0.82 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:37.926802: step 66300, loss = 0.78 (1661.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:38.691739: step 66310, loss = 0.75 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:39.454032: step 66320, loss = 0.76 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:40.308811: step 66330, loss = 0.74 (1497.5 examples/sec; 0.085 sec/batch)
2017-05-05 18:50:40.979411: step 66340, loss = 0.67 (1908.8 examples/sec; 0.067 sec/batch)
2017-05-05 18:50:41.744256: step 66350, loss = 0.60 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:42.511341: step 66360, loss = 0.82 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:43.276587: step 66370, loss = 0.70 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:44.038807: step 66380, loss = 0.75 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:44.804384: step 66390, loss = 0.81 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:45.568424: step 66400, loss = 0.88 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:46.332810: step 66410, loss = 0.80 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:47.103708: step 66420, loss = 0.82 (1660.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:47.860389: step 66430, loss = 0.78 (1691.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:48.621847: step 66440, loss = 0.86 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:49.392038: step 66450, loss = 0.75 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:50.160008: step 66460, loss = 0.77 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:50.926498: step 66470, loss = 0.91 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:51.688862: step 66480, loss = 0.71 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:52.458564: step 66490, loss = 0.82 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:53.218720: step 66500, loss = 0.79 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:53.978872: step 66510, loss = 0.76 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:54.749723: step 66520, loss = 0.87 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:55.510036: step 66530, loss = 0.80 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:56.279010: step 66540, loss = 0.77 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:57.042899: step 66550, loss = 0.64 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:50:57.808679: step 66560, loss = 0.65 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:58.578511: step 66570, loss = 0.74 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:50:59.339887: step 66580, loss = 0.74 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:00.100813: step 66590, loss = 0.84 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:00.866530: step 66600, loss = 0.78 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:01.622821: step 66610, loss = 0.77 (1692.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:02.389328: step 66620, loss = 0.84 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:03.157462: step 66630, loss = 0.86 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:03.919344: step 66640, loss = 0.89 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:04.686513: step 66650, loss = 0.75 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:05.455552: step 66660, loss = 0.74 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:06.224457: step 66670, loss = 0.88 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:06.983611: step 66680, loss = 0.71 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:07.743918: step 66690, loss = 0.73 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:08.517467: step 66700, loss = 0.73 (1654.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:09.286291: step 66710, loss = 0.62 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:10.050780: step 66720, loss = 0.86 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:10.820422: step 66730, loss = 0.91 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:11.584208: step 66740, loss = 0.76 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:12.340375: step 66750, loss = 1.02 (1692.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:13.108197: step 66760, loss = 0.83 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:13.874185: step 66770, loss = 0.73 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:14.641995: step 66780, loss = 0.75 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:15.404332: step 66790, loss = 0.95 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:16.168545: step 66800, loss = 0.77 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:16.929639: step 66810, loss = 0.69 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:17.695662: step 66820, loss = 0.64 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:18.458983: step 66830, loss = 0.83 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:19.222042: step 66840, loss = 0.69 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:19.979162: step 66850, loss = 0.80 (1690.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:20.748848: step 66860, loss = 0.89 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:21.515064: step 66870, loss = 0.80 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:22.275686: step 66880, loss = 0.73 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:23.039432: step 66890, loss = 0.79 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:23.791537: step 66900, loss = 0.71 (1701.9 examples/sec; 0.075 sec/batch)
2017-05-05 18:51:24.564032: step 66910, loss = 0.81 (1657.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:25.336450: step 66920, loss = 0.92 (1657.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:26.104089: step 66930, loss = 0.74 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:26.867865: step 66940, loss = 0.75 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:27.618121: step 66950, loss = 0.82 (1706.1 examples/sec; 0.075 sec/batch)
2017-05-05 18:51:28.387769: step 66960, loss = 0.79 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:29.153632: step 66970, loss = 0.78 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:29.922937: step 66980, loss = 0.75 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:30.694294: step 66990, loss = 0.73 (1659.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:31.459131: step 67000, loss = 0.81 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:32.214165: step 67010, loss = 0.67 (1695.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:32.985950: step 67020, loss = 0.65 (1658.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:33.755069: step 67030, loss = 0.62 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:34.525449: step 67040, loss = 0.84 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:35.293162: step 67050, loss = 0.71 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:36.055675: step 67060, loss = 0.69 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:36.821599: step 67070, loss = 0.72 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:37.588279: step 67080, loss = 0.85 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:38.348529: step 67090, loss = 0.76 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:39.116108: step 67100, loss = 0.90 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:39.874348: step 67110, loss = 0.73 (1688.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:40.634906: step 67120, loss = 0.60 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:41.402099: step 67130, loss = 0.72 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:42.168928: step 67140, loss = 0.78 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:42.932513: step 67150, loss = 0.88 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:43.687777: step 67160, loss = 0.76 (1694.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:44.449838: step 67170, loss = 0.84 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:45.209489: step 67180, loss = 0.78 (1685.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:45.976095: step 67190, loss = 0.82 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:46.741201: step 67200, loss = 0.86 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:47.500172: step 67210, loss = 0.79 (1686.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:48.275242: step 67220, loss = 0.82 (1651.5 examples/sec; 0.078 sec/batch)
2017-05-05 18:51:49.042786: step 67230, loss = 0.93 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:49.811788: step 67240, loss = 0.82 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:50.579452: step 67250, loss = 0.87 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:51.344446: step 67260, loss = 0.66 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:52.105489: step 67270, loss = 0.74 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:52.870206: step 67280, loss = 0.70 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:53.641553: step 67290, loss = 0.78 (1659.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:54.402891: step 67300, loss = 0.82 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:51:55.170716: step 67310, loss = 0.79 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:56.029047: step 67320, loss = 0.72 (1491.3 examples/sec; 0.086 sec/batch)
2017-05-05 18:51:56.689417: step 67330, loss = 0.78 (1938.3 examples/sec; 0.066 sec/batch)
2017-05-05 18:51:57.459051: step 67340, loss = 0.88 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:58.227048: step 67350, loss = 0.99 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:59.000220: step 67360, loss = 0.71 (1655.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:51:59.756020: step 67370, loss = 0.80 (1693.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:00.528732: step 67380, loss = 0.90 (1656.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:01.286887: step 67390, loss = 0.70 (1688.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:02.051527: step 67400, loss = 0.88 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:02.824156: step 67410, loss = 0.79 (1656.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:03.584833: step 67420, loss = 0.84 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:04.349602: step 67430, loss = 0.76 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:05.116045: step 67440, loss = 0.79 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:05.885387: step 67450, loss = 0.84 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:06.651841: step 67460, loss = 0.87 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:07.412836: step 67470, loss = 0.70 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:08.176615: step 67480, loss = 0.78 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:08.941036: step 67490, loss = 0.86 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:09.700709: step 67500, loss = 0.75 (1684.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:10.471584: step 67510, loss = 0.86 (1660.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:11.242756: step 67520, loss = 0.70 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:11.999150: step 67530, loss = 0.82 (1692.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:12.764523: step 67540, loss = 0.61 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:13.531264: step 67550, loss = 0.87 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:14.297499: step 67560, loss = 0.78 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:15.060325: step 67570, loss = 0.71 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:15.818090: step 67580, loss = 0.80 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:16.583486: step 67590, loss = 0.81 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:17.346078: step 67600, loss = 0.86 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:18.110509: step 67610, loss = 0.79 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:18.882898: step 67620, loss = 0.77 (1657.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:19.631458: step 67630, loss = 0.79 (1709.9 examples/sec; 0.075 sec/batch)
2017-05-05 18:52:20.401823: step 67640, loss = 0.82 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:21.164721: step 67650, loss = 0.73 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:21.926504: step 67660, loss = 0.72 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:22.689745: step 67670, loss = 0.82 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:23.456154: step 67680, loss = 0.73 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:24.218455: step 67690, loss = 0.83 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:24.983696: step 67700, loss = 0.76 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:25.746960: step 67710, loss = 0.73 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:26.514176: step 67720, loss = 0.85 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:27.281011: step 67730, loss = 0.80 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:28.038419: step 67740, loss = 0.87 (1690.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:28.807555: step 67750, loss = 0.85 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:29.577562: step 67760, loss = 0.77 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:30.345351: step 67770, loss = 1.07 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:31.113154: step 67780, loss = 0.62 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:31.874430: step 67790, loss = 0.75 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:32.647110: step 67800, loss = 0.92 (1656.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:33.415741: step 67810, loss = 0.86 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:34.181289: step 67820, loss = 0.78 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:34.943056: step 67830, loss = 0.87 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:35.701807: step 67840, loss = 0.84 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:36.461750: step 67850, loss = 0.65 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:37.226665: step 67860, loss = 0.70 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:37.989174: step 67870, loss = 0.84 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:38.755888: step 67880, loss = 0.74 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:39.515985: step 67890, loss = 0.68 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:40.279696: step 67900, loss = 0.98 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:41.049773: step 67910, loss = 0.74 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:41.815853: step 67920, loss = 0.75 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:42.580629: step 67930, loss = 0.77 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:43.345777: step 67940, loss = 0.79 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:44.104395: step 67950, loss = 0.74 (1687.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:44.870142: step 67960, loss = 0.82 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:45.636899: step 67970, loss = 0.89 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:46.397034: step 67980, loss = 0.73 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:47.164889: step 67990, loss = 0.82 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:47.918168: step 68000, loss = 0.85 (1699.2 examples/sec; 0.075 sec/batch)
2017-05-05 18:52:48.683205: step 68010, loss = 0.71 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:49.447759: step 68020, loss = 0.81 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:50.209290: step 68030, loss = 0.77 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:50.976397: step 68040, loss = 0.81 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:51.736611: step 68050, loss = 0.79 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:52.497085: step 68060, loss = 0.76 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:53.265319: step 68070, loss = 0.80 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:54.027340: step 68080, loss = 0.71 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:54.797048: step 68090, loss = 0.91 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:55.554248: step 68100, loss = 0.75 (1690.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:56.322014: step 68110, loss = 0.93 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:57.098497: step 68120, loss = 0.94 (1648.5 examples/sec; 0.078 sec/batch)
2017-05-05 18:52:57.861212: step 68130, loss = 0.70 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:52:58.629250: step 68140, loss = 0.80 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:52:59.398784: step 68150, loss = 0.78 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:00.160457: step 68160, loss = 0.75 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:00.921492: step 68170, loss = 0.86 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:01.682097: step 68180, loss = 0.65 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:02.447060: step 68190, loss = 0.66 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:03.216945: step 68200, loss = 0.95 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:03.972708: step 68210, loss = 0.82 (1693.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:04.738193: step 68220, loss = 0.82 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:05.504015: step 68230, loss = 0.74 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:06.266242: step 68240, loss = 0.71 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:07.031444: step 68250, loss = 0.84 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:07.787140: step 68260, loss = 0.85 (1693.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:08.543966: step 68270, loss = 0.70 (1691.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:09.315631: step 68280, loss = 0.83 (1658.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:10.089633: step 68290, loss = 0.72 (1653.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:10.849193: step 68300, loss = 0.90 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:11.720871: step 68310, loss = 0.79 (1468.4 examples/sec; 0.087 sec/batch)
2017-05-05 18:53:12.375734: step 68320, loss = 0.75 (1954.6 examples/sec; 0.065 sec/batch)
2017-05-05 18:53:13.145069: step 68330, loss = 0.70 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:13.910541: step 68340, loss = 0.79 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:14.674124: step 68350, loss = 0.67 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:15.434781: step 68360, loss = 0.70 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:16.197743: step 68370, loss = 0.79 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:16.962179: step 68380, loss = 0.76 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:17.732693: step 68390, loss = 0.86 (1661.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:18.500607: step 68400, loss = 0.82 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:19.271794: step 68410, loss = 0.89 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:20.030347: step 68420, loss = 0.74 (1687.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:20.798585: step 68430, loss = 0.70 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:21.552335: step 68440, loss = 0.78 (1698.2 examples/sec; 0.075 sec/batch)
2017-05-05 18:53:22.319667: step 68450, loss = 0.65 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:23.084625: step 68460, loss = 0.79 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:23.847187: step 68470, loss = 0.74 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:24.612661: step 68480, loss = 0.90 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:25.386704: step 68490, loss = 0.75 (1653.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:26.149760: step 68500, loss = 0.65 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:26.910844: step 68510, loss = 0.82 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:27.670607: step 68520, loss = 0.70 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:28.439536: step 68530, loss = 0.91 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:29.204557: step 68540, loss = 0.93 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:29.964450: step 68550, loss = 0.68 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:30.729434: step 68560, loss = 0.70 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:31.491720: step 68570, loss = 0.84 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:32.250701: step 68580, loss = 0.83 (1686.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:33.017701: step 68590, loss = 0.69 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:33.777890: step 68600, loss = 0.91 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:34.547532: step 68610, loss = 0.98 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:35.317177: step 68620, loss = 0.74 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:36.071411: step 68630, loss = 0.83 (1697.1 examples/sec; 0.075 sec/batch)
2017-05-05 18:53:36.836266: step 68640, loss = 0.95 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:37.603451: step 68650, loss = 0.63 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:38.367008: step 68660, loss = 0.82 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:39.128806: step 68670, loss = 0.86 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:39.885620: step 68680, loss = 0.94 (1691.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:40.658073: step 68690, loss = 0.75 (1657.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:41.423165: step 68700, loss = 0.71 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:42.192832: step 68710, loss = 0.84 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:42.962954: step 68720, loss = 0.93 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:43.722421: step 68730, loss = 0.80 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:44.485859: step 68740, loss = 0.77 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:45.256753: step 68750, loss = 0.70 (1660.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:46.021272: step 68760, loss = 0.73 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:46.784688: step 68770, loss = 0.79 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:47.543897: step 68780, loss = 0.85 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:48.311544: step 68790, loss = 0.75 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:49.072913: step 68800, loss = 0.65 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:49.837519: step 68810, loss = 0.72 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:50.604785: step 68820, loss = 0.72 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:51.366602: step 68830, loss = 0.63 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:52.132218: step 68840, loss = 0.80 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:52.895564: step 68850, loss = 0.85 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:53.660684: step 68860, loss = 0.97 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:54.427616: step 68870, loss = 0.83 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:55.193833: step 68880, loss = 0.76 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:55.948697: step 68890, loss = 0.76 (1695.7 examples/sec; 0.075 sec/batch)
2017-05-05 18:53:56.708974: step 68900, loss = 0.79 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:57.474016: step 68910, loss = 0.73 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:53:58.235841: step 68920, loss = 0.67 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:58.997308: step 68930, loss = 0.77 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:53:59.757331: step 68940, loss = 0.60 (1684.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:00.527266: step 68950, loss = 0.76 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:01.295287: step 68960, loss = 0.80 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:02.059223: step 68970, loss = 0.73 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:02.821521: step 68980, loss = 0.88 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:03.579160: step 68990, loss = 0.85 (1689.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:04.343084: step 69000, loss = 0.69 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:05.107206: step 69010, loss = 0.77 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:05.872090: step 69020, loss = 0.80 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:06.634030: step 69030, loss = 0.87 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:07.395923: step 69040, loss = 0.73 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:08.157446: step 69050, loss = 0.70 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:08.925889: step 69060, loss = 0.68 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:09.694612: step 69070, loss = 0.69 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:10.462492: step 69080, loss = 0.62 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:11.238052: step 69090, loss = 0.61 (1650.4 examples/sec; 0.078 sec/batch)
2017-05-05 18:54:11.994775: step 69100, loss = 0.68 (1691.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:12.760147: step 69110, loss = 0.80 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:13.522253: step 69120, loss = 0.73 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:14.292959: step 69130, loss = 1.06 (1660.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:15.058988: step 69140, loss = 0.95 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:15.815525: step 69150, loss = 0.84 (1691.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:16.577975: step 69160, loss = 0.77 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:17.344273: step 69170, loss = 0.83 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:18.120595: step 69180, loss = 0.74 (1648.8 examples/sec; 0.078 sec/batch)
2017-05-05 18:54:18.886304: step 69190, loss = 0.88 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:19.645034: step 69200, loss = 0.77 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:20.410971: step 69210, loss = 0.92 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:21.184104: step 69220, loss = 0.87 (1655.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:21.957838: step 69230, loss = 0.81 (1654.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:22.723796: step 69240, loss = 0.73 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:23.482014: step 69250, loss = 0.71 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:24.246906: step 69260, loss = 0.92 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:25.019194: step 69270, loss = 0.78 (1657.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:25.786881: step 69280, loss = 0.79 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:26.552275: step 69290, loss = 0.63 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:27.418492: step 69300, loss = 0.88 (1477.7 examples/sec; 0.087 sec/batch)
2017-05-05 18:54:28.080387: step 69310, loss = 0.94 (1933.9 examples/sec; 0.066 sec/batch)
2017-05-05 18:54:28.847109: step 69320, loss = 0.78 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:29.607876: step 69330, loss = 0.79 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:30.377968: step 69340, loss = 0.74 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:31.143102: step 69350, loss = 0.67 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:31.893848: step 69360, loss = 0.78 (1705.0 examples/sec; 0.075 sec/batch)
2017-05-05 18:54:32.658318: step 69370, loss = 0.89 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:33.428792: step 69380, loss = 0.72 (1661.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:34.194972: step 69390, loss = 0.72 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:34.958271: step 69400, loss = 0.85 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:35.718402: step 69410, loss = 0.90 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:36.480389: step 69420, loss = 0.92 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:37.244166: step 69430, loss = 0.77 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:38.013422: step 69440, loss = 0.81 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:38.773179: step 69450, loss = 0.70 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:39.532234: step 69460, loss = 0.70 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:40.295975: step 69470, loss = 0.68 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:41.061587: step 69480, loss = 0.83 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:41.817953: step 69490, loss = 0.86 (1692.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:42.595088: step 69500, loss = 0.69 (1647.1 examples/sec; 0.078 sec/batch)
2017-05-05 18:54:43.366324: step 69510, loss = 0.80 (1659.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:44.135336: step 69520, loss = 0.83 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:44.904966: step 69530, loss = 0.98 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:45.674111: step 69540, loss = 0.62 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:46.434941: step 69550, loss = 0.75 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:47.205227: step 69560, loss = 0.74 (1661.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:47.958546: step 69570, loss = 0.69 (1699.1 examples/sec; 0.075 sec/batch)
2017-05-05 18:54:48.724422: step 69580, loss = 0.78 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:49.486256: step 69590, loss = 0.59 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:50.255635: step 69600, loss = 0.67 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:51.027510: step 69610, loss = 0.75 (1658.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:51.783677: step 69620, loss = 0.80 (1692.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:52.551188: step 69630, loss = 0.68 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:53.319221: step 69640, loss = 0.82 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:54.086277: step 69650, loss = 0.84 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:54.850242: step 69660, loss = 0.66 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:55.611172: step 69670, loss = 0.66 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:56.373801: step 69680, loss = 0.94 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:57.136566: step 69690, loss = 0.71 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:57.906338: step 69700, loss = 0.68 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:54:58.670789: step 69710, loss = 0.77 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:54:59.432583: step 69720, loss = 0.64 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:00.196225: step 69730, loss = 0.80 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:00.960728: step 69740, loss = 0.77 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:01.740646: step 69750, loss = 0.74 (1641.2 examples/sec; 0.078 sec/batch)
2017-05-05 18:55:02.519257: step 69760, loss = 0.86 (1643.9 examples/sec; 0.078 sec/batch)
2017-05-05 18:55:03.282115: step 69770, loss = 0.82 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:04.041542: step 69780, loss = 0.79 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:04.805962: step 69790, loss = 0.72 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:05.567533: step 69800, loss = 0.85 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:06.335743: step 69810, loss = 0.80 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:55:07.103976: step 69820, loss = 0.90 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:55:07.859765: step 69830, loss = 0.72 (1693.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:08.621791: step 69840, loss = 0.77 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:09.388308: step 69850, loss = 0.68 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:55:10.150576: step 69860, loss = 0.68 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:10.912451: step 69870, loss = 0.95 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:11.670216: step 69880, loss = 0.79 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:12.434070: step 69890, loss = 0.76 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:13.203074: step 69900, loss = 0.82 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:55:13.971317: step 69910, loss = 0.78 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:55:14.738547: step 69920, loss = 0.67 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:55:15.502712: step 69930, loss = 0.95 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:16.266162: step 69940, loss = 0.69 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:17.030613: step 69950, loss = 0.69 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:17.789844: step 69960, loss = 0.81 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:18.555893: step 69970, loss = 0.82 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:55:19.320841: step 69980, loss = 0.75 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:20.078803: step 69990, loss = 0.76 (1688.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:20.844499: step 70000, loss = 0.78 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:55:21.600265: step 70010, loss = 0.70 (1693.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:22.365554: step 70020, loss = 0.70 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:55:23.131709: step 70030, loss = 0.80 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:55:23.883299: step 70040, loss = 0.73 (1703.1 examples/sec; 0.075 sec/batch)
2017-05-05 18:55:24.654739: step 70050, loss = 0.68 (1659.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:55:25.420856: step 70060, loss = 0.83 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:55:26.186607: step 70070, loss = 0.79 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:55:26.944587: step 70080, loss = 0.87 (1688.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:27.703079: step 70090, loss = 0.89 (1687.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:28.470702: step 70100, loss = 0.73 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:55:29.241300: step 70110, loss = 0.81 (1661.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:55:30.010287: step 70120, loss = 0.78 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:55:30.773458: step 70130, loss = 0.71 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:31.530081: step 70140, loss = 0.64 (1691.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:32.292616: step 70150, loss = 0.71 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:33.066403: step 70160, loss = 0.83 (1654.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:55:33.834702: step 70170, loss = 0.77 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:55:34.596928: step 70180, loss = 0.74 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:35.359948: step 70190, loss = 0.68 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:36.121263: step 70200, loss = 0.89 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:36.881644: step 70210, loss = 0.80 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:37.651097: step 70220, loss = 0.92 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:55:38.418339: step 70230, loss = 0.70 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:55:39.191631: step 70240, loss = 0.61 (1655.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:55:39.947187: step 70250, loss = 0.75 (1694.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:40.710442: step 70260, loss = 0.97 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:41.471714: step 70270, loss = 0.69 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:42.231219: step 70280, loss = 0.73 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:43.101146: step 70290, loss = 0.73 (1471.4 examples/sec; 0.087 sec/batch)
2017-05-05 18:55:43.762939: step 70300, loss = 0.82 (1934.1 examples/sec; 0.066 sec/batch)
2017-05-05 18:55:44.535776: step 70310, loss = 0.79 (1656.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:55:45.305152: step 70320, loss = 0.80 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:55:46.067310: step 70330, loss = 0.58 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:46.838116: step 70340, loss = 0.71 (1660.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:55:47.602317: step 70350, loss = 0.83 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:48.368609: step 70360, loss = 0.88 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:55:49.131885: step 70370, loss = 0.77 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:49.893993: step 70380, loss = 0.77 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:50.660831: step 70390, loss = 0.65 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:55:51.429465: step 70400, loss = 0.85 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:55:52.190957: step 70410, loss = 0.74 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:52.954267: step 70420, loss = 0.77 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:53.718143: step 70430, loss = 0.91 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:54.489606: step 70440, loss = 0.78 (1659.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:55:55.258153: step 70450, loss = 0.95 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:55:56.015904: step 70460, loss = 0.63 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:56.778786: step 70470, loss = 0.75 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:55:57.551194: step 70480, loss = 0.95 (1657.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:55:58.327899: step 70490, loss = 0.72 (1648.0 examples/sec; 0.078 sec/batch)
2017-05-05 18:55:59.097458: step 70500, loss = 0.81 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:55:59.850666: step 70510, loss = 0.90 (1699.4 examples/sec; 0.075 sec/batch)
2017-05-05 18:56:00.615982: step 70520, loss = 0.69 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:01.377773: step 70530, loss = 0.76 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:02.143418: step 70540, loss = 0.77 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:02.911441: step 70550, loss = 0.68 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:03.668326: step 70560, loss = 0.78 (1691.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:04.437751: step 70570, loss = 0.74 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:05.207703: step 70580, loss = 0.67 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:06.010086: step 70590, loss = 0.82 (1595.2 examples/sec; 0.080 sec/batch)
2017-05-05 18:56:06.778336: step 70600, loss = 0.71 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:07.535123: step 70610, loss = 0.87 (1691.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:08.297078: step 70620, loss = 0.68 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:09.064154: step 70630, loss = 0.89 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:09.830727: step 70640, loss = 0.78 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:10.596725: step 70650, loss = 0.73 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:11.361473: step 70660, loss = 0.74 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:12.123810: step 70670, loss = 0.94 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:12.884679: step 70680, loss = 0.68 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:13.651831: step 70690, loss = 0.93 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:14.420409: step 70700, loss = 0.78 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:15.191005: step 70710, loss = 0.79 (1661.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:15.950743: step 70720, loss = 0.74 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:16.713257: step 70730, loss = 0.76 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:17.487096: step 70740, loss = 0.85 (1654.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:18.247651: step 70750, loss = 0.75 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:19.017864: step 70760, loss = 0.74 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:19.778212: step 70770, loss = 0.60 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:20.547748: step 70780, loss = 0.68 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:21.317584: step 70790, loss = 0.67 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:22.078085: step 70800, loss = 0.93 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:22.845941: step 70810, loss = 0.66 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:23.610282: step 70820, loss = 0.61 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:24.373165: step 70830, loss = 0.73 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:25.139247: step 70840, loss = 0.73 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:25.913388: step 70850, loss = 0.73 (1653.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:26.676972: step 70860, loss = 0.83 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:27.441305: step 70870, loss = 0.96 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:28.200919: step 70880, loss = 0.78 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:28.970686: step 70890, loss = 0.71 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:29.731794: step 70900, loss = 0.65 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:30.494077: step 70910, loss = 0.93 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:31.254218: step 70920, loss = 0.81 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:32.014986: step 70930, loss = 0.60 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:32.783291: step 70940, loss = 0.77 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:33.557387: step 70950, loss = 0.78 (1653.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:34.318892: step 70960, loss = 0.70 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:35.082815: step 70970, loss = 0.75 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:35.843066: step 70980, loss = 0.87 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:36.616126: step 70990, loss = 0.67 (1655.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:37.383192: step 71000, loss = 0.69 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:38.145116: step 71010, loss = 0.92 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:38.907543: step 71020, loss = 0.66 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:39.659852: step 71030, loss = 0.84 (1701.4 examples/sec; 0.075 sec/batch)
2017-05-05 18:56:40.427540: step 71040, loss = 0.84 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:41.202710: step 71050, loss = 0.80 (1651.3 examples/sec; 0.078 sec/batch)
2017-05-05 18:56:41.966835: step 71060, loss = 0.76 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:42.727119: step 71070, loss = 0.75 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:43.487511: step 71080, loss = 0.79 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:44.252826: step 71090, loss = 0.66 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:45.019838: step 71100, loss = 0.74 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:45.785953: step 71110, loss = 0.76 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:46.548464: step 71120, loss = 0.71 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:47.313078: step 71130, loss = 0.79 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:48.070910: step 71140, loss = 0.87 (1689.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:48.842071: step 71150, loss = 0.82 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:49.605848: step 71160, loss = 0.78 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:50.371262: step 71170, loss = 0.90 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:51.136186: step 71180, loss = 0.71 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:51.895105: step 71190, loss = 0.81 (1686.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:52.664678: step 71200, loss = 0.68 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:53.434385: step 71210, loss = 0.89 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:54.199351: step 71220, loss = 0.67 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:54.968699: step 71230, loss = 0.73 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:55.723740: step 71240, loss = 1.06 (1695.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:56.487913: step 71250, loss = 0.74 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:56:57.256464: step 71260, loss = 0.77 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:58.025541: step 71270, loss = 0.75 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:56:58.894250: step 71280, loss = 0.80 (1473.5 examples/sec; 0.087 sec/batch)
2017-05-05 18:56:59.557006: step 71290, loss = 0.76 (1931.3 examples/sec; 0.066 sec/batch)
2017-05-05 18:57:00.316043: step 71300, loss = 0.67 (1686.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:57:01.085973: step 71310, loss = 0.85 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:01.854919: step 71320, loss = 0.72 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:02.624806: step 71330, loss = 0.69 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:03.387219: step 71340, loss = 0.84 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:57:04.146277: step 71350, loss = 0.69 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:57:04.911604: step 71360, loss = 0.77 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:05.678235: step 71370, loss = 0.86 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:06.440861: step 71380, loss = 0.89 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:57:07.210062: step 71390, loss = 0.68 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:07.962281: step 71400, loss = 0.73 (1701.6 examples/sec; 0.075 sec/batch)
2017-05-05 18:57:08.727591: step 71410, loss = 0.76 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:09.489404: step 71420, loss = 0.74 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:57:10.258715: step 71430, loss = 0.72 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:11.029836: step 71440, loss = 0.73 (1659.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:11.788117: step 71450, loss = 0.81 (1688.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:57:12.558022: step 71460, loss = 0.86 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:13.325222: step 71470, loss = 0.67 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:14.092991: step 71480, loss = 0.76 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:14.858625: step 71490, loss = 0.80 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:15.614799: step 71500, loss = 0.82 (1692.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:57:16.382939: step 71510, loss = 0.80 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:17.149633: step 71520, loss = 0.71 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:17.917078: step 71530, loss = 0.79 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:18.685727: step 71540, loss = 0.65 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:19.449307: step 71550, loss = 0.59 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:57:20.215237: step 71560, loss = 0.73 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:20.983149: step 71570, loss = 0.71 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:21.747185: step 71580, loss = 0.88 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:57:22.518539: step 71590, loss = 0.87 (1659.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:23.280096: step 71600, loss = 0.68 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:57:24.034837: step 71610, loss = 0.69 (1695.9 examples/sec; 0.075 sec/batch)
2017-05-05 18:57:24.808002: step 71620, loss = 0.78 (1655.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:25.573641: step 71630, loss = 0.75 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:26.339939: step 71640, loss = 0.79 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:27.111189: step 71650, loss = 0.93 (1659.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:27.865759: step 71660, loss = 0.77 (1696.3 examples/sec; 0.075 sec/batch)
2017-05-05 18:57:28.640673: step 71670, loss = 0.79 (1651.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:29.406690: step 71680, loss = 0.78 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:30.165536: step 71690, loss = 0.76 (1686.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:57:30.926250: step 71700, loss = 0.57 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:57:31.691754: step 71710, loss = 0.63 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:32.454347: step 71720, loss = 0.74 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:57:33.219869: step 71730, loss = 0.85 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:33.992917: step 71740, loss = 0.71 (1655.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:34.759720: step 71750, loss = 0.74 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:35.530204: step 71760, loss = 0.66 (1661.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:36.283618: step 71770, loss = 0.80 (1698.9 examples/sec; 0.075 sec/batch)
2017-05-05 18:57:37.048399: step 71780, loss = 0.82 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:57:37.816650: step 71790, loss = 0.65 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:38.581159: step 71800, loss = 0.77 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:57:39.358596: step 71810, loss = 0.78 (1646.4 examples/sec; 0.078 sec/batch)
2017-05-05 18:57:40.123865: step 71820, loss = 0.73 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:40.890892: step 71830, loss = 0.91 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:41.649408: step 71840, loss = 0.79 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:57:42.414754: step 71850, loss = 0.86 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:43.182016: step 71860, loss = 0.79 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:43.940857: step 71870, loss = 0.81 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:57:44.716980: step 71880, loss = 0.79 (1649.2 examples/sec; 0.078 sec/batch)
2017-05-05 18:57:45.485256: step 71890, loss = 0.82 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:46.250836: step 71900, loss = 0.75 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:47.021724: step 71910, loss = 0.74 (1660.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:47.776884: step 71920, loss = 0.88 (1695.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:57:48.542191: step 71930, loss = 0.69 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:49.310874: step 71940, loss = 0.87 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:50.077158: step 71950, loss = 0.72 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:50.846638: step 71960, loss = 0.74 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:51.600856: step 71970, loss = 0.91 (1697.1 examples/sec; 0.075 sec/batch)
2017-05-05 18:57:52.366553: step 71980, loss = 0.72 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:53.130974: step 71990, loss = 0.72 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:57:53.891853: step 72000, loss = 0.74 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:57:54.658914: step 72010, loss = 0.95 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:55.421335: step 72020, loss = 0.63 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:57:56.176755: step 72030, loss = 0.80 (1694.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:57:56.938701: step 72040, loss = 1.00 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:57:57.704247: step 72050, loss = 0.76 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:58.468942: step 72060, loss = 0.65 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:57:59.234707: step 72070, loss = 0.74 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:57:59.999491: step 72080, loss = 0.91 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:00.760088: step 72090, loss = 0.83 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:01.527307: step 72100, loss = 0.74 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:58:02.315511: step 72110, loss = 0.86 (1623.9 examples/sec; 0.079 sec/batch)
2017-05-05 18:58:03.078850: step 72120, loss = 0.82 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:03.831413: step 72130, loss = 0.78 (1700.9 examples/sec; 0.075 sec/batch)
2017-05-05 18:58:04.597671: step 72140, loss = 0.71 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:58:05.368135: step 72150, loss = 0.68 (1661.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:58:06.132650: step 72160, loss = 0.85 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:06.897398: step 72170, loss = 0.78 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:07.656557: step 72180, loss = 0.74 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:08.433147: step 72190, loss = 0.69 (1648.2 examples/sec; 0.078 sec/batch)
2017-05-05 18:58:09.200806: step 72200, loss = 0.67 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:58:09.969101: step 72210, loss = 0.71 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:58:10.734791: step 72220, loss = 0.80 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:58:11.494801: step 72230, loss = 0.73 (1684.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:12.255623: step 72240, loss = 0.85 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:13.024151: step 72250, loss = 0.87 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:58:13.789402: step 72260, loss = 0.67 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:58:14.653237: step 72270, loss = 0.68 (1481.7 examples/sec; 0.086 sec/batch)
2017-05-05 18:58:15.324350: step 72280, loss = 0.79 (1907.3 examples/sec; 0.067 sec/batch)
2017-05-05 18:58:16.086073: step 72290, loss = 0.64 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:16.850968: step 72300, loss = 0.70 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:17.617389: step 72310, loss = 0.76 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:58:18.381895: step 72320, loss = 0.81 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:19.145951: step 72330, loss = 0.72 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:19.904728: step 72340, loss = 0.93 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:20.673440: step 72350, loss = 0.77 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:58:21.436130: step 72360, loss = 0.90 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:22.201684: step 72370, loss = 0.73 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:58:22.971303: step 72380, loss = 0.80 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:58:23.732526: step 72390, loss = 0.78 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:24.498658: step 72400, loss = 0.72 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:58:25.267396: step 72410, loss = 1.06 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:58:26.033834: step 72420, loss = 0.72 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:58:26.806923: step 72430, loss = 0.77 (1655.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:58:27.566922: step 72440, loss = 0.74 (1684.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:28.335167: step 72450, loss = 0.82 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:58:29.097432: step 72460, loss = 0.74 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:29.865118: step 72470, loss = 0.64 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:58:30.629693: step 72480, loss = 0.76 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:31.394169: step 72490, loss = 0.82 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:32.159087: step 72500, loss = 0.96 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:32.927305: step 72510, loss = 0.86 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:58:33.691287: step 72520, loss = 0.79 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:34.457208: step 72530, loss = 0.73 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:58:35.221352: step 72540, loss = 0.85 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:35.978279: step 72550, loss = 0.70 (1691.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:36.739800: step 72560, loss = 0.89 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:37.503174: step 72570, loss = 0.88 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:38.268353: step 72580, loss = 0.76 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:58:39.036673: step 72590, loss = 0.85 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:58:39.793442: step 72600, loss = 0.74 (1691.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:40.557142: step 72610, loss = 0.69 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:41.324674: step 72620, loss = 0.68 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:58:42.077335: step 72630, loss = 0.71 (1700.6 examples/sec; 0.075 sec/batch)
2017-05-05 18:58:42.843038: step 72640, loss = 0.71 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:58:43.604980: step 72650, loss = 0.65 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:44.370628: step 72660, loss = 0.71 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:58:45.143488: step 72670, loss = 0.75 (1656.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:58:45.908228: step 72680, loss = 0.69 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:46.670444: step 72690, loss = 0.61 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:47.435057: step 72700, loss = 0.66 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:48.197508: step 72710, loss = 0.82 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:48.961958: step 72720, loss = 0.84 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:49.727927: step 72730, loss = 0.82 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:58:50.489242: step 72740, loss = 0.66 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:51.251382: step 72750, loss = 0.78 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:52.008752: step 72760, loss = 0.74 (1690.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:52.773111: step 72770, loss = 0.84 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:53.539006: step 72780, loss = 0.63 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:58:54.310179: step 72790, loss = 0.87 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:58:55.072438: step 72800, loss = 0.62 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:55.830168: step 72810, loss = 0.84 (1689.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:56.605084: step 72820, loss = 0.83 (1651.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:58:57.375415: step 72830, loss = 0.72 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:58:58.141529: step 72840, loss = 0.73 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:58:58.899473: step 72850, loss = 0.73 (1688.8 examples/sec; 0.076 sec/batch)
2017-05-05 18:58:59.652148: step 72860, loss = 0.78 (1700.6 examples/sec; 0.075 sec/batch)
2017-05-05 18:59:00.423994: step 72870, loss = 0.85 (1658.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:01.190530: step 72880, loss = 0.79 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:01.952000: step 72890, loss = 0.94 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:59:02.718163: step 72900, loss = 0.74 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:03.478590: step 72910, loss = 0.81 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:59:04.243081: step 72920, loss = 0.68 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:59:05.009323: step 72930, loss = 0.72 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:05.769337: step 72940, loss = 0.64 (1684.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:59:06.545743: step 72950, loss = 0.65 (1648.6 examples/sec; 0.078 sec/batch)
2017-05-05 18:59:07.311122: step 72960, loss = 0.65 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:08.067438: step 72970, loss = 0.89 (1692.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:59:08.833457: step 72980, loss = 0.71 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:09.598630: step 72990, loss = 0.74 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:10.366254: step 73000, loss = 0.85 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:11.146566: step 73010, loss = 0.86 (1640.4 examples/sec; 0.078 sec/batch)
2017-05-05 18:59:11.892557: step 73020, loss = 0.94 (1715.8 examples/sec; 0.075 sec/batch)
2017-05-05 18:59:12.656486: step 73030, loss = 0.66 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:59:13.426217: step 73040, loss = 0.86 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:14.201564: step 73050, loss = 0.65 (1650.9 examples/sec; 0.078 sec/batch)
2017-05-05 18:59:14.957741: step 73060, loss = 0.80 (1692.7 examples/sec; 0.076 sec/batch)
2017-05-05 18:59:15.721769: step 73070, loss = 0.70 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:59:16.498572: step 73080, loss = 0.76 (1647.8 examples/sec; 0.078 sec/batch)
2017-05-05 18:59:17.275988: step 73090, loss = 0.71 (1646.5 examples/sec; 0.078 sec/batch)
2017-05-05 18:59:18.043115: step 73100, loss = 0.87 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:18.813949: step 73110, loss = 0.83 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:19.576929: step 73120, loss = 0.77 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 18:59:20.341099: step 73130, loss = 0.74 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:59:21.103771: step 73140, loss = 0.72 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:59:21.871996: step 73150, loss = 0.75 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:22.630709: step 73160, loss = 0.75 (1687.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:59:23.397551: step 73170, loss = 0.71 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:24.156636: step 73180, loss = 0.86 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:59:24.920878: step 73190, loss = 0.67 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:59:25.684448: step 73200, loss = 0.71 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:59:26.445342: step 73210, loss = 0.83 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 18:59:27.212271: step 73220, loss = 0.89 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:27.970166: step 73230, loss = 0.87 (1688.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:59:28.741395: step 73240, loss = 0.80 (1659.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:29.511588: step 73250, loss = 0.79 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:30.376269: step 73260, loss = 0.73 (1480.3 examples/sec; 0.086 sec/batch)
2017-05-05 18:59:31.048589: step 73270, loss = 0.82 (1903.8 examples/sec; 0.067 sec/batch)
2017-05-05 18:59:31.805791: step 73280, loss = 0.84 (1690.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:59:32.575353: step 73290, loss = 0.69 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:33.344885: step 73300, loss = 0.92 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:34.112425: step 73310, loss = 0.77 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:34.877084: step 73320, loss = 0.74 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 18:59:35.632104: step 73330, loss = 0.64 (1695.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:59:36.402114: step 73340, loss = 0.80 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:37.162913: step 73350, loss = 0.68 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:59:37.925065: step 73360, loss = 0.71 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 18:59:38.691441: step 73370, loss = 0.86 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:39.450220: step 73380, loss = 0.86 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 18:59:40.215177: step 73390, loss = 0.88 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:59:40.978319: step 73400, loss = 0.76 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 18:59:41.744698: step 73410, loss = 0.75 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:42.510651: step 73420, loss = 0.60 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:43.271410: step 73430, loss = 0.66 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:59:44.030538: step 73440, loss = 0.81 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:59:44.802916: step 73450, loss = 0.87 (1657.2 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:45.572625: step 73460, loss = 0.81 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:46.342943: step 73470, loss = 0.78 (1661.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:47.107042: step 73480, loss = 0.78 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 18:59:47.872656: step 73490, loss = 0.80 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:48.650700: step 73500, loss = 0.84 (1645.1 examples/sec; 0.078 sec/batch)
2017-05-05 18:59:49.416044: step 73510, loss = 0.80 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:50.185606: step 73520, loss = 0.67 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:50.953564: step 73530, loss = 0.78 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:51.720014: step 73540, loss = 0.69 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:52.487247: step 73550, loss = 0.84 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:53.260124: step 73560, loss = 0.82 (1656.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:54.032069: step 73570, loss = 0.63 (1658.1 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:54.801016: step 73580, loss = 0.80 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:55.560441: step 73590, loss = 0.71 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 18:59:56.331244: step 73600, loss = 0.82 (1660.6 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:57.098010: step 73610, loss = 0.77 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:57.864337: step 73620, loss = 0.73 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:58.637903: step 73630, loss = 0.90 (1654.7 examples/sec; 0.077 sec/batch)
2017-05-05 18:59:59.401192: step 73640, loss = 0.66 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:00.162198: step 73650, loss = 0.77 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:00.929632: step 73660, loss = 0.77 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:00:01.695123: step 73670, loss = 0.70 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:00:02.462317: step 73680, loss = 0.72 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:00:03.240817: step 73690, loss = 0.84 (1644.2 examples/sec; 0.078 sec/batch)
2017-05-05 19:00:03.996720: step 73700, loss = 0.71 (1693.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:04.761252: step 73710, loss = 0.86 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:05.526214: step 73720, loss = 0.82 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:06.296323: step 73730, loss = 0.85 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:00:07.054279: step 73740, loss = 0.66 (1688.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:07.806475: step 73750, loss = 0.54 (1701.7 examples/sec; 0.075 sec/batch)
2017-05-05 19:00:08.579272: step 73760, loss = 0.64 (1656.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:00:09.341500: step 73770, loss = 0.68 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:10.105568: step 73780, loss = 0.96 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:10.868640: step 73790, loss = 0.81 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:11.627153: step 73800, loss = 0.81 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:12.391425: step 73810, loss = 0.64 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:13.158166: step 73820, loss = 0.79 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:00:13.923506: step 73830, loss = 0.69 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:00:14.688916: step 73840, loss = 0.74 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:00:15.450011: step 73850, loss = 0.73 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:16.210350: step 73860, loss = 0.76 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:16.978406: step 73870, loss = 0.76 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:00:17.747834: step 73880, loss = 0.82 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:00:18.508492: step 73890, loss = 0.72 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:19.275516: step 73900, loss = 0.63 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:00:20.032041: step 73910, loss = 0.59 (1692.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:20.799316: step 73920, loss = 0.86 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:00:21.573908: step 73930, loss = 0.82 (1652.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:00:22.341846: step 73940, loss = 0.88 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:00:23.106087: step 73950, loss = 0.66 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:23.863521: step 73960, loss = 0.91 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:24.633282: step 73970, loss = 0.68 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:00:25.400097: step 73980, loss = 0.88 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:00:26.166681: step 73990, loss = 0.90 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:00:26.930664: step 74000, loss = 0.79 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:27.692270: step 74010, loss = 0.89 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:28.457287: step 74020, loss = 0.66 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:00:29.228244: step 74030, loss = 0.78 (1660.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:00:29.992604: step 74040, loss = 0.63 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:30.756486: step 74050, loss = 0.75 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:31.519808: step 74060, loss = 0.79 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:32.283168: step 74070, loss = 0.82 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:33.043264: step 74080, loss = 0.75 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:33.807946: step 74090, loss = 0.89 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:34.573590: step 74100, loss = 0.75 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:00:35.342194: step 74110, loss = 0.69 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:00:36.100427: step 74120, loss = 0.62 (1688.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:36.865563: step 74130, loss = 0.85 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:00:37.628265: step 74140, loss = 0.85 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:38.394317: step 74150, loss = 0.75 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:00:39.152851: step 74160, loss = 0.76 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:39.901977: step 74170, loss = 0.72 (1708.7 examples/sec; 0.075 sec/batch)
2017-05-05 19:00:40.664573: step 74180, loss = 0.68 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:41.437252: step 74190, loss = 0.75 (1656.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:00:42.193958: step 74200, loss = 0.76 (1691.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:42.960808: step 74210, loss = 0.72 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:00:43.714876: step 74220, loss = 0.69 (1697.5 examples/sec; 0.075 sec/batch)
2017-05-05 19:00:44.479062: step 74230, loss = 0.84 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:45.241045: step 74240, loss = 0.75 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:46.098193: step 74250, loss = 0.84 (1493.3 examples/sec; 0.086 sec/batch)
2017-05-05 19:00:46.765548: step 74260, loss = 0.80 (1918.0 examples/sec; 0.067 sec/batch)
2017-05-05 19:00:47.523980: step 74270, loss = 0.70 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:48.290264: step 74280, loss = 0.83 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:00:49.053492: step 74290, loss = 0.80 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:49.823572: step 74300, loss = 0.74 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:00:50.586056: step 74310, loss = 0.80 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:51.350910: step 74320, loss = 0.72 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:52.104527: step 74330, loss = 0.78 (1698.5 examples/sec; 0.075 sec/batch)
2017-05-05 19:00:52.877709: step 74340, loss = 0.82 (1655.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:00:53.644769: step 74350, loss = 0.84 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:00:54.415089: step 74360, loss = 0.78 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:00:55.183513: step 74370, loss = 0.72 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:00:55.937601: step 74380, loss = 0.75 (1697.4 examples/sec; 0.075 sec/batch)
2017-05-05 19:00:56.699500: step 74390, loss = 0.80 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:57.463231: step 74400, loss = 0.81 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:58.226225: step 74410, loss = 0.72 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:00:58.991569: step 74420, loss = 0.78 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:00:59.739238: step 74430, loss = 0.59 (1712.0 examples/sec; 0.075 sec/batch)
2017-05-05 19:01:00.507318: step 74440, loss = 0.85 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:01.270930: step 74450, loss = 0.78 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:02.048350: step 74460, loss = 0.68 (1646.5 examples/sec; 0.078 sec/batch)
2017-05-05 19:01:02.808119: step 74470, loss = 0.67 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:03.563544: step 74480, loss = 0.82 (1694.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:04.326297: step 74490, loss = 0.85 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:05.091654: step 74500, loss = 0.78 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:05.853385: step 74510, loss = 0.81 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:06.616482: step 74520, loss = 0.76 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:07.378650: step 74530, loss = 0.75 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:08.147135: step 74540, loss = 0.63 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:08.917883: step 74550, loss = 0.69 (1660.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:09.690153: step 74560, loss = 0.94 (1657.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:10.455981: step 74570, loss = 0.81 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:11.220109: step 74580, loss = 0.71 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:11.973824: step 74590, loss = 0.81 (1698.3 examples/sec; 0.075 sec/batch)
2017-05-05 19:01:12.742466: step 74600, loss = 0.74 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:13.505395: step 74610, loss = 0.85 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:14.272223: step 74620, loss = 0.63 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:15.035086: step 74630, loss = 0.62 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:15.790807: step 74640, loss = 0.67 (1693.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:16.559979: step 74650, loss = 0.59 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:17.324538: step 74660, loss = 1.00 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:18.087796: step 74670, loss = 0.78 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:18.852784: step 74680, loss = 0.68 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:19.606563: step 74690, loss = 0.68 (1698.1 examples/sec; 0.075 sec/batch)
2017-05-05 19:01:20.369301: step 74700, loss = 0.87 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:21.135102: step 74710, loss = 0.82 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:21.893087: step 74720, loss = 0.93 (1688.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:22.663504: step 74730, loss = 0.73 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:23.420322: step 74740, loss = 0.76 (1691.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:24.188164: step 74750, loss = 0.88 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:24.958376: step 74760, loss = 0.76 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:25.728568: step 74770, loss = 0.84 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:26.484934: step 74780, loss = 0.84 (1692.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:27.256852: step 74790, loss = 0.56 (1658.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:28.017786: step 74800, loss = 0.82 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:28.785740: step 74810, loss = 0.79 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:29.554540: step 74820, loss = 0.76 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:30.329314: step 74830, loss = 0.72 (1652.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:31.091683: step 74840, loss = 0.75 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:31.852560: step 74850, loss = 0.71 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:32.612461: step 74860, loss = 0.73 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:33.378199: step 74870, loss = 0.83 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:34.139642: step 74880, loss = 0.75 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:34.905139: step 74890, loss = 0.74 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:35.665282: step 74900, loss = 0.72 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:36.429427: step 74910, loss = 0.91 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:37.194605: step 74920, loss = 0.67 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:37.956827: step 74930, loss = 0.80 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:38.723610: step 74940, loss = 0.78 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:39.480883: step 74950, loss = 0.59 (1690.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:40.244686: step 74960, loss = 0.75 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:41.008912: step 74970, loss = 0.75 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:41.771242: step 74980, loss = 0.78 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:42.532341: step 74990, loss = 0.99 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:43.298817: step 75000, loss = 0.77 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:44.052300: step 75010, loss = 0.76 (1698.8 examples/sec; 0.075 sec/batch)
2017-05-05 19:01:44.820580: step 75020, loss = 0.71 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:45.586983: step 75030, loss = 0.81 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:46.349137: step 75040, loss = 0.73 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:47.121618: step 75050, loss = 0.81 (1657.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:47.875435: step 75060, loss = 0.78 (1698.0 examples/sec; 0.075 sec/batch)
2017-05-05 19:01:48.645155: step 75070, loss = 0.91 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:49.415833: step 75080, loss = 0.67 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:50.179526: step 75090, loss = 0.75 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:50.948276: step 75100, loss = 0.71 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:51.705346: step 75110, loss = 0.73 (1690.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:52.476119: step 75120, loss = 0.81 (1660.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:53.245573: step 75130, loss = 0.75 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:54.011496: step 75140, loss = 0.74 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:54.778600: step 75150, loss = 0.66 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:55.544215: step 75160, loss = 0.71 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:56.307203: step 75170, loss = 0.75 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:57.073532: step 75180, loss = 0.83 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:01:57.836973: step 75190, loss = 0.55 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:58.601352: step 75200, loss = 0.80 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:01:59.363198: step 75210, loss = 0.70 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:00.129467: step 75220, loss = 0.72 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:02:00.892013: step 75230, loss = 0.85 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:01.759248: step 75240, loss = 0.69 (1476.0 examples/sec; 0.087 sec/batch)
2017-05-05 19:02:02.420920: step 75250, loss = 0.90 (1934.5 examples/sec; 0.066 sec/batch)
2017-05-05 19:02:03.184850: step 75260, loss = 0.76 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:03.943297: step 75270, loss = 0.81 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:04.703745: step 75280, loss = 0.67 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:05.467233: step 75290, loss = 0.79 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:06.228176: step 75300, loss = 0.79 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:06.996828: step 75310, loss = 0.79 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:02:07.760824: step 75320, loss = 0.71 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:08.525506: step 75330, loss = 0.65 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:09.288624: step 75340, loss = 0.66 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:10.053694: step 75350, loss = 0.82 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:02:10.818860: step 75360, loss = 0.91 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:02:11.577698: step 75370, loss = 0.83 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:12.352803: step 75380, loss = 0.86 (1651.4 examples/sec; 0.078 sec/batch)
2017-05-05 19:02:13.116972: step 75390, loss = 0.74 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:13.884836: step 75400, loss = 0.79 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:02:14.663329: step 75410, loss = 0.67 (1644.2 examples/sec; 0.078 sec/batch)
2017-05-05 19:02:15.423304: step 75420, loss = 0.83 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:16.181983: step 75430, loss = 0.76 (1687.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:16.957010: step 75440, loss = 0.74 (1651.6 examples/sec; 0.078 sec/batch)
2017-05-05 19:02:17.719499: step 75450, loss = 0.78 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:18.485540: step 75460, loss = 0.90 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:02:19.252019: step 75470, loss = 0.93 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:02:20.011220: step 75480, loss = 0.68 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:20.776221: step 75490, loss = 0.87 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:02:21.546661: step 75500, loss = 0.80 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:02:22.309334: step 75510, loss = 0.78 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:23.073055: step 75520, loss = 0.77 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:23.832494: step 75530, loss = 0.81 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:24.600011: step 75540, loss = 0.78 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:02:25.365948: step 75550, loss = 0.61 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:02:26.130772: step 75560, loss = 0.77 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:26.901339: step 75570, loss = 0.85 (1661.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:02:27.660574: step 75580, loss = 0.66 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:28.430034: step 75590, loss = 0.90 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:02:29.193551: step 75600, loss = 0.69 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:29.950860: step 75610, loss = 0.73 (1690.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:30.718146: step 75620, loss = 0.65 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:02:31.483993: step 75630, loss = 0.77 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:02:32.241814: step 75640, loss = 0.70 (1689.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:33.004872: step 75650, loss = 0.71 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:33.768136: step 75660, loss = 0.77 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:34.533659: step 75670, loss = 0.79 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:02:35.294844: step 75680, loss = 0.90 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:36.056768: step 75690, loss = 0.95 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:36.819224: step 75700, loss = 0.68 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:37.584019: step 75710, loss = 0.62 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:38.358068: step 75720, loss = 0.68 (1653.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:02:39.120401: step 75730, loss = 0.78 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:39.879811: step 75740, loss = 0.74 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:40.647659: step 75750, loss = 0.90 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:02:41.408970: step 75760, loss = 0.80 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:42.171558: step 75770, loss = 0.85 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:42.933819: step 75780, loss = 0.77 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:43.685480: step 75790, loss = 0.79 (1702.9 examples/sec; 0.075 sec/batch)
2017-05-05 19:02:44.450923: step 75800, loss = 0.85 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:02:45.215140: step 75810, loss = 0.63 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:45.982842: step 75820, loss = 0.81 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:02:46.747166: step 75830, loss = 0.77 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:47.517424: step 75840, loss = 0.75 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:02:48.277200: step 75850, loss = 0.77 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:49.034653: step 75860, loss = 0.90 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:49.801815: step 75870, loss = 0.82 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:02:50.567701: step 75880, loss = 0.70 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:02:51.336924: step 75890, loss = 0.77 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:02:52.093406: step 75900, loss = 0.66 (1692.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:52.865707: step 75910, loss = 0.89 (1657.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:02:53.624629: step 75920, loss = 0.72 (1686.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:54.389739: step 75930, loss = 0.73 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:02:55.150129: step 75940, loss = 0.83 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:55.905762: step 75950, loss = 0.66 (1693.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:56.674913: step 75960, loss = 0.69 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:02:57.442574: step 75970, loss = 0.73 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:02:58.207441: step 75980, loss = 0.70 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:58.966413: step 75990, loss = 0.82 (1686.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:02:59.721033: step 76000, loss = 0.76 (1696.2 examples/sec; 0.075 sec/batch)
2017-05-05 19:03:00.483434: step 76010, loss = 0.61 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:01.243108: step 76020, loss = 0.81 (1684.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:02.009330: step 76030, loss = 0.69 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:03:02.771444: step 76040, loss = 0.65 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:03.526471: step 76050, loss = 0.84 (1695.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:04.288095: step 76060, loss = 0.81 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:05.051560: step 76070, loss = 0.77 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:05.813409: step 76080, loss = 0.61 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:06.577293: step 76090, loss = 0.80 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:07.345127: step 76100, loss = 0.60 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:03:08.100387: step 76110, loss = 0.67 (1694.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:08.865863: step 76120, loss = 0.76 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:03:09.632599: step 76130, loss = 0.90 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:03:10.397591: step 76140, loss = 0.64 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:11.168073: step 76150, loss = 0.63 (1661.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:03:11.925772: step 76160, loss = 0.73 (1689.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:12.685427: step 76170, loss = 0.75 (1685.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:13.449036: step 76180, loss = 0.70 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:14.214824: step 76190, loss = 0.81 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:03:14.982587: step 76200, loss = 0.94 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:03:15.742443: step 76210, loss = 0.79 (1684.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:16.519012: step 76220, loss = 0.63 (1648.3 examples/sec; 0.078 sec/batch)
2017-05-05 19:03:17.383014: step 76230, loss = 0.83 (1481.5 examples/sec; 0.086 sec/batch)
2017-05-05 19:03:18.053286: step 76240, loss = 0.62 (1909.7 examples/sec; 0.067 sec/batch)
2017-05-05 19:03:18.820785: step 76250, loss = 0.80 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:03:19.584335: step 76260, loss = 0.69 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:20.352058: step 76270, loss = 0.79 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:03:21.110020: step 76280, loss = 0.92 (1688.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:21.875062: step 76290, loss = 0.73 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:03:22.640942: step 76300, loss = 0.85 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:03:23.403010: step 76310, loss = 0.96 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:24.162105: step 76320, loss = 0.84 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:24.931028: step 76330, loss = 0.98 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:03:25.692750: step 76340, loss = 0.74 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:26.458795: step 76350, loss = 0.64 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:03:27.222186: step 76360, loss = 0.82 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:27.978500: step 76370, loss = 0.73 (1692.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:28.741470: step 76380, loss = 0.66 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:29.504319: step 76390, loss = 0.70 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:30.274443: step 76400, loss = 0.78 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:03:31.040098: step 76410, loss = 0.85 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:03:31.794848: step 76420, loss = 0.71 (1695.9 examples/sec; 0.075 sec/batch)
2017-05-05 19:03:32.561606: step 76430, loss = 0.65 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:03:33.332949: step 76440, loss = 0.81 (1659.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:03:34.093202: step 76450, loss = 0.78 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:34.858604: step 76460, loss = 0.84 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:03:35.613167: step 76470, loss = 0.78 (1696.4 examples/sec; 0.075 sec/batch)
2017-05-05 19:03:36.378198: step 76480, loss = 0.72 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:03:37.148202: step 76490, loss = 0.74 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:03:37.910898: step 76500, loss = 0.85 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:38.681753: step 76510, loss = 0.72 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:03:39.439504: step 76520, loss = 0.73 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:40.200840: step 76530, loss = 0.74 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:40.961530: step 76540, loss = 0.83 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:41.726987: step 76550, loss = 0.83 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:03:42.490414: step 76560, loss = 0.74 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:43.256177: step 76570, loss = 0.96 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:03:44.013739: step 76580, loss = 0.71 (1689.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:44.780129: step 76590, loss = 0.77 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:03:45.542087: step 76600, loss = 0.76 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:46.307652: step 76610, loss = 0.81 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:03:47.071887: step 76620, loss = 0.78 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:47.829961: step 76630, loss = 0.74 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:48.595495: step 76640, loss = 0.82 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:03:49.368621: step 76650, loss = 0.72 (1655.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:03:50.128744: step 76660, loss = 0.85 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:50.888881: step 76670, loss = 0.82 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:51.646122: step 76680, loss = 0.72 (1690.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:52.414431: step 76690, loss = 0.72 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:03:53.180399: step 76700, loss = 0.77 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:03:53.945217: step 76710, loss = 0.76 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:54.707761: step 76720, loss = 0.77 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:55.465250: step 76730, loss = 0.73 (1689.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:56.226486: step 76740, loss = 0.94 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:56.990611: step 76750, loss = 0.77 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:57.753180: step 76760, loss = 0.70 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:58.517931: step 76770, loss = 0.79 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:03:59.283937: step 76780, loss = 0.59 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:04:00.041828: step 76790, loss = 0.80 (1688.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:00.797538: step 76800, loss = 0.67 (1693.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:01.566538: step 76810, loss = 0.71 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:04:02.332954: step 76820, loss = 0.56 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:04:03.102716: step 76830, loss = 0.71 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:04:03.860198: step 76840, loss = 0.63 (1689.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:04.625763: step 76850, loss = 0.74 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:04:05.390236: step 76860, loss = 0.77 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:06.147228: step 76870, loss = 0.74 (1690.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:06.909563: step 76880, loss = 0.90 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:07.668266: step 76890, loss = 0.92 (1687.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:08.437287: step 76900, loss = 0.78 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:04:09.200428: step 76910, loss = 0.86 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:09.957393: step 76920, loss = 0.98 (1690.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:10.726135: step 76930, loss = 0.73 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:04:11.487933: step 76940, loss = 0.85 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:12.252126: step 76950, loss = 0.91 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:13.014606: step 76960, loss = 0.71 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:13.773231: step 76970, loss = 0.75 (1687.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:14.540497: step 76980, loss = 0.77 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:04:15.297404: step 76990, loss = 0.79 (1691.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:16.055664: step 77000, loss = 0.79 (1688.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:16.829914: step 77010, loss = 0.83 (1653.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:04:17.594841: step 77020, loss = 0.54 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:18.365354: step 77030, loss = 0.80 (1661.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:04:19.137116: step 77040, loss = 0.69 (1658.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:04:19.895942: step 77050, loss = 0.75 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:20.656843: step 77060, loss = 0.80 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:21.423412: step 77070, loss = 0.79 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:04:22.186102: step 77080, loss = 0.62 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:22.948605: step 77090, loss = 0.72 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:23.712641: step 77100, loss = 0.77 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:24.481675: step 77110, loss = 0.88 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:04:25.240636: step 77120, loss = 0.81 (1686.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:26.003650: step 77130, loss = 0.67 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:26.769832: step 77140, loss = 0.87 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:04:27.529004: step 77150, loss = 0.79 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:28.287803: step 77160, loss = 0.85 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:29.059868: step 77170, loss = 0.80 (1657.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:04:29.822233: step 77180, loss = 0.85 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:30.584570: step 77190, loss = 0.65 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:31.344210: step 77200, loss = 0.79 (1685.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:32.103010: step 77210, loss = 0.89 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:32.979846: step 77220, loss = 0.71 (1459.8 examples/sec; 0.088 sec/batch)
2017-05-05 19:04:33.650129: step 77230, loss = 0.78 (1909.6 examples/sec; 0.067 sec/batch)
2017-05-05 19:04:34.416707: step 77240, loss = 0.70 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:04:35.189480: step 77250, loss = 0.68 (1656.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:04:35.942165: step 77260, loss = 0.69 (1700.6 examples/sec; 0.075 sec/batch)
2017-05-05 19:04:36.711778: step 77270, loss = 0.73 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:04:37.490193: step 77280, loss = 0.78 (1644.4 examples/sec; 0.078 sec/batch)
2017-05-05 19:04:38.250539: step 77290, loss = 0.86 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:39.019470: step 77300, loss = 0.88 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:04:39.774762: step 77310, loss = 0.66 (1694.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:40.535486: step 77320, loss = 0.78 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:41.302664: step 77330, loss = 0.68 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:04:42.066485: step 77340, loss = 0.68 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:42.832722: step 77350, loss = 0.67 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:04:43.590289: step 77360, loss = 0.74 (1689.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:44.357501: step 77370, loss = 0.68 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:04:45.121221: step 77380, loss = 0.71 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:45.887035: step 77390, loss = 0.73 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:04:46.651078: step 77400, loss = 0.79 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:47.414172: step 77410, loss = 0.83 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:48.179135: step 77420, loss = 0.75 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:48.946964: step 77430, loss = 0.80 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:04:49.717586: step 77440, loss = 0.64 (1661.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:04:50.485870: step 77450, loss = 0.71 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:04:51.248140: step 77460, loss = 0.91 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:52.005117: step 77470, loss = 0.75 (1690.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:52.772757: step 77480, loss = 0.81 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:04:53.543556: step 77490, loss = 0.79 (1660.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:04:54.311957: step 77500, loss = 0.76 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:04:55.080217: step 77510, loss = 1.02 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:04:55.836348: step 77520, loss = 0.79 (1692.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:56.604465: step 77530, loss = 0.69 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:04:57.367886: step 77540, loss = 0.77 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:58.131190: step 77550, loss = 0.79 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:58.894808: step 77560, loss = 0.81 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:04:59.651037: step 77570, loss = 0.61 (1692.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:00.422535: step 77580, loss = 0.80 (1659.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:05:01.185966: step 77590, loss = 0.75 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:01.953952: step 77600, loss = 0.78 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:05:02.721165: step 77610, loss = 0.71 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:05:03.483493: step 77620, loss = 0.68 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:04.245170: step 77630, loss = 0.84 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:05.009197: step 77640, loss = 0.79 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:05.769752: step 77650, loss = 0.84 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:06.534929: step 77660, loss = 0.87 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:05:07.290025: step 77670, loss = 0.57 (1695.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:08.056458: step 77680, loss = 0.78 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:05:08.818588: step 77690, loss = 0.72 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:09.585242: step 77700, loss = 0.65 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:05:10.351292: step 77710, loss = 0.76 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:05:11.120587: step 77720, loss = 0.60 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:05:11.872149: step 77730, loss = 0.74 (1703.1 examples/sec; 0.075 sec/batch)
2017-05-05 19:05:12.635421: step 77740, loss = 0.61 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:13.403291: step 77750, loss = 0.66 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:05:14.166307: step 77760, loss = 0.75 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:14.929687: step 77770, loss = 0.76 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:15.688939: step 77780, loss = 0.78 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:16.449879: step 77790, loss = 0.87 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:17.215084: step 77800, loss = 0.62 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:05:17.985872: step 77810, loss = 0.67 (1660.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:05:18.751516: step 77820, loss = 0.83 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:05:19.512834: step 77830, loss = 0.94 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:20.274002: step 77840, loss = 0.81 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:21.041335: step 77850, loss = 0.67 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:05:21.803569: step 77860, loss = 0.77 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:22.567475: step 77870, loss = 0.82 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:23.327816: step 77880, loss = 0.86 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:24.089940: step 77890, loss = 0.72 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:24.853537: step 77900, loss = 0.87 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:25.610903: step 77910, loss = 0.64 (1690.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:26.372540: step 77920, loss = 0.70 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:27.147758: step 77930, loss = 0.75 (1651.1 examples/sec; 0.078 sec/batch)
2017-05-05 19:05:27.906020: step 77940, loss = 0.92 (1688.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:28.679611: step 77950, loss = 0.86 (1654.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:05:29.446476: step 77960, loss = 0.77 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:05:30.216011: step 77970, loss = 0.76 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:05:30.979499: step 77980, loss = 0.77 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:31.733286: step 77990, loss = 0.70 (1698.1 examples/sec; 0.075 sec/batch)
2017-05-05 19:05:32.499911: step 78000, loss = 0.72 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:05:33.268281: step 78010, loss = 0.64 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:05:34.037938: step 78020, loss = 0.73 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:05:34.811343: step 78030, loss = 0.74 (1655.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:05:35.570335: step 78040, loss = 0.74 (1686.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:36.332865: step 78050, loss = 0.92 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:37.096460: step 78060, loss = 0.78 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:37.860510: step 78070, loss = 0.77 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:38.631782: step 78080, loss = 0.77 (1659.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:05:39.389740: step 78090, loss = 0.72 (1688.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:40.150731: step 78100, loss = 0.74 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:40.907408: step 78110, loss = 0.73 (1691.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:41.669668: step 78120, loss = 0.68 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:42.442391: step 78130, loss = 0.60 (1656.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:05:43.204036: step 78140, loss = 0.86 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:43.965030: step 78150, loss = 0.82 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:44.728466: step 78160, loss = 0.78 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:45.488454: step 78170, loss = 0.81 (1684.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:46.250081: step 78180, loss = 0.67 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:47.011314: step 78190, loss = 0.92 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:47.767927: step 78200, loss = 0.75 (1691.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:48.649784: step 78210, loss = 0.72 (1451.5 examples/sec; 0.088 sec/batch)
2017-05-05 19:05:49.307517: step 78220, loss = 0.76 (1946.1 examples/sec; 0.066 sec/batch)
2017-05-05 19:05:50.075689: step 78230, loss = 0.91 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:05:50.840668: step 78240, loss = 0.82 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:51.589694: step 78250, loss = 0.87 (1708.9 examples/sec; 0.075 sec/batch)
2017-05-05 19:05:52.357929: step 78260, loss = 0.74 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:05:53.123977: step 78270, loss = 0.72 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:05:53.883161: step 78280, loss = 0.73 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:54.654010: step 78290, loss = 0.83 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:05:55.420039: step 78300, loss = 0.69 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:05:56.184299: step 78310, loss = 0.90 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:05:56.950922: step 78320, loss = 0.71 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:05:57.721527: step 78330, loss = 0.76 (1661.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:05:58.487636: step 78340, loss = 0.96 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:05:59.250335: step 78350, loss = 0.77 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:00.004039: step 78360, loss = 0.86 (1698.3 examples/sec; 0.075 sec/batch)
2017-05-05 19:06:00.765141: step 78370, loss = 0.61 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:01.536407: step 78380, loss = 0.66 (1659.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:06:02.297390: step 78390, loss = 0.76 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:03.063323: step 78400, loss = 0.70 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:06:03.832291: step 78410, loss = 0.77 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:06:04.597604: step 78420, loss = 0.80 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:06:05.373556: step 78430, loss = 0.67 (1649.6 examples/sec; 0.078 sec/batch)
2017-05-05 19:06:06.174925: step 78440, loss = 0.67 (1597.3 examples/sec; 0.080 sec/batch)
2017-05-05 19:06:06.939447: step 78450, loss = 0.71 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:07.697282: step 78460, loss = 0.80 (1689.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:08.468037: step 78470, loss = 0.55 (1660.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:06:09.242989: step 78480, loss = 0.71 (1651.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:06:10.007766: step 78490, loss = 0.70 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:10.779060: step 78500, loss = 1.01 (1659.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:06:11.539236: step 78510, loss = 0.82 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:12.314931: step 78520, loss = 0.74 (1650.1 examples/sec; 0.078 sec/batch)
2017-05-05 19:06:13.082547: step 78530, loss = 0.68 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:06:13.844768: step 78540, loss = 0.67 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:14.620068: step 78550, loss = 0.89 (1651.0 examples/sec; 0.078 sec/batch)
2017-05-05 19:06:15.388142: step 78560, loss = 0.76 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:06:16.151202: step 78570, loss = 0.86 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:16.913737: step 78580, loss = 0.92 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:17.682957: step 78590, loss = 0.73 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:06:18.455545: step 78600, loss = 0.59 (1656.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:06:19.217312: step 78610, loss = 0.86 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:19.971806: step 78620, loss = 0.74 (1696.5 examples/sec; 0.075 sec/batch)
2017-05-05 19:06:20.738169: step 78630, loss = 0.74 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:06:21.507058: step 78640, loss = 0.78 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:06:22.272259: step 78650, loss = 0.77 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:06:23.040052: step 78660, loss = 0.73 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:06:23.797391: step 78670, loss = 0.88 (1690.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:24.561799: step 78680, loss = 0.84 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:25.325846: step 78690, loss = 0.74 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:26.088677: step 78700, loss = 0.66 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:26.846703: step 78710, loss = 0.71 (1688.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:27.609384: step 78720, loss = 0.68 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:28.377013: step 78730, loss = 0.78 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:06:29.137397: step 78740, loss = 0.77 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:29.908433: step 78750, loss = 0.78 (1660.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:06:30.669797: step 78760, loss = 0.78 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:31.430168: step 78770, loss = 0.75 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:32.191149: step 78780, loss = 0.82 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:32.956612: step 78790, loss = 0.86 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:06:33.719614: step 78800, loss = 0.68 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:34.489047: step 78810, loss = 0.90 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:06:35.250904: step 78820, loss = 0.82 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:36.009431: step 78830, loss = 0.78 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:36.773064: step 78840, loss = 0.93 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:37.537132: step 78850, loss = 0.70 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:38.301596: step 78860, loss = 0.59 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:39.064595: step 78870, loss = 0.69 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:39.822035: step 78880, loss = 0.76 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:40.583790: step 78890, loss = 0.62 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:41.355087: step 78900, loss = 0.63 (1659.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:06:42.114016: step 78910, loss = 0.76 (1686.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:42.880265: step 78920, loss = 0.85 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:06:43.633957: step 78930, loss = 0.69 (1698.3 examples/sec; 0.075 sec/batch)
2017-05-05 19:06:44.399777: step 78940, loss = 0.76 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:06:45.164673: step 78950, loss = 0.85 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:45.920312: step 78960, loss = 0.71 (1693.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:46.679549: step 78970, loss = 0.76 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:47.441269: step 78980, loss = 0.83 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:48.206052: step 78990, loss = 0.73 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:48.972320: step 79000, loss = 0.75 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:06:49.743149: step 79010, loss = 0.82 (1660.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:06:50.500171: step 79020, loss = 0.77 (1690.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:51.262934: step 79030, loss = 0.61 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:52.018459: step 79040, loss = 0.89 (1694.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:52.784279: step 79050, loss = 0.91 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:06:53.556834: step 79060, loss = 0.70 (1656.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:06:54.336870: step 79070, loss = 0.74 (1640.9 examples/sec; 0.078 sec/batch)
2017-05-05 19:06:55.095510: step 79080, loss = 0.78 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:55.850132: step 79090, loss = 0.91 (1696.2 examples/sec; 0.075 sec/batch)
2017-05-05 19:06:56.616547: step 79100, loss = 0.88 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:06:57.382046: step 79110, loss = 0.82 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:06:58.149968: step 79120, loss = 0.85 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:06:58.912892: step 79130, loss = 0.73 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:06:59.668398: step 79140, loss = 0.78 (1694.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:00.438581: step 79150, loss = 0.75 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:01.196942: step 79160, loss = 0.91 (1687.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:01.955259: step 79170, loss = 0.67 (1688.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:02.723543: step 79180, loss = 0.79 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:03.481977: step 79190, loss = 0.68 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:04.349560: step 79200, loss = 0.70 (1475.4 examples/sec; 0.087 sec/batch)
2017-05-05 19:07:05.010228: step 79210, loss = 0.68 (1937.4 examples/sec; 0.066 sec/batch)
2017-05-05 19:07:05.775878: step 79220, loss = 0.79 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:06.543594: step 79230, loss = 0.85 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:07.313589: step 79240, loss = 0.65 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:08.066479: step 79250, loss = 0.86 (1700.1 examples/sec; 0.075 sec/batch)
2017-05-05 19:07:08.834012: step 79260, loss = 0.85 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:09.601069: step 79270, loss = 0.71 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:10.366009: step 79280, loss = 0.86 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:11.130478: step 79290, loss = 1.04 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:11.881192: step 79300, loss = 0.85 (1705.1 examples/sec; 0.075 sec/batch)
2017-05-05 19:07:12.650948: step 79310, loss = 0.67 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:13.417793: step 79320, loss = 0.73 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:14.181758: step 79330, loss = 0.97 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:14.946186: step 79340, loss = 0.72 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:15.700801: step 79350, loss = 0.74 (1696.2 examples/sec; 0.075 sec/batch)
2017-05-05 19:07:16.464288: step 79360, loss = 0.82 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:17.229325: step 79370, loss = 0.74 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:18.003282: step 79380, loss = 0.66 (1653.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:18.762459: step 79390, loss = 0.72 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:19.522554: step 79400, loss = 0.92 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:20.287043: step 79410, loss = 0.80 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:21.038948: step 79420, loss = 0.70 (1702.3 examples/sec; 0.075 sec/batch)
2017-05-05 19:07:21.797683: step 79430, loss = 0.68 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:22.567225: step 79440, loss = 0.73 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:23.330301: step 79450, loss = 0.75 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:24.083501: step 79460, loss = 0.85 (1699.4 examples/sec; 0.075 sec/batch)
2017-05-05 19:07:24.851374: step 79470, loss = 0.67 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:25.614385: step 79480, loss = 0.77 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:26.376403: step 79490, loss = 0.77 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:27.142692: step 79500, loss = 0.87 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:27.897726: step 79510, loss = 0.82 (1695.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:28.669277: step 79520, loss = 0.85 (1659.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:29.429180: step 79530, loss = 0.60 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:30.203474: step 79540, loss = 0.90 (1653.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:30.965961: step 79550, loss = 0.84 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:31.723806: step 79560, loss = 0.97 (1689.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:32.494928: step 79570, loss = 0.82 (1659.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:33.257126: step 79580, loss = 0.80 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:34.023166: step 79590, loss = 0.88 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:34.790556: step 79600, loss = 0.73 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:35.547051: step 79610, loss = 0.73 (1692.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:36.317119: step 79620, loss = 0.70 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:37.090615: step 79630, loss = 0.78 (1654.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:37.860694: step 79640, loss = 0.79 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:38.630747: step 79650, loss = 0.84 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:39.400190: step 79660, loss = 0.90 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:40.159370: step 79670, loss = 0.80 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:40.918453: step 79680, loss = 0.88 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:41.684019: step 79690, loss = 0.68 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:42.449745: step 79700, loss = 0.64 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:43.217855: step 79710, loss = 0.84 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:43.967488: step 79720, loss = 0.75 (1707.5 examples/sec; 0.075 sec/batch)
2017-05-05 19:07:44.733080: step 79730, loss = 0.82 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:45.496695: step 79740, loss = 0.82 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:46.256634: step 79750, loss = 0.65 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:47.020052: step 79760, loss = 0.61 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:47.780567: step 79770, loss = 0.81 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:48.550333: step 79780, loss = 0.90 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:49.313924: step 79790, loss = 0.73 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:50.085575: step 79800, loss = 0.66 (1658.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:50.853150: step 79810, loss = 0.77 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:51.612784: step 79820, loss = 0.87 (1685.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:52.379319: step 79830, loss = 0.78 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:53.142401: step 79840, loss = 0.72 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:53.908569: step 79850, loss = 0.73 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:54.677386: step 79860, loss = 0.76 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:55.442990: step 79870, loss = 0.71 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:56.204959: step 79880, loss = 0.82 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:56.963322: step 79890, loss = 0.77 (1687.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:07:57.728708: step 79900, loss = 0.61 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:58.495874: step 79910, loss = 0.60 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:07:59.255155: step 79920, loss = 0.65 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:00.011028: step 79930, loss = 0.85 (1693.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:00.765718: step 79940, loss = 0.83 (1696.1 examples/sec; 0.075 sec/batch)
2017-05-05 19:08:01.527473: step 79950, loss = 0.84 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:02.284770: step 79960, loss = 0.80 (1690.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:03.042468: step 79970, loss = 0.72 (1689.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:03.805929: step 79980, loss = 0.81 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:04.571686: step 79990, loss = 0.83 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:05.342694: step 80000, loss = 0.84 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:06.103006: step 80010, loss = 0.71 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:06.867905: step 80020, loss = 0.74 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:07.633854: step 80030, loss = 0.83 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:08.394601: step 80040, loss = 0.87 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:09.160965: step 80050, loss = 0.69 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:09.927986: step 80060, loss = 0.87 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:10.691112: step 80070, loss = 0.77 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:11.456240: step 80080, loss = 0.77 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:12.222488: step 80090, loss = 0.71 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:12.988537: step 80100, loss = 0.81 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:13.754639: step 80110, loss = 0.73 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:14.529004: step 80120, loss = 0.85 (1653.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:15.285534: step 80130, loss = 0.77 (1691.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:16.043919: step 80140, loss = 0.72 (1687.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:16.811116: step 80150, loss = 0.87 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:17.572795: step 80160, loss = 0.83 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:18.342822: step 80170, loss = 0.72 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:19.105057: step 80180, loss = 0.65 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:19.963111: step 80190, loss = 0.76 (1491.8 examples/sec; 0.086 sec/batch)
2017-05-05 19:08:20.627235: step 80200, loss = 0.68 (1927.3 examples/sec; 0.066 sec/batch)
2017-05-05 19:08:21.398236: step 80210, loss = 0.72 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:22.159088: step 80220, loss = 0.80 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:22.929014: step 80230, loss = 0.80 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:23.687669: step 80240, loss = 0.70 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:24.448369: step 80250, loss = 0.62 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:25.213253: step 80260, loss = 0.73 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:25.977193: step 80270, loss = 0.69 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:26.754769: step 80280, loss = 0.72 (1646.2 examples/sec; 0.078 sec/batch)
2017-05-05 19:08:27.507304: step 80290, loss = 0.82 (1700.9 examples/sec; 0.075 sec/batch)
2017-05-05 19:08:28.271524: step 80300, loss = 0.75 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:29.032034: step 80310, loss = 0.67 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:29.797044: step 80320, loss = 0.68 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:30.554977: step 80330, loss = 0.99 (1688.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:31.314653: step 80340, loss = 0.74 (1684.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:32.071900: step 80350, loss = 1.06 (1690.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:32.834796: step 80360, loss = 0.76 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:33.597960: step 80370, loss = 0.79 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:34.363524: step 80380, loss = 0.82 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:35.131655: step 80390, loss = 0.77 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:35.886212: step 80400, loss = 0.92 (1696.4 examples/sec; 0.075 sec/batch)
2017-05-05 19:08:36.654603: step 80410, loss = 0.67 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:37.422583: step 80420, loss = 0.86 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:38.188320: step 80430, loss = 0.80 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:38.959990: step 80440, loss = 0.74 (1658.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:39.722353: step 80450, loss = 0.76 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:40.490371: step 80460, loss = 0.84 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:41.248754: step 80470, loss = 0.76 (1687.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:42.012028: step 80480, loss = 0.93 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:42.783589: step 80490, loss = 0.75 (1659.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:43.544194: step 80500, loss = 0.75 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:44.313255: step 80510, loss = 0.86 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:45.077578: step 80520, loss = 0.79 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:45.839661: step 80530, loss = 0.55 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:46.604418: step 80540, loss = 0.79 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:47.374604: step 80550, loss = 0.78 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:48.126078: step 80560, loss = 0.82 (1703.3 examples/sec; 0.075 sec/batch)
2017-05-05 19:08:48.894883: step 80570, loss = 0.86 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:49.661287: step 80580, loss = 0.86 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:50.427847: step 80590, loss = 0.78 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:51.188287: step 80600, loss = 0.93 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:51.944262: step 80610, loss = 0.94 (1693.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:52.714595: step 80620, loss = 0.81 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:53.483075: step 80630, loss = 0.79 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:54.245959: step 80640, loss = 0.71 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:55.011450: step 80650, loss = 0.74 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:55.770712: step 80660, loss = 0.83 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:56.535489: step 80670, loss = 0.65 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:57.305792: step 80680, loss = 0.78 (1661.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:58.066677: step 80690, loss = 0.68 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:08:58.834275: step 80700, loss = 0.74 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:08:59.593704: step 80710, loss = 0.83 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:00.358667: step 80720, loss = 0.87 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:01.116113: step 80730, loss = 0.73 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:01.878759: step 80740, loss = 0.84 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:02.642008: step 80750, loss = 0.80 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:03.405840: step 80760, loss = 0.62 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:04.171519: step 80770, loss = 0.68 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:04.935558: step 80780, loss = 0.76 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:05.701479: step 80790, loss = 0.74 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:06.469532: step 80800, loss = 0.77 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:07.233800: step 80810, loss = 0.84 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:07.991573: step 80820, loss = 0.78 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:08.756553: step 80830, loss = 0.69 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:09.518302: step 80840, loss = 0.69 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:10.290127: step 80850, loss = 0.78 (1658.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:11.053141: step 80860, loss = 0.85 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:11.807254: step 80870, loss = 0.77 (1697.4 examples/sec; 0.075 sec/batch)
2017-05-05 19:09:12.569222: step 80880, loss = 0.68 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:13.331967: step 80890, loss = 0.87 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:14.094242: step 80900, loss = 0.84 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:14.859245: step 80910, loss = 0.66 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:15.615381: step 80920, loss = 0.82 (1692.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:16.383160: step 80930, loss = 0.73 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:17.146433: step 80940, loss = 0.76 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:17.916453: step 80950, loss = 0.81 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:18.681717: step 80960, loss = 0.96 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:19.442746: step 80970, loss = 0.66 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:20.202387: step 80980, loss = 0.71 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:20.967645: step 80990, loss = 0.59 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:21.727294: step 81000, loss = 0.87 (1685.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:22.494525: step 81010, loss = 0.74 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:23.256473: step 81020, loss = 0.77 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:24.014859: step 81030, loss = 0.81 (1687.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:24.781043: step 81040, loss = 0.72 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:25.540521: step 81050, loss = 1.00 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:26.304982: step 81060, loss = 0.82 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:27.069102: step 81070, loss = 0.91 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:27.827783: step 81080, loss = 0.78 (1687.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:28.593212: step 81090, loss = 0.89 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:29.364691: step 81100, loss = 0.86 (1659.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:30.138057: step 81110, loss = 0.75 (1655.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:30.906511: step 81120, loss = 0.75 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:31.665058: step 81130, loss = 0.71 (1687.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:32.428172: step 81140, loss = 0.95 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:33.196143: step 81150, loss = 0.76 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:33.969219: step 81160, loss = 0.81 (1655.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:34.726465: step 81170, loss = 0.71 (1690.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:35.587462: step 81180, loss = 0.72 (1486.6 examples/sec; 0.086 sec/batch)
2017-05-05 19:09:36.257665: step 81190, loss = 0.80 (1909.9 examples/sec; 0.067 sec/batch)
2017-05-05 19:09:37.025935: step 81200, loss = 0.69 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:37.791542: step 81210, loss = 0.89 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:38.562523: step 81220, loss = 0.78 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:39.329962: step 81230, loss = 0.77 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:40.080631: step 81240, loss = 0.73 (1705.1 examples/sec; 0.075 sec/batch)
2017-05-05 19:09:40.843742: step 81250, loss = 0.67 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:41.610340: step 81260, loss = 0.60 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:42.368355: step 81270, loss = 0.80 (1688.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:43.139906: step 81280, loss = 0.70 (1659.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:43.898672: step 81290, loss = 0.58 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:44.667151: step 81300, loss = 0.69 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:45.436020: step 81310, loss = 0.68 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:46.198465: step 81320, loss = 0.88 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:46.972269: step 81330, loss = 0.69 (1654.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:47.729339: step 81340, loss = 0.79 (1690.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:48.493316: step 81350, loss = 0.75 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:49.263581: step 81360, loss = 0.76 (1661.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:50.028852: step 81370, loss = 0.60 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:50.803151: step 81380, loss = 0.62 (1653.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:51.557405: step 81390, loss = 0.96 (1697.0 examples/sec; 0.075 sec/batch)
2017-05-05 19:09:52.320486: step 81400, loss = 0.65 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:53.086745: step 81410, loss = 0.57 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:53.855278: step 81420, loss = 0.80 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:54.618264: step 81430, loss = 0.71 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:55.378428: step 81440, loss = 0.73 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:56.137346: step 81450, loss = 0.88 (1686.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:09:56.905579: step 81460, loss = 0.66 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:57.670758: step 81470, loss = 0.79 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:58.436453: step 81480, loss = 0.90 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:59.207395: step 81490, loss = 0.85 (1660.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:09:59.963128: step 81500, loss = 0.77 (1693.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:10:00.719427: step 81510, loss = 0.80 (1692.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:10:01.484496: step 81520, loss = 1.00 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:02.251690: step 81530, loss = 0.78 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:03.016942: step 81540, loss = 0.58 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:03.775550: step 81550, loss = 0.84 (1687.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:10:04.544862: step 81560, loss = 0.71 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:05.305229: step 81570, loss = 0.78 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:10:06.070400: step 81580, loss = 0.68 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:06.840742: step 81590, loss = 0.58 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:07.592944: step 81600, loss = 0.69 (1701.7 examples/sec; 0.075 sec/batch)
2017-05-05 19:10:08.355248: step 81610, loss = 0.61 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:10:09.122695: step 81620, loss = 0.74 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:09.886558: step 81630, loss = 0.62 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:10:10.646130: step 81640, loss = 0.65 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:10:11.400556: step 81650, loss = 0.82 (1696.6 examples/sec; 0.075 sec/batch)
2017-05-05 19:10:12.166638: step 81660, loss = 0.85 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:12.932605: step 81670, loss = 0.67 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:13.701699: step 81680, loss = 0.71 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:14.466888: step 81690, loss = 0.63 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:15.230010: step 81700, loss = 0.84 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:10:15.979155: step 81710, loss = 0.74 (1708.6 examples/sec; 0.075 sec/batch)
2017-05-05 19:10:16.750239: step 81720, loss = 0.87 (1660.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:17.515311: step 81730, loss = 0.75 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:18.279001: step 81740, loss = 1.00 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:10:19.045283: step 81750, loss = 0.81 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:19.804717: step 81760, loss = 0.58 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:10:20.566083: step 81770, loss = 0.69 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:10:21.335885: step 81780, loss = 0.79 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:22.103028: step 81790, loss = 0.80 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:22.864980: step 81800, loss = 0.86 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:10:23.627211: step 81810, loss = 0.57 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:10:24.392599: step 81820, loss = 0.79 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:25.158964: step 81830, loss = 0.71 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:25.923701: step 81840, loss = 0.80 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:10:26.687106: step 81850, loss = 0.77 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:10:27.450122: step 81860, loss = 0.88 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:10:28.220312: step 81870, loss = 0.75 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:28.988620: step 81880, loss = 0.79 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:29.761566: step 81890, loss = 0.66 (1656.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:30.528190: step 81900, loss = 0.74 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:31.293669: step 81910, loss = 0.68 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:32.047574: step 81920, loss = 0.73 (1697.8 examples/sec; 0.075 sec/batch)
2017-05-05 19:10:32.814150: step 81930, loss = 0.86 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:33.579794: step 81940, loss = 0.64 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:34.345175: step 81950, loss = 0.76 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:35.107149: step 81960, loss = 0.60 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:10:35.867279: step 81970, loss = 0.78 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:10:36.638455: step 81980, loss = 0.63 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:37.409448: step 81990, loss = 0.83 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:38.171898: step 82000, loss = 0.66 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:10:38.941835: step 82010, loss = 0.84 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:39.693654: step 82020, loss = 0.93 (1702.5 examples/sec; 0.075 sec/batch)
2017-05-05 19:10:40.458178: step 82030, loss = 0.82 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:10:41.224106: step 82040, loss = 0.70 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:41.990270: step 82050, loss = 0.70 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:42.754930: step 82060, loss = 0.83 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:10:43.510311: step 82070, loss = 0.75 (1694.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:10:44.276157: step 82080, loss = 0.74 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:45.037010: step 82090, loss = 0.86 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:10:45.801086: step 82100, loss = 0.74 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:10:46.563461: step 82110, loss = 0.72 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:10:47.325814: step 82120, loss = 0.57 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:10:48.088223: step 82130, loss = 0.80 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:10:48.852875: step 82140, loss = 0.86 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:10:49.621954: step 82150, loss = 0.68 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:50.388072: step 82160, loss = 0.67 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:51.247164: step 82170, loss = 0.70 (1490.0 examples/sec; 0.086 sec/batch)
2017-05-05 19:10:51.908318: step 82180, loss = 0.75 (1936.0 examples/sec; 0.066 sec/batch)
2017-05-05 19:10:52.669744: step 82190, loss = 0.66 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:10:53.432341: step 82200, loss = 0.91 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:10:54.193600: step 82210, loss = 0.73 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:10:54.959343: step 82220, loss = 0.95 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:55.708879: step 82230, loss = 0.74 (1707.7 examples/sec; 0.075 sec/batch)
2017-05-05 19:10:56.477260: step 82240, loss = 0.89 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:57.244482: step 82250, loss = 0.73 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:58.010170: step 82260, loss = 0.70 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:58.775540: step 82270, loss = 0.63 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:10:59.529959: step 82280, loss = 0.86 (1696.7 examples/sec; 0.075 sec/batch)
2017-05-05 19:11:00.288195: step 82290, loss = 0.80 (1688.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:01.045085: step 82300, loss = 0.78 (1691.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:01.820692: step 82310, loss = 0.78 (1650.3 examples/sec; 0.078 sec/batch)
2017-05-05 19:11:02.585137: step 82320, loss = 0.71 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:03.352007: step 82330, loss = 0.70 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:11:04.110381: step 82340, loss = 0.84 (1687.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:04.874965: step 82350, loss = 0.66 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:05.630950: step 82360, loss = 0.62 (1693.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:06.400545: step 82370, loss = 0.72 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:11:07.161273: step 82380, loss = 0.85 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:07.919505: step 82390, loss = 0.80 (1688.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:08.684005: step 82400, loss = 0.74 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:09.452151: step 82410, loss = 0.77 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:11:10.217445: step 82420, loss = 0.61 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:11:10.976477: step 82430, loss = 0.84 (1686.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:11.738704: step 82440, loss = 0.87 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:12.509009: step 82450, loss = 0.78 (1661.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:11:13.273709: step 82460, loss = 0.56 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:14.036426: step 82470, loss = 0.78 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:14.803229: step 82480, loss = 0.74 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:11:15.562837: step 82490, loss = 0.79 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:16.326242: step 82500, loss = 0.86 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:17.094284: step 82510, loss = 0.76 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:11:17.857193: step 82520, loss = 0.71 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:18.618391: step 82530, loss = 0.81 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:19.382051: step 82540, loss = 0.70 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:20.144776: step 82550, loss = 0.77 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:20.903819: step 82560, loss = 0.75 (1686.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:21.667258: step 82570, loss = 0.88 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:22.435787: step 82580, loss = 0.83 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:11:23.197196: step 82590, loss = 0.70 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:23.954768: step 82600, loss = 0.71 (1689.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:24.716908: step 82610, loss = 0.68 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:25.476154: step 82620, loss = 0.79 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:26.245955: step 82630, loss = 0.84 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:11:27.010735: step 82640, loss = 0.83 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:27.764258: step 82650, loss = 0.83 (1698.7 examples/sec; 0.075 sec/batch)
2017-05-05 19:11:28.532262: step 82660, loss = 0.71 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:11:29.302727: step 82670, loss = 0.88 (1661.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:11:30.060633: step 82680, loss = 0.73 (1688.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:30.824505: step 82690, loss = 0.74 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:31.587120: step 82700, loss = 0.68 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:32.348176: step 82710, loss = 0.80 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:33.107676: step 82720, loss = 0.80 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:33.876405: step 82730, loss = 0.78 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:11:34.645829: step 82740, loss = 0.67 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:11:35.405146: step 82750, loss = 0.74 (1685.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:36.163762: step 82760, loss = 0.74 (1687.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:36.936924: step 82770, loss = 0.70 (1655.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:11:37.695959: step 82780, loss = 0.66 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:38.463531: step 82790, loss = 0.60 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:11:39.229239: step 82800, loss = 0.80 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:11:39.984344: step 82810, loss = 0.71 (1695.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:40.747860: step 82820, loss = 0.80 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:41.513042: step 82830, loss = 0.67 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:11:42.277277: step 82840, loss = 0.77 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:43.042624: step 82850, loss = 0.86 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:11:43.799641: step 82860, loss = 0.74 (1690.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:44.567134: step 82870, loss = 0.70 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:11:45.333903: step 82880, loss = 0.95 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:11:46.102290: step 82890, loss = 0.70 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:11:46.865676: step 82900, loss = 0.65 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:47.619752: step 82910, loss = 0.88 (1697.4 examples/sec; 0.075 sec/batch)
2017-05-05 19:11:48.389554: step 82920, loss = 0.93 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:11:49.157070: step 82930, loss = 0.73 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:11:49.925967: step 82940, loss = 0.73 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:11:50.689513: step 82950, loss = 0.94 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:51.455479: step 82960, loss = 0.83 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:11:52.220696: step 82970, loss = 0.68 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:11:52.982014: step 82980, loss = 0.70 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:53.752193: step 82990, loss = 0.66 (1662.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:11:54.509951: step 83000, loss = 0.69 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:55.279321: step 83010, loss = 0.80 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:11:56.036106: step 83020, loss = 0.91 (1691.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:56.800181: step 83030, loss = 0.68 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:11:57.566864: step 83040, loss = 0.78 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:11:58.332755: step 83050, loss = 0.79 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:11:59.097830: step 83060, loss = 0.74 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:11:59.850066: step 83070, loss = 0.86 (1701.6 examples/sec; 0.075 sec/batch)
2017-05-05 19:12:00.612166: step 83080, loss = 0.75 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:01.378497: step 83090, loss = 0.68 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:02.143911: step 83100, loss = 0.68 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:02.918105: step 83110, loss = 0.69 (1653.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:03.670246: step 83120, loss = 0.86 (1701.8 examples/sec; 0.075 sec/batch)
2017-05-05 19:12:04.435480: step 83130, loss = 0.74 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:05.204602: step 83140, loss = 0.80 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:05.969871: step 83150, loss = 0.65 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:06.829406: step 83160, loss = 0.82 (1489.2 examples/sec; 0.086 sec/batch)
2017-05-05 19:12:07.498077: step 83170, loss = 0.76 (1914.3 examples/sec; 0.067 sec/batch)
2017-05-05 19:12:08.263244: step 83180, loss = 0.78 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:09.020338: step 83190, loss = 0.81 (1690.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:09.783582: step 83200, loss = 0.71 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:10.547085: step 83210, loss = 0.71 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:11.307922: step 83220, loss = 0.59 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:12.065869: step 83230, loss = 0.75 (1688.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:12.825478: step 83240, loss = 0.74 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:13.590753: step 83250, loss = 0.73 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:14.355025: step 83260, loss = 0.69 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:15.119020: step 83270, loss = 0.77 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:15.880967: step 83280, loss = 0.88 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:16.643064: step 83290, loss = 0.71 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:17.408479: step 83300, loss = 0.86 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:18.171043: step 83310, loss = 0.79 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:18.935019: step 83320, loss = 0.68 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:19.690207: step 83330, loss = 0.76 (1695.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:20.452055: step 83340, loss = 0.84 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:21.211388: step 83350, loss = 0.84 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:21.976833: step 83360, loss = 0.54 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:22.743486: step 83370, loss = 0.76 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:23.503240: step 83380, loss = 0.81 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:24.264477: step 83390, loss = 0.74 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:25.027946: step 83400, loss = 0.80 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:25.791483: step 83410, loss = 0.83 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:26.554519: step 83420, loss = 0.84 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:27.318839: step 83430, loss = 0.62 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:28.075261: step 83440, loss = 0.78 (1692.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:28.839608: step 83450, loss = 0.73 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:29.609472: step 83460, loss = 0.75 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:30.377108: step 83470, loss = 0.75 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:31.136720: step 83480, loss = 0.88 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:31.891097: step 83490, loss = 0.64 (1696.8 examples/sec; 0.075 sec/batch)
2017-05-05 19:12:32.662556: step 83500, loss = 0.73 (1659.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:33.427287: step 83510, loss = 0.71 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:34.201770: step 83520, loss = 0.74 (1652.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:34.970207: step 83530, loss = 0.72 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:35.735225: step 83540, loss = 0.75 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:36.504207: step 83550, loss = 0.78 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:37.275450: step 83560, loss = 0.82 (1659.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:38.044300: step 83570, loss = 0.68 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:38.815620: step 83580, loss = 0.78 (1659.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:39.581217: step 83590, loss = 0.82 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:40.345782: step 83600, loss = 0.58 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:41.110547: step 83610, loss = 0.70 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:41.878422: step 83620, loss = 0.70 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:42.644277: step 83630, loss = 0.63 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:43.411436: step 83640, loss = 0.58 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:44.171631: step 83650, loss = 0.75 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:44.937734: step 83660, loss = 0.80 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:45.703161: step 83670, loss = 0.84 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:46.465113: step 83680, loss = 0.86 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:47.228933: step 83690, loss = 0.71 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:47.989311: step 83700, loss = 0.79 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:48.749202: step 83710, loss = 0.83 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:49.517896: step 83720, loss = 0.65 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:50.285820: step 83730, loss = 0.85 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:51.041240: step 83740, loss = 0.79 (1694.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:51.794933: step 83750, loss = 0.78 (1698.3 examples/sec; 0.075 sec/batch)
2017-05-05 19:12:52.562250: step 83760, loss = 0.83 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:53.330554: step 83770, loss = 0.80 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:54.092938: step 83780, loss = 0.82 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:54.862928: step 83790, loss = 0.72 (1662.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:55.620983: step 83800, loss = 0.78 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:56.390069: step 83810, loss = 0.67 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:57.157582: step 83820, loss = 0.65 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:57.924437: step 83830, loss = 0.66 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:12:58.688366: step 83840, loss = 0.84 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:12:59.453979: step 83850, loss = 0.81 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:00.219591: step 83860, loss = 0.65 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:00.980434: step 83870, loss = 0.79 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:01.746498: step 83880, loss = 0.65 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:02.502147: step 83890, loss = 0.73 (1693.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:03.275182: step 83900, loss = 0.68 (1655.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:04.036175: step 83910, loss = 0.74 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:04.794924: step 83920, loss = 0.77 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:05.558740: step 83930, loss = 0.69 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:06.326109: step 83940, loss = 0.89 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:07.090585: step 83950, loss = 0.83 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:07.851721: step 83960, loss = 0.71 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:08.625108: step 83970, loss = 0.76 (1655.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:09.394470: step 83980, loss = 0.82 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:10.162524: step 83990, loss = 0.80 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:10.931075: step 84000, loss = 0.85 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:11.681001: step 84010, loss = 0.74 (1706.8 examples/sec; 0.075 sec/batch)
2017-05-05 19:13:12.456999: step 84020, loss = 0.66 (1649.5 examples/sec; 0.078 sec/batch)
2017-05-05 19:13:13.223604: step 84030, loss = 0.76 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:13.989561: step 84040, loss = 0.63 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:14.757400: step 84050, loss = 0.78 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:15.513485: step 84060, loss = 0.76 (1692.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:16.278347: step 84070, loss = 0.74 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:17.047544: step 84080, loss = 0.74 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:17.810120: step 84090, loss = 0.54 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:18.578094: step 84100, loss = 0.65 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:19.345467: step 84110, loss = 0.88 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:20.102313: step 84120, loss = 0.84 (1691.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:20.865671: step 84130, loss = 0.76 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:21.631260: step 84140, loss = 0.69 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:22.505233: step 84150, loss = 0.79 (1464.6 examples/sec; 0.087 sec/batch)
2017-05-05 19:13:23.176608: step 84160, loss = 0.66 (1906.5 examples/sec; 0.067 sec/batch)
2017-05-05 19:13:23.928865: step 84170, loss = 0.75 (1701.5 examples/sec; 0.075 sec/batch)
2017-05-05 19:13:24.689029: step 84180, loss = 0.84 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:25.452330: step 84190, loss = 0.88 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:26.226762: step 84200, loss = 0.84 (1652.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:26.990034: step 84210, loss = 0.84 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:27.747336: step 84220, loss = 0.77 (1690.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:28.513132: step 84230, loss = 0.89 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:29.280439: step 84240, loss = 0.70 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:30.049064: step 84250, loss = 0.57 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:30.807422: step 84260, loss = 0.78 (1687.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:31.570632: step 84270, loss = 0.93 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:32.329470: step 84280, loss = 0.74 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:33.092430: step 84290, loss = 0.65 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:33.856628: step 84300, loss = 0.85 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:34.622900: step 84310, loss = 0.79 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:35.385632: step 84320, loss = 0.65 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:36.145782: step 84330, loss = 1.01 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:36.912137: step 84340, loss = 0.81 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:37.685430: step 84350, loss = 0.75 (1655.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:38.454326: step 84360, loss = 0.76 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:39.220207: step 84370, loss = 0.70 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:39.975207: step 84380, loss = 0.79 (1695.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:40.735152: step 84390, loss = 0.88 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:41.496426: step 84400, loss = 0.85 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:42.262974: step 84410, loss = 0.79 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:43.029191: step 84420, loss = 0.81 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:43.786538: step 84430, loss = 0.79 (1690.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:44.559240: step 84440, loss = 0.75 (1656.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:45.326705: step 84450, loss = 0.76 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:46.084644: step 84460, loss = 0.86 (1688.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:46.850197: step 84470, loss = 0.70 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:47.609800: step 84480, loss = 0.70 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:48.377823: step 84490, loss = 0.86 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:49.141402: step 84500, loss = 0.68 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:49.920933: step 84510, loss = 0.87 (1642.0 examples/sec; 0.078 sec/batch)
2017-05-05 19:13:50.680685: step 84520, loss = 0.68 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:51.439548: step 84530, loss = 0.62 (1686.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:52.202422: step 84540, loss = 0.76 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:52.969481: step 84550, loss = 0.72 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:53.731624: step 84560, loss = 0.72 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:54.501090: step 84570, loss = 0.62 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:55.261294: step 84580, loss = 0.79 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:56.014724: step 84590, loss = 0.60 (1698.9 examples/sec; 0.075 sec/batch)
2017-05-05 19:13:56.783123: step 84600, loss = 0.56 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:57.542864: step 84610, loss = 0.71 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:58.313464: step 84620, loss = 0.91 (1661.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:13:59.077861: step 84630, loss = 0.79 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:13:59.833310: step 84640, loss = 0.72 (1694.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:00.591834: step 84650, loss = 0.75 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:01.360341: step 84660, loss = 0.71 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:14:02.121058: step 84670, loss = 0.72 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:02.887639: step 84680, loss = 0.87 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:14:03.644722: step 84690, loss = 0.76 (1690.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:04.417414: step 84700, loss = 0.83 (1656.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:14:05.186387: step 84710, loss = 0.75 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:14:05.951304: step 84720, loss = 0.65 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:06.719351: step 84730, loss = 0.86 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:14:07.477777: step 84740, loss = 0.75 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:08.247353: step 84750, loss = 0.75 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:14:09.014595: step 84760, loss = 0.80 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:14:09.782710: step 84770, loss = 0.68 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:14:10.547174: step 84780, loss = 0.77 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:11.310220: step 84790, loss = 0.95 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:12.065559: step 84800, loss = 0.77 (1694.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:12.839621: step 84810, loss = 0.97 (1653.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:14:13.603791: step 84820, loss = 0.78 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:14.366554: step 84830, loss = 0.67 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:15.127615: step 84840, loss = 0.71 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:15.892919: step 84850, loss = 0.66 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:14:16.656845: step 84860, loss = 0.87 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:17.424571: step 84870, loss = 0.72 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:14:18.202528: step 84880, loss = 0.85 (1645.3 examples/sec; 0.078 sec/batch)
2017-05-05 19:14:18.970071: step 84890, loss = 0.75 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:14:19.730513: step 84900, loss = 0.83 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:20.494212: step 84910, loss = 0.77 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:21.258501: step 84920, loss = 0.76 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:22.024429: step 84930, loss = 0.96 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:14:22.789485: step 84940, loss = 0.88 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:14:23.550489: step 84950, loss = 0.69 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:24.316215: step 84960, loss = 0.94 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:14:25.078883: step 84970, loss = 0.89 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:25.843731: step 84980, loss = 0.74 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:26.609321: step 84990, loss = 0.82 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:14:27.377210: step 85000, loss = 0.66 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:14:28.138518: step 85010, loss = 0.73 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:28.903625: step 85020, loss = 0.79 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:14:29.663870: step 85030, loss = 0.90 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:30.429940: step 85040, loss = 0.83 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:14:31.192325: step 85050, loss = 0.75 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:31.945173: step 85060, loss = 0.78 (1700.2 examples/sec; 0.075 sec/batch)
2017-05-05 19:14:32.720572: step 85070, loss = 0.82 (1650.8 examples/sec; 0.078 sec/batch)
2017-05-05 19:14:33.483525: step 85080, loss = 0.57 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:34.249659: step 85090, loss = 0.76 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:14:35.014680: step 85100, loss = 0.76 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:14:35.770388: step 85110, loss = 0.71 (1693.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:36.541008: step 85120, loss = 0.90 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:14:37.305556: step 85130, loss = 0.63 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:38.169739: step 85140, loss = 0.81 (1481.2 examples/sec; 0.086 sec/batch)
2017-05-05 19:14:38.838326: step 85150, loss = 0.74 (1914.5 examples/sec; 0.067 sec/batch)
2017-05-05 19:14:39.596458: step 85160, loss = 0.73 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:40.357484: step 85170, loss = 0.85 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:41.122986: step 85180, loss = 0.86 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:14:41.887519: step 85190, loss = 0.80 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:42.653952: step 85200, loss = 0.62 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:14:43.415752: step 85210, loss = 0.71 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:44.175278: step 85220, loss = 0.69 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:44.943808: step 85230, loss = 0.74 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:14:45.707562: step 85240, loss = 0.85 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:46.471831: step 85250, loss = 0.71 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:47.237739: step 85260, loss = 0.77 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:14:47.998823: step 85270, loss = 0.79 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:48.763811: step 85280, loss = 0.73 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:49.532881: step 85290, loss = 0.86 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:14:50.301541: step 85300, loss = 0.79 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:14:51.056829: step 85310, loss = 0.71 (1694.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:51.811438: step 85320, loss = 0.76 (1696.3 examples/sec; 0.075 sec/batch)
2017-05-05 19:14:52.582733: step 85330, loss = 0.84 (1659.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:14:53.343815: step 85340, loss = 0.67 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:54.108521: step 85350, loss = 0.68 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:54.873075: step 85360, loss = 0.73 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:55.630543: step 85370, loss = 0.84 (1689.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:56.396149: step 85380, loss = 0.79 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:14:57.169980: step 85390, loss = 0.79 (1654.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:14:57.937254: step 85400, loss = 0.69 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:14:58.698894: step 85410, loss = 0.81 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:14:59.456280: step 85420, loss = 0.66 (1690.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:00.217634: step 85430, loss = 0.73 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:00.978630: step 85440, loss = 0.98 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:01.749291: step 85450, loss = 0.73 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:02.520497: step 85460, loss = 0.75 (1659.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:03.282594: step 85470, loss = 0.76 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:04.045806: step 85480, loss = 0.85 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:04.816167: step 85490, loss = 0.88 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:05.575408: step 85500, loss = 0.67 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:06.346090: step 85510, loss = 0.77 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:07.111217: step 85520, loss = 0.74 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:07.868402: step 85530, loss = 0.70 (1690.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:08.638177: step 85540, loss = 0.89 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:09.403896: step 85550, loss = 0.90 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:10.169207: step 85560, loss = 0.88 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:10.927883: step 85570, loss = 0.73 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:11.684923: step 85580, loss = 0.68 (1690.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:12.450569: step 85590, loss = 0.64 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:13.218149: step 85600, loss = 0.81 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:13.981339: step 85610, loss = 0.89 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:14.744451: step 85620, loss = 0.72 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:15.505140: step 85630, loss = 0.68 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:16.269644: step 85640, loss = 0.68 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:17.033396: step 85650, loss = 0.91 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:17.798586: step 85660, loss = 0.83 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:18.564582: step 85670, loss = 0.83 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:19.333314: step 85680, loss = 0.74 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:20.094535: step 85690, loss = 0.72 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:20.859331: step 85700, loss = 0.68 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:21.622374: step 85710, loss = 0.85 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:22.388589: step 85720, loss = 0.83 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:23.154822: step 85730, loss = 0.87 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:23.910921: step 85740, loss = 0.64 (1692.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:24.677702: step 85750, loss = 0.93 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:25.447351: step 85760, loss = 0.78 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:26.207139: step 85770, loss = 0.73 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:26.971552: step 85780, loss = 0.79 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:27.728338: step 85790, loss = 0.71 (1691.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:28.495867: step 85800, loss = 0.82 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:29.268487: step 85810, loss = 0.76 (1656.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:30.030958: step 85820, loss = 0.90 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:30.796401: step 85830, loss = 0.79 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:31.558197: step 85840, loss = 0.74 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:32.322005: step 85850, loss = 0.73 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:33.084237: step 85860, loss = 0.63 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:33.847093: step 85870, loss = 0.86 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:34.613382: step 85880, loss = 0.68 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:35.378920: step 85890, loss = 0.80 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:36.135344: step 85900, loss = 0.65 (1692.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:36.897214: step 85910, loss = 0.87 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:37.663319: step 85920, loss = 0.70 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:38.431878: step 85930, loss = 0.60 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:39.200098: step 85940, loss = 0.80 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:39.953424: step 85950, loss = 0.78 (1699.1 examples/sec; 0.075 sec/batch)
2017-05-05 19:15:40.720637: step 85960, loss = 0.74 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:41.489039: step 85970, loss = 0.82 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:42.251174: step 85980, loss = 0.69 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:43.016172: step 85990, loss = 0.74 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:43.771397: step 86000, loss = 0.79 (1694.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:44.543418: step 86010, loss = 0.78 (1658.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:45.309353: step 86020, loss = 0.72 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:46.067455: step 86030, loss = 0.83 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:46.836980: step 86040, loss = 0.70 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:47.595036: step 86050, loss = 0.75 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:48.358291: step 86060, loss = 0.68 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:49.121390: step 86070, loss = 0.90 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:49.886845: step 86080, loss = 0.83 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:50.646886: step 86090, loss = 0.76 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:51.415136: step 86100, loss = 0.65 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:52.180335: step 86110, loss = 0.80 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:52.939610: step 86120, loss = 0.73 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:15:53.805526: step 86130, loss = 0.71 (1478.2 examples/sec; 0.087 sec/batch)
2017-05-05 19:15:54.472046: step 86140, loss = 0.85 (1920.4 examples/sec; 0.067 sec/batch)
2017-05-05 19:15:55.239226: step 86150, loss = 0.84 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:55.992669: step 86160, loss = 0.74 (1698.9 examples/sec; 0.075 sec/batch)
2017-05-05 19:15:56.758639: step 86170, loss = 0.78 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:57.524325: step 86180, loss = 0.88 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:58.292443: step 86190, loss = 0.72 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:59.062161: step 86200, loss = 0.86 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:15:59.816034: step 86210, loss = 0.85 (1697.9 examples/sec; 0.075 sec/batch)
2017-05-05 19:16:00.577299: step 86220, loss = 0.68 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:01.350020: step 86230, loss = 0.82 (1656.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:02.107149: step 86240, loss = 0.58 (1690.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:02.870059: step 86250, loss = 0.69 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:03.627873: step 86260, loss = 0.78 (1689.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:04.397744: step 86270, loss = 0.71 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:05.163627: step 86280, loss = 0.75 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:05.967979: step 86290, loss = 0.73 (1591.3 examples/sec; 0.080 sec/batch)
2017-05-05 19:16:06.729932: step 86300, loss = 0.74 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:07.491617: step 86310, loss = 0.73 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:08.257893: step 86320, loss = 0.82 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:09.025832: step 86330, loss = 0.63 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:09.788038: step 86340, loss = 0.79 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:10.554725: step 86350, loss = 0.79 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:11.312858: step 86360, loss = 0.84 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:12.072037: step 86370, loss = 0.70 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:12.845045: step 86380, loss = 0.71 (1655.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:13.620091: step 86390, loss = 0.98 (1651.5 examples/sec; 0.078 sec/batch)
2017-05-05 19:16:14.390375: step 86400, loss = 0.72 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:15.151988: step 86410, loss = 0.81 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:15.907998: step 86420, loss = 0.88 (1693.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:16.681289: step 86430, loss = 0.72 (1655.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:17.440953: step 86440, loss = 0.71 (1685.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:18.203334: step 86450, loss = 0.75 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:18.969041: step 86460, loss = 0.79 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:19.726332: step 86470, loss = 0.65 (1690.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:20.493256: step 86480, loss = 0.72 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:21.251100: step 86490, loss = 0.72 (1689.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:22.008637: step 86500, loss = 0.80 (1689.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:22.774851: step 86510, loss = 0.66 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:23.535537: step 86520, loss = 0.69 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:24.299842: step 86530, loss = 0.83 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:25.063380: step 86540, loss = 0.80 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:25.828255: step 86550, loss = 0.79 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:26.589087: step 86560, loss = 0.74 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:27.352150: step 86570, loss = 0.72 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:28.110744: step 86580, loss = 0.80 (1687.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:28.879179: step 86590, loss = 0.66 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:29.645449: step 86600, loss = 0.74 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:30.412325: step 86610, loss = 0.85 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:31.172390: step 86620, loss = 0.70 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:31.932187: step 86630, loss = 0.67 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:32.699545: step 86640, loss = 0.81 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:33.470234: step 86650, loss = 0.71 (1660.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:34.239487: step 86660, loss = 0.72 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:35.009270: step 86670, loss = 0.72 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:35.768230: step 86680, loss = 0.82 (1686.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:36.537175: step 86690, loss = 0.70 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:37.299860: step 86700, loss = 0.82 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:38.068130: step 86710, loss = 0.59 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:38.833581: step 86720, loss = 0.74 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:39.596949: step 86730, loss = 1.00 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:40.364999: step 86740, loss = 0.76 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:41.133198: step 86750, loss = 0.66 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:41.898124: step 86760, loss = 0.63 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:42.663626: step 86770, loss = 0.78 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:43.422476: step 86780, loss = 0.93 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:44.182165: step 86790, loss = 0.68 (1684.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:44.943068: step 86800, loss = 0.82 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:45.705493: step 86810, loss = 0.74 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:46.473813: step 86820, loss = 0.99 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:47.234372: step 86830, loss = 0.82 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:47.995267: step 86840, loss = 0.77 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:48.763296: step 86850, loss = 0.65 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:49.526499: step 86860, loss = 0.63 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:50.291970: step 86870, loss = 0.92 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:51.044898: step 86880, loss = 0.83 (1700.0 examples/sec; 0.075 sec/batch)
2017-05-05 19:16:51.805984: step 86890, loss = 0.69 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:52.564131: step 86900, loss = 0.77 (1688.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:53.335037: step 86910, loss = 0.93 (1660.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:54.099685: step 86920, loss = 0.94 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:16:54.871849: step 86930, loss = 0.81 (1657.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:55.626341: step 86940, loss = 0.73 (1696.5 examples/sec; 0.075 sec/batch)
2017-05-05 19:16:56.398751: step 86950, loss = 0.72 (1657.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:57.166417: step 86960, loss = 0.82 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:57.936037: step 86970, loss = 0.80 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:58.707542: step 86980, loss = 0.74 (1659.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:16:59.469448: step 86990, loss = 0.78 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:00.232856: step 87000, loss = 0.71 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:00.985396: step 87010, loss = 0.75 (1700.9 examples/sec; 0.075 sec/batch)
2017-05-05 19:17:01.746750: step 87020, loss = 0.75 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:02.514837: step 87030, loss = 0.60 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:03.277027: step 87040, loss = 0.78 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:04.039694: step 87050, loss = 0.67 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:04.808155: step 87060, loss = 0.68 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:05.578497: step 87070, loss = 0.74 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:06.347548: step 87080, loss = 0.65 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:07.117136: step 87090, loss = 0.71 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:07.874588: step 87100, loss = 0.68 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:08.637818: step 87110, loss = 0.64 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:09.512469: step 87120, loss = 0.86 (1463.4 examples/sec; 0.087 sec/batch)
2017-05-05 19:17:10.172895: step 87130, loss = 0.78 (1938.1 examples/sec; 0.066 sec/batch)
2017-05-05 19:17:10.929992: step 87140, loss = 0.73 (1690.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:11.687672: step 87150, loss = 0.87 (1689.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:12.455016: step 87160, loss = 0.80 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:13.226049: step 87170, loss = 0.68 (1660.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:13.989037: step 87180, loss = 0.81 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:14.757312: step 87190, loss = 0.69 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:15.516261: step 87200, loss = 0.78 (1686.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:16.281814: step 87210, loss = 0.80 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:17.048628: step 87220, loss = 0.83 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:17.813054: step 87230, loss = 0.81 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:18.580091: step 87240, loss = 0.69 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:19.342779: step 87250, loss = 0.76 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:20.099265: step 87260, loss = 0.75 (1692.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:20.863706: step 87270, loss = 0.76 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:21.633718: step 87280, loss = 0.71 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:22.401081: step 87290, loss = 0.82 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:23.166410: step 87300, loss = 0.66 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:23.926302: step 87310, loss = 0.67 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:24.693675: step 87320, loss = 0.84 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:25.462074: step 87330, loss = 0.84 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:26.227117: step 87340, loss = 0.70 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:26.990502: step 87350, loss = 0.71 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:27.744720: step 87360, loss = 0.84 (1697.1 examples/sec; 0.075 sec/batch)
2017-05-05 19:17:28.512698: step 87370, loss = 0.70 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:29.282823: step 87380, loss = 0.76 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:30.046268: step 87390, loss = 0.92 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:30.803800: step 87400, loss = 0.72 (1689.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:31.561933: step 87410, loss = 0.62 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:32.327494: step 87420, loss = 0.80 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:33.101520: step 87430, loss = 0.72 (1653.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:33.860300: step 87440, loss = 0.64 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:34.624464: step 87450, loss = 0.74 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:35.383439: step 87460, loss = 0.58 (1686.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:36.142684: step 87470, loss = 0.72 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:36.899061: step 87480, loss = 0.82 (1692.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:37.667690: step 87490, loss = 0.77 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:38.436525: step 87500, loss = 0.80 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:39.206565: step 87510, loss = 0.95 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:39.969165: step 87520, loss = 0.85 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:40.734425: step 87530, loss = 0.87 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:41.500340: step 87540, loss = 0.82 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:42.266608: step 87550, loss = 0.79 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:43.026156: step 87560, loss = 0.68 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:43.787247: step 87570, loss = 0.83 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:44.546325: step 87580, loss = 0.71 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:45.316220: step 87590, loss = 0.66 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:46.080950: step 87600, loss = 0.67 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:46.846795: step 87610, loss = 0.84 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:47.605691: step 87620, loss = 0.81 (1686.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:48.369538: step 87630, loss = 0.78 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:49.134464: step 87640, loss = 0.82 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:49.900167: step 87650, loss = 0.79 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:50.661756: step 87660, loss = 0.78 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:51.422814: step 87670, loss = 0.66 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:52.180811: step 87680, loss = 0.76 (1688.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:52.940887: step 87690, loss = 0.96 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:53.707435: step 87700, loss = 0.77 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:54.478585: step 87710, loss = 0.75 (1659.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:55.239781: step 87720, loss = 0.76 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:55.993986: step 87730, loss = 0.76 (1697.2 examples/sec; 0.075 sec/batch)
2017-05-05 19:17:56.758528: step 87740, loss = 0.92 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:57.524348: step 87750, loss = 0.67 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:58.289863: step 87760, loss = 0.58 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:17:59.051431: step 87770, loss = 0.84 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:17:59.809242: step 87780, loss = 0.84 (1689.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:00.570086: step 87790, loss = 0.70 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:01.328319: step 87800, loss = 0.94 (1688.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:02.091133: step 87810, loss = 0.89 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:02.861282: step 87820, loss = 0.81 (1662.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:18:03.615421: step 87830, loss = 0.67 (1697.3 examples/sec; 0.075 sec/batch)
2017-05-05 19:18:04.373835: step 87840, loss = 0.73 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:05.144149: step 87850, loss = 0.78 (1661.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:18:05.904525: step 87860, loss = 0.83 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:06.671986: step 87870, loss = 0.81 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:18:07.430961: step 87880, loss = 0.86 (1686.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:08.193659: step 87890, loss = 0.76 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:08.959927: step 87900, loss = 0.78 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:18:09.726734: step 87910, loss = 0.71 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:18:10.498562: step 87920, loss = 0.69 (1658.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:18:11.265902: step 87930, loss = 0.81 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:18:12.021002: step 87940, loss = 0.75 (1695.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:12.783003: step 87950, loss = 0.71 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:13.546655: step 87960, loss = 0.73 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:14.312274: step 87970, loss = 0.81 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:18:15.074783: step 87980, loss = 0.76 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:15.831709: step 87990, loss = 0.70 (1691.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:16.602459: step 88000, loss = 0.87 (1660.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:18:17.369922: step 88010, loss = 0.72 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:18:18.132370: step 88020, loss = 0.68 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:18.903299: step 88030, loss = 0.83 (1660.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:18:19.660165: step 88040, loss = 0.70 (1691.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:20.426470: step 88050, loss = 0.53 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:18:21.192086: step 88060, loss = 0.78 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:18:21.952325: step 88070, loss = 0.90 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:22.723258: step 88080, loss = 0.89 (1660.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:18:23.486136: step 88090, loss = 0.82 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:24.248062: step 88100, loss = 0.70 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:25.108539: step 88110, loss = 0.65 (1487.5 examples/sec; 0.086 sec/batch)
2017-05-05 19:18:25.780438: step 88120, loss = 0.71 (1905.0 examples/sec; 0.067 sec/batch)
2017-05-05 19:18:26.551348: step 88130, loss = 0.91 (1660.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:18:27.312402: step 88140, loss = 0.82 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:28.069679: step 88150, loss = 0.73 (1690.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:28.830634: step 88160, loss = 0.72 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:29.603149: step 88170, loss = 0.69 (1657.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:18:30.366027: step 88180, loss = 0.90 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:31.127459: step 88190, loss = 0.82 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:31.892449: step 88200, loss = 0.71 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:32.653475: step 88210, loss = 0.57 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:33.420636: step 88220, loss = 0.82 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:18:34.183776: step 88230, loss = 0.65 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:34.954159: step 88240, loss = 0.85 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:18:35.704516: step 88250, loss = 0.69 (1705.9 examples/sec; 0.075 sec/batch)
2017-05-05 19:18:36.468347: step 88260, loss = 0.84 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:37.226648: step 88270, loss = 0.95 (1688.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:37.989015: step 88280, loss = 0.74 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:38.758690: step 88290, loss = 0.73 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:18:39.520105: step 88300, loss = 0.74 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:40.284726: step 88310, loss = 0.86 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:41.047495: step 88320, loss = 0.67 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:41.816190: step 88330, loss = 0.83 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:18:42.583219: step 88340, loss = 0.73 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:18:43.348878: step 88350, loss = 0.82 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:18:44.103320: step 88360, loss = 0.67 (1696.6 examples/sec; 0.075 sec/batch)
2017-05-05 19:18:44.871615: step 88370, loss = 1.03 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:18:45.635942: step 88380, loss = 0.93 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:46.405143: step 88390, loss = 0.78 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:18:47.172468: step 88400, loss = 0.79 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:18:47.924547: step 88410, loss = 0.70 (1702.0 examples/sec; 0.075 sec/batch)
2017-05-05 19:18:48.689346: step 88420, loss = 0.71 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:49.447533: step 88430, loss = 0.74 (1688.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:50.218623: step 88440, loss = 0.89 (1660.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:18:50.982076: step 88450, loss = 0.71 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:51.740506: step 88460, loss = 0.88 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:52.507988: step 88470, loss = 0.77 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:18:53.275584: step 88480, loss = 0.77 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:18:54.047752: step 88490, loss = 0.93 (1657.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:18:54.821843: step 88500, loss = 0.73 (1653.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:18:55.580204: step 88510, loss = 0.73 (1687.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:56.343925: step 88520, loss = 0.69 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:57.104924: step 88530, loss = 0.87 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:57.869219: step 88540, loss = 0.86 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:18:58.634884: step 88550, loss = 0.73 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:18:59.396612: step 88560, loss = 0.82 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:00.154310: step 88570, loss = 0.75 (1689.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:00.918273: step 88580, loss = 0.74 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:01.681219: step 88590, loss = 0.75 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:02.450304: step 88600, loss = 0.67 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:19:03.216152: step 88610, loss = 0.74 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:19:03.976710: step 88620, loss = 0.83 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:04.735560: step 88630, loss = 0.73 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:05.500748: step 88640, loss = 0.77 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:19:06.269857: step 88650, loss = 0.79 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:19:07.035876: step 88660, loss = 0.73 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:19:07.791901: step 88670, loss = 0.67 (1693.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:08.560001: step 88680, loss = 0.90 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:19:09.321276: step 88690, loss = 0.65 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:10.088494: step 88700, loss = 0.74 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:19:10.851900: step 88710, loss = 0.86 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:11.614897: step 88720, loss = 0.80 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:12.379281: step 88730, loss = 0.78 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:13.142728: step 88740, loss = 0.79 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:13.903621: step 88750, loss = 0.68 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:14.664799: step 88760, loss = 0.74 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:15.427478: step 88770, loss = 0.83 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:16.185961: step 88780, loss = 0.85 (1687.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:16.948278: step 88790, loss = 0.79 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:17.713608: step 88800, loss = 0.68 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:19:18.483036: step 88810, loss = 0.61 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:19:19.248500: step 88820, loss = 0.83 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:19:20.001879: step 88830, loss = 0.87 (1699.0 examples/sec; 0.075 sec/batch)
2017-05-05 19:19:20.761298: step 88840, loss = 0.69 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:21.528213: step 88850, loss = 0.78 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:19:22.299669: step 88860, loss = 0.62 (1659.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:19:23.066554: step 88870, loss = 0.76 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:19:23.822543: step 88880, loss = 0.74 (1693.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:24.586422: step 88890, loss = 0.90 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:25.350253: step 88900, loss = 0.77 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:26.117665: step 88910, loss = 0.67 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:19:26.875081: step 88920, loss = 0.69 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:27.634481: step 88930, loss = 0.68 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:28.404269: step 88940, loss = 0.79 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:19:29.164945: step 88950, loss = 0.76 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:29.934473: step 88960, loss = 0.75 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:19:30.691864: step 88970, loss = 0.62 (1690.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:31.448808: step 88980, loss = 0.89 (1691.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:32.207498: step 88990, loss = 0.73 (1687.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:32.969214: step 89000, loss = 0.64 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:33.732937: step 89010, loss = 0.90 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:34.500253: step 89020, loss = 0.68 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:19:35.263508: step 89030, loss = 0.77 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:36.026817: step 89040, loss = 0.87 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:36.792523: step 89050, loss = 0.81 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:19:37.557123: step 89060, loss = 0.72 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:38.325077: step 89070, loss = 0.73 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:19:39.092802: step 89080, loss = 0.73 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:19:39.844071: step 89090, loss = 0.72 (1703.8 examples/sec; 0.075 sec/batch)
2017-05-05 19:19:40.724734: step 89100, loss = 0.71 (1453.4 examples/sec; 0.088 sec/batch)
2017-05-05 19:19:41.372866: step 89110, loss = 0.74 (1974.9 examples/sec; 0.065 sec/batch)
2017-05-05 19:19:42.138381: step 89120, loss = 0.69 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:19:42.904703: step 89130, loss = 0.68 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:19:43.667980: step 89140, loss = 0.77 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:44.431392: step 89150, loss = 0.97 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:45.201889: step 89160, loss = 0.85 (1661.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:19:45.962107: step 89170, loss = 0.92 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:46.727660: step 89180, loss = 0.79 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:19:47.482377: step 89190, loss = 0.78 (1696.0 examples/sec; 0.075 sec/batch)
2017-05-05 19:19:48.248466: step 89200, loss = 0.83 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:19:49.017118: step 89210, loss = 0.83 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:19:49.787988: step 89220, loss = 0.83 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:19:50.551171: step 89230, loss = 0.95 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:51.313093: step 89240, loss = 0.78 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:52.066833: step 89250, loss = 0.92 (1698.2 examples/sec; 0.075 sec/batch)
2017-05-05 19:19:52.828126: step 89260, loss = 0.81 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:53.601927: step 89270, loss = 0.80 (1654.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:19:54.369660: step 89280, loss = 0.72 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:19:55.131475: step 89290, loss = 0.71 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:55.893789: step 89300, loss = 0.82 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:56.657902: step 89310, loss = 0.88 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:57.421216: step 89320, loss = 0.73 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:58.182494: step 89330, loss = 0.62 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:58.942793: step 89340, loss = 0.77 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:19:59.700494: step 89350, loss = 0.69 (1689.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:00.462178: step 89360, loss = 0.76 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:01.223888: step 89370, loss = 0.89 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:01.999278: step 89380, loss = 0.68 (1650.8 examples/sec; 0.078 sec/batch)
2017-05-05 19:20:02.767562: step 89390, loss = 0.72 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:03.523249: step 89400, loss = 0.74 (1693.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:04.290928: step 89410, loss = 0.74 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:05.059049: step 89420, loss = 0.68 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:05.822626: step 89430, loss = 0.91 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:06.587691: step 89440, loss = 0.74 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:07.351455: step 89450, loss = 0.95 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:08.106147: step 89460, loss = 0.73 (1696.1 examples/sec; 0.075 sec/batch)
2017-05-05 19:20:08.872382: step 89470, loss = 0.77 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:09.636078: step 89480, loss = 0.74 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:10.397767: step 89490, loss = 0.83 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:11.164758: step 89500, loss = 0.73 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:11.921627: step 89510, loss = 0.83 (1691.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:12.690343: step 89520, loss = 0.73 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:13.455258: step 89530, loss = 0.71 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:14.218792: step 89540, loss = 0.64 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:14.981384: step 89550, loss = 0.77 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:15.738080: step 89560, loss = 0.80 (1691.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:16.505478: step 89570, loss = 0.64 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:17.274737: step 89580, loss = 0.80 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:18.033788: step 89590, loss = 0.79 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:18.801901: step 89600, loss = 0.85 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:19.558914: step 89610, loss = 0.88 (1690.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:20.321132: step 89620, loss = 0.91 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:21.088790: step 89630, loss = 0.72 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:21.856273: step 89640, loss = 0.72 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:22.620122: step 89650, loss = 0.87 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:23.383026: step 89660, loss = 0.65 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:24.140656: step 89670, loss = 0.84 (1689.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:24.911948: step 89680, loss = 0.72 (1659.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:25.672411: step 89690, loss = 0.78 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:26.439551: step 89700, loss = 0.78 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:27.204863: step 89710, loss = 0.64 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:27.962689: step 89720, loss = 0.84 (1689.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:28.731050: step 89730, loss = 0.75 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:29.504390: step 89740, loss = 0.89 (1655.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:30.267776: step 89750, loss = 0.87 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:31.025161: step 89760, loss = 0.68 (1690.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:31.782822: step 89770, loss = 0.72 (1689.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:32.551903: step 89780, loss = 0.85 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:33.318795: step 89790, loss = 0.95 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:34.085962: step 89800, loss = 0.73 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:34.847312: step 89810, loss = 0.76 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:35.610498: step 89820, loss = 0.78 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:36.371897: step 89830, loss = 0.72 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:37.131343: step 89840, loss = 0.74 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:37.899091: step 89850, loss = 0.64 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:38.665860: step 89860, loss = 0.63 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:39.424824: step 89870, loss = 0.83 (1686.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:40.185630: step 89880, loss = 0.66 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:40.940224: step 89890, loss = 0.74 (1696.3 examples/sec; 0.075 sec/batch)
2017-05-05 19:20:41.706892: step 89900, loss = 0.71 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:42.470331: step 89910, loss = 0.81 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:43.235650: step 89920, loss = 0.65 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:44.001617: step 89930, loss = 0.87 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:44.760213: step 89940, loss = 0.82 (1687.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:45.526426: step 89950, loss = 0.80 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:46.288344: step 89960, loss = 0.77 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:47.054627: step 89970, loss = 0.73 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:47.813628: step 89980, loss = 0.75 (1686.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:48.584581: step 89990, loss = 0.82 (1660.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:49.351443: step 90000, loss = 0.70 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:50.115282: step 90010, loss = 0.79 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:50.880309: step 90020, loss = 0.75 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:51.635186: step 90030, loss = 0.85 (1695.6 examples/sec; 0.075 sec/batch)
2017-05-05 19:20:52.404699: step 90040, loss = 0.68 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:53.171552: step 90050, loss = 0.91 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:53.943569: step 90060, loss = 0.76 (1658.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:54.699712: step 90070, loss = 0.68 (1692.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:55.461077: step 90080, loss = 0.69 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:56.329648: step 90090, loss = 0.70 (1473.7 examples/sec; 0.087 sec/batch)
2017-05-05 19:20:56.998570: step 90100, loss = 0.82 (1913.5 examples/sec; 0.067 sec/batch)
2017-05-05 19:20:57.757053: step 90110, loss = 0.66 (1687.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:20:58.528405: step 90120, loss = 0.83 (1659.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:20:59.293589: step 90130, loss = 0.91 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:21:00.052697: step 90140, loss = 0.72 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:00.816672: step 90150, loss = 0.83 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:01.582405: step 90160, loss = 0.66 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:21:02.347563: step 90170, loss = 0.86 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:21:03.112704: step 90180, loss = 0.76 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:21:03.869251: step 90190, loss = 0.69 (1691.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:04.633433: step 90200, loss = 0.58 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:05.400345: step 90210, loss = 0.65 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:21:06.164715: step 90220, loss = 0.69 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:06.929574: step 90230, loss = 0.89 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:07.690985: step 90240, loss = 0.82 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:08.457662: step 90250, loss = 0.92 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:21:09.220161: step 90260, loss = 0.74 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:09.986462: step 90270, loss = 0.84 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:21:10.748350: step 90280, loss = 0.84 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:11.509761: step 90290, loss = 0.70 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:12.273551: step 90300, loss = 0.83 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:13.033815: step 90310, loss = 0.65 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:13.801181: step 90320, loss = 0.80 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:21:14.561807: step 90330, loss = 0.63 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:15.324255: step 90340, loss = 0.91 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:16.083754: step 90350, loss = 0.88 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:16.845598: step 90360, loss = 0.65 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:17.610541: step 90370, loss = 0.63 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:18.379390: step 90380, loss = 0.81 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:21:19.143834: step 90390, loss = 0.65 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:19.903740: step 90400, loss = 0.83 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:20.670059: step 90410, loss = 0.93 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:21:21.432148: step 90420, loss = 0.86 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:22.194800: step 90430, loss = 0.78 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:22.961975: step 90440, loss = 0.76 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:21:23.718982: step 90450, loss = 0.65 (1690.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:24.474714: step 90460, loss = 0.72 (1693.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:25.246909: step 90470, loss = 0.87 (1657.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:21:26.010084: step 90480, loss = 0.73 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:26.776493: step 90490, loss = 0.81 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:21:27.535671: step 90500, loss = 0.80 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:28.294313: step 90510, loss = 0.81 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:29.061370: step 90520, loss = 0.82 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:21:29.829881: step 90530, loss = 0.74 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:21:30.600275: step 90540, loss = 0.71 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:21:31.363522: step 90550, loss = 0.71 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:32.120465: step 90560, loss = 0.76 (1691.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:32.884789: step 90570, loss = 0.78 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:33.662206: step 90580, loss = 0.91 (1646.5 examples/sec; 0.078 sec/batch)
2017-05-05 19:21:34.433605: step 90590, loss = 0.79 (1659.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:21:35.202992: step 90600, loss = 0.71 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:21:35.956093: step 90610, loss = 0.74 (1699.7 examples/sec; 0.075 sec/batch)
2017-05-05 19:21:36.717838: step 90620, loss = 0.90 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:37.489287: step 90630, loss = 0.82 (1659.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:21:38.249516: step 90640, loss = 0.77 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:39.017146: step 90650, loss = 0.68 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:21:39.767854: step 90660, loss = 0.70 (1705.1 examples/sec; 0.075 sec/batch)
2017-05-05 19:21:40.528812: step 90670, loss = 0.74 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:41.289401: step 90680, loss = 0.78 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:42.059378: step 90690, loss = 0.67 (1662.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:21:42.824510: step 90700, loss = 0.82 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:21:43.583960: step 90710, loss = 0.65 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:44.343068: step 90720, loss = 0.81 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:45.106686: step 90730, loss = 0.86 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:45.866107: step 90740, loss = 0.87 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:46.634159: step 90750, loss = 0.90 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:21:47.395320: step 90760, loss = 0.96 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:48.156635: step 90770, loss = 0.72 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:48.919434: step 90780, loss = 0.78 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:49.691103: step 90790, loss = 0.60 (1658.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:21:50.456447: step 90800, loss = 0.89 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:21:51.217003: step 90810, loss = 0.65 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:51.970943: step 90820, loss = 0.95 (1697.8 examples/sec; 0.075 sec/batch)
2017-05-05 19:21:52.737244: step 90830, loss = 0.78 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:21:53.498605: step 90840, loss = 0.75 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:21:54.266764: step 90850, loss = 0.75 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:21:55.042579: step 90860, loss = 0.73 (1649.9 examples/sec; 0.078 sec/batch)
2017-05-05 19:21:55.796489: step 90870, loss = 0.67 (1697.8 examples/sec; 0.075 sec/batch)
2017-05-05 19:21:56.567169: step 90880, loss = 0.72 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:21:57.344993: step 90890, loss = 0.80 (1645.6 examples/sec; 0.078 sec/batch)
2017-05-05 19:21:58.118599: step 90900, loss = 0.70 (1654.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:21:58.892380: step 90910, loss = 0.87 (1654.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:21:59.656030: step 90920, loss = 0.60 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:00.423703: step 90930, loss = 0.64 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:22:01.193150: step 90940, loss = 0.69 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:22:01.957346: step 90950, loss = 0.85 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:02.719433: step 90960, loss = 0.65 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:03.474603: step 90970, loss = 0.65 (1695.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:04.237735: step 90980, loss = 0.76 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:05.004469: step 90990, loss = 0.76 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:22:05.765275: step 91000, loss = 0.74 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:06.533352: step 91010, loss = 0.79 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:22:07.297842: step 91020, loss = 0.71 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:08.055273: step 91030, loss = 0.70 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:08.822653: step 91040, loss = 0.83 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:22:09.591369: step 91050, loss = 0.68 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:22:10.359558: step 91060, loss = 0.75 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:22:11.119935: step 91070, loss = 0.75 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:11.976241: step 91080, loss = 0.73 (1494.8 examples/sec; 0.086 sec/batch)
2017-05-05 19:22:12.643544: step 91090, loss = 0.68 (1918.2 examples/sec; 0.067 sec/batch)
2017-05-05 19:22:13.413058: step 91100, loss = 0.56 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:22:14.181632: step 91110, loss = 0.69 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:22:14.945021: step 91120, loss = 0.76 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:15.703725: step 91130, loss = 0.70 (1687.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:16.473802: step 91140, loss = 0.80 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:22:17.234573: step 91150, loss = 0.57 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:17.999694: step 91160, loss = 0.80 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:22:18.763121: step 91170, loss = 0.83 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:19.527621: step 91180, loss = 0.74 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:20.290269: step 91190, loss = 0.73 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:21.048720: step 91200, loss = 0.79 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:21.808178: step 91210, loss = 0.83 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:22.570209: step 91220, loss = 0.65 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:23.333449: step 91230, loss = 0.69 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:24.093389: step 91240, loss = 0.76 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:24.859760: step 91250, loss = 0.78 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:22:25.621431: step 91260, loss = 0.78 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:26.389147: step 91270, loss = 0.74 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:22:27.149455: step 91280, loss = 0.78 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:27.907442: step 91290, loss = 1.04 (1688.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:28.671154: step 91300, loss = 0.72 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:29.437852: step 91310, loss = 0.74 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:22:30.208333: step 91320, loss = 0.83 (1661.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:22:30.969098: step 91330, loss = 0.67 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:31.725014: step 91340, loss = 0.71 (1693.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:32.492709: step 91350, loss = 0.79 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:22:33.259539: step 91360, loss = 0.65 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:22:34.028612: step 91370, loss = 0.84 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:22:34.795471: step 91380, loss = 0.79 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:22:35.555632: step 91390, loss = 0.69 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:36.318222: step 91400, loss = 0.70 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:37.083249: step 91410, loss = 0.79 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:22:37.847306: step 91420, loss = 0.75 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:38.614604: step 91430, loss = 0.75 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:22:39.375732: step 91440, loss = 0.77 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:40.139861: step 91450, loss = 0.70 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:40.897437: step 91460, loss = 0.69 (1689.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:41.673033: step 91470, loss = 0.91 (1650.3 examples/sec; 0.078 sec/batch)
2017-05-05 19:22:42.436153: step 91480, loss = 0.68 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:43.200639: step 91490, loss = 0.98 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:43.966279: step 91500, loss = 0.75 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:22:44.731082: step 91510, loss = 0.71 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:45.493683: step 91520, loss = 0.80 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:46.260708: step 91530, loss = 0.71 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:22:47.018496: step 91540, loss = 0.66 (1689.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:47.774566: step 91550, loss = 0.70 (1693.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:48.542087: step 91560, loss = 0.66 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:22:49.310940: step 91570, loss = 0.84 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:22:50.069433: step 91580, loss = 0.76 (1687.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:50.834352: step 91590, loss = 0.71 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:51.589643: step 91600, loss = 0.74 (1694.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:52.355433: step 91610, loss = 0.63 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:22:53.121523: step 91620, loss = 0.77 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:22:53.888783: step 91630, loss = 0.80 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:22:54.654600: step 91640, loss = 0.62 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:22:55.417270: step 91650, loss = 0.72 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:56.174859: step 91660, loss = 0.69 (1689.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:22:56.943902: step 91670, loss = 0.62 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:22:57.724024: step 91680, loss = 0.72 (1640.8 examples/sec; 0.078 sec/batch)
2017-05-05 19:22:58.490217: step 91690, loss = 0.77 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:22:59.258598: step 91700, loss = 0.69 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:00.019192: step 91710, loss = 0.71 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:00.776781: step 91720, loss = 0.65 (1689.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:01.547018: step 91730, loss = 0.71 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:02.310992: step 91740, loss = 0.65 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:03.081355: step 91750, loss = 0.83 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:03.839187: step 91760, loss = 0.78 (1689.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:04.607856: step 91770, loss = 0.76 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:05.369309: step 91780, loss = 0.80 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:06.131103: step 91790, loss = 0.81 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:06.902203: step 91800, loss = 0.85 (1660.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:07.660610: step 91810, loss = 0.71 (1687.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:08.426971: step 91820, loss = 0.93 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:09.191700: step 91830, loss = 0.69 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:09.958537: step 91840, loss = 0.68 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:10.715721: step 91850, loss = 0.56 (1690.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:11.474997: step 91860, loss = 0.68 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:12.241032: step 91870, loss = 0.76 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:13.003605: step 91880, loss = 0.74 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:13.775607: step 91890, loss = 0.65 (1658.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:14.543467: step 91900, loss = 0.66 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:15.309182: step 91910, loss = 0.63 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:16.064269: step 91920, loss = 0.74 (1695.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:16.832342: step 91930, loss = 0.69 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:17.595226: step 91940, loss = 0.80 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:18.361352: step 91950, loss = 0.63 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:19.123625: step 91960, loss = 0.70 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:19.882846: step 91970, loss = 0.77 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:20.645985: step 91980, loss = 0.98 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:21.412195: step 91990, loss = 0.72 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:22.184390: step 92000, loss = 0.68 (1657.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:22.943963: step 92010, loss = 0.82 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:23.698310: step 92020, loss = 0.72 (1696.8 examples/sec; 0.075 sec/batch)
2017-05-05 19:23:24.472579: step 92030, loss = 0.71 (1653.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:25.241588: step 92040, loss = 0.70 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:26.003454: step 92050, loss = 0.75 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:26.767816: step 92060, loss = 0.73 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:27.631099: step 92070, loss = 0.82 (1482.7 examples/sec; 0.086 sec/batch)
2017-05-05 19:23:28.297525: step 92080, loss = 0.79 (1920.7 examples/sec; 0.067 sec/batch)
2017-05-05 19:23:29.066754: step 92090, loss = 0.87 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:29.833620: step 92100, loss = 0.68 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:30.595616: step 92110, loss = 0.73 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:31.355539: step 92120, loss = 0.62 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:32.110000: step 92130, loss = 0.82 (1696.6 examples/sec; 0.075 sec/batch)
2017-05-05 19:23:32.879709: step 92140, loss = 0.75 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:33.645501: step 92150, loss = 0.78 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:34.415473: step 92160, loss = 0.71 (1662.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:35.176814: step 92170, loss = 0.75 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:35.935843: step 92180, loss = 0.84 (1686.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:36.700660: step 92190, loss = 0.69 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:37.464601: step 92200, loss = 0.75 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:38.231529: step 92210, loss = 0.77 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:38.993154: step 92220, loss = 0.67 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:39.756990: step 92230, loss = 0.74 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:40.521068: step 92240, loss = 0.79 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:41.286389: step 92250, loss = 0.70 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:42.045944: step 92260, loss = 0.63 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:42.830645: step 92270, loss = 0.72 (1631.2 examples/sec; 0.078 sec/batch)
2017-05-05 19:23:43.575661: step 92280, loss = 0.81 (1718.1 examples/sec; 0.075 sec/batch)
2017-05-05 19:23:44.338278: step 92290, loss = 0.60 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:45.112161: step 92300, loss = 0.68 (1654.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:45.869584: step 92310, loss = 0.79 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:46.641222: step 92320, loss = 0.69 (1658.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:47.407287: step 92330, loss = 0.76 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:48.164912: step 92340, loss = 0.71 (1689.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:48.931074: step 92350, loss = 0.82 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:49.698782: step 92360, loss = 0.91 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:50.469065: step 92370, loss = 0.78 (1661.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:51.239520: step 92380, loss = 0.77 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:51.996853: step 92390, loss = 0.87 (1690.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:52.754312: step 92400, loss = 0.60 (1689.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:53.520233: step 92410, loss = 0.76 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:54.282789: step 92420, loss = 0.72 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:55.048901: step 92430, loss = 0.61 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:55.804551: step 92440, loss = 0.72 (1693.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:56.564087: step 92450, loss = 0.66 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:23:57.330782: step 92460, loss = 0.92 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:58.101649: step 92470, loss = 0.84 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:58.872178: step 92480, loss = 0.65 (1661.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:23:59.632278: step 92490, loss = 0.77 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:00.393535: step 92500, loss = 0.73 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:01.151177: step 92510, loss = 0.65 (1689.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:01.919258: step 92520, loss = 0.71 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:02.681605: step 92530, loss = 0.81 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:03.440267: step 92540, loss = 0.90 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:04.208823: step 92550, loss = 0.80 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:04.976912: step 92560, loss = 0.73 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:05.743911: step 92570, loss = 0.81 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:06.505207: step 92580, loss = 0.79 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:07.267504: step 92590, loss = 0.79 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:08.027950: step 92600, loss = 0.71 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:08.800375: step 92610, loss = 0.80 (1657.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:09.571046: step 92620, loss = 0.88 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:10.334799: step 92630, loss = 0.79 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:11.101017: step 92640, loss = 0.67 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:11.859071: step 92650, loss = 0.81 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:12.626610: step 92660, loss = 0.76 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:13.394913: step 92670, loss = 0.57 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:14.163509: step 92680, loss = 0.84 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:14.928229: step 92690, loss = 0.94 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:15.682426: step 92700, loss = 0.81 (1697.2 examples/sec; 0.075 sec/batch)
2017-05-05 19:24:16.448054: step 92710, loss = 0.72 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:17.215584: step 92720, loss = 0.98 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:17.978167: step 92730, loss = 0.72 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:18.739595: step 92740, loss = 0.80 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:19.505802: step 92750, loss = 0.84 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:20.260451: step 92760, loss = 0.69 (1696.2 examples/sec; 0.075 sec/batch)
2017-05-05 19:24:21.026281: step 92770, loss = 0.76 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:21.790284: step 92780, loss = 0.76 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:22.556053: step 92790, loss = 0.68 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:23.321112: step 92800, loss = 0.75 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:24.079894: step 92810, loss = 0.77 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:24.843665: step 92820, loss = 0.69 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:25.602657: step 92830, loss = 0.74 (1686.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:26.374470: step 92840, loss = 0.75 (1658.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:27.136750: step 92850, loss = 0.81 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:27.892683: step 92860, loss = 0.72 (1693.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:28.657843: step 92870, loss = 0.76 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:29.426619: step 92880, loss = 0.69 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:30.189747: step 92890, loss = 0.55 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:30.951336: step 92900, loss = 0.74 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:31.706381: step 92910, loss = 0.82 (1695.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:32.469984: step 92920, loss = 0.70 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:33.240086: step 92930, loss = 0.83 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:34.005171: step 92940, loss = 0.70 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:34.773251: step 92950, loss = 0.71 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:35.529112: step 92960, loss = 0.83 (1693.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:36.292579: step 92970, loss = 0.81 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:37.064102: step 92980, loss = 0.68 (1659.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:37.831716: step 92990, loss = 0.62 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:38.592656: step 93000, loss = 0.83 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:39.353382: step 93010, loss = 0.73 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:40.113156: step 93020, loss = 0.58 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:40.880270: step 93030, loss = 0.66 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:41.644923: step 93040, loss = 0.80 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:42.418552: step 93050, loss = 0.75 (1654.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:43.289628: step 93060, loss = 0.72 (1469.4 examples/sec; 0.087 sec/batch)
2017-05-05 19:24:43.950920: step 93070, loss = 0.80 (1935.6 examples/sec; 0.066 sec/batch)
2017-05-05 19:24:44.714434: step 93080, loss = 0.73 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:45.481221: step 93090, loss = 0.66 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:46.248735: step 93100, loss = 0.83 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:47.010112: step 93110, loss = 0.87 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:47.778382: step 93120, loss = 0.69 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:48.549670: step 93130, loss = 0.73 (1659.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:49.316567: step 93140, loss = 0.71 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:50.084782: step 93150, loss = 0.68 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:50.845797: step 93160, loss = 0.61 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:51.606996: step 93170, loss = 0.79 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:52.373170: step 93180, loss = 0.74 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:53.142202: step 93190, loss = 0.71 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:53.907844: step 93200, loss = 0.81 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:54.677583: step 93210, loss = 0.71 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:55.443832: step 93220, loss = 0.75 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:56.201990: step 93230, loss = 0.75 (1688.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:56.966292: step 93240, loss = 0.60 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:24:57.736453: step 93250, loss = 0.78 (1662.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:58.502262: step 93260, loss = 0.64 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:24:59.269496: step 93270, loss = 0.57 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:00.029645: step 93280, loss = 0.81 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:00.796658: step 93290, loss = 0.78 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:01.557650: step 93300, loss = 0.66 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:02.346741: step 93310, loss = 0.73 (1622.1 examples/sec; 0.079 sec/batch)
2017-05-05 19:25:03.107963: step 93320, loss = 0.79 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:03.868871: step 93330, loss = 0.79 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:04.639728: step 93340, loss = 0.67 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:05.403142: step 93350, loss = 0.87 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:06.170701: step 93360, loss = 0.68 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:06.933255: step 93370, loss = 0.68 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:07.687641: step 93380, loss = 0.74 (1696.8 examples/sec; 0.075 sec/batch)
2017-05-05 19:25:08.453236: step 93390, loss = 0.70 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:09.218735: step 93400, loss = 0.56 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:09.987667: step 93410, loss = 0.62 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:10.746685: step 93420, loss = 0.77 (1686.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:11.511379: step 93430, loss = 0.69 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:12.271569: step 93440, loss = 0.97 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:13.040098: step 93450, loss = 0.88 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:13.809761: step 93460, loss = 0.79 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:14.580596: step 93470, loss = 0.83 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:15.341531: step 93480, loss = 0.87 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:16.099017: step 93490, loss = 0.69 (1689.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:16.870552: step 93500, loss = 0.68 (1659.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:17.637561: step 93510, loss = 0.76 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:18.403228: step 93520, loss = 0.74 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:19.170992: step 93530, loss = 0.85 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:19.926964: step 93540, loss = 0.63 (1693.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:20.686184: step 93550, loss = 0.87 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:21.450402: step 93560, loss = 0.73 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:22.215970: step 93570, loss = 0.75 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:22.979661: step 93580, loss = 0.74 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:23.734787: step 93590, loss = 0.67 (1695.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:24.498545: step 93600, loss = 0.76 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:25.264187: step 93610, loss = 0.63 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:26.025630: step 93620, loss = 0.77 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:26.791706: step 93630, loss = 0.66 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:27.549955: step 93640, loss = 0.81 (1688.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:28.318643: step 93650, loss = 0.81 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:29.088030: step 93660, loss = 0.65 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:29.851768: step 93670, loss = 0.71 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:30.609572: step 93680, loss = 0.66 (1689.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:31.376242: step 93690, loss = 0.72 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:32.138743: step 93700, loss = 0.89 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:32.904257: step 93710, loss = 0.67 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:33.672701: step 93720, loss = 0.69 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:34.440591: step 93730, loss = 0.73 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:35.208186: step 93740, loss = 0.73 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:35.970218: step 93750, loss = 0.74 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:36.736077: step 93760, loss = 0.81 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:37.500692: step 93770, loss = 0.76 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:38.266454: step 93780, loss = 0.68 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:39.026842: step 93790, loss = 0.70 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:39.789335: step 93800, loss = 0.65 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:40.554328: step 93810, loss = 0.76 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:41.315940: step 93820, loss = 0.60 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:42.083045: step 93830, loss = 0.75 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:42.847438: step 93840, loss = 0.70 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:43.602848: step 93850, loss = 0.72 (1694.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:44.373552: step 93860, loss = 0.83 (1660.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:45.139053: step 93870, loss = 0.63 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:45.895465: step 93880, loss = 0.76 (1692.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:46.659728: step 93890, loss = 0.79 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:47.419235: step 93900, loss = 0.82 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:48.178635: step 93910, loss = 0.81 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:48.945328: step 93920, loss = 0.72 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:49.707878: step 93930, loss = 0.81 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:50.477926: step 93940, loss = 0.76 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:51.247058: step 93950, loss = 0.81 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:52.002406: step 93960, loss = 0.66 (1694.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:52.764501: step 93970, loss = 0.68 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:53.538296: step 93980, loss = 0.60 (1654.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:54.304616: step 93990, loss = 0.69 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:55.066583: step 94000, loss = 0.84 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:55.821626: step 94010, loss = 0.88 (1695.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:25:56.589891: step 94020, loss = 0.79 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:57.356940: step 94030, loss = 0.72 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:58.129611: step 94040, loss = 0.74 (1656.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:25:58.989704: step 94050, loss = 0.81 (1488.2 examples/sec; 0.086 sec/batch)
2017-05-05 19:25:59.653064: step 94060, loss = 0.64 (1929.6 examples/sec; 0.066 sec/batch)
2017-05-05 19:26:00.423035: step 94070, loss = 0.68 (1662.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:01.183993: step 94080, loss = 0.71 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:01.946040: step 94090, loss = 0.67 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:02.714474: step 94100, loss = 0.61 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:03.471091: step 94110, loss = 0.81 (1691.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:04.233408: step 94120, loss = 0.83 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:05.006350: step 94130, loss = 0.85 (1655.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:05.809715: step 94140, loss = 0.77 (1593.3 examples/sec; 0.080 sec/batch)
2017-05-05 19:26:06.575880: step 94150, loss = 0.73 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:07.336700: step 94160, loss = 0.69 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:08.097090: step 94170, loss = 0.82 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:08.866015: step 94180, loss = 0.76 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:09.626538: step 94190, loss = 0.75 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:10.391186: step 94200, loss = 0.69 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:11.155047: step 94210, loss = 0.84 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:11.908512: step 94220, loss = 0.74 (1698.8 examples/sec; 0.075 sec/batch)
2017-05-05 19:26:12.673516: step 94230, loss = 0.64 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:13.439714: step 94240, loss = 0.69 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:14.205471: step 94250, loss = 0.80 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:14.982776: step 94260, loss = 0.80 (1646.7 examples/sec; 0.078 sec/batch)
2017-05-05 19:26:15.739635: step 94270, loss = 0.89 (1691.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:16.505095: step 94280, loss = 0.89 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:17.270999: step 94290, loss = 0.73 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:18.037396: step 94300, loss = 0.91 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:18.804452: step 94310, loss = 0.68 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:19.560273: step 94320, loss = 0.81 (1693.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:20.322354: step 94330, loss = 0.91 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:21.081787: step 94340, loss = 0.63 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:21.847800: step 94350, loss = 0.90 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:22.612219: step 94360, loss = 0.76 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:23.375738: step 94370, loss = 0.92 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:24.136206: step 94380, loss = 0.79 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:24.905779: step 94390, loss = 0.74 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:25.669946: step 94400, loss = 0.71 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:26.434486: step 94410, loss = 0.77 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:27.202420: step 94420, loss = 0.71 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:27.957547: step 94430, loss = 0.58 (1695.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:28.722493: step 94440, loss = 0.83 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:29.487888: step 94450, loss = 0.77 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:30.258513: step 94460, loss = 0.65 (1661.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:31.016268: step 94470, loss = 0.76 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:31.773118: step 94480, loss = 0.87 (1691.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:32.536007: step 94490, loss = 0.67 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:33.300729: step 94500, loss = 0.64 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:34.067845: step 94510, loss = 0.85 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:34.839220: step 94520, loss = 0.76 (1659.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:35.598847: step 94530, loss = 0.85 (1685.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:36.362699: step 94540, loss = 0.69 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:37.135803: step 94550, loss = 0.90 (1655.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:37.898091: step 94560, loss = 0.79 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:38.665092: step 94570, loss = 0.62 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:39.432335: step 94580, loss = 0.81 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:40.192909: step 94590, loss = 0.78 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:40.953738: step 94600, loss = 0.77 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:41.716816: step 94610, loss = 0.67 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:42.479943: step 94620, loss = 0.71 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:43.245707: step 94630, loss = 0.71 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:44.008271: step 94640, loss = 0.59 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:44.775378: step 94650, loss = 0.75 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:45.541534: step 94660, loss = 0.59 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:46.310435: step 94670, loss = 0.71 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:47.076470: step 94680, loss = 0.76 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:47.836920: step 94690, loss = 0.66 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:48.610255: step 94700, loss = 0.82 (1655.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:49.381161: step 94710, loss = 0.72 (1660.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:50.151143: step 94720, loss = 0.78 (1662.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:50.909514: step 94730, loss = 0.67 (1687.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:51.673533: step 94740, loss = 0.80 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:52.440632: step 94750, loss = 0.79 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:53.207675: step 94760, loss = 0.81 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:53.969035: step 94770, loss = 0.67 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:54.742947: step 94780, loss = 0.82 (1653.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:55.501541: step 94790, loss = 0.84 (1687.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:56.266996: step 94800, loss = 0.81 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:57.032648: step 94810, loss = 0.77 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:57.805426: step 94820, loss = 0.68 (1656.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:26:58.564463: step 94830, loss = 0.93 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:26:59.331044: step 94840, loss = 0.71 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:27:00.090296: step 94850, loss = 0.72 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:00.855383: step 94860, loss = 0.72 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:27:01.614738: step 94870, loss = 0.69 (1685.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:02.382873: step 94880, loss = 0.76 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:27:03.141973: step 94890, loss = 0.74 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:03.899710: step 94900, loss = 0.65 (1689.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:04.662323: step 94910, loss = 0.81 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:05.429045: step 94920, loss = 0.76 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:27:06.188191: step 94930, loss = 0.75 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:06.948423: step 94940, loss = 0.81 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:07.705193: step 94950, loss = 0.69 (1691.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:08.469333: step 94960, loss = 0.88 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:09.240163: step 94970, loss = 0.83 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:27:10.002069: step 94980, loss = 0.89 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:10.767919: step 94990, loss = 0.71 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:27:11.526507: step 95000, loss = 0.68 (1687.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:12.286856: step 95010, loss = 0.72 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:13.050999: step 95020, loss = 0.68 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:13.820606: step 95030, loss = 0.88 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:27:14.695074: step 95040, loss = 0.67 (1463.7 examples/sec; 0.087 sec/batch)
2017-05-05 19:27:15.356516: step 95050, loss = 0.71 (1935.2 examples/sec; 0.066 sec/batch)
2017-05-05 19:27:16.117328: step 95060, loss = 0.74 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:16.879756: step 95070, loss = 0.63 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:17.647708: step 95080, loss = 0.67 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:27:18.423931: step 95090, loss = 0.69 (1649.0 examples/sec; 0.078 sec/batch)
2017-05-05 19:27:19.190764: step 95100, loss = 0.96 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:27:19.944083: step 95110, loss = 0.84 (1699.2 examples/sec; 0.075 sec/batch)
2017-05-05 19:27:20.713677: step 95120, loss = 0.58 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:27:21.475111: step 95130, loss = 0.77 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:22.230897: step 95140, loss = 0.65 (1693.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:22.994712: step 95150, loss = 0.61 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:23.748823: step 95160, loss = 0.75 (1697.4 examples/sec; 0.075 sec/batch)
2017-05-05 19:27:24.516322: step 95170, loss = 0.71 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:27:25.283913: step 95180, loss = 0.74 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:27:26.040353: step 95190, loss = 0.67 (1692.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:26.808805: step 95200, loss = 0.83 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:27:27.567336: step 95210, loss = 0.72 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:28.325069: step 95220, loss = 0.76 (1689.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:29.093459: step 95230, loss = 0.68 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:27:29.858084: step 95240, loss = 0.78 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:30.623961: step 95250, loss = 0.81 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:27:31.384939: step 95260, loss = 0.75 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:32.143993: step 95270, loss = 0.91 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:32.905949: step 95280, loss = 0.72 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:33.669699: step 95290, loss = 0.72 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:34.435733: step 95300, loss = 0.73 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:27:35.202657: step 95310, loss = 0.87 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:27:35.958387: step 95320, loss = 0.65 (1693.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:36.722551: step 95330, loss = 0.80 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:37.491724: step 95340, loss = 0.73 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:27:38.258505: step 95350, loss = 0.60 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:27:39.024185: step 95360, loss = 0.73 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:27:39.784321: step 95370, loss = 0.63 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:40.552243: step 95380, loss = 0.67 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:27:41.313395: step 95390, loss = 0.79 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:42.079540: step 95400, loss = 0.85 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:27:42.841745: step 95410, loss = 0.69 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:43.601843: step 95420, loss = 0.80 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:44.368807: step 95430, loss = 0.63 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:27:45.141462: step 95440, loss = 0.85 (1656.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:27:45.904767: step 95450, loss = 0.72 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:46.670107: step 95460, loss = 0.93 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:27:47.428147: step 95470, loss = 0.82 (1688.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:48.185802: step 95480, loss = 0.70 (1689.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:48.957496: step 95490, loss = 0.80 (1658.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:27:49.721993: step 95500, loss = 0.71 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:50.481803: step 95510, loss = 0.70 (1684.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:51.247952: step 95520, loss = 0.71 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:27:52.003811: step 95530, loss = 0.85 (1693.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:52.768769: step 95540, loss = 0.70 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:53.537872: step 95550, loss = 0.83 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:27:54.304246: step 95560, loss = 0.64 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:27:55.069418: step 95570, loss = 0.69 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:27:55.829465: step 95580, loss = 0.63 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:56.600987: step 95590, loss = 0.74 (1659.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:27:57.371883: step 95600, loss = 0.86 (1660.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:27:58.130607: step 95610, loss = 0.85 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:58.895399: step 95620, loss = 0.65 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:27:59.655167: step 95630, loss = 0.67 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:00.418953: step 95640, loss = 0.74 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:01.180036: step 95650, loss = 0.68 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:01.941543: step 95660, loss = 0.72 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:02.724819: step 95670, loss = 0.80 (1634.2 examples/sec; 0.078 sec/batch)
2017-05-05 19:28:03.488545: step 95680, loss = 0.76 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:04.248500: step 95690, loss = 0.65 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:05.018331: step 95700, loss = 0.70 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:28:05.776685: step 95710, loss = 0.75 (1687.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:06.543009: step 95720, loss = 0.77 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:28:07.308835: step 95730, loss = 0.63 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:28:08.066931: step 95740, loss = 0.75 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:08.832617: step 95750, loss = 0.84 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:28:09.598454: step 95760, loss = 0.79 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:28:10.359340: step 95770, loss = 0.74 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:11.121905: step 95780, loss = 0.72 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:11.877049: step 95790, loss = 0.71 (1695.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:12.636598: step 95800, loss = 0.77 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:13.406127: step 95810, loss = 0.74 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:28:14.172862: step 95820, loss = 0.80 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:28:14.938337: step 95830, loss = 0.62 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:28:15.691578: step 95840, loss = 0.71 (1699.3 examples/sec; 0.075 sec/batch)
2017-05-05 19:28:16.456535: step 95850, loss = 0.66 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:17.226023: step 95860, loss = 0.87 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:28:17.988904: step 95870, loss = 0.72 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:18.758048: step 95880, loss = 0.87 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:28:19.512302: step 95890, loss = 0.69 (1697.0 examples/sec; 0.075 sec/batch)
2017-05-05 19:28:20.274182: step 95900, loss = 0.79 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:21.033617: step 95910, loss = 0.65 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:21.798886: step 95920, loss = 0.76 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:28:22.566427: step 95930, loss = 0.71 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:28:23.327816: step 95940, loss = 0.82 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:24.089837: step 95950, loss = 0.75 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:24.860999: step 95960, loss = 0.67 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:28:25.627611: step 95970, loss = 0.67 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:28:26.394994: step 95980, loss = 0.76 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:28:27.159559: step 95990, loss = 0.70 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:27.916528: step 96000, loss = 0.67 (1691.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:28.680904: step 96010, loss = 0.87 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:29.444597: step 96020, loss = 0.78 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:30.307935: step 96030, loss = 0.70 (1482.6 examples/sec; 0.086 sec/batch)
2017-05-05 19:28:30.976555: step 96040, loss = 0.74 (1914.4 examples/sec; 0.067 sec/batch)
2017-05-05 19:28:31.727970: step 96050, loss = 0.66 (1703.5 examples/sec; 0.075 sec/batch)
2017-05-05 19:28:32.494983: step 96060, loss = 0.84 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:28:33.253186: step 96070, loss = 0.82 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:34.018094: step 96080, loss = 0.75 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:34.780284: step 96090, loss = 0.63 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:35.541874: step 96100, loss = 0.66 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:36.306565: step 96110, loss = 0.68 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:37.070882: step 96120, loss = 0.77 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:37.834827: step 96130, loss = 0.77 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:38.601970: step 96140, loss = 0.57 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:28:39.359798: step 96150, loss = 0.83 (1689.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:40.123511: step 96160, loss = 0.76 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:40.882407: step 96170, loss = 0.61 (1686.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:41.645028: step 96180, loss = 0.63 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:42.411673: step 96190, loss = 0.70 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:28:43.180824: step 96200, loss = 0.80 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:28:43.939018: step 96210, loss = 0.74 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:44.701196: step 96220, loss = 0.70 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:45.471635: step 96230, loss = 0.78 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:28:46.237421: step 96240, loss = 0.77 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:28:47.001172: step 96250, loss = 0.86 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:47.753652: step 96260, loss = 0.74 (1701.0 examples/sec; 0.075 sec/batch)
2017-05-05 19:28:48.521120: step 96270, loss = 0.67 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:28:49.282771: step 96280, loss = 0.84 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:50.048306: step 96290, loss = 0.64 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:28:50.804298: step 96300, loss = 0.75 (1693.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:51.569630: step 96310, loss = 0.72 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:28:52.335390: step 96320, loss = 0.79 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:28:53.099607: step 96330, loss = 0.70 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:53.862622: step 96340, loss = 0.83 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:54.632180: step 96350, loss = 0.69 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:28:55.397502: step 96360, loss = 0.67 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:28:56.150898: step 96370, loss = 0.64 (1699.0 examples/sec; 0.075 sec/batch)
2017-05-05 19:28:56.912046: step 96380, loss = 0.75 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:57.675065: step 96390, loss = 0.98 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:58.439681: step 96400, loss = 0.75 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:28:59.212186: step 96410, loss = 0.77 (1656.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:28:59.966689: step 96420, loss = 0.64 (1696.5 examples/sec; 0.075 sec/batch)
2017-05-05 19:29:00.735226: step 96430, loss = 0.72 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:01.501097: step 96440, loss = 0.77 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:02.264146: step 96450, loss = 0.67 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:29:03.030225: step 96460, loss = 0.88 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:03.787997: step 96470, loss = 0.82 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:29:04.548519: step 96480, loss = 0.85 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:29:05.318214: step 96490, loss = 0.64 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:06.083412: step 96500, loss = 0.83 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:06.849913: step 96510, loss = 0.80 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:07.604300: step 96520, loss = 0.81 (1696.7 examples/sec; 0.075 sec/batch)
2017-05-05 19:29:08.369336: step 96530, loss = 0.86 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:09.137594: step 96540, loss = 0.77 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:09.904123: step 96550, loss = 0.77 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:10.665305: step 96560, loss = 0.76 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:29:11.433142: step 96570, loss = 0.68 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:12.195124: step 96580, loss = 0.75 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:29:12.953776: step 96590, loss = 0.60 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:29:13.720068: step 96600, loss = 0.74 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:14.483798: step 96610, loss = 0.69 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:29:15.252870: step 96620, loss = 0.82 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:16.008201: step 96630, loss = 0.74 (1694.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:29:16.771711: step 96640, loss = 0.78 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:29:17.537514: step 96650, loss = 0.80 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:18.304581: step 96660, loss = 0.78 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:19.075314: step 96670, loss = 0.72 (1660.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:19.831337: step 96680, loss = 0.67 (1693.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:29:20.593033: step 96690, loss = 0.80 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:29:21.354463: step 96700, loss = 0.76 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:29:22.124150: step 96710, loss = 0.75 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:22.885270: step 96720, loss = 0.85 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:29:23.651742: step 96730, loss = 0.85 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:24.418897: step 96740, loss = 0.76 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:25.179269: step 96750, loss = 0.68 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:29:25.939140: step 96760, loss = 0.64 (1684.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:29:26.709922: step 96770, loss = 0.55 (1660.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:27.471015: step 96780, loss = 0.60 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:29:28.227034: step 96790, loss = 0.73 (1693.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:29:28.991181: step 96800, loss = 0.68 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:29:29.759343: step 96810, loss = 0.61 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:30.519237: step 96820, loss = 0.88 (1684.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:29:31.279649: step 96830, loss = 0.58 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:29:32.039866: step 96840, loss = 0.88 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:29:32.814894: step 96850, loss = 0.84 (1651.6 examples/sec; 0.078 sec/batch)
2017-05-05 19:29:33.571282: step 96860, loss = 0.74 (1692.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:29:34.338188: step 96870, loss = 0.76 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:35.104808: step 96880, loss = 0.85 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:35.864640: step 96890, loss = 0.67 (1684.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:29:36.630191: step 96900, loss = 0.80 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:37.391278: step 96910, loss = 0.84 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:29:38.161625: step 96920, loss = 0.69 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:38.929549: step 96930, loss = 0.75 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:39.683853: step 96940, loss = 0.77 (1696.9 examples/sec; 0.075 sec/batch)
2017-05-05 19:29:40.454885: step 96950, loss = 0.79 (1660.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:41.222779: step 96960, loss = 0.75 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:41.989692: step 96970, loss = 0.80 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:42.757531: step 96980, loss = 0.74 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:43.528724: step 96990, loss = 0.86 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:44.294165: step 97000, loss = 0.64 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:45.055991: step 97010, loss = 0.68 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:29:45.933119: step 97020, loss = 0.50 (1459.3 examples/sec; 0.088 sec/batch)
2017-05-05 19:29:46.580748: step 97030, loss = 0.69 (1976.4 examples/sec; 0.065 sec/batch)
2017-05-05 19:29:47.340921: step 97040, loss = 0.67 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:29:48.096286: step 97050, loss = 0.78 (1694.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:29:48.857150: step 97060, loss = 0.75 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:29:49.622138: step 97070, loss = 0.83 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:29:50.382094: step 97080, loss = 0.77 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:29:51.147133: step 97090, loss = 0.91 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:51.901189: step 97100, loss = 0.75 (1697.5 examples/sec; 0.075 sec/batch)
2017-05-05 19:29:52.672721: step 97110, loss = 0.79 (1659.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:53.440786: step 97120, loss = 0.71 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:54.209830: step 97130, loss = 0.73 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:54.982937: step 97140, loss = 0.73 (1655.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:55.739523: step 97150, loss = 0.81 (1691.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:29:56.510307: step 97160, loss = 0.71 (1660.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:57.270401: step 97170, loss = 0.81 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:29:58.039517: step 97180, loss = 0.66 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:58.807329: step 97190, loss = 0.80 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:29:59.562259: step 97200, loss = 0.99 (1695.5 examples/sec; 0.075 sec/batch)
2017-05-05 19:30:00.326023: step 97210, loss = 0.62 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:01.092530: step 97220, loss = 0.75 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:01.870006: step 97230, loss = 0.80 (1646.4 examples/sec; 0.078 sec/batch)
2017-05-05 19:30:02.636918: step 97240, loss = 0.66 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:03.399813: step 97250, loss = 0.77 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:04.166766: step 97260, loss = 0.74 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:04.934009: step 97270, loss = 0.68 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:05.693086: step 97280, loss = 0.91 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:06.462719: step 97290, loss = 0.72 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:07.232858: step 97300, loss = 0.73 (1662.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:07.985615: step 97310, loss = 0.68 (1700.4 examples/sec; 0.075 sec/batch)
2017-05-05 19:30:08.759397: step 97320, loss = 0.71 (1654.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:09.530477: step 97330, loss = 0.81 (1660.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:10.287636: step 97340, loss = 0.73 (1690.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:11.053689: step 97350, loss = 0.86 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:11.812188: step 97360, loss = 0.65 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:12.576049: step 97370, loss = 0.89 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:13.344192: step 97380, loss = 0.84 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:14.112434: step 97390, loss = 0.73 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:14.885707: step 97400, loss = 0.71 (1655.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:15.646246: step 97410, loss = 0.76 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:16.416297: step 97420, loss = 0.79 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:17.188457: step 97430, loss = 0.69 (1657.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:17.950876: step 97440, loss = 0.87 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:18.721787: step 97450, loss = 0.86 (1660.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:19.485893: step 97460, loss = 0.71 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:20.245423: step 97470, loss = 0.83 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:21.005738: step 97480, loss = 0.81 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:21.771071: step 97490, loss = 0.94 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:22.538927: step 97500, loss = 0.88 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:23.304459: step 97510, loss = 0.78 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:24.056490: step 97520, loss = 0.82 (1702.1 examples/sec; 0.075 sec/batch)
2017-05-05 19:30:24.826292: step 97530, loss = 0.75 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:25.586561: step 97540, loss = 0.73 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:26.353273: step 97550, loss = 0.63 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:27.113311: step 97560, loss = 0.64 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:27.869133: step 97570, loss = 0.92 (1693.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:28.636154: step 97580, loss = 0.70 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:29.412251: step 97590, loss = 0.93 (1649.3 examples/sec; 0.078 sec/batch)
2017-05-05 19:30:30.171578: step 97600, loss = 0.74 (1685.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:30.934170: step 97610, loss = 0.70 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:31.695482: step 97620, loss = 0.59 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:32.462123: step 97630, loss = 0.92 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:33.233053: step 97640, loss = 0.71 (1660.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:34.002072: step 97650, loss = 0.83 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:34.777183: step 97660, loss = 0.89 (1651.4 examples/sec; 0.078 sec/batch)
2017-05-05 19:30:35.539211: step 97670, loss = 0.71 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:36.302592: step 97680, loss = 0.87 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:37.071116: step 97690, loss = 0.77 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:37.837518: step 97700, loss = 0.60 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:38.611422: step 97710, loss = 0.66 (1654.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:39.372861: step 97720, loss = 0.81 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:40.133703: step 97730, loss = 0.78 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:40.886514: step 97740, loss = 0.81 (1700.3 examples/sec; 0.075 sec/batch)
2017-05-05 19:30:41.646582: step 97750, loss = 0.70 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:42.410240: step 97760, loss = 0.78 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:43.170383: step 97770, loss = 0.68 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:43.931097: step 97780, loss = 0.96 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:44.690415: step 97790, loss = 0.79 (1685.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:45.453253: step 97800, loss = 0.61 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:46.211872: step 97810, loss = 0.70 (1687.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:46.984271: step 97820, loss = 0.81 (1657.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:47.734311: step 97830, loss = 0.70 (1706.6 examples/sec; 0.075 sec/batch)
2017-05-05 19:30:48.503511: step 97840, loss = 0.76 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:49.270432: step 97850, loss = 0.86 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:50.037153: step 97860, loss = 0.92 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:50.797210: step 97870, loss = 0.75 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:51.558387: step 97880, loss = 0.68 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:52.321287: step 97890, loss = 0.88 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:53.083671: step 97900, loss = 0.71 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:53.852928: step 97910, loss = 0.62 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:54.618782: step 97920, loss = 0.80 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:55.381013: step 97930, loss = 0.67 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:56.139793: step 97940, loss = 0.79 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:56.900890: step 97950, loss = 0.87 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:57.662987: step 97960, loss = 0.86 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:58.434591: step 97970, loss = 0.70 (1658.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:30:59.195381: step 97980, loss = 0.98 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:30:59.947503: step 97990, loss = 0.67 (1701.8 examples/sec; 0.075 sec/batch)
2017-05-05 19:31:00.707721: step 98000, loss = 0.62 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:01.574071: step 98010, loss = 0.69 (1477.5 examples/sec; 0.087 sec/batch)
2017-05-05 19:31:02.236869: step 98020, loss = 0.69 (1931.2 examples/sec; 0.066 sec/batch)
2017-05-05 19:31:02.999752: step 98030, loss = 0.79 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:03.755915: step 98040, loss = 0.63 (1692.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:04.517699: step 98050, loss = 0.72 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:05.284465: step 98060, loss = 0.60 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:06.044324: step 98070, loss = 0.87 (1684.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:06.811425: step 98080, loss = 0.68 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:07.569693: step 98090, loss = 0.82 (1688.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:08.341269: step 98100, loss = 0.80 (1658.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:09.099977: step 98110, loss = 0.68 (1687.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:09.860666: step 98120, loss = 0.54 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:10.620243: step 98130, loss = 0.78 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:11.382625: step 98140, loss = 0.66 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:12.144576: step 98150, loss = 0.84 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:12.907867: step 98160, loss = 0.76 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:13.677468: step 98170, loss = 0.81 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:14.446755: step 98180, loss = 0.72 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:15.212837: step 98190, loss = 0.65 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:15.970303: step 98200, loss = 0.84 (1689.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:16.740766: step 98210, loss = 0.78 (1661.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:17.509035: step 98220, loss = 0.70 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:18.274053: step 98230, loss = 0.83 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:19.041703: step 98240, loss = 0.77 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:19.806285: step 98250, loss = 0.72 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:20.570568: step 98260, loss = 0.64 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:21.337465: step 98270, loss = 1.01 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:22.094657: step 98280, loss = 0.83 (1690.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:22.857739: step 98290, loss = 0.75 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:23.619997: step 98300, loss = 0.64 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:24.389387: step 98310, loss = 0.77 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:25.151127: step 98320, loss = 0.85 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:25.911886: step 98330, loss = 1.07 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:26.683596: step 98340, loss = 0.72 (1658.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:27.447283: step 98350, loss = 0.74 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:28.203534: step 98360, loss = 0.63 (1692.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:28.967317: step 98370, loss = 0.79 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:29.735944: step 98380, loss = 0.58 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:30.499249: step 98390, loss = 0.71 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:31.257843: step 98400, loss = 0.63 (1687.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:32.017508: step 98410, loss = 0.83 (1684.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:32.783196: step 98420, loss = 0.80 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:33.547144: step 98430, loss = 0.60 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:34.324724: step 98440, loss = 0.63 (1646.1 examples/sec; 0.078 sec/batch)
2017-05-05 19:31:35.083288: step 98450, loss = 0.74 (1687.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:35.842491: step 98460, loss = 0.77 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:36.602660: step 98470, loss = 0.74 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:37.371524: step 98480, loss = 0.65 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:38.137516: step 98490, loss = 0.70 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:38.902648: step 98500, loss = 0.85 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:39.662874: step 98510, loss = 0.57 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:40.431848: step 98520, loss = 0.74 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:41.190278: step 98530, loss = 0.75 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:41.957114: step 98540, loss = 0.68 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:42.724422: step 98550, loss = 0.80 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:43.484837: step 98560, loss = 0.64 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:44.249780: step 98570, loss = 0.78 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:45.015509: step 98580, loss = 0.89 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:45.766262: step 98590, loss = 0.84 (1705.0 examples/sec; 0.075 sec/batch)
2017-05-05 19:31:46.529732: step 98600, loss = 0.76 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:47.296097: step 98610, loss = 0.79 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:48.051246: step 98620, loss = 0.66 (1695.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:48.823718: step 98630, loss = 0.71 (1657.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:49.589536: step 98640, loss = 0.66 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:50.357170: step 98650, loss = 0.86 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:51.122646: step 98660, loss = 0.68 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:51.878527: step 98670, loss = 0.80 (1693.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:52.641381: step 98680, loss = 0.73 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:53.410004: step 98690, loss = 0.76 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:54.169990: step 98700, loss = 0.76 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:31:54.943480: step 98710, loss = 0.59 (1654.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:55.690903: step 98720, loss = 0.55 (1712.6 examples/sec; 0.075 sec/batch)
2017-05-05 19:31:56.457043: step 98730, loss = 0.83 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:57.223525: step 98740, loss = 0.70 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:57.994096: step 98750, loss = 0.84 (1661.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:58.763113: step 98760, loss = 0.68 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:31:59.529044: step 98770, loss = 0.65 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:00.293828: step 98780, loss = 0.81 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:01.058409: step 98790, loss = 0.69 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:01.827398: step 98800, loss = 0.66 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:02.600096: step 98810, loss = 0.90 (1656.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:03.371095: step 98820, loss = 0.89 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:04.131474: step 98830, loss = 0.60 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:04.902519: step 98840, loss = 0.83 (1660.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:05.673259: step 98850, loss = 0.82 (1660.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:06.444923: step 98860, loss = 0.72 (1658.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:07.209715: step 98870, loss = 0.64 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:07.975563: step 98880, loss = 0.84 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:08.742024: step 98890, loss = 0.70 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:09.512172: step 98900, loss = 0.72 (1662.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:10.272625: step 98910, loss = 0.82 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:11.035876: step 98920, loss = 0.71 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:11.787958: step 98930, loss = 0.66 (1701.9 examples/sec; 0.075 sec/batch)
2017-05-05 19:32:12.558813: step 98940, loss = 0.82 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:13.320018: step 98950, loss = 0.87 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:14.084811: step 98960, loss = 0.80 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:14.848899: step 98970, loss = 0.80 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:15.601265: step 98980, loss = 0.60 (1701.3 examples/sec; 0.075 sec/batch)
2017-05-05 19:32:16.364210: step 98990, loss = 0.76 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:17.224012: step 99000, loss = 0.65 (1488.7 examples/sec; 0.086 sec/batch)
2017-05-05 19:32:17.896529: step 99010, loss = 0.74 (1903.3 examples/sec; 0.067 sec/batch)
2017-05-05 19:32:18.668828: step 99020, loss = 0.56 (1657.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:19.434208: step 99030, loss = 0.79 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:20.195765: step 99040, loss = 0.87 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:20.955170: step 99050, loss = 0.89 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:21.721563: step 99060, loss = 0.66 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:22.484376: step 99070, loss = 0.81 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:23.253847: step 99080, loss = 0.58 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:24.007768: step 99090, loss = 0.76 (1697.8 examples/sec; 0.075 sec/batch)
2017-05-05 19:32:24.770159: step 99100, loss = 0.63 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:25.534062: step 99110, loss = 0.86 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:26.303059: step 99120, loss = 0.82 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:27.069913: step 99130, loss = 0.64 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:27.824285: step 99140, loss = 0.87 (1696.8 examples/sec; 0.075 sec/batch)
2017-05-05 19:32:28.583152: step 99150, loss = 0.69 (1686.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:29.356520: step 99160, loss = 0.64 (1655.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:30.118647: step 99170, loss = 0.66 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:30.873668: step 99180, loss = 0.89 (1695.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:31.634302: step 99190, loss = 0.77 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:32.403329: step 99200, loss = 0.66 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:33.175004: step 99210, loss = 0.69 (1658.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:33.945344: step 99220, loss = 0.63 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:34.711166: step 99230, loss = 0.61 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:35.471419: step 99240, loss = 0.77 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:36.226149: step 99250, loss = 0.81 (1696.0 examples/sec; 0.075 sec/batch)
2017-05-05 19:32:36.989188: step 99260, loss = 0.96 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:37.757525: step 99270, loss = 0.62 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:38.529466: step 99280, loss = 0.96 (1658.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:39.295174: step 99290, loss = 0.69 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:40.051587: step 99300, loss = 0.66 (1692.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:40.815507: step 99310, loss = 0.77 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:41.575989: step 99320, loss = 0.88 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:42.341381: step 99330, loss = 0.74 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:43.106013: step 99340, loss = 0.64 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:43.865313: step 99350, loss = 0.82 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:44.631256: step 99360, loss = 0.90 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:45.392395: step 99370, loss = 0.68 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:46.155981: step 99380, loss = 0.98 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:46.921430: step 99390, loss = 0.80 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:47.684852: step 99400, loss = 0.84 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:48.450320: step 99410, loss = 0.69 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:49.211320: step 99420, loss = 0.75 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:49.987174: step 99430, loss = 0.70 (1649.8 examples/sec; 0.078 sec/batch)
2017-05-05 19:32:50.748317: step 99440, loss = 0.70 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:51.508014: step 99450, loss = 0.62 (1684.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:52.271058: step 99460, loss = 0.73 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:53.037041: step 99470, loss = 0.69 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:53.804017: step 99480, loss = 0.71 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:54.568805: step 99490, loss = 0.70 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:55.336666: step 99500, loss = 0.60 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:56.087503: step 99510, loss = 0.79 (1704.8 examples/sec; 0.075 sec/batch)
2017-05-05 19:32:56.847922: step 99520, loss = 0.83 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:57.620777: step 99530, loss = 0.78 (1656.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:58.385936: step 99540, loss = 0.60 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:32:59.148209: step 99550, loss = 0.58 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:32:59.903989: step 99560, loss = 0.84 (1693.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:00.664414: step 99570, loss = 0.58 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:01.427410: step 99580, loss = 0.67 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:02.193119: step 99590, loss = 0.68 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:33:02.957039: step 99600, loss = 0.69 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:03.710477: step 99610, loss = 0.60 (1698.9 examples/sec; 0.075 sec/batch)
2017-05-05 19:33:04.470787: step 99620, loss = 0.72 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:05.236741: step 99630, loss = 0.66 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:33:06.002612: step 99640, loss = 0.70 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:33:06.766883: step 99650, loss = 0.72 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:07.527385: step 99660, loss = 0.84 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:08.284258: step 99670, loss = 0.72 (1691.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:09.047585: step 99680, loss = 0.81 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:09.813236: step 99690, loss = 0.83 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:33:10.577990: step 99700, loss = 0.69 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:11.340449: step 99710, loss = 0.69 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:12.098310: step 99720, loss = 0.81 (1689.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:12.868926: step 99730, loss = 0.82 (1661.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:33:13.637415: step 99740, loss = 0.68 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:33:14.404079: step 99750, loss = 0.75 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:33:15.178901: step 99760, loss = 0.80 (1652.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:33:15.930580: step 99770, loss = 0.86 (1702.8 examples/sec; 0.075 sec/batch)
2017-05-05 19:33:16.694811: step 99780, loss = 0.74 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:17.457640: step 99790, loss = 0.81 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:18.225023: step 99800, loss = 0.70 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:33:18.990741: step 99810, loss = 0.75 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:33:19.752350: step 99820, loss = 0.78 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:20.513484: step 99830, loss = 0.97 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:21.276326: step 99840, loss = 0.61 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:22.041731: step 99850, loss = 0.60 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:33:22.810239: step 99860, loss = 0.66 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:33:23.571183: step 99870, loss = 0.78 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:24.338971: step 99880, loss = 0.74 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:33:25.104762: step 99890, loss = 0.79 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:33:25.870899: step 99900, loss = 0.58 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:33:26.632126: step 99910, loss = 0.68 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:27.391568: step 99920, loss = 0.62 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:28.149534: step 99930, loss = 0.87 (1688.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:28.911734: step 99940, loss = 0.77 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:29.675450: step 99950, loss = 0.70 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:30.442042: step 99960, loss = 0.77 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:33:31.204067: step 99970, loss = 0.85 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:31.959436: step 99980, loss = 0.79 (1694.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:32.825585: step 99990, loss = 0.93 (1477.8 examples/sec; 0.087 sec/batch)
2017-05-05 19:33:33.495214: step 100000, loss = 0.71 (1911.5 examples/sec; 0.067 sec/batch)
2017-05-05 19:33:34.262995: step 100010, loss = 0.70 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:33:35.029652: step 100020, loss = 0.63 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:33:35.788106: step 100030, loss = 0.62 (1687.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:36.551458: step 100040, loss = 0.74 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:37.317664: step 100050, loss = 0.91 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:33:38.087178: step 100060, loss = 0.67 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:33:38.851173: step 100070, loss = 0.68 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:39.608450: step 100080, loss = 0.65 (1690.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:40.371795: step 100090, loss = 0.62 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:41.135420: step 100100, loss = 0.77 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:41.905819: step 100110, loss = 0.75 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:33:42.675467: step 100120, loss = 0.72 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:33:43.437306: step 100130, loss = 0.75 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:44.195352: step 100140, loss = 0.75 (1688.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:44.962072: step 100150, loss = 0.73 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:33:45.724185: step 100160, loss = 0.72 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:46.492329: step 100170, loss = 0.81 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:33:47.257311: step 100180, loss = 0.77 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:48.010852: step 100190, loss = 0.63 (1698.7 examples/sec; 0.075 sec/batch)
2017-05-05 19:33:48.778636: step 100200, loss = 0.77 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:33:49.543080: step 100210, loss = 0.65 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:50.305617: step 100220, loss = 0.79 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:51.065412: step 100230, loss = 0.77 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:51.825109: step 100240, loss = 0.76 (1684.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:52.589845: step 100250, loss = 0.95 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:53.355202: step 100260, loss = 0.70 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:33:54.123213: step 100270, loss = 0.59 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:33:54.885861: step 100280, loss = 0.87 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:55.642587: step 100290, loss = 0.71 (1691.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:56.404855: step 100300, loss = 0.80 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:33:57.172875: step 100310, loss = 0.83 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:33:57.940107: step 100320, loss = 0.82 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:33:58.707898: step 100330, loss = 0.78 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:33:59.464518: step 100340, loss = 0.70 (1691.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:00.232808: step 100350, loss = 0.68 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:00.991626: step 100360, loss = 0.86 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:01.751962: step 100370, loss = 0.87 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:02.523433: step 100380, loss = 0.65 (1659.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:03.289338: step 100390, loss = 0.69 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:04.046099: step 100400, loss = 0.69 (1691.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:04.812329: step 100410, loss = 0.69 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:05.575704: step 100420, loss = 0.89 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:06.343573: step 100430, loss = 0.77 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:07.107620: step 100440, loss = 0.78 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:07.865485: step 100450, loss = 0.87 (1689.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:08.635613: step 100460, loss = 1.11 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:09.403980: step 100470, loss = 0.68 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:10.168003: step 100480, loss = 0.69 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:10.930233: step 100490, loss = 0.65 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:11.682675: step 100500, loss = 0.77 (1701.1 examples/sec; 0.075 sec/batch)
2017-05-05 19:34:12.452908: step 100510, loss = 0.71 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:13.217383: step 100520, loss = 0.83 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:13.991895: step 100530, loss = 0.71 (1652.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:14.750676: step 100540, loss = 0.81 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:15.509846: step 100550, loss = 0.72 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:16.270335: step 100560, loss = 0.76 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:17.043715: step 100570, loss = 0.65 (1655.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:17.809075: step 100580, loss = 0.69 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:18.580213: step 100590, loss = 0.76 (1659.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:19.361301: step 100600, loss = 0.78 (1638.7 examples/sec; 0.078 sec/batch)
2017-05-05 19:34:20.114394: step 100610, loss = 0.78 (1699.7 examples/sec; 0.075 sec/batch)
2017-05-05 19:34:20.878534: step 100620, loss = 0.77 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:21.641127: step 100630, loss = 0.62 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:22.408282: step 100640, loss = 0.71 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:23.173893: step 100650, loss = 0.65 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:23.926794: step 100660, loss = 0.66 (1700.1 examples/sec; 0.075 sec/batch)
2017-05-05 19:34:24.697436: step 100670, loss = 0.63 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:25.462751: step 100680, loss = 0.62 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:26.226411: step 100690, loss = 0.88 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:26.994480: step 100700, loss = 0.88 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:27.752933: step 100710, loss = 0.77 (1687.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:28.524763: step 100720, loss = 0.83 (1658.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:29.284316: step 100730, loss = 0.69 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:30.049167: step 100740, loss = 0.81 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:30.807769: step 100750, loss = 0.74 (1687.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:31.571804: step 100760, loss = 0.97 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:32.341733: step 100770, loss = 0.52 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:33.103751: step 100780, loss = 0.67 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:33.871873: step 100790, loss = 0.78 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:34.636401: step 100800, loss = 0.60 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:35.397514: step 100810, loss = 0.67 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:36.155612: step 100820, loss = 0.77 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:36.920624: step 100830, loss = 0.78 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:37.683444: step 100840, loss = 0.58 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:38.451474: step 100850, loss = 0.62 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:39.214899: step 100860, loss = 0.77 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:39.972831: step 100870, loss = 0.75 (1688.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:40.740055: step 100880, loss = 0.73 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:41.507350: step 100890, loss = 0.71 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:42.274932: step 100900, loss = 0.74 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:43.041822: step 100910, loss = 0.65 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:43.796082: step 100920, loss = 0.72 (1697.0 examples/sec; 0.075 sec/batch)
2017-05-05 19:34:44.571744: step 100930, loss = 0.62 (1650.2 examples/sec; 0.078 sec/batch)
2017-05-05 19:34:45.331454: step 100940, loss = 0.84 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:46.100503: step 100950, loss = 1.03 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:46.867779: step 100960, loss = 0.70 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:47.617520: step 100970, loss = 0.66 (1707.2 examples/sec; 0.075 sec/batch)
2017-05-05 19:34:48.502185: step 100980, loss = 0.75 (1446.9 examples/sec; 0.088 sec/batch)
2017-05-05 19:34:49.153865: step 100990, loss = 0.80 (1964.2 examples/sec; 0.065 sec/batch)
2017-05-05 19:34:49.920461: step 101000, loss = 0.75 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:50.684299: step 101010, loss = 0.65 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:51.445552: step 101020, loss = 0.83 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:52.212702: step 101030, loss = 0.57 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:52.982539: step 101040, loss = 0.71 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:53.740232: step 101050, loss = 0.65 (1689.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:54.505288: step 101060, loss = 0.80 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:55.270841: step 101070, loss = 0.79 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:56.027917: step 101080, loss = 0.84 (1690.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:56.790613: step 101090, loss = 0.60 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:57.561046: step 101100, loss = 0.73 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:58.332435: step 101110, loss = 0.80 (1659.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:34:59.091013: step 101120, loss = 0.88 (1687.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:34:59.843300: step 101130, loss = 0.77 (1701.5 examples/sec; 0.075 sec/batch)
2017-05-05 19:35:00.607856: step 101140, loss = 0.80 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:01.376402: step 101150, loss = 0.70 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:02.147399: step 101160, loss = 0.76 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:02.913716: step 101170, loss = 0.66 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:03.674179: step 101180, loss = 0.73 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:04.435889: step 101190, loss = 0.72 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:05.199787: step 101200, loss = 0.97 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:05.967911: step 101210, loss = 0.77 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:06.731568: step 101220, loss = 0.78 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:07.492897: step 101230, loss = 0.82 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:08.251987: step 101240, loss = 0.69 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:09.021975: step 101250, loss = 0.83 (1662.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:09.788441: step 101260, loss = 0.73 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:10.552275: step 101270, loss = 0.61 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:11.314347: step 101280, loss = 0.68 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:12.064246: step 101290, loss = 0.74 (1706.9 examples/sec; 0.075 sec/batch)
2017-05-05 19:35:12.828704: step 101300, loss = 0.64 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:13.595747: step 101310, loss = 0.77 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:14.360847: step 101320, loss = 0.71 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:15.128120: step 101330, loss = 0.77 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:15.880591: step 101340, loss = 0.77 (1701.1 examples/sec; 0.075 sec/batch)
2017-05-05 19:35:16.642755: step 101350, loss = 0.69 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:17.409946: step 101360, loss = 0.68 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:18.175316: step 101370, loss = 0.90 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:18.946712: step 101380, loss = 0.86 (1659.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:19.709707: step 101390, loss = 0.80 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:20.467444: step 101400, loss = 0.71 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:21.235904: step 101410, loss = 0.69 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:21.999081: step 101420, loss = 0.77 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:22.759807: step 101430, loss = 0.82 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:23.524349: step 101440, loss = 0.74 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:24.280966: step 101450, loss = 0.84 (1691.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:25.039285: step 101460, loss = 0.68 (1687.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:25.803548: step 101470, loss = 0.72 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:26.571855: step 101480, loss = 0.83 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:27.336190: step 101490, loss = 0.66 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:28.092536: step 101500, loss = 0.76 (1692.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:28.857557: step 101510, loss = 0.76 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:29.626740: step 101520, loss = 0.91 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:30.391071: step 101530, loss = 0.70 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:31.152921: step 101540, loss = 0.86 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:31.907255: step 101550, loss = 0.70 (1696.9 examples/sec; 0.075 sec/batch)
2017-05-05 19:35:32.667603: step 101560, loss = 0.72 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:33.436214: step 101570, loss = 0.73 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:34.206652: step 101580, loss = 0.79 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:34.967606: step 101590, loss = 0.87 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:35.721299: step 101600, loss = 0.72 (1698.3 examples/sec; 0.075 sec/batch)
2017-05-05 19:35:36.488002: step 101610, loss = 0.65 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:37.259697: step 101620, loss = 0.78 (1658.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:38.026492: step 101630, loss = 0.68 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:38.793065: step 101640, loss = 0.70 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:39.547528: step 101650, loss = 0.62 (1696.6 examples/sec; 0.075 sec/batch)
2017-05-05 19:35:40.311120: step 101660, loss = 0.78 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:41.072874: step 101670, loss = 0.80 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:41.839483: step 101680, loss = 0.90 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:42.605680: step 101690, loss = 0.74 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:43.369471: step 101700, loss = 0.81 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:44.130296: step 101710, loss = 0.67 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:44.897086: step 101720, loss = 0.85 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:45.658024: step 101730, loss = 0.66 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:46.423340: step 101740, loss = 0.76 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:47.193443: step 101750, loss = 0.81 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:47.950266: step 101760, loss = 0.80 (1691.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:48.715328: step 101770, loss = 0.72 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:49.488091: step 101780, loss = 0.97 (1656.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:50.253712: step 101790, loss = 0.55 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:51.014120: step 101800, loss = 0.90 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:51.776396: step 101810, loss = 0.86 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:52.542688: step 101820, loss = 0.72 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:53.307280: step 101830, loss = 0.75 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:54.075448: step 101840, loss = 0.71 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:54.837114: step 101850, loss = 0.84 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:55.595515: step 101860, loss = 0.79 (1687.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:56.356282: step 101870, loss = 0.73 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:57.120444: step 101880, loss = 0.67 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:57.882866: step 101890, loss = 0.82 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:35:58.653868: step 101900, loss = 0.72 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:35:59.409805: step 101910, loss = 0.69 (1693.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:00.169208: step 101920, loss = 0.71 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:00.921593: step 101930, loss = 0.79 (1701.3 examples/sec; 0.075 sec/batch)
2017-05-05 19:36:01.687409: step 101940, loss = 0.69 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:36:02.455322: step 101950, loss = 0.61 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:36:03.222928: step 101960, loss = 0.70 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:36:04.070100: step 101970, loss = 0.61 (1510.9 examples/sec; 0.085 sec/batch)
2017-05-05 19:36:04.730987: step 101980, loss = 0.77 (1936.8 examples/sec; 0.066 sec/batch)
2017-05-05 19:36:05.498155: step 101990, loss = 0.78 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:36:06.304326: step 102000, loss = 0.74 (1587.8 examples/sec; 0.081 sec/batch)
2017-05-05 19:36:07.066336: step 102010, loss = 0.86 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:07.819817: step 102020, loss = 0.68 (1698.8 examples/sec; 0.075 sec/batch)
2017-05-05 19:36:08.582576: step 102030, loss = 0.67 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:09.345654: step 102040, loss = 0.64 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:10.117602: step 102050, loss = 0.81 (1658.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:36:10.877774: step 102060, loss = 0.72 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:11.634988: step 102070, loss = 0.83 (1690.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:12.403730: step 102080, loss = 0.67 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:36:13.181553: step 102090, loss = 0.71 (1645.6 examples/sec; 0.078 sec/batch)
2017-05-05 19:36:13.946871: step 102100, loss = 0.67 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:36:14.713493: step 102110, loss = 0.78 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:36:15.476968: step 102120, loss = 0.80 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:16.239213: step 102130, loss = 0.71 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:17.003406: step 102140, loss = 0.82 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:17.769186: step 102150, loss = 0.89 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:36:18.532502: step 102160, loss = 0.76 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:19.306250: step 102170, loss = 0.87 (1654.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:36:20.063537: step 102180, loss = 0.87 (1690.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:20.825708: step 102190, loss = 0.72 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:21.591856: step 102200, loss = 0.71 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:36:22.354577: step 102210, loss = 0.78 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:23.128073: step 102220, loss = 0.67 (1654.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:36:23.878513: step 102230, loss = 0.68 (1705.7 examples/sec; 0.075 sec/batch)
2017-05-05 19:36:24.645450: step 102240, loss = 0.76 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:36:25.418393: step 102250, loss = 0.63 (1656.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:36:26.182584: step 102260, loss = 0.93 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:26.949723: step 102270, loss = 0.71 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:36:27.707697: step 102280, loss = 0.68 (1688.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:28.472064: step 102290, loss = 0.88 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:29.238556: step 102300, loss = 0.68 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:36:30.006622: step 102310, loss = 0.63 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:36:30.769876: step 102320, loss = 0.74 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:31.534636: step 102330, loss = 0.81 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:32.296453: step 102340, loss = 0.57 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:33.055854: step 102350, loss = 0.66 (1685.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:33.816981: step 102360, loss = 0.90 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:34.587268: step 102370, loss = 0.94 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:36:35.345589: step 102380, loss = 0.76 (1687.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:36.109139: step 102390, loss = 0.65 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:36.867537: step 102400, loss = 0.73 (1687.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:37.639526: step 102410, loss = 0.66 (1658.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:36:38.402038: step 102420, loss = 0.67 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:39.159753: step 102430, loss = 0.71 (1689.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:39.917352: step 102440, loss = 0.75 (1689.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:40.682550: step 102450, loss = 0.76 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:36:41.442530: step 102460, loss = 0.69 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:42.213437: step 102470, loss = 0.78 (1660.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:36:42.981377: step 102480, loss = 0.78 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:36:43.731964: step 102490, loss = 0.61 (1705.3 examples/sec; 0.075 sec/batch)
2017-05-05 19:36:44.499577: step 102500, loss = 0.81 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:36:45.267248: step 102510, loss = 0.80 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:36:46.042421: step 102520, loss = 0.56 (1651.2 examples/sec; 0.078 sec/batch)
2017-05-05 19:36:46.805166: step 102530, loss = 0.76 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:47.563912: step 102540, loss = 0.69 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:48.323456: step 102550, loss = 0.77 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:49.099459: step 102560, loss = 0.81 (1649.5 examples/sec; 0.078 sec/batch)
2017-05-05 19:36:49.868123: step 102570, loss = 0.78 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:36:50.643021: step 102580, loss = 0.88 (1651.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:36:51.401732: step 102590, loss = 0.63 (1687.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:52.159881: step 102600, loss = 0.72 (1688.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:52.918031: step 102610, loss = 0.72 (1688.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:53.688861: step 102620, loss = 0.81 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:36:54.452863: step 102630, loss = 0.69 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:55.219571: step 102640, loss = 0.81 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:36:55.974773: step 102650, loss = 0.68 (1694.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:56.736093: step 102660, loss = 0.65 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:57.500116: step 102670, loss = 0.68 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:36:58.266867: step 102680, loss = 0.63 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:36:59.036591: step 102690, loss = 0.94 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:36:59.786678: step 102700, loss = 0.64 (1706.5 examples/sec; 0.075 sec/batch)
2017-05-05 19:37:00.553459: step 102710, loss = 0.69 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:01.318041: step 102720, loss = 0.68 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:02.089742: step 102730, loss = 0.67 (1658.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:02.858420: step 102740, loss = 0.99 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:03.618321: step 102750, loss = 0.86 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:04.377692: step 102760, loss = 0.84 (1685.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:05.139870: step 102770, loss = 0.58 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:05.899837: step 102780, loss = 0.61 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:06.671950: step 102790, loss = 0.72 (1657.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:07.430637: step 102800, loss = 0.74 (1687.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:08.185514: step 102810, loss = 0.66 (1695.6 examples/sec; 0.075 sec/batch)
2017-05-05 19:37:08.950196: step 102820, loss = 0.69 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:09.714613: step 102830, loss = 0.73 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:10.480954: step 102840, loss = 0.73 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:11.247477: step 102850, loss = 0.87 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:12.001454: step 102860, loss = 0.82 (1697.7 examples/sec; 0.075 sec/batch)
2017-05-05 19:37:12.772424: step 102870, loss = 0.67 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:13.545221: step 102880, loss = 0.60 (1656.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:14.312897: step 102890, loss = 0.71 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:15.073428: step 102900, loss = 0.68 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:15.828201: step 102910, loss = 0.68 (1695.9 examples/sec; 0.075 sec/batch)
2017-05-05 19:37:16.594995: step 102920, loss = 0.51 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:17.359253: step 102930, loss = 0.80 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:18.127263: step 102940, loss = 0.68 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:18.895549: step 102950, loss = 0.83 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:19.752083: step 102960, loss = 0.70 (1494.4 examples/sec; 0.086 sec/batch)
2017-05-05 19:37:20.422867: step 102970, loss = 0.79 (1908.2 examples/sec; 0.067 sec/batch)
2017-05-05 19:37:21.187600: step 102980, loss = 0.71 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:21.954475: step 102990, loss = 0.73 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:22.722527: step 103000, loss = 0.78 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:23.485143: step 103010, loss = 0.76 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:24.245740: step 103020, loss = 0.70 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:25.011792: step 103030, loss = 0.76 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:25.774170: step 103040, loss = 0.73 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:26.536193: step 103050, loss = 0.71 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:27.305035: step 103060, loss = 0.86 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:28.063460: step 103070, loss = 0.67 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:28.826386: step 103080, loss = 0.71 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:29.597462: step 103090, loss = 0.92 (1660.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:30.360498: step 103100, loss = 0.82 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:31.125038: step 103110, loss = 0.73 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:31.881998: step 103120, loss = 0.65 (1691.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:32.648789: step 103130, loss = 0.86 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:33.419176: step 103140, loss = 0.90 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:34.187941: step 103150, loss = 0.70 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:34.951528: step 103160, loss = 0.78 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:35.709041: step 103170, loss = 0.81 (1689.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:36.475595: step 103180, loss = 0.70 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:37.233785: step 103190, loss = 0.72 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:38.007656: step 103200, loss = 0.75 (1654.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:38.781408: step 103210, loss = 0.62 (1654.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:39.538235: step 103220, loss = 0.86 (1691.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:40.304087: step 103230, loss = 0.77 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:41.066386: step 103240, loss = 0.79 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:41.828586: step 103250, loss = 0.59 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:42.593649: step 103260, loss = 0.68 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:43.358308: step 103270, loss = 0.69 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:44.115637: step 103280, loss = 0.73 (1690.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:44.880734: step 103290, loss = 0.85 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:45.637716: step 103300, loss = 0.64 (1690.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:46.405962: step 103310, loss = 0.60 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:47.172653: step 103320, loss = 0.79 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:47.926947: step 103330, loss = 0.71 (1696.9 examples/sec; 0.075 sec/batch)
2017-05-05 19:37:48.693572: step 103340, loss = 0.77 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:49.464731: step 103350, loss = 0.74 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:50.250396: step 103360, loss = 0.71 (1629.3 examples/sec; 0.079 sec/batch)
2017-05-05 19:37:51.013608: step 103370, loss = 0.84 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:51.776153: step 103380, loss = 0.80 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:52.527683: step 103390, loss = 0.73 (1703.2 examples/sec; 0.075 sec/batch)
2017-05-05 19:37:53.294095: step 103400, loss = 0.61 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:54.054690: step 103410, loss = 0.77 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:54.823886: step 103420, loss = 0.92 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:55.583252: step 103430, loss = 0.73 (1685.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:56.342960: step 103440, loss = 0.68 (1684.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:57.103109: step 103450, loss = 0.81 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:37:57.874542: step 103460, loss = 0.62 (1659.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:58.641070: step 103470, loss = 0.72 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:37:59.401704: step 103480, loss = 0.80 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:00.162567: step 103490, loss = 0.72 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:00.929302: step 103500, loss = 0.88 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:01.689882: step 103510, loss = 0.75 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:02.456922: step 103520, loss = 0.66 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:03.228915: step 103530, loss = 0.67 (1658.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:03.983228: step 103540, loss = 0.68 (1696.9 examples/sec; 0.075 sec/batch)
2017-05-05 19:38:04.751888: step 103550, loss = 0.78 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:05.523928: step 103560, loss = 0.66 (1657.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:06.288066: step 103570, loss = 0.67 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:07.049537: step 103580, loss = 0.67 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:07.801674: step 103590, loss = 0.67 (1701.8 examples/sec; 0.075 sec/batch)
2017-05-05 19:38:08.571923: step 103600, loss = 0.91 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:09.336111: step 103610, loss = 0.70 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:10.100518: step 103620, loss = 0.80 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:10.865402: step 103630, loss = 0.90 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:11.628729: step 103640, loss = 0.88 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:12.394479: step 103650, loss = 0.81 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:13.153811: step 103660, loss = 0.64 (1685.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:13.924668: step 103670, loss = 0.59 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:14.693648: step 103680, loss = 0.81 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:15.461704: step 103690, loss = 0.77 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:16.227310: step 103700, loss = 0.88 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:16.986625: step 103710, loss = 0.74 (1685.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:17.747939: step 103720, loss = 0.81 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:18.511064: step 103730, loss = 0.75 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:19.278303: step 103740, loss = 0.74 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:20.038802: step 103750, loss = 0.86 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:20.805254: step 103760, loss = 0.71 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:21.567723: step 103770, loss = 0.69 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:22.334830: step 103780, loss = 0.72 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:23.099527: step 103790, loss = 0.70 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:23.858164: step 103800, loss = 0.72 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:24.629352: step 103810, loss = 0.72 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:25.391939: step 103820, loss = 0.66 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:26.157806: step 103830, loss = 0.62 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:26.919102: step 103840, loss = 0.74 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:27.676392: step 103850, loss = 0.78 (1690.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:28.440379: step 103860, loss = 0.75 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:29.209448: step 103870, loss = 0.79 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:29.983423: step 103880, loss = 0.93 (1653.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:30.746253: step 103890, loss = 0.66 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:31.506545: step 103900, loss = 0.65 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:32.272698: step 103910, loss = 0.76 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:33.036458: step 103920, loss = 0.68 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:33.805450: step 103930, loss = 0.88 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:34.572879: step 103940, loss = 0.75 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:35.439223: step 103950, loss = 0.78 (1477.5 examples/sec; 0.087 sec/batch)
2017-05-05 19:38:36.102286: step 103960, loss = 0.65 (1930.5 examples/sec; 0.066 sec/batch)
2017-05-05 19:38:36.864239: step 103970, loss = 0.62 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:37.632924: step 103980, loss = 0.76 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:38.400463: step 103990, loss = 0.69 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:39.170692: step 104000, loss = 0.88 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:39.929536: step 104010, loss = 0.66 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:40.694335: step 104020, loss = 0.75 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:41.464866: step 104030, loss = 0.86 (1661.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:42.228610: step 104040, loss = 0.75 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:42.992933: step 104050, loss = 0.72 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:43.759810: step 104060, loss = 0.72 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:44.521739: step 104070, loss = 0.85 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:45.285933: step 104080, loss = 0.64 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:46.051050: step 104090, loss = 0.63 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:46.817518: step 104100, loss = 0.66 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:47.573272: step 104110, loss = 0.61 (1693.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:48.331937: step 104120, loss = 0.70 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:49.102029: step 104130, loss = 0.76 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:49.869663: step 104140, loss = 0.72 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:50.637952: step 104150, loss = 0.76 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:51.401408: step 104160, loss = 0.71 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:52.156677: step 104170, loss = 0.76 (1694.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:52.921443: step 104180, loss = 0.75 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:53.687466: step 104190, loss = 0.76 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:54.449894: step 104200, loss = 0.78 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:55.219394: step 104210, loss = 0.64 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:55.973405: step 104220, loss = 0.81 (1697.6 examples/sec; 0.075 sec/batch)
2017-05-05 19:38:56.736803: step 104230, loss = 0.73 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:57.504722: step 104240, loss = 0.77 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:58.273299: step 104250, loss = 0.74 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:38:59.034228: step 104260, loss = 0.85 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:38:59.793477: step 104270, loss = 0.66 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:00.558819: step 104280, loss = 0.65 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:01.327110: step 104290, loss = 0.71 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:02.092900: step 104300, loss = 0.71 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:02.862655: step 104310, loss = 0.68 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:03.615370: step 104320, loss = 0.87 (1700.5 examples/sec; 0.075 sec/batch)
2017-05-05 19:39:04.383237: step 104330, loss = 0.67 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:05.146891: step 104340, loss = 0.66 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:05.906572: step 104350, loss = 0.76 (1684.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:06.674533: step 104360, loss = 0.69 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:07.441380: step 104370, loss = 0.76 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:08.198337: step 104380, loss = 0.78 (1691.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:08.963303: step 104390, loss = 0.73 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:09.727415: step 104400, loss = 0.82 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:10.494065: step 104410, loss = 0.70 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:11.258729: step 104420, loss = 0.77 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:12.012645: step 104430, loss = 0.76 (1697.8 examples/sec; 0.075 sec/batch)
2017-05-05 19:39:12.772109: step 104440, loss = 0.68 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:13.539924: step 104450, loss = 0.65 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:14.309872: step 104460, loss = 0.74 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:15.073760: step 104470, loss = 0.82 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:15.835588: step 104480, loss = 0.73 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:16.591512: step 104490, loss = 0.58 (1693.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:17.359238: step 104500, loss = 0.70 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:18.126160: step 104510, loss = 0.80 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:18.892693: step 104520, loss = 0.61 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:19.646459: step 104530, loss = 0.80 (1698.1 examples/sec; 0.075 sec/batch)
2017-05-05 19:39:20.407449: step 104540, loss = 0.96 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:21.173889: step 104550, loss = 0.62 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:21.939692: step 104560, loss = 0.62 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:22.702903: step 104570, loss = 0.89 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:23.462838: step 104580, loss = 0.77 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:24.229742: step 104590, loss = 0.74 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:24.998478: step 104600, loss = 0.72 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:25.757519: step 104610, loss = 0.82 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:26.518722: step 104620, loss = 0.77 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:27.282523: step 104630, loss = 0.68 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:28.039487: step 104640, loss = 0.77 (1691.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:28.804093: step 104650, loss = 0.87 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:29.571961: step 104660, loss = 0.69 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:30.331017: step 104670, loss = 0.66 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:31.097818: step 104680, loss = 0.62 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:31.858424: step 104690, loss = 0.65 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:32.622812: step 104700, loss = 0.72 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:33.392245: step 104710, loss = 0.79 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:34.159991: step 104720, loss = 0.73 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:34.919450: step 104730, loss = 0.79 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:35.677414: step 104740, loss = 0.79 (1688.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:36.447142: step 104750, loss = 0.75 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:37.215864: step 104760, loss = 0.69 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:37.978009: step 104770, loss = 0.64 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:38.744279: step 104780, loss = 0.54 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:39.508681: step 104790, loss = 0.66 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:40.270582: step 104800, loss = 0.69 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:41.030680: step 104810, loss = 0.79 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:41.796518: step 104820, loss = 0.81 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:42.562470: step 104830, loss = 0.69 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:43.320672: step 104840, loss = 0.66 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:44.078679: step 104850, loss = 0.63 (1688.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:44.839472: step 104860, loss = 0.71 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:45.608697: step 104870, loss = 0.79 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:46.367774: step 104880, loss = 0.64 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:47.132576: step 104890, loss = 0.83 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:47.890937: step 104900, loss = 0.69 (1687.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:48.657707: step 104910, loss = 0.63 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:49.424849: step 104920, loss = 0.74 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:50.188711: step 104930, loss = 0.70 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:51.054595: step 104940, loss = 0.63 (1478.3 examples/sec; 0.087 sec/batch)
2017-05-05 19:39:51.717168: step 104950, loss = 0.75 (1931.9 examples/sec; 0.066 sec/batch)
2017-05-05 19:39:52.484666: step 104960, loss = 0.75 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:53.255568: step 104970, loss = 0.74 (1660.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:54.026536: step 104980, loss = 0.75 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:54.795087: step 104990, loss = 0.75 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:55.553650: step 105000, loss = 0.67 (1687.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:56.316083: step 105010, loss = 0.88 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:39:57.082538: step 105020, loss = 0.71 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:57.852944: step 105030, loss = 0.70 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:58.620075: step 105040, loss = 0.70 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:39:59.388636: step 105050, loss = 0.74 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:00.143939: step 105060, loss = 0.53 (1694.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:00.909479: step 105070, loss = 0.63 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:01.692211: step 105080, loss = 0.68 (1635.3 examples/sec; 0.078 sec/batch)
2017-05-05 19:40:02.460859: step 105090, loss = 0.70 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:03.231225: step 105100, loss = 0.73 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:03.986299: step 105110, loss = 0.65 (1695.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:04.749927: step 105120, loss = 0.76 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:05.514900: step 105130, loss = 0.71 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:06.283172: step 105140, loss = 0.71 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:07.050637: step 105150, loss = 0.65 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:07.804900: step 105160, loss = 0.79 (1697.0 examples/sec; 0.075 sec/batch)
2017-05-05 19:40:08.567190: step 105170, loss = 0.71 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:09.330140: step 105180, loss = 0.81 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:10.093803: step 105190, loss = 0.79 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:10.853090: step 105200, loss = 0.71 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:11.617126: step 105210, loss = 0.76 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:12.387899: step 105220, loss = 0.77 (1660.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:13.149022: step 105230, loss = 0.66 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:13.910621: step 105240, loss = 0.75 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:14.673035: step 105250, loss = 0.60 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:15.433087: step 105260, loss = 0.66 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:16.196848: step 105270, loss = 0.70 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:16.962990: step 105280, loss = 0.73 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:17.729244: step 105290, loss = 0.69 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:18.501506: step 105300, loss = 0.77 (1657.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:19.264843: step 105310, loss = 0.73 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:20.019501: step 105320, loss = 0.84 (1696.2 examples/sec; 0.075 sec/batch)
2017-05-05 19:40:20.782834: step 105330, loss = 0.73 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:21.551651: step 105340, loss = 0.82 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:22.323854: step 105350, loss = 0.61 (1657.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:23.087222: step 105360, loss = 0.71 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:23.842185: step 105370, loss = 0.75 (1695.5 examples/sec; 0.075 sec/batch)
2017-05-05 19:40:24.607714: step 105380, loss = 0.63 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:25.380105: step 105390, loss = 0.71 (1657.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:26.139840: step 105400, loss = 0.87 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:26.911462: step 105410, loss = 0.87 (1658.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:27.665333: step 105420, loss = 0.75 (1697.9 examples/sec; 0.075 sec/batch)
2017-05-05 19:40:28.432945: step 105430, loss = 0.66 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:29.202427: step 105440, loss = 0.78 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:29.966678: step 105450, loss = 0.75 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:30.727638: step 105460, loss = 0.70 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:31.492825: step 105470, loss = 0.73 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:32.250916: step 105480, loss = 0.77 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:33.013288: step 105490, loss = 0.91 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:33.782798: step 105500, loss = 0.82 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:34.550567: step 105510, loss = 0.71 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:35.312394: step 105520, loss = 0.69 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:36.067289: step 105530, loss = 0.74 (1695.6 examples/sec; 0.075 sec/batch)
2017-05-05 19:40:36.839617: step 105540, loss = 0.63 (1657.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:37.615153: step 105550, loss = 0.87 (1650.5 examples/sec; 0.078 sec/batch)
2017-05-05 19:40:38.385123: step 105560, loss = 0.71 (1662.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:39.153542: step 105570, loss = 0.58 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:39.905200: step 105580, loss = 0.85 (1702.9 examples/sec; 0.075 sec/batch)
2017-05-05 19:40:40.668297: step 105590, loss = 0.64 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:41.438156: step 105600, loss = 0.73 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:42.200637: step 105610, loss = 0.70 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:42.966250: step 105620, loss = 0.65 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:43.725835: step 105630, loss = 0.76 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:44.488034: step 105640, loss = 0.71 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:45.262464: step 105650, loss = 0.74 (1652.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:46.021084: step 105660, loss = 0.87 (1687.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:46.785904: step 105670, loss = 0.68 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:47.542836: step 105680, loss = 0.74 (1691.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:48.304956: step 105690, loss = 0.58 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:49.075765: step 105700, loss = 0.74 (1660.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:49.838851: step 105710, loss = 0.64 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:50.604579: step 105720, loss = 0.76 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:51.365700: step 105730, loss = 0.73 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:52.128736: step 105740, loss = 0.67 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:52.889245: step 105750, loss = 0.75 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:53.659184: step 105760, loss = 0.78 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:54.426863: step 105770, loss = 0.70 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:55.197349: step 105780, loss = 0.67 (1661.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:55.956488: step 105790, loss = 0.63 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:56.721296: step 105800, loss = 0.65 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:40:57.486557: step 105810, loss = 0.75 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:58.252995: step 105820, loss = 0.76 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:59.018477: step 105830, loss = 0.85 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:40:59.776977: step 105840, loss = 0.60 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:00.538854: step 105850, loss = 0.62 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:01.302066: step 105860, loss = 0.68 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:02.065570: step 105870, loss = 0.63 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:02.830139: step 105880, loss = 0.77 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:03.588038: step 105890, loss = 0.81 (1688.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:04.351397: step 105900, loss = 0.64 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:05.113473: step 105910, loss = 0.66 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:05.874805: step 105920, loss = 0.71 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:06.757287: step 105930, loss = 0.69 (1450.5 examples/sec; 0.088 sec/batch)
2017-05-05 19:41:07.419136: step 105940, loss = 0.63 (1934.0 examples/sec; 0.066 sec/batch)
2017-05-05 19:41:08.184015: step 105950, loss = 0.76 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:08.952835: step 105960, loss = 0.69 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:09.715731: step 105970, loss = 0.66 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:10.481166: step 105980, loss = 0.80 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:11.249411: step 105990, loss = 0.71 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:12.006919: step 106000, loss = 0.73 (1689.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:12.772801: step 106010, loss = 0.73 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:13.543345: step 106020, loss = 0.63 (1661.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:14.304937: step 106030, loss = 0.86 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:15.069734: step 106040, loss = 0.79 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:15.826415: step 106050, loss = 0.90 (1691.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:16.592542: step 106060, loss = 0.80 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:17.367566: step 106070, loss = 0.71 (1651.6 examples/sec; 0.078 sec/batch)
2017-05-05 19:41:18.122633: step 106080, loss = 0.74 (1695.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:18.891456: step 106090, loss = 0.58 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:19.657923: step 106100, loss = 0.87 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:20.425760: step 106110, loss = 0.73 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:21.192500: step 106120, loss = 0.72 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:21.959829: step 106130, loss = 0.65 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:22.726869: step 106140, loss = 0.84 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:23.488746: step 106150, loss = 0.69 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:24.255234: step 106160, loss = 0.71 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:25.021217: step 106170, loss = 0.63 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:25.781633: step 106180, loss = 0.74 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:26.547854: step 106190, loss = 0.66 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:27.313536: step 106200, loss = 0.71 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:28.069804: step 106210, loss = 0.94 (1692.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:28.835634: step 106220, loss = 0.76 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:29.600091: step 106230, loss = 0.69 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:30.366450: step 106240, loss = 0.89 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:31.130958: step 106250, loss = 0.78 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:31.885715: step 106260, loss = 0.69 (1695.9 examples/sec; 0.075 sec/batch)
2017-05-05 19:41:32.649971: step 106270, loss = 0.73 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:33.412521: step 106280, loss = 0.68 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:34.173402: step 106290, loss = 0.86 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:34.936038: step 106300, loss = 0.82 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:35.695275: step 106310, loss = 0.77 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:36.461524: step 106320, loss = 0.68 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:37.230470: step 106330, loss = 0.72 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:37.998784: step 106340, loss = 0.80 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:38.764680: step 106350, loss = 0.88 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:39.519970: step 106360, loss = 0.77 (1694.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:40.283314: step 106370, loss = 0.87 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:41.052271: step 106380, loss = 0.92 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:41.820156: step 106390, loss = 0.52 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:42.587130: step 106400, loss = 0.71 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:43.353254: step 106410, loss = 0.71 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:44.113383: step 106420, loss = 0.72 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:44.888638: step 106430, loss = 0.74 (1651.0 examples/sec; 0.078 sec/batch)
2017-05-05 19:41:45.650420: step 106440, loss = 0.90 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:46.417669: step 106450, loss = 0.84 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:47.181137: step 106460, loss = 0.69 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:47.941197: step 106470, loss = 0.72 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:48.711015: step 106480, loss = 0.69 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:49.470711: step 106490, loss = 0.82 (1684.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:50.237051: step 106500, loss = 0.81 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:51.004288: step 106510, loss = 0.79 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:51.758502: step 106520, loss = 0.87 (1697.1 examples/sec; 0.075 sec/batch)
2017-05-05 19:41:52.522019: step 106530, loss = 0.68 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:53.290625: step 106540, loss = 0.76 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:54.056709: step 106550, loss = 0.90 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:54.819451: step 106560, loss = 0.75 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:55.582263: step 106570, loss = 0.78 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:56.343308: step 106580, loss = 0.62 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:57.103930: step 106590, loss = 0.59 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:41:57.873427: step 106600, loss = 0.74 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:41:58.649605: step 106610, loss = 0.67 (1649.1 examples/sec; 0.078 sec/batch)
2017-05-05 19:41:59.433282: step 106620, loss = 0.70 (1633.3 examples/sec; 0.078 sec/batch)
2017-05-05 19:42:00.208387: step 106630, loss = 0.61 (1651.4 examples/sec; 0.078 sec/batch)
2017-05-05 19:42:00.968396: step 106640, loss = 0.65 (1684.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:01.732161: step 106650, loss = 0.85 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:02.497990: step 106660, loss = 0.67 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:03.258062: step 106670, loss = 0.78 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:04.021048: step 106680, loss = 0.96 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:04.785050: step 106690, loss = 0.83 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:05.549339: step 106700, loss = 0.60 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:06.322474: step 106710, loss = 0.72 (1655.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:07.088160: step 106720, loss = 0.80 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:07.860289: step 106730, loss = 0.68 (1657.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:08.689532: step 106740, loss = 0.96 (1543.6 examples/sec; 0.083 sec/batch)
2017-05-05 19:42:09.505743: step 106750, loss = 0.65 (1568.2 examples/sec; 0.082 sec/batch)
2017-05-05 19:42:10.276604: step 106760, loss = 0.74 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:11.038249: step 106770, loss = 0.66 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:11.791302: step 106780, loss = 0.78 (1699.8 examples/sec; 0.075 sec/batch)
2017-05-05 19:42:12.558678: step 106790, loss = 0.72 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:13.326654: step 106800, loss = 0.71 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:14.093531: step 106810, loss = 0.82 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:14.861151: step 106820, loss = 0.81 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:15.623041: step 106830, loss = 0.78 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:16.387026: step 106840, loss = 0.79 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:17.154638: step 106850, loss = 0.74 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:17.927464: step 106860, loss = 0.78 (1656.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:18.695999: step 106870, loss = 0.84 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:19.460915: step 106880, loss = 0.72 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:20.224834: step 106890, loss = 0.70 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:20.985878: step 106900, loss = 0.71 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:21.752188: step 106910, loss = 0.65 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:22.622682: step 106920, loss = 0.68 (1470.4 examples/sec; 0.087 sec/batch)
2017-05-05 19:42:23.290723: step 106930, loss = 0.68 (1916.0 examples/sec; 0.067 sec/batch)
2017-05-05 19:42:24.049554: step 106940, loss = 0.81 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:24.814324: step 106950, loss = 0.71 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:25.577305: step 106960, loss = 0.72 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:26.337884: step 106970, loss = 0.76 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:27.107305: step 106980, loss = 0.64 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:27.856916: step 106990, loss = 0.80 (1707.6 examples/sec; 0.075 sec/batch)
2017-05-05 19:42:28.617758: step 107000, loss = 0.70 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:29.386428: step 107010, loss = 0.79 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:30.152632: step 107020, loss = 0.66 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:30.921208: step 107030, loss = 0.63 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:31.677277: step 107040, loss = 0.60 (1693.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:32.444418: step 107050, loss = 0.73 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:33.212990: step 107060, loss = 0.67 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:33.979260: step 107070, loss = 0.92 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:34.748739: step 107080, loss = 0.75 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:35.508037: step 107090, loss = 0.78 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:36.277201: step 107100, loss = 0.71 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:37.040721: step 107110, loss = 0.76 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:37.810831: step 107120, loss = 0.79 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:38.576180: step 107130, loss = 0.80 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:39.342226: step 107140, loss = 0.79 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:40.101663: step 107150, loss = 0.82 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:40.862287: step 107160, loss = 0.72 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:41.623246: step 107170, loss = 0.61 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:42.398104: step 107180, loss = 0.70 (1651.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:43.161873: step 107190, loss = 0.75 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:43.917678: step 107200, loss = 0.70 (1693.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:44.686942: step 107210, loss = 0.83 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:45.456213: step 107220, loss = 0.76 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:46.225348: step 107230, loss = 0.78 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:47.001321: step 107240, loss = 0.73 (1649.5 examples/sec; 0.078 sec/batch)
2017-05-05 19:42:47.763980: step 107250, loss = 0.69 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:48.536183: step 107260, loss = 0.71 (1657.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:49.299772: step 107270, loss = 0.66 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:50.065729: step 107280, loss = 0.95 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:50.826610: step 107290, loss = 0.73 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:51.593551: step 107300, loss = 0.70 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:52.358325: step 107310, loss = 0.74 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:53.122029: step 107320, loss = 0.71 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:53.884394: step 107330, loss = 0.60 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:54.656695: step 107340, loss = 0.71 (1657.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:55.420205: step 107350, loss = 0.81 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:56.176638: step 107360, loss = 0.78 (1692.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:56.939177: step 107370, loss = 0.67 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:57.705597: step 107380, loss = 0.59 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:58.469981: step 107390, loss = 0.73 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:42:59.238004: step 107400, loss = 0.61 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:42:59.996699: step 107410, loss = 0.78 (1687.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:00.753537: step 107420, loss = 0.70 (1691.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:01.520483: step 107430, loss = 0.62 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:02.285709: step 107440, loss = 0.86 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:03.051704: step 107450, loss = 0.66 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:03.804470: step 107460, loss = 0.65 (1700.4 examples/sec; 0.075 sec/batch)
2017-05-05 19:43:04.567129: step 107470, loss = 0.88 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:05.334337: step 107480, loss = 0.64 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:06.108193: step 107490, loss = 0.99 (1654.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:06.872068: step 107500, loss = 0.73 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:07.626047: step 107510, loss = 0.80 (1697.7 examples/sec; 0.075 sec/batch)
2017-05-05 19:43:08.393384: step 107520, loss = 0.72 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:09.158825: step 107530, loss = 0.69 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:09.920769: step 107540, loss = 0.71 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:10.684942: step 107550, loss = 0.70 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:11.448857: step 107560, loss = 0.82 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:12.206958: step 107570, loss = 0.68 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:12.970232: step 107580, loss = 0.77 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:13.736183: step 107590, loss = 0.90 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:14.497189: step 107600, loss = 0.76 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:15.264463: step 107610, loss = 0.75 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:16.021398: step 107620, loss = 0.86 (1691.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:16.781590: step 107630, loss = 0.75 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:17.547573: step 107640, loss = 0.82 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:18.310857: step 107650, loss = 0.76 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:19.082556: step 107660, loss = 0.74 (1658.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:19.844247: step 107670, loss = 0.80 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:20.602682: step 107680, loss = 0.75 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:21.363314: step 107690, loss = 0.75 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:22.125263: step 107700, loss = 0.71 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:22.892929: step 107710, loss = 0.82 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:23.655443: step 107720, loss = 0.78 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:24.419980: step 107730, loss = 0.79 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:25.184439: step 107740, loss = 0.69 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:25.945066: step 107750, loss = 0.68 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:26.715492: step 107760, loss = 0.76 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:27.473117: step 107770, loss = 0.72 (1689.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:28.244671: step 107780, loss = 0.74 (1659.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:29.010451: step 107790, loss = 0.74 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:29.779105: step 107800, loss = 0.76 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:30.564570: step 107810, loss = 0.80 (1629.6 examples/sec; 0.079 sec/batch)
2017-05-05 19:43:31.322992: step 107820, loss = 0.68 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:32.083103: step 107830, loss = 0.99 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:32.847113: step 107840, loss = 0.77 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:33.618749: step 107850, loss = 0.80 (1658.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:34.386913: step 107860, loss = 0.85 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:35.156971: step 107870, loss = 0.65 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:35.923494: step 107880, loss = 0.78 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:36.685548: step 107890, loss = 0.79 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:37.456589: step 107900, loss = 0.87 (1660.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:38.332940: step 107910, loss = 0.66 (1460.6 examples/sec; 0.088 sec/batch)
2017-05-05 19:43:39.003996: step 107920, loss = 0.67 (1907.4 examples/sec; 0.067 sec/batch)
2017-05-05 19:43:39.759231: step 107930, loss = 0.74 (1694.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:40.521434: step 107940, loss = 0.70 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:41.288290: step 107950, loss = 0.82 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:42.052845: step 107960, loss = 0.84 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:42.819396: step 107970, loss = 0.68 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:43.579876: step 107980, loss = 0.63 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:44.337083: step 107990, loss = 0.75 (1690.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:45.111302: step 108000, loss = 0.73 (1653.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:45.869914: step 108010, loss = 0.73 (1687.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:46.634825: step 108020, loss = 0.73 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:47.390583: step 108030, loss = 0.74 (1693.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:48.156144: step 108040, loss = 0.75 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:48.933521: step 108050, loss = 0.69 (1646.6 examples/sec; 0.078 sec/batch)
2017-05-05 19:43:49.697257: step 108060, loss = 0.65 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:50.468686: step 108070, loss = 0.81 (1659.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:51.233878: step 108080, loss = 0.67 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:51.992285: step 108090, loss = 0.62 (1687.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:52.761409: step 108100, loss = 0.73 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:53.531229: step 108110, loss = 0.56 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:54.298160: step 108120, loss = 0.67 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:55.069485: step 108130, loss = 0.61 (1659.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:55.828494: step 108140, loss = 0.58 (1686.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:56.593422: step 108150, loss = 0.67 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:57.362153: step 108160, loss = 0.68 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:58.131568: step 108170, loss = 0.80 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:43:58.896208: step 108180, loss = 0.69 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:43:59.651440: step 108190, loss = 0.57 (1694.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:00.418934: step 108200, loss = 0.92 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:01.191315: step 108210, loss = 0.62 (1657.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:01.954193: step 108220, loss = 0.68 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:02.726391: step 108230, loss = 0.58 (1657.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:03.487903: step 108240, loss = 0.73 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:04.260668: step 108250, loss = 0.78 (1656.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:05.028247: step 108260, loss = 0.66 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:05.799750: step 108270, loss = 0.82 (1659.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:06.571426: step 108280, loss = 0.78 (1658.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:07.344927: step 108290, loss = 0.61 (1654.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:08.105326: step 108300, loss = 0.81 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:08.872812: step 108310, loss = 0.63 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:09.644468: step 108320, loss = 0.87 (1658.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:10.401469: step 108330, loss = 0.75 (1690.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:11.169455: step 108340, loss = 0.70 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:11.921656: step 108350, loss = 0.76 (1701.7 examples/sec; 0.075 sec/batch)
2017-05-05 19:44:12.689519: step 108360, loss = 0.80 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:13.459195: step 108370, loss = 0.67 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:14.224890: step 108380, loss = 0.71 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:14.990400: step 108390, loss = 0.74 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:15.754181: step 108400, loss = 0.66 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:16.518745: step 108410, loss = 0.67 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:17.288724: step 108420, loss = 0.71 (1662.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:18.052172: step 108430, loss = 0.67 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:18.815572: step 108440, loss = 0.64 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:19.573070: step 108450, loss = 0.81 (1689.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:20.336754: step 108460, loss = 0.71 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:21.093323: step 108470, loss = 0.81 (1691.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:21.860326: step 108480, loss = 0.69 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:22.626975: step 108490, loss = 0.66 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:23.399969: step 108500, loss = 0.83 (1655.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:24.160187: step 108510, loss = 0.74 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:24.926449: step 108520, loss = 0.71 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:25.691249: step 108530, loss = 0.74 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:26.461181: step 108540, loss = 0.68 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:27.226757: step 108550, loss = 0.94 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:27.982890: step 108560, loss = 0.79 (1692.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:28.750653: step 108570, loss = 0.63 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:29.512839: step 108580, loss = 0.74 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:30.281447: step 108590, loss = 0.65 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:31.051903: step 108600, loss = 0.68 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:31.801791: step 108610, loss = 0.66 (1706.9 examples/sec; 0.075 sec/batch)
2017-05-05 19:44:32.569113: step 108620, loss = 0.82 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:33.331305: step 108630, loss = 0.75 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:34.094661: step 108640, loss = 0.79 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:34.868035: step 108650, loss = 0.66 (1655.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:35.620097: step 108660, loss = 0.76 (1702.0 examples/sec; 0.075 sec/batch)
2017-05-05 19:44:36.384866: step 108670, loss = 0.81 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:37.148252: step 108680, loss = 0.67 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:37.912964: step 108690, loss = 0.65 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:38.680784: step 108700, loss = 0.76 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:39.443058: step 108710, loss = 0.63 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:40.208691: step 108720, loss = 0.58 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:40.966233: step 108730, loss = 0.75 (1689.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:41.739321: step 108740, loss = 0.70 (1655.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:42.508262: step 108750, loss = 0.80 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:43.279290: step 108760, loss = 0.71 (1660.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:44.036870: step 108770, loss = 0.72 (1689.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:44.803558: step 108780, loss = 0.84 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:45.568194: step 108790, loss = 0.58 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:46.332817: step 108800, loss = 0.64 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:47.095542: step 108810, loss = 0.63 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:47.856720: step 108820, loss = 0.56 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:48.617885: step 108830, loss = 0.67 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:49.381536: step 108840, loss = 0.62 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:50.143147: step 108850, loss = 0.67 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:50.906114: step 108860, loss = 0.74 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:51.658847: step 108870, loss = 0.71 (1700.5 examples/sec; 0.075 sec/batch)
2017-05-05 19:44:52.427968: step 108880, loss = 0.82 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:53.196883: step 108890, loss = 0.71 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:54.068847: step 108900, loss = 0.85 (1468.0 examples/sec; 0.087 sec/batch)
2017-05-05 19:44:54.738961: step 108910, loss = 0.80 (1910.1 examples/sec; 0.067 sec/batch)
2017-05-05 19:44:55.495493: step 108920, loss = 0.69 (1691.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:56.259204: step 108930, loss = 0.72 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:57.022760: step 108940, loss = 0.69 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:57.786976: step 108950, loss = 0.69 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:44:58.552670: step 108960, loss = 0.79 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:44:59.320822: step 108970, loss = 0.70 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:00.077699: step 108980, loss = 0.75 (1691.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:00.840188: step 108990, loss = 0.79 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:01.610966: step 109000, loss = 0.76 (1660.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:02.380733: step 109010, loss = 0.58 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:03.145231: step 109020, loss = 0.61 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:03.904979: step 109030, loss = 0.74 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:04.680927: step 109040, loss = 0.73 (1649.6 examples/sec; 0.078 sec/batch)
2017-05-05 19:45:05.453617: step 109050, loss = 0.73 (1656.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:06.212748: step 109060, loss = 0.87 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:06.983709: step 109070, loss = 0.66 (1660.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:07.735908: step 109080, loss = 0.72 (1701.7 examples/sec; 0.075 sec/batch)
2017-05-05 19:45:08.500230: step 109090, loss = 0.72 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:09.260425: step 109100, loss = 0.74 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:10.027524: step 109110, loss = 0.73 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:10.784751: step 109120, loss = 0.62 (1690.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:11.542497: step 109130, loss = 0.77 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:12.309293: step 109140, loss = 0.61 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:13.073655: step 109150, loss = 0.90 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:13.838446: step 109160, loss = 0.80 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:14.604157: step 109170, loss = 0.69 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:15.360771: step 109180, loss = 0.56 (1691.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:16.121130: step 109190, loss = 0.64 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:16.880765: step 109200, loss = 0.80 (1685.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:17.647082: step 109210, loss = 0.50 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:18.411508: step 109220, loss = 0.75 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:19.177185: step 109230, loss = 0.65 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:19.926450: step 109240, loss = 0.66 (1708.3 examples/sec; 0.075 sec/batch)
2017-05-05 19:45:20.688702: step 109250, loss = 0.63 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:21.465317: step 109260, loss = 0.77 (1648.2 examples/sec; 0.078 sec/batch)
2017-05-05 19:45:22.230002: step 109270, loss = 0.77 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:22.994031: step 109280, loss = 0.88 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:23.750111: step 109290, loss = 0.69 (1692.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:24.520580: step 109300, loss = 0.79 (1661.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:25.281047: step 109310, loss = 0.80 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:26.046073: step 109320, loss = 0.88 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:26.808242: step 109330, loss = 0.73 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:27.571277: step 109340, loss = 0.87 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:28.339456: step 109350, loss = 0.65 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:29.106129: step 109360, loss = 0.80 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:29.867198: step 109370, loss = 0.64 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:30.632333: step 109380, loss = 1.04 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:31.396681: step 109390, loss = 0.79 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:32.157675: step 109400, loss = 0.84 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:32.925058: step 109410, loss = 0.68 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:33.689774: step 109420, loss = 0.71 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:34.455282: step 109430, loss = 0.78 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:35.228650: step 109440, loss = 0.76 (1655.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:35.986425: step 109450, loss = 0.68 (1689.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:36.760003: step 109460, loss = 0.74 (1654.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:37.533465: step 109470, loss = 0.84 (1654.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:38.309961: step 109480, loss = 0.63 (1648.4 examples/sec; 0.078 sec/batch)
2017-05-05 19:45:39.083308: step 109490, loss = 0.81 (1655.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:39.848946: step 109500, loss = 0.73 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:40.618634: step 109510, loss = 0.79 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:41.385049: step 109520, loss = 0.65 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:42.150445: step 109530, loss = 0.79 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:42.918457: step 109540, loss = 0.62 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:43.675600: step 109550, loss = 0.75 (1690.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:44.444826: step 109560, loss = 0.68 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:45.218625: step 109570, loss = 0.61 (1654.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:45.975567: step 109580, loss = 0.74 (1691.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:46.736833: step 109590, loss = 0.78 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:47.503595: step 109600, loss = 0.69 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:48.275509: step 109610, loss = 0.73 (1658.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:49.036517: step 109620, loss = 0.60 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:49.800078: step 109630, loss = 0.61 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:50.569285: step 109640, loss = 0.81 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:51.335280: step 109650, loss = 0.75 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:52.095476: step 109660, loss = 0.66 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:52.860547: step 109670, loss = 0.80 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:53.632680: step 109680, loss = 0.69 (1657.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:54.396938: step 109690, loss = 0.83 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:55.167178: step 109700, loss = 0.90 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:55.919834: step 109710, loss = 0.82 (1700.7 examples/sec; 0.075 sec/batch)
2017-05-05 19:45:56.686814: step 109720, loss = 0.68 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:57.453650: step 109730, loss = 0.74 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:58.216154: step 109740, loss = 0.77 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:45:58.981929: step 109750, loss = 0.75 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:45:59.739590: step 109760, loss = 0.71 (1689.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:00.506165: step 109770, loss = 0.74 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:46:01.267675: step 109780, loss = 0.75 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:02.032875: step 109790, loss = 0.57 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:46:02.790738: step 109800, loss = 0.80 (1689.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:03.541259: step 109810, loss = 0.57 (1705.5 examples/sec; 0.075 sec/batch)
2017-05-05 19:46:04.311106: step 109820, loss = 0.61 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:46:05.072132: step 109830, loss = 0.74 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:05.874672: step 109840, loss = 0.73 (1595.0 examples/sec; 0.080 sec/batch)
2017-05-05 19:46:06.650613: step 109850, loss = 0.81 (1649.6 examples/sec; 0.078 sec/batch)
2017-05-05 19:46:07.406675: step 109860, loss = 0.82 (1693.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:08.166586: step 109870, loss = 0.55 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:08.929442: step 109880, loss = 0.73 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:09.789945: step 109890, loss = 0.69 (1487.5 examples/sec; 0.086 sec/batch)
2017-05-05 19:46:10.465941: step 109900, loss = 0.77 (1893.5 examples/sec; 0.068 sec/batch)
2017-05-05 19:46:11.230275: step 109910, loss = 0.79 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:11.988921: step 109920, loss = 0.67 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:12.755534: step 109930, loss = 0.76 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:46:13.524614: step 109940, loss = 0.56 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:46:14.288722: step 109950, loss = 0.62 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:15.057903: step 109960, loss = 0.61 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:46:15.818033: step 109970, loss = 0.68 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:16.576114: step 109980, loss = 0.65 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:17.343890: step 109990, loss = 0.74 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:46:18.102845: step 110000, loss = 0.85 (1686.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:18.869997: step 110010, loss = 0.72 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:46:19.630283: step 110020, loss = 0.65 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:20.398165: step 110030, loss = 0.73 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:46:21.164278: step 110040, loss = 0.68 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:46:21.929934: step 110050, loss = 0.79 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:46:22.707240: step 110060, loss = 0.67 (1646.7 examples/sec; 0.078 sec/batch)
2017-05-05 19:46:23.468733: step 110070, loss = 0.77 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:24.231652: step 110080, loss = 0.76 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:25.001731: step 110090, loss = 0.76 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:46:25.766207: step 110100, loss = 0.75 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:26.529112: step 110110, loss = 0.67 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:27.293485: step 110120, loss = 0.65 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:28.049859: step 110130, loss = 0.79 (1692.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:28.816113: step 110140, loss = 0.78 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:46:29.584887: step 110150, loss = 0.72 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:46:30.344251: step 110160, loss = 0.71 (1685.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:31.104793: step 110170, loss = 0.87 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:31.861037: step 110180, loss = 0.56 (1692.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:32.629884: step 110190, loss = 0.66 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:46:33.396291: step 110200, loss = 0.95 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:46:34.163344: step 110210, loss = 0.69 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:46:34.923395: step 110220, loss = 0.82 (1684.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:35.680545: step 110230, loss = 0.74 (1690.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:36.448064: step 110240, loss = 0.71 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:46:37.216262: step 110250, loss = 0.62 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:46:37.983506: step 110260, loss = 0.68 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:46:38.757322: step 110270, loss = 0.73 (1654.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:46:39.520892: step 110280, loss = 0.72 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:40.285250: step 110290, loss = 0.63 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:41.043944: step 110300, loss = 0.82 (1687.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:41.810420: step 110310, loss = 0.78 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:46:42.575915: step 110320, loss = 0.87 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:46:43.341223: step 110330, loss = 0.79 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:46:44.101140: step 110340, loss = 0.59 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:44.860451: step 110350, loss = 0.74 (1685.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:45.625292: step 110360, loss = 0.61 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:46.394926: step 110370, loss = 0.86 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:46:47.159418: step 110380, loss = 0.82 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:47.916689: step 110390, loss = 0.79 (1690.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:48.678160: step 110400, loss = 0.85 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:49.450327: step 110410, loss = 0.61 (1657.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:46:50.213081: step 110420, loss = 0.88 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:50.973270: step 110430, loss = 0.67 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:51.730937: step 110440, loss = 0.74 (1689.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:52.499854: step 110450, loss = 0.69 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:46:53.263063: step 110460, loss = 0.54 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:54.024994: step 110470, loss = 0.71 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:54.801424: step 110480, loss = 0.67 (1648.6 examples/sec; 0.078 sec/batch)
2017-05-05 19:46:55.563398: step 110490, loss = 0.81 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:46:56.329223: step 110500, loss = 0.81 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:46:57.094814: step 110510, loss = 0.80 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:46:57.868482: step 110520, loss = 0.65 (1654.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:46:58.637421: step 110530, loss = 0.90 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:46:59.398317: step 110540, loss = 0.74 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:00.156244: step 110550, loss = 0.75 (1688.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:00.914390: step 110560, loss = 0.69 (1688.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:01.671767: step 110570, loss = 0.67 (1690.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:02.439919: step 110580, loss = 0.74 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:47:03.205553: step 110590, loss = 0.72 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:47:03.968928: step 110600, loss = 0.95 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:04.729652: step 110610, loss = 0.81 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:05.494959: step 110620, loss = 0.73 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:47:06.257882: step 110630, loss = 0.78 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:07.020134: step 110640, loss = 0.81 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:07.773263: step 110650, loss = 0.63 (1699.6 examples/sec; 0.075 sec/batch)
2017-05-05 19:47:08.535908: step 110660, loss = 0.76 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:09.300513: step 110670, loss = 0.70 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:10.063505: step 110680, loss = 0.85 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:10.829641: step 110690, loss = 0.69 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:47:11.589613: step 110700, loss = 0.67 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:12.351771: step 110710, loss = 0.84 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:13.120771: step 110720, loss = 0.80 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:47:13.882811: step 110730, loss = 0.71 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:14.650412: step 110740, loss = 0.61 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:47:15.410848: step 110750, loss = 0.69 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:16.167691: step 110760, loss = 0.80 (1691.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:16.931036: step 110770, loss = 0.70 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:17.696582: step 110780, loss = 0.73 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:47:18.464580: step 110790, loss = 0.84 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:47:19.236306: step 110800, loss = 0.70 (1658.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:47:19.996632: step 110810, loss = 0.75 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:20.767086: step 110820, loss = 0.91 (1661.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:47:21.529406: step 110830, loss = 0.61 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:22.290249: step 110840, loss = 0.71 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:23.058747: step 110850, loss = 0.73 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:47:23.814175: step 110860, loss = 0.73 (1694.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:24.578957: step 110870, loss = 0.79 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:25.452229: step 110880, loss = 0.68 (1465.8 examples/sec; 0.087 sec/batch)
2017-05-05 19:47:26.114529: step 110890, loss = 0.75 (1932.7 examples/sec; 0.066 sec/batch)
2017-05-05 19:47:26.887559: step 110900, loss = 0.76 (1655.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:47:27.642619: step 110910, loss = 0.70 (1695.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:28.411205: step 110920, loss = 0.72 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:47:29.173514: step 110930, loss = 0.56 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:29.939855: step 110940, loss = 0.70 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:47:30.702206: step 110950, loss = 0.63 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:31.468016: step 110960, loss = 0.76 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:47:32.230353: step 110970, loss = 0.75 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:32.998673: step 110980, loss = 0.88 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:47:33.772874: step 110990, loss = 0.68 (1653.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:47:34.536422: step 111000, loss = 0.67 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:35.302398: step 111010, loss = 0.75 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:47:36.056751: step 111020, loss = 0.65 (1696.8 examples/sec; 0.075 sec/batch)
2017-05-05 19:47:36.817725: step 111030, loss = 0.63 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:37.586309: step 111040, loss = 0.79 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:47:38.358850: step 111050, loss = 0.76 (1656.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:47:39.124040: step 111060, loss = 0.76 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:47:39.871695: step 111070, loss = 0.79 (1712.0 examples/sec; 0.075 sec/batch)
2017-05-05 19:47:40.641115: step 111080, loss = 0.66 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:47:41.413068: step 111090, loss = 0.75 (1658.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:47:42.176696: step 111100, loss = 0.64 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:42.944047: step 111110, loss = 0.63 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:47:43.695632: step 111120, loss = 0.64 (1703.0 examples/sec; 0.075 sec/batch)
2017-05-05 19:47:44.471174: step 111130, loss = 0.68 (1650.5 examples/sec; 0.078 sec/batch)
2017-05-05 19:47:45.243265: step 111140, loss = 0.74 (1657.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:47:46.005664: step 111150, loss = 0.79 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:46.770323: step 111160, loss = 0.71 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:47.533735: step 111170, loss = 0.70 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:48.291483: step 111180, loss = 0.82 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:49.055319: step 111190, loss = 0.86 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:49.821924: step 111200, loss = 0.72 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:47:50.584699: step 111210, loss = 0.70 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:51.348789: step 111220, loss = 0.77 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:52.105502: step 111230, loss = 0.81 (1691.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:52.866059: step 111240, loss = 0.65 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:53.636599: step 111250, loss = 0.76 (1661.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:47:54.402909: step 111260, loss = 0.65 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:47:55.171186: step 111270, loss = 0.70 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:47:55.922022: step 111280, loss = 0.73 (1704.8 examples/sec; 0.075 sec/batch)
2017-05-05 19:47:56.687030: step 111290, loss = 0.81 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:47:57.449644: step 111300, loss = 0.57 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:58.212595: step 111310, loss = 0.73 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:47:58.981422: step 111320, loss = 0.68 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:47:59.733295: step 111330, loss = 0.93 (1702.4 examples/sec; 0.075 sec/batch)
2017-05-05 19:48:00.497012: step 111340, loss = 0.62 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:01.258550: step 111350, loss = 0.73 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:02.020207: step 111360, loss = 0.79 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:02.789955: step 111370, loss = 0.64 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:03.549082: step 111380, loss = 0.65 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:04.310818: step 111390, loss = 0.80 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:05.074216: step 111400, loss = 0.89 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:05.833415: step 111410, loss = 0.68 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:06.599515: step 111420, loss = 0.71 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:07.366956: step 111430, loss = 0.83 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:08.127228: step 111440, loss = 0.84 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:08.892769: step 111450, loss = 0.82 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:09.654130: step 111460, loss = 0.92 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:10.425397: step 111470, loss = 0.67 (1659.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:11.190637: step 111480, loss = 0.67 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:11.947188: step 111490, loss = 0.70 (1691.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:12.714517: step 111500, loss = 0.59 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:13.485574: step 111510, loss = 0.59 (1660.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:14.249334: step 111520, loss = 0.84 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:15.018042: step 111530, loss = 0.78 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:15.781307: step 111540, loss = 0.91 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:16.542268: step 111550, loss = 0.73 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:17.311904: step 111560, loss = 0.73 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:18.075558: step 111570, loss = 0.73 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:18.839660: step 111580, loss = 0.75 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:19.598390: step 111590, loss = 0.69 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:20.368217: step 111600, loss = 0.75 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:21.127634: step 111610, loss = 0.78 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:21.899710: step 111620, loss = 0.65 (1657.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:22.660225: step 111630, loss = 0.90 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:23.419472: step 111640, loss = 0.66 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:24.177862: step 111650, loss = 0.74 (1687.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:24.942420: step 111660, loss = 0.77 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:25.707667: step 111670, loss = 0.77 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:26.472908: step 111680, loss = 0.64 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:27.239237: step 111690, loss = 0.71 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:27.990080: step 111700, loss = 0.85 (1704.8 examples/sec; 0.075 sec/batch)
2017-05-05 19:48:28.758050: step 111710, loss = 0.71 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:29.528461: step 111720, loss = 0.71 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:30.292523: step 111730, loss = 0.71 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:31.050927: step 111740, loss = 0.74 (1687.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:31.808305: step 111750, loss = 0.77 (1690.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:32.571444: step 111760, loss = 0.63 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:33.338764: step 111770, loss = 0.73 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:34.103623: step 111780, loss = 0.82 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:34.870574: step 111790, loss = 0.71 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:35.628743: step 111800, loss = 0.64 (1688.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:36.392948: step 111810, loss = 0.74 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:37.162151: step 111820, loss = 0.64 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:37.931302: step 111830, loss = 0.79 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:38.704716: step 111840, loss = 0.64 (1655.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:39.465634: step 111850, loss = 0.70 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:40.223846: step 111860, loss = 0.87 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:41.083704: step 111870, loss = 0.65 (1488.6 examples/sec; 0.086 sec/batch)
2017-05-05 19:48:41.750750: step 111880, loss = 0.64 (1918.9 examples/sec; 0.067 sec/batch)
2017-05-05 19:48:42.511495: step 111890, loss = 0.78 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:43.279046: step 111900, loss = 0.61 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:44.031403: step 111910, loss = 0.70 (1701.4 examples/sec; 0.075 sec/batch)
2017-05-05 19:48:44.793136: step 111920, loss = 0.65 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:45.553266: step 111930, loss = 0.85 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:46.316929: step 111940, loss = 0.62 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:47.084676: step 111950, loss = 0.83 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:47.842789: step 111960, loss = 0.65 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:48.610098: step 111970, loss = 0.79 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:49.378065: step 111980, loss = 0.82 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:50.142483: step 111990, loss = 0.74 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:50.906577: step 112000, loss = 0.75 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:51.665531: step 112010, loss = 0.78 (1686.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:52.431205: step 112020, loss = 0.75 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:53.200809: step 112030, loss = 0.69 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:53.970231: step 112040, loss = 0.75 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:54.736995: step 112050, loss = 0.69 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:55.496565: step 112060, loss = 0.79 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:48:56.261761: step 112070, loss = 0.84 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:57.029727: step 112080, loss = 0.67 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:57.804350: step 112090, loss = 0.61 (1652.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:58.573915: step 112100, loss = 0.74 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:48:59.334411: step 112110, loss = 0.74 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:00.089842: step 112120, loss = 0.70 (1694.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:00.851347: step 112130, loss = 0.83 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:01.609437: step 112140, loss = 0.65 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:02.373536: step 112150, loss = 0.71 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:03.134354: step 112160, loss = 0.64 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:03.890648: step 112170, loss = 0.61 (1692.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:04.653080: step 112180, loss = 0.67 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:05.420073: step 112190, loss = 0.70 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:49:06.187920: step 112200, loss = 0.69 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:49:06.952801: step 112210, loss = 0.64 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:07.708102: step 112220, loss = 0.70 (1694.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:08.472460: step 112230, loss = 0.68 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:09.235228: step 112240, loss = 0.73 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:10.000577: step 112250, loss = 0.72 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:49:10.760589: step 112260, loss = 0.64 (1684.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:11.523630: step 112270, loss = 0.71 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:12.289338: step 112280, loss = 0.85 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:49:13.050603: step 112290, loss = 0.68 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:13.828857: step 112300, loss = 0.67 (1644.7 examples/sec; 0.078 sec/batch)
2017-05-05 19:49:14.595623: step 112310, loss = 0.75 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:49:15.362280: step 112320, loss = 0.83 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:49:16.121173: step 112330, loss = 0.70 (1686.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:16.879351: step 112340, loss = 0.78 (1688.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:17.645381: step 112350, loss = 0.68 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:49:18.412796: step 112360, loss = 0.77 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:49:19.187943: step 112370, loss = 0.74 (1651.3 examples/sec; 0.078 sec/batch)
2017-05-05 19:49:19.939122: step 112380, loss = 1.03 (1704.0 examples/sec; 0.075 sec/batch)
2017-05-05 19:49:20.698651: step 112390, loss = 0.67 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:21.462417: step 112400, loss = 0.70 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:22.228408: step 112410, loss = 0.63 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:49:22.990321: step 112420, loss = 0.68 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:23.745946: step 112430, loss = 0.64 (1694.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:24.514341: step 112440, loss = 0.89 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:49:25.278131: step 112450, loss = 0.83 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:26.035563: step 112460, loss = 0.88 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:26.796682: step 112470, loss = 0.70 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:27.556098: step 112480, loss = 0.74 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:28.327475: step 112490, loss = 0.77 (1659.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:49:29.089916: step 112500, loss = 0.70 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:29.856209: step 112510, loss = 0.67 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:49:30.618348: step 112520, loss = 0.65 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:31.379634: step 112530, loss = 0.60 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:32.139880: step 112540, loss = 0.70 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:32.904711: step 112550, loss = 0.75 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:33.670053: step 112560, loss = 0.84 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:49:34.437597: step 112570, loss = 1.06 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:49:35.206406: step 112580, loss = 0.77 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:49:35.961050: step 112590, loss = 0.82 (1696.2 examples/sec; 0.075 sec/batch)
2017-05-05 19:49:36.726595: step 112600, loss = 0.69 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:49:37.491472: step 112610, loss = 0.66 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:38.256778: step 112620, loss = 0.69 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:49:39.020563: step 112630, loss = 0.66 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:39.779232: step 112640, loss = 0.66 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:40.544304: step 112650, loss = 0.75 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:49:41.313017: step 112660, loss = 0.74 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:49:42.079641: step 112670, loss = 0.60 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:49:42.847271: step 112680, loss = 0.70 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:49:43.606305: step 112690, loss = 0.59 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:44.369016: step 112700, loss = 0.61 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:45.134190: step 112710, loss = 0.65 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:49:45.898504: step 112720, loss = 0.70 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:46.664708: step 112730, loss = 0.94 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:49:47.431715: step 112740, loss = 0.96 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:49:48.196990: step 112750, loss = 0.77 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:49:48.963912: step 112760, loss = 0.76 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:49:49.721117: step 112770, loss = 0.87 (1690.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:50.483681: step 112780, loss = 0.82 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:51.245684: step 112790, loss = 0.75 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:52.005825: step 112800, loss = 0.90 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:52.771656: step 112810, loss = 0.68 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:49:53.540777: step 112820, loss = 0.65 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:49:54.312237: step 112830, loss = 0.63 (1659.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:49:55.074583: step 112840, loss = 0.88 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:55.832447: step 112850, loss = 0.60 (1689.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:56.702663: step 112860, loss = 0.65 (1470.9 examples/sec; 0.087 sec/batch)
2017-05-05 19:49:57.376073: step 112870, loss = 0.64 (1900.8 examples/sec; 0.067 sec/batch)
2017-05-05 19:49:58.136574: step 112880, loss = 0.82 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:49:58.903475: step 112890, loss = 0.70 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:49:59.663568: step 112900, loss = 0.80 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:00.418300: step 112910, loss = 0.65 (1696.0 examples/sec; 0.075 sec/batch)
2017-05-05 19:50:01.184933: step 112920, loss = 0.71 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:01.953697: step 112930, loss = 0.82 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:02.721507: step 112940, loss = 0.74 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:03.482761: step 112950, loss = 0.77 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:04.244483: step 112960, loss = 0.89 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:05.008271: step 112970, loss = 0.81 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:05.770260: step 112980, loss = 0.82 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:06.536079: step 112990, loss = 0.58 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:07.300978: step 113000, loss = 0.79 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:08.060505: step 113010, loss = 0.83 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:08.817416: step 113020, loss = 0.78 (1691.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:09.580895: step 113030, loss = 0.70 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:10.351134: step 113040, loss = 0.88 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:11.116027: step 113050, loss = 0.71 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:11.870061: step 113060, loss = 0.75 (1697.5 examples/sec; 0.075 sec/batch)
2017-05-05 19:50:12.633254: step 113070, loss = 0.68 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:13.400466: step 113080, loss = 0.72 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:14.160137: step 113090, loss = 0.60 (1684.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:14.926029: step 113100, loss = 0.92 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:15.684546: step 113110, loss = 0.66 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:16.454949: step 113120, loss = 0.77 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:17.220796: step 113130, loss = 0.67 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:17.986589: step 113140, loss = 0.72 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:18.764265: step 113150, loss = 0.87 (1645.9 examples/sec; 0.078 sec/batch)
2017-05-05 19:50:19.520908: step 113160, loss = 0.83 (1691.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:20.284482: step 113170, loss = 0.69 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:21.043151: step 113180, loss = 0.83 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:21.809254: step 113190, loss = 0.66 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:22.570155: step 113200, loss = 0.93 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:23.335635: step 113210, loss = 0.59 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:24.097575: step 113220, loss = 0.71 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:24.865413: step 113230, loss = 0.76 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:25.632535: step 113240, loss = 0.65 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:26.406203: step 113250, loss = 0.78 (1654.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:27.169204: step 113260, loss = 0.77 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:27.929347: step 113270, loss = 0.77 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:28.698911: step 113280, loss = 0.83 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:29.468013: step 113290, loss = 0.65 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:30.230655: step 113300, loss = 0.60 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:30.999135: step 113310, loss = 0.74 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:31.753815: step 113320, loss = 0.65 (1696.1 examples/sec; 0.075 sec/batch)
2017-05-05 19:50:32.523673: step 113330, loss = 0.60 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:33.289096: step 113340, loss = 0.76 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:34.054637: step 113350, loss = 0.71 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:34.825498: step 113360, loss = 0.63 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:35.583590: step 113370, loss = 0.71 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:36.350916: step 113380, loss = 0.62 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:37.116630: step 113390, loss = 0.79 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:37.883333: step 113400, loss = 0.66 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:38.646944: step 113410, loss = 0.82 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:39.414075: step 113420, loss = 0.68 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:40.169816: step 113430, loss = 0.72 (1693.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:40.934908: step 113440, loss = 0.81 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:41.697721: step 113450, loss = 0.72 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:42.469863: step 113460, loss = 0.79 (1657.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:43.230962: step 113470, loss = 0.68 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:43.988812: step 113480, loss = 0.77 (1689.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:44.757778: step 113490, loss = 0.67 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:45.525697: step 113500, loss = 0.72 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:46.291110: step 113510, loss = 0.66 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:47.054243: step 113520, loss = 0.65 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:47.806973: step 113530, loss = 0.84 (1700.5 examples/sec; 0.075 sec/batch)
2017-05-05 19:50:48.572490: step 113540, loss = 0.76 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:49.336405: step 113550, loss = 0.73 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:50.096104: step 113560, loss = 0.81 (1684.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:50.862220: step 113570, loss = 0.67 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:51.624106: step 113580, loss = 0.75 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:52.384351: step 113590, loss = 0.75 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:53.151307: step 113600, loss = 0.73 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:53.913501: step 113610, loss = 0.84 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:54.680084: step 113620, loss = 0.73 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:55.440787: step 113630, loss = 0.74 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:56.195124: step 113640, loss = 0.67 (1696.9 examples/sec; 0.075 sec/batch)
2017-05-05 19:50:56.962783: step 113650, loss = 0.72 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:57.728562: step 113660, loss = 0.69 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:50:58.491465: step 113670, loss = 0.87 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:50:59.255489: step 113680, loss = 0.69 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:00.007138: step 113690, loss = 0.83 (1702.9 examples/sec; 0.075 sec/batch)
2017-05-05 19:51:00.767612: step 113700, loss = 0.76 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:01.529450: step 113710, loss = 0.74 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:02.294178: step 113720, loss = 0.95 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:03.054493: step 113730, loss = 0.62 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:03.807439: step 113740, loss = 0.71 (1700.0 examples/sec; 0.075 sec/batch)
2017-05-05 19:51:04.572633: step 113750, loss = 0.86 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:51:05.333770: step 113760, loss = 0.64 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:06.095973: step 113770, loss = 0.73 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:06.859921: step 113780, loss = 0.93 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:07.611656: step 113790, loss = 0.58 (1702.7 examples/sec; 0.075 sec/batch)
2017-05-05 19:51:08.376620: step 113800, loss = 0.71 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:09.146306: step 113810, loss = 0.65 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:51:09.907254: step 113820, loss = 0.66 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:10.674479: step 113830, loss = 0.77 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:51:11.426046: step 113840, loss = 0.64 (1703.1 examples/sec; 0.075 sec/batch)
2017-05-05 19:51:12.294417: step 113850, loss = 0.69 (1474.0 examples/sec; 0.087 sec/batch)
2017-05-05 19:51:12.954969: step 113860, loss = 0.62 (1937.8 examples/sec; 0.066 sec/batch)
2017-05-05 19:51:13.719207: step 113870, loss = 0.81 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:14.484677: step 113880, loss = 0.84 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:51:15.254641: step 113890, loss = 0.56 (1662.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:51:16.008173: step 113900, loss = 0.80 (1698.7 examples/sec; 0.075 sec/batch)
2017-05-05 19:51:16.780180: step 113910, loss = 0.76 (1658.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:51:17.546293: step 113920, loss = 0.86 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:51:18.314697: step 113930, loss = 0.62 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:51:19.078861: step 113940, loss = 0.70 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:19.830033: step 113950, loss = 0.74 (1704.0 examples/sec; 0.075 sec/batch)
2017-05-05 19:51:20.595403: step 113960, loss = 0.79 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:51:21.365543: step 113970, loss = 0.79 (1662.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:51:22.124565: step 113980, loss = 0.68 (1686.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:22.890453: step 113990, loss = 0.75 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:51:23.648276: step 114000, loss = 0.60 (1689.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:24.411985: step 114010, loss = 0.79 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:25.177673: step 114020, loss = 0.69 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:51:25.937924: step 114030, loss = 0.86 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:26.707890: step 114040, loss = 0.73 (1662.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:51:27.467998: step 114050, loss = 0.68 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:28.229543: step 114060, loss = 0.78 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:28.988711: step 114070, loss = 0.77 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:29.756250: step 114080, loss = 0.79 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:51:30.527799: step 114090, loss = 0.61 (1659.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:51:31.297809: step 114100, loss = 0.70 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:51:32.056621: step 114110, loss = 0.71 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:32.824873: step 114120, loss = 0.75 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:51:33.587110: step 114130, loss = 0.89 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:34.351579: step 114140, loss = 0.70 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:35.117792: step 114150, loss = 0.66 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:51:35.880908: step 114160, loss = 0.75 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:36.645042: step 114170, loss = 0.86 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:37.412207: step 114180, loss = 0.68 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:51:38.175296: step 114190, loss = 0.71 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:38.946882: step 114200, loss = 0.66 (1658.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:51:39.708965: step 114210, loss = 0.77 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:40.471586: step 114220, loss = 0.75 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:41.239981: step 114230, loss = 0.87 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:51:41.998167: step 114240, loss = 0.83 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:42.767858: step 114250, loss = 0.70 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:51:43.526593: step 114260, loss = 0.75 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:44.295183: step 114270, loss = 0.71 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:51:45.062766: step 114280, loss = 0.81 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:51:45.826514: step 114290, loss = 0.67 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:46.598208: step 114300, loss = 0.75 (1658.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:51:47.368262: step 114310, loss = 0.74 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:51:48.121071: step 114320, loss = 0.87 (1700.3 examples/sec; 0.075 sec/batch)
2017-05-05 19:51:48.883592: step 114330, loss = 0.67 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:49.652924: step 114340, loss = 0.62 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:51:50.423160: step 114350, loss = 0.72 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:51:51.184573: step 114360, loss = 0.83 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:51.939254: step 114370, loss = 0.90 (1696.1 examples/sec; 0.075 sec/batch)
2017-05-05 19:51:52.702080: step 114380, loss = 0.68 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:53.462945: step 114390, loss = 0.89 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:54.228068: step 114400, loss = 0.73 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:51:54.995343: step 114410, loss = 0.69 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:51:55.750757: step 114420, loss = 0.67 (1694.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:56.514469: step 114430, loss = 0.61 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:57.284341: step 114440, loss = 0.80 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:51:58.046159: step 114450, loss = 0.89 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:51:58.814796: step 114460, loss = 0.73 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:51:59.571073: step 114470, loss = 0.72 (1692.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:00.332876: step 114480, loss = 0.66 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:01.100729: step 114490, loss = 0.75 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:52:01.870637: step 114500, loss = 0.72 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:52:02.628911: step 114510, loss = 0.78 (1688.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:03.389107: step 114520, loss = 0.87 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:04.152419: step 114530, loss = 0.67 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:04.919306: step 114540, loss = 0.76 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:52:05.683228: step 114550, loss = 0.70 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:06.456498: step 114560, loss = 0.54 (1655.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:52:07.225136: step 114570, loss = 0.69 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:52:07.980875: step 114580, loss = 0.79 (1693.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:08.754725: step 114590, loss = 0.71 (1654.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:52:09.522837: step 114600, loss = 0.72 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:52:10.293910: step 114610, loss = 0.69 (1660.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:52:11.059028: step 114620, loss = 0.77 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:52:11.821666: step 114630, loss = 0.66 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:12.586810: step 114640, loss = 0.68 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:52:13.353749: step 114650, loss = 0.69 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:52:14.118196: step 114660, loss = 0.73 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:14.897742: step 114670, loss = 0.69 (1642.0 examples/sec; 0.078 sec/batch)
2017-05-05 19:52:15.641731: step 114680, loss = 0.73 (1720.5 examples/sec; 0.074 sec/batch)
2017-05-05 19:52:16.405561: step 114690, loss = 0.67 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:17.167476: step 114700, loss = 0.71 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:17.930059: step 114710, loss = 0.70 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:18.695204: step 114720, loss = 0.77 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:52:19.453056: step 114730, loss = 0.61 (1689.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:20.215596: step 114740, loss = 0.74 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:20.974619: step 114750, loss = 0.62 (1686.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:21.739429: step 114760, loss = 0.92 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:22.503939: step 114770, loss = 0.69 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:23.268418: step 114780, loss = 0.62 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:24.024317: step 114790, loss = 0.64 (1693.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:24.785874: step 114800, loss = 0.57 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:25.550034: step 114810, loss = 0.85 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:26.316711: step 114820, loss = 0.63 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:52:27.084177: step 114830, loss = 0.70 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:52:27.937003: step 114840, loss = 0.72 (1500.9 examples/sec; 0.085 sec/batch)
2017-05-05 19:52:28.605888: step 114850, loss = 0.83 (1913.6 examples/sec; 0.067 sec/batch)
2017-05-05 19:52:29.367192: step 114860, loss = 0.87 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:30.133217: step 114870, loss = 0.73 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:52:30.895694: step 114880, loss = 0.77 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:31.651417: step 114890, loss = 0.70 (1693.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:32.414740: step 114900, loss = 0.87 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:33.181456: step 114910, loss = 0.69 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:52:33.944712: step 114920, loss = 0.73 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:34.709427: step 114930, loss = 0.73 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:35.473178: step 114940, loss = 0.75 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:36.232938: step 114950, loss = 0.80 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:36.993597: step 114960, loss = 0.84 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:37.758952: step 114970, loss = 0.76 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:52:38.523406: step 114980, loss = 0.78 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:39.287802: step 114990, loss = 0.57 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:40.042215: step 115000, loss = 0.73 (1696.7 examples/sec; 0.075 sec/batch)
2017-05-05 19:52:40.801296: step 115010, loss = 0.68 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:41.572134: step 115020, loss = 0.66 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:52:42.335015: step 115030, loss = 0.67 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:43.111948: step 115040, loss = 0.59 (1647.5 examples/sec; 0.078 sec/batch)
2017-05-05 19:52:43.854413: step 115050, loss = 0.82 (1724.0 examples/sec; 0.074 sec/batch)
2017-05-05 19:52:44.623920: step 115060, loss = 0.69 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:52:45.393306: step 115070, loss = 0.72 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:52:46.161866: step 115080, loss = 0.81 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:52:46.921307: step 115090, loss = 0.67 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:47.673908: step 115100, loss = 0.69 (1700.8 examples/sec; 0.075 sec/batch)
2017-05-05 19:52:48.440045: step 115110, loss = 0.73 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:52:49.209316: step 115120, loss = 0.75 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:52:49.979115: step 115130, loss = 0.85 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:52:50.735551: step 115140, loss = 0.72 (1692.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:51.500729: step 115150, loss = 0.63 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:52:52.259796: step 115160, loss = 0.70 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:53.024548: step 115170, loss = 0.72 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:53.788610: step 115180, loss = 0.77 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:54.570911: step 115190, loss = 0.72 (1636.2 examples/sec; 0.078 sec/batch)
2017-05-05 19:52:55.335942: step 115200, loss = 0.68 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:52:56.096548: step 115210, loss = 0.73 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:56.858721: step 115220, loss = 0.74 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:52:57.629947: step 115230, loss = 0.65 (1659.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:52:58.395232: step 115240, loss = 0.60 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:52:59.161526: step 115250, loss = 0.64 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:52:59.914755: step 115260, loss = 0.62 (1699.4 examples/sec; 0.075 sec/batch)
2017-05-05 19:53:00.679050: step 115270, loss = 0.90 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:01.448126: step 115280, loss = 0.82 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:53:02.206636: step 115290, loss = 0.75 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:02.972122: step 115300, loss = 0.81 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:53:03.732377: step 115310, loss = 0.73 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:04.501295: step 115320, loss = 0.72 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:53:05.262032: step 115330, loss = 0.68 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:06.021220: step 115340, loss = 0.66 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:06.791544: step 115350, loss = 0.73 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:53:07.550469: step 115360, loss = 0.74 (1686.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:08.309195: step 115370, loss = 0.73 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:09.071519: step 115380, loss = 0.97 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:09.831587: step 115390, loss = 0.76 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:10.593495: step 115400, loss = 0.89 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:11.354197: step 115410, loss = 0.89 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:12.115523: step 115420, loss = 0.77 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:12.879420: step 115430, loss = 0.73 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:13.646579: step 115440, loss = 0.64 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:53:14.405067: step 115450, loss = 0.62 (1687.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:15.173189: step 115460, loss = 0.72 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:53:15.922103: step 115470, loss = 0.68 (1709.1 examples/sec; 0.075 sec/batch)
2017-05-05 19:53:16.686549: step 115480, loss = 0.70 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:17.461077: step 115490, loss = 0.68 (1652.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:53:18.224074: step 115500, loss = 0.81 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:19.000872: step 115510, loss = 0.75 (1647.8 examples/sec; 0.078 sec/batch)
2017-05-05 19:53:19.759532: step 115520, loss = 0.80 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:20.521138: step 115530, loss = 0.70 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:21.288158: step 115540, loss = 0.64 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:53:22.049575: step 115550, loss = 0.68 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:22.815993: step 115560, loss = 0.65 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:53:23.572459: step 115570, loss = 0.73 (1692.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:24.337588: step 115580, loss = 0.83 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:53:25.106547: step 115590, loss = 0.75 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:53:25.871249: step 115600, loss = 0.85 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:26.635166: step 115610, loss = 0.80 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:27.404180: step 115620, loss = 0.67 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:53:28.162002: step 115630, loss = 0.80 (1689.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:28.927910: step 115640, loss = 0.69 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:53:29.697018: step 115650, loss = 0.71 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:53:30.462248: step 115660, loss = 0.73 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:53:31.225422: step 115670, loss = 0.85 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:31.980202: step 115680, loss = 0.65 (1695.9 examples/sec; 0.075 sec/batch)
2017-05-05 19:53:32.751268: step 115690, loss = 0.80 (1660.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:53:33.525677: step 115700, loss = 0.70 (1652.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:53:34.290144: step 115710, loss = 0.68 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:35.055522: step 115720, loss = 0.54 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:53:35.816383: step 115730, loss = 0.68 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:36.574351: step 115740, loss = 0.84 (1688.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:37.340217: step 115750, loss = 0.63 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:53:38.100695: step 115760, loss = 0.63 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:38.863347: step 115770, loss = 0.65 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:39.621953: step 115780, loss = 0.80 (1687.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:40.384643: step 115790, loss = 0.74 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:41.146627: step 115800, loss = 0.82 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:41.908960: step 115810, loss = 0.76 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:42.671793: step 115820, loss = 0.70 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:43.534860: step 115830, loss = 0.81 (1483.1 examples/sec; 0.086 sec/batch)
2017-05-05 19:53:44.188953: step 115840, loss = 0.85 (1956.9 examples/sec; 0.065 sec/batch)
2017-05-05 19:53:44.949797: step 115850, loss = 0.74 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:45.714609: step 115860, loss = 0.62 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:46.477888: step 115870, loss = 0.73 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:47.239076: step 115880, loss = 0.77 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:48.001276: step 115890, loss = 0.72 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:48.762575: step 115900, loss = 0.82 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:49.529125: step 115910, loss = 0.91 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:53:50.296344: step 115920, loss = 0.84 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:53:51.062287: step 115930, loss = 0.63 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:53:51.812820: step 115940, loss = 0.78 (1705.5 examples/sec; 0.075 sec/batch)
2017-05-05 19:53:52.581492: step 115950, loss = 0.74 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:53:53.338553: step 115960, loss = 0.83 (1690.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:54.109619: step 115970, loss = 0.76 (1660.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:53:54.874576: step 115980, loss = 0.69 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:55.634592: step 115990, loss = 0.61 (1684.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:56.391730: step 116000, loss = 0.61 (1690.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:57.157078: step 116010, loss = 0.78 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:53:57.917095: step 116020, loss = 0.68 (1684.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:53:58.693736: step 116030, loss = 0.73 (1648.1 examples/sec; 0.078 sec/batch)
2017-05-05 19:53:59.459782: step 116040, loss = 0.70 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:54:00.214635: step 116050, loss = 0.73 (1695.7 examples/sec; 0.075 sec/batch)
2017-05-05 19:54:00.977166: step 116060, loss = 0.65 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:01.738163: step 116070, loss = 0.71 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:02.508138: step 116080, loss = 0.76 (1662.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:54:03.268377: step 116090, loss = 0.72 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:04.023727: step 116100, loss = 0.76 (1694.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:04.792703: step 116110, loss = 0.77 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:54:05.564431: step 116120, loss = 0.78 (1658.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:54:06.330493: step 116130, loss = 0.65 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:54:07.092868: step 116140, loss = 0.75 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:07.840973: step 116150, loss = 0.72 (1711.0 examples/sec; 0.075 sec/batch)
2017-05-05 19:54:08.603903: step 116160, loss = 0.72 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:09.370622: step 116170, loss = 0.73 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:54:10.132996: step 116180, loss = 0.89 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:10.893798: step 116190, loss = 0.77 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:11.647427: step 116200, loss = 0.70 (1698.5 examples/sec; 0.075 sec/batch)
2017-05-05 19:54:12.409250: step 116210, loss = 0.70 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:13.169943: step 116220, loss = 0.67 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:13.933485: step 116230, loss = 0.74 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:14.697145: step 116240, loss = 0.67 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:15.452467: step 116250, loss = 0.75 (1694.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:16.224126: step 116260, loss = 0.64 (1658.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:54:16.984932: step 116270, loss = 0.67 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:17.750909: step 116280, loss = 0.79 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:54:18.515490: step 116290, loss = 0.64 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:19.274537: step 116300, loss = 0.53 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:20.031044: step 116310, loss = 0.80 (1692.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:20.786917: step 116320, loss = 0.72 (1693.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:21.551780: step 116330, loss = 0.66 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:22.321396: step 116340, loss = 0.80 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:54:23.084196: step 116350, loss = 0.69 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:23.841431: step 116360, loss = 0.77 (1690.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:24.603665: step 116370, loss = 0.72 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:25.368593: step 116380, loss = 0.75 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:26.133339: step 116390, loss = 0.76 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:26.899210: step 116400, loss = 0.77 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:54:27.652757: step 116410, loss = 0.75 (1698.6 examples/sec; 0.075 sec/batch)
2017-05-05 19:54:28.421212: step 116420, loss = 0.87 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:54:29.193142: step 116430, loss = 0.78 (1658.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:54:29.954011: step 116440, loss = 0.69 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:30.718113: step 116450, loss = 0.72 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:31.476879: step 116460, loss = 0.71 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:32.243891: step 116470, loss = 0.65 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:54:33.004417: step 116480, loss = 0.93 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:33.765647: step 116490, loss = 0.65 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:34.532087: step 116500, loss = 0.54 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:54:35.296973: step 116510, loss = 0.94 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:36.054266: step 116520, loss = 0.60 (1690.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:36.820133: step 116530, loss = 0.75 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:54:37.588440: step 116540, loss = 0.62 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:54:38.361252: step 116550, loss = 0.67 (1656.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:54:39.129518: step 116560, loss = 0.69 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:54:39.889860: step 116570, loss = 0.76 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:40.651479: step 116580, loss = 0.62 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:41.415629: step 116590, loss = 0.65 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:42.179469: step 116600, loss = 0.48 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:42.941213: step 116610, loss = 0.78 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:43.697258: step 116620, loss = 0.75 (1693.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:44.462595: step 116630, loss = 0.76 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:54:45.229082: step 116640, loss = 0.77 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:54:45.998801: step 116650, loss = 0.71 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:54:46.764508: step 116660, loss = 0.74 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:54:47.530692: step 116670, loss = 0.67 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:54:48.288039: step 116680, loss = 0.77 (1690.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:49.052445: step 116690, loss = 0.84 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:49.819420: step 116700, loss = 0.84 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:54:50.576295: step 116710, loss = 0.73 (1691.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:51.347693: step 116720, loss = 0.68 (1659.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:54:52.107143: step 116730, loss = 0.72 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:52.871924: step 116740, loss = 0.85 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:53.637330: step 116750, loss = 0.70 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:54:54.403865: step 116760, loss = 0.79 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:54:55.168869: step 116770, loss = 0.71 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:54:55.927311: step 116780, loss = 0.66 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:56.691710: step 116790, loss = 0.75 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:57.463689: step 116800, loss = 0.72 (1658.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:54:58.228676: step 116810, loss = 0.80 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:54:59.102407: step 116820, loss = 0.66 (1465.0 examples/sec; 0.087 sec/batch)
2017-05-05 19:54:59.759520: step 116830, loss = 0.71 (1947.9 examples/sec; 0.066 sec/batch)
2017-05-05 19:55:00.523215: step 116840, loss = 0.69 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:01.287537: step 116850, loss = 0.84 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:02.059166: step 116860, loss = 0.71 (1658.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:02.831945: step 116870, loss = 0.77 (1656.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:03.586158: step 116880, loss = 0.91 (1697.1 examples/sec; 0.075 sec/batch)
2017-05-05 19:55:04.349392: step 116890, loss = 0.77 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:05.112166: step 116900, loss = 0.62 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:05.877391: step 116910, loss = 0.73 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:06.652097: step 116920, loss = 0.68 (1652.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:07.405824: step 116930, loss = 0.80 (1698.2 examples/sec; 0.075 sec/batch)
2017-05-05 19:55:08.172585: step 116940, loss = 0.66 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:08.941407: step 116950, loss = 0.88 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:09.706316: step 116960, loss = 0.74 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:10.475765: step 116970, loss = 0.67 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:11.233751: step 116980, loss = 0.70 (1688.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:11.991965: step 116990, loss = 0.79 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:12.754666: step 117000, loss = 0.67 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:13.524226: step 117010, loss = 0.80 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:14.287738: step 117020, loss = 0.87 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:15.057815: step 117030, loss = 0.56 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:15.820927: step 117040, loss = 0.66 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:16.587920: step 117050, loss = 0.69 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:17.359003: step 117060, loss = 0.79 (1660.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:18.126198: step 117070, loss = 0.59 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:18.894785: step 117080, loss = 0.69 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:19.656074: step 117090, loss = 0.76 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:20.419930: step 117100, loss = 0.90 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:21.180766: step 117110, loss = 0.62 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:21.947535: step 117120, loss = 0.69 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:22.713610: step 117130, loss = 0.68 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:23.472459: step 117140, loss = 0.76 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:24.235588: step 117150, loss = 0.75 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:25.003627: step 117160, loss = 0.64 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:25.761788: step 117170, loss = 0.67 (1688.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:26.524466: step 117180, loss = 0.71 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:27.289050: step 117190, loss = 0.79 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:28.045346: step 117200, loss = 0.75 (1692.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:28.810088: step 117210, loss = 0.79 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:29.575427: step 117220, loss = 0.91 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:30.343741: step 117230, loss = 0.70 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:31.101317: step 117240, loss = 0.69 (1689.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:31.858053: step 117250, loss = 0.85 (1691.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:32.618758: step 117260, loss = 0.63 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:33.382922: step 117270, loss = 0.72 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:34.148557: step 117280, loss = 0.74 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:34.912875: step 117290, loss = 0.88 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:35.667724: step 117300, loss = 0.62 (1695.7 examples/sec; 0.075 sec/batch)
2017-05-05 19:55:36.430804: step 117310, loss = 0.70 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:37.195545: step 117320, loss = 0.68 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:37.963322: step 117330, loss = 0.72 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:38.726989: step 117340, loss = 0.81 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:39.487120: step 117350, loss = 0.81 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:40.254773: step 117360, loss = 0.63 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:41.018481: step 117370, loss = 0.62 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:41.784768: step 117380, loss = 0.77 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:42.551350: step 117390, loss = 0.78 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:43.319140: step 117400, loss = 0.59 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:44.066931: step 117410, loss = 0.74 (1711.7 examples/sec; 0.075 sec/batch)
2017-05-05 19:55:44.830816: step 117420, loss = 0.62 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:45.600867: step 117430, loss = 0.70 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:46.358209: step 117440, loss = 0.81 (1690.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:47.125644: step 117450, loss = 0.67 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:47.881795: step 117460, loss = 0.86 (1692.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:48.646316: step 117470, loss = 0.74 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:49.411487: step 117480, loss = 0.69 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:50.174275: step 117490, loss = 0.77 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:50.938572: step 117500, loss = 0.65 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:51.691507: step 117510, loss = 0.66 (1700.0 examples/sec; 0.075 sec/batch)
2017-05-05 19:55:52.461150: step 117520, loss = 0.69 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:53.224402: step 117530, loss = 0.79 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:53.990431: step 117540, loss = 0.73 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:54.754578: step 117550, loss = 0.69 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:55.517072: step 117560, loss = 0.69 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:55:56.287629: step 117570, loss = 0.63 (1661.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:57.053115: step 117580, loss = 0.56 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:57.818268: step 117590, loss = 0.60 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:58.586645: step 117600, loss = 0.72 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:55:59.346942: step 117610, loss = 0.62 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:00.110649: step 117620, loss = 0.85 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:00.869812: step 117630, loss = 0.68 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:01.632643: step 117640, loss = 0.67 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:02.395579: step 117650, loss = 0.74 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:03.160669: step 117660, loss = 0.67 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:56:03.917249: step 117670, loss = 0.76 (1691.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:04.679655: step 117680, loss = 0.62 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:05.441472: step 117690, loss = 0.78 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:06.236599: step 117700, loss = 0.54 (1609.8 examples/sec; 0.080 sec/batch)
2017-05-05 19:56:07.001043: step 117710, loss = 0.76 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:07.755724: step 117720, loss = 0.87 (1696.1 examples/sec; 0.075 sec/batch)
2017-05-05 19:56:08.518720: step 117730, loss = 0.76 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:09.283814: step 117740, loss = 0.73 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:56:10.048763: step 117750, loss = 0.91 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:10.808340: step 117760, loss = 0.76 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:11.568296: step 117770, loss = 0.68 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:12.324609: step 117780, loss = 1.01 (1692.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:13.089444: step 117790, loss = 0.73 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:13.853056: step 117800, loss = 0.67 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:14.713494: step 117810, loss = 0.79 (1487.6 examples/sec; 0.086 sec/batch)
2017-05-05 19:56:15.383052: step 117820, loss = 0.64 (1911.7 examples/sec; 0.067 sec/batch)
2017-05-05 19:56:16.136584: step 117830, loss = 0.60 (1698.7 examples/sec; 0.075 sec/batch)
2017-05-05 19:56:16.895731: step 117840, loss = 0.78 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:17.651566: step 117850, loss = 0.81 (1693.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:18.423596: step 117860, loss = 0.72 (1658.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:56:19.188015: step 117870, loss = 0.68 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:19.940638: step 117880, loss = 0.65 (1700.7 examples/sec; 0.075 sec/batch)
2017-05-05 19:56:20.707140: step 117890, loss = 0.69 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:56:21.469689: step 117900, loss = 0.70 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:22.233275: step 117910, loss = 0.63 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:23.002894: step 117920, loss = 0.73 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:56:23.758456: step 117930, loss = 0.80 (1694.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:24.528904: step 117940, loss = 0.88 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:56:25.297611: step 117950, loss = 0.64 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:56:26.059647: step 117960, loss = 0.80 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:26.820179: step 117970, loss = 0.75 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:27.580693: step 117980, loss = 0.80 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:28.343762: step 117990, loss = 0.73 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:29.111213: step 118000, loss = 0.61 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:56:29.875007: step 118010, loss = 0.62 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:30.636088: step 118020, loss = 0.69 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:31.398469: step 118030, loss = 0.64 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:32.157515: step 118040, loss = 0.75 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:32.918213: step 118050, loss = 0.68 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:33.677729: step 118060, loss = 0.77 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:34.443852: step 118070, loss = 0.76 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:56:35.211406: step 118080, loss = 0.84 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:56:35.960350: step 118090, loss = 0.80 (1709.1 examples/sec; 0.075 sec/batch)
2017-05-05 19:56:36.723684: step 118100, loss = 0.72 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:37.492701: step 118110, loss = 0.76 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:56:38.259544: step 118120, loss = 0.79 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:56:39.024173: step 118130, loss = 0.78 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:39.779239: step 118140, loss = 0.71 (1695.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:40.548503: step 118150, loss = 0.81 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:56:41.311596: step 118160, loss = 0.88 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:42.085848: step 118170, loss = 0.64 (1653.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:56:42.854396: step 118180, loss = 0.66 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:56:43.613609: step 118190, loss = 0.71 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:44.380667: step 118200, loss = 0.79 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:56:45.153551: step 118210, loss = 0.63 (1656.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:56:45.912804: step 118220, loss = 0.76 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:46.674394: step 118230, loss = 0.69 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:47.438258: step 118240, loss = 0.79 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:48.196295: step 118250, loss = 0.79 (1688.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:48.967999: step 118260, loss = 0.77 (1658.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:56:49.732471: step 118270, loss = 0.68 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:50.497065: step 118280, loss = 0.57 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:51.263080: step 118290, loss = 0.75 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:56:52.025516: step 118300, loss = 0.65 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:52.787436: step 118310, loss = 0.72 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:53.550746: step 118320, loss = 0.80 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:54.314155: step 118330, loss = 0.78 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:55.082944: step 118340, loss = 0.70 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:56:55.837602: step 118350, loss = 0.74 (1696.1 examples/sec; 0.075 sec/batch)
2017-05-05 19:56:56.601149: step 118360, loss = 0.70 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:57.364803: step 118370, loss = 0.70 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:58.125234: step 118380, loss = 0.63 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:58.884838: step 118390, loss = 0.75 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:56:59.641140: step 118400, loss = 0.75 (1692.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:00.406649: step 118410, loss = 0.67 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:57:01.165787: step 118420, loss = 0.71 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:01.927549: step 118430, loss = 0.67 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:02.693902: step 118440, loss = 0.71 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:57:03.451636: step 118450, loss = 0.86 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:04.212409: step 118460, loss = 0.63 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:04.973981: step 118470, loss = 0.69 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:05.737630: step 118480, loss = 0.79 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:06.500326: step 118490, loss = 0.68 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:07.261966: step 118500, loss = 0.91 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:08.020286: step 118510, loss = 0.78 (1687.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:08.788467: step 118520, loss = 0.78 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:57:09.555151: step 118530, loss = 0.84 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:57:10.317760: step 118540, loss = 0.72 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:11.083936: step 118550, loss = 0.82 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:57:11.843109: step 118560, loss = 0.60 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:12.604061: step 118570, loss = 0.82 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:13.368423: step 118580, loss = 0.71 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:14.134076: step 118590, loss = 0.71 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:57:14.902416: step 118600, loss = 0.71 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:57:15.665338: step 118610, loss = 0.67 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:16.430293: step 118620, loss = 0.74 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:17.196410: step 118630, loss = 0.70 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:57:17.967677: step 118640, loss = 0.90 (1659.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:57:18.736376: step 118650, loss = 0.66 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:57:19.491740: step 118660, loss = 0.60 (1694.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:20.253740: step 118670, loss = 0.75 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:21.021933: step 118680, loss = 0.78 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:57:21.784811: step 118690, loss = 0.74 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:22.549179: step 118700, loss = 0.65 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:23.319402: step 118710, loss = 0.75 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:57:24.072673: step 118720, loss = 0.79 (1699.3 examples/sec; 0.075 sec/batch)
2017-05-05 19:57:24.834438: step 118730, loss = 0.74 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:25.598091: step 118740, loss = 0.75 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:26.365787: step 118750, loss = 0.79 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:57:27.137715: step 118760, loss = 0.78 (1658.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:57:27.894394: step 118770, loss = 0.73 (1691.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:28.653768: step 118780, loss = 0.77 (1685.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:29.423809: step 118790, loss = 0.74 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:57:30.285721: step 118800, loss = 0.63 (1485.1 examples/sec; 0.086 sec/batch)
2017-05-05 19:57:30.951506: step 118810, loss = 0.61 (1922.5 examples/sec; 0.067 sec/batch)
2017-05-05 19:57:31.706740: step 118820, loss = 0.62 (1694.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:32.466391: step 118830, loss = 0.87 (1685.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:33.231560: step 118840, loss = 0.67 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:57:34.006309: step 118850, loss = 0.72 (1652.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:57:34.771620: step 118860, loss = 0.66 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:57:35.534754: step 118870, loss = 0.65 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:36.293932: step 118880, loss = 0.82 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:37.053612: step 118890, loss = 0.68 (1684.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:37.816817: step 118900, loss = 0.74 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:38.585638: step 118910, loss = 0.91 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:57:39.348169: step 118920, loss = 0.80 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:40.110729: step 118930, loss = 0.74 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:40.871838: step 118940, loss = 0.59 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:41.636942: step 118950, loss = 0.64 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:57:42.404609: step 118960, loss = 0.82 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:57:43.168152: step 118970, loss = 0.67 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:43.918711: step 118980, loss = 0.70 (1705.4 examples/sec; 0.075 sec/batch)
2017-05-05 19:57:44.680570: step 118990, loss = 0.77 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:45.444711: step 119000, loss = 0.71 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:46.207115: step 119010, loss = 0.70 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:46.977813: step 119020, loss = 0.71 (1660.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:57:47.731904: step 119030, loss = 0.67 (1697.4 examples/sec; 0.075 sec/batch)
2017-05-05 19:57:48.503584: step 119040, loss = 0.71 (1658.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:57:49.264676: step 119050, loss = 0.77 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:50.037551: step 119060, loss = 0.67 (1656.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:57:50.798153: step 119070, loss = 0.89 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:51.560026: step 119080, loss = 0.81 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:52.324261: step 119090, loss = 0.71 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:53.090278: step 119100, loss = 0.80 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:57:53.850604: step 119110, loss = 0.62 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:54.616645: step 119120, loss = 0.71 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:57:55.379440: step 119130, loss = 0.71 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:56.142631: step 119140, loss = 0.73 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:56.903659: step 119150, loss = 0.66 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:57.663570: step 119160, loss = 0.70 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:58.435346: step 119170, loss = 0.65 (1658.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:57:59.194397: step 119180, loss = 0.80 (1686.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:57:59.952820: step 119190, loss = 0.74 (1687.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:00.722754: step 119200, loss = 0.76 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:58:01.513463: step 119210, loss = 0.67 (1618.8 examples/sec; 0.079 sec/batch)
2017-05-05 19:58:02.277008: step 119220, loss = 0.71 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:03.039863: step 119230, loss = 0.67 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:03.797976: step 119240, loss = 0.71 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:04.559004: step 119250, loss = 0.70 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:05.326210: step 119260, loss = 0.68 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:58:06.092005: step 119270, loss = 0.80 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:58:06.847897: step 119280, loss = 0.68 (1693.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:07.603505: step 119290, loss = 0.73 (1694.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:08.367533: step 119300, loss = 0.65 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:09.131380: step 119310, loss = 0.84 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:09.900176: step 119320, loss = 0.78 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:58:10.670569: step 119330, loss = 0.76 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:58:11.427996: step 119340, loss = 0.83 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:12.191857: step 119350, loss = 0.62 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:12.954557: step 119360, loss = 0.68 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:13.724672: step 119370, loss = 0.76 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:58:14.484089: step 119380, loss = 0.68 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:15.256161: step 119390, loss = 0.69 (1657.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:58:16.010497: step 119400, loss = 0.79 (1696.9 examples/sec; 0.075 sec/batch)
2017-05-05 19:58:16.772276: step 119410, loss = 0.86 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:17.535604: step 119420, loss = 0.81 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:18.301724: step 119430, loss = 0.83 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:58:19.067338: step 119440, loss = 0.66 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:58:19.832060: step 119450, loss = 0.69 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:20.593074: step 119460, loss = 0.72 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:21.350601: step 119470, loss = 0.83 (1689.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:22.119269: step 119480, loss = 0.83 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:58:22.888286: step 119490, loss = 0.64 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:58:23.647632: step 119500, loss = 0.72 (1685.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:24.419746: step 119510, loss = 0.90 (1657.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:58:25.181436: step 119520, loss = 0.65 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:25.953193: step 119530, loss = 0.77 (1658.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:58:26.716474: step 119540, loss = 0.67 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:27.481469: step 119550, loss = 0.84 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:28.243711: step 119560, loss = 0.77 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:29.008925: step 119570, loss = 0.90 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:58:29.777789: step 119580, loss = 0.83 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:58:30.540934: step 119590, loss = 0.73 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:31.301278: step 119600, loss = 0.70 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:32.057331: step 119610, loss = 0.70 (1693.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:32.821009: step 119620, loss = 0.69 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:33.599061: step 119630, loss = 0.63 (1645.1 examples/sec; 0.078 sec/batch)
2017-05-05 19:58:34.370457: step 119640, loss = 0.62 (1659.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:58:35.134854: step 119650, loss = 0.76 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:35.886515: step 119660, loss = 0.73 (1702.9 examples/sec; 0.075 sec/batch)
2017-05-05 19:58:36.651395: step 119670, loss = 0.74 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:37.415791: step 119680, loss = 0.70 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:38.181872: step 119690, loss = 0.66 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:58:38.945219: step 119700, loss = 0.66 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:39.699792: step 119710, loss = 0.66 (1696.3 examples/sec; 0.075 sec/batch)
2017-05-05 19:58:40.458580: step 119720, loss = 0.78 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:41.223731: step 119730, loss = 0.83 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:58:41.989307: step 119740, loss = 0.86 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:58:42.752269: step 119750, loss = 0.68 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:43.514753: step 119760, loss = 0.71 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:44.273759: step 119770, loss = 0.69 (1686.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:45.036722: step 119780, loss = 0.70 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:45.903653: step 119790, loss = 0.68 (1476.5 examples/sec; 0.087 sec/batch)
2017-05-05 19:58:46.564975: step 119800, loss = 0.77 (1935.5 examples/sec; 0.066 sec/batch)
2017-05-05 19:58:47.326086: step 119810, loss = 0.66 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:48.082411: step 119820, loss = 0.85 (1692.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:48.849753: step 119830, loss = 0.70 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:58:49.613557: step 119840, loss = 0.71 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:50.377163: step 119850, loss = 0.75 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:51.144141: step 119860, loss = 0.71 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:58:51.897116: step 119870, loss = 0.77 (1699.9 examples/sec; 0.075 sec/batch)
2017-05-05 19:58:52.663285: step 119880, loss = 0.79 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:58:53.430038: step 119890, loss = 0.70 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:58:54.198178: step 119900, loss = 0.57 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:58:54.960109: step 119910, loss = 0.82 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:55.716893: step 119920, loss = 0.63 (1691.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:56.480261: step 119930, loss = 0.78 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:57.243104: step 119940, loss = 0.72 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:58.003310: step 119950, loss = 0.84 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:58:58.768985: step 119960, loss = 0.76 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:58:59.525434: step 119970, loss = 0.82 (1692.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:00.290871: step 119980, loss = 0.91 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:01.046399: step 119990, loss = 0.63 (1694.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:01.812001: step 120000, loss = 0.71 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:02.579153: step 120010, loss = 0.62 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:03.343318: step 120020, loss = 0.72 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:04.100515: step 120030, loss = 0.67 (1690.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:04.867091: step 120040, loss = 0.91 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:05.627631: step 120050, loss = 0.67 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:06.398062: step 120060, loss = 0.55 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:07.161596: step 120070, loss = 0.65 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:07.913639: step 120080, loss = 0.72 (1702.0 examples/sec; 0.075 sec/batch)
2017-05-05 19:59:08.684889: step 120090, loss = 0.74 (1659.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:09.451393: step 120100, loss = 0.61 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:10.215398: step 120110, loss = 0.72 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:10.976786: step 120120, loss = 0.80 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:11.736395: step 120130, loss = 0.62 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:12.496565: step 120140, loss = 0.71 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:13.265102: step 120150, loss = 0.81 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:14.031892: step 120160, loss = 0.66 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:14.806467: step 120170, loss = 0.62 (1652.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:15.567988: step 120180, loss = 0.64 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:16.337226: step 120190, loss = 0.76 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:17.102235: step 120200, loss = 0.68 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:17.866987: step 120210, loss = 0.77 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:18.637417: step 120220, loss = 0.78 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:19.399350: step 120230, loss = 0.76 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:20.159819: step 120240, loss = 0.59 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:20.928300: step 120250, loss = 0.72 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:21.687104: step 120260, loss = 0.76 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:22.451417: step 120270, loss = 0.61 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:23.220141: step 120280, loss = 0.58 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:23.967811: step 120290, loss = 0.73 (1712.0 examples/sec; 0.075 sec/batch)
2017-05-05 19:59:24.742100: step 120300, loss = 0.81 (1653.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:25.507768: step 120310, loss = 0.68 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:26.269557: step 120320, loss = 0.91 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:27.030700: step 120330, loss = 0.70 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:27.788149: step 120340, loss = 0.71 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:28.552073: step 120350, loss = 0.64 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:29.319021: step 120360, loss = 0.73 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:30.084084: step 120370, loss = 0.68 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:30.847117: step 120380, loss = 0.73 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:31.604419: step 120390, loss = 0.68 (1690.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:32.369119: step 120400, loss = 0.73 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:33.132819: step 120410, loss = 0.64 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:33.898221: step 120420, loss = 0.85 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:34.664570: step 120430, loss = 0.93 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:35.424703: step 120440, loss = 0.72 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:36.188653: step 120450, loss = 0.84 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:36.948438: step 120460, loss = 0.73 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:37.720736: step 120470, loss = 0.72 (1657.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:38.495885: step 120480, loss = 0.64 (1651.3 examples/sec; 0.078 sec/batch)
2017-05-05 19:59:39.267094: step 120490, loss = 0.64 (1659.7 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:40.025425: step 120500, loss = 0.68 (1687.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:40.789791: step 120510, loss = 0.71 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:41.554334: step 120520, loss = 0.67 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:42.322408: step 120530, loss = 0.61 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:43.093036: step 120540, loss = 0.76 (1661.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:43.845125: step 120550, loss = 0.73 (1701.9 examples/sec; 0.075 sec/batch)
2017-05-05 19:59:44.618659: step 120560, loss = 0.67 (1654.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:45.380608: step 120570, loss = 0.73 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:46.146183: step 120580, loss = 0.83 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:46.913923: step 120590, loss = 0.63 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:47.666321: step 120600, loss = 0.75 (1701.2 examples/sec; 0.075 sec/batch)
2017-05-05 19:59:48.429098: step 120610, loss = 0.78 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:49.190882: step 120620, loss = 0.67 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:49.953536: step 120630, loss = 0.79 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:50.716693: step 120640, loss = 0.76 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:51.478087: step 120650, loss = 0.71 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:52.235152: step 120660, loss = 0.77 (1690.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:52.999028: step 120670, loss = 0.79 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:53.766654: step 120680, loss = 0.75 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:54.532493: step 120690, loss = 0.75 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:55.301520: step 120700, loss = 0.78 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:56.055163: step 120710, loss = 0.62 (1698.4 examples/sec; 0.075 sec/batch)
2017-05-05 19:59:56.822620: step 120720, loss = 0.69 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:57.590700: step 120730, loss = 0.71 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:58.358069: step 120740, loss = 0.89 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 19:59:59.117269: step 120750, loss = 0.66 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 19:59:59.876502: step 120760, loss = 0.64 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:00.637829: step 120770, loss = 0.69 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:01.501278: step 120780, loss = 0.60 (1482.4 examples/sec; 0.086 sec/batch)
2017-05-05 20:00:02.170060: step 120790, loss = 0.76 (1913.9 examples/sec; 0.067 sec/batch)
2017-05-05 20:00:02.932159: step 120800, loss = 0.67 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:03.688809: step 120810, loss = 0.72 (1691.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:04.455680: step 120820, loss = 0.63 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:00:05.216728: step 120830, loss = 0.72 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:05.982520: step 120840, loss = 0.58 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:00:06.750411: step 120850, loss = 0.78 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:00:07.510548: step 120860, loss = 0.71 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:08.266160: step 120870, loss = 0.70 (1694.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:09.031213: step 120880, loss = 0.68 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:00:09.795175: step 120890, loss = 0.80 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:10.556075: step 120900, loss = 0.83 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:11.322580: step 120910, loss = 0.68 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:00:12.077174: step 120920, loss = 0.71 (1696.3 examples/sec; 0.075 sec/batch)
2017-05-05 20:00:12.846823: step 120930, loss = 0.76 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:00:13.610436: step 120940, loss = 0.76 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:14.373674: step 120950, loss = 0.78 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:15.142075: step 120960, loss = 0.62 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:00:15.893588: step 120970, loss = 0.80 (1703.2 examples/sec; 0.075 sec/batch)
2017-05-05 20:00:16.659502: step 120980, loss = 0.72 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:00:17.429620: step 120990, loss = 0.72 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:00:18.195284: step 121000, loss = 0.77 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:00:18.966051: step 121010, loss = 1.04 (1660.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:00:19.721405: step 121020, loss = 0.78 (1694.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:20.488197: step 121030, loss = 0.71 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:00:21.252138: step 121040, loss = 0.71 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:22.013323: step 121050, loss = 0.72 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:22.782253: step 121060, loss = 0.75 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:00:23.539428: step 121070, loss = 0.81 (1690.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:24.297856: step 121080, loss = 0.70 (1687.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:25.062334: step 121090, loss = 0.88 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:25.827220: step 121100, loss = 0.60 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:26.597853: step 121110, loss = 0.71 (1661.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:00:27.360227: step 121120, loss = 0.64 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:28.116492: step 121130, loss = 0.79 (1692.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:28.881296: step 121140, loss = 0.73 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:29.644289: step 121150, loss = 0.75 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:30.409136: step 121160, loss = 0.71 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:31.173767: step 121170, loss = 0.69 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:31.933605: step 121180, loss = 0.69 (1684.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:32.696588: step 121190, loss = 0.70 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:33.459477: step 121200, loss = 0.66 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:34.226489: step 121210, loss = 0.60 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:00:34.993509: step 121220, loss = 0.73 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:00:35.748165: step 121230, loss = 0.69 (1696.1 examples/sec; 0.075 sec/batch)
2017-05-05 20:00:36.515230: step 121240, loss = 0.72 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:00:37.282487: step 121250, loss = 0.75 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:00:38.048984: step 121260, loss = 0.72 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:00:38.816163: step 121270, loss = 0.89 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:00:39.568740: step 121280, loss = 0.93 (1700.8 examples/sec; 0.075 sec/batch)
2017-05-05 20:00:40.326247: step 121290, loss = 0.73 (1689.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:41.092945: step 121300, loss = 0.75 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:00:41.860418: step 121310, loss = 0.72 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:00:42.631401: step 121320, loss = 0.62 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:00:43.391947: step 121330, loss = 0.61 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:44.170151: step 121340, loss = 0.77 (1644.8 examples/sec; 0.078 sec/batch)
2017-05-05 20:00:44.938301: step 121350, loss = 0.71 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:00:45.697272: step 121360, loss = 0.65 (1686.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:46.465868: step 121370, loss = 0.64 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:00:47.237803: step 121380, loss = 0.68 (1658.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:00:47.993269: step 121390, loss = 0.89 (1694.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:48.761290: step 121400, loss = 0.66 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:00:49.526689: step 121410, loss = 0.74 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:00:50.286227: step 121420, loss = 0.82 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:51.052152: step 121430, loss = 0.80 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:00:51.806871: step 121440, loss = 0.60 (1696.0 examples/sec; 0.075 sec/batch)
2017-05-05 20:00:52.569241: step 121450, loss = 0.71 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:53.333755: step 121460, loss = 0.73 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:54.091047: step 121470, loss = 0.75 (1690.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:54.856228: step 121480, loss = 0.90 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:00:55.615439: step 121490, loss = 0.75 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:56.386272: step 121500, loss = 0.79 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:00:57.150608: step 121510, loss = 0.70 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:57.910857: step 121520, loss = 0.89 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:00:58.676210: step 121530, loss = 0.78 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:00:59.443528: step 121540, loss = 0.98 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:00.202756: step 121550, loss = 0.60 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:00.964537: step 121560, loss = 0.67 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:01.741007: step 121570, loss = 1.06 (1648.5 examples/sec; 0.078 sec/batch)
2017-05-05 20:01:02.505301: step 121580, loss = 0.75 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:03.272240: step 121590, loss = 0.74 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:04.036242: step 121600, loss = 0.69 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:04.792508: step 121610, loss = 0.82 (1692.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:05.551350: step 121620, loss = 0.66 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:06.315236: step 121630, loss = 0.76 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:07.077604: step 121640, loss = 0.69 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:07.830547: step 121650, loss = 0.73 (1700.0 examples/sec; 0.075 sec/batch)
2017-05-05 20:01:08.599263: step 121660, loss = 0.71 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:09.362624: step 121670, loss = 0.64 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:10.127434: step 121680, loss = 0.82 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:10.886904: step 121690, loss = 0.61 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:11.641403: step 121700, loss = 0.78 (1696.5 examples/sec; 0.075 sec/batch)
2017-05-05 20:01:12.407164: step 121710, loss = 0.77 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:13.169160: step 121720, loss = 0.76 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:13.937121: step 121730, loss = 0.70 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:14.701971: step 121740, loss = 0.65 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:15.470647: step 121750, loss = 0.79 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:16.232953: step 121760, loss = 0.61 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:17.093541: step 121770, loss = 0.76 (1487.4 examples/sec; 0.086 sec/batch)
2017-05-05 20:01:17.763041: step 121780, loss = 0.70 (1911.9 examples/sec; 0.067 sec/batch)
2017-05-05 20:01:18.529509: step 121790, loss = 0.80 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:19.291733: step 121800, loss = 0.73 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:20.051099: step 121810, loss = 0.66 (1685.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:20.812973: step 121820, loss = 0.77 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:21.578040: step 121830, loss = 0.67 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:22.346586: step 121840, loss = 0.71 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:23.110815: step 121850, loss = 0.65 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:23.870897: step 121860, loss = 0.69 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:24.645376: step 121870, loss = 0.69 (1652.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:25.414883: step 121880, loss = 0.75 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:26.183864: step 121890, loss = 0.67 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:26.956719: step 121900, loss = 0.57 (1656.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:27.708708: step 121910, loss = 0.75 (1702.2 examples/sec; 0.075 sec/batch)
2017-05-05 20:01:28.475770: step 121920, loss = 0.70 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:29.241061: step 121930, loss = 0.62 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:30.003669: step 121940, loss = 0.65 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:30.765352: step 121950, loss = 0.82 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:31.524398: step 121960, loss = 0.52 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:32.290962: step 121970, loss = 0.68 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:33.055200: step 121980, loss = 0.62 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:33.819130: step 121990, loss = 0.69 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:34.584976: step 122000, loss = 0.70 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:35.346758: step 122010, loss = 0.62 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:36.107617: step 122020, loss = 0.85 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:36.878054: step 122030, loss = 0.68 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:37.641350: step 122040, loss = 0.77 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:38.414320: step 122050, loss = 0.71 (1656.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:39.178126: step 122060, loss = 0.81 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:39.934846: step 122070, loss = 0.70 (1691.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:40.695778: step 122080, loss = 0.60 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:41.459794: step 122090, loss = 0.80 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:42.221154: step 122100, loss = 0.63 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:42.988924: step 122110, loss = 0.76 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:43.749199: step 122120, loss = 0.70 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:44.514205: step 122130, loss = 0.70 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:45.284243: step 122140, loss = 0.65 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:46.049550: step 122150, loss = 0.65 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:46.816762: step 122160, loss = 0.69 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:47.577225: step 122170, loss = 0.76 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:48.348438: step 122180, loss = 0.81 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:49.115565: step 122190, loss = 0.91 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:49.884262: step 122200, loss = 0.72 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:50.645566: step 122210, loss = 0.63 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:51.409683: step 122220, loss = 0.74 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:52.169247: step 122230, loss = 0.70 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:52.936323: step 122240, loss = 0.84 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:53.701503: step 122250, loss = 0.62 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:54.464154: step 122260, loss = 0.85 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:55.235593: step 122270, loss = 0.70 (1659.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:55.992317: step 122280, loss = 0.67 (1691.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:01:56.761973: step 122290, loss = 0.59 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:57.534545: step 122300, loss = 0.61 (1656.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:58.301540: step 122310, loss = 0.78 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:59.069845: step 122320, loss = 0.75 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:01:59.831800: step 122330, loss = 0.74 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:00.597026: step 122340, loss = 0.69 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:02:01.357237: step 122350, loss = 0.74 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:02.120030: step 122360, loss = 0.76 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:02.890004: step 122370, loss = 0.75 (1662.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:02:03.647888: step 122380, loss = 0.63 (1688.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:04.409059: step 122390, loss = 0.75 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:05.172347: step 122400, loss = 0.63 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:05.929855: step 122410, loss = 0.65 (1689.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:06.697698: step 122420, loss = 0.70 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:02:07.458287: step 122430, loss = 0.75 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:08.220277: step 122440, loss = 0.76 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:08.986818: step 122450, loss = 0.64 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:02:09.758872: step 122460, loss = 0.90 (1657.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:02:10.522934: step 122470, loss = 0.87 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:11.284556: step 122480, loss = 0.92 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:12.042745: step 122490, loss = 0.78 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:12.812546: step 122500, loss = 0.78 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:02:13.571772: step 122510, loss = 0.72 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:14.337517: step 122520, loss = 0.61 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:02:15.099838: step 122530, loss = 0.59 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:15.855915: step 122540, loss = 0.69 (1692.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:16.618142: step 122550, loss = 0.74 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:17.387615: step 122560, loss = 0.75 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:02:18.152000: step 122570, loss = 0.58 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:18.919071: step 122580, loss = 0.77 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:02:19.672130: step 122590, loss = 0.79 (1699.7 examples/sec; 0.075 sec/batch)
2017-05-05 20:02:20.444754: step 122600, loss = 0.74 (1656.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:02:21.212825: step 122610, loss = 0.63 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:02:21.977190: step 122620, loss = 0.71 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:22.739021: step 122630, loss = 0.75 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:23.504569: step 122640, loss = 0.75 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:02:24.268964: step 122650, loss = 0.61 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:25.030375: step 122660, loss = 0.72 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:25.797494: step 122670, loss = 0.78 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:02:26.564425: step 122680, loss = 0.78 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:02:27.329646: step 122690, loss = 0.65 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:02:28.094311: step 122700, loss = 0.80 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:28.851874: step 122710, loss = 0.81 (1689.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:29.614157: step 122720, loss = 0.69 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:30.376378: step 122730, loss = 0.76 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:31.140536: step 122740, loss = 0.64 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:31.893285: step 122750, loss = 0.70 (1700.4 examples/sec; 0.075 sec/batch)
2017-05-05 20:02:32.756538: step 122760, loss = 0.54 (1482.8 examples/sec; 0.086 sec/batch)
2017-05-05 20:02:33.427632: step 122770, loss = 0.76 (1907.3 examples/sec; 0.067 sec/batch)
2017-05-05 20:02:34.191330: step 122780, loss = 0.64 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:34.955849: step 122790, loss = 0.58 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:35.713314: step 122800, loss = 0.80 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:36.479665: step 122810, loss = 0.69 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:02:37.247206: step 122820, loss = 0.72 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:02:38.013636: step 122830, loss = 0.83 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:02:38.776667: step 122840, loss = 0.63 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:39.541235: step 122850, loss = 0.67 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:40.295088: step 122860, loss = 0.94 (1697.9 examples/sec; 0.075 sec/batch)
2017-05-05 20:02:41.056734: step 122870, loss = 0.83 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:41.822419: step 122880, loss = 0.81 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:02:42.592003: step 122890, loss = 0.65 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:02:43.349868: step 122900, loss = 0.76 (1688.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:44.113244: step 122910, loss = 0.78 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:44.874799: step 122920, loss = 0.75 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:45.634975: step 122930, loss = 0.93 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:46.401051: step 122940, loss = 0.67 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:02:47.167616: step 122950, loss = 0.71 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:02:47.921456: step 122960, loss = 0.73 (1698.0 examples/sec; 0.075 sec/batch)
2017-05-05 20:02:48.683291: step 122970, loss = 0.65 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:49.451383: step 122980, loss = 0.65 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:02:50.230028: step 122990, loss = 0.63 (1643.9 examples/sec; 0.078 sec/batch)
2017-05-05 20:02:50.987985: step 123000, loss = 0.57 (1688.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:51.746400: step 123010, loss = 0.65 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:52.515235: step 123020, loss = 0.71 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:02:53.275115: step 123030, loss = 0.83 (1684.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:54.040957: step 123040, loss = 0.79 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:02:54.812641: step 123050, loss = 0.77 (1658.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:02:55.570058: step 123060, loss = 0.77 (1690.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:56.332699: step 123070, loss = 0.71 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:57.094452: step 123080, loss = 0.66 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:02:57.866559: step 123090, loss = 0.66 (1657.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:02:58.632633: step 123100, loss = 0.67 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:02:59.397936: step 123110, loss = 0.82 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:03:00.160427: step 123120, loss = 0.58 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:00.919741: step 123130, loss = 0.74 (1685.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:01.685305: step 123140, loss = 0.65 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:03:02.450728: step 123150, loss = 0.76 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:03:03.218402: step 123160, loss = 0.66 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:03:03.976312: step 123170, loss = 0.71 (1688.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:04.736710: step 123180, loss = 0.69 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:05.502155: step 123190, loss = 0.66 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:03:06.262246: step 123200, loss = 0.85 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:07.025285: step 123210, loss = 0.72 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:07.780087: step 123220, loss = 0.82 (1695.8 examples/sec; 0.075 sec/batch)
2017-05-05 20:03:08.542850: step 123230, loss = 0.59 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:09.313429: step 123240, loss = 0.73 (1661.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:03:10.077497: step 123250, loss = 0.62 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:10.841077: step 123260, loss = 0.73 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:11.607674: step 123270, loss = 0.67 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:03:12.368913: step 123280, loss = 0.89 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:13.139224: step 123290, loss = 0.67 (1661.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:03:13.902560: step 123300, loss = 0.67 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:14.671851: step 123310, loss = 0.79 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:03:15.431684: step 123320, loss = 0.76 (1684.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:16.198848: step 123330, loss = 0.88 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:03:16.966338: step 123340, loss = 0.60 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:03:17.736420: step 123350, loss = 0.85 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:03:18.511908: step 123360, loss = 0.67 (1650.6 examples/sec; 0.078 sec/batch)
2017-05-05 20:03:19.271123: step 123370, loss = 0.84 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:20.031029: step 123380, loss = 0.85 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:20.796404: step 123390, loss = 0.64 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:03:21.557009: step 123400, loss = 0.83 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:22.319237: step 123410, loss = 0.77 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:23.083904: step 123420, loss = 0.86 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:23.837563: step 123430, loss = 0.65 (1698.4 examples/sec; 0.075 sec/batch)
2017-05-05 20:03:24.600246: step 123440, loss = 0.85 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:25.363710: step 123450, loss = 0.75 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:26.130620: step 123460, loss = 0.63 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:03:26.894189: step 123470, loss = 0.70 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:27.656246: step 123480, loss = 0.72 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:28.416534: step 123490, loss = 0.60 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:29.179744: step 123500, loss = 0.80 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:29.942114: step 123510, loss = 0.76 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:30.702627: step 123520, loss = 0.70 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:31.468236: step 123530, loss = 0.73 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:03:32.223330: step 123540, loss = 0.76 (1695.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:32.993791: step 123550, loss = 0.82 (1661.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:03:33.759419: step 123560, loss = 0.76 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:03:34.523866: step 123570, loss = 0.86 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:35.289377: step 123580, loss = 0.75 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:03:36.050940: step 123590, loss = 0.70 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:36.816024: step 123600, loss = 0.84 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:03:37.583807: step 123610, loss = 0.68 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:03:38.345822: step 123620, loss = 0.67 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:39.116734: step 123630, loss = 0.67 (1660.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:03:39.872560: step 123640, loss = 0.81 (1693.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:40.638885: step 123650, loss = 0.62 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:03:41.400973: step 123660, loss = 0.64 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:42.162421: step 123670, loss = 0.65 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:42.932217: step 123680, loss = 0.79 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:03:43.688430: step 123690, loss = 0.81 (1692.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:44.454314: step 123700, loss = 0.66 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:03:45.219990: step 123710, loss = 0.76 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:03:45.981994: step 123720, loss = 0.69 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:46.744714: step 123730, loss = 0.66 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:47.508253: step 123740, loss = 0.82 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:48.372610: step 123750, loss = 0.80 (1480.9 examples/sec; 0.086 sec/batch)
2017-05-05 20:03:49.039179: step 123760, loss = 0.75 (1920.3 examples/sec; 0.067 sec/batch)
2017-05-05 20:03:49.803639: step 123770, loss = 0.77 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:50.559573: step 123780, loss = 0.64 (1693.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:51.321452: step 123790, loss = 0.75 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:52.080275: step 123800, loss = 0.72 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:52.845507: step 123810, loss = 0.70 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:03:53.613510: step 123820, loss = 0.75 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:03:54.380532: step 123830, loss = 0.63 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:03:55.149183: step 123840, loss = 0.77 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:03:55.906569: step 123850, loss = 0.94 (1690.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:56.671721: step 123860, loss = 0.65 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:03:57.444101: step 123870, loss = 0.59 (1657.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:03:58.206115: step 123880, loss = 0.87 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:03:58.974989: step 123890, loss = 0.77 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:03:59.733083: step 123900, loss = 0.74 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:00.494129: step 123910, loss = 0.65 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:01.257970: step 123920, loss = 0.70 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:02.027232: step 123930, loss = 0.81 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:04:02.794820: step 123940, loss = 0.72 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:04:03.565967: step 123950, loss = 0.63 (1659.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:04:04.335696: step 123960, loss = 0.66 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:04:05.111314: step 123970, loss = 0.87 (1650.3 examples/sec; 0.078 sec/batch)
2017-05-05 20:04:05.877532: step 123980, loss = 0.69 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:04:06.640208: step 123990, loss = 0.76 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:07.400717: step 124000, loss = 0.88 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:08.172277: step 124010, loss = 0.71 (1659.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:04:08.933188: step 124020, loss = 0.68 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:09.696837: step 124030, loss = 0.65 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:10.460166: step 124040, loss = 0.82 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:11.225521: step 124050, loss = 0.67 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:04:11.978003: step 124060, loss = 0.77 (1701.0 examples/sec; 0.075 sec/batch)
2017-05-05 20:04:12.743019: step 124070, loss = 0.56 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:04:13.505765: step 124080, loss = 0.77 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:14.272196: step 124090, loss = 0.87 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:04:15.034586: step 124100, loss = 0.64 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:15.791667: step 124110, loss = 0.72 (1690.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:16.561156: step 124120, loss = 0.75 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:04:17.328922: step 124130, loss = 0.62 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:04:18.104803: step 124140, loss = 0.74 (1649.7 examples/sec; 0.078 sec/batch)
2017-05-05 20:04:18.874418: step 124150, loss = 0.79 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:04:19.635420: step 124160, loss = 0.77 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:20.393718: step 124170, loss = 0.77 (1688.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:21.155946: step 124180, loss = 0.84 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:21.917403: step 124190, loss = 0.73 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:22.683952: step 124200, loss = 0.71 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:04:23.447170: step 124210, loss = 0.68 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:24.214983: step 124220, loss = 0.79 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:04:24.981812: step 124230, loss = 0.91 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:04:25.742479: step 124240, loss = 0.63 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:26.502216: step 124250, loss = 0.70 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:27.273105: step 124260, loss = 0.62 (1660.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:04:28.028154: step 124270, loss = 0.76 (1695.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:28.802963: step 124280, loss = 0.60 (1652.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:04:29.564728: step 124290, loss = 0.71 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:30.328148: step 124300, loss = 0.75 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:31.090228: step 124310, loss = 0.67 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:31.841088: step 124320, loss = 0.66 (1704.7 examples/sec; 0.075 sec/batch)
2017-05-05 20:04:32.603878: step 124330, loss = 0.72 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:33.374171: step 124340, loss = 0.70 (1661.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:04:34.142868: step 124350, loss = 0.80 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:04:34.907491: step 124360, loss = 0.93 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:35.663780: step 124370, loss = 0.82 (1692.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:36.427427: step 124380, loss = 0.69 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:37.194082: step 124390, loss = 0.73 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:04:37.958248: step 124400, loss = 0.78 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:38.727694: step 124410, loss = 0.62 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:04:39.484157: step 124420, loss = 0.89 (1692.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:40.251054: step 124430, loss = 0.91 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:04:41.009244: step 124440, loss = 0.66 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:41.770203: step 124450, loss = 0.64 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:42.542601: step 124460, loss = 0.93 (1657.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:04:43.309204: step 124470, loss = 0.69 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:04:44.071563: step 124480, loss = 0.67 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:44.836739: step 124490, loss = 0.66 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:04:45.601067: step 124500, loss = 0.77 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:46.365545: step 124510, loss = 0.75 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:47.127863: step 124520, loss = 0.69 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:47.884590: step 124530, loss = 0.73 (1691.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:48.654452: step 124540, loss = 0.82 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:04:49.422666: step 124550, loss = 0.66 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:04:50.182546: step 124560, loss = 0.69 (1684.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:50.942505: step 124570, loss = 0.83 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:51.694815: step 124580, loss = 0.74 (1701.4 examples/sec; 0.075 sec/batch)
2017-05-05 20:04:52.457215: step 124590, loss = 0.66 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:53.227498: step 124600, loss = 0.72 (1661.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:04:53.989248: step 124610, loss = 0.82 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:54.755466: step 124620, loss = 0.72 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:04:55.516089: step 124630, loss = 0.62 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:04:56.283089: step 124640, loss = 0.73 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:04:57.050489: step 124650, loss = 0.69 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:04:57.825793: step 124660, loss = 0.76 (1651.0 examples/sec; 0.078 sec/batch)
2017-05-05 20:04:58.591117: step 124670, loss = 0.79 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:04:59.351213: step 124680, loss = 0.89 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:00.109205: step 124690, loss = 0.70 (1688.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:00.872144: step 124700, loss = 0.67 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:01.636547: step 124710, loss = 0.66 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:02.411634: step 124720, loss = 0.81 (1651.4 examples/sec; 0.078 sec/batch)
2017-05-05 20:05:03.168702: step 124730, loss = 0.73 (1690.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:04.022831: step 124740, loss = 0.80 (1498.6 examples/sec; 0.085 sec/batch)
2017-05-05 20:05:04.685237: step 124750, loss = 0.80 (1932.3 examples/sec; 0.066 sec/batch)
2017-05-05 20:05:05.452592: step 124760, loss = 0.68 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:05:06.222216: step 124770, loss = 0.61 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:05:06.986157: step 124780, loss = 0.76 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:07.736530: step 124790, loss = 0.63 (1705.8 examples/sec; 0.075 sec/batch)
2017-05-05 20:05:08.499126: step 124800, loss = 0.71 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:09.267267: step 124810, loss = 0.64 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:05:10.031706: step 124820, loss = 0.74 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:10.794155: step 124830, loss = 0.69 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:11.552887: step 124840, loss = 0.73 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:12.307175: step 124850, loss = 0.48 (1697.0 examples/sec; 0.075 sec/batch)
2017-05-05 20:05:13.077208: step 124860, loss = 0.87 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:05:13.838242: step 124870, loss = 0.59 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:14.609204: step 124880, loss = 0.75 (1660.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:05:15.376437: step 124890, loss = 0.67 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:05:16.140257: step 124900, loss = 0.68 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:16.911614: step 124910, loss = 0.69 (1659.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:05:17.678089: step 124920, loss = 0.76 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:05:18.445821: step 124930, loss = 0.88 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:05:19.218561: step 124940, loss = 0.88 (1656.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:05:19.973140: step 124950, loss = 0.65 (1696.3 examples/sec; 0.075 sec/batch)
2017-05-05 20:05:20.731267: step 124960, loss = 0.75 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:21.496588: step 124970, loss = 0.80 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:05:22.262666: step 124980, loss = 0.80 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:05:23.024564: step 124990, loss = 0.63 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:23.782058: step 125000, loss = 0.58 (1689.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:24.546234: step 125010, loss = 0.76 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:25.318161: step 125020, loss = 0.70 (1658.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:05:26.075393: step 125030, loss = 0.75 (1690.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:26.841641: step 125040, loss = 0.64 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:05:27.592656: step 125050, loss = 0.88 (1704.4 examples/sec; 0.075 sec/batch)
2017-05-05 20:05:28.360235: step 125060, loss = 0.58 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:05:29.125470: step 125070, loss = 0.81 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:05:29.886278: step 125080, loss = 0.70 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:30.643941: step 125090, loss = 0.65 (1689.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:31.405595: step 125100, loss = 0.82 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:32.165347: step 125110, loss = 0.76 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:32.922403: step 125120, loss = 0.69 (1690.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:33.699149: step 125130, loss = 0.80 (1647.9 examples/sec; 0.078 sec/batch)
2017-05-05 20:05:34.470948: step 125140, loss = 0.64 (1658.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:05:35.234887: step 125150, loss = 0.59 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:35.994109: step 125160, loss = 0.74 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:36.755271: step 125170, loss = 0.75 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:37.518582: step 125180, loss = 0.83 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:38.287157: step 125190, loss = 0.71 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:05:39.049969: step 125200, loss = 0.65 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:39.810705: step 125210, loss = 0.67 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:40.573424: step 125220, loss = 0.88 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:41.340060: step 125230, loss = 0.71 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:05:42.092087: step 125240, loss = 0.66 (1702.1 examples/sec; 0.075 sec/batch)
2017-05-05 20:05:42.853866: step 125250, loss = 0.73 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:43.613837: step 125260, loss = 0.68 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:44.381331: step 125270, loss = 0.83 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:05:45.147545: step 125280, loss = 0.78 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:05:45.907616: step 125290, loss = 0.85 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:46.670827: step 125300, loss = 0.82 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:47.431669: step 125310, loss = 0.61 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:48.196532: step 125320, loss = 0.80 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:48.957522: step 125330, loss = 0.81 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:49.739154: step 125340, loss = 0.74 (1637.6 examples/sec; 0.078 sec/batch)
2017-05-05 20:05:50.504236: step 125350, loss = 0.72 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:05:51.270619: step 125360, loss = 0.75 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:05:52.030752: step 125370, loss = 0.66 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:52.801405: step 125380, loss = 0.67 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:05:53.563725: step 125390, loss = 0.62 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:54.326022: step 125400, loss = 0.70 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:05:55.091504: step 125410, loss = 0.66 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:05:55.843346: step 125420, loss = 0.86 (1702.5 examples/sec; 0.075 sec/batch)
2017-05-05 20:05:56.613008: step 125430, loss = 0.60 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:05:57.379353: step 125440, loss = 0.62 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:05:58.144573: step 125450, loss = 0.65 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:05:58.924668: step 125460, loss = 0.67 (1640.8 examples/sec; 0.078 sec/batch)
2017-05-05 20:05:59.680868: step 125470, loss = 0.76 (1692.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:00.438932: step 125480, loss = 0.66 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:01.203317: step 125490, loss = 0.77 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:01.967691: step 125500, loss = 0.72 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:02.725252: step 125510, loss = 0.63 (1689.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:03.486670: step 125520, loss = 0.79 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:04.241068: step 125530, loss = 0.89 (1696.7 examples/sec; 0.075 sec/batch)
2017-05-05 20:06:05.011702: step 125540, loss = 0.54 (1661.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:06:05.863922: step 125550, loss = 0.74 (1502.0 examples/sec; 0.085 sec/batch)
2017-05-05 20:06:06.612728: step 125560, loss = 0.69 (1709.4 examples/sec; 0.075 sec/batch)
2017-05-05 20:06:07.380044: step 125570, loss = 0.78 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:06:08.137076: step 125580, loss = 0.70 (1690.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:08.906116: step 125590, loss = 0.72 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:06:09.672479: step 125600, loss = 0.65 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:06:10.439165: step 125610, loss = 0.67 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:06:11.208984: step 125620, loss = 0.67 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:06:11.969098: step 125630, loss = 0.74 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:12.737013: step 125640, loss = 0.61 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:06:13.502687: step 125650, loss = 0.94 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:06:14.269597: step 125660, loss = 0.67 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:06:15.049893: step 125670, loss = 0.79 (1640.4 examples/sec; 0.078 sec/batch)
2017-05-05 20:06:15.809295: step 125680, loss = 0.82 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:16.568424: step 125690, loss = 0.62 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:17.339605: step 125700, loss = 0.78 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:06:18.107996: step 125710, loss = 0.75 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:06:18.873998: step 125720, loss = 0.71 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:06:19.754035: step 125730, loss = 0.82 (1454.5 examples/sec; 0.088 sec/batch)
2017-05-05 20:06:20.407982: step 125740, loss = 0.80 (1957.3 examples/sec; 0.065 sec/batch)
2017-05-05 20:06:21.173534: step 125750, loss = 0.85 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:06:21.932804: step 125760, loss = 0.69 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:22.696441: step 125770, loss = 0.78 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:23.458489: step 125780, loss = 0.69 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:24.216531: step 125790, loss = 0.68 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:24.979278: step 125800, loss = 0.73 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:25.742808: step 125810, loss = 0.86 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:26.506952: step 125820, loss = 0.57 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:27.269052: step 125830, loss = 0.75 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:28.023587: step 125840, loss = 0.67 (1696.4 examples/sec; 0.075 sec/batch)
2017-05-05 20:06:28.791581: step 125850, loss = 0.76 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:06:29.556384: step 125860, loss = 0.77 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:30.318799: step 125870, loss = 0.82 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:31.080543: step 125880, loss = 0.66 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:31.832075: step 125890, loss = 0.71 (1703.2 examples/sec; 0.075 sec/batch)
2017-05-05 20:06:32.597824: step 125900, loss = 0.70 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:06:33.360900: step 125910, loss = 0.76 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:34.127424: step 125920, loss = 0.67 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:06:34.896325: step 125930, loss = 0.86 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:06:35.654267: step 125940, loss = 0.69 (1688.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:36.418833: step 125950, loss = 0.65 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:37.183284: step 125960, loss = 0.71 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:37.947185: step 125970, loss = 0.68 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:38.715293: step 125980, loss = 0.66 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:06:39.485803: step 125990, loss = 0.66 (1661.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:06:40.239698: step 126000, loss = 0.91 (1697.8 examples/sec; 0.075 sec/batch)
2017-05-05 20:06:41.001596: step 126010, loss = 0.81 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:41.762497: step 126020, loss = 0.63 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:42.526800: step 126030, loss = 0.82 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:43.288337: step 126040, loss = 0.85 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:44.041460: step 126050, loss = 0.80 (1699.6 examples/sec; 0.075 sec/batch)
2017-05-05 20:06:44.809490: step 126060, loss = 0.68 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:06:45.577028: step 126070, loss = 0.60 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:06:46.346267: step 126080, loss = 0.69 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:06:47.115734: step 126090, loss = 0.63 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:06:47.871855: step 126100, loss = 0.81 (1692.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:48.640544: step 126110, loss = 0.79 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:06:49.404854: step 126120, loss = 0.76 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:50.167232: step 126130, loss = 0.74 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:50.925679: step 126140, loss = 0.64 (1687.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:51.682266: step 126150, loss = 0.73 (1691.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:52.443272: step 126160, loss = 0.69 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:53.205989: step 126170, loss = 0.74 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:53.970128: step 126180, loss = 0.88 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:54.733100: step 126190, loss = 0.68 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:55.492924: step 126200, loss = 0.71 (1684.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:56.252336: step 126210, loss = 0.69 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:57.008943: step 126220, loss = 0.79 (1691.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:06:57.774431: step 126230, loss = 0.68 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:06:58.542171: step 126240, loss = 0.77 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:06:59.307013: step 126250, loss = 0.68 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:00.061552: step 126260, loss = 0.60 (1696.4 examples/sec; 0.075 sec/batch)
2017-05-05 20:07:00.826661: step 126270, loss = 0.84 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:07:01.585841: step 126280, loss = 0.68 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:02.353065: step 126290, loss = 0.81 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:07:03.120451: step 126300, loss = 0.61 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:07:03.872098: step 126310, loss = 0.86 (1702.9 examples/sec; 0.075 sec/batch)
2017-05-05 20:07:04.636397: step 126320, loss = 0.77 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:05.401097: step 126330, loss = 0.59 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:06.166459: step 126340, loss = 0.53 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:07:06.931119: step 126350, loss = 0.63 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:07.688966: step 126360, loss = 0.70 (1689.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:08.456541: step 126370, loss = 0.79 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:07:09.222502: step 126380, loss = 0.71 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:07:09.992133: step 126390, loss = 0.78 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:07:10.758062: step 126400, loss = 0.72 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:07:11.516163: step 126410, loss = 0.68 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:12.279350: step 126420, loss = 0.63 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:13.038390: step 126430, loss = 0.68 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:13.802213: step 126440, loss = 0.84 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:14.569202: step 126450, loss = 0.73 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:07:15.329246: step 126460, loss = 0.72 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:16.088317: step 126470, loss = 0.68 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:16.849857: step 126480, loss = 0.69 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:17.613982: step 126490, loss = 0.57 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:18.387202: step 126500, loss = 0.77 (1655.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:07:19.155189: step 126510, loss = 0.60 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:07:19.904509: step 126520, loss = 0.75 (1708.2 examples/sec; 0.075 sec/batch)
2017-05-05 20:07:20.681484: step 126530, loss = 0.71 (1647.4 examples/sec; 0.078 sec/batch)
2017-05-05 20:07:21.449664: step 126540, loss = 0.68 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:07:22.210597: step 126550, loss = 0.75 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:22.978999: step 126560, loss = 0.75 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:07:23.738728: step 126570, loss = 0.69 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:24.503014: step 126580, loss = 0.82 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:25.265693: step 126590, loss = 0.78 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:26.032591: step 126600, loss = 0.73 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:07:26.794839: step 126610, loss = 0.75 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:27.550520: step 126620, loss = 0.79 (1693.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:28.314925: step 126630, loss = 0.61 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:29.078818: step 126640, loss = 0.74 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:29.847731: step 126650, loss = 0.70 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:07:30.615399: step 126660, loss = 0.66 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:07:31.372818: step 126670, loss = 0.62 (1690.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:32.124105: step 126680, loss = 0.79 (1703.7 examples/sec; 0.075 sec/batch)
2017-05-05 20:07:32.886432: step 126690, loss = 0.71 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:33.645695: step 126700, loss = 0.71 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:34.412271: step 126710, loss = 0.78 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:07:35.273628: step 126720, loss = 0.67 (1486.0 examples/sec; 0.086 sec/batch)
2017-05-05 20:07:35.931109: step 126730, loss = 0.76 (1946.8 examples/sec; 0.066 sec/batch)
2017-05-05 20:07:36.696463: step 126740, loss = 0.67 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:07:37.461196: step 126750, loss = 0.62 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:38.226397: step 126760, loss = 0.75 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:07:38.988127: step 126770, loss = 0.56 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:39.750977: step 126780, loss = 0.66 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:40.512853: step 126790, loss = 0.63 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:41.279588: step 126800, loss = 0.59 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:07:42.042522: step 126810, loss = 0.55 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:42.803891: step 126820, loss = 0.66 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:43.560347: step 126830, loss = 0.68 (1692.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:44.322978: step 126840, loss = 0.73 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:45.084751: step 126850, loss = 0.95 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:45.844276: step 126860, loss = 0.73 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:46.601746: step 126870, loss = 0.73 (1689.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:47.365959: step 126880, loss = 0.81 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:48.124913: step 126890, loss = 0.70 (1686.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:48.885005: step 126900, loss = 0.66 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:49.652544: step 126910, loss = 0.75 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:07:50.419654: step 126920, loss = 0.68 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:07:51.178339: step 126930, loss = 0.70 (1687.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:51.939025: step 126940, loss = 0.79 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:52.711645: step 126950, loss = 0.68 (1656.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:07:53.476654: step 126960, loss = 0.78 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:07:54.246349: step 126970, loss = 0.86 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:07:55.016735: step 126980, loss = 0.69 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:07:55.771030: step 126990, loss = 0.77 (1696.9 examples/sec; 0.075 sec/batch)
2017-05-05 20:07:56.536502: step 127000, loss = 0.68 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:07:57.310170: step 127010, loss = 0.63 (1654.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:07:58.076349: step 127020, loss = 0.78 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:07:58.839453: step 127030, loss = 0.84 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:07:59.598497: step 127040, loss = 0.74 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:00.358056: step 127050, loss = 0.93 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:01.119289: step 127060, loss = 0.62 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:01.874635: step 127070, loss = 0.70 (1694.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:02.644834: step 127080, loss = 0.68 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:08:03.408689: step 127090, loss = 0.68 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:04.161758: step 127100, loss = 0.66 (1699.7 examples/sec; 0.075 sec/batch)
2017-05-05 20:08:04.924604: step 127110, loss = 0.67 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:05.688377: step 127120, loss = 0.55 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:06.454159: step 127130, loss = 0.67 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:08:07.220090: step 127140, loss = 0.66 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:08:07.976351: step 127150, loss = 0.79 (1692.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:08.734178: step 127160, loss = 0.91 (1689.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:09.496031: step 127170, loss = 0.72 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:10.260664: step 127180, loss = 0.85 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:11.018685: step 127190, loss = 0.78 (1688.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:11.774465: step 127200, loss = 0.64 (1693.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:12.535088: step 127210, loss = 0.72 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:13.295875: step 127220, loss = 0.77 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:14.057573: step 127230, loss = 0.59 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:14.823542: step 127240, loss = 0.71 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:08:15.576640: step 127250, loss = 0.66 (1699.6 examples/sec; 0.075 sec/batch)
2017-05-05 20:08:16.340997: step 127260, loss = 0.76 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:17.103348: step 127270, loss = 0.64 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:17.870952: step 127280, loss = 0.74 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:08:18.633451: step 127290, loss = 0.54 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:19.395393: step 127300, loss = 0.73 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:20.157964: step 127310, loss = 0.67 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:20.921089: step 127320, loss = 0.70 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:21.688536: step 127330, loss = 0.83 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:08:22.442747: step 127340, loss = 0.77 (1697.1 examples/sec; 0.075 sec/batch)
2017-05-05 20:08:23.214639: step 127350, loss = 0.86 (1658.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:08:23.975309: step 127360, loss = 0.74 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:24.736200: step 127370, loss = 0.71 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:25.499280: step 127380, loss = 0.78 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:26.263597: step 127390, loss = 0.89 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:27.024496: step 127400, loss = 0.71 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:27.780175: step 127410, loss = 0.87 (1693.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:28.550349: step 127420, loss = 0.51 (1662.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:08:29.310658: step 127430, loss = 0.72 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:30.077451: step 127440, loss = 0.76 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:08:30.832568: step 127450, loss = 0.74 (1695.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:31.590322: step 127460, loss = 0.59 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:32.349843: step 127470, loss = 0.75 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:33.124149: step 127480, loss = 0.57 (1653.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:08:33.897521: step 127490, loss = 0.81 (1655.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:08:34.671854: step 127500, loss = 0.71 (1653.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:08:35.430353: step 127510, loss = 0.77 (1687.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:36.194913: step 127520, loss = 0.67 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:36.960277: step 127530, loss = 0.87 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:08:37.725195: step 127540, loss = 0.69 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:38.497931: step 127550, loss = 0.73 (1656.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:08:39.267967: step 127560, loss = 0.77 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:08:40.023989: step 127570, loss = 0.77 (1693.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:40.787175: step 127580, loss = 0.66 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:41.551544: step 127590, loss = 0.71 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:42.317431: step 127600, loss = 0.80 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:08:43.083532: step 127610, loss = 0.72 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:08:43.842696: step 127620, loss = 0.83 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:44.610869: step 127630, loss = 0.83 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:08:45.381211: step 127640, loss = 0.77 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:08:46.144000: step 127650, loss = 0.73 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:46.905748: step 127660, loss = 0.57 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:47.659488: step 127670, loss = 0.87 (1698.2 examples/sec; 0.075 sec/batch)
2017-05-05 20:08:48.425097: step 127680, loss = 0.82 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:08:49.185005: step 127690, loss = 0.81 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:49.950202: step 127700, loss = 0.80 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:08:50.811257: step 127710, loss = 0.50 (1486.6 examples/sec; 0.086 sec/batch)
2017-05-05 20:08:51.487117: step 127720, loss = 0.77 (1893.9 examples/sec; 0.068 sec/batch)
2017-05-05 20:08:52.245849: step 127730, loss = 0.71 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:53.013792: step 127740, loss = 0.61 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:08:53.780313: step 127750, loss = 0.65 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:08:54.543566: step 127760, loss = 0.62 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:55.306726: step 127770, loss = 0.61 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:56.070600: step 127780, loss = 0.77 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:56.833320: step 127790, loss = 0.80 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:08:57.599453: step 127800, loss = 0.65 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:08:58.370185: step 127810, loss = 0.67 (1660.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:08:59.136326: step 127820, loss = 0.71 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:08:59.888736: step 127830, loss = 0.67 (1701.2 examples/sec; 0.075 sec/batch)
2017-05-05 20:09:00.651226: step 127840, loss = 0.81 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:01.414180: step 127850, loss = 0.76 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:02.175014: step 127860, loss = 0.69 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:02.946183: step 127870, loss = 0.62 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:09:03.710062: step 127880, loss = 0.71 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:04.477388: step 127890, loss = 0.71 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:09:05.242392: step 127900, loss = 0.68 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:09:06.006306: step 127910, loss = 0.72 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:06.773536: step 127920, loss = 0.64 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:09:07.538701: step 127930, loss = 0.74 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:09:08.300515: step 127940, loss = 0.59 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:09.057084: step 127950, loss = 0.57 (1691.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:09.823305: step 127960, loss = 0.81 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:09:10.584766: step 127970, loss = 0.75 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:11.349730: step 127980, loss = 0.73 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:09:12.114494: step 127990, loss = 0.77 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:12.873594: step 128000, loss = 0.76 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:13.640273: step 128010, loss = 0.82 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:09:14.405743: step 128020, loss = 0.74 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:09:15.168656: step 128030, loss = 0.78 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:15.926335: step 128040, loss = 0.91 (1689.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:16.691020: step 128050, loss = 0.68 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:17.459516: step 128060, loss = 0.67 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:09:18.228282: step 128070, loss = 0.69 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:09:18.991413: step 128080, loss = 0.90 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:19.751030: step 128090, loss = 0.84 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:20.519168: step 128100, loss = 0.66 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:09:21.285480: step 128110, loss = 0.76 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:09:22.043757: step 128120, loss = 0.95 (1688.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:22.805779: step 128130, loss = 0.62 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:23.569024: step 128140, loss = 0.57 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:24.331434: step 128150, loss = 0.70 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:25.098424: step 128160, loss = 0.83 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:09:25.865466: step 128170, loss = 0.88 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:09:26.631315: step 128180, loss = 0.77 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:09:27.399557: step 128190, loss = 0.92 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:09:28.156176: step 128200, loss = 0.85 (1691.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:28.923906: step 128210, loss = 0.75 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:09:29.687231: step 128220, loss = 0.65 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:30.445781: step 128230, loss = 0.66 (1687.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:31.210659: step 128240, loss = 0.70 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:31.962415: step 128250, loss = 0.71 (1702.7 examples/sec; 0.075 sec/batch)
2017-05-05 20:09:32.722609: step 128260, loss = 0.66 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:33.488649: step 128270, loss = 0.84 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:09:34.254744: step 128280, loss = 0.84 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:09:35.016282: step 128290, loss = 0.69 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:35.770005: step 128300, loss = 0.87 (1698.2 examples/sec; 0.075 sec/batch)
2017-05-05 20:09:36.537727: step 128310, loss = 0.57 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:09:37.304959: step 128320, loss = 0.74 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:09:38.071310: step 128330, loss = 0.71 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:09:38.840425: step 128340, loss = 0.81 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:09:39.591425: step 128350, loss = 0.60 (1704.4 examples/sec; 0.075 sec/batch)
2017-05-05 20:09:40.364241: step 128360, loss = 0.72 (1656.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:09:41.116756: step 128370, loss = 0.84 (1701.0 examples/sec; 0.075 sec/batch)
2017-05-05 20:09:41.874120: step 128380, loss = 0.67 (1690.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:42.638481: step 128390, loss = 0.65 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:43.398832: step 128400, loss = 0.67 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:44.159232: step 128410, loss = 0.77 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:44.926062: step 128420, loss = 0.86 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:09:45.682515: step 128430, loss = 0.66 (1692.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:46.451218: step 128440, loss = 0.72 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:09:47.220687: step 128450, loss = 0.84 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:09:47.971111: step 128460, loss = 0.67 (1705.7 examples/sec; 0.075 sec/batch)
2017-05-05 20:09:48.734410: step 128470, loss = 0.80 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:49.497809: step 128480, loss = 0.83 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:50.260373: step 128490, loss = 0.70 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:51.023383: step 128500, loss = 0.70 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:51.782304: step 128510, loss = 0.61 (1686.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:52.541385: step 128520, loss = 0.64 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:53.311912: step 128530, loss = 0.76 (1661.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:09:54.074063: step 128540, loss = 0.65 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:54.837509: step 128550, loss = 0.84 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:55.593832: step 128560, loss = 0.78 (1692.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:56.357352: step 128570, loss = 0.61 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:57.125777: step 128580, loss = 0.65 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:09:57.888647: step 128590, loss = 0.67 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:58.653445: step 128600, loss = 0.72 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:09:59.418677: step 128610, loss = 0.70 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:10:00.168795: step 128620, loss = 0.75 (1706.4 examples/sec; 0.075 sec/batch)
2017-05-05 20:10:00.932473: step 128630, loss = 0.90 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:01.693544: step 128640, loss = 0.61 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:02.473266: step 128650, loss = 0.69 (1641.6 examples/sec; 0.078 sec/batch)
2017-05-05 20:10:03.242629: step 128660, loss = 0.74 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:10:04.004486: step 128670, loss = 0.74 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:04.769260: step 128680, loss = 0.67 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:05.532484: step 128690, loss = 0.74 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:06.389512: step 128700, loss = 0.72 (1493.5 examples/sec; 0.086 sec/batch)
2017-05-05 20:10:07.059932: step 128710, loss = 0.73 (1909.2 examples/sec; 0.067 sec/batch)
2017-05-05 20:10:07.812855: step 128720, loss = 0.63 (1700.0 examples/sec; 0.075 sec/batch)
2017-05-05 20:10:08.574112: step 128730, loss = 0.71 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:09.342259: step 128740, loss = 0.65 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:10:10.104421: step 128750, loss = 0.75 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:10.870489: step 128760, loss = 0.76 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:10:11.622959: step 128770, loss = 0.71 (1701.1 examples/sec; 0.075 sec/batch)
2017-05-05 20:10:12.387743: step 128780, loss = 0.57 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:13.149689: step 128790, loss = 0.75 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:13.913484: step 128800, loss = 0.83 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:14.685582: step 128810, loss = 0.67 (1657.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:10:15.446633: step 128820, loss = 0.82 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:16.206952: step 128830, loss = 0.56 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:16.966674: step 128840, loss = 0.69 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:17.734767: step 128850, loss = 0.72 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:10:18.496262: step 128860, loss = 0.74 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:19.261478: step 128870, loss = 0.74 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:10:20.016481: step 128880, loss = 0.60 (1695.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:20.779790: step 128890, loss = 0.75 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:21.544189: step 128900, loss = 0.78 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:22.307069: step 128910, loss = 0.86 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:23.070526: step 128920, loss = 0.81 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:23.825131: step 128930, loss = 0.68 (1696.3 examples/sec; 0.075 sec/batch)
2017-05-05 20:10:24.593877: step 128940, loss = 0.63 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:10:25.373884: step 128950, loss = 0.83 (1641.0 examples/sec; 0.078 sec/batch)
2017-05-05 20:10:26.134378: step 128960, loss = 0.72 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:26.903511: step 128970, loss = 0.78 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:10:27.659674: step 128980, loss = 0.74 (1692.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:28.423660: step 128990, loss = 0.68 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:29.191734: step 129000, loss = 0.82 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:10:29.956125: step 129010, loss = 0.76 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:30.718897: step 129020, loss = 0.72 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:31.479993: step 129030, loss = 0.80 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:32.239918: step 129040, loss = 0.69 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:32.998389: step 129050, loss = 0.68 (1687.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:33.765978: step 129060, loss = 0.54 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:10:34.534565: step 129070, loss = 0.68 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:10:35.293835: step 129080, loss = 0.66 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:36.055644: step 129090, loss = 0.66 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:36.823035: step 129100, loss = 0.75 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:10:37.589461: step 129110, loss = 0.66 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:10:38.350490: step 129120, loss = 0.87 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:39.114010: step 129130, loss = 0.72 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:39.871855: step 129140, loss = 0.62 (1689.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:40.630641: step 129150, loss = 0.79 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:41.393098: step 129160, loss = 0.78 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:42.162747: step 129170, loss = 0.62 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:10:42.925893: step 129180, loss = 0.63 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:43.685149: step 129190, loss = 0.60 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:44.456195: step 129200, loss = 0.68 (1660.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:10:45.227456: step 129210, loss = 0.63 (1659.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:10:45.999302: step 129220, loss = 0.64 (1658.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:10:46.763125: step 129230, loss = 0.71 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:47.530513: step 129240, loss = 0.71 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:10:48.294604: step 129250, loss = 0.70 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:49.053432: step 129260, loss = 0.81 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:49.823648: step 129270, loss = 0.69 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:10:50.585271: step 129280, loss = 0.84 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:51.347194: step 129290, loss = 0.61 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:52.110912: step 129300, loss = 0.63 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:52.875459: step 129310, loss = 0.74 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:53.640907: step 129320, loss = 0.65 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:10:54.403784: step 129330, loss = 0.82 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:55.167116: step 129340, loss = 0.93 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:55.925320: step 129350, loss = 0.72 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:56.688181: step 129360, loss = 0.63 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:57.454506: step 129370, loss = 0.76 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:10:58.218119: step 129380, loss = 0.79 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:10:58.983575: step 129390, loss = 0.67 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:10:59.734383: step 129400, loss = 0.67 (1704.8 examples/sec; 0.075 sec/batch)
2017-05-05 20:11:00.497221: step 129410, loss = 0.77 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:01.256312: step 129420, loss = 0.84 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:02.015062: step 129430, loss = 0.66 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:02.781865: step 129440, loss = 0.65 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:11:03.538287: step 129450, loss = 0.73 (1692.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:04.300261: step 129460, loss = 0.73 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:05.064334: step 129470, loss = 0.73 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:05.826519: step 129480, loss = 0.75 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:06.588309: step 129490, loss = 0.64 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:07.355690: step 129500, loss = 0.91 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:11:08.109609: step 129510, loss = 0.59 (1697.8 examples/sec; 0.075 sec/batch)
2017-05-05 20:11:08.875494: step 129520, loss = 0.59 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:11:09.639893: step 129530, loss = 0.69 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:10.394769: step 129540, loss = 0.59 (1695.6 examples/sec; 0.075 sec/batch)
2017-05-05 20:11:11.164554: step 129550, loss = 0.77 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:11:11.917723: step 129560, loss = 0.73 (1699.5 examples/sec; 0.075 sec/batch)
2017-05-05 20:11:12.680735: step 129570, loss = 0.86 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:13.445338: step 129580, loss = 0.77 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:14.205903: step 129590, loss = 0.69 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:14.969256: step 129600, loss = 0.84 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:15.726391: step 129610, loss = 0.90 (1690.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:16.488598: step 129620, loss = 0.70 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:17.250887: step 129630, loss = 0.69 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:18.019088: step 129640, loss = 0.66 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:11:18.787072: step 129650, loss = 0.70 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:11:19.543103: step 129660, loss = 0.72 (1693.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:20.306878: step 129670, loss = 0.85 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:21.068260: step 129680, loss = 0.67 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:21.929503: step 129690, loss = 0.72 (1486.2 examples/sec; 0.086 sec/batch)
2017-05-05 20:11:22.597931: step 129700, loss = 0.59 (1915.0 examples/sec; 0.067 sec/batch)
2017-05-05 20:11:23.360411: step 129710, loss = 0.79 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:24.119844: step 129720, loss = 0.69 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:24.887543: step 129730, loss = 0.85 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:11:25.645594: step 129740, loss = 0.77 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:26.411804: step 129750, loss = 0.74 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:11:27.182342: step 129760, loss = 0.63 (1661.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:11:27.940424: step 129770, loss = 0.67 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:28.702032: step 129780, loss = 0.82 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:29.475237: step 129790, loss = 0.63 (1655.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:11:30.236745: step 129800, loss = 0.64 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:30.997515: step 129810, loss = 0.79 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:31.748268: step 129820, loss = 0.78 (1705.0 examples/sec; 0.075 sec/batch)
2017-05-05 20:11:32.513070: step 129830, loss = 0.68 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:33.286326: step 129840, loss = 0.62 (1655.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:11:34.050951: step 129850, loss = 0.68 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:34.814149: step 129860, loss = 0.95 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:35.571523: step 129870, loss = 0.73 (1690.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:36.332230: step 129880, loss = 0.85 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:37.100871: step 129890, loss = 0.83 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:11:37.858266: step 129900, loss = 0.69 (1690.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:38.630271: step 129910, loss = 0.75 (1658.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:11:39.392340: step 129920, loss = 0.72 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:40.156594: step 129930, loss = 0.70 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:40.911046: step 129940, loss = 0.78 (1696.6 examples/sec; 0.075 sec/batch)
2017-05-05 20:11:41.682405: step 129950, loss = 0.69 (1659.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:11:42.451561: step 129960, loss = 0.84 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:11:43.212035: step 129970, loss = 0.85 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:43.963839: step 129980, loss = 0.63 (1702.6 examples/sec; 0.075 sec/batch)
2017-05-05 20:11:44.738630: step 129990, loss = 0.63 (1652.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:11:45.498107: step 130000, loss = 0.70 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:46.259940: step 130010, loss = 0.66 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:47.030105: step 130020, loss = 0.71 (1662.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:11:47.795203: step 130030, loss = 0.81 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:11:48.557138: step 130040, loss = 1.03 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:49.328092: step 130050, loss = 0.82 (1660.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:11:50.089877: step 130060, loss = 0.77 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:50.847515: step 130070, loss = 0.69 (1689.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:51.606750: step 130080, loss = 0.65 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:52.374611: step 130090, loss = 0.65 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:11:53.128959: step 130100, loss = 0.79 (1696.8 examples/sec; 0.075 sec/batch)
2017-05-05 20:11:53.895434: step 130110, loss = 0.61 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:11:54.653123: step 130120, loss = 0.63 (1689.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:55.411513: step 130130, loss = 0.65 (1687.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:56.166835: step 130140, loss = 0.61 (1694.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:56.932064: step 130150, loss = 0.75 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:11:57.703110: step 130160, loss = 0.68 (1660.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:11:58.466687: step 130170, loss = 0.61 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:11:59.239007: step 130180, loss = 0.76 (1657.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:11:59.996197: step 130190, loss = 0.74 (1690.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:00.757825: step 130200, loss = 0.74 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:01.522193: step 130210, loss = 0.63 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:02.286310: step 130220, loss = 0.65 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:03.048033: step 130230, loss = 0.80 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:03.808690: step 130240, loss = 0.69 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:04.571694: step 130250, loss = 0.65 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:05.335093: step 130260, loss = 0.63 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:06.093113: step 130270, loss = 0.65 (1688.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:06.859958: step 130280, loss = 0.68 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:12:07.623981: step 130290, loss = 0.65 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:08.389708: step 130300, loss = 0.59 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:12:09.154116: step 130310, loss = 0.96 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:09.925197: step 130320, loss = 0.70 (1660.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:12:10.686593: step 130330, loss = 0.81 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:11.447419: step 130340, loss = 0.59 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:12.210612: step 130350, loss = 0.68 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:12.974280: step 130360, loss = 0.61 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:13.742376: step 130370, loss = 0.78 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:12:14.511508: step 130380, loss = 0.76 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:12:15.276413: step 130390, loss = 0.67 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:16.029292: step 130400, loss = 0.76 (1700.1 examples/sec; 0.075 sec/batch)
2017-05-05 20:12:16.792123: step 130410, loss = 0.71 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:17.557640: step 130420, loss = 0.64 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:12:18.324012: step 130430, loss = 0.72 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:12:19.092295: step 130440, loss = 0.88 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:12:19.849157: step 130450, loss = 0.70 (1691.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:20.610615: step 130460, loss = 0.72 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:21.378272: step 130470, loss = 0.62 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:12:22.142472: step 130480, loss = 0.84 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:22.910549: step 130490, loss = 0.74 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:12:23.670720: step 130500, loss = 0.86 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:24.436406: step 130510, loss = 0.72 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:12:25.199650: step 130520, loss = 0.90 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:25.960037: step 130530, loss = 0.65 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:26.723064: step 130540, loss = 0.66 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:27.476800: step 130550, loss = 0.75 (1698.2 examples/sec; 0.075 sec/batch)
2017-05-05 20:12:28.235699: step 130560, loss = 0.76 (1686.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:29.005845: step 130570, loss = 0.67 (1662.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:12:29.776895: step 130580, loss = 0.73 (1660.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:12:30.530814: step 130590, loss = 0.77 (1697.8 examples/sec; 0.075 sec/batch)
2017-05-05 20:12:31.296444: step 130600, loss = 0.70 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:12:32.049936: step 130610, loss = 0.83 (1698.8 examples/sec; 0.075 sec/batch)
2017-05-05 20:12:32.814611: step 130620, loss = 0.75 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:33.578888: step 130630, loss = 0.60 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:34.344772: step 130640, loss = 0.65 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:12:35.109214: step 130650, loss = 0.66 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:35.869819: step 130660, loss = 0.76 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:36.632154: step 130670, loss = 0.79 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:37.499516: step 130680, loss = 0.84 (1475.7 examples/sec; 0.087 sec/batch)
2017-05-05 20:12:38.166556: step 130690, loss = 0.69 (1918.9 examples/sec; 0.067 sec/batch)
2017-05-05 20:12:38.935004: step 130700, loss = 0.69 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:12:39.690218: step 130710, loss = 0.72 (1694.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:40.454840: step 130720, loss = 0.70 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:41.215261: step 130730, loss = 0.83 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:41.976944: step 130740, loss = 0.74 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:42.749331: step 130750, loss = 0.70 (1657.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:12:43.508819: step 130760, loss = 0.67 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:44.268684: step 130770, loss = 0.78 (1684.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:45.028878: step 130780, loss = 0.84 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:45.783637: step 130790, loss = 0.82 (1695.9 examples/sec; 0.075 sec/batch)
2017-05-05 20:12:46.549652: step 130800, loss = 0.75 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:12:47.316487: step 130810, loss = 0.67 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:12:48.070117: step 130820, loss = 0.76 (1698.4 examples/sec; 0.075 sec/batch)
2017-05-05 20:12:48.835874: step 130830, loss = 0.72 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:12:49.604944: step 130840, loss = 0.72 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:12:50.370795: step 130850, loss = 0.66 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:12:51.134202: step 130860, loss = 0.79 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:51.896285: step 130870, loss = 0.91 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:52.666995: step 130880, loss = 0.79 (1660.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:12:53.434143: step 130890, loss = 0.78 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:12:54.201642: step 130900, loss = 0.63 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:12:54.971844: step 130910, loss = 0.67 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:12:55.723493: step 130920, loss = 0.60 (1702.9 examples/sec; 0.075 sec/batch)
2017-05-05 20:12:56.488428: step 130930, loss = 0.74 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:57.266525: step 130940, loss = 0.65 (1645.0 examples/sec; 0.078 sec/batch)
2017-05-05 20:12:58.030790: step 130950, loss = 0.78 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:12:58.797160: step 130960, loss = 0.80 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:12:59.557172: step 130970, loss = 0.80 (1684.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:00.320531: step 130980, loss = 0.65 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:01.082897: step 130990, loss = 0.82 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:01.850620: step 131000, loss = 0.78 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:13:02.612875: step 131010, loss = 0.74 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:03.375173: step 131020, loss = 0.66 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:04.138675: step 131030, loss = 0.65 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:04.908092: step 131040, loss = 0.91 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:13:05.668622: step 131050, loss = 0.70 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:06.438577: step 131060, loss = 0.69 (1662.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:13:07.202498: step 131070, loss = 0.77 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:07.960703: step 131080, loss = 0.60 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:08.730161: step 131090, loss = 0.86 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:13:09.495641: step 131100, loss = 0.69 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:13:10.261458: step 131110, loss = 0.66 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:13:11.021427: step 131120, loss = 0.70 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:11.774593: step 131130, loss = 0.78 (1699.5 examples/sec; 0.075 sec/batch)
2017-05-05 20:13:12.536503: step 131140, loss = 0.81 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:13.303742: step 131150, loss = 0.69 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:13:14.071409: step 131160, loss = 0.55 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:13:14.832521: step 131170, loss = 0.82 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:15.591343: step 131180, loss = 0.76 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:16.352530: step 131190, loss = 0.64 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:17.114992: step 131200, loss = 0.75 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:17.884254: step 131210, loss = 0.70 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:13:18.648541: step 131220, loss = 0.62 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:19.409559: step 131230, loss = 0.62 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:20.164271: step 131240, loss = 0.71 (1696.0 examples/sec; 0.075 sec/batch)
2017-05-05 20:13:20.920999: step 131250, loss = 0.61 (1691.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:21.680831: step 131260, loss = 0.81 (1684.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:22.447744: step 131270, loss = 0.74 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:13:23.216553: step 131280, loss = 0.62 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:13:23.975321: step 131290, loss = 0.67 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:24.740428: step 131300, loss = 0.69 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:13:25.507112: step 131310, loss = 0.63 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:13:26.274083: step 131320, loss = 0.65 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:13:27.039394: step 131330, loss = 0.70 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:13:27.799028: step 131340, loss = 0.77 (1685.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:28.557407: step 131350, loss = 0.70 (1687.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:29.329897: step 131360, loss = 0.84 (1657.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:13:30.090214: step 131370, loss = 0.65 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:30.847676: step 131380, loss = 0.72 (1689.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:31.609088: step 131390, loss = 0.67 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:32.371724: step 131400, loss = 0.73 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:33.135647: step 131410, loss = 0.76 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:33.901886: step 131420, loss = 0.76 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:13:34.665499: step 131430, loss = 0.79 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:35.423431: step 131440, loss = 0.77 (1688.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:36.185855: step 131450, loss = 0.79 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:36.954012: step 131460, loss = 0.64 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:13:37.727020: step 131470, loss = 0.76 (1655.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:13:38.503852: step 131480, loss = 0.70 (1647.7 examples/sec; 0.078 sec/batch)
2017-05-05 20:13:39.268029: step 131490, loss = 0.70 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:40.021326: step 131500, loss = 0.70 (1699.2 examples/sec; 0.075 sec/batch)
2017-05-05 20:13:40.787276: step 131510, loss = 0.75 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:13:41.553291: step 131520, loss = 0.64 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:13:42.314051: step 131530, loss = 0.62 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:43.080454: step 131540, loss = 0.79 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:13:43.835690: step 131550, loss = 0.79 (1694.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:44.597283: step 131560, loss = 0.63 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:45.359330: step 131570, loss = 0.64 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:46.124574: step 131580, loss = 0.76 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:13:46.884049: step 131590, loss = 0.88 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:47.642757: step 131600, loss = 0.72 (1687.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:48.406445: step 131610, loss = 0.73 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:49.171830: step 131620, loss = 0.79 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:13:49.934274: step 131630, loss = 0.81 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:50.696039: step 131640, loss = 0.84 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:51.457745: step 131650, loss = 0.63 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:52.220413: step 131660, loss = 0.68 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:53.084101: step 131670, loss = 0.78 (1482.0 examples/sec; 0.086 sec/batch)
2017-05-05 20:13:53.749538: step 131680, loss = 0.71 (1923.5 examples/sec; 0.067 sec/batch)
2017-05-05 20:13:54.512347: step 131690, loss = 0.60 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:55.277341: step 131700, loss = 0.64 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:56.033114: step 131710, loss = 0.69 (1693.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:56.798651: step 131720, loss = 0.79 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:13:57.566812: step 131730, loss = 0.72 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:13:58.327480: step 131740, loss = 0.67 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:13:59.095123: step 131750, loss = 0.69 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:13:59.853592: step 131760, loss = 0.66 (1687.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:00.614895: step 131770, loss = 0.84 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:01.377168: step 131780, loss = 0.75 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:02.138672: step 131790, loss = 0.69 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:02.906317: step 131800, loss = 0.93 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:14:03.664457: step 131810, loss = 0.74 (1688.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:04.427590: step 131820, loss = 0.80 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:05.194134: step 131830, loss = 0.66 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:14:05.958115: step 131840, loss = 0.52 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:06.723257: step 131850, loss = 0.57 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:14:07.481093: step 131860, loss = 0.76 (1689.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:08.238061: step 131870, loss = 0.67 (1691.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:09.004974: step 131880, loss = 0.75 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:14:09.767214: step 131890, loss = 0.81 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:10.527287: step 131900, loss = 0.76 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:11.300263: step 131910, loss = 0.64 (1655.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:14:12.056347: step 131920, loss = 0.59 (1692.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:12.817924: step 131930, loss = 0.65 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:13.580880: step 131940, loss = 0.65 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:14.341926: step 131950, loss = 0.86 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:15.107923: step 131960, loss = 0.74 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:14:15.859478: step 131970, loss = 0.73 (1703.1 examples/sec; 0.075 sec/batch)
2017-05-05 20:14:16.622839: step 131980, loss = 0.74 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:17.398889: step 131990, loss = 0.68 (1649.4 examples/sec; 0.078 sec/batch)
2017-05-05 20:14:18.154327: step 132000, loss = 0.88 (1694.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:18.922119: step 132010, loss = 0.68 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:14:19.676545: step 132020, loss = 0.73 (1696.7 examples/sec; 0.075 sec/batch)
2017-05-05 20:14:20.450746: step 132030, loss = 0.64 (1653.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:14:21.212613: step 132040, loss = 0.68 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:21.964636: step 132050, loss = 0.68 (1702.0 examples/sec; 0.075 sec/batch)
2017-05-05 20:14:22.737762: step 132060, loss = 0.72 (1655.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:14:23.498250: step 132070, loss = 0.78 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:24.264650: step 132080, loss = 0.66 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:14:25.026434: step 132090, loss = 0.72 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:25.790435: step 132100, loss = 0.76 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:26.552072: step 132110, loss = 0.77 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:27.316539: step 132120, loss = 0.65 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:28.070590: step 132130, loss = 0.66 (1697.5 examples/sec; 0.075 sec/batch)
2017-05-05 20:14:28.838709: step 132140, loss = 0.64 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:14:29.611830: step 132150, loss = 0.72 (1655.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:14:30.372448: step 132160, loss = 0.68 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:31.138539: step 132170, loss = 0.75 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:14:31.897945: step 132180, loss = 0.80 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:32.664090: step 132190, loss = 0.71 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:14:33.428771: step 132200, loss = 0.65 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:34.197203: step 132210, loss = 0.75 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:14:34.956154: step 132220, loss = 0.75 (1686.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:35.710965: step 132230, loss = 0.64 (1695.8 examples/sec; 0.075 sec/batch)
2017-05-05 20:14:36.473172: step 132240, loss = 0.74 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:37.239927: step 132250, loss = 0.68 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:14:38.003260: step 132260, loss = 0.75 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:38.770418: step 132270, loss = 0.69 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:14:39.529195: step 132280, loss = 0.72 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:40.293847: step 132290, loss = 0.71 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:41.066154: step 132300, loss = 0.65 (1657.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:14:41.822028: step 132310, loss = 0.75 (1693.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:42.591851: step 132320, loss = 0.74 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:14:43.358001: step 132330, loss = 0.73 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:14:44.108315: step 132340, loss = 0.80 (1706.0 examples/sec; 0.075 sec/batch)
2017-05-05 20:14:44.877517: step 132350, loss = 0.78 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:14:45.639535: step 132360, loss = 0.76 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:46.408789: step 132370, loss = 0.73 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:14:47.168917: step 132380, loss = 0.56 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:47.917077: step 132390, loss = 0.75 (1710.9 examples/sec; 0.075 sec/batch)
2017-05-05 20:14:48.684877: step 132400, loss = 0.61 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:14:49.451618: step 132410, loss = 0.62 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:14:50.219895: step 132420, loss = 0.66 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:14:50.978940: step 132430, loss = 0.83 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:51.733816: step 132440, loss = 0.67 (1695.7 examples/sec; 0.075 sec/batch)
2017-05-05 20:14:52.494188: step 132450, loss = 0.77 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:53.252298: step 132460, loss = 0.76 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:54.022918: step 132470, loss = 0.73 (1661.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:14:54.791033: step 132480, loss = 0.73 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:14:55.550411: step 132490, loss = 0.73 (1685.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:56.317819: step 132500, loss = 0.78 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:14:57.078592: step 132510, loss = 0.73 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:14:57.856425: step 132520, loss = 0.72 (1645.6 examples/sec; 0.078 sec/batch)
2017-05-05 20:14:58.628070: step 132530, loss = 0.66 (1658.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:14:59.388520: step 132540, loss = 0.81 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:00.145027: step 132550, loss = 0.76 (1692.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:00.914153: step 132560, loss = 0.71 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:15:01.678313: step 132570, loss = 0.76 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:02.455719: step 132580, loss = 0.88 (1646.5 examples/sec; 0.078 sec/batch)
2017-05-05 20:15:03.220633: step 132590, loss = 0.68 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:03.977045: step 132600, loss = 0.83 (1692.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:04.746465: step 132610, loss = 0.72 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:15:05.510735: step 132620, loss = 0.78 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:06.277038: step 132630, loss = 0.61 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:15:07.049824: step 132640, loss = 0.83 (1656.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:15:07.827786: step 132650, loss = 0.79 (1645.3 examples/sec; 0.078 sec/batch)
2017-05-05 20:15:08.688191: step 132660, loss = 0.77 (1487.7 examples/sec; 0.086 sec/batch)
2017-05-05 20:15:09.357774: step 132670, loss = 0.60 (1911.6 examples/sec; 0.067 sec/batch)
2017-05-05 20:15:10.127067: step 132680, loss = 0.75 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:15:10.882336: step 132690, loss = 0.73 (1694.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:11.636510: step 132700, loss = 0.72 (1697.2 examples/sec; 0.075 sec/batch)
2017-05-05 20:15:12.407157: step 132710, loss = 0.83 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:15:13.170909: step 132720, loss = 0.72 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:13.936663: step 132730, loss = 0.66 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:15:14.699540: step 132740, loss = 0.74 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:15.460565: step 132750, loss = 0.74 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:16.216482: step 132760, loss = 0.78 (1693.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:16.980630: step 132770, loss = 0.55 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:17.751643: step 132780, loss = 0.81 (1660.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:15:18.524476: step 132790, loss = 0.76 (1656.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:15:19.288673: step 132800, loss = 0.71 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:20.049756: step 132810, loss = 0.73 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:20.809124: step 132820, loss = 0.83 (1685.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:21.573689: step 132830, loss = 0.63 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:22.337503: step 132840, loss = 0.80 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:23.105320: step 132850, loss = 0.84 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:15:23.867186: step 132860, loss = 0.69 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:24.639038: step 132870, loss = 0.86 (1658.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:15:25.402576: step 132880, loss = 0.70 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:26.169992: step 132890, loss = 0.60 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:15:26.943471: step 132900, loss = 0.69 (1654.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:15:27.698555: step 132910, loss = 0.78 (1695.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:28.465317: step 132920, loss = 0.80 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:15:29.228589: step 132930, loss = 0.76 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:29.989479: step 132940, loss = 0.60 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:30.755443: step 132950, loss = 0.87 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:15:31.514143: step 132960, loss = 0.77 (1687.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:32.272782: step 132970, loss = 0.61 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:33.037629: step 132980, loss = 0.66 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:33.805175: step 132990, loss = 0.57 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:15:34.572992: step 133000, loss = 0.71 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:15:35.328790: step 133010, loss = 0.71 (1693.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:36.091179: step 133020, loss = 0.75 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:36.852153: step 133030, loss = 0.78 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:37.614324: step 133040, loss = 0.73 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:38.378094: step 133050, loss = 0.80 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:39.145776: step 133060, loss = 0.71 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:15:39.899075: step 133070, loss = 0.85 (1699.2 examples/sec; 0.075 sec/batch)
2017-05-05 20:15:40.661393: step 133080, loss = 0.74 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:41.426099: step 133090, loss = 0.81 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:42.195846: step 133100, loss = 0.68 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:15:42.962621: step 133110, loss = 0.70 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:15:43.718036: step 133120, loss = 0.74 (1694.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:44.485018: step 133130, loss = 0.75 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:15:45.260947: step 133140, loss = 0.74 (1649.6 examples/sec; 0.078 sec/batch)
2017-05-05 20:15:46.027254: step 133150, loss = 0.67 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:15:46.792091: step 133160, loss = 0.60 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:47.549685: step 133170, loss = 0.69 (1689.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:48.311475: step 133180, loss = 0.73 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:49.074210: step 133190, loss = 0.69 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:49.837345: step 133200, loss = 0.78 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:50.598832: step 133210, loss = 0.79 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:51.360823: step 133220, loss = 0.69 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:52.130193: step 133230, loss = 0.73 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:15:52.893701: step 133240, loss = 0.81 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:53.655443: step 133250, loss = 0.76 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:54.423858: step 133260, loss = 0.91 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:15:55.190333: step 133270, loss = 0.68 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:15:55.946403: step 133280, loss = 0.68 (1693.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:56.715886: step 133290, loss = 0.76 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:15:57.483751: step 133300, loss = 0.63 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:15:58.249739: step 133310, loss = 0.61 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:15:59.014666: step 133320, loss = 0.64 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:15:59.772672: step 133330, loss = 0.83 (1688.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:00.538047: step 133340, loss = 0.76 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:01.308495: step 133350, loss = 0.54 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:02.068547: step 133360, loss = 0.69 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:02.831160: step 133370, loss = 0.80 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:03.593170: step 133380, loss = 0.83 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:04.358190: step 133390, loss = 0.69 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:05.123929: step 133400, loss = 0.73 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:05.920054: step 133410, loss = 0.78 (1607.8 examples/sec; 0.080 sec/batch)
2017-05-05 20:16:06.684537: step 133420, loss = 0.57 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:07.450522: step 133430, loss = 0.86 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:08.211812: step 133440, loss = 0.73 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:08.973698: step 133450, loss = 0.81 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:09.742490: step 133460, loss = 0.75 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:10.503350: step 133470, loss = 0.67 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:11.270016: step 133480, loss = 0.64 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:12.028921: step 133490, loss = 0.67 (1686.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:12.794169: step 133500, loss = 0.69 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:13.562248: step 133510, loss = 0.60 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:14.327441: step 133520, loss = 1.03 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:15.091388: step 133530, loss = 0.73 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:15.853013: step 133540, loss = 0.68 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:16.615631: step 133550, loss = 0.60 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:17.385320: step 133560, loss = 0.58 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:18.154411: step 133570, loss = 0.82 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:18.912455: step 133580, loss = 0.71 (1688.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:19.666826: step 133590, loss = 0.65 (1696.8 examples/sec; 0.075 sec/batch)
2017-05-05 20:16:20.428875: step 133600, loss = 0.62 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:21.190145: step 133610, loss = 0.61 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:21.955684: step 133620, loss = 0.75 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:22.718753: step 133630, loss = 0.74 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:23.474405: step 133640, loss = 0.66 (1693.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:24.327077: step 133650, loss = 0.77 (1501.2 examples/sec; 0.085 sec/batch)
2017-05-05 20:16:25.007462: step 133660, loss = 0.80 (1881.3 examples/sec; 0.068 sec/batch)
2017-05-05 20:16:25.768094: step 133670, loss = 0.61 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:26.533785: step 133680, loss = 0.66 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:27.303759: step 133690, loss = 0.79 (1662.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:28.073575: step 133700, loss = 0.73 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:28.854337: step 133710, loss = 0.67 (1639.4 examples/sec; 0.078 sec/batch)
2017-05-05 20:16:29.617227: step 133720, loss = 0.85 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:30.386377: step 133730, loss = 0.63 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:31.150033: step 133740, loss = 0.72 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:31.903143: step 133750, loss = 0.72 (1699.6 examples/sec; 0.075 sec/batch)
2017-05-05 20:16:32.666559: step 133760, loss = 0.57 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:33.433485: step 133770, loss = 0.62 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:34.202746: step 133780, loss = 0.73 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:34.962944: step 133790, loss = 0.87 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:35.721934: step 133800, loss = 0.74 (1686.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:36.487012: step 133810, loss = 0.65 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:37.249673: step 133820, loss = 0.73 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:38.017432: step 133830, loss = 0.50 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:38.779333: step 133840, loss = 0.64 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:39.546767: step 133850, loss = 0.80 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:40.306233: step 133860, loss = 0.61 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:41.068059: step 133870, loss = 0.65 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:41.833801: step 133880, loss = 0.75 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:42.606328: step 133890, loss = 0.71 (1656.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:43.368955: step 133900, loss = 0.70 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:44.122997: step 133910, loss = 0.74 (1697.5 examples/sec; 0.075 sec/batch)
2017-05-05 20:16:44.890697: step 133920, loss = 0.86 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:45.658941: step 133930, loss = 0.79 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:46.432400: step 133940, loss = 0.77 (1654.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:47.197103: step 133950, loss = 0.59 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:47.961174: step 133960, loss = 0.68 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:48.732559: step 133970, loss = 0.72 (1659.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:49.496456: step 133980, loss = 0.75 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:50.266862: step 133990, loss = 0.81 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:51.036760: step 134000, loss = 0.88 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:51.788012: step 134010, loss = 0.62 (1703.8 examples/sec; 0.075 sec/batch)
2017-05-05 20:16:52.553136: step 134020, loss = 0.83 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:53.321347: step 134030, loss = 0.69 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:54.088665: step 134040, loss = 0.64 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:54.857190: step 134050, loss = 0.63 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:16:55.615087: step 134060, loss = 0.85 (1688.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:56.391114: step 134070, loss = 0.77 (1649.4 examples/sec; 0.078 sec/batch)
2017-05-05 20:16:57.154179: step 134080, loss = 0.62 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:57.916599: step 134090, loss = 0.76 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:58.677032: step 134100, loss = 0.62 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:16:59.437099: step 134110, loss = 0.65 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:00.193709: step 134120, loss = 0.73 (1691.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:00.949614: step 134130, loss = 0.70 (1693.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:01.711633: step 134140, loss = 0.69 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:02.476213: step 134150, loss = 0.88 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:03.247477: step 134160, loss = 0.76 (1659.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:17:04.007173: step 134170, loss = 0.75 (1684.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:04.769002: step 134180, loss = 0.83 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:05.534193: step 134190, loss = 0.63 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:17:06.301185: step 134200, loss = 0.78 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:17:07.064150: step 134210, loss = 0.67 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:07.825348: step 134220, loss = 0.79 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:08.597154: step 134230, loss = 0.75 (1658.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:17:09.370750: step 134240, loss = 0.76 (1654.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:17:10.139534: step 134250, loss = 0.67 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:17:10.909551: step 134260, loss = 0.79 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:17:11.670457: step 134270, loss = 0.76 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:12.439541: step 134280, loss = 0.64 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:17:13.207799: step 134290, loss = 0.88 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:17:13.974772: step 134300, loss = 0.68 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:17:14.744426: step 134310, loss = 0.72 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:17:15.506669: step 134320, loss = 0.78 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:16.269597: step 134330, loss = 0.57 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:17.033077: step 134340, loss = 0.64 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:17.802383: step 134350, loss = 0.77 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:17:18.563425: step 134360, loss = 0.86 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:19.326180: step 134370, loss = 0.79 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:20.082007: step 134380, loss = 0.78 (1693.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:20.847580: step 134390, loss = 0.70 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:17:21.607963: step 134400, loss = 0.81 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:22.371686: step 134410, loss = 0.76 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:23.145393: step 134420, loss = 0.76 (1654.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:17:23.899234: step 134430, loss = 0.77 (1698.0 examples/sec; 0.075 sec/batch)
2017-05-05 20:17:24.667300: step 134440, loss = 0.80 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:17:25.429481: step 134450, loss = 0.85 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:26.205070: step 134460, loss = 0.61 (1650.4 examples/sec; 0.078 sec/batch)
2017-05-05 20:17:26.966656: step 134470, loss = 0.66 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:27.721564: step 134480, loss = 0.67 (1695.6 examples/sec; 0.075 sec/batch)
2017-05-05 20:17:28.485920: step 134490, loss = 0.74 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:29.249800: step 134500, loss = 0.61 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:30.016805: step 134510, loss = 0.65 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:17:30.779282: step 134520, loss = 0.66 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:31.538344: step 134530, loss = 0.83 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:32.299462: step 134540, loss = 0.62 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:33.065515: step 134550, loss = 0.73 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:17:33.832603: step 134560, loss = 0.79 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:17:34.597839: step 134570, loss = 0.75 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:17:35.359755: step 134580, loss = 0.56 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:36.116080: step 134590, loss = 0.64 (1692.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:36.881790: step 134600, loss = 0.85 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:17:37.647960: step 134610, loss = 0.64 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:17:38.414324: step 134620, loss = 0.77 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:17:39.185519: step 134630, loss = 0.76 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:17:40.045499: step 134640, loss = 0.75 (1488.4 examples/sec; 0.086 sec/batch)
2017-05-05 20:17:40.707527: step 134650, loss = 0.72 (1933.5 examples/sec; 0.066 sec/batch)
2017-05-05 20:17:41.461562: step 134660, loss = 0.63 (1697.5 examples/sec; 0.075 sec/batch)
2017-05-05 20:17:42.227524: step 134670, loss = 0.71 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:17:42.988149: step 134680, loss = 0.75 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:43.748332: step 134690, loss = 0.63 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:44.509833: step 134700, loss = 0.70 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:45.277043: step 134710, loss = 0.77 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:17:46.035228: step 134720, loss = 0.83 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:46.798122: step 134730, loss = 0.72 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:47.555248: step 134740, loss = 0.69 (1690.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:48.315648: step 134750, loss = 0.63 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:49.084832: step 134760, loss = 0.60 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:17:49.851893: step 134770, loss = 0.75 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:17:50.618213: step 134780, loss = 0.67 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:17:51.381065: step 134790, loss = 0.72 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:52.141960: step 134800, loss = 0.82 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:52.901558: step 134810, loss = 0.81 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:53.663710: step 134820, loss = 0.63 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:54.427028: step 134830, loss = 0.61 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:55.195654: step 134840, loss = 0.64 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:17:55.953938: step 134850, loss = 0.78 (1688.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:56.720544: step 134860, loss = 0.72 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:17:57.489401: step 134870, loss = 0.73 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:17:58.253205: step 134880, loss = 0.86 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:59.016105: step 134890, loss = 0.73 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:17:59.775652: step 134900, loss = 0.66 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:00.537072: step 134910, loss = 0.65 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:01.304046: step 134920, loss = 0.72 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:18:02.063616: step 134930, loss = 0.74 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:02.830891: step 134940, loss = 0.86 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:18:03.583936: step 134950, loss = 0.74 (1699.8 examples/sec; 0.075 sec/batch)
2017-05-05 20:18:04.340215: step 134960, loss = 0.81 (1692.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:05.112554: step 134970, loss = 0.76 (1657.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:18:05.870014: step 134980, loss = 0.74 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:06.633775: step 134990, loss = 0.66 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:07.402083: step 135000, loss = 0.85 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:18:08.157523: step 135010, loss = 0.71 (1694.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:08.917755: step 135020, loss = 0.83 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:09.687201: step 135030, loss = 0.76 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:18:10.456597: step 135040, loss = 0.73 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:18:11.219430: step 135050, loss = 0.70 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:11.975671: step 135060, loss = 0.69 (1692.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:12.738699: step 135070, loss = 0.62 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:13.502559: step 135080, loss = 0.80 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:14.267295: step 135090, loss = 0.73 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:15.034112: step 135100, loss = 0.72 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:18:15.788172: step 135110, loss = 0.68 (1697.5 examples/sec; 0.075 sec/batch)
2017-05-05 20:18:16.553751: step 135120, loss = 0.73 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:18:17.320409: step 135130, loss = 0.78 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:18:18.079720: step 135140, loss = 0.69 (1685.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:18.846399: step 135150, loss = 0.74 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:18:19.603057: step 135160, loss = 0.66 (1691.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:20.362969: step 135170, loss = 0.84 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:21.133811: step 135180, loss = 0.64 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:18:21.899716: step 135190, loss = 0.68 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:18:22.668266: step 135200, loss = 0.70 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:18:23.436092: step 135210, loss = 0.91 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:18:24.195324: step 135220, loss = 0.76 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:24.956046: step 135230, loss = 0.75 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:25.723668: step 135240, loss = 0.67 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:18:26.491051: step 135250, loss = 0.64 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:18:27.254229: step 135260, loss = 0.61 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:28.017136: step 135270, loss = 0.75 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:28.784747: step 135280, loss = 0.58 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:18:29.546196: step 135290, loss = 0.68 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:30.309509: step 135300, loss = 0.78 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:31.071976: step 135310, loss = 0.72 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:31.833341: step 135320, loss = 0.62 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:32.592614: step 135330, loss = 0.76 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:33.355674: step 135340, loss = 0.77 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:34.120713: step 135350, loss = 0.67 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:18:34.885550: step 135360, loss = 0.65 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:35.641163: step 135370, loss = 0.73 (1694.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:36.411944: step 135380, loss = 0.78 (1660.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:18:37.177149: step 135390, loss = 0.88 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:18:37.946331: step 135400, loss = 0.69 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:18:38.712201: step 135410, loss = 0.72 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:18:39.475954: step 135420, loss = 0.73 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:40.235228: step 135430, loss = 0.65 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:40.996613: step 135440, loss = 0.70 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:41.765437: step 135450, loss = 0.70 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:18:42.529849: step 135460, loss = 0.83 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:43.296059: step 135470, loss = 0.75 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:18:44.054620: step 135480, loss = 0.73 (1687.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:44.820096: step 135490, loss = 0.70 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:18:45.587972: step 135500, loss = 0.70 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:18:46.351974: step 135510, loss = 0.71 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:47.121297: step 135520, loss = 0.84 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:18:47.880157: step 135530, loss = 0.70 (1686.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:48.638180: step 135540, loss = 0.62 (1688.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:49.401849: step 135550, loss = 0.69 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:50.164091: step 135560, loss = 0.74 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:50.927701: step 135570, loss = 0.75 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:51.691658: step 135580, loss = 0.95 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:52.458285: step 135590, loss = 1.00 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:18:53.224715: step 135600, loss = 0.73 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:18:53.994262: step 135610, loss = 0.63 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:18:54.758726: step 135620, loss = 0.66 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:55.610761: step 135630, loss = 0.71 (1502.3 examples/sec; 0.085 sec/batch)
2017-05-05 20:18:56.283401: step 135640, loss = 0.60 (1902.9 examples/sec; 0.067 sec/batch)
2017-05-05 20:18:57.050125: step 135650, loss = 0.66 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:18:57.813032: step 135660, loss = 0.61 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:58.577398: step 135670, loss = 0.74 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:18:59.341779: step 135680, loss = 0.76 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:00.104668: step 135690, loss = 0.74 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:00.867486: step 135700, loss = 0.81 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:01.630771: step 135710, loss = 0.72 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:02.397223: step 135720, loss = 0.71 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:03.168876: step 135730, loss = 0.81 (1658.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:03.920186: step 135740, loss = 0.74 (1703.7 examples/sec; 0.075 sec/batch)
2017-05-05 20:19:04.681221: step 135750, loss = 0.66 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:05.451649: step 135760, loss = 0.61 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:06.214256: step 135770, loss = 0.75 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:06.981487: step 135780, loss = 0.72 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:07.739011: step 135790, loss = 0.64 (1689.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:08.509586: step 135800, loss = 0.57 (1661.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:09.269541: step 135810, loss = 0.66 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:10.036581: step 135820, loss = 0.72 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:10.800250: step 135830, loss = 0.73 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:11.559280: step 135840, loss = 0.74 (1686.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:12.325792: step 135850, loss = 0.66 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:13.093427: step 135860, loss = 0.65 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:13.857712: step 135870, loss = 0.62 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:14.623697: step 135880, loss = 0.72 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:15.384663: step 135890, loss = 0.65 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:16.142201: step 135900, loss = 0.56 (1689.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:16.903277: step 135910, loss = 0.59 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:17.671814: step 135920, loss = 0.73 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:18.434537: step 135930, loss = 0.97 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:19.201513: step 135940, loss = 0.55 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:19.952906: step 135950, loss = 0.76 (1703.5 examples/sec; 0.075 sec/batch)
2017-05-05 20:19:20.713746: step 135960, loss = 0.66 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:21.480924: step 135970, loss = 0.67 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:22.256163: step 135980, loss = 0.73 (1651.1 examples/sec; 0.078 sec/batch)
2017-05-05 20:19:23.024124: step 135990, loss = 0.72 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:23.784983: step 136000, loss = 0.68 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:24.551245: step 136010, loss = 0.66 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:25.317414: step 136020, loss = 0.71 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:26.084230: step 136030, loss = 0.73 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:26.845506: step 136040, loss = 0.81 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:27.603312: step 136050, loss = 0.73 (1689.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:28.364531: step 136060, loss = 0.64 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:29.125157: step 136070, loss = 0.65 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:29.893805: step 136080, loss = 0.62 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:30.660591: step 136090, loss = 0.96 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:31.423225: step 136100, loss = 0.61 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:32.183741: step 136110, loss = 0.78 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:32.948601: step 136120, loss = 0.66 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:33.709552: step 136130, loss = 0.69 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:34.472195: step 136140, loss = 0.62 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:35.231989: step 136150, loss = 0.68 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:35.984329: step 136160, loss = 0.71 (1701.3 examples/sec; 0.075 sec/batch)
2017-05-05 20:19:36.748383: step 136170, loss = 0.83 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:37.518375: step 136180, loss = 0.88 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:38.284116: step 136190, loss = 1.08 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:39.051244: step 136200, loss = 0.66 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:39.813411: step 136210, loss = 0.88 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:40.580088: step 136220, loss = 0.69 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:41.347462: step 136230, loss = 0.95 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:42.106824: step 136240, loss = 0.70 (1685.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:42.882938: step 136250, loss = 0.76 (1649.2 examples/sec; 0.078 sec/batch)
2017-05-05 20:19:43.641369: step 136260, loss = 0.72 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:44.412478: step 136270, loss = 0.64 (1659.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:45.176644: step 136280, loss = 0.68 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:45.939748: step 136290, loss = 0.82 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:46.703353: step 136300, loss = 0.67 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:47.460162: step 136310, loss = 0.82 (1691.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:48.223544: step 136320, loss = 0.68 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:48.991069: step 136330, loss = 0.66 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:49.764619: step 136340, loss = 0.66 (1654.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:50.533407: step 136350, loss = 0.65 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:51.304338: step 136360, loss = 0.89 (1660.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:52.068908: step 136370, loss = 0.75 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:19:52.840181: step 136380, loss = 0.83 (1659.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:53.615341: step 136390, loss = 0.74 (1651.3 examples/sec; 0.078 sec/batch)
2017-05-05 20:19:54.386477: step 136400, loss = 0.84 (1659.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:55.151926: step 136410, loss = 0.69 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:55.918999: step 136420, loss = 0.69 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:56.688146: step 136430, loss = 0.59 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:57.462434: step 136440, loss = 0.58 (1653.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:58.230265: step 136450, loss = 0.75 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:58.998244: step 136460, loss = 0.64 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:19:59.756117: step 136470, loss = 0.89 (1688.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:00.514769: step 136480, loss = 0.58 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:01.277947: step 136490, loss = 0.68 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:02.047983: step 136500, loss = 0.77 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:02.809179: step 136510, loss = 0.71 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:03.565619: step 136520, loss = 0.67 (1692.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:04.333974: step 136530, loss = 0.81 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:05.096229: step 136540, loss = 0.66 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:05.862126: step 136550, loss = 0.72 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:06.633877: step 136560, loss = 0.73 (1658.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:07.397150: step 136570, loss = 0.72 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:08.155823: step 136580, loss = 0.66 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:08.922611: step 136590, loss = 0.72 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:09.685117: step 136600, loss = 0.61 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:10.448868: step 136610, loss = 0.75 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:11.333904: step 136620, loss = 0.77 (1446.3 examples/sec; 0.089 sec/batch)
2017-05-05 20:20:11.980446: step 136630, loss = 0.61 (1979.8 examples/sec; 0.065 sec/batch)
2017-05-05 20:20:12.746992: step 136640, loss = 0.87 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:13.514938: step 136650, loss = 0.57 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:14.279624: step 136660, loss = 0.64 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:15.046692: step 136670, loss = 0.74 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:15.802714: step 136680, loss = 0.70 (1693.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:16.563459: step 136690, loss = 0.65 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:17.331262: step 136700, loss = 0.75 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:18.098089: step 136710, loss = 0.78 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:18.867030: step 136720, loss = 0.74 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:19.633293: step 136730, loss = 0.71 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:20.394151: step 136740, loss = 0.63 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:21.160568: step 136750, loss = 0.70 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:21.923960: step 136760, loss = 0.78 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:22.694840: step 136770, loss = 0.53 (1660.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:23.454799: step 136780, loss = 0.75 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:24.217403: step 136790, loss = 0.68 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:24.980812: step 136800, loss = 0.77 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:25.747598: step 136810, loss = 0.66 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:26.516481: step 136820, loss = 0.79 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:27.287098: step 136830, loss = 0.80 (1661.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:28.046584: step 136840, loss = 0.70 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:28.811005: step 136850, loss = 0.64 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:29.572939: step 136860, loss = 0.89 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:30.338193: step 136870, loss = 0.70 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:31.097983: step 136880, loss = 0.62 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:31.854339: step 136890, loss = 0.81 (1692.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:32.621111: step 136900, loss = 0.72 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:33.385914: step 136910, loss = 0.82 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:34.150787: step 136920, loss = 0.68 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:34.915853: step 136930, loss = 0.71 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:35.676521: step 136940, loss = 0.75 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:36.439615: step 136950, loss = 0.71 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:37.205181: step 136960, loss = 0.69 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:37.977263: step 136970, loss = 0.83 (1657.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:38.745919: step 136980, loss = 0.72 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:39.501671: step 136990, loss = 0.69 (1693.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:40.265836: step 137000, loss = 0.67 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:41.032892: step 137010, loss = 0.54 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:41.801374: step 137020, loss = 0.76 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:42.564776: step 137030, loss = 0.75 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:43.330277: step 137040, loss = 0.65 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:44.090236: step 137050, loss = 0.67 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:44.855548: step 137060, loss = 0.68 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:45.615606: step 137070, loss = 0.62 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:46.380391: step 137080, loss = 0.67 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:47.146044: step 137090, loss = 0.82 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:47.898532: step 137100, loss = 0.76 (1701.0 examples/sec; 0.075 sec/batch)
2017-05-05 20:20:48.671496: step 137110, loss = 0.74 (1656.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:49.437277: step 137120, loss = 0.82 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:50.204929: step 137130, loss = 0.79 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:50.970744: step 137140, loss = 0.72 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:51.722742: step 137150, loss = 0.69 (1702.1 examples/sec; 0.075 sec/batch)
2017-05-05 20:20:52.488240: step 137160, loss = 0.61 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:53.252158: step 137170, loss = 0.66 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:54.013493: step 137180, loss = 0.87 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:54.780601: step 137190, loss = 0.73 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:55.540526: step 137200, loss = 0.80 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:56.308975: step 137210, loss = 0.77 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:20:57.073295: step 137220, loss = 0.74 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:57.851216: step 137230, loss = 0.58 (1645.4 examples/sec; 0.078 sec/batch)
2017-05-05 20:20:58.612739: step 137240, loss = 0.60 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:20:59.375191: step 137250, loss = 0.82 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:00.135487: step 137260, loss = 0.77 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:00.895449: step 137270, loss = 0.70 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:01.664039: step 137280, loss = 0.56 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:21:02.432957: step 137290, loss = 0.80 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:21:03.201868: step 137300, loss = 0.65 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:21:03.958137: step 137310, loss = 0.84 (1692.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:04.720572: step 137320, loss = 0.66 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:05.479070: step 137330, loss = 0.65 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:06.239189: step 137340, loss = 0.61 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:07.002047: step 137350, loss = 0.63 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:07.756515: step 137360, loss = 0.66 (1696.6 examples/sec; 0.075 sec/batch)
2017-05-05 20:21:08.523846: step 137370, loss = 0.74 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:21:09.287954: step 137380, loss = 0.70 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:10.055677: step 137390, loss = 0.73 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:21:10.820425: step 137400, loss = 0.66 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:11.576677: step 137410, loss = 0.71 (1692.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:12.342513: step 137420, loss = 0.74 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:21:13.104857: step 137430, loss = 0.92 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:13.868186: step 137440, loss = 0.74 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:14.634289: step 137450, loss = 0.64 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:21:15.396717: step 137460, loss = 0.74 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:16.152418: step 137470, loss = 0.84 (1693.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:16.917542: step 137480, loss = 0.66 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:21:17.677628: step 137490, loss = 0.72 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:18.443245: step 137500, loss = 0.77 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:21:19.208460: step 137510, loss = 0.60 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:21:19.957552: step 137520, loss = 0.72 (1708.7 examples/sec; 0.075 sec/batch)
2017-05-05 20:21:20.715917: step 137530, loss = 0.74 (1687.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:21.482150: step 137540, loss = 0.84 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:21:22.246606: step 137550, loss = 0.79 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:23.007820: step 137560, loss = 0.58 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:23.767599: step 137570, loss = 0.68 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:24.535440: step 137580, loss = 0.69 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:21:25.304826: step 137590, loss = 0.57 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:21:26.070310: step 137600, loss = 0.67 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:21:26.928877: step 137610, loss = 0.77 (1490.9 examples/sec; 0.086 sec/batch)
2017-05-05 20:21:27.592283: step 137620, loss = 0.82 (1929.4 examples/sec; 0.066 sec/batch)
2017-05-05 20:21:28.350243: step 137630, loss = 0.69 (1688.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:29.116577: step 137640, loss = 0.74 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:21:29.882150: step 137650, loss = 0.72 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:21:30.646226: step 137660, loss = 0.69 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:31.411682: step 137670, loss = 0.90 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:21:32.170459: step 137680, loss = 0.71 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:32.934956: step 137690, loss = 0.88 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:33.698453: step 137700, loss = 0.68 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:34.465990: step 137710, loss = 0.71 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:21:35.226965: step 137720, loss = 0.82 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:35.979867: step 137730, loss = 0.80 (1700.1 examples/sec; 0.075 sec/batch)
2017-05-05 20:21:36.745059: step 137740, loss = 0.73 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:21:37.511561: step 137750, loss = 0.77 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:21:38.279637: step 137760, loss = 0.78 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:21:39.044081: step 137770, loss = 0.80 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:39.794518: step 137780, loss = 0.87 (1705.7 examples/sec; 0.075 sec/batch)
2017-05-05 20:21:40.563429: step 137790, loss = 0.54 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:21:41.329854: step 137800, loss = 0.67 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:21:42.093889: step 137810, loss = 0.69 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:42.855118: step 137820, loss = 0.73 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:43.610349: step 137830, loss = 0.74 (1694.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:44.372320: step 137840, loss = 0.75 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:45.140956: step 137850, loss = 0.74 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:21:45.899830: step 137860, loss = 0.65 (1686.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:46.660604: step 137870, loss = 0.77 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:47.420295: step 137880, loss = 0.72 (1684.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:48.182468: step 137890, loss = 0.71 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:48.945691: step 137900, loss = 0.72 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:49.710775: step 137910, loss = 0.74 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:21:50.478452: step 137920, loss = 0.73 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:21:51.244336: step 137930, loss = 0.70 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:21:52.000165: step 137940, loss = 0.74 (1693.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:52.759636: step 137950, loss = 0.64 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:53.532984: step 137960, loss = 0.71 (1655.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:21:54.297668: step 137970, loss = 0.71 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:55.059185: step 137980, loss = 0.62 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:55.813914: step 137990, loss = 0.75 (1696.0 examples/sec; 0.075 sec/batch)
2017-05-05 20:21:56.575459: step 138000, loss = 0.61 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:57.344178: step 138010, loss = 0.66 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:21:58.107149: step 138020, loss = 0.75 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:21:58.875046: step 138030, loss = 0.73 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:21:59.634769: step 138040, loss = 0.75 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:00.398967: step 138050, loss = 0.72 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:01.161741: step 138060, loss = 0.70 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:01.922353: step 138070, loss = 0.78 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:02.688299: step 138080, loss = 0.61 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:22:03.453824: step 138090, loss = 0.88 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:22:04.216559: step 138100, loss = 0.73 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:04.978252: step 138110, loss = 0.69 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:05.741618: step 138120, loss = 0.64 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:06.500774: step 138130, loss = 0.77 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:07.270822: step 138140, loss = 0.65 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:22:08.021790: step 138150, loss = 0.74 (1704.5 examples/sec; 0.075 sec/batch)
2017-05-05 20:22:08.790420: step 138160, loss = 0.74 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:22:09.550637: step 138170, loss = 0.69 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:10.316568: step 138180, loss = 0.68 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:22:11.074927: step 138190, loss = 0.69 (1687.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:11.830512: step 138200, loss = 0.62 (1694.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:12.603797: step 138210, loss = 0.56 (1655.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:22:13.366483: step 138220, loss = 0.63 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:14.140960: step 138230, loss = 0.68 (1652.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:22:14.914158: step 138240, loss = 0.70 (1655.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:22:15.677815: step 138250, loss = 0.65 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:16.439519: step 138260, loss = 0.72 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:17.209229: step 138270, loss = 0.64 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:22:17.972486: step 138280, loss = 0.64 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:18.737078: step 138290, loss = 0.74 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:19.493714: step 138300, loss = 0.75 (1691.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:20.260982: step 138310, loss = 0.84 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:22:21.025045: step 138320, loss = 0.73 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:21.788164: step 138330, loss = 0.83 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:22.554126: step 138340, loss = 0.56 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:22:23.318051: step 138350, loss = 0.71 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:24.076579: step 138360, loss = 0.63 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:24.843353: step 138370, loss = 0.84 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:22:25.606954: step 138380, loss = 0.62 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:26.373509: step 138390, loss = 0.63 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:22:27.141005: step 138400, loss = 0.63 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:22:27.896126: step 138410, loss = 0.60 (1695.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:28.661407: step 138420, loss = 0.65 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:22:29.427232: step 138430, loss = 0.73 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:22:30.197009: step 138440, loss = 0.86 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:22:30.963986: step 138450, loss = 0.79 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:22:31.718213: step 138460, loss = 0.82 (1697.1 examples/sec; 0.075 sec/batch)
2017-05-05 20:22:32.487623: step 138470, loss = 0.53 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:22:33.245804: step 138480, loss = 0.61 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:34.018698: step 138490, loss = 0.74 (1656.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:22:34.781442: step 138500, loss = 0.69 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:35.539660: step 138510, loss = 0.60 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:36.297664: step 138520, loss = 0.76 (1688.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:37.060312: step 138530, loss = 0.82 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:37.822273: step 138540, loss = 0.75 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:38.593722: step 138550, loss = 0.61 (1659.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:22:39.360291: step 138560, loss = 0.66 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:22:40.116657: step 138570, loss = 0.72 (1692.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:40.881464: step 138580, loss = 0.70 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:41.646215: step 138590, loss = 0.84 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:42.509889: step 138600, loss = 0.69 (1482.0 examples/sec; 0.086 sec/batch)
2017-05-05 20:22:43.176767: step 138610, loss = 0.54 (1919.4 examples/sec; 0.067 sec/batch)
2017-05-05 20:22:43.937872: step 138620, loss = 0.68 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:44.701203: step 138630, loss = 0.67 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:45.470084: step 138640, loss = 0.59 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:22:46.236151: step 138650, loss = 0.73 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:22:47.000418: step 138660, loss = 0.78 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:47.750679: step 138670, loss = 0.79 (1706.1 examples/sec; 0.075 sec/batch)
2017-05-05 20:22:48.511735: step 138680, loss = 0.80 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:49.282178: step 138690, loss = 0.48 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:22:50.049343: step 138700, loss = 0.73 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:22:50.806515: step 138710, loss = 0.72 (1690.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:51.560963: step 138720, loss = 0.72 (1696.6 examples/sec; 0.075 sec/batch)
2017-05-05 20:22:52.329013: step 138730, loss = 0.74 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:22:53.096028: step 138740, loss = 0.85 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:22:53.861229: step 138750, loss = 0.61 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:22:54.620728: step 138760, loss = 0.75 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:55.388309: step 138770, loss = 0.63 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:22:56.144888: step 138780, loss = 0.74 (1691.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:56.907862: step 138790, loss = 0.73 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:57.684505: step 138800, loss = 0.58 (1648.1 examples/sec; 0.078 sec/batch)
2017-05-05 20:22:58.451632: step 138810, loss = 0.80 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:22:59.215476: step 138820, loss = 0.60 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:22:59.962417: step 138830, loss = 0.71 (1713.7 examples/sec; 0.075 sec/batch)
2017-05-05 20:23:00.727020: step 138840, loss = 0.60 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:01.493440: step 138850, loss = 0.85 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:23:02.259179: step 138860, loss = 0.93 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:23:03.030154: step 138870, loss = 0.78 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:23:03.783438: step 138880, loss = 0.73 (1699.2 examples/sec; 0.075 sec/batch)
2017-05-05 20:23:04.551934: step 138890, loss = 0.75 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:23:05.315046: step 138900, loss = 0.62 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:06.076535: step 138910, loss = 0.73 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:06.834731: step 138920, loss = 0.99 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:07.593695: step 138930, loss = 0.78 (1686.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:08.360494: step 138940, loss = 0.77 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:23:09.123712: step 138950, loss = 0.73 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:09.889338: step 138960, loss = 0.69 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:23:10.662499: step 138970, loss = 0.77 (1655.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:23:11.423987: step 138980, loss = 0.83 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:12.182472: step 138990, loss = 0.94 (1687.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:12.946522: step 139000, loss = 0.76 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:13.713098: step 139010, loss = 0.69 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:23:14.479375: step 139020, loss = 0.71 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:23:15.246486: step 139030, loss = 0.74 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:23:16.002913: step 139040, loss = 0.84 (1692.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:16.765276: step 139050, loss = 0.73 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:17.534842: step 139060, loss = 0.68 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:23:18.300254: step 139070, loss = 0.74 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:23:19.064523: step 139080, loss = 0.73 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:19.818418: step 139090, loss = 0.69 (1697.8 examples/sec; 0.075 sec/batch)
2017-05-05 20:23:20.581981: step 139100, loss = 0.82 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:21.343645: step 139110, loss = 0.65 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:22.103827: step 139120, loss = 0.71 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:22.869014: step 139130, loss = 0.72 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:23:23.628881: step 139140, loss = 0.78 (1684.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:24.391750: step 139150, loss = 0.70 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:25.153081: step 139160, loss = 0.70 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:25.928559: step 139170, loss = 0.75 (1650.6 examples/sec; 0.078 sec/batch)
2017-05-05 20:23:26.700960: step 139180, loss = 0.70 (1657.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:23:27.451973: step 139190, loss = 0.65 (1704.4 examples/sec; 0.075 sec/batch)
2017-05-05 20:23:28.216622: step 139200, loss = 0.72 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:28.980642: step 139210, loss = 0.78 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:29.743114: step 139220, loss = 0.61 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:30.511650: step 139230, loss = 0.70 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:23:31.272397: step 139240, loss = 0.61 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:32.021758: step 139250, loss = 0.75 (1708.1 examples/sec; 0.075 sec/batch)
2017-05-05 20:23:32.787579: step 139260, loss = 0.66 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:23:33.558267: step 139270, loss = 0.72 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:23:34.317081: step 139280, loss = 0.75 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:35.084542: step 139290, loss = 0.61 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:23:35.839763: step 139300, loss = 0.74 (1694.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:36.605102: step 139310, loss = 0.65 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:23:37.377518: step 139320, loss = 0.74 (1657.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:23:38.139789: step 139330, loss = 0.69 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:38.903672: step 139340, loss = 0.73 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:39.666088: step 139350, loss = 0.81 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:40.427007: step 139360, loss = 0.66 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:41.196172: step 139370, loss = 0.68 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:23:41.953347: step 139380, loss = 0.77 (1690.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:42.723951: step 139390, loss = 0.58 (1661.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:23:43.487701: step 139400, loss = 0.77 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:44.247039: step 139410, loss = 0.76 (1685.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:45.005330: step 139420, loss = 0.75 (1688.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:45.772968: step 139430, loss = 0.86 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:23:46.530340: step 139440, loss = 0.73 (1690.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:47.293288: step 139450, loss = 0.72 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:48.052386: step 139460, loss = 0.79 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:48.814876: step 139470, loss = 0.59 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:49.578740: step 139480, loss = 0.71 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:50.344689: step 139490, loss = 0.93 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:23:51.102155: step 139500, loss = 0.85 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:51.859526: step 139510, loss = 0.61 (1690.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:52.628322: step 139520, loss = 0.74 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:23:53.400625: step 139530, loss = 0.58 (1657.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:23:54.160485: step 139540, loss = 0.64 (1684.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:54.924716: step 139550, loss = 0.93 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:55.686903: step 139560, loss = 0.73 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:56.444072: step 139570, loss = 0.65 (1690.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:23:57.209220: step 139580, loss = 0.76 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:23:58.075583: step 139590, loss = 0.55 (1477.4 examples/sec; 0.087 sec/batch)
2017-05-05 20:23:58.741784: step 139600, loss = 0.78 (1921.3 examples/sec; 0.067 sec/batch)
2017-05-05 20:23:59.499331: step 139610, loss = 0.75 (1689.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:00.260191: step 139620, loss = 0.81 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:01.022448: step 139630, loss = 0.67 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:01.788406: step 139640, loss = 0.63 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:02.551029: step 139650, loss = 0.76 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:03.313471: step 139660, loss = 0.66 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:04.075481: step 139670, loss = 0.74 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:04.841097: step 139680, loss = 0.78 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:05.607775: step 139690, loss = 0.70 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:06.375215: step 139700, loss = 0.62 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:07.144340: step 139710, loss = 0.72 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:07.896620: step 139720, loss = 0.68 (1701.5 examples/sec; 0.075 sec/batch)
2017-05-05 20:24:08.660678: step 139730, loss = 0.82 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:09.425061: step 139740, loss = 0.71 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:10.196030: step 139750, loss = 0.60 (1660.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:10.962087: step 139760, loss = 0.65 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:11.720864: step 139770, loss = 0.72 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:12.486176: step 139780, loss = 0.81 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:13.253305: step 139790, loss = 0.71 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:14.021928: step 139800, loss = 0.71 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:14.786518: step 139810, loss = 0.71 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:15.548088: step 139820, loss = 0.67 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:16.312592: step 139830, loss = 0.74 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:17.078844: step 139840, loss = 0.89 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:17.849643: step 139850, loss = 0.78 (1660.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:18.619657: step 139860, loss = 0.63 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:19.385191: step 139870, loss = 0.58 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:20.142583: step 139880, loss = 0.69 (1690.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:20.911711: step 139890, loss = 0.68 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:21.663373: step 139900, loss = 0.74 (1702.9 examples/sec; 0.075 sec/batch)
2017-05-05 20:24:22.427309: step 139910, loss = 0.59 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:23.197556: step 139920, loss = 0.79 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:23.949783: step 139930, loss = 0.67 (1701.6 examples/sec; 0.075 sec/batch)
2017-05-05 20:24:24.728129: step 139940, loss = 0.69 (1644.5 examples/sec; 0.078 sec/batch)
2017-05-05 20:24:25.499598: step 139950, loss = 0.68 (1659.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:26.264541: step 139960, loss = 0.60 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:27.025806: step 139970, loss = 0.72 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:27.780791: step 139980, loss = 0.66 (1695.4 examples/sec; 0.075 sec/batch)
2017-05-05 20:24:28.546130: step 139990, loss = 0.69 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:29.311301: step 140000, loss = 0.73 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:30.080934: step 140010, loss = 0.74 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:30.847625: step 140020, loss = 0.74 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:31.616234: step 140030, loss = 0.76 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:32.382264: step 140040, loss = 0.61 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:33.140777: step 140050, loss = 0.72 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:33.908893: step 140060, loss = 0.75 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:34.674873: step 140070, loss = 0.69 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:35.436217: step 140080, loss = 0.68 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:36.198259: step 140090, loss = 0.72 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:36.958417: step 140100, loss = 0.75 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:37.729904: step 140110, loss = 0.72 (1659.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:38.498023: step 140120, loss = 0.79 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:39.263343: step 140130, loss = 0.65 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:40.026488: step 140140, loss = 0.81 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:40.784321: step 140150, loss = 0.70 (1689.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:41.547318: step 140160, loss = 0.67 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:42.314409: step 140170, loss = 0.72 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:43.090622: step 140180, loss = 0.80 (1649.0 examples/sec; 0.078 sec/batch)
2017-05-05 20:24:43.839408: step 140190, loss = 0.64 (1709.4 examples/sec; 0.075 sec/batch)
2017-05-05 20:24:44.611787: step 140200, loss = 0.65 (1657.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:45.375291: step 140210, loss = 0.76 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:46.135543: step 140220, loss = 0.59 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:46.898828: step 140230, loss = 0.71 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:47.657071: step 140240, loss = 0.70 (1688.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:48.417750: step 140250, loss = 0.78 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:49.180052: step 140260, loss = 0.66 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:49.945352: step 140270, loss = 0.76 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:50.720299: step 140280, loss = 0.62 (1651.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:51.470125: step 140290, loss = 0.95 (1707.1 examples/sec; 0.075 sec/batch)
2017-05-05 20:24:52.228829: step 140300, loss = 0.63 (1687.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:52.998228: step 140310, loss = 0.69 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:53.766016: step 140320, loss = 0.67 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:54.533387: step 140330, loss = 0.94 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:55.294778: step 140340, loss = 0.68 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:56.054498: step 140350, loss = 0.67 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:56.817422: step 140360, loss = 0.68 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:57.582562: step 140370, loss = 0.70 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:58.349488: step 140380, loss = 0.82 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:24:59.114209: step 140390, loss = 0.64 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:24:59.866873: step 140400, loss = 0.70 (1700.6 examples/sec; 0.075 sec/batch)
2017-05-05 20:25:00.637091: step 140410, loss = 0.71 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:25:01.409118: step 140420, loss = 0.84 (1658.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:25:02.192966: step 140430, loss = 0.65 (1633.0 examples/sec; 0.078 sec/batch)
2017-05-05 20:25:02.954928: step 140440, loss = 0.80 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:03.717704: step 140450, loss = 0.66 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:04.482308: step 140460, loss = 0.69 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:05.244262: step 140470, loss = 0.77 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:06.008928: step 140480, loss = 0.64 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:06.773712: step 140490, loss = 0.68 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:07.538331: step 140500, loss = 0.76 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:08.301808: step 140510, loss = 0.67 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:09.063241: step 140520, loss = 0.75 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:09.828444: step 140530, loss = 0.61 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:25:10.587811: step 140540, loss = 0.62 (1685.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:11.352616: step 140550, loss = 0.73 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:12.110085: step 140560, loss = 0.76 (1689.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:12.877633: step 140570, loss = 0.69 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:25:13.739956: step 140580, loss = 0.64 (1484.4 examples/sec; 0.086 sec/batch)
2017-05-05 20:25:14.410342: step 140590, loss = 0.77 (1909.4 examples/sec; 0.067 sec/batch)
2017-05-05 20:25:15.171940: step 140600, loss = 0.57 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:15.921952: step 140610, loss = 0.52 (1706.6 examples/sec; 0.075 sec/batch)
2017-05-05 20:25:16.686419: step 140620, loss = 0.76 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:17.447021: step 140630, loss = 0.60 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:18.215974: step 140640, loss = 0.61 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:25:18.979671: step 140650, loss = 0.68 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:19.737885: step 140660, loss = 0.49 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:20.511438: step 140670, loss = 0.75 (1654.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:25:21.275014: step 140680, loss = 0.69 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:22.036487: step 140690, loss = 0.79 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:22.801461: step 140700, loss = 0.64 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:23.568044: step 140710, loss = 0.67 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:25:24.333852: step 140720, loss = 0.61 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:25:25.100443: step 140730, loss = 0.72 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:25:25.856415: step 140740, loss = 0.64 (1693.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:26.626156: step 140750, loss = 0.74 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:25:27.389477: step 140760, loss = 0.62 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:28.144776: step 140770, loss = 0.62 (1694.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:28.911483: step 140780, loss = 0.80 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:25:29.673335: step 140790, loss = 0.75 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:30.437161: step 140800, loss = 0.68 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:31.206952: step 140810, loss = 0.71 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:25:31.961195: step 140820, loss = 0.59 (1697.1 examples/sec; 0.075 sec/batch)
2017-05-05 20:25:32.724136: step 140830, loss = 0.68 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:33.489397: step 140840, loss = 0.69 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:25:34.257005: step 140850, loss = 0.66 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:25:35.021619: step 140860, loss = 0.68 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:35.778855: step 140870, loss = 0.72 (1690.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:36.545933: step 140880, loss = 0.72 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:25:37.314258: step 140890, loss = 0.71 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:25:38.078515: step 140900, loss = 0.68 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:38.850101: step 140910, loss = 0.82 (1658.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:25:39.606874: step 140920, loss = 0.63 (1691.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:40.376622: step 140930, loss = 0.54 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:25:41.132717: step 140940, loss = 0.75 (1692.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:41.890042: step 140950, loss = 0.75 (1690.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:42.654938: step 140960, loss = 0.76 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:43.413225: step 140970, loss = 0.79 (1688.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:44.169982: step 140980, loss = 0.75 (1691.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:44.933760: step 140990, loss = 0.67 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:45.697135: step 141000, loss = 0.63 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:46.464586: step 141010, loss = 0.65 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:25:47.226779: step 141020, loss = 0.68 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:47.980101: step 141030, loss = 0.76 (1699.1 examples/sec; 0.075 sec/batch)
2017-05-05 20:25:48.747945: step 141040, loss = 0.77 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:25:49.515895: step 141050, loss = 0.73 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:25:50.283879: step 141060, loss = 0.52 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:25:51.050326: step 141070, loss = 0.61 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:25:51.803874: step 141080, loss = 0.90 (1698.6 examples/sec; 0.075 sec/batch)
2017-05-05 20:25:52.572181: step 141090, loss = 0.64 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:25:53.342259: step 141100, loss = 0.75 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:25:54.105891: step 141110, loss = 0.76 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:54.880647: step 141120, loss = 0.72 (1652.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:25:55.654634: step 141130, loss = 0.79 (1653.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:25:56.424305: step 141140, loss = 0.76 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:25:57.186769: step 141150, loss = 0.66 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:57.953062: step 141160, loss = 0.56 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:25:58.715878: step 141170, loss = 0.89 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:25:59.485017: step 141180, loss = 0.72 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:26:00.242700: step 141190, loss = 0.74 (1689.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:01.003941: step 141200, loss = 0.56 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:01.764336: step 141210, loss = 0.71 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:02.529300: step 141220, loss = 0.79 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:03.295629: step 141230, loss = 0.60 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:26:04.055388: step 141240, loss = 0.87 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:04.818372: step 141250, loss = 0.60 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:05.579638: step 141260, loss = 0.81 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:06.374736: step 141270, loss = 0.71 (1609.9 examples/sec; 0.080 sec/batch)
2017-05-05 20:26:07.140493: step 141280, loss = 0.70 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:26:07.892635: step 141290, loss = 0.81 (1701.8 examples/sec; 0.075 sec/batch)
2017-05-05 20:26:08.660947: step 141300, loss = 0.68 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:26:09.427492: step 141310, loss = 0.91 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:26:10.198615: step 141320, loss = 0.70 (1659.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:26:10.961192: step 141330, loss = 0.86 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:11.720280: step 141340, loss = 0.65 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:12.480344: step 141350, loss = 0.71 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:13.246530: step 141360, loss = 0.67 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:26:14.015907: step 141370, loss = 0.71 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:26:14.786574: step 141380, loss = 0.69 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:26:15.549219: step 141390, loss = 0.72 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:16.306918: step 141400, loss = 0.71 (1689.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:17.063706: step 141410, loss = 0.64 (1691.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:17.826556: step 141420, loss = 0.73 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:18.591606: step 141430, loss = 0.67 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:26:19.357474: step 141440, loss = 0.80 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:26:20.115130: step 141450, loss = 0.73 (1689.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:20.879523: step 141460, loss = 0.78 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:21.642844: step 141470, loss = 0.82 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:22.406192: step 141480, loss = 0.71 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:23.173713: step 141490, loss = 0.61 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:26:23.921393: step 141500, loss = 0.77 (1712.0 examples/sec; 0.075 sec/batch)
2017-05-05 20:26:24.691955: step 141510, loss = 0.70 (1661.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:26:25.458499: step 141520, loss = 0.70 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:26:26.221216: step 141530, loss = 0.51 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:26.982402: step 141540, loss = 0.65 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:27.743020: step 141550, loss = 0.76 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:28.509061: step 141560, loss = 0.72 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:26:29.381708: step 141570, loss = 0.61 (1466.8 examples/sec; 0.087 sec/batch)
2017-05-05 20:26:30.044075: step 141580, loss = 0.71 (1932.5 examples/sec; 0.066 sec/batch)
2017-05-05 20:26:30.807086: step 141590, loss = 0.69 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:31.568899: step 141600, loss = 0.81 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:32.330315: step 141610, loss = 0.74 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:33.092529: step 141620, loss = 0.54 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:33.858169: step 141630, loss = 0.52 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:26:34.625638: step 141640, loss = 0.81 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:26:35.389705: step 141650, loss = 0.65 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:36.153670: step 141660, loss = 0.62 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:36.917192: step 141670, loss = 0.70 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:37.682577: step 141680, loss = 0.79 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:26:38.450787: step 141690, loss = 0.75 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:26:39.220247: step 141700, loss = 0.62 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:26:39.967685: step 141710, loss = 0.62 (1712.5 examples/sec; 0.075 sec/batch)
2017-05-05 20:26:40.737582: step 141720, loss = 0.76 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:26:41.505965: step 141730, loss = 0.69 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:26:42.271276: step 141740, loss = 0.66 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:26:43.037142: step 141750, loss = 0.60 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:26:43.795757: step 141760, loss = 0.65 (1687.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:44.560172: step 141770, loss = 0.78 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:45.325216: step 141780, loss = 0.89 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:26:46.093914: step 141790, loss = 0.67 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:26:46.859986: step 141800, loss = 0.57 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:26:47.623415: step 141810, loss = 0.70 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:48.387582: step 141820, loss = 0.86 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:49.154167: step 141830, loss = 0.61 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:26:49.918324: step 141840, loss = 0.57 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:50.683322: step 141850, loss = 0.71 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:51.446741: step 141860, loss = 0.82 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:52.212835: step 141870, loss = 0.96 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:26:52.977649: step 141880, loss = 0.72 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:53.749314: step 141890, loss = 0.53 (1658.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:26:54.511918: step 141900, loss = 0.68 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:55.272034: step 141910, loss = 0.81 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:56.033060: step 141920, loss = 0.80 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:56.791200: step 141930, loss = 0.74 (1688.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:57.554129: step 141940, loss = 0.77 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:58.317835: step 141950, loss = 0.63 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:26:59.084582: step 141960, loss = 0.66 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:26:59.839922: step 141970, loss = 0.78 (1694.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:00.603485: step 141980, loss = 0.64 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:01.359941: step 141990, loss = 0.64 (1692.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:02.122515: step 142000, loss = 0.62 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:02.886674: step 142010, loss = 0.73 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:03.639216: step 142020, loss = 0.66 (1700.9 examples/sec; 0.075 sec/batch)
2017-05-05 20:27:04.396501: step 142030, loss = 0.77 (1690.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:05.157819: step 142040, loss = 0.69 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:05.920651: step 142050, loss = 0.57 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:06.678621: step 142060, loss = 0.74 (1688.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:07.440690: step 142070, loss = 0.71 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:08.199436: step 142080, loss = 0.77 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:08.962949: step 142090, loss = 0.79 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:09.726955: step 142100, loss = 0.77 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:10.495596: step 142110, loss = 0.75 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:27:11.259996: step 142120, loss = 0.66 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:12.014117: step 142130, loss = 0.74 (1697.3 examples/sec; 0.075 sec/batch)
2017-05-05 20:27:12.773517: step 142140, loss = 0.68 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:13.540061: step 142150, loss = 0.70 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:27:14.302285: step 142160, loss = 0.74 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:15.063886: step 142170, loss = 0.74 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:15.817121: step 142180, loss = 0.70 (1699.3 examples/sec; 0.075 sec/batch)
2017-05-05 20:27:16.581007: step 142190, loss = 0.76 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:17.346599: step 142200, loss = 0.86 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:27:18.115277: step 142210, loss = 0.83 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:27:18.880514: step 142220, loss = 0.86 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:27:19.634803: step 142230, loss = 0.81 (1697.0 examples/sec; 0.075 sec/batch)
2017-05-05 20:27:20.400463: step 142240, loss = 0.71 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:27:21.159102: step 142250, loss = 0.60 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:21.930978: step 142260, loss = 0.71 (1658.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:27:22.704436: step 142270, loss = 0.73 (1654.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:27:23.483995: step 142280, loss = 0.76 (1642.0 examples/sec; 0.078 sec/batch)
2017-05-05 20:27:24.253718: step 142290, loss = 0.65 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:27:25.016970: step 142300, loss = 0.84 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:25.781817: step 142310, loss = 0.76 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:26.548643: step 142320, loss = 0.64 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:27:27.313995: step 142330, loss = 0.74 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:27:28.075168: step 142340, loss = 0.63 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:28.844999: step 142350, loss = 0.70 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:27:29.608403: step 142360, loss = 0.68 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:30.376620: step 142370, loss = 0.69 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:27:31.147741: step 142380, loss = 0.73 (1659.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:27:31.907217: step 142390, loss = 0.63 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:32.666052: step 142400, loss = 0.63 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:33.433719: step 142410, loss = 0.93 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:27:34.193924: step 142420, loss = 0.65 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:34.954981: step 142430, loss = 0.71 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:35.710320: step 142440, loss = 0.80 (1694.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:36.475246: step 142450, loss = 0.84 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:37.245063: step 142460, loss = 0.73 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:27:38.014787: step 142470, loss = 0.77 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:27:38.774238: step 142480, loss = 0.71 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:39.532855: step 142490, loss = 0.82 (1687.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:40.295304: step 142500, loss = 0.73 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:41.056293: step 142510, loss = 0.70 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:41.816666: step 142520, loss = 0.75 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:42.580284: step 142530, loss = 0.70 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:43.350586: step 142540, loss = 0.81 (1661.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:27:44.108064: step 142550, loss = 0.70 (1689.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:44.974524: step 142560, loss = 0.75 (1477.3 examples/sec; 0.087 sec/batch)
2017-05-05 20:27:45.634448: step 142570, loss = 0.62 (1939.6 examples/sec; 0.066 sec/batch)
2017-05-05 20:27:46.400240: step 142580, loss = 0.76 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:27:47.166267: step 142590, loss = 0.81 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:27:47.925998: step 142600, loss = 0.78 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:48.690827: step 142610, loss = 0.60 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:49.455282: step 142620, loss = 0.81 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:50.221952: step 142630, loss = 0.94 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:27:50.982003: step 142640, loss = 0.72 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:51.738287: step 142650, loss = 0.68 (1692.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:52.503706: step 142660, loss = 0.59 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:27:53.264459: step 142670, loss = 0.67 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:54.026522: step 142680, loss = 0.67 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:54.794243: step 142690, loss = 0.90 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:27:55.550392: step 142700, loss = 0.68 (1692.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:56.316261: step 142710, loss = 0.64 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:27:57.080863: step 142720, loss = 0.78 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:27:57.851872: step 142730, loss = 0.77 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:27:58.620661: step 142740, loss = 0.77 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:27:59.385518: step 142750, loss = 0.71 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:28:00.149382: step 142760, loss = 0.70 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:28:00.916022: step 142770, loss = 0.74 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:01.684880: step 142780, loss = 0.74 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:02.473949: step 142790, loss = 0.75 (1622.2 examples/sec; 0.079 sec/batch)
2017-05-05 20:28:03.239305: step 142800, loss = 0.68 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:03.996925: step 142810, loss = 0.76 (1689.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:28:04.761965: step 142820, loss = 0.75 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:05.528556: step 142830, loss = 0.72 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:06.292131: step 142840, loss = 0.60 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:28:07.057105: step 142850, loss = 0.73 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:28:07.820019: step 142860, loss = 0.65 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:28:08.590968: step 142870, loss = 0.66 (1660.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:09.356850: step 142880, loss = 0.82 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:10.122956: step 142890, loss = 0.73 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:10.895773: step 142900, loss = 0.75 (1656.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:11.657105: step 142910, loss = 0.73 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:28:12.423981: step 142920, loss = 0.68 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:13.190817: step 142930, loss = 0.99 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:13.964624: step 142940, loss = 0.68 (1654.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:14.740790: step 142950, loss = 0.69 (1649.1 examples/sec; 0.078 sec/batch)
2017-05-05 20:28:15.508105: step 142960, loss = 0.79 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:16.273457: step 142970, loss = 0.76 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:17.040656: step 142980, loss = 0.61 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:17.810345: step 142990, loss = 0.93 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:18.576134: step 143000, loss = 0.72 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:19.342240: step 143010, loss = 0.67 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:20.101322: step 143020, loss = 0.78 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:28:20.868709: step 143030, loss = 0.66 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:21.641769: step 143040, loss = 0.68 (1655.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:22.410556: step 143050, loss = 0.71 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:23.177741: step 143060, loss = 0.76 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:23.938999: step 143070, loss = 0.74 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:28:24.705314: step 143080, loss = 0.77 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:25.476128: step 143090, loss = 0.65 (1660.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:26.246985: step 143100, loss = 0.70 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:27.017238: step 143110, loss = 0.71 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:27.773730: step 143120, loss = 0.68 (1692.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:28:28.543243: step 143130, loss = 0.66 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:29.311843: step 143140, loss = 0.68 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:30.083271: step 143150, loss = 0.73 (1659.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:30.843586: step 143160, loss = 0.71 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:28:31.608999: step 143170, loss = 0.67 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:32.377219: step 143180, loss = 0.62 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:33.141767: step 143190, loss = 0.81 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:28:33.914116: step 143200, loss = 0.76 (1657.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:34.683142: step 143210, loss = 0.76 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:35.446097: step 143220, loss = 0.70 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:28:36.213670: step 143230, loss = 0.69 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:36.985242: step 143240, loss = 0.66 (1659.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:37.762574: step 143250, loss = 0.65 (1646.6 examples/sec; 0.078 sec/batch)
2017-05-05 20:28:38.526449: step 143260, loss = 0.74 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:28:39.293248: step 143270, loss = 0.75 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:40.057721: step 143280, loss = 0.72 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:28:40.823822: step 143290, loss = 0.78 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:41.582654: step 143300, loss = 0.79 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:28:42.348325: step 143310, loss = 0.77 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:43.113687: step 143320, loss = 0.69 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:43.874146: step 143330, loss = 0.72 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:28:44.631269: step 143340, loss = 0.73 (1690.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:28:45.395745: step 143350, loss = 0.77 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:28:46.161726: step 143360, loss = 0.70 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:46.929653: step 143370, loss = 0.65 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:47.687779: step 143380, loss = 0.65 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:28:48.452138: step 143390, loss = 0.92 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:28:49.217110: step 143400, loss = 0.70 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:28:49.982802: step 143410, loss = 0.68 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:50.743442: step 143420, loss = 0.80 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:28:51.504303: step 143430, loss = 0.64 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:28:52.264260: step 143440, loss = 0.59 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:28:53.029287: step 143450, loss = 0.69 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:53.792834: step 143460, loss = 0.71 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:28:54.556963: step 143470, loss = 0.66 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:28:55.317192: step 143480, loss = 0.66 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:28:56.074637: step 143490, loss = 0.77 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:28:56.835117: step 143500, loss = 0.66 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:28:57.597215: step 143510, loss = 0.67 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:28:58.363863: step 143520, loss = 0.71 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:28:59.128052: step 143530, loss = 0.62 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:28:59.879352: step 143540, loss = 0.75 (1703.7 examples/sec; 0.075 sec/batch)
2017-05-05 20:29:00.742654: step 143550, loss = 0.69 (1482.7 examples/sec; 0.086 sec/batch)
2017-05-05 20:29:01.411569: step 143560, loss = 0.81 (1913.6 examples/sec; 0.067 sec/batch)
2017-05-05 20:29:02.174182: step 143570, loss = 0.75 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:02.939181: step 143580, loss = 0.95 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:29:03.694214: step 143590, loss = 0.85 (1695.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:04.456871: step 143600, loss = 0.71 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:05.224483: step 143610, loss = 0.80 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:29:05.987737: step 143620, loss = 0.68 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:06.745723: step 143630, loss = 0.73 (1688.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:07.507750: step 143640, loss = 0.73 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:08.278519: step 143650, loss = 0.89 (1660.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:29:09.038442: step 143660, loss = 0.60 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:09.799231: step 143670, loss = 0.85 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:10.566933: step 143680, loss = 0.58 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:29:11.331380: step 143690, loss = 0.67 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:12.091012: step 143700, loss = 0.54 (1685.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:12.862982: step 143710, loss = 0.75 (1658.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:29:13.625060: step 143720, loss = 0.67 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:14.388164: step 143730, loss = 0.74 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:15.153348: step 143740, loss = 0.73 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:29:15.911903: step 143750, loss = 0.65 (1687.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:16.674765: step 143760, loss = 0.65 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:17.436835: step 143770, loss = 0.64 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:18.202730: step 143780, loss = 0.56 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:29:18.969973: step 143790, loss = 0.76 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:29:19.729052: step 143800, loss = 0.66 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:20.489426: step 143810, loss = 0.65 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:21.256408: step 143820, loss = 0.78 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:29:22.017075: step 143830, loss = 0.80 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:22.781534: step 143840, loss = 0.66 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:23.544096: step 143850, loss = 0.60 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:24.308110: step 143860, loss = 0.74 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:25.067677: step 143870, loss = 0.79 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:25.836789: step 143880, loss = 0.66 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:29:26.601672: step 143890, loss = 0.77 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:27.361511: step 143900, loss = 0.87 (1684.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:28.120396: step 143910, loss = 0.71 (1686.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:28.879881: step 143920, loss = 0.66 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:29.648404: step 143930, loss = 0.78 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:29:30.409180: step 143940, loss = 0.82 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:31.176425: step 143950, loss = 0.75 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:29:31.930829: step 143960, loss = 0.77 (1696.7 examples/sec; 0.075 sec/batch)
2017-05-05 20:29:32.690401: step 143970, loss = 0.77 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:33.454304: step 143980, loss = 0.77 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:34.214917: step 143990, loss = 0.72 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:34.980488: step 144000, loss = 0.68 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:29:35.732788: step 144010, loss = 0.68 (1701.4 examples/sec; 0.075 sec/batch)
2017-05-05 20:29:36.498933: step 144020, loss = 0.73 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:29:37.268989: step 144030, loss = 0.80 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:29:38.033207: step 144040, loss = 0.80 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:38.795990: step 144050, loss = 0.76 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:39.550005: step 144060, loss = 0.74 (1697.6 examples/sec; 0.075 sec/batch)
2017-05-05 20:29:40.312085: step 144070, loss = 0.73 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:41.080219: step 144080, loss = 0.73 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:29:41.838916: step 144090, loss = 0.86 (1687.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:42.597476: step 144100, loss = 0.67 (1687.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:43.357003: step 144110, loss = 0.69 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:44.112194: step 144120, loss = 0.65 (1694.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:44.877459: step 144130, loss = 0.79 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:29:45.643750: step 144140, loss = 0.69 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:29:46.416249: step 144150, loss = 0.77 (1657.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:29:47.178184: step 144160, loss = 0.75 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:47.930896: step 144170, loss = 0.70 (1700.5 examples/sec; 0.075 sec/batch)
2017-05-05 20:29:48.691303: step 144180, loss = 0.74 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:49.456855: step 144190, loss = 0.65 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:29:50.221311: step 144200, loss = 0.81 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:50.979186: step 144210, loss = 0.74 (1688.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:51.740297: step 144220, loss = 0.56 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:52.507605: step 144230, loss = 0.64 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:29:53.271008: step 144240, loss = 0.83 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:54.035150: step 144250, loss = 0.68 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:54.802585: step 144260, loss = 0.77 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:29:55.561763: step 144270, loss = 0.70 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:56.326572: step 144280, loss = 0.68 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:57.094299: step 144290, loss = 0.62 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:29:57.858948: step 144300, loss = 0.78 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:58.621980: step 144310, loss = 0.68 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:29:59.386418: step 144320, loss = 0.95 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:00.144720: step 144330, loss = 0.83 (1688.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:00.903079: step 144340, loss = 0.90 (1687.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:01.661430: step 144350, loss = 0.69 (1687.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:02.436742: step 144360, loss = 0.78 (1651.0 examples/sec; 0.078 sec/batch)
2017-05-05 20:30:03.205461: step 144370, loss = 0.74 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:30:03.962445: step 144380, loss = 0.71 (1690.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:04.729264: step 144390, loss = 0.77 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:30:05.489443: step 144400, loss = 0.79 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:06.252814: step 144410, loss = 0.66 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:07.016895: step 144420, loss = 0.80 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:07.777722: step 144430, loss = 0.86 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:08.539801: step 144440, loss = 0.61 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:09.313878: step 144450, loss = 0.65 (1653.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:30:10.078767: step 144460, loss = 0.70 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:10.840659: step 144470, loss = 0.83 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:11.596869: step 144480, loss = 0.78 (1692.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:12.357930: step 144490, loss = 0.80 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:13.120860: step 144500, loss = 0.76 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:13.886271: step 144510, loss = 0.84 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:30:14.654724: step 144520, loss = 0.70 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:30:15.408921: step 144530, loss = 0.58 (1697.2 examples/sec; 0.075 sec/batch)
2017-05-05 20:30:16.265098: step 144540, loss = 0.72 (1495.0 examples/sec; 0.086 sec/batch)
2017-05-05 20:30:16.935238: step 144550, loss = 0.70 (1910.1 examples/sec; 0.067 sec/batch)
2017-05-05 20:30:17.700219: step 144560, loss = 0.51 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:18.461830: step 144570, loss = 0.61 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:19.227177: step 144580, loss = 0.57 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:30:19.981829: step 144590, loss = 0.67 (1696.2 examples/sec; 0.075 sec/batch)
2017-05-05 20:30:20.742146: step 144600, loss = 0.62 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:21.510830: step 144610, loss = 0.65 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:30:22.272318: step 144620, loss = 0.81 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:23.038206: step 144630, loss = 0.61 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:30:23.793517: step 144640, loss = 0.71 (1694.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:24.554508: step 144650, loss = 0.74 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:25.320397: step 144660, loss = 0.77 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:30:26.079420: step 144670, loss = 0.69 (1686.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:26.841862: step 144680, loss = 0.74 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:27.601986: step 144690, loss = 0.67 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:28.364479: step 144700, loss = 0.88 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:29.127455: step 144710, loss = 0.70 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:29.887658: step 144720, loss = 0.68 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:30.652050: step 144730, loss = 0.75 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:31.412296: step 144740, loss = 0.77 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:32.170352: step 144750, loss = 0.71 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:32.928539: step 144760, loss = 0.61 (1688.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:33.697291: step 144770, loss = 0.62 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:30:34.466648: step 144780, loss = 0.62 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:30:35.230592: step 144790, loss = 0.77 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:35.986678: step 144800, loss = 0.68 (1692.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:36.749913: step 144810, loss = 0.74 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:37.510662: step 144820, loss = 0.78 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:38.279123: step 144830, loss = 0.67 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:30:39.040323: step 144840, loss = 0.74 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:39.792180: step 144850, loss = 0.64 (1702.4 examples/sec; 0.075 sec/batch)
2017-05-05 20:30:40.548621: step 144860, loss = 0.76 (1692.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:41.317258: step 144870, loss = 0.59 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:30:42.076615: step 144880, loss = 0.83 (1685.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:42.844606: step 144890, loss = 0.64 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:30:43.599112: step 144900, loss = 0.77 (1696.5 examples/sec; 0.075 sec/batch)
2017-05-05 20:30:44.359665: step 144910, loss = 0.75 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:45.122934: step 144920, loss = 0.92 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:45.887105: step 144930, loss = 0.69 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:46.658768: step 144940, loss = 0.70 (1658.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:30:47.416247: step 144950, loss = 0.57 (1689.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:48.178146: step 144960, loss = 0.69 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:48.940728: step 144970, loss = 0.86 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:49.703800: step 144980, loss = 0.69 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:50.466847: step 144990, loss = 0.72 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:51.231979: step 145000, loss = 0.84 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:30:51.986284: step 145010, loss = 0.75 (1696.9 examples/sec; 0.075 sec/batch)
2017-05-05 20:30:52.749024: step 145020, loss = 0.69 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:53.519422: step 145030, loss = 0.85 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:30:54.285978: step 145040, loss = 0.76 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:30:55.051465: step 145050, loss = 0.74 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:30:55.811943: step 145060, loss = 0.79 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:56.578529: step 145070, loss = 0.73 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:30:57.345211: step 145080, loss = 0.71 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:30:58.107609: step 145090, loss = 0.67 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:30:58.877795: step 145100, loss = 0.66 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:30:59.629555: step 145110, loss = 0.75 (1702.7 examples/sec; 0.075 sec/batch)
2017-05-05 20:31:00.395082: step 145120, loss = 0.79 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:01.157311: step 145130, loss = 0.80 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:01.923090: step 145140, loss = 0.68 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:02.685049: step 145150, loss = 0.78 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:03.441532: step 145160, loss = 0.73 (1692.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:04.203762: step 145170, loss = 0.63 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:04.964408: step 145180, loss = 0.68 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:05.727567: step 145190, loss = 0.61 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:06.487269: step 145200, loss = 0.76 (1684.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:07.246885: step 145210, loss = 0.65 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:08.006139: step 145220, loss = 0.73 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:08.767903: step 145230, loss = 0.68 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:09.530303: step 145240, loss = 0.68 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:10.299339: step 145250, loss = 0.69 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:11.062094: step 145260, loss = 0.64 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:11.815076: step 145270, loss = 0.77 (1699.9 examples/sec; 0.075 sec/batch)
2017-05-05 20:31:12.571178: step 145280, loss = 0.68 (1692.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:13.340465: step 145290, loss = 0.94 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:14.111535: step 145300, loss = 0.66 (1660.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:14.885765: step 145310, loss = 0.83 (1653.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:15.646563: step 145320, loss = 0.64 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:16.415561: step 145330, loss = 0.69 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:17.190469: step 145340, loss = 0.67 (1651.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:17.958592: step 145350, loss = 0.58 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:18.732381: step 145360, loss = 0.68 (1654.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:19.491251: step 145370, loss = 0.69 (1686.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:20.261047: step 145380, loss = 0.72 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:21.022473: step 145390, loss = 0.71 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:21.787722: step 145400, loss = 0.65 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:22.554573: step 145410, loss = 0.67 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:23.309788: step 145420, loss = 0.78 (1694.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:24.070259: step 145430, loss = 0.80 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:24.841215: step 145440, loss = 0.87 (1660.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:25.610187: step 145450, loss = 0.62 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:26.381337: step 145460, loss = 0.82 (1659.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:27.147567: step 145470, loss = 0.62 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:27.903082: step 145480, loss = 0.53 (1694.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:28.673232: step 145490, loss = 0.76 (1662.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:29.444422: step 145500, loss = 0.88 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:30.214034: step 145510, loss = 0.69 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:30.977502: step 145520, loss = 0.68 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:31.832219: step 145530, loss = 0.63 (1497.6 examples/sec; 0.085 sec/batch)
2017-05-05 20:31:32.507193: step 145540, loss = 0.81 (1896.4 examples/sec; 0.067 sec/batch)
2017-05-05 20:31:33.274032: step 145550, loss = 0.81 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:34.043957: step 145560, loss = 0.70 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:34.811396: step 145570, loss = 0.68 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:35.573268: step 145580, loss = 0.67 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:36.333035: step 145590, loss = 0.63 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:37.096297: step 145600, loss = 0.76 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:37.861386: step 145610, loss = 0.79 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:38.623068: step 145620, loss = 0.82 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:39.387567: step 145630, loss = 0.74 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:40.153057: step 145640, loss = 0.71 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:40.913632: step 145650, loss = 0.71 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:41.678246: step 145660, loss = 1.00 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:42.445288: step 145670, loss = 0.65 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:43.215152: step 145680, loss = 0.62 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:43.969970: step 145690, loss = 0.73 (1695.8 examples/sec; 0.075 sec/batch)
2017-05-05 20:31:44.729479: step 145700, loss = 0.60 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:45.486978: step 145710, loss = 0.67 (1689.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:46.254964: step 145720, loss = 0.78 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:47.024739: step 145730, loss = 0.83 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:47.776142: step 145740, loss = 0.73 (1703.5 examples/sec; 0.075 sec/batch)
2017-05-05 20:31:48.540748: step 145750, loss = 0.70 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:49.313114: step 145760, loss = 0.69 (1657.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:50.077695: step 145770, loss = 0.79 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:50.835654: step 145780, loss = 0.72 (1688.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:51.591774: step 145790, loss = 0.72 (1692.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:52.344267: step 145800, loss = 0.72 (1701.0 examples/sec; 0.075 sec/batch)
2017-05-05 20:31:53.109696: step 145810, loss = 0.78 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:53.876747: step 145820, loss = 0.60 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:54.637039: step 145830, loss = 0.69 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:55.395083: step 145840, loss = 0.81 (1688.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:56.155386: step 145850, loss = 0.70 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:56.915103: step 145860, loss = 0.57 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:57.684268: step 145870, loss = 0.72 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:58.454740: step 145880, loss = 0.75 (1661.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:31:59.214486: step 145890, loss = 0.81 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:31:59.976715: step 145900, loss = 0.59 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:00.731461: step 145910, loss = 0.67 (1695.9 examples/sec; 0.075 sec/batch)
2017-05-05 20:32:01.494781: step 145920, loss = 0.62 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:02.259898: step 145930, loss = 0.71 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:32:03.024624: step 145940, loss = 0.71 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:03.787012: step 145950, loss = 0.63 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:04.569297: step 145960, loss = 0.69 (1636.2 examples/sec; 0.078 sec/batch)
2017-05-05 20:32:05.341923: step 145970, loss = 0.79 (1656.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:32:06.108477: step 145980, loss = 0.67 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:32:06.871707: step 145990, loss = 0.82 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:07.629826: step 146000, loss = 0.64 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:08.392841: step 146010, loss = 0.74 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:09.153123: step 146020, loss = 0.68 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:09.921794: step 146030, loss = 0.79 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:32:10.688535: step 146040, loss = 0.66 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:32:11.450570: step 146050, loss = 0.74 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:12.215183: step 146060, loss = 0.73 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:12.977897: step 146070, loss = 0.77 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:13.740721: step 146080, loss = 0.79 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:14.505986: step 146090, loss = 0.70 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:32:15.272190: step 146100, loss = 0.73 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:32:16.034697: step 146110, loss = 0.69 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:16.795913: step 146120, loss = 0.73 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:17.570101: step 146130, loss = 0.91 (1653.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:32:18.338815: step 146140, loss = 0.73 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:32:19.108289: step 146150, loss = 0.69 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:32:19.855509: step 146160, loss = 0.92 (1713.0 examples/sec; 0.075 sec/batch)
2017-05-05 20:32:20.619277: step 146170, loss = 0.68 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:21.385407: step 146180, loss = 0.74 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:32:22.146588: step 146190, loss = 0.66 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:22.913930: step 146200, loss = 0.71 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:32:23.675274: step 146210, loss = 0.82 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:24.447106: step 146220, loss = 0.80 (1658.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:32:25.208421: step 146230, loss = 0.71 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:25.977663: step 146240, loss = 0.75 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:32:26.737837: step 146250, loss = 0.65 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:27.501071: step 146260, loss = 0.73 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:28.260104: step 146270, loss = 0.54 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:29.024124: step 146280, loss = 0.74 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:29.792147: step 146290, loss = 0.64 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:32:30.552104: step 146300, loss = 0.92 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:31.313756: step 146310, loss = 0.70 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:32.074631: step 146320, loss = 0.88 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:32.849077: step 146330, loss = 0.76 (1652.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:32:33.611936: step 146340, loss = 0.71 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:34.381161: step 146350, loss = 0.77 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:32:35.145628: step 146360, loss = 0.57 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:35.903989: step 146370, loss = 0.63 (1687.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:36.670614: step 146380, loss = 0.66 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:32:37.439874: step 146390, loss = 0.71 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:32:38.207876: step 146400, loss = 0.68 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:32:38.978127: step 146410, loss = 0.73 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:32:39.739760: step 146420, loss = 0.60 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:40.524216: step 146430, loss = 0.62 (1631.7 examples/sec; 0.078 sec/batch)
2017-05-05 20:32:41.304032: step 146440, loss = 0.71 (1641.4 examples/sec; 0.078 sec/batch)
2017-05-05 20:32:42.065974: step 146450, loss = 0.59 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:42.830331: step 146460, loss = 0.54 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:43.585676: step 146470, loss = 0.70 (1694.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:44.356084: step 146480, loss = 0.70 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:32:45.120171: step 146490, loss = 0.74 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:45.955860: step 146500, loss = 0.65 (1531.7 examples/sec; 0.084 sec/batch)
2017-05-05 20:32:46.786340: step 146510, loss = 0.65 (1541.3 examples/sec; 0.083 sec/batch)
2017-05-05 20:32:47.651860: step 146520, loss = 0.82 (1478.9 examples/sec; 0.087 sec/batch)
2017-05-05 20:32:48.315955: step 146530, loss = 0.63 (1927.4 examples/sec; 0.066 sec/batch)
2017-05-05 20:32:49.078272: step 146540, loss = 0.75 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:49.848463: step 146550, loss = 0.80 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:32:50.609309: step 146560, loss = 0.62 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:51.370280: step 146570, loss = 0.71 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:52.127942: step 146580, loss = 0.68 (1689.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:52.890944: step 146590, loss = 0.71 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:53.655102: step 146600, loss = 0.79 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:54.426693: step 146610, loss = 0.86 (1658.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:32:55.191734: step 146620, loss = 0.83 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:32:55.946703: step 146630, loss = 0.73 (1695.4 examples/sec; 0.075 sec/batch)
2017-05-05 20:32:56.712796: step 146640, loss = 0.68 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:32:57.483257: step 146650, loss = 0.59 (1661.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:32:58.244084: step 146660, loss = 0.63 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:32:59.010238: step 146670, loss = 0.69 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:32:59.758734: step 146680, loss = 0.73 (1710.1 examples/sec; 0.075 sec/batch)
2017-05-05 20:33:00.522293: step 146690, loss = 0.91 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:01.286430: step 146700, loss = 0.67 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:02.051202: step 146710, loss = 0.69 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:02.815532: step 146720, loss = 0.64 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:03.573179: step 146730, loss = 0.59 (1689.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:04.335459: step 146740, loss = 0.66 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:05.102079: step 146750, loss = 0.81 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:05.864680: step 146760, loss = 0.78 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:06.633136: step 146770, loss = 0.75 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:07.397405: step 146780, loss = 0.78 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:08.159561: step 146790, loss = 0.71 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:08.924528: step 146800, loss = 0.79 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:09.689691: step 146810, loss = 0.65 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:10.450368: step 146820, loss = 0.72 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:11.219024: step 146830, loss = 0.84 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:11.974456: step 146840, loss = 0.72 (1694.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:12.743755: step 146850, loss = 0.86 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:13.510950: step 146860, loss = 0.66 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:14.286849: step 146870, loss = 0.72 (1649.7 examples/sec; 0.078 sec/batch)
2017-05-05 20:33:15.055747: step 146880, loss = 0.69 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:15.812798: step 146890, loss = 0.65 (1690.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:16.582648: step 146900, loss = 0.70 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:17.347329: step 146910, loss = 0.79 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:18.114308: step 146920, loss = 0.71 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:18.878716: step 146930, loss = 0.64 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:19.638351: step 146940, loss = 0.72 (1685.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:20.404048: step 146950, loss = 0.75 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:21.163515: step 146960, loss = 0.78 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:21.925279: step 146970, loss = 0.70 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:22.692881: step 146980, loss = 0.58 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:23.453626: step 146990, loss = 0.94 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:24.219777: step 147000, loss = 0.88 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:24.982079: step 147010, loss = 0.76 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:25.747135: step 147020, loss = 0.75 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:26.519760: step 147030, loss = 0.63 (1656.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:27.284872: step 147040, loss = 0.72 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:28.049884: step 147050, loss = 0.76 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:28.818445: step 147060, loss = 0.72 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:29.587582: step 147070, loss = 0.56 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:30.351530: step 147080, loss = 0.69 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:31.117174: step 147090, loss = 0.77 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:31.871795: step 147100, loss = 0.75 (1696.2 examples/sec; 0.075 sec/batch)
2017-05-05 20:33:32.628238: step 147110, loss = 0.76 (1692.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:33.399943: step 147120, loss = 0.62 (1658.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:34.167844: step 147130, loss = 0.74 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:34.929792: step 147140, loss = 0.75 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:35.684782: step 147150, loss = 0.73 (1695.4 examples/sec; 0.075 sec/batch)
2017-05-05 20:33:36.457018: step 147160, loss = 0.65 (1657.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:37.224454: step 147170, loss = 0.87 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:37.990836: step 147180, loss = 0.87 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:38.753592: step 147190, loss = 0.91 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:39.518391: step 147200, loss = 0.78 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:40.280683: step 147210, loss = 0.72 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:41.044712: step 147220, loss = 0.60 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:41.812556: step 147230, loss = 0.60 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:42.575654: step 147240, loss = 0.73 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:43.337557: step 147250, loss = 0.76 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:44.092647: step 147260, loss = 0.61 (1695.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:44.859049: step 147270, loss = 0.76 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:45.623577: step 147280, loss = 0.88 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:46.394324: step 147290, loss = 0.54 (1660.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:47.160324: step 147300, loss = 1.00 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:47.919169: step 147310, loss = 0.75 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:48.686751: step 147320, loss = 0.63 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:49.453281: step 147330, loss = 0.64 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:50.227090: step 147340, loss = 0.79 (1654.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:50.979586: step 147350, loss = 0.76 (1701.0 examples/sec; 0.075 sec/batch)
2017-05-05 20:33:51.741746: step 147360, loss = 0.81 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:52.499332: step 147370, loss = 0.78 (1689.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:53.268429: step 147380, loss = 0.66 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:54.036093: step 147390, loss = 0.73 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:54.803857: step 147400, loss = 0.98 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:55.559642: step 147410, loss = 0.52 (1693.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:56.328433: step 147420, loss = 0.64 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:57.088315: step 147430, loss = 0.64 (1684.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:33:57.855685: step 147440, loss = 0.71 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:58.623698: step 147450, loss = 0.63 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:33:59.386050: step 147460, loss = 0.72 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:34:00.147445: step 147470, loss = 0.73 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:34:00.916820: step 147480, loss = 0.75 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:01.682934: step 147490, loss = 0.77 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:02.452113: step 147500, loss = 0.68 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:03.314156: step 147510, loss = 0.56 (1484.8 examples/sec; 0.086 sec/batch)
2017-05-05 20:34:03.980314: step 147520, loss = 0.72 (1921.5 examples/sec; 0.067 sec/batch)
2017-05-05 20:34:04.740345: step 147530, loss = 0.77 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:34:05.505633: step 147540, loss = 0.71 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:06.271263: step 147550, loss = 0.67 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:07.032476: step 147560, loss = 0.74 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:34:07.786253: step 147570, loss = 0.85 (1698.1 examples/sec; 0.075 sec/batch)
2017-05-05 20:34:08.555342: step 147580, loss = 0.64 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:09.317391: step 147590, loss = 0.74 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:34:10.083536: step 147600, loss = 0.69 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:10.843987: step 147610, loss = 0.64 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:34:11.603326: step 147620, loss = 0.75 (1685.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:34:12.367238: step 147630, loss = 0.82 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:34:13.134542: step 147640, loss = 0.81 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:13.899874: step 147650, loss = 0.71 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:14.666999: step 147660, loss = 0.76 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:15.424105: step 147670, loss = 0.73 (1690.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:34:16.178592: step 147680, loss = 0.84 (1696.5 examples/sec; 0.075 sec/batch)
2017-05-05 20:34:16.938653: step 147690, loss = 0.82 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:34:17.710516: step 147700, loss = 0.60 (1658.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:18.482007: step 147710, loss = 0.65 (1659.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:19.249453: step 147720, loss = 0.84 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:20.009485: step 147730, loss = 0.73 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:34:20.774613: step 147740, loss = 0.71 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:21.545736: step 147750, loss = 0.70 (1659.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:22.308227: step 147760, loss = 0.63 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:34:23.077657: step 147770, loss = 0.74 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:23.831106: step 147780, loss = 0.84 (1698.9 examples/sec; 0.075 sec/batch)
2017-05-05 20:34:24.604339: step 147790, loss = 0.61 (1655.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:25.368035: step 147800, loss = 0.75 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:34:26.135194: step 147810, loss = 0.50 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:26.900609: step 147820, loss = 0.72 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:27.657818: step 147830, loss = 0.79 (1690.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:34:28.429494: step 147840, loss = 0.76 (1658.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:29.197701: step 147850, loss = 0.79 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:29.967374: step 147860, loss = 0.63 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:30.738273: step 147870, loss = 0.76 (1660.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:31.498770: step 147880, loss = 0.68 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:34:32.263242: step 147890, loss = 0.85 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:34:33.031341: step 147900, loss = 0.65 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:33.799768: step 147910, loss = 0.81 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:34.563017: step 147920, loss = 0.78 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:34:35.325112: step 147930, loss = 0.74 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:34:36.079105: step 147940, loss = 0.82 (1697.6 examples/sec; 0.075 sec/batch)
2017-05-05 20:34:36.848918: step 147950, loss = 0.75 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:37.616422: step 147960, loss = 0.94 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:38.377091: step 147970, loss = 0.74 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:34:39.139526: step 147980, loss = 0.62 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:34:39.891293: step 147990, loss = 0.84 (1702.6 examples/sec; 0.075 sec/batch)
2017-05-05 20:34:40.659254: step 148000, loss = 0.68 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:41.421487: step 148010, loss = 0.62 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:34:42.188055: step 148020, loss = 0.67 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:42.945730: step 148030, loss = 0.67 (1689.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:34:43.707051: step 148040, loss = 0.61 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:34:44.470248: step 148050, loss = 0.94 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:34:45.236383: step 148060, loss = 0.74 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:45.999785: step 148070, loss = 0.68 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:34:46.758319: step 148080, loss = 0.66 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:34:47.517143: step 148090, loss = 0.70 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:34:48.277981: step 148100, loss = 0.63 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:34:49.050950: step 148110, loss = 0.83 (1656.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:49.815990: step 148120, loss = 0.80 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:50.580159: step 148130, loss = 0.57 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:34:51.345857: step 148140, loss = 0.71 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:52.102660: step 148150, loss = 0.66 (1691.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:34:52.869849: step 148160, loss = 0.68 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:53.637047: step 148170, loss = 0.65 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:54.402022: step 148180, loss = 0.74 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:34:55.167923: step 148190, loss = 0.59 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:55.918073: step 148200, loss = 0.90 (1706.3 examples/sec; 0.075 sec/batch)
2017-05-05 20:34:56.686427: step 148210, loss = 0.73 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:57.454622: step 148220, loss = 0.70 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:58.215245: step 148230, loss = 0.62 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:34:58.984511: step 148240, loss = 0.81 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:34:59.747753: step 148250, loss = 0.84 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:00.509491: step 148260, loss = 0.85 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:01.276414: step 148270, loss = 0.68 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:35:02.045715: step 148280, loss = 0.81 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:35:02.807471: step 148290, loss = 0.76 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:03.568272: step 148300, loss = 0.65 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:04.332919: step 148310, loss = 0.82 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:05.098594: step 148320, loss = 0.65 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:35:05.860057: step 148330, loss = 0.62 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:06.621613: step 148340, loss = 0.71 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:07.384985: step 148350, loss = 0.86 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:08.142133: step 148360, loss = 0.73 (1690.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:08.912348: step 148370, loss = 0.69 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:35:09.689785: step 148380, loss = 0.79 (1646.4 examples/sec; 0.078 sec/batch)
2017-05-05 20:35:10.456746: step 148390, loss = 0.73 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:35:11.214174: step 148400, loss = 0.65 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:11.978690: step 148410, loss = 0.81 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:12.745275: step 148420, loss = 0.75 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:35:13.515627: step 148430, loss = 0.74 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:35:14.277672: step 148440, loss = 0.61 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:15.042593: step 148450, loss = 0.74 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:15.801504: step 148460, loss = 0.67 (1686.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:16.564236: step 148470, loss = 0.65 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:17.330422: step 148480, loss = 0.72 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:35:18.099062: step 148490, loss = 0.72 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:35:18.966406: step 148500, loss = 0.60 (1475.8 examples/sec; 0.087 sec/batch)
2017-05-05 20:35:19.623800: step 148510, loss = 0.78 (1947.1 examples/sec; 0.066 sec/batch)
2017-05-05 20:35:20.390728: step 148520, loss = 0.78 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:35:21.149462: step 148530, loss = 0.73 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:21.915337: step 148540, loss = 0.72 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:35:22.679389: step 148550, loss = 0.75 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:23.441805: step 148560, loss = 0.76 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:24.201611: step 148570, loss = 0.77 (1684.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:24.965556: step 148580, loss = 0.59 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:25.730918: step 148590, loss = 0.78 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:35:26.495627: step 148600, loss = 0.65 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:27.257574: step 148610, loss = 0.78 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:28.011628: step 148620, loss = 0.65 (1697.5 examples/sec; 0.075 sec/batch)
2017-05-05 20:35:28.775493: step 148630, loss = 0.79 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:29.537926: step 148640, loss = 0.84 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:30.302143: step 148650, loss = 0.77 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:31.062632: step 148660, loss = 0.65 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:31.824933: step 148670, loss = 0.71 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:32.585883: step 148680, loss = 0.61 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:33.350547: step 148690, loss = 0.66 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:34.117237: step 148700, loss = 0.50 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:35:34.884426: step 148710, loss = 0.73 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:35:35.638260: step 148720, loss = 0.87 (1698.0 examples/sec; 0.075 sec/batch)
2017-05-05 20:35:36.404215: step 148730, loss = 0.76 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:35:37.173421: step 148740, loss = 0.67 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:35:37.936727: step 148750, loss = 0.67 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:38.695175: step 148760, loss = 0.69 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:39.454522: step 148770, loss = 0.83 (1685.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:40.215598: step 148780, loss = 0.64 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:40.977686: step 148790, loss = 0.68 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:41.739796: step 148800, loss = 0.69 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:42.504009: step 148810, loss = 0.69 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:43.268914: step 148820, loss = 0.71 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:44.022475: step 148830, loss = 0.78 (1698.6 examples/sec; 0.075 sec/batch)
2017-05-05 20:35:44.788527: step 148840, loss = 0.68 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:35:45.546461: step 148850, loss = 0.82 (1688.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:46.309576: step 148860, loss = 0.85 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:47.081269: step 148870, loss = 0.78 (1658.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:35:47.834319: step 148880, loss = 0.76 (1699.8 examples/sec; 0.075 sec/batch)
2017-05-05 20:35:48.594856: step 148890, loss = 0.76 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:49.364334: step 148900, loss = 0.70 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:35:50.129313: step 148910, loss = 0.79 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:50.895043: step 148920, loss = 0.78 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:35:51.654175: step 148930, loss = 0.60 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:52.425057: step 148940, loss = 0.72 (1660.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:35:53.191093: step 148950, loss = 0.68 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:35:53.960202: step 148960, loss = 0.72 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:35:54.726294: step 148970, loss = 0.75 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:35:55.495872: step 148980, loss = 0.70 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:35:56.257553: step 148990, loss = 0.81 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:57.018383: step 149000, loss = 0.69 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:57.781073: step 149010, loss = 0.81 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:35:58.551746: step 149020, loss = 0.66 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:35:59.313394: step 149030, loss = 0.54 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:00.071465: step 149040, loss = 0.55 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:00.828278: step 149050, loss = 0.95 (1691.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:01.599756: step 149060, loss = 0.71 (1659.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:02.363157: step 149070, loss = 0.80 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:03.137126: step 149080, loss = 0.85 (1653.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:03.891997: step 149090, loss = 0.70 (1695.6 examples/sec; 0.075 sec/batch)
2017-05-05 20:36:04.658937: step 149100, loss = 0.66 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:05.425668: step 149110, loss = 0.66 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:06.233047: step 149120, loss = 0.77 (1585.4 examples/sec; 0.081 sec/batch)
2017-05-05 20:36:07.001805: step 149130, loss = 0.65 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:07.758716: step 149140, loss = 0.70 (1691.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:08.526685: step 149150, loss = 0.66 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:09.288031: step 149160, loss = 0.73 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:10.054688: step 149170, loss = 0.64 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:10.819275: step 149180, loss = 0.67 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:11.579106: step 149190, loss = 0.79 (1684.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:12.344946: step 149200, loss = 0.66 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:13.106248: step 149210, loss = 0.70 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:13.873748: step 149220, loss = 0.67 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:14.642976: step 149230, loss = 0.83 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:15.406075: step 149240, loss = 0.67 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:16.169863: step 149250, loss = 0.70 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:16.934473: step 149260, loss = 0.70 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:17.701793: step 149270, loss = 0.67 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:18.470617: step 149280, loss = 0.80 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:19.247588: step 149290, loss = 0.68 (1647.4 examples/sec; 0.078 sec/batch)
2017-05-05 20:36:20.007987: step 149300, loss = 0.67 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:20.771725: step 149310, loss = 0.67 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:21.534400: step 149320, loss = 0.63 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:22.304773: step 149330, loss = 0.70 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:23.072906: step 149340, loss = 0.72 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:23.829657: step 149350, loss = 0.49 (1691.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:24.599138: step 149360, loss = 0.67 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:25.361489: step 149370, loss = 0.84 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:26.128355: step 149380, loss = 0.70 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:26.892923: step 149390, loss = 0.72 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:27.654439: step 149400, loss = 0.73 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:28.419335: step 149410, loss = 0.65 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:29.190855: step 149420, loss = 0.87 (1659.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:29.953079: step 149430, loss = 0.80 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:30.715402: step 149440, loss = 0.87 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:31.474532: step 149450, loss = 0.70 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:32.239750: step 149460, loss = 0.65 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:33.001672: step 149470, loss = 0.73 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:33.765779: step 149480, loss = 0.72 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:34.633745: step 149490, loss = 0.75 (1474.7 examples/sec; 0.087 sec/batch)
2017-05-05 20:36:35.298829: step 149500, loss = 0.73 (1924.6 examples/sec; 0.067 sec/batch)
2017-05-05 20:36:36.054075: step 149510, loss = 0.71 (1694.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:36.820602: step 149520, loss = 0.76 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:37.594904: step 149530, loss = 0.81 (1653.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:38.361518: step 149540, loss = 0.72 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:39.129629: step 149550, loss = 0.64 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:39.885323: step 149560, loss = 0.77 (1693.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:40.653498: step 149570, loss = 0.75 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:41.420464: step 149580, loss = 0.72 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:42.182296: step 149590, loss = 0.70 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:42.943872: step 149600, loss = 0.77 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:43.699189: step 149610, loss = 0.83 (1694.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:44.463293: step 149620, loss = 0.62 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:45.229660: step 149630, loss = 0.59 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:45.991918: step 149640, loss = 0.57 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:46.756596: step 149650, loss = 0.80 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:47.512337: step 149660, loss = 0.68 (1693.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:48.279148: step 149670, loss = 0.65 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:49.049334: step 149680, loss = 0.73 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:49.823969: step 149690, loss = 0.80 (1652.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:50.582970: step 149700, loss = 0.78 (1686.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:51.345719: step 149710, loss = 0.57 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:52.106890: step 149720, loss = 0.73 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:52.869506: step 149730, loss = 0.74 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:53.638809: step 149740, loss = 0.86 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:54.411234: step 149750, loss = 0.66 (1657.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:55.180341: step 149760, loss = 0.64 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:55.936479: step 149770, loss = 0.84 (1692.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:56.700872: step 149780, loss = 0.69 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:57.475493: step 149790, loss = 0.62 (1652.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:58.232903: step 149800, loss = 0.84 (1690.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:36:59.005036: step 149810, loss = 0.71 (1657.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:36:59.761278: step 149820, loss = 0.81 (1692.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:00.525293: step 149830, loss = 0.85 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:01.291628: step 149840, loss = 0.70 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:02.048849: step 149850, loss = 0.84 (1690.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:02.815940: step 149860, loss = 0.63 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:03.571103: step 149870, loss = 0.73 (1695.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:04.340102: step 149880, loss = 0.77 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:05.106067: step 149890, loss = 0.66 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:05.867967: step 149900, loss = 0.74 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:06.635265: step 149910, loss = 0.82 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:07.398587: step 149920, loss = 0.64 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:08.160201: step 149930, loss = 0.64 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:08.929747: step 149940, loss = 0.69 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:09.698568: step 149950, loss = 0.66 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:10.455583: step 149960, loss = 0.76 (1690.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:11.224853: step 149970, loss = 0.76 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:11.979087: step 149980, loss = 0.79 (1697.1 examples/sec; 0.075 sec/batch)
2017-05-05 20:37:12.750341: step 149990, loss = 0.66 (1659.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:13.517968: step 150000, loss = 0.72 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:14.291230: step 150010, loss = 0.69 (1655.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:15.059403: step 150020, loss = 0.69 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:15.822308: step 150030, loss = 0.66 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:16.586937: step 150040, loss = 0.82 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:17.353231: step 150050, loss = 0.83 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:18.121725: step 150060, loss = 0.52 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:18.888662: step 150070, loss = 0.91 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:19.640881: step 150080, loss = 0.71 (1701.6 examples/sec; 0.075 sec/batch)
2017-05-05 20:37:20.412644: step 150090, loss = 0.68 (1658.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:21.181266: step 150100, loss = 0.62 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:21.950925: step 150110, loss = 0.63 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:22.713653: step 150120, loss = 0.68 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:23.477411: step 150130, loss = 0.64 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:24.239575: step 150140, loss = 0.77 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:24.996913: step 150150, loss = 0.66 (1690.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:25.762230: step 150160, loss = 0.87 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:26.526233: step 150170, loss = 0.84 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:27.289956: step 150180, loss = 0.85 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:28.046593: step 150190, loss = 0.80 (1691.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:28.814588: step 150200, loss = 0.78 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:29.583265: step 150210, loss = 0.71 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:30.344111: step 150220, loss = 0.68 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:31.110401: step 150230, loss = 0.67 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:31.861516: step 150240, loss = 0.67 (1704.1 examples/sec; 0.075 sec/batch)
2017-05-05 20:37:32.626880: step 150250, loss = 0.71 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:33.394339: step 150260, loss = 0.91 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:34.159217: step 150270, loss = 0.78 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:34.925995: step 150280, loss = 0.87 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:35.685923: step 150290, loss = 0.66 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:36.467811: step 150300, loss = 0.76 (1637.1 examples/sec; 0.078 sec/batch)
2017-05-05 20:37:37.235983: step 150310, loss = 0.73 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:38.002149: step 150320, loss = 0.73 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:38.768657: step 150330, loss = 0.69 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:39.531092: step 150340, loss = 0.67 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:40.294241: step 150350, loss = 0.79 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:41.054803: step 150360, loss = 0.74 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:41.823499: step 150370, loss = 0.71 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:42.583670: step 150380, loss = 0.69 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:43.344902: step 150390, loss = 0.66 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:44.107158: step 150400, loss = 0.83 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:44.869413: step 150410, loss = 0.82 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:45.636057: step 150420, loss = 0.78 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:46.412861: step 150430, loss = 0.68 (1647.8 examples/sec; 0.078 sec/batch)
2017-05-05 20:37:47.182719: step 150440, loss = 0.73 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:47.933304: step 150450, loss = 0.76 (1705.3 examples/sec; 0.075 sec/batch)
2017-05-05 20:37:48.695893: step 150460, loss = 0.71 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:49.462754: step 150470, loss = 0.63 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:50.332998: step 150480, loss = 0.89 (1470.8 examples/sec; 0.087 sec/batch)
2017-05-05 20:37:50.991652: step 150490, loss = 0.76 (1943.4 examples/sec; 0.066 sec/batch)
2017-05-05 20:37:51.748425: step 150500, loss = 0.52 (1691.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:52.514688: step 150510, loss = 0.77 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:53.278469: step 150520, loss = 0.60 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:54.038628: step 150530, loss = 0.66 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:54.806775: step 150540, loss = 0.70 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:55.563273: step 150550, loss = 0.63 (1692.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:56.333128: step 150560, loss = 0.71 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:57.098850: step 150570, loss = 0.61 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:57.869995: step 150580, loss = 0.68 (1659.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:37:58.631061: step 150590, loss = 0.71 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:37:59.399408: step 150600, loss = 0.76 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:00.158292: step 150610, loss = 0.65 (1686.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:00.926513: step 150620, loss = 0.60 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:01.686624: step 150630, loss = 0.81 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:02.452122: step 150640, loss = 0.72 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:03.224656: step 150650, loss = 0.79 (1656.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:03.974229: step 150660, loss = 0.65 (1707.6 examples/sec; 0.075 sec/batch)
2017-05-05 20:38:04.736892: step 150670, loss = 0.81 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:05.503872: step 150680, loss = 0.91 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:06.269205: step 150690, loss = 0.71 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:07.034449: step 150700, loss = 0.73 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:07.791097: step 150710, loss = 0.62 (1691.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:08.553846: step 150720, loss = 0.71 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:09.321607: step 150730, loss = 0.58 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:10.095174: step 150740, loss = 0.71 (1654.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:10.867559: step 150750, loss = 0.61 (1657.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:11.624347: step 150760, loss = 0.79 (1691.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:12.387648: step 150770, loss = 0.69 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:13.150427: step 150780, loss = 0.58 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:13.909558: step 150790, loss = 0.69 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:14.679208: step 150800, loss = 0.88 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:15.434588: step 150810, loss = 0.68 (1694.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:16.195727: step 150820, loss = 0.70 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:16.962498: step 150830, loss = 0.79 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:17.728322: step 150840, loss = 0.83 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:18.498646: step 150850, loss = 0.75 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:19.264247: step 150860, loss = 0.61 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:20.023896: step 150870, loss = 0.67 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:20.791411: step 150880, loss = 0.63 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:21.559405: step 150890, loss = 0.63 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:22.325370: step 150900, loss = 0.49 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:23.084103: step 150910, loss = 0.58 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:23.837063: step 150920, loss = 0.73 (1699.9 examples/sec; 0.075 sec/batch)
2017-05-05 20:38:24.603862: step 150930, loss = 0.69 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:25.369601: step 150940, loss = 0.70 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:26.132924: step 150950, loss = 0.63 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:26.893822: step 150960, loss = 0.77 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:27.649906: step 150970, loss = 0.68 (1692.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:28.420642: step 150980, loss = 0.95 (1660.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:29.184517: step 150990, loss = 0.77 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:29.950974: step 151000, loss = 0.75 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:30.714147: step 151010, loss = 0.78 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:31.476427: step 151020, loss = 0.62 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:32.239612: step 151030, loss = 0.77 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:33.002778: step 151040, loss = 0.82 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:33.764229: step 151050, loss = 0.63 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:34.527444: step 151060, loss = 0.56 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:35.289531: step 151070, loss = 0.70 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:36.049801: step 151080, loss = 0.71 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:36.810257: step 151090, loss = 0.71 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:37.578245: step 151100, loss = 0.65 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:38.345089: step 151110, loss = 0.79 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:39.111053: step 151120, loss = 0.67 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:39.866696: step 151130, loss = 0.73 (1693.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:40.622278: step 151140, loss = 0.50 (1694.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:41.385089: step 151150, loss = 0.94 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:42.143868: step 151160, loss = 0.72 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:42.915698: step 151170, loss = 0.64 (1658.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:43.674916: step 151180, loss = 0.82 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:44.443787: step 151190, loss = 0.88 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:45.211742: step 151200, loss = 0.55 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:45.969129: step 151210, loss = 0.59 (1690.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:46.735044: step 151220, loss = 0.69 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:47.490647: step 151230, loss = 0.72 (1694.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:48.250643: step 151240, loss = 0.68 (1684.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:49.013490: step 151250, loss = 0.61 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:49.778544: step 151260, loss = 0.79 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:50.536219: step 151270, loss = 0.83 (1689.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:51.307849: step 151280, loss = 0.77 (1658.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:52.067792: step 151290, loss = 0.72 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:52.840277: step 151300, loss = 0.73 (1657.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:53.601772: step 151310, loss = 0.80 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:54.373249: step 151320, loss = 0.69 (1659.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:55.138918: step 151330, loss = 0.83 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:55.898803: step 151340, loss = 0.78 (1684.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:56.662562: step 151350, loss = 0.65 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:57.425887: step 151360, loss = 0.74 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:58.187039: step 151370, loss = 0.78 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:38:58.954322: step 151380, loss = 0.69 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:38:59.706318: step 151390, loss = 0.79 (1702.1 examples/sec; 0.075 sec/batch)
2017-05-05 20:39:00.463639: step 151400, loss = 0.74 (1690.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:01.233489: step 151410, loss = 0.66 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:39:01.996706: step 151420, loss = 0.62 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:02.760731: step 151430, loss = 0.75 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:03.520993: step 151440, loss = 0.67 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:04.292091: step 151450, loss = 0.86 (1660.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:39:05.073796: step 151460, loss = 0.74 (1637.4 examples/sec; 0.078 sec/batch)
2017-05-05 20:39:05.933835: step 151470, loss = 0.63 (1488.3 examples/sec; 0.086 sec/batch)
2017-05-05 20:39:06.609838: step 151480, loss = 0.66 (1893.5 examples/sec; 0.068 sec/batch)
2017-05-05 20:39:07.377163: step 151490, loss = 0.70 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:39:08.151074: step 151500, loss = 0.96 (1653.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:39:08.917529: step 151510, loss = 0.64 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:39:09.686341: step 151520, loss = 0.82 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:39:10.449867: step 151530, loss = 0.68 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:11.219277: step 151540, loss = 0.68 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:39:11.977022: step 151550, loss = 0.67 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:12.740272: step 151560, loss = 0.77 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:13.503563: step 151570, loss = 0.66 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:14.268041: step 151580, loss = 0.61 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:15.035962: step 151590, loss = 0.66 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:39:15.784955: step 151600, loss = 0.80 (1709.0 examples/sec; 0.075 sec/batch)
2017-05-05 20:39:16.558022: step 151610, loss = 0.74 (1655.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:39:17.321344: step 151620, loss = 0.81 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:18.085685: step 151630, loss = 0.67 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:18.851236: step 151640, loss = 0.73 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:39:19.611049: step 151650, loss = 0.77 (1684.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:20.378926: step 151660, loss = 0.68 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:39:21.145330: step 151670, loss = 0.79 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:39:21.905468: step 151680, loss = 0.58 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:22.668890: step 151690, loss = 0.64 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:23.431110: step 151700, loss = 0.75 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:24.190436: step 151710, loss = 0.64 (1685.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:24.955727: step 151720, loss = 0.80 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:39:25.723652: step 151730, loss = 0.83 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:39:26.492059: step 151740, loss = 0.60 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:39:27.255096: step 151750, loss = 0.63 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:28.011806: step 151760, loss = 0.77 (1691.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:28.772821: step 151770, loss = 0.68 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:29.549902: step 151780, loss = 0.66 (1647.2 examples/sec; 0.078 sec/batch)
2017-05-05 20:39:30.313492: step 151790, loss = 0.78 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:31.075507: step 151800, loss = 0.76 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:31.832837: step 151810, loss = 0.54 (1690.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:32.601197: step 151820, loss = 0.81 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:39:33.371627: step 151830, loss = 0.83 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:39:34.134381: step 151840, loss = 0.81 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:34.910036: step 151850, loss = 0.61 (1650.2 examples/sec; 0.078 sec/batch)
2017-05-05 20:39:35.661419: step 151860, loss = 0.91 (1703.5 examples/sec; 0.075 sec/batch)
2017-05-05 20:39:36.427763: step 151870, loss = 0.70 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:39:37.191763: step 151880, loss = 0.65 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:37.955239: step 151890, loss = 0.70 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:38.721022: step 151900, loss = 0.69 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:39:39.483857: step 151910, loss = 0.77 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:40.242496: step 151920, loss = 0.67 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:41.002006: step 151930, loss = 0.88 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:41.765398: step 151940, loss = 0.75 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:42.529487: step 151950, loss = 0.80 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:43.294218: step 151960, loss = 0.68 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:44.048537: step 151970, loss = 0.87 (1696.9 examples/sec; 0.075 sec/batch)
2017-05-05 20:39:44.809195: step 151980, loss = 0.74 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:45.575986: step 151990, loss = 0.73 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:39:46.337792: step 152000, loss = 0.64 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:47.106328: step 152010, loss = 0.72 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:39:47.864695: step 152020, loss = 0.71 (1687.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:48.625810: step 152030, loss = 0.72 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:49.389576: step 152040, loss = 0.68 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:50.153729: step 152050, loss = 0.56 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:50.916343: step 152060, loss = 0.78 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:51.676491: step 152070, loss = 0.84 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:52.445390: step 152080, loss = 0.72 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:39:53.217908: step 152090, loss = 0.61 (1656.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:39:53.980478: step 152100, loss = 0.65 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:54.750903: step 152110, loss = 0.76 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:39:55.507286: step 152120, loss = 0.65 (1692.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:56.268418: step 152130, loss = 0.79 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:39:57.035340: step 152140, loss = 0.71 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:39:57.805443: step 152150, loss = 0.87 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:39:58.576080: step 152160, loss = 0.77 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:39:59.342581: step 152170, loss = 0.76 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:00.098280: step 152180, loss = 0.70 (1693.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:00.862148: step 152190, loss = 0.83 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:01.634849: step 152200, loss = 0.85 (1656.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:02.406084: step 152210, loss = 0.60 (1659.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:03.174403: step 152220, loss = 0.54 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:03.927837: step 152230, loss = 0.70 (1698.9 examples/sec; 0.075 sec/batch)
2017-05-05 20:40:04.692839: step 152240, loss = 0.81 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:05.453715: step 152250, loss = 1.01 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:06.223051: step 152260, loss = 0.67 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:06.995661: step 152270, loss = 0.55 (1656.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:07.754765: step 152280, loss = 0.57 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:08.518338: step 152290, loss = 0.68 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:09.290088: step 152300, loss = 0.68 (1658.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:10.054268: step 152310, loss = 0.69 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:10.818156: step 152320, loss = 0.78 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:11.581970: step 152330, loss = 0.71 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:12.345467: step 152340, loss = 0.74 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:13.103513: step 152350, loss = 0.79 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:13.871077: step 152360, loss = 0.89 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:14.638611: step 152370, loss = 0.58 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:15.401507: step 152380, loss = 0.70 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:16.160252: step 152390, loss = 0.75 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:16.927714: step 152400, loss = 0.74 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:17.698726: step 152410, loss = 0.69 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:18.458751: step 152420, loss = 0.61 (1684.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:19.224509: step 152430, loss = 0.67 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:19.979499: step 152440, loss = 0.60 (1695.4 examples/sec; 0.075 sec/batch)
2017-05-05 20:40:20.743819: step 152450, loss = 0.70 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:21.610503: step 152460, loss = 0.62 (1476.9 examples/sec; 0.087 sec/batch)
2017-05-05 20:40:22.273484: step 152470, loss = 0.79 (1930.7 examples/sec; 0.066 sec/batch)
2017-05-05 20:40:23.036275: step 152480, loss = 0.85 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:23.788926: step 152490, loss = 0.75 (1700.7 examples/sec; 0.075 sec/batch)
2017-05-05 20:40:24.557815: step 152500, loss = 0.71 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:25.323571: step 152510, loss = 0.60 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:26.085635: step 152520, loss = 0.67 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:26.849143: step 152530, loss = 0.74 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:27.606885: step 152540, loss = 0.68 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:28.363483: step 152550, loss = 0.69 (1691.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:29.125350: step 152560, loss = 0.69 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:29.894606: step 152570, loss = 0.70 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:30.654224: step 152580, loss = 0.66 (1685.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:31.420263: step 152590, loss = 0.70 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:32.178107: step 152600, loss = 0.64 (1689.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:32.942536: step 152610, loss = 0.69 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:33.708024: step 152620, loss = 0.69 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:34.474457: step 152630, loss = 0.83 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:35.237343: step 152640, loss = 0.77 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:35.988666: step 152650, loss = 0.59 (1703.7 examples/sec; 0.075 sec/batch)
2017-05-05 20:40:36.758939: step 152660, loss = 0.75 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:37.525210: step 152670, loss = 0.72 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:38.294888: step 152680, loss = 0.74 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:39.058620: step 152690, loss = 0.59 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:39.809769: step 152700, loss = 0.85 (1704.0 examples/sec; 0.075 sec/batch)
2017-05-05 20:40:40.574342: step 152710, loss = 0.71 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:41.341046: step 152720, loss = 0.53 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:42.108543: step 152730, loss = 0.86 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:42.867979: step 152740, loss = 0.65 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:43.624867: step 152750, loss = 0.87 (1691.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:44.390853: step 152760, loss = 0.64 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:45.162224: step 152770, loss = 0.73 (1659.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:45.926055: step 152780, loss = 0.81 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:46.696169: step 152790, loss = 0.73 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:47.457791: step 152800, loss = 0.79 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:48.224917: step 152810, loss = 0.77 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:48.983155: step 152820, loss = 0.80 (1688.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:49.750498: step 152830, loss = 0.74 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:50.512048: step 152840, loss = 0.70 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:51.273085: step 152850, loss = 0.69 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:52.034848: step 152860, loss = 0.70 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:52.799498: step 152870, loss = 0.68 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:53.570519: step 152880, loss = 0.60 (1660.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:54.340322: step 152890, loss = 0.74 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:55.105589: step 152900, loss = 0.59 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:55.859514: step 152910, loss = 0.63 (1697.8 examples/sec; 0.075 sec/batch)
2017-05-05 20:40:56.627623: step 152920, loss = 0.65 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:57.392870: step 152930, loss = 0.68 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:58.155690: step 152940, loss = 0.76 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:40:58.927105: step 152950, loss = 0.61 (1659.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:40:59.679261: step 152960, loss = 0.61 (1701.8 examples/sec; 0.075 sec/batch)
2017-05-05 20:41:00.440636: step 152970, loss = 0.69 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:01.209140: step 152980, loss = 0.66 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:41:01.966344: step 152990, loss = 0.65 (1690.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:02.727109: step 153000, loss = 0.68 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:03.490080: step 153010, loss = 0.64 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:04.250465: step 153020, loss = 0.82 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:05.010966: step 153030, loss = 0.65 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:05.773426: step 153040, loss = 0.68 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:06.538100: step 153050, loss = 0.62 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:07.309087: step 153060, loss = 0.70 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:41:08.060267: step 153070, loss = 0.69 (1704.0 examples/sec; 0.075 sec/batch)
2017-05-05 20:41:08.827103: step 153080, loss = 0.67 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:41:09.595989: step 153090, loss = 0.73 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:41:10.361794: step 153100, loss = 0.63 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:41:11.129796: step 153110, loss = 0.70 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:41:11.882132: step 153120, loss = 0.67 (1701.4 examples/sec; 0.075 sec/batch)
2017-05-05 20:41:12.649232: step 153130, loss = 0.55 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:41:13.421984: step 153140, loss = 0.85 (1656.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:41:14.182535: step 153150, loss = 0.65 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:14.948014: step 153160, loss = 0.62 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:41:15.704197: step 153170, loss = 0.84 (1692.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:16.465936: step 153180, loss = 0.53 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:17.233329: step 153190, loss = 0.83 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:41:18.000464: step 153200, loss = 0.76 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:41:18.766411: step 153210, loss = 0.72 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:41:19.529668: step 153220, loss = 0.60 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:20.292209: step 153230, loss = 0.77 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:21.056787: step 153240, loss = 0.64 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:21.820221: step 153250, loss = 0.63 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:22.591549: step 153260, loss = 0.61 (1659.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:41:23.358784: step 153270, loss = 0.64 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:41:24.115745: step 153280, loss = 0.62 (1691.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:24.875901: step 153290, loss = 0.59 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:25.644663: step 153300, loss = 0.69 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:41:26.411693: step 153310, loss = 0.67 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:41:27.173598: step 153320, loss = 0.73 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:27.931190: step 153330, loss = 0.88 (1689.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:28.690094: step 153340, loss = 0.79 (1686.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:29.460810: step 153350, loss = 0.70 (1660.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:41:30.222967: step 153360, loss = 0.76 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:30.983207: step 153370, loss = 0.68 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:31.736023: step 153380, loss = 0.67 (1700.3 examples/sec; 0.075 sec/batch)
2017-05-05 20:41:32.497727: step 153390, loss = 0.76 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:33.264838: step 153400, loss = 0.77 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:41:34.026797: step 153410, loss = 0.71 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:34.787995: step 153420, loss = 0.65 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:35.550342: step 153430, loss = 0.76 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:36.315196: step 153440, loss = 0.76 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:37.186722: step 153450, loss = 0.68 (1468.7 examples/sec; 0.087 sec/batch)
2017-05-05 20:41:37.855333: step 153460, loss = 0.65 (1914.4 examples/sec; 0.067 sec/batch)
2017-05-05 20:41:38.616500: step 153470, loss = 0.70 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:39.382169: step 153480, loss = 0.77 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:41:40.133709: step 153490, loss = 0.67 (1703.2 examples/sec; 0.075 sec/batch)
2017-05-05 20:41:40.900748: step 153500, loss = 0.77 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:41:41.661971: step 153510, loss = 0.85 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:42.428528: step 153520, loss = 0.73 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:41:43.197975: step 153530, loss = 0.76 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:41:43.955979: step 153540, loss = 0.69 (1688.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:44.716949: step 153550, loss = 0.58 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:45.477658: step 153560, loss = 0.73 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:46.241978: step 153570, loss = 0.75 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:47.007535: step 153580, loss = 0.78 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:41:47.762880: step 153590, loss = 0.59 (1694.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:48.526064: step 153600, loss = 0.62 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:49.293540: step 153610, loss = 0.62 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:41:50.056374: step 153620, loss = 0.62 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:50.818276: step 153630, loss = 0.61 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:51.576788: step 153640, loss = 0.64 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:52.339859: step 153650, loss = 0.69 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:53.110304: step 153660, loss = 0.76 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:41:53.875884: step 153670, loss = 0.80 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:41:54.653546: step 153680, loss = 0.77 (1646.0 examples/sec; 0.078 sec/batch)
2017-05-05 20:41:55.412824: step 153690, loss = 0.63 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:56.176873: step 153700, loss = 0.74 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:56.941765: step 153710, loss = 0.67 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:57.705565: step 153720, loss = 0.79 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:58.467943: step 153730, loss = 0.81 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:41:59.236307: step 153740, loss = 0.72 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:41:59.995975: step 153750, loss = 0.69 (1684.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:00.755023: step 153760, loss = 0.92 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:01.517662: step 153770, loss = 0.68 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:02.285572: step 153780, loss = 0.73 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:42:03.052975: step 153790, loss = 0.64 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:42:03.813238: step 153800, loss = 0.56 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:04.583570: step 153810, loss = 0.67 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:42:05.344771: step 153820, loss = 0.56 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:06.117927: step 153830, loss = 0.78 (1655.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:42:06.879301: step 153840, loss = 0.69 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:07.633212: step 153850, loss = 0.76 (1697.8 examples/sec; 0.075 sec/batch)
2017-05-05 20:42:08.398630: step 153860, loss = 0.84 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:42:09.167168: step 153870, loss = 0.81 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:42:09.931015: step 153880, loss = 0.59 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:10.690093: step 153890, loss = 0.73 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:11.445877: step 153900, loss = 0.80 (1693.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:12.205047: step 153910, loss = 0.54 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:12.965519: step 153920, loss = 0.56 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:13.731873: step 153930, loss = 0.58 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:42:14.498027: step 153940, loss = 0.66 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:42:15.264640: step 153950, loss = 0.78 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:42:16.019543: step 153960, loss = 0.69 (1695.6 examples/sec; 0.075 sec/batch)
2017-05-05 20:42:16.783984: step 153970, loss = 0.76 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:17.543192: step 153980, loss = 0.74 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:18.311818: step 153990, loss = 0.71 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:42:19.084561: step 154000, loss = 0.86 (1656.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:42:19.834895: step 154010, loss = 0.80 (1705.9 examples/sec; 0.075 sec/batch)
2017-05-05 20:42:20.601390: step 154020, loss = 0.60 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:42:21.374395: step 154030, loss = 0.68 (1655.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:42:22.136462: step 154040, loss = 0.75 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:22.900838: step 154050, loss = 0.76 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:23.660435: step 154060, loss = 0.81 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:24.423306: step 154070, loss = 0.69 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:25.190718: step 154080, loss = 0.63 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:42:25.949927: step 154090, loss = 0.76 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:26.712189: step 154100, loss = 0.59 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:27.471958: step 154110, loss = 0.74 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:28.232352: step 154120, loss = 0.70 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:28.993295: step 154130, loss = 0.75 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:29.763352: step 154140, loss = 0.71 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:42:30.519056: step 154150, loss = 0.62 (1693.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:31.282894: step 154160, loss = 0.72 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:32.044374: step 154170, loss = 0.63 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:32.802214: step 154180, loss = 0.58 (1689.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:33.565231: step 154190, loss = 0.55 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:34.330564: step 154200, loss = 0.69 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:42:35.106574: step 154210, loss = 0.75 (1649.5 examples/sec; 0.078 sec/batch)
2017-05-05 20:42:35.855543: step 154220, loss = 0.72 (1709.0 examples/sec; 0.075 sec/batch)
2017-05-05 20:42:36.631990: step 154230, loss = 0.75 (1648.5 examples/sec; 0.078 sec/batch)
2017-05-05 20:42:37.395855: step 154240, loss = 0.70 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:38.169210: step 154250, loss = 0.74 (1655.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:42:38.930396: step 154260, loss = 0.78 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:39.685627: step 154270, loss = 0.58 (1694.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:40.447338: step 154280, loss = 0.70 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:41.213799: step 154290, loss = 0.61 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:42:41.979537: step 154300, loss = 0.70 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:42:42.746533: step 154310, loss = 0.67 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:42:43.507610: step 154320, loss = 0.82 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:44.265336: step 154330, loss = 0.77 (1689.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:45.029160: step 154340, loss = 0.61 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:45.795005: step 154350, loss = 0.74 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:42:46.564780: step 154360, loss = 0.74 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:42:47.335545: step 154370, loss = 0.61 (1660.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:42:48.092941: step 154380, loss = 0.81 (1690.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:48.854649: step 154390, loss = 0.71 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:49.619917: step 154400, loss = 0.59 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:42:50.379487: step 154410, loss = 0.67 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:51.149406: step 154420, loss = 0.70 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:42:51.898991: step 154430, loss = 0.77 (1707.6 examples/sec; 0.075 sec/batch)
2017-05-05 20:42:52.766968: step 154440, loss = 0.79 (1474.7 examples/sec; 0.087 sec/batch)
2017-05-05 20:42:53.430916: step 154450, loss = 0.80 (1927.8 examples/sec; 0.066 sec/batch)
2017-05-05 20:42:54.197372: step 154460, loss = 0.61 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:42:54.964568: step 154470, loss = 0.70 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:42:55.723106: step 154480, loss = 0.74 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:56.486861: step 154490, loss = 0.71 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:57.256612: step 154500, loss = 0.69 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:42:58.018904: step 154510, loss = 0.60 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:58.781267: step 154520, loss = 0.71 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:42:59.538744: step 154530, loss = 0.75 (1689.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:00.304191: step 154540, loss = 0.68 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:43:01.066825: step 154550, loss = 0.73 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:01.827454: step 154560, loss = 0.63 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:02.598975: step 154570, loss = 0.66 (1659.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:43:03.367419: step 154580, loss = 0.64 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:43:04.128070: step 154590, loss = 0.69 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:04.886314: step 154600, loss = 0.74 (1688.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:05.657163: step 154610, loss = 0.73 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:43:06.419916: step 154620, loss = 0.72 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:07.183238: step 154630, loss = 0.80 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:07.943036: step 154640, loss = 0.58 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:08.707886: step 154650, loss = 0.78 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:09.466750: step 154660, loss = 0.74 (1686.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:10.234924: step 154670, loss = 0.65 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:43:10.988486: step 154680, loss = 0.70 (1698.6 examples/sec; 0.075 sec/batch)
2017-05-05 20:43:11.752876: step 154690, loss = 0.73 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:12.518243: step 154700, loss = 0.68 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:43:13.287708: step 154710, loss = 0.77 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:43:14.046279: step 154720, loss = 0.62 (1687.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:14.808993: step 154730, loss = 0.67 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:15.564972: step 154740, loss = 0.69 (1693.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:16.331058: step 154750, loss = 0.71 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:43:17.097421: step 154760, loss = 0.57 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:43:17.866611: step 154770, loss = 0.79 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:43:18.634002: step 154780, loss = 0.73 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:43:19.399185: step 154790, loss = 0.76 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:43:20.161429: step 154800, loss = 0.67 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:20.921798: step 154810, loss = 0.72 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:21.684193: step 154820, loss = 0.65 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:22.454718: step 154830, loss = 0.62 (1661.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:43:23.220249: step 154840, loss = 0.74 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:43:23.971269: step 154850, loss = 0.65 (1704.3 examples/sec; 0.075 sec/batch)
2017-05-05 20:43:24.738187: step 154860, loss = 0.68 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:43:25.502424: step 154870, loss = 0.73 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:26.266344: step 154880, loss = 0.60 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:27.029643: step 154890, loss = 0.69 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:27.790852: step 154900, loss = 0.67 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:28.553339: step 154910, loss = 0.71 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:29.316873: step 154920, loss = 0.65 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:30.083873: step 154930, loss = 0.68 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:43:30.845385: step 154940, loss = 0.76 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:31.605228: step 154950, loss = 0.73 (1684.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:32.366604: step 154960, loss = 0.72 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:33.133972: step 154970, loss = 0.67 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:43:33.897523: step 154980, loss = 0.75 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:34.672299: step 154990, loss = 0.70 (1652.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:43:35.429701: step 155000, loss = 0.73 (1690.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:36.194437: step 155010, loss = 0.68 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:36.959177: step 155020, loss = 0.72 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:37.728858: step 155030, loss = 0.72 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:43:38.493808: step 155040, loss = 0.86 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:39.263064: step 155050, loss = 0.83 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:43:40.024259: step 155060, loss = 0.75 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:40.785880: step 155070, loss = 0.68 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:41.550234: step 155080, loss = 0.67 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:42.311930: step 155090, loss = 0.71 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:43.077757: step 155100, loss = 0.74 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:43:43.827862: step 155110, loss = 0.79 (1706.4 examples/sec; 0.075 sec/batch)
2017-05-05 20:43:44.593195: step 155120, loss = 0.62 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:43:45.356714: step 155130, loss = 0.59 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:46.134393: step 155140, loss = 0.73 (1645.9 examples/sec; 0.078 sec/batch)
2017-05-05 20:43:46.900956: step 155150, loss = 0.82 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:43:47.659113: step 155160, loss = 0.70 (1688.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:48.424164: step 155170, loss = 0.61 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:43:49.186228: step 155180, loss = 0.77 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:49.949532: step 155190, loss = 0.71 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:50.713102: step 155200, loss = 0.78 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:51.469668: step 155210, loss = 0.82 (1691.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:52.232396: step 155220, loss = 0.59 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:53.001503: step 155230, loss = 0.72 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:43:53.765917: step 155240, loss = 0.69 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:54.536534: step 155250, loss = 0.65 (1661.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:43:55.298853: step 155260, loss = 0.71 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:56.053914: step 155270, loss = 0.60 (1695.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:56.822618: step 155280, loss = 0.64 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:43:57.584488: step 155290, loss = 0.70 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:58.353802: step 155300, loss = 0.75 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:43:59.112641: step 155310, loss = 0.66 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:43:59.865506: step 155320, loss = 0.80 (1700.2 examples/sec; 0.075 sec/batch)
2017-05-05 20:44:00.627163: step 155330, loss = 0.63 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:01.391083: step 155340, loss = 0.74 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:02.151660: step 155350, loss = 0.83 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:02.922419: step 155360, loss = 0.59 (1660.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:44:03.678015: step 155370, loss = 0.75 (1694.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:04.446263: step 155380, loss = 0.77 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:44:05.211801: step 155390, loss = 0.66 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:44:05.972635: step 155400, loss = 0.62 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:06.743838: step 155410, loss = 0.75 (1659.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:44:07.511536: step 155420, loss = 0.61 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:44:08.369230: step 155430, loss = 0.66 (1492.4 examples/sec; 0.086 sec/batch)
2017-05-05 20:44:09.039031: step 155440, loss = 0.64 (1911.0 examples/sec; 0.067 sec/batch)
2017-05-05 20:44:09.809788: step 155450, loss = 0.85 (1660.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:44:10.566334: step 155460, loss = 0.88 (1691.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:11.330244: step 155470, loss = 0.67 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:12.090866: step 155480, loss = 0.73 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:12.860079: step 155490, loss = 0.63 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:44:13.628465: step 155500, loss = 0.73 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:44:14.398556: step 155510, loss = 0.55 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:44:15.165720: step 155520, loss = 0.79 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:44:15.920677: step 155530, loss = 0.78 (1695.5 examples/sec; 0.075 sec/batch)
2017-05-05 20:44:16.683784: step 155540, loss = 0.67 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:17.453582: step 155550, loss = 0.62 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:44:18.213070: step 155560, loss = 0.58 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:18.981142: step 155570, loss = 0.69 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:44:19.729880: step 155580, loss = 0.64 (1709.5 examples/sec; 0.075 sec/batch)
2017-05-05 20:44:20.488169: step 155590, loss = 0.74 (1688.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:21.252970: step 155600, loss = 0.82 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:22.020847: step 155610, loss = 0.60 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:44:22.779402: step 155620, loss = 0.70 (1687.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:23.541129: step 155630, loss = 0.83 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:24.295035: step 155640, loss = 0.73 (1697.8 examples/sec; 0.075 sec/batch)
2017-05-05 20:44:25.057913: step 155650, loss = 0.75 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:25.824889: step 155660, loss = 0.74 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:44:26.594799: step 155670, loss = 0.72 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:44:27.354221: step 155680, loss = 0.79 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:28.115900: step 155690, loss = 0.63 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:28.882660: step 155700, loss = 0.70 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:44:29.650654: step 155710, loss = 0.67 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:44:30.422839: step 155720, loss = 0.68 (1657.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:44:31.187858: step 155730, loss = 0.75 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:44:31.938034: step 155740, loss = 0.65 (1706.3 examples/sec; 0.075 sec/batch)
2017-05-05 20:44:32.703991: step 155750, loss = 0.75 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:44:33.467988: step 155760, loss = 0.84 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:34.228826: step 155770, loss = 0.71 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:34.991813: step 155780, loss = 0.77 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:35.752562: step 155790, loss = 0.82 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:36.513469: step 155800, loss = 0.58 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:37.277577: step 155810, loss = 0.64 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:38.035331: step 155820, loss = 0.62 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:38.818983: step 155830, loss = 0.64 (1633.4 examples/sec; 0.078 sec/batch)
2017-05-05 20:44:39.575100: step 155840, loss = 0.78 (1692.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:40.338513: step 155850, loss = 0.77 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:41.099925: step 155860, loss = 0.69 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:41.867436: step 155870, loss = 0.93 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:44:42.623675: step 155880, loss = 0.67 (1692.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:43.380743: step 155890, loss = 0.68 (1690.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:44.142423: step 155900, loss = 0.72 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:44.907593: step 155910, loss = 0.88 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:44:45.676042: step 155920, loss = 0.74 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:44:46.445130: step 155930, loss = 0.74 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:44:47.206046: step 155940, loss = 0.79 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:47.958213: step 155950, loss = 0.78 (1701.7 examples/sec; 0.075 sec/batch)
2017-05-05 20:44:48.726876: step 155960, loss = 0.90 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:44:49.502075: step 155970, loss = 0.80 (1651.2 examples/sec; 0.078 sec/batch)
2017-05-05 20:44:50.268129: step 155980, loss = 0.65 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:44:51.032876: step 155990, loss = 0.73 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:51.786865: step 156000, loss = 0.70 (1697.6 examples/sec; 0.075 sec/batch)
2017-05-05 20:44:52.554808: step 156010, loss = 0.81 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:44:53.324736: step 156020, loss = 0.62 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:44:54.085118: step 156030, loss = 0.56 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:54.853612: step 156040, loss = 0.80 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:44:55.614710: step 156050, loss = 0.69 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:56.384955: step 156060, loss = 0.62 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:44:57.148498: step 156070, loss = 0.82 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:57.910648: step 156080, loss = 0.75 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:44:58.678832: step 156090, loss = 0.78 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:44:59.455966: step 156100, loss = 0.77 (1647.1 examples/sec; 0.078 sec/batch)
2017-05-05 20:45:00.219716: step 156110, loss = 0.78 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:00.984542: step 156120, loss = 0.58 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:01.751534: step 156130, loss = 0.77 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:02.521793: step 156140, loss = 0.80 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:03.286222: step 156150, loss = 0.72 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:04.046083: step 156160, loss = 0.80 (1684.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:04.812182: step 156170, loss = 0.66 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:05.572139: step 156180, loss = 0.66 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:06.340646: step 156190, loss = 0.75 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:07.110128: step 156200, loss = 0.68 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:07.873596: step 156210, loss = 0.74 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:08.631022: step 156220, loss = 0.88 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:09.400813: step 156230, loss = 0.91 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:10.170099: step 156240, loss = 0.79 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:10.931653: step 156250, loss = 0.65 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:11.689662: step 156260, loss = 0.87 (1688.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:12.457581: step 156270, loss = 0.74 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:13.222076: step 156280, loss = 0.60 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:13.985203: step 156290, loss = 0.59 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:14.750487: step 156300, loss = 0.69 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:15.508876: step 156310, loss = 0.52 (1687.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:16.274656: step 156320, loss = 0.64 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:17.035097: step 156330, loss = 0.65 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:17.802848: step 156340, loss = 0.66 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:18.561886: step 156350, loss = 0.68 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:19.328470: step 156360, loss = 0.71 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:20.082520: step 156370, loss = 0.89 (1697.5 examples/sec; 0.075 sec/batch)
2017-05-05 20:45:20.841655: step 156380, loss = 0.65 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:21.606962: step 156390, loss = 0.74 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:22.373227: step 156400, loss = 0.65 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:23.139632: step 156410, loss = 0.63 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:24.001258: step 156420, loss = 0.75 (1485.6 examples/sec; 0.086 sec/batch)
2017-05-05 20:45:24.671309: step 156430, loss = 0.83 (1910.3 examples/sec; 0.067 sec/batch)
2017-05-05 20:45:25.437101: step 156440, loss = 0.61 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:26.199310: step 156450, loss = 0.64 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:26.963737: step 156460, loss = 0.74 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:27.720343: step 156470, loss = 0.65 (1691.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:28.479219: step 156480, loss = 0.69 (1686.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:29.242998: step 156490, loss = 0.89 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:30.006058: step 156500, loss = 0.72 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:30.774963: step 156510, loss = 0.74 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:31.535159: step 156520, loss = 0.71 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:32.298769: step 156530, loss = 0.67 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:33.065105: step 156540, loss = 0.91 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:33.834089: step 156550, loss = 0.77 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:34.596680: step 156560, loss = 0.69 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:35.365590: step 156570, loss = 0.68 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:36.124301: step 156580, loss = 0.79 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:36.891840: step 156590, loss = 0.68 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:37.655503: step 156600, loss = 0.77 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:38.423290: step 156610, loss = 0.64 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:39.197566: step 156620, loss = 0.78 (1653.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:39.960957: step 156630, loss = 0.62 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:40.730849: step 156640, loss = 0.78 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:41.498933: step 156650, loss = 0.95 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:42.261235: step 156660, loss = 0.65 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:43.027943: step 156670, loss = 0.81 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:43.787300: step 156680, loss = 0.65 (1685.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:44.551219: step 156690, loss = 0.66 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:45.324598: step 156700, loss = 0.69 (1655.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:46.093626: step 156710, loss = 0.68 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:46.862510: step 156720, loss = 0.81 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:47.630083: step 156730, loss = 0.65 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:48.394100: step 156740, loss = 0.61 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:49.166322: step 156750, loss = 0.70 (1657.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:49.931791: step 156760, loss = 0.79 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:50.702158: step 156770, loss = 0.63 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:51.464732: step 156780, loss = 0.79 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:52.232315: step 156790, loss = 0.57 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:53.004015: step 156800, loss = 0.79 (1658.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:53.783871: step 156810, loss = 0.63 (1641.3 examples/sec; 0.078 sec/batch)
2017-05-05 20:45:54.564750: step 156820, loss = 0.76 (1639.2 examples/sec; 0.078 sec/batch)
2017-05-05 20:45:55.328861: step 156830, loss = 0.72 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:56.090905: step 156840, loss = 0.76 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:56.859008: step 156850, loss = 0.70 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:57.628861: step 156860, loss = 0.66 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:58.392082: step 156870, loss = 0.68 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:45:59.162656: step 156880, loss = 0.78 (1661.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:45:59.921742: step 156890, loss = 0.56 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:46:00.690019: step 156900, loss = 0.83 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:01.453331: step 156910, loss = 0.69 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:46:02.226814: step 156920, loss = 0.62 (1654.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:02.988829: step 156930, loss = 0.66 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:46:03.751975: step 156940, loss = 0.63 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:46:04.528701: step 156950, loss = 0.68 (1647.9 examples/sec; 0.078 sec/batch)
2017-05-05 20:46:05.291132: step 156960, loss = 0.68 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:46:06.097181: step 156970, loss = 0.66 (1588.0 examples/sec; 0.081 sec/batch)
2017-05-05 20:46:06.863165: step 156980, loss = 0.68 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:07.618683: step 156990, loss = 0.73 (1694.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:46:08.388236: step 157000, loss = 0.76 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:09.152283: step 157010, loss = 0.68 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:46:09.930485: step 157020, loss = 0.61 (1644.8 examples/sec; 0.078 sec/batch)
2017-05-05 20:46:10.695554: step 157030, loss = 0.66 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:11.461257: step 157040, loss = 0.70 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:12.224970: step 157050, loss = 0.78 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:46:12.995790: step 157060, loss = 0.75 (1660.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:13.759379: step 157070, loss = 0.57 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:46:14.534613: step 157080, loss = 0.75 (1651.1 examples/sec; 0.078 sec/batch)
2017-05-05 20:46:15.294314: step 157090, loss = 0.67 (1684.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:46:16.053930: step 157100, loss = 0.78 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:46:16.817905: step 157110, loss = 0.82 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:46:17.600307: step 157120, loss = 0.78 (1636.0 examples/sec; 0.078 sec/batch)
2017-05-05 20:46:18.379434: step 157130, loss = 0.78 (1642.9 examples/sec; 0.078 sec/batch)
2017-05-05 20:46:19.153928: step 157140, loss = 0.73 (1652.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:19.910036: step 157150, loss = 0.75 (1692.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:46:20.676559: step 157160, loss = 0.67 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:21.449430: step 157170, loss = 0.75 (1656.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:22.208100: step 157180, loss = 0.70 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:46:22.979247: step 157190, loss = 0.85 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:23.739216: step 157200, loss = 0.76 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:46:24.509506: step 157210, loss = 0.76 (1661.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:25.284008: step 157220, loss = 0.58 (1652.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:26.053334: step 157230, loss = 0.71 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:26.826757: step 157240, loss = 0.79 (1655.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:27.590829: step 157250, loss = 0.77 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:46:28.355092: step 157260, loss = 0.88 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:46:29.128824: step 157270, loss = 0.77 (1654.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:29.891589: step 157280, loss = 0.62 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:46:30.655501: step 157290, loss = 0.74 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:46:31.420837: step 157300, loss = 0.75 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:32.181206: step 157310, loss = 0.72 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:46:32.949790: step 157320, loss = 0.77 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:33.717228: step 157330, loss = 0.72 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:34.489557: step 157340, loss = 0.79 (1657.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:35.251738: step 157350, loss = 0.68 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:46:36.013326: step 157360, loss = 0.71 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:46:36.781273: step 157370, loss = 0.56 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:37.552173: step 157380, loss = 0.62 (1660.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:38.323283: step 157390, loss = 0.72 (1659.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:39.093734: step 157400, loss = 0.69 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:39.947739: step 157410, loss = 0.67 (1498.8 examples/sec; 0.085 sec/batch)
2017-05-05 20:46:40.626056: step 157420, loss = 0.71 (1887.0 examples/sec; 0.068 sec/batch)
2017-05-05 20:46:41.391006: step 157430, loss = 0.70 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:46:42.158262: step 157440, loss = 0.59 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:42.932250: step 157450, loss = 0.75 (1653.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:43.693855: step 157460, loss = 0.62 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:46:44.461407: step 157470, loss = 0.80 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:45.235644: step 157480, loss = 0.79 (1653.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:46.000061: step 157490, loss = 0.77 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:46:46.770602: step 157500, loss = 0.81 (1661.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:47.524472: step 157510, loss = 0.73 (1697.9 examples/sec; 0.075 sec/batch)
2017-05-05 20:46:48.286844: step 157520, loss = 0.74 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:46:49.054938: step 157530, loss = 0.78 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:49.820980: step 157540, loss = 0.64 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:50.593763: step 157550, loss = 0.78 (1656.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:51.363713: step 157560, loss = 0.64 (1662.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:52.125533: step 157570, loss = 0.71 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:46:52.890202: step 157580, loss = 0.75 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:46:53.656996: step 157590, loss = 0.66 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:54.427196: step 157600, loss = 0.72 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:55.194873: step 157610, loss = 0.68 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:55.952314: step 157620, loss = 0.77 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:46:56.721019: step 157630, loss = 0.77 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:57.490557: step 157640, loss = 0.65 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:58.262011: step 157650, loss = 0.74 (1659.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:46:59.039233: step 157660, loss = 0.70 (1646.9 examples/sec; 0.078 sec/batch)
2017-05-05 20:46:59.793658: step 157670, loss = 0.71 (1696.7 examples/sec; 0.075 sec/batch)
2017-05-05 20:47:00.554872: step 157680, loss = 0.71 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:47:01.323563: step 157690, loss = 0.77 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:02.088253: step 157700, loss = 0.89 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:47:02.852034: step 157710, loss = 0.76 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:47:03.608377: step 157720, loss = 0.61 (1692.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:47:04.376426: step 157730, loss = 0.73 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:05.152095: step 157740, loss = 0.57 (1650.2 examples/sec; 0.078 sec/batch)
2017-05-05 20:47:05.918158: step 157750, loss = 0.74 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:06.684092: step 157760, loss = 0.64 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:07.450156: step 157770, loss = 0.70 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:08.213166: step 157780, loss = 0.82 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:47:08.975976: step 157790, loss = 0.66 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:47:09.745204: step 157800, loss = 0.63 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:10.512965: step 157810, loss = 0.65 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:11.276207: step 157820, loss = 0.63 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:47:12.040078: step 157830, loss = 0.84 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:47:12.807009: step 157840, loss = 0.73 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:13.578727: step 157850, loss = 0.79 (1658.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:14.344501: step 157860, loss = 0.72 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:15.113728: step 157870, loss = 0.69 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:15.874103: step 157880, loss = 0.57 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:47:16.636846: step 157890, loss = 0.78 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:47:17.411284: step 157900, loss = 0.69 (1652.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:18.175551: step 157910, loss = 0.81 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:47:18.944296: step 157920, loss = 0.73 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:19.704121: step 157930, loss = 0.68 (1684.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:47:20.474455: step 157940, loss = 0.63 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:21.245808: step 157950, loss = 0.83 (1659.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:22.016020: step 157960, loss = 0.68 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:22.788778: step 157970, loss = 0.71 (1656.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:23.545683: step 157980, loss = 0.69 (1691.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:47:24.316738: step 157990, loss = 0.81 (1660.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:25.085862: step 158000, loss = 0.78 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:25.853555: step 158010, loss = 0.75 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:26.620415: step 158020, loss = 0.72 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:27.386652: step 158030, loss = 0.62 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:28.148840: step 158040, loss = 0.70 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:47:28.918424: step 158050, loss = 0.63 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:29.684387: step 158060, loss = 0.76 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:30.449750: step 158070, loss = 0.67 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:31.221564: step 158080, loss = 0.70 (1658.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:31.977380: step 158090, loss = 0.64 (1693.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:47:32.743241: step 158100, loss = 0.59 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:33.515414: step 158110, loss = 0.62 (1657.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:34.289335: step 158120, loss = 0.72 (1653.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:35.058394: step 158130, loss = 0.78 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:35.815930: step 158140, loss = 0.76 (1689.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:47:36.587972: step 158150, loss = 0.74 (1657.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:37.356827: step 158160, loss = 0.76 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:38.129130: step 158170, loss = 0.84 (1657.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:38.897917: step 158180, loss = 0.75 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:39.658289: step 158190, loss = 0.79 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:47:40.427291: step 158200, loss = 0.46 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:41.198589: step 158210, loss = 0.58 (1659.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:41.969456: step 158220, loss = 0.68 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:42.739791: step 158230, loss = 0.66 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:43.502998: step 158240, loss = 0.84 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:47:44.269779: step 158250, loss = 0.59 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:45.032439: step 158260, loss = 0.60 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:47:45.804680: step 158270, loss = 0.78 (1657.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:46.572383: step 158280, loss = 0.67 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:47.334998: step 158290, loss = 0.77 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:47:48.092102: step 158300, loss = 0.74 (1690.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:47:48.864577: step 158310, loss = 0.77 (1657.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:49.631618: step 158320, loss = 0.87 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:50.402629: step 158330, loss = 0.64 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:51.167397: step 158340, loss = 0.75 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:47:51.932890: step 158350, loss = 0.82 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:52.698914: step 158360, loss = 0.76 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:53.463224: step 158370, loss = 0.82 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:47:54.233287: step 158380, loss = 0.69 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:55.005616: step 158390, loss = 0.84 (1657.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:55.868845: step 158400, loss = 0.59 (1482.8 examples/sec; 0.086 sec/batch)
2017-05-05 20:47:56.530342: step 158410, loss = 0.69 (1935.0 examples/sec; 0.066 sec/batch)
2017-05-05 20:47:57.297345: step 158420, loss = 0.68 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:58.068314: step 158430, loss = 0.71 (1660.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:47:58.831808: step 158440, loss = 0.65 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:47:59.598846: step 158450, loss = 0.78 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:00.363235: step 158460, loss = 0.52 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:48:01.125883: step 158470, loss = 0.88 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:48:01.893251: step 158480, loss = 0.58 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:02.663014: step 158490, loss = 0.56 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:03.432349: step 158500, loss = 0.72 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:04.201474: step 158510, loss = 0.59 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:04.976567: step 158520, loss = 0.73 (1651.4 examples/sec; 0.078 sec/batch)
2017-05-05 20:48:05.745613: step 158530, loss = 0.58 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:06.509731: step 158540, loss = 0.64 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:48:07.279973: step 158550, loss = 0.66 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:08.046515: step 158560, loss = 0.74 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:08.819069: step 158570, loss = 0.60 (1656.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:09.584432: step 158580, loss = 0.66 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:10.352343: step 158590, loss = 0.74 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:11.120773: step 158600, loss = 0.61 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:11.884161: step 158610, loss = 0.73 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:48:12.651092: step 158620, loss = 0.59 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:13.423549: step 158630, loss = 0.65 (1657.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:14.195153: step 158640, loss = 0.72 (1658.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:14.967789: step 158650, loss = 0.63 (1656.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:15.721682: step 158660, loss = 0.66 (1697.9 examples/sec; 0.075 sec/batch)
2017-05-05 20:48:16.488284: step 158670, loss = 0.73 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:17.258677: step 158680, loss = 0.72 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:18.030162: step 158690, loss = 0.70 (1659.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:18.798109: step 158700, loss = 0.66 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:19.563183: step 158710, loss = 0.64 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:20.328964: step 158720, loss = 0.72 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:21.099549: step 158730, loss = 0.70 (1661.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:21.867984: step 158740, loss = 0.76 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:22.635425: step 158750, loss = 0.70 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:23.401606: step 158760, loss = 0.72 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:24.163865: step 158770, loss = 0.78 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:48:24.933066: step 158780, loss = 0.59 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:25.693868: step 158790, loss = 0.53 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:48:26.468179: step 158800, loss = 0.62 (1653.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:27.238075: step 158810, loss = 0.64 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:27.995199: step 158820, loss = 0.76 (1690.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:48:28.765462: step 158830, loss = 0.65 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:29.532959: step 158840, loss = 0.73 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:30.299160: step 158850, loss = 0.57 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:31.065610: step 158860, loss = 0.62 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:31.821910: step 158870, loss = 0.73 (1692.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:48:32.600066: step 158880, loss = 0.76 (1644.9 examples/sec; 0.078 sec/batch)
2017-05-05 20:48:33.380213: step 158890, loss = 0.70 (1640.7 examples/sec; 0.078 sec/batch)
2017-05-05 20:48:34.146816: step 158900, loss = 0.74 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:34.917800: step 158910, loss = 0.70 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:35.673912: step 158920, loss = 0.78 (1692.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:48:36.442938: step 158930, loss = 0.56 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:37.212790: step 158940, loss = 0.70 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:37.975577: step 158950, loss = 0.77 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:48:38.750568: step 158960, loss = 0.80 (1651.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:39.506573: step 158970, loss = 0.64 (1693.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:48:40.270576: step 158980, loss = 0.75 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:48:41.035873: step 158990, loss = 0.72 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:41.807942: step 159000, loss = 0.83 (1657.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:42.577342: step 159010, loss = 0.71 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:43.343820: step 159020, loss = 0.74 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:44.104329: step 159030, loss = 0.70 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:48:44.874931: step 159040, loss = 0.88 (1661.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:45.646040: step 159050, loss = 0.76 (1659.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:46.415123: step 159060, loss = 0.76 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:47.181756: step 159070, loss = 0.79 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:47.947800: step 159080, loss = 0.62 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:48.717402: step 159090, loss = 0.58 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:49.485300: step 159100, loss = 0.70 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:50.248080: step 159110, loss = 0.84 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:48:51.015052: step 159120, loss = 0.54 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:51.780008: step 159130, loss = 0.57 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:48:52.554343: step 159140, loss = 0.72 (1653.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:53.316659: step 159150, loss = 0.74 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:48:54.083749: step 159160, loss = 0.72 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:54.861592: step 159170, loss = 0.78 (1645.6 examples/sec; 0.078 sec/batch)
2017-05-05 20:48:55.614821: step 159180, loss = 0.78 (1699.4 examples/sec; 0.075 sec/batch)
2017-05-05 20:48:56.381430: step 159190, loss = 0.72 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:57.148569: step 159200, loss = 0.69 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:57.921524: step 159210, loss = 0.70 (1656.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:58.687815: step 159220, loss = 0.70 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:48:59.457324: step 159230, loss = 0.80 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:00.218908: step 159240, loss = 0.60 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:49:00.985946: step 159250, loss = 0.76 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:01.755739: step 159260, loss = 0.55 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:02.531684: step 159270, loss = 0.83 (1649.6 examples/sec; 0.078 sec/batch)
2017-05-05 20:49:03.302147: step 159280, loss = 0.70 (1661.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:04.068041: step 159290, loss = 0.65 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:04.842915: step 159300, loss = 0.64 (1651.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:05.608643: step 159310, loss = 0.63 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:06.376620: step 159320, loss = 0.86 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:07.148467: step 159330, loss = 0.88 (1658.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:07.906768: step 159340, loss = 0.64 (1688.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:49:08.670764: step 159350, loss = 0.65 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:49:09.444985: step 159360, loss = 0.82 (1653.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:10.213741: step 159370, loss = 0.69 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:10.982071: step 159380, loss = 0.66 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:11.834778: step 159390, loss = 0.66 (1501.1 examples/sec; 0.085 sec/batch)
2017-05-05 20:49:12.512973: step 159400, loss = 0.90 (1887.4 examples/sec; 0.068 sec/batch)
2017-05-05 20:49:13.280014: step 159410, loss = 0.71 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:14.050174: step 159420, loss = 0.84 (1662.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:14.822820: step 159430, loss = 0.64 (1656.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:15.580949: step 159440, loss = 0.75 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:49:16.345501: step 159450, loss = 0.68 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:49:17.114952: step 159460, loss = 0.74 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:17.890720: step 159470, loss = 0.72 (1650.0 examples/sec; 0.078 sec/batch)
2017-05-05 20:49:18.662174: step 159480, loss = 0.73 (1659.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:19.426410: step 159490, loss = 0.71 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:49:20.195444: step 159500, loss = 0.70 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:20.963164: step 159510, loss = 0.65 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:21.731554: step 159520, loss = 0.60 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:22.496393: step 159530, loss = 0.77 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:49:23.261321: step 159540, loss = 0.70 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:49:24.022104: step 159550, loss = 0.56 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:49:24.792527: step 159560, loss = 0.66 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:25.561039: step 159570, loss = 0.78 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:26.334453: step 159580, loss = 0.59 (1655.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:27.106222: step 159590, loss = 0.64 (1658.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:27.862130: step 159600, loss = 0.66 (1693.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:49:28.631033: step 159610, loss = 0.67 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:29.406380: step 159620, loss = 0.74 (1650.9 examples/sec; 0.078 sec/batch)
2017-05-05 20:49:30.173492: step 159630, loss = 0.62 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:30.938701: step 159640, loss = 0.77 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:31.699376: step 159650, loss = 0.74 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:49:32.470892: step 159660, loss = 0.64 (1659.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:33.237158: step 159670, loss = 0.85 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:34.013763: step 159680, loss = 0.65 (1648.2 examples/sec; 0.078 sec/batch)
2017-05-05 20:49:34.785725: step 159690, loss = 0.71 (1658.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:35.543320: step 159700, loss = 0.61 (1689.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:49:36.312407: step 159710, loss = 0.79 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:37.083604: step 159720, loss = 0.78 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:37.860962: step 159730, loss = 0.76 (1646.6 examples/sec; 0.078 sec/batch)
2017-05-05 20:49:38.627707: step 159740, loss = 0.75 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:39.387306: step 159750, loss = 0.61 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:49:40.152288: step 159760, loss = 0.60 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:49:40.921653: step 159770, loss = 0.62 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:41.686399: step 159780, loss = 0.69 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:49:42.454851: step 159790, loss = 0.73 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:43.223267: step 159800, loss = 0.73 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:43.976002: step 159810, loss = 0.67 (1700.5 examples/sec; 0.075 sec/batch)
2017-05-05 20:49:44.742170: step 159820, loss = 0.87 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:45.507180: step 159830, loss = 0.77 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:46.269893: step 159840, loss = 0.78 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:49:47.043207: step 159850, loss = 0.68 (1655.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:47.806959: step 159860, loss = 0.74 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:49:48.570915: step 159870, loss = 0.82 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:49:49.343196: step 159880, loss = 0.66 (1657.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:50.114647: step 159890, loss = 0.60 (1659.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:50.881339: step 159900, loss = 0.70 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:51.641723: step 159910, loss = 0.64 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:49:52.408470: step 159920, loss = 0.68 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:53.177072: step 159930, loss = 0.64 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:53.954849: step 159940, loss = 0.77 (1645.7 examples/sec; 0.078 sec/batch)
2017-05-05 20:49:54.721122: step 159950, loss = 0.76 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:55.487592: step 159960, loss = 0.75 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:56.252736: step 159970, loss = 0.65 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:57.020353: step 159980, loss = 0.70 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:57.792866: step 159990, loss = 0.70 (1656.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:58.566964: step 160000, loss = 0.76 (1653.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:49:59.329420: step 160010, loss = 0.77 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:50:00.088477: step 160020, loss = 0.89 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:50:00.854957: step 160030, loss = 0.70 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:01.625860: step 160040, loss = 0.68 (1660.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:02.394325: step 160050, loss = 0.68 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:03.172437: step 160060, loss = 0.80 (1645.0 examples/sec; 0.078 sec/batch)
2017-05-05 20:50:03.933216: step 160070, loss = 0.72 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:50:04.702145: step 160080, loss = 0.72 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:05.463821: step 160090, loss = 0.71 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:50:06.229157: step 160100, loss = 0.65 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:06.999981: step 160110, loss = 0.73 (1660.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:07.759965: step 160120, loss = 0.76 (1684.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:50:08.530316: step 160130, loss = 0.65 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:09.302456: step 160140, loss = 0.69 (1657.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:10.074162: step 160150, loss = 0.75 (1658.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:10.841476: step 160160, loss = 0.77 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:11.606160: step 160170, loss = 0.91 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:50:12.373236: step 160180, loss = 0.63 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:13.147156: step 160190, loss = 0.71 (1653.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:13.918532: step 160200, loss = 0.55 (1659.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:14.687010: step 160210, loss = 0.71 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:15.452826: step 160220, loss = 0.89 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:16.219856: step 160230, loss = 0.53 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:16.990776: step 160240, loss = 0.69 (1660.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:17.757919: step 160250, loss = 0.70 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:18.525639: step 160260, loss = 0.60 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:19.299469: step 160270, loss = 0.61 (1654.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:20.059516: step 160280, loss = 0.72 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:50:20.824888: step 160290, loss = 0.67 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:21.594089: step 160300, loss = 0.61 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:22.367847: step 160310, loss = 0.64 (1654.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:23.131549: step 160320, loss = 0.78 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:50:23.892474: step 160330, loss = 0.78 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:50:24.661223: step 160340, loss = 0.73 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:25.427249: step 160350, loss = 0.72 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:26.191129: step 160360, loss = 0.70 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:50:26.957901: step 160370, loss = 0.81 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:27.815022: step 160380, loss = 0.74 (1493.4 examples/sec; 0.086 sec/batch)
2017-05-05 20:50:28.487890: step 160390, loss = 0.69 (1902.3 examples/sec; 0.067 sec/batch)
2017-05-05 20:50:29.258249: step 160400, loss = 0.76 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:30.030022: step 160410, loss = 0.69 (1658.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:30.793753: step 160420, loss = 0.76 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:50:31.558169: step 160430, loss = 0.77 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:50:32.321418: step 160440, loss = 0.78 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:50:33.096552: step 160450, loss = 0.66 (1651.3 examples/sec; 0.078 sec/batch)
2017-05-05 20:50:33.866962: step 160460, loss = 0.58 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:34.640097: step 160470, loss = 0.66 (1655.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:35.408989: step 160480, loss = 0.86 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:36.168540: step 160490, loss = 0.72 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:50:36.939913: step 160500, loss = 0.60 (1659.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:37.712245: step 160510, loss = 0.78 (1657.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:38.475623: step 160520, loss = 0.58 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:50:39.244272: step 160530, loss = 0.63 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:40.008143: step 160540, loss = 0.71 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:50:40.778549: step 160550, loss = 0.75 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:41.547909: step 160560, loss = 0.82 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:42.313548: step 160570, loss = 0.78 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:43.088686: step 160580, loss = 0.75 (1651.3 examples/sec; 0.078 sec/batch)
2017-05-05 20:50:43.845389: step 160590, loss = 0.65 (1691.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:50:44.614658: step 160600, loss = 0.65 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:45.382390: step 160610, loss = 0.71 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:46.144067: step 160620, loss = 0.79 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:50:46.919077: step 160630, loss = 0.84 (1651.6 examples/sec; 0.078 sec/batch)
2017-05-05 20:50:47.673737: step 160640, loss = 0.81 (1696.1 examples/sec; 0.075 sec/batch)
2017-05-05 20:50:48.445567: step 160650, loss = 0.73 (1658.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:49.213982: step 160660, loss = 0.79 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:49.985836: step 160670, loss = 0.66 (1658.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:50.750583: step 160680, loss = 0.67 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:50:51.514004: step 160690, loss = 0.68 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:50:52.275698: step 160700, loss = 0.88 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:50:53.044132: step 160710, loss = 0.86 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:53.815995: step 160720, loss = 0.82 (1658.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:54.585409: step 160730, loss = 0.64 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:55.350874: step 160740, loss = 0.74 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:56.116589: step 160750, loss = 0.66 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:56.885910: step 160760, loss = 0.64 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:57.659355: step 160770, loss = 0.70 (1654.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:58.431060: step 160780, loss = 0.71 (1658.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:50:59.192210: step 160790, loss = 0.74 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:50:59.952223: step 160800, loss = 0.75 (1684.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:00.719226: step 160810, loss = 0.78 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:51:01.488177: step 160820, loss = 0.72 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:51:02.247280: step 160830, loss = 0.66 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:03.019703: step 160840, loss = 0.79 (1657.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:51:03.783793: step 160850, loss = 0.66 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:04.554098: step 160860, loss = 0.66 (1661.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:51:05.330121: step 160870, loss = 0.75 (1649.4 examples/sec; 0.078 sec/batch)
2017-05-05 20:51:06.094914: step 160880, loss = 0.99 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:06.867515: step 160890, loss = 0.84 (1656.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:51:07.629523: step 160900, loss = 0.61 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:08.392883: step 160910, loss = 0.83 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:09.166495: step 160920, loss = 0.72 (1654.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:51:09.929345: step 160930, loss = 0.72 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:10.690761: step 160940, loss = 0.72 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:11.459813: step 160950, loss = 0.74 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:51:12.214502: step 160960, loss = 0.74 (1696.1 examples/sec; 0.075 sec/batch)
2017-05-05 20:51:12.982674: step 160970, loss = 0.69 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:51:13.755033: step 160980, loss = 0.71 (1657.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:51:14.521909: step 160990, loss = 0.62 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:51:15.287335: step 161000, loss = 0.58 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:51:16.036856: step 161010, loss = 0.77 (1707.8 examples/sec; 0.075 sec/batch)
2017-05-05 20:51:16.800386: step 161020, loss = 0.69 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:17.566932: step 161030, loss = 0.72 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:51:18.337755: step 161040, loss = 0.77 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:51:19.100573: step 161050, loss = 0.64 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:19.855953: step 161060, loss = 0.72 (1694.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:20.615998: step 161070, loss = 0.77 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:21.383240: step 161080, loss = 0.69 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:51:22.148068: step 161090, loss = 0.71 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:22.916272: step 161100, loss = 0.73 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:51:23.670459: step 161110, loss = 0.61 (1697.2 examples/sec; 0.075 sec/batch)
2017-05-05 20:51:24.437967: step 161120, loss = 0.72 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:51:25.209463: step 161130, loss = 0.72 (1659.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:51:25.974863: step 161140, loss = 0.62 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:51:26.734393: step 161150, loss = 0.61 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:27.493994: step 161160, loss = 0.62 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:28.258987: step 161170, loss = 0.76 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:29.028274: step 161180, loss = 0.76 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:51:29.788447: step 161190, loss = 0.70 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:30.554244: step 161200, loss = 0.67 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:51:31.317307: step 161210, loss = 0.69 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:32.071097: step 161220, loss = 0.76 (1698.1 examples/sec; 0.075 sec/batch)
2017-05-05 20:51:32.836432: step 161230, loss = 0.75 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:51:33.605875: step 161240, loss = 0.69 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:51:34.368463: step 161250, loss = 0.65 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:35.131899: step 161260, loss = 0.75 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:35.884572: step 161270, loss = 0.65 (1700.6 examples/sec; 0.075 sec/batch)
2017-05-05 20:51:36.646701: step 161280, loss = 0.73 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:37.413575: step 161290, loss = 0.64 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:51:38.177256: step 161300, loss = 0.68 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:38.946007: step 161310, loss = 0.64 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:51:39.704350: step 161320, loss = 0.69 (1687.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:40.469182: step 161330, loss = 0.63 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:41.231950: step 161340, loss = 0.70 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:41.996539: step 161350, loss = 0.76 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:42.760565: step 161360, loss = 0.63 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:43.616415: step 161370, loss = 0.68 (1495.6 examples/sec; 0.086 sec/batch)
2017-05-05 20:51:44.283386: step 161380, loss = 0.61 (1919.1 examples/sec; 0.067 sec/batch)
2017-05-05 20:51:45.047730: step 161390, loss = 0.62 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:45.813908: step 161400, loss = 0.66 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:51:46.578274: step 161410, loss = 0.82 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:47.348452: step 161420, loss = 0.68 (1662.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:51:48.116127: step 161430, loss = 0.73 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:51:48.882869: step 161440, loss = 0.80 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:51:49.654117: step 161450, loss = 0.92 (1659.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:51:50.414967: step 161460, loss = 0.69 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:51.181301: step 161470, loss = 0.64 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:51:51.946256: step 161480, loss = 0.68 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:52.705532: step 161490, loss = 0.69 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:53.485648: step 161500, loss = 0.57 (1640.8 examples/sec; 0.078 sec/batch)
2017-05-05 20:51:54.248377: step 161510, loss = 0.77 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:55.009556: step 161520, loss = 0.80 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:55.762782: step 161530, loss = 0.78 (1699.4 examples/sec; 0.075 sec/batch)
2017-05-05 20:51:56.526030: step 161540, loss = 0.83 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:57.291079: step 161550, loss = 0.71 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:51:58.054040: step 161560, loss = 0.69 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:51:58.820016: step 161570, loss = 0.72 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:51:59.578017: step 161580, loss = 0.87 (1688.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:00.340268: step 161590, loss = 0.80 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:01.096827: step 161600, loss = 0.58 (1691.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:01.862467: step 161610, loss = 0.74 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:52:02.621651: step 161620, loss = 0.60 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:03.386329: step 161630, loss = 0.67 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:04.145173: step 161640, loss = 0.60 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:04.912255: step 161650, loss = 0.59 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:52:05.677549: step 161660, loss = 0.79 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:52:06.442059: step 161670, loss = 0.64 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:07.205197: step 161680, loss = 0.56 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:07.964779: step 161690, loss = 0.64 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:08.730305: step 161700, loss = 0.81 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:52:09.496777: step 161710, loss = 0.62 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:52:10.266012: step 161720, loss = 0.86 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:52:11.035498: step 161730, loss = 0.69 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:52:11.792037: step 161740, loss = 0.74 (1691.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:12.558783: step 161750, loss = 0.60 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:52:13.324498: step 161760, loss = 0.58 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:52:14.094586: step 161770, loss = 0.70 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:52:14.858226: step 161780, loss = 0.62 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:15.619337: step 161790, loss = 0.71 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:16.383481: step 161800, loss = 0.71 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:17.147085: step 161810, loss = 0.69 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:17.912108: step 161820, loss = 0.77 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:52:18.679007: step 161830, loss = 0.82 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:52:19.438465: step 161840, loss = 0.68 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:20.198024: step 161850, loss = 0.73 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:20.960518: step 161860, loss = 0.69 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:21.721750: step 161870, loss = 0.60 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:22.486901: step 161880, loss = 0.77 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:52:23.257123: step 161890, loss = 0.64 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:52:24.014908: step 161900, loss = 0.64 (1689.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:24.783986: step 161910, loss = 0.72 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:52:25.551305: step 161920, loss = 0.71 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:52:26.308677: step 161930, loss = 0.79 (1690.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:27.070184: step 161940, loss = 0.72 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:27.827599: step 161950, loss = 0.71 (1690.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:28.589400: step 161960, loss = 0.83 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:29.355992: step 161970, loss = 0.64 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:52:30.117858: step 161980, loss = 0.57 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:30.876032: step 161990, loss = 0.70 (1688.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:31.630333: step 162000, loss = 0.87 (1697.0 examples/sec; 0.075 sec/batch)
2017-05-05 20:52:32.399041: step 162010, loss = 0.91 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:52:33.170438: step 162020, loss = 0.66 (1659.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:52:33.932132: step 162030, loss = 0.72 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:34.704161: step 162040, loss = 0.75 (1658.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:52:35.464642: step 162050, loss = 0.61 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:36.220798: step 162060, loss = 0.74 (1692.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:36.985859: step 162070, loss = 0.75 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:52:37.747171: step 162080, loss = 0.80 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:38.513538: step 162090, loss = 0.87 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:52:39.278850: step 162100, loss = 0.48 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:52:40.033505: step 162110, loss = 0.67 (1696.2 examples/sec; 0.075 sec/batch)
2017-05-05 20:52:40.790585: step 162120, loss = 0.64 (1690.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:41.549005: step 162130, loss = 0.70 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:42.311780: step 162140, loss = 0.64 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:43.085016: step 162150, loss = 0.56 (1655.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:52:43.837546: step 162160, loss = 0.82 (1700.9 examples/sec; 0.075 sec/batch)
2017-05-05 20:52:44.604695: step 162170, loss = 0.62 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:52:45.370599: step 162180, loss = 0.92 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:52:46.134831: step 162190, loss = 0.73 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:46.905509: step 162200, loss = 0.87 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:52:47.665151: step 162210, loss = 0.67 (1685.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:48.442305: step 162220, loss = 0.73 (1647.0 examples/sec; 0.078 sec/batch)
2017-05-05 20:52:49.204627: step 162230, loss = 0.59 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:49.973848: step 162240, loss = 0.61 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:52:50.742297: step 162250, loss = 0.67 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:52:51.506388: step 162260, loss = 0.82 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:52.271374: step 162270, loss = 0.67 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:53.030222: step 162280, loss = 0.64 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:53.794290: step 162290, loss = 0.75 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:54.565252: step 162300, loss = 0.65 (1660.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:52:55.332425: step 162310, loss = 0.73 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:52:56.085256: step 162320, loss = 0.68 (1700.2 examples/sec; 0.075 sec/batch)
2017-05-05 20:52:56.852986: step 162330, loss = 0.68 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:52:57.615731: step 162340, loss = 0.65 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:58.375303: step 162350, loss = 0.74 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:52:59.239213: step 162360, loss = 0.88 (1481.6 examples/sec; 0.086 sec/batch)
2017-05-05 20:52:59.906880: step 162370, loss = 0.67 (1917.1 examples/sec; 0.067 sec/batch)
2017-05-05 20:53:00.670865: step 162380, loss = 0.55 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:01.434197: step 162390, loss = 0.74 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:02.199252: step 162400, loss = 0.69 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:02.963171: step 162410, loss = 0.69 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:03.718581: step 162420, loss = 0.74 (1694.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:04.484641: step 162430, loss = 0.78 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:05.255224: step 162440, loss = 0.72 (1661.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:06.020268: step 162450, loss = 0.69 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:06.790083: step 162460, loss = 0.64 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:07.547585: step 162470, loss = 0.77 (1689.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:08.307680: step 162480, loss = 0.60 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:09.075594: step 162490, loss = 0.89 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:09.851106: step 162500, loss = 0.69 (1650.5 examples/sec; 0.078 sec/batch)
2017-05-05 20:53:10.612700: step 162510, loss = 0.75 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:11.365594: step 162520, loss = 0.83 (1700.1 examples/sec; 0.075 sec/batch)
2017-05-05 20:53:12.128775: step 162530, loss = 0.51 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:12.895551: step 162540, loss = 0.62 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:13.668915: step 162550, loss = 0.69 (1655.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:14.451210: step 162560, loss = 0.70 (1636.2 examples/sec; 0.078 sec/batch)
2017-05-05 20:53:15.221381: step 162570, loss = 0.67 (1662.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:15.975714: step 162580, loss = 0.67 (1696.8 examples/sec; 0.075 sec/batch)
2017-05-05 20:53:16.734871: step 162590, loss = 0.70 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:17.505852: step 162600, loss = 0.75 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:18.271244: step 162610, loss = 0.84 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:19.039702: step 162620, loss = 0.81 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:19.795856: step 162630, loss = 0.68 (1692.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:20.556001: step 162640, loss = 0.74 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:21.321493: step 162650, loss = 0.84 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:22.077303: step 162660, loss = 0.47 (1693.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:22.846884: step 162670, loss = 0.58 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:23.608253: step 162680, loss = 0.70 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:24.374526: step 162690, loss = 0.75 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:25.138498: step 162700, loss = 0.66 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:25.901005: step 162710, loss = 0.88 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:26.666951: step 162720, loss = 0.70 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:27.428051: step 162730, loss = 0.66 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:28.187164: step 162740, loss = 0.76 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:28.953484: step 162750, loss = 0.71 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:29.723209: step 162760, loss = 0.62 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:30.480510: step 162770, loss = 0.53 (1690.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:31.245873: step 162780, loss = 0.67 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:31.999978: step 162790, loss = 0.84 (1697.4 examples/sec; 0.075 sec/batch)
2017-05-05 20:53:32.766450: step 162800, loss = 0.64 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:33.535124: step 162810, loss = 0.67 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:34.303780: step 162820, loss = 0.71 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:35.071459: step 162830, loss = 0.64 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:35.832298: step 162840, loss = 0.70 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:36.593700: step 162850, loss = 0.81 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:37.360709: step 162860, loss = 0.58 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:38.132642: step 162870, loss = 0.72 (1658.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:38.895491: step 162880, loss = 0.70 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:39.647906: step 162890, loss = 0.69 (1701.2 examples/sec; 0.075 sec/batch)
2017-05-05 20:53:40.408603: step 162900, loss = 0.77 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:41.171539: step 162910, loss = 0.81 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:41.936535: step 162920, loss = 0.64 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:42.705066: step 162930, loss = 0.70 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:43.470672: step 162940, loss = 0.75 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:44.230854: step 162950, loss = 0.74 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:45.002274: step 162960, loss = 0.65 (1659.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:45.767927: step 162970, loss = 0.76 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:46.536640: step 162980, loss = 0.66 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:47.300541: step 162990, loss = 0.70 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:48.060688: step 163000, loss = 0.72 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:48.828164: step 163010, loss = 0.74 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:49.600551: step 163020, loss = 0.90 (1657.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:50.368408: step 163030, loss = 0.77 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:51.127696: step 163040, loss = 0.62 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:51.882932: step 163050, loss = 0.69 (1694.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:52.646977: step 163060, loss = 0.49 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:53.417580: step 163070, loss = 0.80 (1661.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:54.181735: step 163080, loss = 0.71 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:54.948636: step 163090, loss = 0.82 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:55.705839: step 163100, loss = 0.60 (1690.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:56.469391: step 163110, loss = 0.65 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:57.233098: step 163120, loss = 0.80 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:53:58.004861: step 163130, loss = 0.83 (1658.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:58.774507: step 163140, loss = 0.66 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:53:59.532234: step 163150, loss = 0.72 (1689.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:00.294266: step 163160, loss = 0.66 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:01.049949: step 163170, loss = 0.73 (1693.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:01.804612: step 163180, loss = 0.65 (1696.1 examples/sec; 0.075 sec/batch)
2017-05-05 20:54:02.572966: step 163190, loss = 0.78 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:54:03.340487: step 163200, loss = 0.61 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:54:04.100999: step 163210, loss = 0.68 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:04.859728: step 163220, loss = 0.93 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:05.623270: step 163230, loss = 0.74 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:06.388067: step 163240, loss = 0.65 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:07.159617: step 163250, loss = 0.61 (1659.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:54:07.909270: step 163260, loss = 0.68 (1707.5 examples/sec; 0.075 sec/batch)
2017-05-05 20:54:08.671722: step 163270, loss = 0.70 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:09.436266: step 163280, loss = 0.72 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:10.209077: step 163290, loss = 0.89 (1656.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:54:10.965965: step 163300, loss = 0.81 (1691.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:11.724040: step 163310, loss = 0.75 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:12.488464: step 163320, loss = 0.76 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:13.253794: step 163330, loss = 0.64 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:54:14.022042: step 163340, loss = 0.78 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:54:14.877173: step 163350, loss = 0.73 (1496.8 examples/sec; 0.086 sec/batch)
2017-05-05 20:54:15.538874: step 163360, loss = 0.72 (1934.4 examples/sec; 0.066 sec/batch)
2017-05-05 20:54:16.306968: step 163370, loss = 0.71 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:54:17.073209: step 163380, loss = 0.66 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:54:17.845744: step 163390, loss = 0.78 (1656.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:54:18.608662: step 163400, loss = 0.53 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:19.366034: step 163410, loss = 0.74 (1690.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:20.128849: step 163420, loss = 0.61 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:20.899206: step 163430, loss = 0.77 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:54:21.661431: step 163440, loss = 0.71 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:22.432278: step 163450, loss = 0.75 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:54:23.190083: step 163460, loss = 0.70 (1689.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:23.940689: step 163470, loss = 0.67 (1705.3 examples/sec; 0.075 sec/batch)
2017-05-05 20:54:24.709744: step 163480, loss = 0.58 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:54:25.472255: step 163490, loss = 0.69 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:26.236990: step 163500, loss = 0.59 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:26.998791: step 163510, loss = 0.79 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:27.751970: step 163520, loss = 0.84 (1699.5 examples/sec; 0.075 sec/batch)
2017-05-05 20:54:28.513659: step 163530, loss = 0.85 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:29.276571: step 163540, loss = 0.67 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:30.042861: step 163550, loss = 0.73 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:54:30.807217: step 163560, loss = 0.76 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:31.564721: step 163570, loss = 0.62 (1689.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:32.326773: step 163580, loss = 0.77 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:33.100669: step 163590, loss = 0.70 (1654.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:54:33.860242: step 163600, loss = 0.83 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:34.619161: step 163610, loss = 0.79 (1686.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:35.375531: step 163620, loss = 0.66 (1692.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:36.138023: step 163630, loss = 0.68 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:36.904776: step 163640, loss = 0.67 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:54:37.674370: step 163650, loss = 0.77 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:54:38.443809: step 163660, loss = 0.57 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:54:39.210550: step 163670, loss = 0.70 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:54:39.958863: step 163680, loss = 0.82 (1710.5 examples/sec; 0.075 sec/batch)
2017-05-05 20:54:40.724734: step 163690, loss = 0.66 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:54:41.485777: step 163700, loss = 0.76 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:42.247748: step 163710, loss = 0.80 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:43.013263: step 163720, loss = 0.76 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:54:43.768945: step 163730, loss = 0.69 (1693.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:44.534286: step 163740, loss = 0.74 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:54:45.302014: step 163750, loss = 0.81 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:54:46.076144: step 163760, loss = 0.71 (1653.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:54:46.840864: step 163770, loss = 0.63 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:47.591310: step 163780, loss = 0.71 (1705.6 examples/sec; 0.075 sec/batch)
2017-05-05 20:54:48.353741: step 163790, loss = 0.76 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:49.118607: step 163800, loss = 0.65 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:49.888504: step 163810, loss = 0.71 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:54:50.648753: step 163820, loss = 0.58 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:51.411542: step 163830, loss = 0.69 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:52.165652: step 163840, loss = 0.68 (1697.4 examples/sec; 0.075 sec/batch)
2017-05-05 20:54:52.923486: step 163850, loss = 0.69 (1689.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:53.691420: step 163860, loss = 0.71 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:54:54.458460: step 163870, loss = 0.67 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:54:55.222670: step 163880, loss = 0.83 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:55.980842: step 163890, loss = 0.92 (1688.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:54:56.746955: step 163900, loss = 0.66 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:54:57.514499: step 163910, loss = 0.73 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:54:58.281451: step 163920, loss = 0.73 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:54:59.048132: step 163930, loss = 0.79 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:54:59.802049: step 163940, loss = 0.70 (1697.8 examples/sec; 0.075 sec/batch)
2017-05-05 20:55:00.570412: step 163950, loss = 0.65 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:55:01.354486: step 163960, loss = 0.80 (1632.5 examples/sec; 0.078 sec/batch)
2017-05-05 20:55:02.120222: step 163970, loss = 0.71 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:55:02.884730: step 163980, loss = 0.70 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:03.644031: step 163990, loss = 0.75 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:04.405320: step 164000, loss = 0.57 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:05.171844: step 164010, loss = 0.65 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:55:05.938038: step 164020, loss = 0.75 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:55:06.704168: step 164030, loss = 0.74 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:55:07.458975: step 164040, loss = 0.65 (1695.8 examples/sec; 0.075 sec/batch)
2017-05-05 20:55:08.225343: step 164050, loss = 0.79 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:55:08.984072: step 164060, loss = 0.68 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:09.747632: step 164070, loss = 0.68 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:10.511999: step 164080, loss = 0.60 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:11.276144: step 164090, loss = 0.69 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:12.031863: step 164100, loss = 0.71 (1693.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:12.809210: step 164110, loss = 0.72 (1646.6 examples/sec; 0.078 sec/batch)
2017-05-05 20:55:13.576756: step 164120, loss = 0.70 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:55:14.342112: step 164130, loss = 0.75 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:55:15.108254: step 164140, loss = 0.67 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:55:15.866810: step 164150, loss = 0.80 (1687.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:16.626300: step 164160, loss = 0.70 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:17.395948: step 164170, loss = 0.60 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:55:18.159516: step 164180, loss = 0.76 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:18.929050: step 164190, loss = 0.75 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:55:19.689282: step 164200, loss = 0.53 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:20.452112: step 164210, loss = 0.58 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:21.222384: step 164220, loss = 0.99 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:55:21.983406: step 164230, loss = 0.60 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:22.748522: step 164240, loss = 0.66 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:55:23.508118: step 164250, loss = 0.78 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:24.270973: step 164260, loss = 0.69 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:25.035248: step 164270, loss = 0.65 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:25.806239: step 164280, loss = 0.63 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:55:26.563211: step 164290, loss = 0.53 (1691.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:27.333029: step 164300, loss = 0.57 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:55:28.091863: step 164310, loss = 0.63 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:28.864158: step 164320, loss = 0.73 (1657.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:55:29.629564: step 164330, loss = 0.59 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:55:30.492231: step 164340, loss = 0.77 (1483.8 examples/sec; 0.086 sec/batch)
2017-05-05 20:55:31.162291: step 164350, loss = 0.79 (1910.3 examples/sec; 0.067 sec/batch)
2017-05-05 20:55:31.915089: step 164360, loss = 0.67 (1700.3 examples/sec; 0.075 sec/batch)
2017-05-05 20:55:32.681106: step 164370, loss = 0.72 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:55:33.448821: step 164380, loss = 0.72 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:55:34.213257: step 164390, loss = 0.73 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:34.979224: step 164400, loss = 0.63 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:55:35.734880: step 164410, loss = 0.77 (1693.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:36.501061: step 164420, loss = 0.72 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:55:37.258002: step 164430, loss = 0.73 (1691.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:38.029785: step 164440, loss = 0.72 (1658.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:55:38.793276: step 164450, loss = 0.73 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:39.552973: step 164460, loss = 0.68 (1684.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:40.311501: step 164470, loss = 0.63 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:41.073979: step 164480, loss = 0.66 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:41.843403: step 164490, loss = 0.73 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:55:42.606249: step 164500, loss = 0.73 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:43.369220: step 164510, loss = 0.65 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:44.126485: step 164520, loss = 0.77 (1690.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:44.898123: step 164530, loss = 0.72 (1658.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:55:45.668801: step 164540, loss = 0.67 (1660.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:55:46.433260: step 164550, loss = 0.68 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:47.203762: step 164560, loss = 0.69 (1661.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:55:47.956415: step 164570, loss = 0.67 (1700.7 examples/sec; 0.075 sec/batch)
2017-05-05 20:55:48.722483: step 164580, loss = 0.68 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:55:49.488127: step 164590, loss = 0.70 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:55:50.246795: step 164600, loss = 0.73 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:51.008648: step 164610, loss = 0.63 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:51.766298: step 164620, loss = 0.74 (1689.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:52.541584: step 164630, loss = 0.63 (1651.0 examples/sec; 0.078 sec/batch)
2017-05-05 20:55:53.304504: step 164640, loss = 0.55 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:54.065896: step 164650, loss = 0.79 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:54.837337: step 164660, loss = 0.74 (1659.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:55:55.600782: step 164670, loss = 0.61 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:56.364475: step 164680, loss = 0.86 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:57.131403: step 164690, loss = 0.63 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:55:57.904132: step 164700, loss = 0.66 (1656.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:55:58.667488: step 164710, loss = 0.69 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:55:59.433276: step 164720, loss = 0.75 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:56:00.189241: step 164730, loss = 0.61 (1693.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:00.956637: step 164740, loss = 0.83 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:56:01.722069: step 164750, loss = 0.87 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:56:02.482663: step 164760, loss = 0.69 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:03.249787: step 164770, loss = 0.74 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:56:04.008093: step 164780, loss = 0.56 (1688.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:04.763956: step 164790, loss = 0.68 (1693.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:05.535293: step 164800, loss = 0.64 (1659.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:56:06.327660: step 164810, loss = 0.63 (1615.4 examples/sec; 0.079 sec/batch)
2017-05-05 20:56:07.091109: step 164820, loss = 0.69 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:07.847028: step 164830, loss = 0.71 (1693.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:08.606281: step 164840, loss = 0.55 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:09.375321: step 164850, loss = 0.69 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:56:10.137109: step 164860, loss = 0.74 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:10.896164: step 164870, loss = 0.67 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:11.653548: step 164880, loss = 0.66 (1690.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:12.417401: step 164890, loss = 0.65 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:13.185409: step 164900, loss = 0.71 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:56:13.941958: step 164910, loss = 0.74 (1691.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:14.706282: step 164920, loss = 0.70 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:15.465119: step 164930, loss = 0.62 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:16.228731: step 164940, loss = 0.76 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:16.993616: step 164950, loss = 0.76 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:17.763618: step 164960, loss = 0.68 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:56:18.532571: step 164970, loss = 0.62 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:56:19.296376: step 164980, loss = 0.64 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:20.047667: step 164990, loss = 0.57 (1703.7 examples/sec; 0.075 sec/batch)
2017-05-05 20:56:20.810733: step 165000, loss = 0.67 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:21.576773: step 165010, loss = 0.78 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:56:22.344868: step 165020, loss = 0.67 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:56:23.114809: step 165030, loss = 0.77 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:56:23.866904: step 165040, loss = 0.77 (1701.9 examples/sec; 0.075 sec/batch)
2017-05-05 20:56:24.635558: step 165050, loss = 0.60 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:56:25.401618: step 165060, loss = 0.82 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:56:26.165643: step 165070, loss = 0.59 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:26.926613: step 165080, loss = 0.70 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:27.683212: step 165090, loss = 0.70 (1691.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:28.450650: step 165100, loss = 0.65 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:56:29.219926: step 165110, loss = 0.56 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:56:29.985883: step 165120, loss = 0.77 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:56:30.744025: step 165130, loss = 0.70 (1688.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:31.503479: step 165140, loss = 0.70 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:32.262478: step 165150, loss = 0.60 (1686.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:33.024488: step 165160, loss = 0.75 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:33.788553: step 165170, loss = 0.70 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:34.550888: step 165180, loss = 0.72 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:35.316348: step 165190, loss = 0.61 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:56:36.077313: step 165200, loss = 0.69 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:36.847499: step 165210, loss = 0.76 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:56:37.610527: step 165220, loss = 0.77 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:38.375996: step 165230, loss = 0.67 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:56:39.141900: step 165240, loss = 0.63 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:56:39.902669: step 165250, loss = 0.68 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:40.661909: step 165260, loss = 0.73 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:41.433467: step 165270, loss = 0.77 (1659.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:56:42.193503: step 165280, loss = 0.59 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:42.960251: step 165290, loss = 0.69 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:56:43.707928: step 165300, loss = 0.72 (1712.0 examples/sec; 0.075 sec/batch)
2017-05-05 20:56:44.474410: step 165310, loss = 0.69 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:56:45.237142: step 165320, loss = 0.63 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:46.097242: step 165330, loss = 0.63 (1488.2 examples/sec; 0.086 sec/batch)
2017-05-05 20:56:46.767580: step 165340, loss = 0.72 (1909.5 examples/sec; 0.067 sec/batch)
2017-05-05 20:56:47.532005: step 165350, loss = 0.65 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:48.296963: step 165360, loss = 0.67 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:49.060980: step 165370, loss = 0.67 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:49.825500: step 165380, loss = 0.55 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:50.584678: step 165390, loss = 0.69 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:51.355548: step 165400, loss = 0.55 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:56:52.107265: step 165410, loss = 0.60 (1702.8 examples/sec; 0.075 sec/batch)
2017-05-05 20:56:52.873602: step 165420, loss = 0.67 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:56:53.635807: step 165430, loss = 0.73 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:54.397856: step 165440, loss = 0.63 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:55.161809: step 165450, loss = 0.68 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:55.919059: step 165460, loss = 0.71 (1690.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:56.686417: step 165470, loss = 0.61 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:56:57.451962: step 165480, loss = 0.67 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:56:58.213881: step 165490, loss = 0.68 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:56:58.980179: step 165500, loss = 0.69 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:56:59.738431: step 165510, loss = 0.71 (1688.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:00.502729: step 165520, loss = 0.86 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:01.286144: step 165530, loss = 0.69 (1633.9 examples/sec; 0.078 sec/batch)
2017-05-05 20:57:02.049803: step 165540, loss = 0.78 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:02.813916: step 165550, loss = 0.60 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:03.578736: step 165560, loss = 0.76 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:04.340007: step 165570, loss = 0.71 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:05.103140: step 165580, loss = 0.59 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:05.866325: step 165590, loss = 0.61 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:06.631516: step 165600, loss = 0.68 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:57:07.395150: step 165610, loss = 0.72 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:08.148631: step 165620, loss = 0.88 (1698.8 examples/sec; 0.075 sec/batch)
2017-05-05 20:57:08.913592: step 165630, loss = 0.66 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:09.677845: step 165640, loss = 0.70 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:10.441236: step 165650, loss = 0.83 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:11.210886: step 165660, loss = 0.64 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:57:11.967122: step 165670, loss = 0.90 (1692.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:12.728773: step 165680, loss = 0.63 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:13.490907: step 165690, loss = 0.78 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:14.258973: step 165700, loss = 0.65 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:57:15.027880: step 165710, loss = 0.95 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:57:15.777251: step 165720, loss = 0.53 (1708.1 examples/sec; 0.075 sec/batch)
2017-05-05 20:57:16.544193: step 165730, loss = 0.60 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:57:17.309334: step 165740, loss = 0.69 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:57:18.072988: step 165750, loss = 0.67 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:18.839513: step 165760, loss = 0.77 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:57:19.594799: step 165770, loss = 0.63 (1694.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:20.353340: step 165780, loss = 0.67 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:21.116643: step 165790, loss = 0.71 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:21.880459: step 165800, loss = 0.70 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:22.647071: step 165810, loss = 0.64 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:57:23.413744: step 165820, loss = 0.69 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:57:24.177041: step 165830, loss = 0.74 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:24.945068: step 165840, loss = 0.84 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:57:25.705200: step 165850, loss = 0.77 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:26.472948: step 165860, loss = 0.62 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:57:27.242415: step 165870, loss = 0.69 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:57:27.996495: step 165880, loss = 0.68 (1697.4 examples/sec; 0.075 sec/batch)
2017-05-05 20:57:28.758556: step 165890, loss = 0.75 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:29.517811: step 165900, loss = 0.59 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:30.282480: step 165910, loss = 0.76 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:31.044712: step 165920, loss = 0.72 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:31.802453: step 165930, loss = 0.88 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:32.571708: step 165940, loss = 0.70 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:57:33.329656: step 165950, loss = 0.83 (1688.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:34.089628: step 165960, loss = 0.67 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:34.859087: step 165970, loss = 0.77 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:57:35.616490: step 165980, loss = 0.59 (1690.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:36.383780: step 165990, loss = 0.71 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:57:37.150703: step 166000, loss = 0.68 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:57:37.915404: step 166010, loss = 0.71 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:38.683443: step 166020, loss = 0.57 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:57:39.449525: step 166030, loss = 0.66 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:57:40.214977: step 166040, loss = 0.67 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:57:40.972535: step 166050, loss = 0.70 (1689.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:41.745231: step 166060, loss = 0.73 (1656.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:57:42.509251: step 166070, loss = 0.72 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:43.274780: step 166080, loss = 0.68 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:57:44.027074: step 166090, loss = 0.62 (1701.5 examples/sec; 0.075 sec/batch)
2017-05-05 20:57:44.789410: step 166100, loss = 0.58 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:45.552698: step 166110, loss = 0.72 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:46.319772: step 166120, loss = 0.75 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:57:47.089659: step 166130, loss = 0.66 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:57:47.848557: step 166140, loss = 0.77 (1686.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:48.613037: step 166150, loss = 0.86 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:49.375827: step 166160, loss = 0.67 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:50.131536: step 166170, loss = 0.84 (1693.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:50.898773: step 166180, loss = 0.69 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:57:51.656076: step 166190, loss = 0.70 (1690.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:52.415809: step 166200, loss = 0.64 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:53.188469: step 166210, loss = 0.62 (1656.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:57:53.944382: step 166220, loss = 0.61 (1693.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:54.714951: step 166230, loss = 0.67 (1661.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:57:55.475743: step 166240, loss = 0.78 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:56.234343: step 166250, loss = 0.59 (1687.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:56.996430: step 166260, loss = 0.75 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:57.759616: step 166270, loss = 0.68 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:57:58.524639: step 166280, loss = 0.69 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:57:59.291697: step 166290, loss = 0.77 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:00.045404: step 166300, loss = 0.72 (1698.4 examples/sec; 0.075 sec/batch)
2017-05-05 20:58:00.810295: step 166310, loss = 0.59 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:01.691769: step 166320, loss = 0.53 (1452.1 examples/sec; 0.088 sec/batch)
2017-05-05 20:58:02.367922: step 166330, loss = 0.70 (1893.1 examples/sec; 0.068 sec/batch)
2017-05-05 20:58:03.137545: step 166340, loss = 0.64 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:03.901569: step 166350, loss = 0.65 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:04.657491: step 166360, loss = 0.66 (1693.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:05.429216: step 166370, loss = 0.79 (1658.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:06.193997: step 166380, loss = 0.67 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:06.958712: step 166390, loss = 0.57 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:07.718438: step 166400, loss = 0.67 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:08.483614: step 166410, loss = 0.59 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:09.254297: step 166420, loss = 0.78 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:10.010869: step 166430, loss = 0.75 (1691.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:10.777014: step 166440, loss = 0.90 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:11.542144: step 166450, loss = 0.75 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:12.294980: step 166460, loss = 0.64 (1700.2 examples/sec; 0.075 sec/batch)
2017-05-05 20:58:13.061674: step 166470, loss = 0.76 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:13.827309: step 166480, loss = 0.63 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:14.595796: step 166490, loss = 0.80 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:15.359609: step 166500, loss = 0.63 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:16.120247: step 166510, loss = 0.76 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:16.887395: step 166520, loss = 0.74 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:17.652322: step 166530, loss = 0.68 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:18.415509: step 166540, loss = 0.69 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:19.182513: step 166550, loss = 0.70 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:19.937795: step 166560, loss = 0.72 (1694.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:20.708015: step 166570, loss = 0.52 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:21.461511: step 166580, loss = 0.63 (1698.8 examples/sec; 0.075 sec/batch)
2017-05-05 20:58:22.226264: step 166590, loss = 0.75 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:22.992069: step 166600, loss = 0.80 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:23.747826: step 166610, loss = 0.85 (1693.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:24.506223: step 166620, loss = 0.87 (1687.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:25.271627: step 166630, loss = 0.77 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:26.037644: step 166640, loss = 0.69 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:26.802289: step 166650, loss = 0.61 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:27.562780: step 166660, loss = 0.73 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:28.323751: step 166670, loss = 0.75 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:29.088289: step 166680, loss = 0.80 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:29.853945: step 166690, loss = 0.71 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:30.616981: step 166700, loss = 0.63 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:31.381457: step 166710, loss = 0.87 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:32.134973: step 166720, loss = 0.71 (1698.7 examples/sec; 0.075 sec/batch)
2017-05-05 20:58:32.896778: step 166730, loss = 0.93 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:33.664712: step 166740, loss = 0.77 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:34.429274: step 166750, loss = 0.64 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:35.201481: step 166760, loss = 0.77 (1657.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:35.958612: step 166770, loss = 0.73 (1690.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:36.717075: step 166780, loss = 0.63 (1687.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:37.482670: step 166790, loss = 0.65 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:38.253832: step 166800, loss = 0.64 (1659.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:39.020057: step 166810, loss = 0.65 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:39.775800: step 166820, loss = 0.69 (1693.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:40.547338: step 166830, loss = 0.67 (1659.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:41.312598: step 166840, loss = 0.70 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:42.070753: step 166850, loss = 0.67 (1688.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:42.837046: step 166860, loss = 0.81 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:43.592707: step 166870, loss = 0.68 (1693.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:44.354278: step 166880, loss = 0.78 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:45.124160: step 166890, loss = 0.76 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:45.888417: step 166900, loss = 0.68 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:46.653824: step 166910, loss = 0.57 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:47.416126: step 166920, loss = 0.57 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:48.177168: step 166930, loss = 0.75 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:48.947249: step 166940, loss = 0.78 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:49.716037: step 166950, loss = 0.58 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:50.485728: step 166960, loss = 0.85 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:51.242039: step 166970, loss = 0.64 (1692.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:52.002377: step 166980, loss = 0.62 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:52.775030: step 166990, loss = 0.79 (1656.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:53.549895: step 167000, loss = 0.71 (1651.9 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:54.311343: step 167010, loss = 0.81 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:55.081270: step 167020, loss = 0.74 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:55.842875: step 167030, loss = 0.77 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:56.605834: step 167040, loss = 0.76 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:57.378467: step 167050, loss = 0.70 (1656.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:58.139758: step 167060, loss = 0.73 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:58:58.906060: step 167070, loss = 0.70 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:58:59.660411: step 167080, loss = 0.74 (1696.8 examples/sec; 0.075 sec/batch)
2017-05-05 20:59:00.420732: step 167090, loss = 0.79 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:01.187296: step 167100, loss = 0.79 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:59:01.948201: step 167110, loss = 0.70 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:02.712585: step 167120, loss = 0.88 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:03.473231: step 167130, loss = 0.61 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:04.237593: step 167140, loss = 0.74 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:04.997688: step 167150, loss = 0.80 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:05.761904: step 167160, loss = 0.55 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:06.526725: step 167170, loss = 0.63 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:07.288350: step 167180, loss = 0.62 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:08.046406: step 167190, loss = 0.66 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:08.807746: step 167200, loss = 0.66 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:09.581083: step 167210, loss = 0.63 (1655.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:59:10.343044: step 167220, loss = 0.81 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:11.106937: step 167230, loss = 0.61 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:11.865703: step 167240, loss = 0.64 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:12.625527: step 167250, loss = 0.59 (1684.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:13.391151: step 167260, loss = 0.75 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:59:14.155834: step 167270, loss = 0.63 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:14.919894: step 167280, loss = 0.62 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:15.677798: step 167290, loss = 0.74 (1688.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:16.435324: step 167300, loss = 0.70 (1689.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:17.294617: step 167310, loss = 0.75 (1489.6 examples/sec; 0.086 sec/batch)
2017-05-05 20:59:17.961463: step 167320, loss = 0.77 (1919.5 examples/sec; 0.067 sec/batch)
2017-05-05 20:59:18.726328: step 167330, loss = 0.70 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:19.490093: step 167340, loss = 0.65 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:20.251856: step 167350, loss = 0.78 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:21.009409: step 167360, loss = 0.69 (1689.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:21.772175: step 167370, loss = 0.63 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:22.536068: step 167380, loss = 0.72 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:23.300876: step 167390, loss = 0.74 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:24.062858: step 167400, loss = 0.72 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:24.826461: step 167410, loss = 0.64 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:25.601515: step 167420, loss = 0.73 (1651.5 examples/sec; 0.078 sec/batch)
2017-05-05 20:59:26.362682: step 167430, loss = 0.79 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:27.135163: step 167440, loss = 0.74 (1657.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:59:27.893892: step 167450, loss = 0.75 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:28.652843: step 167460, loss = 0.91 (1686.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:29.424161: step 167470, loss = 0.71 (1659.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:59:30.192300: step 167480, loss = 0.70 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 20:59:30.956089: step 167490, loss = 0.68 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:31.715517: step 167500, loss = 0.75 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:32.490543: step 167510, loss = 0.62 (1651.6 examples/sec; 0.078 sec/batch)
2017-05-05 20:59:33.270093: step 167520, loss = 0.70 (1642.0 examples/sec; 0.078 sec/batch)
2017-05-05 20:59:34.034058: step 167530, loss = 0.74 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:34.801544: step 167540, loss = 0.66 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:59:35.560961: step 167550, loss = 0.64 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:36.324177: step 167560, loss = 0.57 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:37.085824: step 167570, loss = 0.78 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:37.850894: step 167580, loss = 0.75 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:59:38.614838: step 167590, loss = 0.69 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:39.374858: step 167600, loss = 0.56 (1684.2 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:40.126323: step 167610, loss = 0.71 (1703.3 examples/sec; 0.075 sec/batch)
2017-05-05 20:59:40.889413: step 167620, loss = 0.59 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:41.654517: step 167630, loss = 0.65 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:59:42.415495: step 167640, loss = 0.73 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:43.176936: step 167650, loss = 0.76 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:43.937738: step 167660, loss = 0.61 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:44.692163: step 167670, loss = 0.56 (1696.7 examples/sec; 0.075 sec/batch)
2017-05-05 20:59:45.463810: step 167680, loss = 0.55 (1658.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:59:46.229541: step 167690, loss = 0.80 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 20:59:46.995508: step 167700, loss = 0.66 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 20:59:47.758115: step 167710, loss = 0.66 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:48.524507: step 167720, loss = 0.71 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:59:49.294332: step 167730, loss = 0.62 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 20:59:50.054240: step 167740, loss = 0.80 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:50.817150: step 167750, loss = 0.63 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:51.581170: step 167760, loss = 0.67 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:52.339402: step 167770, loss = 0.73 (1688.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:53.104344: step 167780, loss = 0.81 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:53.874506: step 167790, loss = 0.63 (1662.0 examples/sec; 0.077 sec/batch)
2017-05-05 20:59:54.640276: step 167800, loss = 0.69 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 20:59:55.402903: step 167810, loss = 0.74 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:56.165201: step 167820, loss = 0.70 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:56.929329: step 167830, loss = 0.72 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:57.693201: step 167840, loss = 0.79 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 20:59:58.458639: step 167850, loss = 0.63 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 20:59:59.226585: step 167860, loss = 0.66 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 20:59:59.987030: step 167870, loss = 0.69 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:00.750228: step 167880, loss = 0.60 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:01.511285: step 167890, loss = 0.72 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:02.283397: step 167900, loss = 0.57 (1657.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:00:03.048287: step 167910, loss = 0.64 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:03.801941: step 167920, loss = 0.84 (1698.4 examples/sec; 0.075 sec/batch)
2017-05-05 21:00:04.565802: step 167930, loss = 0.78 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:05.336460: step 167940, loss = 0.61 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:00:06.098523: step 167950, loss = 0.69 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:06.857105: step 167960, loss = 0.85 (1687.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:07.616443: step 167970, loss = 0.47 (1685.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:08.379284: step 167980, loss = 0.74 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:09.142302: step 167990, loss = 0.59 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:09.911630: step 168000, loss = 0.75 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:00:10.677579: step 168010, loss = 0.77 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:00:11.439521: step 168020, loss = 0.70 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:12.199286: step 168030, loss = 0.75 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:12.968300: step 168040, loss = 0.61 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:00:13.730764: step 168050, loss = 0.62 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:14.504292: step 168060, loss = 0.63 (1654.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:00:15.278279: step 168070, loss = 0.89 (1653.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:00:16.037074: step 168080, loss = 0.61 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:16.799317: step 168090, loss = 0.72 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:17.566608: step 168100, loss = 0.60 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:00:18.335458: step 168110, loss = 0.61 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:00:19.103911: step 168120, loss = 0.69 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:00:19.856909: step 168130, loss = 0.76 (1699.9 examples/sec; 0.075 sec/batch)
2017-05-05 21:00:20.622168: step 168140, loss = 0.73 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:00:21.384070: step 168150, loss = 0.64 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:22.144480: step 168160, loss = 0.54 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:22.911413: step 168170, loss = 0.93 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:00:23.672430: step 168180, loss = 0.74 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:24.441817: step 168190, loss = 0.87 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:00:25.204569: step 168200, loss = 0.77 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:25.968620: step 168210, loss = 0.68 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:26.728924: step 168220, loss = 0.72 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:27.488040: step 168230, loss = 0.69 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:28.246935: step 168240, loss = 0.69 (1686.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:29.009743: step 168250, loss = 1.00 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:29.773712: step 168260, loss = 0.72 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:30.538771: step 168270, loss = 0.67 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:00:31.305878: step 168280, loss = 0.66 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:00:32.072188: step 168290, loss = 0.62 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:00:32.934264: step 168300, loss = 0.70 (1484.8 examples/sec; 0.086 sec/batch)
2017-05-05 21:00:33.605819: step 168310, loss = 0.56 (1906.0 examples/sec; 0.067 sec/batch)
2017-05-05 21:00:34.370317: step 168320, loss = 0.85 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:35.132837: step 168330, loss = 0.61 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:35.893896: step 168340, loss = 0.59 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:36.663722: step 168350, loss = 0.63 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:00:37.421235: step 168360, loss = 0.64 (1689.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:38.184312: step 168370, loss = 0.65 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:38.947875: step 168380, loss = 0.55 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:39.700959: step 168390, loss = 0.56 (1699.7 examples/sec; 0.075 sec/batch)
2017-05-05 21:00:40.464752: step 168400, loss = 0.80 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:41.229961: step 168410, loss = 0.64 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:00:41.990201: step 168420, loss = 0.60 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:42.757121: step 168430, loss = 0.78 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:00:43.525568: step 168440, loss = 0.66 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:00:44.284995: step 168450, loss = 0.68 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:45.044002: step 168460, loss = 0.72 (1686.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:45.812861: step 168470, loss = 0.65 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:00:46.570545: step 168480, loss = 0.68 (1689.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:47.344485: step 168490, loss = 0.66 (1653.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:00:48.100380: step 168500, loss = 0.66 (1693.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:48.866474: step 168510, loss = 0.70 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:00:49.637677: step 168520, loss = 0.74 (1659.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:00:50.404812: step 168530, loss = 0.62 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:00:51.170640: step 168540, loss = 0.82 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:00:51.930729: step 168550, loss = 0.77 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:52.696905: step 168560, loss = 0.58 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:00:53.458734: step 168570, loss = 0.73 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:54.223220: step 168580, loss = 0.76 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:54.984038: step 168590, loss = 0.80 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:55.735811: step 168600, loss = 0.82 (1702.6 examples/sec; 0.075 sec/batch)
2017-05-05 21:00:56.501998: step 168610, loss = 0.76 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:00:57.262562: step 168620, loss = 0.62 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:58.031129: step 168630, loss = 0.73 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:00:58.791389: step 168640, loss = 0.81 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:00:59.546030: step 168650, loss = 0.64 (1696.2 examples/sec; 0.075 sec/batch)
2017-05-05 21:01:00.305188: step 168660, loss = 0.61 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:01.070997: step 168670, loss = 0.66 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:01.839271: step 168680, loss = 0.67 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:02.613738: step 168690, loss = 0.81 (1652.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:03.382964: step 168700, loss = 0.73 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:04.142661: step 168710, loss = 0.65 (1684.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:04.905040: step 168720, loss = 0.70 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:05.667284: step 168730, loss = 0.77 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:06.434375: step 168740, loss = 0.52 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:07.196494: step 168750, loss = 0.67 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:07.951865: step 168760, loss = 0.72 (1694.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:08.712301: step 168770, loss = 0.69 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:09.476893: step 168780, loss = 0.64 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:10.240285: step 168790, loss = 0.80 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:11.007191: step 168800, loss = 0.60 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:11.757732: step 168810, loss = 0.71 (1705.4 examples/sec; 0.075 sec/batch)
2017-05-05 21:01:12.525760: step 168820, loss = 0.78 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:13.294714: step 168830, loss = 0.78 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:14.060631: step 168840, loss = 0.58 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:14.827919: step 168850, loss = 0.62 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:15.593031: step 168860, loss = 0.71 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:16.353731: step 168870, loss = 0.68 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:17.119527: step 168880, loss = 0.68 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:17.880233: step 168890, loss = 0.75 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:18.654563: step 168900, loss = 0.74 (1653.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:19.417158: step 168910, loss = 0.74 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:20.172994: step 168920, loss = 0.61 (1693.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:20.937394: step 168930, loss = 0.72 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:21.707095: step 168940, loss = 0.65 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:22.475237: step 168950, loss = 0.81 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:23.240471: step 168960, loss = 0.66 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:23.993938: step 168970, loss = 0.58 (1698.8 examples/sec; 0.075 sec/batch)
2017-05-05 21:01:24.756862: step 168980, loss = 0.78 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:25.529259: step 168990, loss = 0.69 (1657.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:26.293962: step 169000, loss = 0.68 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:27.067658: step 169010, loss = 0.66 (1654.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:27.818874: step 169020, loss = 0.73 (1703.9 examples/sec; 0.075 sec/batch)
2017-05-05 21:01:28.587533: step 169030, loss = 0.72 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:29.355993: step 169040, loss = 0.59 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:30.122737: step 169050, loss = 0.80 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:30.896040: step 169060, loss = 0.72 (1655.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:31.648852: step 169070, loss = 0.82 (1700.3 examples/sec; 0.075 sec/batch)
2017-05-05 21:01:32.409156: step 169080, loss = 0.77 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:33.180793: step 169090, loss = 0.73 (1658.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:33.947274: step 169100, loss = 0.55 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:34.709649: step 169110, loss = 0.70 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:35.470540: step 169120, loss = 0.69 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:36.233389: step 169130, loss = 0.70 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:36.997244: step 169140, loss = 0.71 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:37.763902: step 169150, loss = 0.71 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:38.524564: step 169160, loss = 0.79 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:39.287515: step 169170, loss = 0.68 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:40.050406: step 169180, loss = 0.77 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:40.810997: step 169190, loss = 0.81 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:41.579560: step 169200, loss = 0.83 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:42.345076: step 169210, loss = 0.72 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:43.108370: step 169220, loss = 0.80 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:43.859538: step 169230, loss = 0.75 (1704.0 examples/sec; 0.075 sec/batch)
2017-05-05 21:01:44.624720: step 169240, loss = 0.69 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:45.390251: step 169250, loss = 0.65 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:46.151044: step 169260, loss = 0.66 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:46.915401: step 169270, loss = 0.65 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:47.679216: step 169280, loss = 0.75 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:48.535612: step 169290, loss = 0.77 (1494.6 examples/sec; 0.086 sec/batch)
2017-05-05 21:01:49.204943: step 169300, loss = 0.77 (1912.4 examples/sec; 0.067 sec/batch)
2017-05-05 21:01:49.973331: step 169310, loss = 0.87 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:50.743721: step 169320, loss = 0.63 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:51.512210: step 169330, loss = 0.59 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:52.274351: step 169340, loss = 0.80 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:53.039248: step 169350, loss = 0.76 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:53.807966: step 169360, loss = 0.68 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:54.581673: step 169370, loss = 0.54 (1654.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:55.342655: step 169380, loss = 0.62 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:56.098601: step 169390, loss = 0.74 (1693.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:56.865180: step 169400, loss = 0.81 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:57.633976: step 169410, loss = 0.71 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:58.401741: step 169420, loss = 0.79 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:01:59.166578: step 169430, loss = 0.71 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:01:59.925020: step 169440, loss = 0.61 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:00.689401: step 169450, loss = 0.71 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:01.451908: step 169460, loss = 0.59 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:02.218625: step 169470, loss = 0.70 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:02:02.983380: step 169480, loss = 0.71 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:03.745602: step 169490, loss = 0.57 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:04.510274: step 169500, loss = 0.81 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:05.277545: step 169510, loss = 0.65 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:02:06.044565: step 169520, loss = 0.92 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:02:06.810872: step 169530, loss = 0.69 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:02:07.570002: step 169540, loss = 1.00 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:08.335200: step 169550, loss = 0.69 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:02:09.098301: step 169560, loss = 0.65 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:09.865065: step 169570, loss = 0.62 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:02:10.629525: step 169580, loss = 0.79 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:11.389928: step 169590, loss = 0.72 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:12.147720: step 169600, loss = 0.77 (1689.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:12.910313: step 169610, loss = 0.61 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:13.676253: step 169620, loss = 0.74 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:02:14.438509: step 169630, loss = 0.72 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:15.200464: step 169640, loss = 0.75 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:15.953828: step 169650, loss = 0.68 (1699.0 examples/sec; 0.075 sec/batch)
2017-05-05 21:02:16.710044: step 169660, loss = 0.71 (1692.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:17.480695: step 169670, loss = 0.64 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:02:18.244864: step 169680, loss = 0.67 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:19.013946: step 169690, loss = 0.68 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:02:19.772208: step 169700, loss = 0.65 (1688.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:20.539792: step 169710, loss = 0.72 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:02:21.304136: step 169720, loss = 0.51 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:22.063916: step 169730, loss = 0.70 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:22.830377: step 169740, loss = 0.79 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:02:23.580314: step 169750, loss = 0.73 (1706.8 examples/sec; 0.075 sec/batch)
2017-05-05 21:02:24.345654: step 169760, loss = 0.81 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:02:25.113300: step 169770, loss = 0.73 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:02:25.885973: step 169780, loss = 0.49 (1656.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:02:26.651021: step 169790, loss = 0.66 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:02:27.414936: step 169800, loss = 0.72 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:28.179127: step 169810, loss = 0.71 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:28.938299: step 169820, loss = 0.66 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:29.708968: step 169830, loss = 0.68 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:02:30.475621: step 169840, loss = 0.75 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:02:31.239103: step 169850, loss = 0.65 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:31.993489: step 169860, loss = 0.73 (1696.7 examples/sec; 0.075 sec/batch)
2017-05-05 21:02:32.760523: step 169870, loss = 0.69 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:02:33.524968: step 169880, loss = 0.57 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:34.291399: step 169890, loss = 0.69 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:02:35.054521: step 169900, loss = 0.83 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:35.816949: step 169910, loss = 0.61 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:36.580314: step 169920, loss = 0.64 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:37.344032: step 169930, loss = 0.68 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:38.106049: step 169940, loss = 0.64 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:38.870508: step 169950, loss = 0.76 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:39.623351: step 169960, loss = 0.62 (1700.2 examples/sec; 0.075 sec/batch)
2017-05-05 21:02:40.394555: step 169970, loss = 0.82 (1659.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:02:41.172748: step 169980, loss = 0.74 (1644.8 examples/sec; 0.078 sec/batch)
2017-05-05 21:02:41.938299: step 169990, loss = 0.70 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:02:42.698634: step 170000, loss = 0.71 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:43.461874: step 170010, loss = 0.82 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:44.220717: step 170020, loss = 0.66 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:44.985033: step 170030, loss = 0.79 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:45.748018: step 170040, loss = 0.66 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:46.511889: step 170050, loss = 0.59 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:47.276018: step 170060, loss = 0.65 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:48.033578: step 170070, loss = 0.77 (1689.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:48.796349: step 170080, loss = 0.63 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:49.566664: step 170090, loss = 0.63 (1661.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:02:50.328565: step 170100, loss = 0.92 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:51.098618: step 170110, loss = 0.66 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:02:51.857529: step 170120, loss = 0.73 (1686.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:52.623668: step 170130, loss = 0.76 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:02:53.389336: step 170140, loss = 0.75 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:02:54.152175: step 170150, loss = 0.79 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:54.917594: step 170160, loss = 0.70 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:02:55.674530: step 170170, loss = 0.64 (1691.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:56.435227: step 170180, loss = 0.70 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:57.197649: step 170190, loss = 0.92 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:02:57.966218: step 170200, loss = 0.77 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:02:58.734131: step 170210, loss = 0.83 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:02:59.498560: step 170220, loss = 0.79 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:00.256997: step 170230, loss = 0.81 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:01.027918: step 170240, loss = 0.69 (1660.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:03:01.790010: step 170250, loss = 0.79 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:02.559656: step 170260, loss = 0.81 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:03:03.322373: step 170270, loss = 0.62 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:04.172523: step 170280, loss = 0.84 (1505.6 examples/sec; 0.085 sec/batch)
2017-05-05 21:03:04.840112: step 170290, loss = 0.75 (1917.4 examples/sec; 0.067 sec/batch)
2017-05-05 21:03:05.608599: step 170300, loss = 0.62 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:03:06.374915: step 170310, loss = 0.65 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:03:07.135823: step 170320, loss = 0.60 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:07.891939: step 170330, loss = 0.69 (1692.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:08.652567: step 170340, loss = 0.53 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:09.418665: step 170350, loss = 0.80 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:03:10.184783: step 170360, loss = 0.71 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:03:10.957498: step 170370, loss = 0.71 (1656.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:03:11.711341: step 170380, loss = 0.82 (1698.0 examples/sec; 0.075 sec/batch)
2017-05-05 21:03:12.478135: step 170390, loss = 0.65 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:03:13.237847: step 170400, loss = 0.63 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:14.009183: step 170410, loss = 0.75 (1659.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:03:14.769479: step 170420, loss = 0.71 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:15.526634: step 170430, loss = 0.71 (1690.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:16.292044: step 170440, loss = 0.70 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:03:17.049930: step 170450, loss = 0.71 (1688.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:17.820883: step 170460, loss = 0.71 (1660.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:03:18.581766: step 170470, loss = 0.85 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:19.342698: step 170480, loss = 0.62 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:20.100305: step 170490, loss = 0.72 (1689.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:20.868248: step 170500, loss = 0.65 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:03:21.627985: step 170510, loss = 0.76 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:22.386561: step 170520, loss = 0.71 (1687.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:23.158259: step 170530, loss = 0.68 (1658.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:03:23.914306: step 170540, loss = 0.70 (1693.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:24.674490: step 170550, loss = 0.74 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:25.443321: step 170560, loss = 0.73 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:03:26.208770: step 170570, loss = 0.57 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:03:26.969062: step 170580, loss = 0.62 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:27.721800: step 170590, loss = 0.67 (1700.5 examples/sec; 0.075 sec/batch)
2017-05-05 21:03:28.487772: step 170600, loss = 0.66 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:03:29.254681: step 170610, loss = 0.70 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:03:30.016861: step 170620, loss = 0.75 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:30.785776: step 170630, loss = 0.74 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:03:31.543324: step 170640, loss = 0.82 (1689.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:32.308595: step 170650, loss = 0.73 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:03:33.072676: step 170660, loss = 0.68 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:33.839295: step 170670, loss = 0.79 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:03:34.608680: step 170680, loss = 0.74 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:03:35.371873: step 170690, loss = 0.62 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:36.124063: step 170700, loss = 0.64 (1701.7 examples/sec; 0.075 sec/batch)
2017-05-05 21:03:36.891070: step 170710, loss = 0.71 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:03:37.654845: step 170720, loss = 0.61 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:38.417482: step 170730, loss = 0.63 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:39.180221: step 170740, loss = 0.60 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:39.933430: step 170750, loss = 0.72 (1699.4 examples/sec; 0.075 sec/batch)
2017-05-05 21:03:40.691985: step 170760, loss = 0.75 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:41.454032: step 170770, loss = 0.80 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:42.214342: step 170780, loss = 0.62 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:42.980177: step 170790, loss = 0.64 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:03:43.736942: step 170800, loss = 0.70 (1691.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:44.504660: step 170810, loss = 0.61 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:03:45.266488: step 170820, loss = 0.62 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:46.029187: step 170830, loss = 0.72 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:46.792826: step 170840, loss = 0.64 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:47.551603: step 170850, loss = 0.66 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:48.315011: step 170860, loss = 0.89 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:49.080053: step 170870, loss = 0.81 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:03:49.851579: step 170880, loss = 0.82 (1659.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:03:50.615329: step 170890, loss = 0.73 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:51.371166: step 170900, loss = 0.76 (1693.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:52.131002: step 170910, loss = 0.73 (1684.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:52.893792: step 170920, loss = 0.76 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:53.659103: step 170930, loss = 0.94 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:03:54.423296: step 170940, loss = 0.65 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:55.191218: step 170950, loss = 0.71 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:03:55.952891: step 170960, loss = 0.69 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:56.712475: step 170970, loss = 0.56 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:03:57.478213: step 170980, loss = 0.61 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:03:58.244904: step 170990, loss = 0.79 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:03:59.017382: step 171000, loss = 0.69 (1657.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:03:59.773311: step 171010, loss = 0.52 (1693.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:00.538594: step 171020, loss = 0.66 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:04:01.300223: step 171030, loss = 0.72 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:02.061282: step 171040, loss = 0.65 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:02.828459: step 171050, loss = 0.64 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:04:03.587566: step 171060, loss = 0.90 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:04.354561: step 171070, loss = 0.72 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:04:05.121673: step 171080, loss = 0.63 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:04:05.884276: step 171090, loss = 0.68 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:06.648350: step 171100, loss = 0.77 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:07.404569: step 171110, loss = 0.69 (1692.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:08.157919: step 171120, loss = 0.66 (1699.1 examples/sec; 0.075 sec/batch)
2017-05-05 21:04:08.919335: step 171130, loss = 0.74 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:09.682174: step 171140, loss = 0.78 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:10.444934: step 171150, loss = 0.73 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:11.206810: step 171160, loss = 0.66 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:11.961891: step 171170, loss = 0.69 (1695.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:12.720657: step 171180, loss = 0.71 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:13.487329: step 171190, loss = 0.69 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:04:14.250280: step 171200, loss = 0.75 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:15.020019: step 171210, loss = 0.63 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:04:15.778907: step 171220, loss = 0.68 (1686.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:16.538641: step 171230, loss = 0.76 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:17.306000: step 171240, loss = 0.66 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:04:18.069070: step 171250, loss = 0.77 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:18.833352: step 171260, loss = 0.84 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:19.688225: step 171270, loss = 0.57 (1497.3 examples/sec; 0.085 sec/batch)
2017-05-05 21:04:20.353381: step 171280, loss = 0.60 (1924.4 examples/sec; 0.067 sec/batch)
2017-05-05 21:04:21.117403: step 171290, loss = 0.73 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:21.889515: step 171300, loss = 0.82 (1657.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:04:22.645942: step 171310, loss = 0.71 (1692.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:23.408845: step 171320, loss = 0.81 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:24.169759: step 171330, loss = 0.77 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:24.930336: step 171340, loss = 0.73 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:25.693626: step 171350, loss = 0.89 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:26.458731: step 171360, loss = 0.56 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:04:27.223856: step 171370, loss = 0.62 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:04:27.983410: step 171380, loss = 0.69 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:28.747047: step 171390, loss = 0.67 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:29.510848: step 171400, loss = 0.73 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:30.278829: step 171410, loss = 0.75 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:04:31.049549: step 171420, loss = 0.80 (1660.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:04:31.803703: step 171430, loss = 0.80 (1697.3 examples/sec; 0.075 sec/batch)
2017-05-05 21:04:32.570005: step 171440, loss = 0.53 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:04:33.335671: step 171450, loss = 0.61 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:04:34.110254: step 171460, loss = 0.67 (1652.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:04:34.880869: step 171470, loss = 0.62 (1661.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:04:35.639429: step 171480, loss = 0.66 (1687.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:36.401182: step 171490, loss = 0.59 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:37.168120: step 171500, loss = 0.80 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:04:37.936912: step 171510, loss = 0.63 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:04:38.703086: step 171520, loss = 0.66 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:04:39.460599: step 171530, loss = 0.74 (1689.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:40.221906: step 171540, loss = 0.67 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:40.983141: step 171550, loss = 0.71 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:41.754149: step 171560, loss = 0.71 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:04:42.516322: step 171570, loss = 0.68 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:43.277064: step 171580, loss = 0.79 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:44.033345: step 171590, loss = 0.75 (1692.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:44.797540: step 171600, loss = 0.79 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:45.557078: step 171610, loss = 0.63 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:46.315984: step 171620, loss = 0.71 (1686.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:47.078200: step 171630, loss = 0.71 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:47.834968: step 171640, loss = 0.75 (1691.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:48.599691: step 171650, loss = 0.69 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:49.363267: step 171660, loss = 0.65 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:50.129276: step 171670, loss = 0.84 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:04:50.895856: step 171680, loss = 0.63 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:04:51.648993: step 171690, loss = 0.80 (1699.5 examples/sec; 0.075 sec/batch)
2017-05-05 21:04:52.412048: step 171700, loss = 0.55 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:53.173992: step 171710, loss = 0.84 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:53.936573: step 171720, loss = 0.93 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:54.701729: step 171730, loss = 0.66 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:04:55.466326: step 171740, loss = 0.65 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:56.222606: step 171750, loss = 0.55 (1692.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:56.982153: step 171760, loss = 0.65 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:57.753893: step 171770, loss = 0.90 (1658.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:04:58.514743: step 171780, loss = 0.89 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:04:59.271122: step 171790, loss = 0.70 (1692.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:00.030342: step 171800, loss = 0.78 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:00.795655: step 171810, loss = 0.83 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:01.557351: step 171820, loss = 0.62 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:02.330291: step 171830, loss = 0.57 (1656.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:03.093387: step 171840, loss = 0.79 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:03.851269: step 171850, loss = 0.74 (1688.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:04.613814: step 171860, loss = 0.67 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:05.381039: step 171870, loss = 0.62 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:06.148598: step 171880, loss = 0.82 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:06.908203: step 171890, loss = 0.80 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:07.667956: step 171900, loss = 0.72 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:08.433825: step 171910, loss = 0.66 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:09.200424: step 171920, loss = 0.60 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:09.963992: step 171930, loss = 0.72 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:10.728246: step 171940, loss = 0.62 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:11.485730: step 171950, loss = 0.84 (1689.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:12.243886: step 171960, loss = 0.71 (1688.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:13.009229: step 171970, loss = 0.88 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:13.776554: step 171980, loss = 0.68 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:14.544322: step 171990, loss = 0.61 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:15.314106: step 172000, loss = 0.63 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:16.072388: step 172010, loss = 0.67 (1688.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:16.840157: step 172020, loss = 0.79 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:17.604750: step 172030, loss = 0.56 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:18.367935: step 172040, loss = 0.62 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:19.134392: step 172050, loss = 0.56 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:19.891545: step 172060, loss = 0.75 (1690.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:20.658290: step 172070, loss = 0.65 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:21.419633: step 172080, loss = 0.72 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:22.189662: step 172090, loss = 0.69 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:22.948952: step 172100, loss = 0.78 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:23.702943: step 172110, loss = 0.74 (1697.6 examples/sec; 0.075 sec/batch)
2017-05-05 21:05:24.468431: step 172120, loss = 0.59 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:25.236793: step 172130, loss = 0.65 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:26.003043: step 172140, loss = 0.83 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:26.761976: step 172150, loss = 0.64 (1686.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:27.514319: step 172160, loss = 0.64 (1701.4 examples/sec; 0.075 sec/batch)
2017-05-05 21:05:28.282347: step 172170, loss = 0.63 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:29.049765: step 172180, loss = 0.79 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:29.815246: step 172190, loss = 0.81 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:30.580960: step 172200, loss = 0.52 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:31.343681: step 172210, loss = 0.77 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:32.103685: step 172220, loss = 0.78 (1684.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:32.863969: step 172230, loss = 0.72 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:33.633338: step 172240, loss = 0.89 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:34.401431: step 172250, loss = 0.65 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:35.268381: step 172260, loss = 0.79 (1476.5 examples/sec; 0.087 sec/batch)
2017-05-05 21:05:35.923223: step 172270, loss = 0.74 (1954.6 examples/sec; 0.065 sec/batch)
2017-05-05 21:05:36.683624: step 172280, loss = 0.65 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:37.447928: step 172290, loss = 0.73 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:38.217100: step 172300, loss = 0.67 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:38.978128: step 172310, loss = 0.73 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:39.730332: step 172320, loss = 0.80 (1701.7 examples/sec; 0.075 sec/batch)
2017-05-05 21:05:40.494652: step 172330, loss = 0.67 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:41.256989: step 172340, loss = 0.68 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:42.021514: step 172350, loss = 0.61 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:42.783896: step 172360, loss = 0.78 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:43.542048: step 172370, loss = 0.75 (1688.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:44.305348: step 172380, loss = 0.83 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:45.066069: step 172390, loss = 0.56 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:45.829993: step 172400, loss = 0.87 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:46.596471: step 172410, loss = 0.61 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:47.354726: step 172420, loss = 0.69 (1688.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:48.113945: step 172430, loss = 0.74 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:48.878955: step 172440, loss = 0.73 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:49.644572: step 172450, loss = 0.65 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:50.411971: step 172460, loss = 0.75 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:51.177208: step 172470, loss = 0.72 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:51.940179: step 172480, loss = 0.64 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:52.704371: step 172490, loss = 0.90 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:53.464907: step 172500, loss = 0.69 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:54.230030: step 172510, loss = 0.96 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:54.999613: step 172520, loss = 0.79 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:55.750425: step 172530, loss = 0.71 (1704.8 examples/sec; 0.075 sec/batch)
2017-05-05 21:05:56.514910: step 172540, loss = 0.83 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:57.287385: step 172550, loss = 0.73 (1657.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:58.053251: step 172560, loss = 0.72 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:05:58.814344: step 172570, loss = 0.64 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:05:59.582125: step 172580, loss = 0.69 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:00.351978: step 172590, loss = 0.76 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:01.120669: step 172600, loss = 0.72 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:01.881159: step 172610, loss = 0.63 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:02.643230: step 172620, loss = 0.91 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:03.403462: step 172630, loss = 0.66 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:04.163384: step 172640, loss = 0.59 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:04.922334: step 172650, loss = 0.67 (1686.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:05.700954: step 172660, loss = 0.85 (1643.9 examples/sec; 0.078 sec/batch)
2017-05-05 21:06:06.502007: step 172670, loss = 0.73 (1597.9 examples/sec; 0.080 sec/batch)
2017-05-05 21:06:07.266866: step 172680, loss = 0.70 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:08.019913: step 172690, loss = 0.68 (1699.8 examples/sec; 0.075 sec/batch)
2017-05-05 21:06:08.776889: step 172700, loss = 0.73 (1690.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:09.543419: step 172710, loss = 0.65 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:10.304662: step 172720, loss = 0.87 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:11.067954: step 172730, loss = 0.91 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:11.824699: step 172740, loss = 0.72 (1691.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:12.589068: step 172750, loss = 0.72 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:13.356706: step 172760, loss = 0.70 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:14.119994: step 172770, loss = 0.77 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:14.883692: step 172780, loss = 0.75 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:15.649127: step 172790, loss = 0.67 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:16.419785: step 172800, loss = 0.65 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:17.180666: step 172810, loss = 0.74 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:17.950069: step 172820, loss = 0.95 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:18.712329: step 172830, loss = 0.83 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:19.476495: step 172840, loss = 0.62 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:20.239310: step 172850, loss = 0.68 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:21.009148: step 172860, loss = 0.68 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:21.777101: step 172870, loss = 0.75 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:22.542839: step 172880, loss = 0.75 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:23.310095: step 172890, loss = 0.74 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:24.070915: step 172900, loss = 0.68 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:24.828159: step 172910, loss = 0.62 (1690.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:25.592179: step 172920, loss = 0.69 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:26.360392: step 172930, loss = 0.56 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:27.126611: step 172940, loss = 0.70 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:27.881493: step 172950, loss = 0.59 (1695.6 examples/sec; 0.075 sec/batch)
2017-05-05 21:06:28.647448: step 172960, loss = 0.70 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:29.412962: step 172970, loss = 0.76 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:30.177453: step 172980, loss = 0.77 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:30.943350: step 172990, loss = 0.68 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:31.698149: step 173000, loss = 0.74 (1695.8 examples/sec; 0.075 sec/batch)
2017-05-05 21:06:32.465341: step 173010, loss = 0.70 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:33.227578: step 173020, loss = 0.71 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:33.988882: step 173030, loss = 0.67 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:34.755187: step 173040, loss = 0.72 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:35.510672: step 173050, loss = 0.84 (1694.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:36.274108: step 173060, loss = 0.86 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:37.041071: step 173070, loss = 0.72 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:37.807972: step 173080, loss = 0.79 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:38.571461: step 173090, loss = 0.68 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:39.338295: step 173100, loss = 0.70 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:40.095018: step 173110, loss = 0.70 (1691.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:40.859921: step 173120, loss = 0.58 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:41.628172: step 173130, loss = 0.74 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:42.394658: step 173140, loss = 0.69 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:43.160066: step 173150, loss = 0.64 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:43.922326: step 173160, loss = 0.84 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:44.692773: step 173170, loss = 0.62 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:45.459220: step 173180, loss = 0.60 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:46.230016: step 173190, loss = 0.79 (1660.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:46.997467: step 173200, loss = 0.77 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:47.754326: step 173210, loss = 0.66 (1691.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:48.520179: step 173220, loss = 0.59 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:49.290898: step 173230, loss = 0.56 (1660.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:50.052851: step 173240, loss = 0.70 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:50.913010: step 173250, loss = 0.82 (1488.1 examples/sec; 0.086 sec/batch)
2017-05-05 21:06:51.581693: step 173260, loss = 0.56 (1914.2 examples/sec; 0.067 sec/batch)
2017-05-05 21:06:52.348377: step 173270, loss = 0.58 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:53.113605: step 173280, loss = 0.74 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:53.882810: step 173290, loss = 0.81 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:54.648330: step 173300, loss = 0.75 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:55.414215: step 173310, loss = 0.91 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:56.170803: step 173320, loss = 0.72 (1691.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:56.932849: step 173330, loss = 0.76 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:57.694998: step 173340, loss = 0.79 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:58.457614: step 173350, loss = 0.73 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:06:59.225563: step 173360, loss = 0.61 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:06:59.978806: step 173370, loss = 0.57 (1699.3 examples/sec; 0.075 sec/batch)
2017-05-05 21:07:00.751338: step 173380, loss = 0.70 (1656.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:01.516271: step 173390, loss = 0.59 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:02.282853: step 173400, loss = 0.83 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:03.051682: step 173410, loss = 0.45 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:03.813142: step 173420, loss = 0.66 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:04.579700: step 173430, loss = 0.70 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:05.343498: step 173440, loss = 0.67 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:06.121499: step 173450, loss = 0.61 (1645.2 examples/sec; 0.078 sec/batch)
2017-05-05 21:07:06.898521: step 173460, loss = 0.73 (1647.3 examples/sec; 0.078 sec/batch)
2017-05-05 21:07:07.662922: step 173470, loss = 0.66 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:08.439513: step 173480, loss = 0.80 (1648.2 examples/sec; 0.078 sec/batch)
2017-05-05 21:07:09.209707: step 173490, loss = 0.68 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:09.975417: step 173500, loss = 0.66 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:10.741949: step 173510, loss = 0.69 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:11.502280: step 173520, loss = 0.56 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:12.262218: step 173530, loss = 0.66 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:13.028578: step 173540, loss = 0.76 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:13.799006: step 173550, loss = 0.68 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:14.558608: step 173560, loss = 0.78 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:15.324201: step 173570, loss = 0.73 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:16.081857: step 173580, loss = 0.87 (1689.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:16.857936: step 173590, loss = 0.78 (1649.3 examples/sec; 0.078 sec/batch)
2017-05-05 21:07:17.625988: step 173600, loss = 0.65 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:18.389041: step 173610, loss = 0.83 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:19.161068: step 173620, loss = 0.71 (1658.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:19.916721: step 173630, loss = 0.69 (1693.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:20.680930: step 173640, loss = 0.68 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:21.447545: step 173650, loss = 0.77 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:22.215538: step 173660, loss = 0.65 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:22.979483: step 173670, loss = 0.60 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:23.732713: step 173680, loss = 0.75 (1699.4 examples/sec; 0.075 sec/batch)
2017-05-05 21:07:24.498695: step 173690, loss = 0.63 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:25.261104: step 173700, loss = 0.63 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:26.029200: step 173710, loss = 0.93 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:26.792835: step 173720, loss = 0.87 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:27.550929: step 173730, loss = 0.60 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:28.310464: step 173740, loss = 0.48 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:29.075113: step 173750, loss = 0.65 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:29.837919: step 173760, loss = 0.77 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:30.602135: step 173770, loss = 0.65 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:31.359702: step 173780, loss = 0.60 (1689.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:32.126207: step 173790, loss = 0.55 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:32.886318: step 173800, loss = 0.81 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:33.653124: step 173810, loss = 0.62 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:34.416097: step 173820, loss = 0.75 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:35.187912: step 173830, loss = 0.75 (1658.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:35.943405: step 173840, loss = 0.50 (1694.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:36.707132: step 173850, loss = 0.69 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:37.470397: step 173860, loss = 0.87 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:38.240239: step 173870, loss = 0.67 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:39.003832: step 173880, loss = 0.67 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:39.762180: step 173890, loss = 0.59 (1687.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:40.519451: step 173900, loss = 0.80 (1690.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:41.284192: step 173910, loss = 0.62 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:42.043379: step 173920, loss = 0.77 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:42.809868: step 173930, loss = 0.67 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:43.576230: step 173940, loss = 0.74 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:44.334212: step 173950, loss = 0.66 (1688.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:45.095860: step 173960, loss = 0.75 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:45.862416: step 173970, loss = 0.59 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:46.627978: step 173980, loss = 0.72 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:47.391258: step 173990, loss = 0.81 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:48.151998: step 174000, loss = 0.68 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:48.914376: step 174010, loss = 0.60 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:49.684603: step 174020, loss = 0.73 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:50.450333: step 174030, loss = 0.73 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:51.217910: step 174040, loss = 0.68 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:51.979209: step 174050, loss = 0.89 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:52.749133: step 174060, loss = 0.73 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:53.516691: step 174070, loss = 0.83 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:54.283331: step 174080, loss = 0.68 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:55.049279: step 174090, loss = 0.66 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:55.807002: step 174100, loss = 0.73 (1689.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:07:56.576147: step 174110, loss = 0.66 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:57.350748: step 174120, loss = 0.72 (1652.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:58.127056: step 174130, loss = 0.62 (1648.8 examples/sec; 0.078 sec/batch)
2017-05-05 21:07:58.897572: step 174140, loss = 0.76 (1661.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:07:59.654907: step 174150, loss = 0.78 (1690.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:00.418851: step 174160, loss = 0.69 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:01.191208: step 174170, loss = 0.79 (1657.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:08:01.952939: step 174180, loss = 0.69 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:02.717746: step 174190, loss = 0.73 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:03.486788: step 174200, loss = 0.82 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:08:04.254448: step 174210, loss = 0.79 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:08:05.015698: step 174220, loss = 0.70 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:05.782708: step 174230, loss = 0.82 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:08:06.653955: step 174240, loss = 0.76 (1469.2 examples/sec; 0.087 sec/batch)
2017-05-05 21:08:07.314752: step 174250, loss = 0.64 (1937.1 examples/sec; 0.066 sec/batch)
2017-05-05 21:08:08.073003: step 174260, loss = 0.52 (1688.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:08.838656: step 174270, loss = 0.74 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:08:09.599621: step 174280, loss = 0.86 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:10.367997: step 174290, loss = 0.58 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:08:11.133309: step 174300, loss = 0.63 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:08:11.890992: step 174310, loss = 0.66 (1689.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:12.656069: step 174320, loss = 0.63 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:08:13.416103: step 174330, loss = 0.68 (1684.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:14.184657: step 174340, loss = 0.55 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:08:14.948207: step 174350, loss = 0.63 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:15.704374: step 174360, loss = 0.74 (1692.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:16.470512: step 174370, loss = 0.65 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:08:17.234680: step 174380, loss = 0.71 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:18.004774: step 174390, loss = 0.66 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:08:18.771800: step 174400, loss = 0.57 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:08:19.534059: step 174410, loss = 0.60 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:20.297501: step 174420, loss = 0.81 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:21.056017: step 174430, loss = 0.69 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:21.821004: step 174440, loss = 0.69 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:22.587749: step 174450, loss = 0.59 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:08:23.348649: step 174460, loss = 0.67 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:24.104042: step 174470, loss = 0.67 (1694.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:24.863372: step 174480, loss = 0.66 (1685.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:25.624414: step 174490, loss = 0.64 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:26.392539: step 174500, loss = 0.83 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:08:27.154900: step 174510, loss = 0.69 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:27.912408: step 174520, loss = 0.66 (1689.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:28.674575: step 174530, loss = 0.50 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:29.446891: step 174540, loss = 0.68 (1657.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:08:30.207183: step 174550, loss = 0.64 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:30.973327: step 174560, loss = 0.58 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:08:31.734396: step 174570, loss = 0.71 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:32.500616: step 174580, loss = 0.76 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:08:33.273026: step 174590, loss = 0.80 (1657.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:08:34.035216: step 174600, loss = 0.77 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:34.797350: step 174610, loss = 0.61 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:35.558273: step 174620, loss = 0.70 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:36.315303: step 174630, loss = 0.69 (1690.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:37.085742: step 174640, loss = 0.69 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:08:37.849011: step 174650, loss = 0.70 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:38.613166: step 174660, loss = 0.75 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:39.375541: step 174670, loss = 0.70 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:40.134658: step 174680, loss = 0.69 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:40.900349: step 174690, loss = 0.72 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:08:41.660379: step 174700, loss = 0.75 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:42.429761: step 174710, loss = 0.60 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:08:43.198378: step 174720, loss = 0.66 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:08:43.959504: step 174730, loss = 0.73 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:44.718306: step 174740, loss = 0.69 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:45.478498: step 174750, loss = 0.74 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:46.235422: step 174760, loss = 0.69 (1691.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:46.999019: step 174770, loss = 0.72 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:47.757159: step 174780, loss = 0.69 (1688.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:48.522762: step 174790, loss = 0.77 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:08:49.287241: step 174800, loss = 0.62 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:50.049845: step 174810, loss = 0.87 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:50.814090: step 174820, loss = 0.68 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:51.574569: step 174830, loss = 0.85 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:52.333060: step 174840, loss = 0.81 (1687.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:53.108068: step 174850, loss = 0.68 (1651.6 examples/sec; 0.078 sec/batch)
2017-05-05 21:08:53.876263: step 174860, loss = 0.74 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:08:54.649770: step 174870, loss = 0.77 (1654.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:08:55.412150: step 174880, loss = 0.65 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:56.179290: step 174890, loss = 0.66 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:08:56.939548: step 174900, loss = 0.65 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:08:57.706761: step 174910, loss = 0.64 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:08:58.476767: step 174920, loss = 0.67 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:08:59.241830: step 174930, loss = 0.75 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:08:59.998484: step 174940, loss = 0.72 (1691.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:00.762134: step 174950, loss = 0.60 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:01.525311: step 174960, loss = 0.78 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:02.290011: step 174970, loss = 0.79 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:03.054075: step 174980, loss = 0.67 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:03.817593: step 174990, loss = 0.76 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:04.578720: step 175000, loss = 0.74 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:05.343040: step 175010, loss = 0.61 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:06.107228: step 175020, loss = 0.82 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:06.875268: step 175030, loss = 0.70 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:09:07.636939: step 175040, loss = 0.77 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:08.407092: step 175050, loss = 0.67 (1662.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:09:09.170328: step 175060, loss = 0.61 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:09.933480: step 175070, loss = 0.63 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:10.699057: step 175080, loss = 0.73 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:09:11.463433: step 175090, loss = 0.62 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:12.222866: step 175100, loss = 0.67 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:12.986746: step 175110, loss = 0.61 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:13.749159: step 175120, loss = 0.79 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:14.513398: step 175130, loss = 0.78 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:15.272999: step 175140, loss = 0.68 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:16.030651: step 175150, loss = 0.67 (1689.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:16.797011: step 175160, loss = 0.61 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:09:17.564928: step 175170, loss = 0.83 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:09:18.345937: step 175180, loss = 0.66 (1638.9 examples/sec; 0.078 sec/batch)
2017-05-05 21:09:19.115325: step 175190, loss = 0.73 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:09:19.877328: step 175200, loss = 0.79 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:20.643547: step 175210, loss = 0.66 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:09:21.401131: step 175220, loss = 0.66 (1689.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:22.263208: step 175230, loss = 0.69 (1484.8 examples/sec; 0.086 sec/batch)
2017-05-05 21:09:22.929143: step 175240, loss = 0.78 (1922.1 examples/sec; 0.067 sec/batch)
2017-05-05 21:09:23.689017: step 175250, loss = 0.73 (1684.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:24.456175: step 175260, loss = 0.62 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:09:25.218453: step 175270, loss = 0.68 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:25.980959: step 175280, loss = 0.72 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:26.741020: step 175290, loss = 0.87 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:27.507053: step 175300, loss = 0.65 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:09:28.265550: step 175310, loss = 0.59 (1687.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:29.027110: step 175320, loss = 0.93 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:29.787855: step 175330, loss = 0.56 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:30.554068: step 175340, loss = 0.73 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:09:31.319598: step 175350, loss = 0.64 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:09:32.082913: step 175360, loss = 0.75 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:32.849490: step 175370, loss = 0.66 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:09:33.617413: step 175380, loss = 0.80 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:09:34.381072: step 175390, loss = 0.70 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:35.148103: step 175400, loss = 0.60 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:09:35.902603: step 175410, loss = 0.68 (1696.5 examples/sec; 0.075 sec/batch)
2017-05-05 21:09:36.667402: step 175420, loss = 0.74 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:37.436203: step 175430, loss = 0.65 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:09:38.209276: step 175440, loss = 0.66 (1655.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:09:38.974297: step 175450, loss = 0.72 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:39.735183: step 175460, loss = 0.57 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:40.502606: step 175470, loss = 0.69 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:09:41.265479: step 175480, loss = 0.67 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:42.024086: step 175490, loss = 0.73 (1687.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:42.788391: step 175500, loss = 0.70 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:43.546250: step 175510, loss = 0.73 (1689.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:44.309670: step 175520, loss = 0.56 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:45.071514: step 175530, loss = 0.62 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:45.832815: step 175540, loss = 0.66 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:46.597548: step 175550, loss = 0.78 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:47.362470: step 175560, loss = 0.86 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:48.131916: step 175570, loss = 0.95 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:09:48.898920: step 175580, loss = 0.73 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:09:49.662417: step 175590, loss = 0.85 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:50.421668: step 175600, loss = 0.53 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:09:51.186705: step 175610, loss = 0.82 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:09:51.939694: step 175620, loss = 0.59 (1699.9 examples/sec; 0.075 sec/batch)
2017-05-05 21:09:52.710885: step 175630, loss = 0.73 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:09:53.477420: step 175640, loss = 0.66 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:09:54.251518: step 175650, loss = 0.89 (1653.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:09:55.032654: step 175660, loss = 0.61 (1638.6 examples/sec; 0.078 sec/batch)
2017-05-05 21:09:55.787397: step 175670, loss = 0.67 (1695.9 examples/sec; 0.075 sec/batch)
2017-05-05 21:09:56.555246: step 175680, loss = 0.69 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:09:57.321273: step 175690, loss = 0.61 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:09:58.086726: step 175700, loss = 0.71 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:09:58.852997: step 175710, loss = 0.65 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:09:59.613388: step 175720, loss = 0.55 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:00.376267: step 175730, loss = 0.82 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:01.140770: step 175740, loss = 0.74 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:01.912493: step 175750, loss = 0.74 (1658.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:10:02.683816: step 175760, loss = 0.73 (1659.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:10:03.444540: step 175770, loss = 0.77 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:04.199602: step 175780, loss = 0.84 (1695.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:04.967821: step 175790, loss = 0.68 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:10:05.730443: step 175800, loss = 0.61 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:06.495723: step 175810, loss = 0.67 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:10:07.259897: step 175820, loss = 0.89 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:08.021529: step 175830, loss = 0.61 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:08.783855: step 175840, loss = 0.74 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:09.557808: step 175850, loss = 0.70 (1653.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:10:10.317891: step 175860, loss = 0.60 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:11.083754: step 175870, loss = 0.57 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:10:11.837957: step 175880, loss = 0.77 (1697.2 examples/sec; 0.075 sec/batch)
2017-05-05 21:10:12.605496: step 175890, loss = 0.62 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:10:13.376565: step 175900, loss = 0.74 (1660.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:10:14.147552: step 175910, loss = 0.69 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:10:14.908923: step 175920, loss = 0.62 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:15.668526: step 175930, loss = 0.79 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:16.425797: step 175940, loss = 0.67 (1690.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:17.189835: step 175950, loss = 0.75 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:17.963304: step 175960, loss = 0.71 (1654.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:10:18.729388: step 175970, loss = 0.64 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:10:19.493685: step 175980, loss = 0.65 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:20.258598: step 175990, loss = 0.74 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:21.025928: step 176000, loss = 0.75 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:10:21.788332: step 176010, loss = 0.74 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:22.552170: step 176020, loss = 0.65 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:23.312938: step 176030, loss = 0.73 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:24.075086: step 176040, loss = 0.80 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:24.838006: step 176050, loss = 0.64 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:25.605204: step 176060, loss = 0.65 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:10:26.365735: step 176070, loss = 0.75 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:27.129493: step 176080, loss = 0.77 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:27.884492: step 176090, loss = 0.82 (1695.4 examples/sec; 0.075 sec/batch)
2017-05-05 21:10:28.645388: step 176100, loss = 0.62 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:29.412666: step 176110, loss = 0.65 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:10:30.172895: step 176120, loss = 0.78 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:30.940280: step 176130, loss = 0.65 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:10:31.694842: step 176140, loss = 0.70 (1696.4 examples/sec; 0.075 sec/batch)
2017-05-05 21:10:32.455930: step 176150, loss = 0.67 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:33.222731: step 176160, loss = 0.65 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:10:33.993401: step 176170, loss = 0.76 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:10:34.762571: step 176180, loss = 0.59 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:10:35.517695: step 176190, loss = 0.63 (1695.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:36.280653: step 176200, loss = 0.62 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:37.044660: step 176210, loss = 0.71 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:37.908047: step 176220, loss = 0.59 (1482.5 examples/sec; 0.086 sec/batch)
2017-05-05 21:10:38.582137: step 176230, loss = 0.76 (1898.9 examples/sec; 0.067 sec/batch)
2017-05-05 21:10:39.346657: step 176240, loss = 0.60 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:40.103133: step 176250, loss = 0.55 (1692.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:40.865179: step 176260, loss = 0.69 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:41.626541: step 176270, loss = 0.65 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:42.397872: step 176280, loss = 0.83 (1659.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:10:43.164518: step 176290, loss = 0.70 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:10:43.920088: step 176300, loss = 0.63 (1694.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:44.686676: step 176310, loss = 0.76 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:10:45.448318: step 176320, loss = 0.71 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:46.213282: step 176330, loss = 0.61 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:46.977676: step 176340, loss = 0.72 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:47.732615: step 176350, loss = 0.69 (1695.5 examples/sec; 0.075 sec/batch)
2017-05-05 21:10:48.499716: step 176360, loss = 0.72 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:10:49.264687: step 176370, loss = 0.70 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:50.027664: step 176380, loss = 0.82 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:50.801679: step 176390, loss = 0.79 (1653.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:10:51.559735: step 176400, loss = 0.72 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:52.319372: step 176410, loss = 0.75 (1685.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:53.084886: step 176420, loss = 0.62 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:10:53.855823: step 176430, loss = 0.67 (1660.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:10:54.616831: step 176440, loss = 0.62 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:55.381480: step 176450, loss = 0.71 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:56.141896: step 176460, loss = 0.67 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:56.908188: step 176470, loss = 0.59 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:10:57.666196: step 176480, loss = 0.74 (1688.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:58.428497: step 176490, loss = 0.73 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:10:59.197894: step 176500, loss = 0.74 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:10:59.952712: step 176510, loss = 0.71 (1695.8 examples/sec; 0.075 sec/batch)
2017-05-05 21:11:00.721820: step 176520, loss = 0.59 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:11:01.486127: step 176530, loss = 0.69 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:02.248263: step 176540, loss = 0.64 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:03.012306: step 176550, loss = 0.65 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:03.769485: step 176560, loss = 0.68 (1690.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:04.532401: step 176570, loss = 0.70 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:05.298664: step 176580, loss = 0.66 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:11:06.066097: step 176590, loss = 0.70 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:11:06.829992: step 176600, loss = 0.76 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:07.589753: step 176610, loss = 0.82 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:08.360286: step 176620, loss = 0.53 (1661.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:11:09.118811: step 176630, loss = 0.91 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:09.889899: step 176640, loss = 0.76 (1660.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:11:10.654776: step 176650, loss = 0.83 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:11.410640: step 176660, loss = 0.63 (1693.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:12.178223: step 176670, loss = 0.75 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:11:12.943131: step 176680, loss = 0.87 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:13.705669: step 176690, loss = 0.65 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:14.471841: step 176700, loss = 0.73 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:11:15.239008: step 176710, loss = 0.67 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:11:15.993621: step 176720, loss = 0.66 (1696.2 examples/sec; 0.075 sec/batch)
2017-05-05 21:11:16.752023: step 176730, loss = 0.61 (1687.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:17.515226: step 176740, loss = 0.72 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:18.280319: step 176750, loss = 0.71 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:11:19.045882: step 176760, loss = 0.76 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:11:19.805379: step 176770, loss = 0.84 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:20.565252: step 176780, loss = 0.54 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:21.328293: step 176790, loss = 0.61 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:22.084512: step 176800, loss = 0.57 (1692.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:22.849579: step 176810, loss = 0.74 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:11:23.608890: step 176820, loss = 0.63 (1685.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:24.366846: step 176830, loss = 0.68 (1688.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:25.136284: step 176840, loss = 0.63 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:11:25.901332: step 176850, loss = 0.69 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:11:26.667115: step 176860, loss = 0.60 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:11:27.428968: step 176870, loss = 0.70 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:28.186855: step 176880, loss = 0.73 (1688.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:28.951163: step 176890, loss = 0.72 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:29.718149: step 176900, loss = 0.76 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:11:30.476694: step 176910, loss = 0.78 (1687.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:31.244536: step 176920, loss = 0.74 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:11:31.994909: step 176930, loss = 0.61 (1705.8 examples/sec; 0.075 sec/batch)
2017-05-05 21:11:32.757320: step 176940, loss = 0.63 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:33.519479: step 176950, loss = 0.69 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:34.288131: step 176960, loss = 0.62 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:11:35.051446: step 176970, loss = 0.71 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:35.804578: step 176980, loss = 0.68 (1699.6 examples/sec; 0.075 sec/batch)
2017-05-05 21:11:36.567143: step 176990, loss = 0.54 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:37.334221: step 177000, loss = 0.76 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:11:38.100889: step 177010, loss = 0.73 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:11:38.876502: step 177020, loss = 0.72 (1650.3 examples/sec; 0.078 sec/batch)
2017-05-05 21:11:39.632921: step 177030, loss = 0.71 (1692.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:40.401661: step 177040, loss = 0.73 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:11:41.165131: step 177050, loss = 0.83 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:41.928987: step 177060, loss = 0.72 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:42.687543: step 177070, loss = 0.67 (1687.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:43.447154: step 177080, loss = 0.60 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:44.204592: step 177090, loss = 0.64 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:44.972873: step 177100, loss = 0.81 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:11:45.736734: step 177110, loss = 0.80 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:46.496399: step 177120, loss = 0.71 (1685.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:47.261636: step 177130, loss = 0.75 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:11:48.022086: step 177140, loss = 0.69 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:48.778151: step 177150, loss = 0.71 (1693.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:49.545965: step 177160, loss = 0.71 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:11:50.315148: step 177170, loss = 0.55 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:11:51.073581: step 177180, loss = 0.80 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:51.831325: step 177190, loss = 0.68 (1689.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:52.597025: step 177200, loss = 0.70 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:11:53.459040: step 177210, loss = 0.81 (1484.9 examples/sec; 0.086 sec/batch)
2017-05-05 21:11:54.130264: step 177220, loss = 0.59 (1907.0 examples/sec; 0.067 sec/batch)
2017-05-05 21:11:54.895860: step 177230, loss = 0.85 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:11:55.648758: step 177240, loss = 0.66 (1700.1 examples/sec; 0.075 sec/batch)
2017-05-05 21:11:56.415078: step 177250, loss = 0.67 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:11:57.175298: step 177260, loss = 0.67 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:11:57.944863: step 177270, loss = 0.68 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:11:58.713870: step 177280, loss = 0.60 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:11:59.472188: step 177290, loss = 0.80 (1688.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:00.233302: step 177300, loss = 0.83 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:00.996698: step 177310, loss = 0.65 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:01.764893: step 177320, loss = 0.80 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:02.528820: step 177330, loss = 0.63 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:03.293651: step 177340, loss = 0.55 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:04.055048: step 177350, loss = 0.73 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:04.818259: step 177360, loss = 0.76 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:05.582717: step 177370, loss = 0.84 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:06.347038: step 177380, loss = 0.69 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:07.113995: step 177390, loss = 0.67 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:07.864074: step 177400, loss = 0.86 (1706.5 examples/sec; 0.075 sec/batch)
2017-05-05 21:12:08.639579: step 177410, loss = 0.69 (1650.5 examples/sec; 0.078 sec/batch)
2017-05-05 21:12:09.405578: step 177420, loss = 0.55 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:10.174642: step 177430, loss = 0.74 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:10.941010: step 177440, loss = 0.71 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:11.690761: step 177450, loss = 0.81 (1707.2 examples/sec; 0.075 sec/batch)
2017-05-05 21:12:12.460251: step 177460, loss = 0.71 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:13.223091: step 177470, loss = 0.70 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:13.993681: step 177480, loss = 0.70 (1661.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:14.757288: step 177490, loss = 0.65 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:15.515745: step 177500, loss = 0.52 (1687.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:16.271820: step 177510, loss = 0.54 (1693.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:17.039283: step 177520, loss = 0.69 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:17.806047: step 177530, loss = 0.67 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:18.573235: step 177540, loss = 0.64 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:19.343121: step 177550, loss = 0.66 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:20.101660: step 177560, loss = 0.72 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:20.868036: step 177570, loss = 0.68 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:21.626649: step 177580, loss = 0.72 (1687.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:22.393523: step 177590, loss = 0.56 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:23.157921: step 177600, loss = 0.77 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:23.916417: step 177610, loss = 0.81 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:24.683421: step 177620, loss = 0.59 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:25.448782: step 177630, loss = 0.67 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:26.219872: step 177640, loss = 0.57 (1660.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:26.979536: step 177650, loss = 0.50 (1685.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:27.735484: step 177660, loss = 0.63 (1693.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:28.503691: step 177670, loss = 0.74 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:29.275858: step 177680, loss = 0.67 (1657.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:30.042142: step 177690, loss = 0.65 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:30.812774: step 177700, loss = 0.61 (1661.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:31.572412: step 177710, loss = 0.61 (1685.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:32.334482: step 177720, loss = 0.70 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:33.105717: step 177730, loss = 0.75 (1659.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:33.869650: step 177740, loss = 0.75 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:34.635166: step 177750, loss = 0.74 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:35.400547: step 177760, loss = 0.63 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:36.157471: step 177770, loss = 0.84 (1691.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:36.918716: step 177780, loss = 0.58 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:37.679505: step 177790, loss = 0.63 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:38.448731: step 177800, loss = 0.78 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:39.218999: step 177810, loss = 0.74 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:39.977200: step 177820, loss = 0.69 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:40.743493: step 177830, loss = 0.76 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:41.512194: step 177840, loss = 0.71 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:42.286769: step 177850, loss = 0.64 (1652.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:43.052002: step 177860, loss = 0.72 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:43.803334: step 177870, loss = 0.65 (1703.7 examples/sec; 0.075 sec/batch)
2017-05-05 21:12:44.569591: step 177880, loss = 0.91 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:45.334031: step 177890, loss = 0.63 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:46.101892: step 177900, loss = 0.66 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:46.868079: step 177910, loss = 0.70 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:47.622868: step 177920, loss = 0.77 (1695.8 examples/sec; 0.075 sec/batch)
2017-05-05 21:12:48.386379: step 177930, loss = 0.68 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:49.156559: step 177940, loss = 0.74 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:49.920282: step 177950, loss = 0.70 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:50.684142: step 177960, loss = 0.54 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:51.447228: step 177970, loss = 0.69 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:52.205700: step 177980, loss = 0.70 (1687.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:52.967560: step 177990, loss = 0.77 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:53.734859: step 178000, loss = 0.74 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:54.506501: step 178010, loss = 0.77 (1658.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:55.273678: step 178020, loss = 0.72 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:56.027656: step 178030, loss = 1.04 (1697.7 examples/sec; 0.075 sec/batch)
2017-05-05 21:12:56.791487: step 178040, loss = 0.54 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:57.555279: step 178050, loss = 0.80 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:12:58.328087: step 178060, loss = 0.82 (1656.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:59.093561: step 178070, loss = 0.67 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:12:59.843845: step 178080, loss = 0.56 (1706.0 examples/sec; 0.075 sec/batch)
2017-05-05 21:13:00.612269: step 178090, loss = 0.62 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:01.376431: step 178100, loss = 0.55 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:02.136052: step 178110, loss = 0.67 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:02.906318: step 178120, loss = 0.69 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:03.666532: step 178130, loss = 0.53 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:04.431373: step 178140, loss = 0.45 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:05.197137: step 178150, loss = 0.62 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:05.957182: step 178160, loss = 0.70 (1684.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:06.719299: step 178170, loss = 0.79 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:07.477631: step 178180, loss = 0.64 (1687.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:08.244480: step 178190, loss = 0.66 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:09.103162: step 178200, loss = 0.74 (1490.7 examples/sec; 0.086 sec/batch)
2017-05-05 21:13:09.773679: step 178210, loss = 0.74 (1909.0 examples/sec; 0.067 sec/batch)
2017-05-05 21:13:10.541647: step 178220, loss = 0.51 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:11.310476: step 178230, loss = 0.68 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:12.065456: step 178240, loss = 0.68 (1695.4 examples/sec; 0.075 sec/batch)
2017-05-05 21:13:12.831625: step 178250, loss = 0.64 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:13.597544: step 178260, loss = 0.68 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:14.359351: step 178270, loss = 0.57 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:15.132849: step 178280, loss = 0.74 (1654.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:15.885653: step 178290, loss = 0.66 (1700.3 examples/sec; 0.075 sec/batch)
2017-05-05 21:13:16.644306: step 178300, loss = 0.65 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:17.408564: step 178310, loss = 0.64 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:18.174762: step 178320, loss = 0.93 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:18.946214: step 178330, loss = 0.67 (1659.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:19.702076: step 178340, loss = 0.69 (1693.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:20.468918: step 178350, loss = 0.65 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:21.230777: step 178360, loss = 0.71 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:21.992205: step 178370, loss = 0.70 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:22.759641: step 178380, loss = 0.71 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:23.519889: step 178390, loss = 0.59 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:24.289961: step 178400, loss = 0.62 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:25.053684: step 178410, loss = 0.68 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:25.816850: step 178420, loss = 0.76 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:26.578128: step 178430, loss = 0.68 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:27.340573: step 178440, loss = 0.60 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:28.102589: step 178450, loss = 0.75 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:28.869284: step 178460, loss = 0.71 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:29.630009: step 178470, loss = 0.62 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:30.397440: step 178480, loss = 0.74 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:31.164604: step 178490, loss = 0.82 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:31.917996: step 178500, loss = 0.54 (1699.0 examples/sec; 0.075 sec/batch)
2017-05-05 21:13:32.684915: step 178510, loss = 0.71 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:33.450645: step 178520, loss = 0.72 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:34.215312: step 178530, loss = 0.45 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:34.981336: step 178540, loss = 0.57 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:35.736189: step 178550, loss = 0.64 (1695.7 examples/sec; 0.075 sec/batch)
2017-05-05 21:13:36.498894: step 178560, loss = 0.59 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:37.266129: step 178570, loss = 0.69 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:38.031690: step 178580, loss = 0.61 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:38.800713: step 178590, loss = 0.72 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:39.560944: step 178600, loss = 0.68 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:40.319276: step 178610, loss = 0.63 (1687.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:41.082545: step 178620, loss = 0.59 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:41.844673: step 178630, loss = 0.62 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:42.613526: step 178640, loss = 0.75 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:43.373550: step 178650, loss = 0.73 (1684.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:44.130806: step 178660, loss = 0.74 (1690.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:44.894345: step 178670, loss = 0.81 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:45.662626: step 178680, loss = 0.67 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:46.425457: step 178690, loss = 0.68 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:47.190521: step 178700, loss = 0.76 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:47.951043: step 178710, loss = 0.78 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:48.719215: step 178720, loss = 0.58 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:49.486611: step 178730, loss = 0.54 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:50.253505: step 178740, loss = 0.61 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:51.017239: step 178750, loss = 0.75 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:51.773116: step 178760, loss = 0.69 (1693.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:52.540544: step 178770, loss = 0.57 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:53.303980: step 178780, loss = 0.67 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:54.073507: step 178790, loss = 0.66 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:54.840190: step 178800, loss = 0.61 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:55.597679: step 178810, loss = 0.77 (1689.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:56.362045: step 178820, loss = 0.69 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:57.126700: step 178830, loss = 0.81 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:57.889673: step 178840, loss = 0.66 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:13:58.661529: step 178850, loss = 0.67 (1658.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:13:59.421040: step 178860, loss = 0.69 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:00.174693: step 178870, loss = 0.57 (1698.4 examples/sec; 0.075 sec/batch)
2017-05-05 21:14:00.937690: step 178880, loss = 0.58 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:01.699297: step 178890, loss = 0.63 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:02.458391: step 178900, loss = 0.57 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:03.227329: step 178910, loss = 0.71 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:14:03.983580: step 178920, loss = 0.76 (1692.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:04.744351: step 178930, loss = 0.81 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:05.509749: step 178940, loss = 0.70 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:14:06.269260: step 178950, loss = 0.79 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:07.031457: step 178960, loss = 0.74 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:07.782720: step 178970, loss = 0.88 (1703.8 examples/sec; 0.075 sec/batch)
2017-05-05 21:14:08.547930: step 178980, loss = 0.73 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:14:09.313680: step 178990, loss = 0.56 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:14:10.085388: step 179000, loss = 0.61 (1658.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:14:10.850030: step 179010, loss = 0.61 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:11.609957: step 179020, loss = 0.94 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:12.367109: step 179030, loss = 0.74 (1690.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:13.137422: step 179040, loss = 0.59 (1661.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:14:13.899885: step 179050, loss = 0.58 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:14.664790: step 179060, loss = 0.81 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:15.421729: step 179070, loss = 0.61 (1691.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:16.185783: step 179080, loss = 0.61 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:16.942534: step 179090, loss = 0.67 (1691.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:17.708664: step 179100, loss = 0.61 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:14:18.472901: step 179110, loss = 0.68 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:19.241775: step 179120, loss = 0.75 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:14:20.000738: step 179130, loss = 0.73 (1686.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:20.760618: step 179140, loss = 0.74 (1684.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:21.528722: step 179150, loss = 0.84 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:14:22.289397: step 179160, loss = 0.62 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:23.060509: step 179170, loss = 0.65 (1659.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:14:23.820839: step 179180, loss = 0.74 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:24.685500: step 179190, loss = 0.73 (1480.3 examples/sec; 0.086 sec/batch)
2017-05-05 21:14:25.356575: step 179200, loss = 0.76 (1907.4 examples/sec; 0.067 sec/batch)
2017-05-05 21:14:26.120764: step 179210, loss = 0.68 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:26.883247: step 179220, loss = 0.61 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:27.640963: step 179230, loss = 0.68 (1689.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:28.406818: step 179240, loss = 0.70 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:14:29.180083: step 179250, loss = 0.82 (1655.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:14:29.948131: step 179260, loss = 0.78 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:14:30.722136: step 179270, loss = 0.65 (1653.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:14:31.475379: step 179280, loss = 0.72 (1699.3 examples/sec; 0.075 sec/batch)
2017-05-05 21:14:32.241946: step 179290, loss = 0.85 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:14:33.007824: step 179300, loss = 0.64 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:14:33.771618: step 179310, loss = 0.72 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:34.533723: step 179320, loss = 0.62 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:35.295404: step 179330, loss = 0.72 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:36.057003: step 179340, loss = 0.70 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:36.813300: step 179350, loss = 0.78 (1692.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:37.583322: step 179360, loss = 0.87 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:14:38.349758: step 179370, loss = 0.74 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:14:39.110574: step 179380, loss = 0.78 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:39.866301: step 179390, loss = 0.85 (1693.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:40.629549: step 179400, loss = 0.63 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:41.396121: step 179410, loss = 0.50 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:14:42.160062: step 179420, loss = 0.74 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:42.915307: step 179430, loss = 0.79 (1694.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:43.669896: step 179440, loss = 0.69 (1696.3 examples/sec; 0.075 sec/batch)
2017-05-05 21:14:44.435519: step 179450, loss = 0.72 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:14:45.200582: step 179460, loss = 0.82 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:14:45.968476: step 179470, loss = 0.79 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:14:46.732217: step 179480, loss = 0.67 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:47.490877: step 179490, loss = 0.65 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:48.263674: step 179500, loss = 0.66 (1656.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:14:49.028077: step 179510, loss = 0.79 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:49.795659: step 179520, loss = 0.77 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:14:50.559682: step 179530, loss = 0.87 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:51.320540: step 179540, loss = 0.68 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:52.076777: step 179550, loss = 0.63 (1692.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:52.838342: step 179560, loss = 0.79 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:53.605634: step 179570, loss = 0.64 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:14:54.369309: step 179580, loss = 0.65 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:55.136647: step 179590, loss = 0.70 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:14:55.892475: step 179600, loss = 0.62 (1693.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:56.670647: step 179610, loss = 0.60 (1644.9 examples/sec; 0.078 sec/batch)
2017-05-05 21:14:57.437982: step 179620, loss = 0.72 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:14:58.206125: step 179630, loss = 0.71 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:14:58.965997: step 179640, loss = 0.69 (1684.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:14:59.720533: step 179650, loss = 0.65 (1696.4 examples/sec; 0.075 sec/batch)
2017-05-05 21:15:00.488289: step 179660, loss = 0.59 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:01.252717: step 179670, loss = 0.63 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:02.063221: step 179680, loss = 0.81 (1579.3 examples/sec; 0.081 sec/batch)
2017-05-05 21:15:02.856749: step 179690, loss = 0.70 (1613.1 examples/sec; 0.079 sec/batch)
2017-05-05 21:15:03.617157: step 179700, loss = 0.82 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:04.385454: step 179710, loss = 0.81 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:05.153062: step 179720, loss = 0.68 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:05.914856: step 179730, loss = 0.73 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:06.675108: step 179740, loss = 0.78 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:07.440791: step 179750, loss = 0.77 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:08.199928: step 179760, loss = 0.65 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:08.968456: step 179770, loss = 0.64 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:09.728208: step 179780, loss = 0.78 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:10.495097: step 179790, loss = 0.83 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:11.263304: step 179800, loss = 0.73 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:12.024403: step 179810, loss = 0.70 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:12.798685: step 179820, loss = 0.67 (1653.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:13.567269: step 179830, loss = 0.67 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:14.332272: step 179840, loss = 0.81 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:15.102063: step 179850, loss = 0.71 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:15.857972: step 179860, loss = 0.83 (1693.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:16.618405: step 179870, loss = 0.60 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:17.379348: step 179880, loss = 0.66 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:18.149084: step 179890, loss = 0.67 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:18.912152: step 179900, loss = 0.74 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:19.675012: step 179910, loss = 0.57 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:20.442311: step 179920, loss = 0.79 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:21.211227: step 179930, loss = 0.64 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:21.972532: step 179940, loss = 0.74 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:22.741960: step 179950, loss = 0.65 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:23.507005: step 179960, loss = 0.75 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:24.268980: step 179970, loss = 0.72 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:25.028374: step 179980, loss = 0.83 (1685.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:25.793141: step 179990, loss = 0.59 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:26.562347: step 180000, loss = 0.61 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:27.326754: step 180010, loss = 0.66 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:28.081053: step 180020, loss = 0.74 (1696.9 examples/sec; 0.075 sec/batch)
2017-05-05 21:15:28.845224: step 180030, loss = 0.64 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:29.611486: step 180040, loss = 0.62 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:30.382211: step 180050, loss = 0.69 (1660.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:31.147708: step 180060, loss = 0.64 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:31.904419: step 180070, loss = 0.66 (1691.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:32.670185: step 180080, loss = 0.67 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:33.432980: step 180090, loss = 0.65 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:34.199371: step 180100, loss = 0.62 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:34.960417: step 180110, loss = 0.72 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:35.720722: step 180120, loss = 0.83 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:36.480273: step 180130, loss = 0.78 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:37.243509: step 180140, loss = 0.62 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:38.008328: step 180150, loss = 0.65 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:38.774000: step 180160, loss = 0.67 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:39.530827: step 180170, loss =E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494019552.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 16 events to /tmp/cifar10_train/events.out.tfevents.1494019552.GHC33.GHC.ANDREW.CMU.EDU
 0.79 (1691.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:40.386312: step 180180, loss = 0.58 (1496.2 examples/sec; 0.086 sec/batch)
2017-05-05 21:15:41.056460: step 180190, loss = 0.63 (1910.0 examples/sec; 0.067 sec/batch)
2017-05-05 21:15:41.815110: step 180200, loss = 0.69 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:42.580491: step 180210, loss = 0.73 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:43.339550: step 180220, loss = 0.76 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:44.100733: step 180230, loss = 0.68 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:44.865037: step 180240, loss = 0.62 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:45.630422: step 180250, loss = 0.59 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:46.398668: step 180260, loss = 0.60 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:47.165886: step 180270, loss = 0.66 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:47.923524: step 180280, loss = 0.79 (1689.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:48.687641: step 180290, loss = 0.66 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:49.449218: step 180300, loss = 0.60 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:50.213904: step 180310, loss = 1.00 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:50.977029: step 180320, loss = 0.77 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:51.733708: step 180330, loss = 0.89 (1691.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:52.501277: step 180340, loss = 0.60 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:53.269302: step 180350, loss = 0.86 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:54.035057: step 180360, loss = 0.78 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:54.799165: step 180370, loss = 0.74 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:55.559524: step 180380, loss = 0.74 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:56.325942: step 180390, loss = 0.58 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:57.093979: step 180400, loss = 0.61 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:57.852582: step 180410, loss = 0.71 (1687.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:15:58.621705: step 180420, loss = 0.68 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:15:59.382870: step 180430, loss = 0.81 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:00.142036: step 180440, loss = 0.65 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:00.907675: step 180450, loss = 0.74 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:01.669388: step 180460, loss = 0.76 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:02.434614: step 180470, loss = 0.60 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:03.195522: step 180480, loss = 0.79 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:03.955655: step 180490, loss = 0.76 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:04.722018: step 180500, loss = 0.60 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:05.484345: step 180510, loss = 0.57 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:06.291875: step 180520, loss = 0.64 (1585.1 examples/sec; 0.081 sec/batch)
2017-05-05 21:16:07.059651: step 180530, loss = 0.73 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:07.815133: step 180540, loss = 0.63 (1694.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:08.578205: step 180550, loss = 0.70 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:09.349452: step 180560, loss = 0.67 (1659.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:10.115034: step 180570, loss = 0.78 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:10.882469: step 180580, loss = 0.56 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:11.639068: step 180590, loss = 0.63 (1691.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:12.405855: step 180600, loss = 0.60 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:13.172485: step 180610, loss = 0.76 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:13.941044: step 180620, loss = 0.69 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:14.707855: step 180630, loss = 0.67 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:15.466672: step 180640, loss = 0.63 (1686.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:16.227906: step 180650, loss = 0.88 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:16.991960: step 180660, loss = 0.66 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:17.762159: step 180670, loss = 0.48 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:18.526827: step 180680, loss = 0.62 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:19.291503: step 180690, loss = 0.77 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:20.052650: step 180700, loss = 0.66 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:20.812563: step 180710, loss = 0.69 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:21.573282: step 180720, loss = 0.86 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:22.339585: step 180730, loss = 0.64 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:23.098621: step 180740, loss = 0.71 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:23.849953: step 180750, loss = 0.73 (1703.7 examples/sec; 0.075 sec/batch)
2017-05-05 21:16:24.615546: step 180760, loss = 0.83 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:25.380904: step 180770, loss = 0.66 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:26.149315: step 180780, loss = 0.69 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:26.917174: step 180790, loss = 0.79 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:27.676768: step 180800, loss = 0.68 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:28.434688: step 180810, loss = 0.76 (1688.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:29.199484: step 180820, loss = 0.74 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:29.979646: step 180830, loss = 0.67 (1640.7 examples/sec; 0.078 sec/batch)
2017-05-05 21:16:30.752516: step 180840, loss = 0.67 (1656.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:31.514102: step 180850, loss = 0.67 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:32.276588: step 180860, loss = 0.83 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:33.048319: step 180870, loss = 0.61 (1658.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:33.808936: step 180880, loss = 0.60 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:34.574587: step 180890, loss = 0.57 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:35.335826: step 180900, loss = 0.56 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:36.091942: step 180910, loss = 0.65 (1692.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:36.858846: step 180920, loss = 0.75 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:37.626171: step 180930, loss = 0.61 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:38.386522: step 180940, loss = 0.75 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:39.154161: step 180950, loss = 0.73 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:39.908473: step 180960, loss = 0.64 (1696.9 examples/sec; 0.075 sec/batch)
2017-05-05 21:16:40.675178: step 180970, loss = 0.77 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:41.440680: step 180980, loss = 0.78 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:42.210038: step 180990, loss = 0.58 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:42.972180: step 181000, loss = 0.68 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:43.731349: step 181010, loss = 0.59 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:44.492052: step 181020, loss = 0.71 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:45.256451: step 181030, loss = 0.66 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:46.018653: step 181040, loss = 0.83 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:46.783288: step 181050, loss = 0.71 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:47.540194: step 181060, loss = 0.77 (1691.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:48.306442: step 181070, loss = 0.87 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:49.078228: step 181080, loss = 0.70 (1658.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:49.849699: step 181090, loss = 0.66 (1659.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:50.616542: step 181100, loss = 0.66 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:51.376873: step 181110, loss = 0.76 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:52.134470: step 181120, loss = 0.72 (1689.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:16:52.902887: step 181130, loss = 0.71 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:53.670785: step 181140, loss = 0.60 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:54.435891: step 181150, loss = 0.81 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:55.203506: step 181160, loss = 0.80 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:56.060293: step 181170, loss = 0.62 (1494.0 examples/sec; 0.086 sec/batch)
2017-05-05 21:16:56.724747: step 181180, loss = 0.67 (1926.4 examples/sec; 0.066 sec/batch)
2017-05-05 21:16:57.493063: step 181190, loss = 0.80 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:58.274504: step 181200, loss = 0.78 (1638.0 examples/sec; 0.078 sec/batch)
2017-05-05 21:16:59.041165: step 181210, loss = 0.79 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:16:59.797876: step 181220, loss = 0.74 (1691.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:00.563468: step 181230, loss = 0.64 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:17:01.324978: step 181240, loss = 0.74 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:02.088989: step 181250, loss = 0.66 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:02.853951: step 181260, loss = 0.79 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:03.610374: step 181270, loss = 0.66 (1692.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:04.377048: step 181280, loss = 0.85 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:17:05.141053: step 181290, loss = 0.70 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:05.908275: step 181300, loss = 0.81 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:17:06.674432: step 181310, loss = 0.72 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:17:07.435228: step 181320, loss = 0.67 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:08.197512: step 181330, loss = 0.66 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:08.960740: step 181340, loss = 0.69 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:09.729143: step 181350, loss = 0.71 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:17:10.491266: step 181360, loss = 0.68 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:11.255552: step 181370, loss = 0.71 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:12.013885: step 181380, loss = 0.68 (1687.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:12.778576: step 181390, loss = 0.63 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:13.549745: step 181400, loss = 0.63 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:17:14.312716: step 181410, loss = 0.66 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:15.083799: step 181420, loss = 0.70 (1660.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:17:15.842003: step 181430, loss = 0.85 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:16.607504: step 181440, loss = 0.62 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:17:17.378230: step 181450, loss = 0.67 (1660.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:17:18.137317: step 181460, loss = 0.68 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:18.903854: step 181470, loss = 0.86 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:17:19.661648: step 181480, loss = 0.70 (1689.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:20.426869: step 181490, loss = 0.72 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:17:21.189193: step 181500, loss = 0.62 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:21.952481: step 181510, loss = 0.61 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:22.714555: step 181520, loss = 0.63 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:23.481520: step 181530, loss = 0.61 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:17:24.244835: step 181540, loss = 0.69 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:25.013727: step 181550, loss = 0.91 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:17:25.780208: step 181560, loss = 0.71 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:17:26.540732: step 181570, loss = 0.61 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:27.310146: step 181580, loss = 0.71 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:17:28.069015: step 181590, loss = 0.81 (1686.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:28.841282: step 181600, loss = 0.75 (1657.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:17:29.608806: step 181610, loss = 0.67 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:17:30.385531: step 181620, loss = 0.70 (1647.9 examples/sec; 0.078 sec/batch)
2017-05-05 21:17:31.149063: step 181630, loss = 0.60 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:31.902659: step 181640, loss = 0.70 (1698.5 examples/sec; 0.075 sec/batch)
2017-05-05 21:17:32.671721: step 181650, loss = 0.68 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:17:33.434992: step 181660, loss = 0.80 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:34.207865: step 181670, loss = 0.68 (1656.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:17:34.970069: step 181680, loss = 0.74 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:35.726254: step 181690, loss = 0.71 (1692.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:36.483353: step 181700, loss = 0.74 (1690.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:37.246588: step 181710, loss = 0.52 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:38.007245: step 181720, loss = 0.80 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:38.774635: step 181730, loss = 0.89 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:17:39.532016: step 181740, loss = 0.75 (1690.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:40.295875: step 181750, loss = 0.80 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:41.061517: step 181760, loss = 0.69 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:17:41.828401: step 181770, loss = 0.61 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:17:42.591534: step 181780, loss = 0.63 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:43.355809: step 181790, loss = 0.64 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:44.110194: step 181800, loss = 0.74 (1696.7 examples/sec; 0.075 sec/batch)
2017-05-05 21:17:44.873470: step 181810, loss = 0.81 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:45.639036: step 181820, loss = 0.67 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:17:46.433392: step 181830, loss = 0.71 (1611.4 examples/sec; 0.079 sec/batch)
2017-05-05 21:17:47.199754: step 181840, loss = 0.73 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:17:47.953084: step 181850, loss = 0.69 (1699.1 examples/sec; 0.075 sec/batch)
2017-05-05 21:17:48.722913: step 181860, loss = 0.61 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:17:49.487522: step 181870, loss = 0.58 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:50.288890: step 181880, loss = 0.76 (1597.3 examples/sec; 0.080 sec/batch)
2017-05-05 21:17:51.076010: step 181890, loss = 0.63 (1626.2 examples/sec; 0.079 sec/batch)
2017-05-05 21:17:51.849631: step 181900, loss = 0.60 (1654.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:17:52.746860: step 181910, loss = 1.00 (1426.6 examples/sec; 0.090 sec/batch)
2017-05-05 21:17:53.522836: step 181920, loss = 0.71 (1649.5 examples/sec; 0.078 sec/batch)
2017-05-05 21:17:54.293079: step 181930, loss = 0.65 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:17:55.067321: step 181940, loss = 0.79 (1653.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:17:55.827380: step 181950, loss = 0.71 (1E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494019552.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 33 events to /tmp/cifar10_train/events.out.tfevents.1494019552.GHC33.GHC.ANDREW.CMU.EDU
684.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:56.601089: step 181960, loss = 0.54 (1654.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:17:57.367008: step 181970, loss = 0.64 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:17:58.131141: step 181980, loss = 0.74 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:17:58.899190: step 181990, loss = 0.73 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:17:59.654710: step 182000, loss = 0.78 (1694.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:18:00.418846: step 182010, loss = 0.80 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:18:01.184778: step 182020, loss = 0.73 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:01.950942: step 182030, loss = 0.76 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:02.721983: step 182040, loss = 0.77 (1660.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:03.486028: step 182050, loss = 0.65 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:18:04.251915: step 182060, loss = 0.87 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:05.021322: step 182070, loss = 0.72 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:05.799991: step 182080, loss = 0.48 (1643.8 examples/sec; 0.078 sec/batch)
2017-05-05 21:18:06.565529: step 182090, loss = 0.69 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:07.335426: step 182100, loss = 0.74 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:08.092563: step 182110, loss = 0.70 (1690.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:18:08.866230: step 182120, loss = 0.64 (1654.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:09.631174: step 182130, loss = 0.68 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:18:10.399585: step 182140, loss = 0.74 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:11.163882: step 182150, loss = 0.67 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:18:12.027819: step 182160, loss = 0.62 (1481.6 examples/sec; 0.086 sec/batch)
2017-05-05 21:18:12.702994: step 182170, loss = 0.64 (1895.8 examples/sec; 0.068 sec/batch)
2017-05-05 21:18:13.479597: step 182180, loss = 0.60 (1648.2 examples/sec; 0.078 sec/batch)
2017-05-05 21:18:14.245651: step 182190, loss = 0.69 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:15.009703: step 182200, loss = 0.92 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:18:15.770183: step 182210, loss = 0.86 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:18:16.534163: step 182220, loss = 0.75 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:18:17.305011: step 182230, loss = 0.55 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:18.071590: step 182240, loss = 0.71 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:18.836520: step 182250, loss = 0.73 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:18:19.603734: step 182260, loss = 0.70 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:20.369049: step 182270, loss = 0.70 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:21.211569: step 182280, loss = 0.73 (1519.3 examples/sec; 0.084 sec/batch)
2017-05-05 21:18:21.969671: step 182290, loss = 0.69 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:18:22.738378: step 182300, loss = 0.80 (1665.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:23.497540: step 182310, loss = 0.80 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:18:24.255235: step 182320, loss = 0.69 (1689.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:18:25.024299: step 182330, loss = 0.72 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:25.791035: step 182340, loss = 0.71 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:26.554468: step 182350, loss = 0.72 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:18:27.319209: step 182360, loss = 0.71 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:18:28.078711: step 182370, loss = 0.62 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:18:28.837752: step 182380, loss = 0.67 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:18:29.603051: step 182390, loss = 0.66 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:30.385793: step 182400, loss = 0.65 (1635.3 examples/sec; 0.078 sec/batch)
2017-05-05 21:18:31.146078: step 182410, loss = 0.64 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:18:31.910683: step 182420, loss = 0.64 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:18:32.681840: step 182430, loss = 0.65 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:33.458237: step 182440, loss = 0.60 (1648.6 examples/sec; 0.078 sec/batch)
2017-05-05 21:18:34.226818: step 182450, loss = 0.70 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:35.000996: step 182460, loss = 0.72 (1653.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:35.764933: step 182470, loss = 0.64 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:18:36.534742: step 182480, loss = 0.77 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:37.306609: step 182490, loss = 0.71 (1658.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:38.073562: step 182500, loss = 0.80 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:38.842858: step 182510, loss = 0.62 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:39.602704: step 182520, loss = 0.71 (1684.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:18:40.376696: step 182530, loss = 0.56 (1653.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:41.146748: step 182540, loss = 0.64 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:41.914527: step 182550, loss = 0.74 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:42.688512: step 182560, loss = 0.74 (1653.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:43.449170: step 182570, loss = 0.62 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:18:44.208883: step 182580, loss = 0.71 (1684.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:18:44.972822: step 182590, loss = 0.63 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:18:45.733317: step 182600, loss = 0.77 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:18:46.498566: step 182610, loss = 0.76 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:47.261219: step 182620, loss = 0.63 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:18:48.032397: step 182630, loss = 0.77 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:48.799667: step 182640, loss = 0.74 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:49.565464: step 182650, loss = 0.70 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:50.331117: step 182660, loss = 0.67 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:51.096387: step 182670, loss = 0.69 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:51.851549: step 182680, loss = 0.59 (1695.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:18:52.624345: step 182690, loss = 0.81 (1656.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:53.385872: step 182700, loss = 0.69 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:18:54.150303: step 182710, loss = 0.72 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:18:54.916424: step 182720, loss = 0.53 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:55.671283: step 182730, loss = 0.60 (1695.7 examples/sec; 0.075 sec/batch)
2017-05-05 21:18:56.440963: step 182740, loss = 0.62 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:57.197672: step 182750, loss = 0.75 (1691.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:18:57.968007: step 182760, loss = 0.67 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:58.736878: step 182770, loss = 0.78 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:18:59.498958: step 182780, loss = 0.68 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:00.263701: step 182790, loss = 0.64 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:01.024224: step 182800, loss = 0.76 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:01.785107: step 182810, loss = 0.69 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:02.543399: step 182820, loss = 0.51 (1688.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:03.307832: step 182830, loss = 0.54 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:04.062510: step 182840, loss = 0.64 (1696.1 examples/sec; 0.075 sec/batch)
2017-05-05 21:19:04.824725: step 182850, loss = 0.82 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:05.592380: step 182860, loss = 0.85 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:19:06.357713: step 182870, loss = 0.64 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:19:07.121286: step 182880, loss = 0.68 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:07.873157: step 182890, loss = 0.75 (1702.4 examples/sec; 0.075 sec/batch)
2017-05-05 21:19:08.632962: step 182900, loss = 0.56 (1684.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:09.399508: step 182910, loss = 0.64 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:19:10.164102: step 182920, loss = 0.71 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:10.930536: step 182930, loss = 0.78 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:19:11.680182: step 182940, loss = 0.69 (1707.5 examples/sec; 0.075 sec/batch)
2017-05-05 21:19:12.449595: step 182950, loss = 0.71 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:19:13.212293: step 182960, loss = 0.67 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:13.975828: step 182970, loss = 0.50 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:14.745178: step 182980, loss = 0.55 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:19:15.503786: step 182990, loss = 0.81 (1687.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:16.266400: step 183000, loss = 0.78 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:17.029314: step 183010, loss = 0.62 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:17.794760: step 183020, loss = 0.81 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:19:18.559551: step 183030, loss = 0.86 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:19.323065: step 183040, loss = 0.86 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:20.086679: step 183050, loss = 0.72 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:20.844801: step 183060, loss = 0.78 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:21.605180: step 183070, loss = 0.75 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:22.372288: step 183080, loss = 0.79 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:19:23.135241: step 183090, loss = 0.71 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:23.892210: step 183100, loss = 0.62 (1690.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:24.657543: step 183110, loss = 0.80 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:19:25.420375: step 183120, loss = 0.79 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:26.189259: step 183130, loss = 0.70 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:19:26.960122: step 183140, loss = 0.54 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:19:27.821782: step 183150, loss = 0.66 (1485.5 examples/sec; 0.086 sec/batch)
2017-05-05 21:19:28.481230: step 183160, loss = 0.77 (1941.0 examples/sec; 0.066 sec/batch)
2017-05-05 21:19:29.245898: step 183170, loss = 0.78 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:30.015598: step 183180, loss = 0.64 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:19:30.787858: step 183190, loss = 0.71 (1657.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:19:31.548510: step 183200, loss = 0.75 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:32.309127: step 183210, loss = 0.73 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:33.072809: step 183220, loss = 0.75 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:33.834917: step 183230, loss = 0.79 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:34.605296: step 183240, loss = 0.64 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:19:35.365449: step 183250, loss = 0.76 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:36.126808: step 183260, loss = 0.74 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:36.891125: step 183270, loss = 0.69 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:37.657760: step 183280, loss = 0.68 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:1E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494019552.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 49 events to /tmp/cifar10_train/events.out.tfevents.1494019552.GHC33.GHC.ANDREW.CMU.EDU
9:38.427823: step 183290, loss = 0.83 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:19:39.197992: step 183300, loss = 0.87 (1662.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:19:39.956681: step 183310, loss = 0.73 (1687.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:40.717770: step 183320, loss = 0.61 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:41.482909: step 183330, loss = 0.81 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:19:42.244594: step 183340, loss = 0.67 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:43.012116: step 183350, loss = 0.74 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:19:43.773932: step 183360, loss = 0.67 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:44.544451: step 183370, loss = 0.61 (1661.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:19:45.306122: step 183380, loss = 0.66 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:46.065599: step 183390, loss = 0.69 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:46.828710: step 183400, loss = 0.63 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:47.583433: step 183410, loss = 0.67 (1696.0 examples/sec; 0.075 sec/batch)
2017-05-05 21:19:48.343650: step 183420, loss = 0.67 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:49.110631: step 183430, loss = 0.82 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:19:49.871631: step 183440, loss = 0.79 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:50.635747: step 183450, loss = 0.77 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:51.394661: step 183460, loss = 0.67 (1686.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:52.152083: step 183470, loss = 0.78 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:52.914851: step 183480, loss = 0.79 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:53.681439: step 183490, loss = 0.70 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:19:54.445814: step 183500, loss = 0.72 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:55.211743: step 183510, loss = 0.60 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:19:55.966515: step 183520, loss = 0.70 (1695.9 examples/sec; 0.075 sec/batch)
2017-05-05 21:19:56.730014: step 183530, loss = 0.63 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:57.496337: step 183540, loss = 0.58 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:19:58.265063: step 183550, loss = 0.55 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:19:59.023028: step 183560, loss = 0.73 (1688.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:19:59.773993: step 183570, loss = 0.64 (1704.5 examples/sec; 0.075 sec/batch)
2017-05-05 21:20:00.531717: step 183580, loss = 0.68 (1689.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:01.304871: step 183590, loss = 0.73 (1655.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:02.071114: step 183600, loss = 0.70 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:02.832630: step 183610, loss = 0.61 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:03.597233: step 183620, loss = 0.62 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:04.369577: step 183630, loss = 0.74 (1657.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:05.130074: step 183640, loss = 0.64 (1683.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:05.894884: step 183650, loss = 0.73 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:06.664204: step 183660, loss = 0.81 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:07.417621: step 183670, loss = 0.66 (1698.9 examples/sec; 0.075 sec/batch)
2017-05-05 21:20:08.183784: step 183680, loss = 0.82 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:08.952004: step 183690, loss = 0.67 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:09.724416: step 183700, loss = 0.68 (1657.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:10.492496: step 183710, loss = 0.72 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:11.258626: step 183720, loss = 0.68 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:12.018262: step 183730, loss = 0.69 (1685.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:12.786811: step 183740, loss = 0.81 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:13.562533: step 183750, loss = 0.51 (1650.1 examples/sec; 0.078 sec/batch)
2017-05-05 21:20:14.332334: step 183760, loss = 0.57 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:15.095041: step 183770, loss = 0.72 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:15.855785: step 183780, loss = 0.57 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:16.617251: step 183790, loss = 0.70 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:17.381084: step 183800, loss = 0.68 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:18.144946: step 183810, loss = 0.72 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:18.915402: step 183820, loss = 0.78 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:19.672481: step 183830, loss = 0.64 (1690.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:20.436182: step 183840, loss = 0.66 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:21.204618: step 183850, loss = 0.65 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:21.959281: step 183860, loss = 0.76 (1696.1 examples/sec; 0.075 sec/batch)
2017-05-05 21:20:22.719892: step 183870, loss = 0.62 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:23.479831: step 183880, loss = 0.61 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:24.243476: step 183890, loss = 0.62 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:25.004646: step 183900, loss = 0.74 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:25.766216: step 183910, loss = 0.72 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:26.531431: step 183920, loss = 0.71 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:27.294421: step 183930, loss = 0.61 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:28.053068: step 183940, loss = 0.69 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:28.824150: step 183950, loss = 0.65 (1660.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:29.589818: step 183960, loss = 0.74 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:30.354115: step 183970, loss = 0.64 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:31.114250: step 183980, loss = 0.58 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:31.869226: step 183990, loss = 0.73 (1695.4 examples/sec; 0.075 sec/batch)
2017-05-05 21:20:32.635097: step 184000, loss = 0.74 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:33.400405: step 184010, loss = 0.76 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:34.167708: step 184020, loss = 0.57 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:34.934638: step 184030, loss = 0.66 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:35.688575: step 184040, loss = 0.60 (1697.8 examples/sec; 0.075 sec/batch)
2017-05-05 21:20:36.453287: step 184050, loss = 0.72 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:37.213591: step 184060, loss = 0.64 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:37.976472: step 184070, loss = 0.72 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:38.748523: step 184080, loss = 0.68 (1657.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:39.504435: step 184090, loss = 0.63 (1693.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:40.267515: step 184100, loss = 0.77 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:41.024555: step 184110, loss = 0.65 (1690.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:41.785434: step 184120, loss = 0.74 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:42.553599: step 184130, loss = 0.89 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:43.413229: step 184140, loss = 0.59 (1489.0 examples/sec; 0.086 sec/batch)
2017-05-05 21:20:44.071869: step 184150, loss = 0.73 (1943.4 examples/sec; 0.066 sec/batch)
2017-05-05 21:20:44.838028: step 184160, loss = 0.84 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:45.602957: step 184170, loss = 0.73 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:46.364034: step 184180, loss = 0.84 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:47.132527: step 184190, loss = 0.63 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:47.886938: step 184200, loss = 0.65 (1696.7 examples/sec; 0.075 sec/batch)
2017-05-05 21:20:48.646265: step 184210, loss = 0.73 (1685.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:49.415428: step 184220, loss = 0.63 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:50.176014: step 184230, loss = 0.69 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:50.945945: step 184240, loss = 0.70 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:51.704615: step 184250, loss = 0.74 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:52.478955: step 184260, loss = 0.73 (1653.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:53.248210: step 184270, loss = 0.65 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:54.013221: step 184280, loss = 0.71 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:54.782402: step 184290, loss = 0.64 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:55.546930: step 184300, loss = 0.68 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:56.313678: step 184310, loss = 0.70 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:57.074445: step 184320, loss = 0.64 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:20:57.839897: step 184330, loss = 0.71 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:58.606399: step 184340, loss = 0.74 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:20:59.371839: step 184350, loss = 0.72 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:00.129670: step 184360, loss = 0.77 (1689.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:00.892697: step 184370, loss = 0.75 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:01.652157: step 184380, loss = 0.65 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:02.422056: step 184390, loss = 0.53 (1662.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:03.186775: step 184400, loss = 0.72 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:03.935746: step 184410, loss = 0.72 (1709.0 examples/sec; 0.075 sec/batch)
2017-05-05 21:21:04.710487: step 184420, loss = 0.66 (1652.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:05.473591: step 184430, loss = 0.66 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:06.238259: step 184440, loss = 0.65 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:07.004687: step 184450, loss = 0.72 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:07.755847: step 184460, loss = 0.77 (1704.0 examples/sec; 0.075 sec/batch)
2017-05-05 21:21:08.515633: step 184470, loss = 0.74 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:09.281574: step 184480, loss = 0.81 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:10.049973: step 184490, loss = 0.70 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:10.815346: step 184500, loss = 0.60 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:11.568679: step 184510, loss = 0.71 (1699.1 examples/sec; 0.075 sec/batch)
2017-05-05 21:21:12.332893: step 184520, loss = 0.63 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:13.096854: step 184530, loss = 0.76 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:13.866271: step 184540, loss = 0.61 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:14.631803: step 184550, loss = 0.55 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:15.393228: step 184560, loss = 0.72 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:16.161276: step 184570, loss = 0.74 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:16.926786: step 184580, loss = 0.63 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:17.699576: step 184590, loss = 0.60 (1656.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:18.468483: step 184600, loss = 0.89 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:19.243650: step 184610, loss = 0.64 (1651.3 examples/sec; 0.078 sec/batch)
2017-05-05 21:21:19.995315: step 184620, loss = 0.57 (1702.9 examples/sec; 0.075 sec/batch)
2017-05-05 21:21:20.763061: step 184630, loss = 0.62 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:21.525088: step 184640, loss = 0.70 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:22.296880: step 184650, loss = 0.67 (1658.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:23.066262: step 184660, loss = 0.59 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:23.826662: step 184670, loss = 0.82 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:24.599000: step 184680, loss = 0.75 (1657.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:25.372135: step 184690, loss = 0.74 (1655.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:26.140291: step 184700, loss = 0.69 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:26.909875: step 184710, loss = 0.79 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:27.667011: step 184720, loss = 0.68 (1690.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:28.439820: step 184730, loss = 0.72 (1656.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:29.209010: step 184740, loss = 0.57 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:29.980528: step 184750, loss = 0.66 (1659.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:30.750342: step 184760, loss = 0.63 (1662.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:31.512134: step 184770, loss = 0.56 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:32.277631: step 184780, loss = 0.61 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:33.040336: step 184790, loss = 0.65 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:33.799708: step 184800, loss = 0.73 (1685.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:34.563690: step 184810, loss = 0.72 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:35.331646: step 184820, loss = 0.79 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:36.093816: step 184830, loss = 0.73 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:36.854542: step 184840, loss = 0.61 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:37.619105: step 184850, loss = 0.68 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:38.388669: step 184860, loss = 0.84 (1663.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:39.148573: step 184870, loss = 0.68 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:39.908843: step 184880, loss = 0.85 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:40.674763: step 184890, loss = 0.71 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:41.437397: step 184900, loss = 0.64 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:42.207110: step 184910, loss = 0.67 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:42.967572: step 184920, loss = 0.56 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:43.731605: step 184930, loss = 0.53 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:44.501352: step 184940, loss = 0.64 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:45.263440: step 184950, loss = 0.74 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:46.030269: step 184960, loss = 0.73 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:46.792122: step 184970, loss = 0.74 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:47.551347: step 184980, loss = 0.80 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:48.314158: step 184990, loss = 0.61 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:49.077197: step 185000, loss = 0.75 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:49.847870: step 185010, loss = 0.70 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:50.613028: step 185020, loss = 0.67 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:51.375630: step 185030, loss = 0.79 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:52.131059: step 185040, loss = 0.71 (1694.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:52.899193: step 185050, loss = 0.66 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:53.667077: step 185060, loss = 0.69 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:54.438E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494019552.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 65 events to /tmp/cifar10_train/events.out.tfevents.1494019552.GHC33.GHC.ANDREW.CMU.EDU
206: step 185070, loss = 0.69 (1659.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:55.196723: step 185080, loss = 0.85 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:55.957986: step 185090, loss = 0.84 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:56.720159: step 185100, loss = 0.65 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:21:57.485841: step 185110, loss = 0.79 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:58.255283: step 185120, loss = 0.80 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:21:59.115123: step 185130, loss = 0.81 (1488.7 examples/sec; 0.086 sec/batch)
2017-05-05 21:21:59.784561: step 185140, loss = 0.70 (1912.0 examples/sec; 0.067 sec/batch)
2017-05-05 21:22:00.551790: step 185150, loss = 0.63 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:01.306910: step 185160, loss = 0.69 (1695.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:02.072764: step 185170, loss = 0.83 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:02.840559: step 185180, loss = 0.75 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:03.598781: step 185190, loss = 0.52 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:04.360638: step 185200, loss = 0.56 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:05.125692: step 185210, loss = 0.66 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:05.891690: step 185220, loss = 0.78 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:06.652599: step 185230, loss = 0.63 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:07.413706: step 185240, loss = 0.76 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:08.173022: step 185250, loss = 0.71 (1685.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:08.943724: step 185260, loss = 0.76 (1660.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:09.707489: step 185270, loss = 0.66 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:10.469815: step 185280, loss = 0.70 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:11.232536: step 185290, loss = 0.84 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:11.990590: step 185300, loss = 0.64 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:12.757468: step 185310, loss = 0.71 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:13.523247: step 185320, loss = 0.72 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:14.292061: step 185330, loss = 0.66 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:15.060765: step 185340, loss = 0.65 (1665.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:15.815376: step 185350, loss = 0.57 (1696.3 examples/sec; 0.075 sec/batch)
2017-05-05 21:22:16.580772: step 185360, loss = 0.57 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:17.349257: step 185370, loss = 0.71 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:18.118346: step 185380, loss = 0.57 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:18.886026: step 185390, loss = 0.67 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:19.638561: step 185400, loss = 0.66 (1700.9 examples/sec; 0.075 sec/batch)
2017-05-05 21:22:20.405797: step 185410, loss = 0.79 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:21.169088: step 185420, loss = 0.59 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:21.927196: step 185430, loss = 0.55 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:22.699455: step 185440, loss = 0.71 (1657.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:23.461690: step 185450, loss = 0.78 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:24.220134: step 185460, loss = 0.77 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:24.985496: step 185470, loss = 0.70 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:25.749067: step 185480, loss = 0.74 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:26.509664: step 185490, loss = 0.58 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:27.276370: step 185500, loss = 0.72 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:28.037462: step 185510, loss = 0.87 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:28.803966: step 185520, loss = 0.72 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:29.576662: step 185530, loss = 0.62 (1656.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:30.345898: step 185540, loss = 0.74 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:31.117089: step 185550, loss = 0.81 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:31.876218: step 185560, loss = 0.59 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:32.649656: step 185570, loss = 0.65 (1655.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:33.408558: step 185580, loss = 0.61 (1686.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:34.175923: step 185590, loss = 0.67 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:34.942206: step 185600, loss = 0.75 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:35.702294: step 185610, loss = 0.64 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:36.469933: step 185620, loss = 0.57 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:37.235401: step 185630, loss = 0.80 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:38.006196: step 185640, loss = 0.62 (1660.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:38.770785: step 185650, loss = 0.71 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:39.532943: step 185660, loss = 0.60 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:40.294384: step 185670, loss = 0.84 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:41.063012: step 185680, loss = 0.66 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:41.826923: step 185690, loss = 0.89 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:42.596898: step 185700, loss = 0.64 (1662.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:43.357315: step 185710, loss = 0.65 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:44.118747: step 185720, loss = 0.70 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:44.884901: step 185730, loss = 0.56 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:45.650190: step 185740, loss = 0.74 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:46.409989: step 185750, loss = 0.62 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:47.166296: step 185760, loss = 0.65 (1692.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:47.921472: step 185770, loss = 0.73 (1695.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:48.692105: step 185780, loss = 0.83 (1661.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:49.453129: step 185790, loss = 0.81 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:50.221541: step 185800, loss = 0.63 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:50.988855: step 185810, loss = 0.84 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:51.741651: step 185820, loss = 0.64 (1700.3 examples/sec; 0.075 sec/batch)
2017-05-05 21:22:52.505391: step 185830, loss = 0.74 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:53.273596: step 185840, loss = 0.65 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:54.047284: step 185850, loss = 0.61 (1654.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:54.808849: step 185860, loss = 0.73 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:55.567237: step 185870, loss = 0.78 (1687.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:56.330079: step 185880, loss = 0.88 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:57.089883: step 185890, loss = 0.62 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:22:57.861658: step 185900, loss = 0.58 (1658.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:58.626781: step 185910, loss = 0.76 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:22:59.386530: step 185920, loss = 0.58 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:00.148237: step 185930, loss = 0.85 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:00.910576: step 185940, loss = 0.85 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:01.677089: step 185950, loss = 0.73 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:23:02.439102: step 185960, loss = 0.73 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:03.200652: step 185970, loss = 0.74 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:03.953800: step 185980, loss = 0.67 (1699.5 examples/sec; 0.075 sec/batch)
2017-05-05 21:23:04.717764: step 185990, loss = 0.63 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:05.481321: step 186000, loss = 0.73 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:06.248136: step 186010, loss = 0.69 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:23:07.008531: step 186020, loss = 0.88 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:07.763555: step 186030, loss = 0.65 (1695.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:08.529315: step 186040, loss = 0.78 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:23:09.289959: step 186050, loss = 0.67 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:10.052076: step 186060, loss = 0.74 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:10.816709: step 186070, loss = 0.81 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:11.572328: step 186080, loss = 0.62 (1694.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:12.337674: step 186090, loss = 0.61 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:23:13.108080: step 186100, loss = 0.64 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:23:13.879786: step 186110, loss = 0.77 (1658.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:23:14.745820: step 186120, loss = 0.69 (1478.0 examples/sec; 0.087 sec/batch)
2017-05-05 21:23:15.416763: step 186130, loss = 0.84 (1907.8 examples/sec; 0.067 sec/batch)
2017-05-05 21:23:16.173026: step 186140, loss = 0.64 (1692.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:16.931988: step 186150, loss = 0.69 (1686.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:17.696309: step 186160, loss = 0.58 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:18.463637: step 186170, loss = 0.70 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:23:19.233377: step 186180, loss = 0.73 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:23:19.982336: step 186190, loss = 0.61 (1709.0 examples/sec; 0.075 sec/batch)
2017-05-05 21:23:20.744174: step 186200, loss = 0.76 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:21.506067: step 186210, loss = 0.55 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:22.272852: step 186220, loss = 0.59 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:23:23.048506: step 186230, loss = 0.66 (1650.2 examples/sec; 0.078 sec/batch)
2017-05-05 21:23:23.813908: step 186240, loss = 0.75 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:23:24.578983: step 186250, loss = 0.69 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:23:25.339097: step 186260, loss = 0.61 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:26.102984: step 186270, loss = 0.73 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:26.868068: step 186280, loss = 0.75 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:23:27.624700: step 186290, loss = 0.53 (1691.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:28.402235: step 186300, loss = 0.71 (1646.2 examples/sec; 0.078 sec/batch)
2017-05-05 21:23:29.163788: step 186310, loss = 0.63 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:29.931695: step 186320, loss = 0.71 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:23:30.696686: step 186330, loss = 0.54 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:31.457594: step 186340, loss = 0.66 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:32.221419: step 186350, loss = 0.62 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:32.988177: step 186360, loss = 0.85 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:23:33.757654: step 186370, loss = 0.76 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:23:34.519413: step 186380, loss = 0.76 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:35.287702: step 186390, loss = 0.58 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:23:36.043512: step 186400, loss = 0.60 (1693.6 examples/sec; E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494019552.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 81 events to /tmp/cifar10_train/events.out.tfevents.1494019552.GHC33.GHC.ANDREW.CMU.EDU
0.076 sec/batch)
2017-05-05 21:23:36.807611: step 186410, loss = 0.65 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:37.578200: step 186420, loss = 0.60 (1661.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:23:38.344432: step 186430, loss = 0.66 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:23:39.104725: step 186440, loss = 0.61 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:39.857753: step 186450, loss = 0.69 (1699.8 examples/sec; 0.075 sec/batch)
2017-05-05 21:23:40.621347: step 186460, loss = 0.82 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:41.384541: step 186470, loss = 0.61 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:42.148011: step 186480, loss = 0.77 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:42.913013: step 186490, loss = 0.73 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:23:43.668721: step 186500, loss = 0.66 (1693.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:44.435578: step 186510, loss = 0.66 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:23:45.202047: step 186520, loss = 0.78 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:23:45.966912: step 186530, loss = 0.69 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:46.733476: step 186540, loss = 0.72 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:23:47.492348: step 186550, loss = 0.72 (1686.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:48.258383: step 186560, loss = 0.68 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:23:49.016542: step 186570, loss = 0.79 (1688.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:49.782316: step 186580, loss = 0.65 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:23:50.544513: step 186590, loss = 0.69 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:51.312609: step 186600, loss = 0.65 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:23:52.069882: step 186610, loss = 0.80 (1690.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:52.837967: step 186620, loss = 0.67 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:23:53.600791: step 186630, loss = 0.81 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:54.372260: step 186640, loss = 0.72 (1659.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:23:55.131898: step 186650, loss = 0.61 (1685.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:55.888058: step 186660, loss = 0.75 (1692.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:56.654480: step 186670, loss = 0.63 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:23:57.419035: step 186680, loss = 0.64 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:23:58.191039: step 186690, loss = 0.84 (1658.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:23:58.961157: step 186700, loss = 0.72 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:23:59.717297: step 186710, loss = 0.78 (1692.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:00.490731: step 186720, loss = 0.64 (1655.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:01.255807: step 186730, loss = 0.66 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:02.018008: step 186740, loss = 0.63 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:02.785565: step 186750, loss = 0.59 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:03.543638: step 186760, loss = 0.59 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:04.310494: step 186770, loss = 0.65 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:05.074565: step 186780, loss = 0.77 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:05.850447: step 186790, loss = 0.73 (1649.7 examples/sec; 0.078 sec/batch)
2017-05-05 21:24:06.605934: step 186800, loss = 0.68 (1694.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:07.362762: step 186810, loss = 0.60 (1691.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:08.115152: step 186820, loss = 0.70 (1701.2 examples/sec; 0.075 sec/batch)
2017-05-05 21:24:08.885313: step 186830, loss = 0.74 (1662.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:09.651245: step 186840, loss = 0.70 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:10.420225: step 186850, loss = 0.68 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:11.180103: step 186860, loss = 0.77 (1684.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:11.936999: step 186870, loss = 0.72 (1691.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:12.699119: step 186880, loss = 0.61 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:13.460802: step 186890, loss = 0.79 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:14.225300: step 186900, loss = 0.77 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:14.993230: step 186910, loss = 0.62 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:15.748263: step 186920, loss = 0.60 (1695.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:16.510584: step 186930, loss = 0.71 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:17.275365: step 186940, loss = 0.57 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:18.040768: step 186950, loss = 0.88 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:18.810237: step 186960, loss = 0.62 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:19.568019: step 186970, loss = 0.64 (1689.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:20.334971: step 186980, loss = 0.83 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:21.093517: step 186990, loss = 0.65 (1687.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:21.868161: step 187000, loss = 0.60 (1652.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:22.634010: step 187010, loss = 0.70 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:23.391853: step 187020, loss = 0.75 (1689.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:24.153667: step 187030, loss = 0.70 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:24.915611: step 187040, loss = 0.83 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:25.686419: step 187050, loss = 0.78 (1660.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:26.448516: step 187060, loss = 0.63 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:27.221019: step 187070, loss = 0.61 (1657.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:27.969472: step 187080, loss = 0.75 (1710.2 examples/sec; 0.075 sec/batch)
2017-05-05 21:24:28.739102: step 187090, loss = 0.65 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:29.503213: step 187100, loss = 0.62 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:30.363340: step 187110, loss = 0.55 (1488.1 examples/sec; 0.086 sec/batch)
2017-05-05 21:24:31.037077: step 187120, loss = 0.51 (1899.9 examples/sec; 0.067 sec/batch)
2017-05-05 21:24:31.795464: step 187130, loss = 0.64 (1687.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:32.562079: step 187140, loss = 0.71 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:33.328285: step 187150, loss = 0.61 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:34.097546: step 187160, loss = 0.78 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:34.863549: step 187170, loss = 0.85 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:35.623127: step 187180, loss = 0.70 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:36.386797: step 187190, loss = 0.79 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:37.152583: step 187200, loss = 0.63 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:37.917867: step 187210, loss = 0.69 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:38.686310: step 187220, loss = 0.69 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:39.447999: step 187230, loss = 0.73 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:40.215836: step 187240, loss = 0.65 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:40.982327: step 187250, loss = 0.73 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:41.744326: step 187260, loss = 0.77 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:42.506979: step 187270, loss = 0.72 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:43.273656: step 187280, loss = 0.80 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:44.025549: step 187290, loss = 0.56 (1702.4 examples/sec; 0.075 sec/batch)
2017-05-05 21:24:44.788170: step 187300, loss = 0.76 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:45.559725: step 187310, loss = 0.75 (1659.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:46.322064: step 187320, loss = 0.70 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:47.089956: step 187330, loss = 0.67 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:47.848438: step 187340, loss = 0.66 (1687.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:48.612856: step 187350, loss = 0.70 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:49.374950: step 187360, loss = 0.74 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:50.143129: step 187370, loss = 0.72 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:50.911246: step 187380, loss = 0.60 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:51.668156: step 187390, loss = 0.73 (1691.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:52.429403: step 187400, loss = 0.58 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:53.197875: step 187410, loss = 0.67 (1665.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:53.954392: step 187420, loss = 0.72 (1692.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:54.720059: step 187430, loss = 0.67 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:55.474738: step 187440, loss = 0.63 (1696.1 examples/sec; 0.075 sec/batch)
2017-05-05 21:24:56.229507: step 187450, loss = 0.71 (1695.9 examples/sec; 0.075 sec/batch)
2017-05-05 21:24:56.990704: step 187460, loss = 0.91 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:57.756031: step 187470, loss = 0.71 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:24:58.520844: step 187480, loss = 0.75 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:24:59.292390: step 187490, loss = 0.71 (1659.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:25:00.052905: step 187500, loss = 0.60 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:00.813958: step 187510, loss = 0.63 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:01.599855: step 187520, loss = 0.74 (1628.7 examples/sec; 0.079 sec/batch)
2017-05-05 21:25:02.361802: step 187530, loss = 0.63 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:03.123334: step 187540, loss = 0.63 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:03.876410: step 187550, loss = 0.72 (1699.7 examples/sec; 0.075 sec/batch)
2017-05-05 21:25:04.638170: step 187560, loss = 0.86 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:05.409177: step 187570, loss = 0.75 (1660.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:25:06.172808: step 187580, loss = 0.86 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:06.932610: step 187590, loss = 0.63 (1684.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:07.691521: step 187600, loss = 0.72 (1686.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:08.456320: step 187610, loss = 0.62 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:09.217183: step 187620, loss = 0.52 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:09.977058: step 187630, loss = 0.75 (1684.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:10.742575: step 187640, loss = 0.77 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:25:11.500031: step 187650, loss = 0.78 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:12.264255: step 187660, loss = 0.65 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:13.029657: step 187670, loss = 0.78 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:25:13.795035: step 187680, loss = 0.59 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:25:14.553393: step 187690, loss = 0.72 (1687.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:15.314751: step 187700, loss = 0.81 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:16.070278: step 187710, loss = 0.91 (1694.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:16.831536: step 187720, loss = 0.63 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:17.602804: step 187730, loss = 0.77 (1659.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:25:18.370224: step 187740, loss = 0.73 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:25:19.136270: step 187750, loss = 0.67 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:25:19.895618: step 187760, loss = 0.77 (1685.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:20.658343: step 187770, loss = 0.79 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:21.415670: step 187780, loss = 0.59 (1690.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:22.183545: step 187790, loss = 0.59 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:25:22.953851: step 187800, loss = 0.81 (1661.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:25:23.712129: step 187810, loss = 0.67 (1688.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:24.478194: step 187820, loss = 0.71 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:25:25.247028: step 187830, loss = 0.68 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:25:26.009099: step 187840, loss = 0.85 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:26.776129: step 187850, loss = 0.69 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:25:27.537031: step 187860, loss = 0.69 (1682.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:28.299207: step 187870, loss = 0.75 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:29.061353: step 187880, loss = 0.67 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:29.822049: step 187890, loss = 0.64 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:30.592404: step 187900, loss = 0.64 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:25:31.352308: step 187910, loss = 0.68 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:32.110448: step 187920, loss = 0.70 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:32.874789: step 187930, loss = 0.59 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:33.641128: step 187940, loss = 0.68 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:25:34.410138: step 187950, loss = 0.65 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:25:35.171227: step 187960, loss = 0.70 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:35.927700: step 187970, loss = 0.67 (1692.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:36.686094: step 187980, loss = 0.80 (1687.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:37.454558: step 187990, loss = 0.67 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:25:38.217482: step 188000, loss = 0.54 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:38.982958: step 188010, loss = 0.65 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:25:39.736744: step 188020, loss = 0.69 (1698.1 examples/sec; 0.075 sec/batch)
2017-05-05 21:25:40.502640: step 188030, loss = 0.74 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:25:41.270594: step 188040, loss = 0.75 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:25:42.028052: step 188050, loss = 0.63 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:42.791551: step 188060, loss = 0.73 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:43.547829: step 188070, loss = 0.72 (1692.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:44.311680: step 188080, loss = 0.65 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:45.072692: step 188090, loss = 0.69 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:45.938630: step 188100, loss = 0.79 (1478.2 examples/sec; 0.087 sec/batch)
2017-05-05 21:25:46.600005: step 188110, loss = 0.69 (1935.4 examples/sec; 0.066 sec/batch)
2017-05-05 21:25:47.360198: step 188120, loss = 0.67 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:48.125196: step 188130, loss = 0.63 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:48.896035: step 188140, loss = 0.61 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:25:49.665901: step 188150, loss = 0.71 (1662.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:25:50.428493: step 188160, loss = 0.71 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:51.200216: step 188170, loss = 0.82 (1658.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:25:51.961259: step 188180, loss = 0.82 (1681.9 examples/sec; 0.076 seE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494019552.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 97 events to /tmp/cifar10_train/events.out.tfevents.1494019552.GHC33.GHC.ANDREW.CMU.EDU
c/batch)
2017-05-05 21:25:52.725581: step 188190, loss = 0.72 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:53.492672: step 188200, loss = 0.64 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:25:54.253861: step 188210, loss = 0.64 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:55.017400: step 188220, loss = 0.59 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:55.773309: step 188230, loss = 0.64 (1693.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:56.540618: step 188240, loss = 0.84 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:25:57.303991: step 188250, loss = 0.70 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:25:58.077547: step 188260, loss = 0.70 (1654.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:25:58.845441: step 188270, loss = 0.71 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:25:59.602508: step 188280, loss = 0.58 (1690.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:00.365577: step 188290, loss = 0.59 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:01.130804: step 188300, loss = 0.71 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:26:01.893922: step 188310, loss = 0.77 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:02.660298: step 188320, loss = 0.65 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:26:03.422234: step 188330, loss = 0.70 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:04.185634: step 188340, loss = 0.68 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:04.941256: step 188350, loss = 0.64 (1694.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:05.707378: step 188360, loss = 0.78 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:26:06.508798: step 188370, loss = 0.78 (1597.1 examples/sec; 0.080 sec/batch)
2017-05-05 21:26:07.272653: step 188380, loss = 0.66 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:08.024335: step 188390, loss = 0.79 (1702.9 examples/sec; 0.075 sec/batch)
2017-05-05 21:26:08.790341: step 188400, loss = 0.76 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:26:09.553154: step 188410, loss = 0.58 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:10.316292: step 188420, loss = 0.67 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:11.080939: step 188430, loss = 0.66 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:11.841335: step 188440, loss = 0.69 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:12.611059: step 188450, loss = 0.70 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:26:13.381236: step 188460, loss = 0.64 (1662.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:26:14.147597: step 188470, loss = 0.67 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:26:14.915935: step 188480, loss = 0.62 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:26:15.682478: step 188490, loss = 0.89 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:26:16.453034: step 188500, loss = 0.79 (1661.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:26:17.212800: step 188510, loss = 0.66 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:17.976316: step 188520, loss = 0.81 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:18.744627: step 188530, loss = 0.74 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:26:19.498580: step 188540, loss = 0.74 (1697.7 examples/sec; 0.075 sec/batch)
2017-05-05 21:26:20.262152: step 188550, loss = 0.65 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:21.025647: step 188560, loss = 0.68 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:21.786393: step 188570, loss = 0.78 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:22.551693: step 188580, loss = 0.67 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:26:23.311342: step 188590, loss = 0.68 (1685.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:24.075806: step 188600, loss = 0.83 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:24.845291: step 188610, loss = 0.58 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:26:25.604691: step 188620, loss = 0.56 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:26.369393: step 188630, loss = 0.69 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:27.134418: step 188640, loss = 0.56 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:26:27.891054: step 188650, loss = 0.76 (1691.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:28.658265: step 188660, loss = 0.75 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:26:29.422666: step 188670, loss = 0.82 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:30.184870: step 188680, loss = 0.70 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:30.952438: step 188690, loss = 0.76 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:26:31.706305: step 188700, loss = 0.64 (1697.9 examples/sec; 0.075 sec/batch)
2017-05-05 21:26:32.467647: step 188710, loss = 0.76 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:33.232699: step 188720, loss = 0.76 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:26:33.993828: step 188730, loss = 0.73 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:34.758767: step 188740, loss = 0.71 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:35.514151: step 188750, loss = 0.74 (1694.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:36.275858: step 188760, loss = 0.70 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:37.039202: step 188770, loss = 0.64 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:37.801968: step 188780, loss = 0.68 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:38.565307: step 188790, loss = 0.75 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:39.328470: step 188800, loss = 0.79 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:40.088794: step 188810, loss = 0.65 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:40.849977: step 188820, loss = 0.69 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:41.616617: step 188830, loss = 0.54 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:26:42.388577: step 188840, loss = 0.65 (1658.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:26:43.152593: step 188850, loss = 0.74 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:43.908080: step 188860, loss = 0.70 (1694.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:44.677497: step 188870, loss = 0.63 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:26:45.443183: step 188880, loss = 0.81 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:26:46.202535: step 188890, loss = 0.67 (1685.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:46.967447: step 188900, loss = 0.75 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:47.721348: step 188910, loss = 0.69 (1697.8 examples/sec; 0.075 sec/batch)
2017-05-05 21:26:48.488378: step 188920, loss = 0.77 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:26:49.255974: step 188930, loss = 0.50 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:26:50.022352: step 188940, loss = 0.63 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:26:50.788989: step 188950, loss = 0.76 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:26:51.551030: step 188960, loss = 0.70 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:52.316753: step 188970, loss = 0.65 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:26:53.082182: step 188980, loss = 0.80 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:26:53.842100: step 188990, loss = 0.64 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:54.608477: step 189000, loss = 0.62 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:26:55.368905: step 189010, loss = 0.70 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:56.127474: step 189020, loss = 0.61 (1687.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:56.888457: step 189030, loss = 0.71 (1682.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:57.655555: step 189040, loss = 0.72 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:26:58.416918: step 189050, loss = 0.67 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:26:59.190757: step 189060, loss = 0.61 (1654.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:26:59.945152: step 189070, loss = 0.61 (1696.7 examples/sec; 0.075 sec/batch)
2017-05-05 21:27:00.702273: step 189080, loss = 0.92 (1690.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:01.559783: step 189090, loss = 0.78 (1492.7 examples/sec; 0.086 sec/batch)
2017-05-05 21:27:02.226386: step 189100, loss = 0.80 (1920.2 examples/sec; 0.067 sec/batch)
2017-05-05 21:27:02.998209: step 189110, loss = 0.71 (1658.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:27:03.747551: step 189120, loss = 0.65 (1708.2 examples/sec; 0.075 sec/batch)
2017-05-05 21:27:04.509053: step 189130, loss = 0.58 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:05.275814: step 189140, loss = 0.55 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:27:06.044630: step 189150, loss = 0.84 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:27:06.806561: step 189160, loss = 0.77 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:07.570448: step 189170, loss = 0.69 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:08.335573: step 189180, loss = 0.61 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:27:09.100240: step 189190, loss = 0.63 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:09.864191: step 189200, loss = 0.75 (1675.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:10.625834: step 189210, loss = 0.79 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:11.382436: step 189220, loss = 0.64 (1691.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:12.143102: step 189230, loss = 0.74 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:12.907293: step 189240, loss = 0.59 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:13.671555: step 189250, loss = 0.76 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:14.430342: step 189260, loss = 0.56 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:15.190485: step 189270, loss = 0.55 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:15.945031: step 189280, loss = 0.71 (1696.4 examples/sec; 0.075 sec/batch)
2017-05-05 21:27:16.715393: step 189290, loss = 0.62 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:27:17.489614: step 189300, loss = 0.80 (1653.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:27:18.247121: step 189310, loss = 0.67 (1689.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:19.009817: step 189320, loss = 0.74 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:19.766254: step 189330, loss = 0.69 (1692.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:20.529942: step 189340, loss = 0.73 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:21.295868: step 189350, loss = 0.65 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:27:22.057536: step 189360, loss = 0.56 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:22.824191: step 189370, loss = 0.72 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:27:23.579076: step 189380, loss = 0.62 (1695.6 examples/sec; 0.075 sec/batch)
2017-05-05 21:27:24.343445: step 189390, loss = 0.67 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:25.108402: step 189400, loss = 0.78 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:25.877565: step 189410, loss = 0.64 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:27:26.637001: step 189420, loss = 0.71 (1685.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:27.396203: step 189430, loss = 0.68 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:28.159053: step 189440, loss = 0.78 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:28.939533: step 189450, loss = 0.71 (1640.0 examples/sec; 0.078 sec/batch)
2017-05-05 21:27:29.707233: step 189460, loss = 0.57 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:27:30.471837: step 189470, loss = 0.78 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:31.229682: step 189480, loss = 0.57 (1689.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:31.989589: step 189490, loss = 0.62 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:32.751926: step 189500, loss = 0.70 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:33.517779: step 189510, loss = 0.71 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:27:34.281297: step 189520, loE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494019552.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 114 events to /tmp/cifar10_train/events.out.tfevents.1494019552.GHC33.GHC.ANDREW.CMU.EDU
ss = 0.71 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:35.047552: step 189530, loss = 0.73 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:27:35.803468: step 189540, loss = 0.80 (1693.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:36.560697: step 189550, loss = 0.66 (1690.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:37.328681: step 189560, loss = 0.67 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:27:38.096717: step 189570, loss = 0.68 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:27:38.861445: step 189580, loss = 0.62 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:39.616935: step 189590, loss = 0.60 (1694.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:40.381553: step 189600, loss = 0.57 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:41.151823: step 189610, loss = 0.56 (1661.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:27:41.915495: step 189620, loss = 0.71 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:42.682090: step 189630, loss = 0.63 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:27:43.436328: step 189640, loss = 0.71 (1697.1 examples/sec; 0.075 sec/batch)
2017-05-05 21:27:44.204346: step 189650, loss = 0.58 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:27:44.970398: step 189660, loss = 0.69 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:27:45.729071: step 189670, loss = 0.61 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:46.491819: step 189680, loss = 0.72 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:47.255498: step 189690, loss = 0.75 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:48.010535: step 189700, loss = 0.84 (1695.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:48.771869: step 189710, loss = 0.72 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:49.533567: step 189720, loss = 0.69 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:50.294726: step 189730, loss = 0.78 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:51.057321: step 189740, loss = 0.65 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:51.816481: step 189750, loss = 0.61 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:52.576592: step 189760, loss = 0.67 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:53.338931: step 189770, loss = 0.69 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:54.104701: step 189780, loss = 0.71 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:27:54.866621: step 189790, loss = 0.78 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:55.617933: step 189800, loss = 0.56 (1703.7 examples/sec; 0.075 sec/batch)
2017-05-05 21:27:56.385784: step 189810, loss = 0.61 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:27:57.149825: step 189820, loss = 0.80 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:57.915562: step 189830, loss = 0.76 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:27:58.676423: step 189840, loss = 0.71 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:27:59.433226: step 189850, loss = 0.77 (1691.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:00.191408: step 189860, loss = 0.65 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:00.955638: step 189870, loss = 0.65 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:01.748288: step 189880, loss = 0.79 (1614.8 examples/sec; 0.079 sec/batch)
2017-05-05 21:28:02.511994: step 189890, loss = 0.71 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:03.274794: step 189900, loss = 0.58 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:04.024926: step 189910, loss = 0.73 (1706.4 examples/sec; 0.075 sec/batch)
2017-05-05 21:28:04.793103: step 189920, loss = 0.85 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:28:05.554837: step 189930, loss = 0.69 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:06.323274: step 189940, loss = 0.80 (1665.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:28:07.089262: step 189950, loss = 0.74 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:28:07.842908: step 189960, loss = 0.85 (1698.4 examples/sec; 0.075 sec/batch)
2017-05-05 21:28:08.613393: step 189970, loss = 0.53 (1661.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:28:09.374058: step 189980, loss = 0.74 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:10.137849: step 189990, loss = 0.69 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:10.902704: step 190000, loss = 0.72 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:11.654541: step 190010, loss = 0.65 (1702.5 examples/sec; 0.075 sec/batch)
2017-05-05 21:28:12.425206: step 190020, loss = 0.59 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:28:13.186157: step 190030, loss = 0.78 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:13.955108: step 190040, loss = 0.71 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:28:14.721077: step 190050, loss = 0.59 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:28:15.484594: step 190060, loss = 0.76 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:16.243711: step 190070, loss = 0.64 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:17.100914: step 190080, loss = 0.70 (1493.2 examples/sec; 0.086 sec/batch)
2017-05-05 21:28:17.776411: step 190090, loss = 0.61 (1894.9 examples/sec; 0.068 sec/batch)
2017-05-05 21:28:18.533613: step 190100, loss = 0.75 (1690.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:19.301635: step 190110, loss = 0.64 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:28:20.059783: step 190120, loss = 0.71 (1688.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:20.820521: step 190130, loss = 0.67 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:21.586051: step 190140, loss = 0.61 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:28:22.350930: step 190150, loss = 0.77 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:23.105677: step 190160, loss = 0.67 (1695.9 examples/sec; 0.075 sec/batch)
2017-05-05 21:28:23.858738: step 190170, loss = 0.75 (1699.7 examples/sec; 0.075 sec/batch)
2017-05-05 21:28:24.623950: step 190180, loss = 0.63 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:28:25.393037: step 190190, loss = 0.80 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:28:26.156587: step 190200, loss = 0.74 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:26.926036: step 190210, loss = 0.78 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:28:27.677524: step 190220, loss = 0.67 (1703.3 examples/sec; 0.075 sec/batch)
2017-05-05 21:28:28.445909: step 190230, loss = 0.76 (1665.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:28:29.211410: step 190240, loss = 0.69 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:28:29.975490: step 190250, loss = 0.82 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:30.740292: step 190260, loss = 0.72 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:31.505747: step 190270, loss = 0.79 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:28:32.264749: step 190280, loss = 0.67 (1686.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:33.027974: step 190290, loss = 0.66 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:33.795169: step 190300, loss = 0.86 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:28:34.558841: step 190310, loss = 0.79 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:35.326037: step 190320, loss = 0.76 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:28:36.087905: step 190330, loss = 0.65 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:36.856815: step 190340, loss = 0.63 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:28:37.622094: step 190350, loss = 0.72 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:28:38.386927: step 190360, loss = 0.75 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:39.146491: step 190370, loss = 0.71 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:39.899834: step 190380, loss = 0.62 (1699.1 examples/sec; 0.075 sec/batch)
2017-05-05 21:28:40.663693: step 190390, loss = 0.60 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:41.425216: step 190400, loss = 0.72 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:42.184778: step 190410, loss = 0.72 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:42.945573: step 190420, loss = 0.75 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:43.704448: step 190430, loss = 0.80 (1686.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:44.474796: step 190440, loss = 0.68 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:28:45.236658: step 190450, loss = 0.75 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:46.007114: step 190460, loss = 0.64 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:28:46.768838: step 190470, loss = 0.66 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:47.527461: step 190480, loss = 0.64 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:48.292761: step 190490, loss = 0.64 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:28:49.049941: step 190500, loss = 0.76 (1690.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:49.817435: step 190510, loss = 0.57 (1667.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:28:50.580748: step 190520, loss = 0.85 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:51.342889: step 190530, loss = 0.85 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:52.110617: step 190540, loss = 0.82 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:28:52.872763: step 190550, loss = 0.72 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:53.634632: step 190560, loss = 0.77 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:54.400568: step 190570, loss = 0.67 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:28:55.161881: step 190580, loss = 0.61 (1681.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:55.915566: step 190590, loss = 0.83 (1698.3 examples/sec; 0.075 sec/batch)
2017-05-05 21:28:56.681051: step 190600, loss = 0.82 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:28:57.454688: step 190610, loss = 0.72 (1654.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:28:58.213618: step 190620, loss = 0.80 (1686.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:28:58.980196: step 190630, loss = 0.71 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:28:59.731715: step 190640, loss = 0.67 (1703.2 examples/sec; 0.075 sec/batch)
2017-05-05 21:29:00.496222: step 190650, loss = 0.69 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:01.264513: step 190660, loss = 0.72 (1666.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:29:02.028650: step 190670, loss = 0.65 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:02.795412: step 190680, loss = 0.67 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:29:03.557061: step 190690, loss = 0.75 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:04.322081: step 190700, loss = 0.73 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:29:05.087582: step 190710, loss = 0.62 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:29:05.853537: step 190720, loss = 0.64 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:29:06.613707: step 190730, loss = 0.60 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:07.378628: step 190740, loss = 0.78 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:08.139473: step 190750, loss = 0.69 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:08.909106: step 190760, loss = 0.71 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:29:09.666286: step 190770, loss = 0.76 (1690.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:10.431833: step 190780, loss = 0.64 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:29:11.197196: step 190790, loss = 0.75 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:29:11.953249: step 190800, loss = 0.81 (1693.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:12.729781: step 190810, loss = 0.73 (1648.4 examples/sec; 0.078 sec/batch)
2017-05-05 21:29:13.489098: step 190820, loss = 0.65 (1685.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:14.251135: step 190830, loss = 0.70 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:15.018417: step 190840, loss = 0.80 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:29:15.772259: step 190850, loss = 0.75 (1698.0 examples/sec; 0.075 sec/batch)
2017-05-05 21:29:16.543274: step 190860, loss = 0.66 (1660.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:29:17.311874: step 190870, loss = 0.85 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:29:18.069010: step 190880, loss = 0.83 (1690.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:18.837241: step 190890, loss = 0.69 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:29:19.600905: step 190900, loss = 0.66 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:20.364257: step 190910, loss = 0.65 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:21.126706: step 190920, loss = 0.72 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:21.891075: step 190930, loss = 0.67 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:22.659008: step 190940, loss = 0.77 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:29:23.419017: step 190950, loss = 0.87 (1684.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:24.170336: step 190960, loss = 0.74 (1703.7 examples/sec; 0.075 sec/batch)
2017-05-05 21:29:24.940030: step 190970, loss = 0.79 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:29:25.702481: step 190980, loss = 0.83 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:26.471952: step 190990, loss = 0.75 (1663.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:29:27.235471: step 191000, loss = 0.73 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:27.990669: step 191010, loss = 0.77 (1694.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:28.748810: step 191020, loss = 0.76 (1688.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:29.516081: step 191030, loss = 0.66 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:29:30.286255: step 191040, loss = 0.81 (1662.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:29:31.048311: step 191050, loss = 0.60 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:31.804948: step 191060, loss = 0.75 (1691.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:32.668276: step 191070, loss = 0.70 (1482.6 examples/sec; 0.086 sec/batch)
2017-05-05 21:29:33.342917: step 191080, loss = 0.67 (1897.3 examples/sec; 0.067 sec/batch)
2017-05-05 21:29:34.112940: step 191090, loss = 0.68 (1662.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:29:34.868661: step 191100, loss = 0.72 (1693.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:35.623049: step 191110, loss = 0.79 (1696.7 examples/sec; 0.075 sec/batch)
2017-05-05 21:29:36.383269: step 191120, loss = 0.85 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:37.151086: step 191130, loss = 0.63 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:29:37.914224: step 191140, loss = 0.68 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:38.684334: step 191150, loss = 0.63 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:29:39.447563: step 191160, loss = 0.73 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:40.207977: step 191170, loss = 1.01 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:40.971581: step 191180, loss = 0.90 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:41.735372: step 191190, loss = 0.68 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:42.501590: step 191200, loss = 0.69 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:29:43.269296: step 191210, loss = 0.78 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:29:44.026191: step 191220, loss = 0.58 (1691.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:44.793890: step 191230, loss = 0.62 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:29:45.556852: step 191240, loss = 0.68 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:46.319924: step 191250, loss = 0.80 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:47.082635: step 191260, loss = 0.76 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:47.843809: step 191270, loss = 0.52 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:48.601263: step 191280, loss = 0.65 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:49.367383: step 191290, loss = 0.72 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:29:50.130822: step 191300, loss = 0.7E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494019552.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 130 events to /tmp/cifar10_train/events.out.tfevents.1494019552.GHC33.GHC.ANDREW.CMU.EDU
6 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:50.898189: step 191310, loss = 0.74 (1668.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:29:51.651286: step 191320, loss = 0.74 (1699.6 examples/sec; 0.075 sec/batch)
2017-05-05 21:29:52.423268: step 191330, loss = 0.75 (1658.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:29:53.181909: step 191340, loss = 0.74 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:53.947241: step 191350, loss = 0.67 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:29:54.711928: step 191360, loss = 0.66 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:55.474908: step 191370, loss = 0.71 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:56.237437: step 191380, loss = 0.63 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:57.000956: step 191390, loss = 0.80 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:29:57.773522: step 191400, loss = 0.68 (1656.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:29:58.542843: step 191410, loss = 0.65 (1663.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:29:59.312451: step 191420, loss = 0.76 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:00.068596: step 191430, loss = 0.67 (1692.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:00.838709: step 191440, loss = 0.71 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:01.595805: step 191450, loss = 0.69 (1690.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:02.371624: step 191460, loss = 0.56 (1649.9 examples/sec; 0.078 sec/batch)
2017-05-05 21:30:03.146764: step 191470, loss = 0.68 (1651.3 examples/sec; 0.078 sec/batch)
2017-05-05 21:30:03.907166: step 191480, loss = 0.74 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:04.676783: step 191490, loss = 0.82 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:05.442835: step 191500, loss = 0.63 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:06.206289: step 191510, loss = 0.78 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:06.971220: step 191520, loss = 0.63 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:07.739101: step 191530, loss = 0.65 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:08.502979: step 191540, loss = 0.76 (1675.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:09.272694: step 191550, loss = 0.65 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:10.044594: step 191560, loss = 0.61 (1658.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:10.807612: step 191570, loss = 0.60 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:11.568778: step 191580, loss = 0.63 (1681.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:12.342110: step 191590, loss = 0.88 (1655.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:13.114596: step 191600, loss = 0.71 (1657.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:13.886475: step 191610, loss = 0.79 (1658.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:14.651983: step 191620, loss = 0.65 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:15.414241: step 191630, loss = 0.82 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:16.176196: step 191640, loss = 0.81 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:16.937407: step 191650, loss = 0.69 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:17.709454: step 191660, loss = 0.62 (1657.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:18.479570: step 191670, loss = 0.63 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:19.247276: step 191680, loss = 0.54 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:20.000459: step 191690, loss = 0.67 (1699.3 examples/sec; 0.075 sec/batch)
2017-05-05 21:30:20.763535: step 191700, loss = 0.80 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:21.527746: step 191710, loss = 0.69 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:22.294207: step 191720, loss = 0.72 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:23.063853: step 191730, loss = 0.55 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:23.818652: step 191740, loss = 0.70 (1695.8 examples/sec; 0.075 sec/batch)
2017-05-05 21:30:24.587777: step 191750, loss = 0.63 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:25.351224: step 191760, loss = 0.73 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:26.117486: step 191770, loss = 0.86 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:26.876869: step 191780, loss = 0.82 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:27.639060: step 191790, loss = 0.63 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:28.402714: step 191800, loss = 0.76 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:29.165775: step 191810, loss = 0.76 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:29.929780: step 191820, loss = 0.52 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:30.698580: step 191830, loss = 0.66 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:31.454487: step 191840, loss = 0.66 (1693.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:32.220073: step 191850, loss = 0.68 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:32.980801: step 191860, loss = 0.58 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:33.758687: step 191870, loss = 0.60 (1645.5 examples/sec; 0.078 sec/batch)
2017-05-05 21:30:34.519810: step 191880, loss = 0.60 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:35.283611: step 191890, loss = 0.57 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:36.041661: step 191900, loss = 0.78 (1688.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:36.797225: step 191910, loss = 0.67 (1694.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:37.566639: step 191920, loss = 0.86 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:38.340589: step 191930, loss = 0.65 (1653.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:39.108270: step 191940, loss = 0.76 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:39.863463: step 191950, loss = 0.92 (1694.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:40.633245: step 191960, loss = 0.80 (1662.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:41.392812: step 191970, loss = 0.67 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:42.159611: step 191980, loss = 0.61 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:42.926484: step 191990, loss = 0.64 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:43.693296: step 192000, loss = 0.78 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:44.463652: step 192010, loss = 0.57 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:45.231075: step 192020, loss = 0.70 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:45.991522: step 192030, loss = 0.63 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:46.756990: step 192040, loss = 0.71 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:47.521162: step 192050, loss = 0.61 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:48.382312: step 192060, loss = 0.75 (1486.4 examples/sec; 0.086 sec/batch)
2017-05-05 21:30:49.046744: step 192070, loss = 0.69 (1926.5 examples/sec; 0.066 sec/batch)
2017-05-05 21:30:49.813272: step 192080, loss = 0.73 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:50.578066: step 192090, loss = 0.72 (1673.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:51.342218: step 192100, loss = 0.58 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:52.098400: step 192110, loss = 0.67 (1692.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:52.866383: step 192120, loss = 0.83 (1666.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:53.633228: step 192130, loss = 0.66 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:54.404909: step 192140, loss = 0.72 (1658.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:55.166457: step 192150, loss = 0.70 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:55.922519: step 192160, loss = 0.72 (1693.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:56.688651: step 192170, loss = 0.69 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:57.456223: step 192180, loss = 0.65 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:58.221851: step 192190, loss = 0.75 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:30:58.985165: step 192200, loss = 0.80 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:30:59.735109: step 192210, loss = 0.60 (1706.8 examples/sec; 0.075 sec/batch)
2017-05-05 21:31:00.501737: step 192220, loss = 0.63 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:31:01.268488: step 192230, loss = 0.60 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:31:02.026526: step 192240, loss = 0.85 (1688.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:02.792296: step 192250, loss = 0.68 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:31:03.546374: step 192260, loss = 0.66 (1697.5 examples/sec; 0.075 sec/batch)
2017-05-05 21:31:04.310579: step 192270, loss = 0.64 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:05.074764: step 192280, loss = 0.67 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:05.838142: step 192290, loss = 0.56 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:06.600139: step 192300, loss = 0.63 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:07.366489: step 192310, loss = 0.89 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:31:08.125997: step 192320, loss = 0.78 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:08.885602: step 192330, loss = 0.69 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:09.653458: step 192340, loss = 0.69 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:31:10.416029: step 192350, loss = 0.70 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:11.179861: step 192360, loss = 0.55 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:11.940701: step 192370, loss = 0.85 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:12.699977: step 192380, loss = 0.69 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:13.473428: step 192390, loss = 0.72 (1654.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:31:14.230709: step 192400, loss = 0.63 (1690.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:14.996208: step 192410, loss = 0.84 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:31:15.753607: step 192420, loss = 0.89 (1690.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:16.518288: step 192430, loss = 0.71 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:17.281234: step 192440, loss = 0.69 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:18.044827: step 192450, loss = 0.71 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:18.806196: step 192460, loss = 0.68 (1681.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:19.568641: step 192470, loss = 0.70 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:20.329029: step 192480, loss = 0.71 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:21.091879: step 192490, loss = 0.74 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:21.851323: step 192500, loss = 0.64 (1685.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:22.614009: step 192510, loss = 0.71 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:23.375904: step 192520, loss = 0.71 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:24.133239: step 192530, loss = 0.75 (1690.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:24.900752: step 192540, loss = 0.68 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:31:25.668944: step 192550, loss = 0.65 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:31:26.428211: step 192560, loss = 0.58 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:27.190100: step 192570, loss = 0.64 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:27.948235: step 192580, loss = 0.70 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:28.709043: step 192590, loss = 0.55 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:29.480928: step 192600, loss = 0.58 (1658.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:31:30.247547: step 192610, loss = 0.72 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:31:31.012236: step 192620, loss = 0.58 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:31.771838: step 192630, loss = 0.82 (1685.1 examples/sec; 0.076 sec/batch)
2017-05-05 E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494019552.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 146 events to /tmp/cifar10_train/events.out.tfevents.1494019552.GHC33.GHC.ANDREW.CMU.EDU
21:31:32.544372: step 192640, loss = 0.66 (1656.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:31:33.304286: step 192650, loss = 0.84 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:34.074502: step 192660, loss = 0.74 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:31:34.842269: step 192670, loss = 0.76 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:31:35.603505: step 192680, loss = 0.74 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:36.364981: step 192690, loss = 0.71 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:37.130171: step 192700, loss = 0.70 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:31:37.899439: step 192710, loss = 0.73 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:31:38.665742: step 192720, loss = 0.65 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:31:39.426184: step 192730, loss = 0.70 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:40.189801: step 192740, loss = 0.70 (1676.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:40.951873: step 192750, loss = 0.68 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:41.711015: step 192760, loss = 0.64 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:42.475836: step 192770, loss = 0.80 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:43.239855: step 192780, loss = 0.67 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:43.995127: step 192790, loss = 0.66 (1694.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:44.757278: step 192800, loss = 0.73 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:45.524878: step 192810, loss = 0.75 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:31:46.290664: step 192820, loss = 0.64 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:31:47.055525: step 192830, loss = 0.80 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:47.809214: step 192840, loss = 0.72 (1698.3 examples/sec; 0.075 sec/batch)
2017-05-05 21:31:48.578915: step 192850, loss = 0.71 (1663.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:31:49.348562: step 192860, loss = 0.71 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:31:50.111926: step 192870, loss = 0.65 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:50.883251: step 192880, loss = 0.68 (1659.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:31:51.634795: step 192890, loss = 0.65 (1703.2 examples/sec; 0.075 sec/batch)
2017-05-05 21:31:52.403072: step 192900, loss = 0.59 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:31:53.167991: step 192910, loss = 0.81 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:53.934801: step 192920, loss = 0.67 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:31:54.701752: step 192930, loss = 0.79 (1668.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:31:55.459874: step 192940, loss = 0.76 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:56.220197: step 192950, loss = 0.62 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:31:56.987428: step 192960, loss = 0.76 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:31:57.757818: step 192970, loss = 0.65 (1661.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:31:58.523014: step 192980, loss = 0.73 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:31:59.285184: step 192990, loss = 0.55 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:00.044732: step 193000, loss = 0.86 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:00.806918: step 193010, loss = 0.72 (1679.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:01.578399: step 193020, loss = 0.61 (1659.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:32:02.336739: step 193030, loss = 0.62 (1687.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:03.100867: step 193040, loss = 0.64 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:03.952246: step 193050, loss = 0.64 (1503.4 examples/sec; 0.085 sec/batch)
2017-05-05 21:32:04.622786: step 193060, loss = 0.58 (1908.9 examples/sec; 0.067 sec/batch)
2017-05-05 21:32:05.383309: step 193070, loss = 0.68 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:06.149110: step 193080, loss = 0.73 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:32:06.903218: step 193090, loss = 0.83 (1697.4 examples/sec; 0.075 sec/batch)
2017-05-05 21:32:07.666169: step 193100, loss = 0.63 (1677.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:08.427897: step 193110, loss = 0.83 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:09.192879: step 193120, loss = 0.67 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:09.960220: step 193130, loss = 0.76 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:32:10.720507: step 193140, loss = 0.81 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:11.483623: step 193150, loss = 0.84 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:12.238175: step 193160, loss = 0.79 (1696.4 examples/sec; 0.075 sec/batch)
2017-05-05 21:32:13.006783: step 193170, loss = 0.67 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:32:13.776944: step 193180, loss = 0.73 (1662.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:32:14.548759: step 193190, loss = 0.76 (1658.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:32:15.317941: step 193200, loss = 0.57 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:32:16.074239: step 193210, loss = 0.74 (1692.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:16.832994: step 193220, loss = 0.64 (1687.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:17.592273: step 193230, loss = 0.86 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:18.357564: step 193240, loss = 0.61 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:32:19.125188: step 193250, loss = 0.83 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:32:19.880781: step 193260, loss = 0.62 (1694.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:20.644125: step 193270, loss = 0.71 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:21.405234: step 193280, loss = 0.69 (1681.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:22.166916: step 193290, loss = 0.66 (1680.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:22.935737: step 193300, loss = 0.64 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:32:23.691003: step 193310, loss = 0.79 (1694.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:24.456996: step 193320, loss = 0.68 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:32:25.228840: step 193330, loss = 0.72 (1658.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:32:25.988179: step 193340, loss = 0.81 (1685.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:26.753327: step 193350, loss = 0.70 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:32:27.515956: step 193360, loss = 0.65 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:28.279124: step 193370, loss = 0.78 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:29.040664: step 193380, loss = 0.62 (1680.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:29.806158: step 193390, loss = 0.70 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:32:30.570663: step 193400, loss = 0.66 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:31.341103: step 193410, loss = 0.69 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:32:32.096280: step 193420, loss = 0.72 (1695.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:32.863138: step 193430, loss = 0.67 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:32:33.633240: step 193440, loss = 0.82 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:32:34.399246: step 193450, loss = 0.72 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:32:35.163223: step 193460, loss = 0.71 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:35.925841: step 193470, loss = 0.70 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:36.690009: step 193480, loss = 0.67 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:37.454406: step 193490, loss = 0.75 (1674.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:38.224696: step 193500, loss = 0.76 (1661.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:32:38.987343: step 193510, loss = 0.62 (1678.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:39.745216: step 193520, loss = 0.68 (1688.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:40.510957: step 193530, loss = 0.78 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:32:41.273802: step 193540, loss = 0.69 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:42.039656: step 193550, loss = 0.84 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:32:42.803185: step 193560, loss = 0.73 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:43.560301: step 193570, loss = 0.62 (1690.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:44.316680: step 193580, loss = 0.66 (1692.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:45.085905: step 193590, loss = 0.75 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:32:45.849683: step 193600, loss = 0.73 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:46.616166: step 193610, loss = 0.67 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:32:47.378094: step 193620, loss = 0.65 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:48.140186: step 193630, loss = 0.81 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:48.895306: step 193640, loss = 0.73 (1695.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:49.669088: step 193650, loss = 0.60 (1654.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:32:50.443319: step 193660, loss = 0.68 (1653.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:32:51.202423: step 193670, loss = 0.86 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:51.956979: step 193680, loss = 0.60 (1696.4 examples/sec; 0.075 sec/batch)
2017-05-05 21:32:52.733452: step 193690, loss = 0.64 (1648.5 examples/sec; 0.078 sec/batch)
2017-05-05 21:32:53.502442: step 193700, loss = 0.80 (1664.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:32:54.266973: step 193710, loss = 0.70 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:55.032786: step 193720, loss = 0.71 (1671.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:32:55.790248: step 193730, loss = 0.88 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:56.549869: step 193740, loss = 0.67 (1685.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:32:57.315057: step 193750, loss = 0.66 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:32:58.082110: step 193760, loss = 0.56 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:32:58.849818: step 193770, loss = 0.66 (1667.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:32:59.611464: step 193780, loss = 0.62 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:00.377513: step 193790, loss = 0.74 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:33:01.141855: step 193800, loss = 0.71 (1674.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:01.904871: step 193810, loss = 0.71 (1677.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:02.669958: step 193820, loss = 0.87 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:33:03.430061: step 193830, loss = 0.64 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:04.197147: step 193840, loss = 0.80 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:33:04.963284: step 193850, loss = 0.67 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:33:05.726773: step 193860, loss = 0.74 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:06.495864: step 193870, loss = 0.56 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:33:07.264433: step 193880, loss = 0.70 (1665.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:33:08.022889: step 193890, loss = 0.55 (1687.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:08.785792: step 193900, loss = 0.69 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:09.553701: step 193910, loss = 0.61 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:33:10.317810: step 193920, loss = 0.62 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:11.079078: step 193930, loss = 0.60 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:11.835884: step 193940, loss = 0.63 (1691.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:12.602905: step 193950, loss = 0.92 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:33:13.368544: step 193960, loss = 0.72 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:33:14.132253: step 193970, loss = 0.69 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:14.896963: step 193980, loss = 0.72 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:15.659073: step 193990, loss = 0.60 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:16.422589: step 194000, loss = 0.69 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:17.188742: step 194010, loss = 0.71 (1670.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:33:17.958807: step 194020, loss = 0.77 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:33:18.720740: step 194030, loss = 0.58 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:19.591472: step 194040, loss = 0.74 (1470.0 examples/sec; 0.087 sec/batch)
2017-05-05 21:33:20.243193: step 194050, loss = 0.77 (1964.0 examples/sec; 0.065 sec/batch)
2017-05-05 21:33:21.003638: step 194060, loss = 0.63 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:21.771167: step 194070, loss = 0.59 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:33:22.533703: step 194080, loss = 0.65 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:23.302672: step 194090, loss = 0.71 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:33:24.062231: step 194100, loss = 0.79 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:24.827784: step 194110, loss = 0.57 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:33:25.584957: step 194120, loss = 0.72 (1690.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:26.357875: step 194130, loss = 0.65 (1656.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:33:27.117834: step 194140, loss = 0.75 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:27.876926: step 194150, loss = 0.79 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:28.644272: step 194160, loss = 0.58 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:33:29.412411: step 194170, loss = 0.60 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:33:30.180965: step 194180, loss = 0.64 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:33:30.949760: step 194190, loss = 0.70 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:33:31.708301: step 194200, loss = 0.74 (1687.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:32.469386: step 194210, loss = 0.61 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:33.232417: step 194220, loss = 0.70 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:33.996497: step 194230, loss = 0.68 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:34.768780: step 194240, loss = 0.82 (1657.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:33:35.525315: step 194250, loss = 0.68 (1691.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:36.288010: step 194260, loss = 0.67 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:37.044833: step 194270, loss = 0.73 (1691.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:37.811264: step 194280, loss = 0.67 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:33:38.578877: step 194290, loss = 0.71 (1667.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:33:39.345351: step 194300, loss = 0.68 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:33:40.101049: step 194310, loss = 0.57 (1693.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:40.861051: step 194320, loss = 0.83 (1684.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:41.633192: step 194330, loss = 0.86 (1657.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:33:42.393134: step 194340, loss = 0.64 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:43.157414: step 194350, loss = 0.68 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:43.911631: step 194360, loss = 0.88 (1697.1 examples/sec; 0.075 sec/batch)
2017-05-05 21:33:44.677541: step 194370, loss = 0.64 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:33:45.445663: step 194380, loss = 0.56 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:33:46.212303: step 194390, loss = 0.83 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:33:46.974776: step 194400, loss = 0.50 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:47.731835: step 194410, loss = 0.76 (1690.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:48E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494019552.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 162 events to /tmp/cifar10_train/events.out.tfevents.1494019552.GHC33.GHC.ANDREW.CMU.EDU
.494211: step 194420, loss = 0.68 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:49.257459: step 194430, loss = 0.67 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:50.020715: step 194440, loss = 0.72 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:50.786798: step 194450, loss = 0.64 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:33:51.545501: step 194460, loss = 0.84 (1687.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:52.308749: step 194470, loss = 0.77 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:53.070890: step 194480, loss = 0.51 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:53.837297: step 194490, loss = 0.70 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:33:54.601034: step 194500, loss = 0.65 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:55.365500: step 194510, loss = 0.68 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:56.126905: step 194520, loss = 0.75 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:56.884419: step 194530, loss = 0.69 (1689.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:57.649923: step 194540, loss = 0.70 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:33:58.418945: step 194550, loss = 0.65 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:33:59.182188: step 194560, loss = 0.67 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:33:59.942628: step 194570, loss = 0.58 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:00.710880: step 194580, loss = 0.72 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:01.471483: step 194590, loss = 0.58 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:02.240682: step 194600, loss = 0.67 (1664.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:03.001294: step 194610, loss = 0.55 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:03.758806: step 194620, loss = 0.83 (1689.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:04.525996: step 194630, loss = 0.68 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:05.293091: step 194640, loss = 0.67 (1668.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:06.058779: step 194650, loss = 0.72 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:06.821586: step 194660, loss = 0.70 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:07.582369: step 194670, loss = 0.69 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:08.351725: step 194680, loss = 0.66 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:09.115201: step 194690, loss = 0.77 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:09.873979: step 194700, loss = 0.81 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:10.640091: step 194710, loss = 0.74 (1670.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:11.402353: step 194720, loss = 0.75 (1679.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:12.163304: step 194730, loss = 0.73 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:12.929531: step 194740, loss = 0.74 (1670.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:13.701021: step 194750, loss = 0.61 (1659.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:14.461604: step 194760, loss = 0.73 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:15.231939: step 194770, loss = 0.81 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:15.987800: step 194780, loss = 0.54 (1693.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:16.751126: step 194790, loss = 0.82 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:17.522052: step 194800, loss = 0.65 (1660.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:18.295942: step 194810, loss = 0.62 (1654.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:19.055665: step 194820, loss = 0.57 (1684.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:19.809531: step 194830, loss = 0.73 (1697.9 examples/sec; 0.075 sec/batch)
2017-05-05 21:34:20.576877: step 194840, loss = 0.78 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:21.340093: step 194850, loss = 0.68 (1677.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:22.109694: step 194860, loss = 0.62 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:22.873215: step 194870, loss = 0.70 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:23.630171: step 194880, loss = 0.71 (1691.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:24.392295: step 194890, loss = 0.64 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:25.156981: step 194900, loss = 0.77 (1673.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:25.921153: step 194910, loss = 0.69 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:26.681751: step 194920, loss = 0.71 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:27.448804: step 194930, loss = 0.67 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:28.217828: step 194940, loss = 0.66 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:28.981112: step 194950, loss = 0.77 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:29.748757: step 194960, loss = 0.61 (1667.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:30.519492: step 194970, loss = 0.69 (1660.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:31.285283: step 194980, loss = 0.60 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:32.043160: step 194990, loss = 0.72 (1688.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:32.808327: step 195000, loss = 0.57 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:33.570914: step 195010, loss = 0.66 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:34.337437: step 195020, loss = 0.61 (1669.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:35.202051: step 195030, loss = 0.65 (1480.4 examples/sec; 0.086 sec/batch)
2017-05-05 21:34:35.863129: step 195040, loss = 0.69 (1936.2 examples/sec; 0.066 sec/batch)
2017-05-05 21:34:36.629401: step 195050, loss = 0.81 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:37.392054: step 195060, loss = 0.56 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:38.160827: step 195070, loss = 0.62 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:38.924244: step 195080, loss = 0.64 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:39.684223: step 195090, loss = 0.76 (1684.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:40.453274: step 195100, loss = 0.59 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:41.219907: step 195110, loss = 0.75 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:41.985633: step 195120, loss = 0.76 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:42.751518: step 195130, loss = 0.65 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:43.512400: step 195140, loss = 0.92 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:44.272675: step 195150, loss = 0.77 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:45.046759: step 195160, loss = 0.83 (1653.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:45.812548: step 195170, loss = 0.67 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:46.580348: step 195180, loss = 0.68 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:47.346319: step 195190, loss = 0.76 (1671.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:48.102245: step 195200, loss = 0.69 (1693.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:48.866954: step 195210, loss = 0.74 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:49.638828: step 195220, loss = 0.64 (1658.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:50.413807: step 195230, loss = 0.81 (1651.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:51.169539: step 195240, loss = 0.71 (1693.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:51.926981: step 195250, loss = 0.73 (1689.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:52.697867: step 195260, loss = 0.71 (1660.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:53.459787: step 195270, loss = 0.63 (1680.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:54.226435: step 195280, loss = 0.68 (1669.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:54.988082: step 195290, loss = 0.77 (1680.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:55.747163: step 195300, loss = 0.66 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:56.507712: step 195310, loss = 0.71 (1683.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:57.276255: step 195320, loss = 0.74 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:58.034567: step 195330, loss = 0.67 (1688.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:34:58.805711: step 195340, loss = 0.59 (1659.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:34:59.567483: step 195350, loss = 0.72 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:00.338413: step 195360, loss = 0.66 (1660.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:35:01.106465: step 195370, loss = 0.61 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:35:01.863305: step 195380, loss = 0.65 (1691.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:02.635053: step 195390, loss = 0.76 (1658.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:35:03.398432: step 195400, loss = 0.66 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:04.154411: step 195410, loss = 0.62 (1693.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:04.919396: step 195420, loss = 0.77 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:05.688404: step 195430, loss = 0.67 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:35:06.454336: step 195440, loss = 0.67 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:35:07.218875: step 195450, loss = 0.85 (1674.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:07.975781: step 195460, loss = 0.91 (1691.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:08.745144: step 195470, loss = 0.73 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:35:09.510890: step 195480, loss = 0.71 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:35:10.279231: step 195490, loss = 0.74 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:35:11.038786: step 195500, loss = 0.65 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:11.793454: step 195510, loss = 0.83 (1696.1 examples/sec; 0.075 sec/batch)
2017-05-05 21:35:12.556048: step 195520, loss = 0.70 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:13.318773: step 195530, loss = 0.77 (1678.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:14.078844: step 195540, loss = 0.65 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:14.847042: step 195550, loss = 0.66 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:35:15.604201: step 195560, loss = 0.76 (1690.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:16.367971: step 195570, loss = 0.59 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:17.128567: step 195580, loss = 0.71 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:17.892154: step 195590, loss = 0.58 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:18.652473: step 195600, loss = 0.87 (1683.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:19.421582: step 195610, loss = 0.70 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:35:20.183716: step 195620, loss = 0.71 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:20.952468: step 195630, loss = 0.68 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:35:21.714944: step 195640, loss = 0.79 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:22.476230: step 195650, loss = 0.74 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:23.241627: step 195660, loss = 0.69 (1672.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:35:23.998595: step 195670, loss = 0.71 (1691.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:24.766613: step 195680, loss = 0.57 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:35:25.530670: step 195690, loss = 0.70 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:26.300034: step 195700, loss = 0.80 (1663.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:35:27.065267: step 195710, loss = 0.59 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:35:27.827016: step 195720, loss = 0.64 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:28.599059: step 195730, loss = 0.65 (1657.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:35:29.371814: step 195740, loss = 0.75 (1656.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:35:30.137058: step 195750, loss = 0.65 (1672.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:35:30.899823: step 195760, loss = 0.65 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:31.655502: step 195770, loss = 0.59 (1693.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:32.424756: step 195780, loss = 0.66 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:35:33.183812: step 195790, loss = 0.73 (1686.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:33.944751: step 195800, loss = 0.68 (1682.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:34.715918: step 195810, loss = 0.62 (1659.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:35:35.472843: step 195820, loss = 0.64 (1691.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:36.234917: step 195830, loss = 0.75 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:36.997054: step 195840, loss = 0.67 (1679.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:37.760712: step 195850, loss = 0.60 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:38.527245: step 195860, loss = 0.72 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:35:39.289924: step 195870, loss = 0.60 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:40.046588: step 195880, loss = 0.65 (1691.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:40.814132: step 195890, loss = 0.66 (1667.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:35:41.575537: step 195900, loss = 0.72 (1681.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:42.349868: step 195910, loss = 0.62 (1653.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:35:43.115655: step 195920, loss = 0.77 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:35:43.867776: step 195930, loss = 0.76 (1701.9 examples/sec; 0.075 sec/batch)
2017-05-05 21:35:44.631141: step 195940, loss = 0.79 (1676.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:45.397184: step 195950, loss = 0.65 (1670.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:35:46.163936: step 195960, loss = 0.61 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:35:46.924045: step 195970, loss = 0.87 (1684.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:47.681965: step 195980, loss = 0.69 (1688.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:48.446304: step 195990, loss = 0.68 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:49.209820: step 196000, loss = 0.68 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:49.970589: step 196010, loss = 0.76 (1682.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:50.834983: step 196020, loss = 0.60 (1480.8 examples/sec; 0.086 sec/batch)
2017-05-05 21:35:51.495344: step 196030, loss = 0.69 (1938.3 examples/sec; 0.066 sec/batch)
2017-05-05 21:35:52.259268: step 196040, loss = 0.76 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:53.026111: step 196050, loss = 0.67 (1669.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:35:53.792973: step 196060, loss = 0.73 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:35:54.559851: step 196070, loss = 0.81 (1669.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:35:55.322554: step 196080, loss = 0.76 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:56.082826: step 196090, loss = 0.78 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:56.837958: step 196100, loss = 0.67 (1695.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:57.604896: step 196110, loss = 0.77 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:35:58.367111: step 196120, loss = 0.67 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:59.126906: step 196130, loss = 0.58 (1684.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:35:59.877649: step 196140, loss = 0.86 (1705.0 examples/sec; 0.075 sec/batch)
2017-05-05 21:36:00.637040: step 196150, loss = 0.81 (1685.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:01.401863: step 196160, loss = 0.59 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:02.156420: step 196170, loss = 0.62 (1696.4 examples/sec; 0.075 sec/batch)
2017-05-05 21:36:02.916230: step 196180, loss = 0.69 (1684.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:03.672616: step 196190, loss = 0.82 (1692.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:04.430609:E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494019552.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 178 events to /tmp/cifar10_train/events.out.tfevents.1494019552.GHC33.GHC.ANDREW.CMU.EDU
 step 196200, loss = 0.58 (1688.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:05.194800: step 196210, loss = 0.69 (1675.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:05.961562: step 196220, loss = 0.61 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:36:06.756967: step 196230, loss = 0.77 (1609.2 examples/sec; 0.080 sec/batch)
2017-05-05 21:36:07.509853: step 196240, loss = 0.57 (1700.1 examples/sec; 0.075 sec/batch)
2017-05-05 21:36:08.274747: step 196250, loss = 0.69 (1673.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:09.051302: step 196260, loss = 0.79 (1648.3 examples/sec; 0.078 sec/batch)
2017-05-05 21:36:09.818318: step 196270, loss = 0.64 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:36:10.589782: step 196280, loss = 0.66 (1659.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:36:11.352638: step 196290, loss = 0.72 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:12.113503: step 196300, loss = 0.70 (1682.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:12.881669: step 196310, loss = 0.72 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:36:13.649228: step 196320, loss = 0.62 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:36:14.426560: step 196330, loss = 0.65 (1646.7 examples/sec; 0.078 sec/batch)
2017-05-05 21:36:15.190011: step 196340, loss = 0.72 (1676.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:15.948574: step 196350, loss = 0.66 (1687.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:16.705913: step 196360, loss = 0.74 (1690.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:17.478583: step 196370, loss = 0.52 (1656.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:36:18.243420: step 196380, loss = 0.68 (1673.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:19.006275: step 196390, loss = 0.64 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:19.758274: step 196400, loss = 0.74 (1702.1 examples/sec; 0.075 sec/batch)
2017-05-05 21:36:20.520626: step 196410, loss = 0.67 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:21.288451: step 196420, loss = 0.65 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:36:22.055998: step 196430, loss = 0.72 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:36:22.825986: step 196440, loss = 0.59 (1662.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:36:23.581777: step 196450, loss = 0.72 (1693.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:24.346761: step 196460, loss = 0.63 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:25.108352: step 196470, loss = 0.85 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:25.870843: step 196480, loss = 0.75 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:26.637218: step 196490, loss = 0.77 (1670.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:36:27.395681: step 196500, loss = 0.67 (1687.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:28.156131: step 196510, loss = 0.61 (1683.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:28.920218: step 196520, loss = 0.64 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:29.690762: step 196530, loss = 0.67 (1661.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:36:30.454458: step 196540, loss = 0.65 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:31.213970: step 196550, loss = 0.81 (1685.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:31.970458: step 196560, loss = 0.71 (1692.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:32.732904: step 196570, loss = 0.67 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:33.496657: step 196580, loss = 0.61 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:34.263084: step 196590, loss = 0.72 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:36:35.025130: step 196600, loss = 0.57 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:35.778742: step 196610, loss = 0.77 (1698.5 examples/sec; 0.075 sec/batch)
2017-05-05 21:36:36.551318: step 196620, loss = 0.75 (1656.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:36:37.317011: step 196630, loss = 0.62 (1671.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:36:38.083784: step 196640, loss = 0.75 (1669.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:36:38.847612: step 196650, loss = 0.60 (1675.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:39.608844: step 196660, loss = 0.70 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:40.372957: step 196670, loss = 0.72 (1675.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:41.131592: step 196680, loss = 0.75 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:41.891126: step 196690, loss = 0.67 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:42.656862: step 196700, loss = 0.78 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:36:43.420406: step 196710, loss = 0.57 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:44.185505: step 196720, loss = 0.60 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:36:44.948093: step 196730, loss = 0.69 (1678.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:45.718138: step 196740, loss = 0.62 (1662.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:36:46.478286: step 196750, loss = 0.65 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:47.246552: step 196760, loss = 0.73 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:36:47.997941: step 196770, loss = 0.67 (1703.5 examples/sec; 0.075 sec/batch)
2017-05-05 21:36:48.762289: step 196780, loss = 0.65 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:49.536204: step 196790, loss = 0.67 (1653.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:36:50.303431: step 196800, loss = 0.60 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:36:51.062097: step 196810, loss = 0.71 (1687.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:51.817908: step 196820, loss = 0.62 (1693.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:52.582630: step 196830, loss = 0.71 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:53.342848: step 196840, loss = 0.74 (1683.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:54.107585: step 196850, loss = 0.73 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:54.870470: step 196860, loss = 0.54 (1677.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:55.625378: step 196870, loss = 0.64 (1695.6 examples/sec; 0.075 sec/batch)
2017-05-05 21:36:56.390636: step 196880, loss = 0.73 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:36:57.159553: step 196890, loss = 0.71 (1664.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:36:57.925009: step 196900, loss = 0.76 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:36:58.689257: step 196910, loss = 0.64 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:36:59.449625: step 196920, loss = 0.61 (1683.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:00.215558: step 196930, loss = 0.57 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:37:00.973409: step 196940, loss = 0.57 (1689.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:01.732381: step 196950, loss = 0.57 (1686.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:02.493850: step 196960, loss = 0.58 (1680.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:03.254640: step 196970, loss = 0.60 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:04.017027: step 196980, loss = 0.70 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:04.777667: step 196990, loss = 0.70 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:05.544467: step 197000, loss = 0.74 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:37:06.421904: step 197010, loss = 0.62 (1458.8 examples/sec; 0.088 sec/batch)
2017-05-05 21:37:07.083188: step 197020, loss = 0.65 (1935.6 examples/sec; 0.066 sec/batch)
2017-05-05 21:37:07.836326: step 197030, loss = 0.75 (1699.6 examples/sec; 0.075 sec/batch)
2017-05-05 21:37:08.597592: step 197040, loss = 0.60 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:09.368051: step 197050, loss = 0.63 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:37:10.130344: step 197060, loss = 0.74 (1679.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:10.899939: step 197070, loss = 0.70 (1663.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:37:11.657588: step 197080, loss = 0.67 (1689.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:12.421847: step 197090, loss = 0.78 (1674.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:13.189073: step 197100, loss = 1.08 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:37:13.955262: step 197110, loss = 0.65 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:37:14.717227: step 197120, loss = 0.80 (1679.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:15.479577: step 197130, loss = 0.69 (1679.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:16.241646: step 197140, loss = 0.71 (1679.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:17.008413: step 197150, loss = 0.65 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:37:17.771504: step 197160, loss = 0.67 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:18.537924: step 197170, loss = 0.59 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:37:19.309573: step 197180, loss = 0.72 (1658.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:37:20.065509: step 197190, loss = 0.70 (1693.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:20.839733: step 197200, loss = 0.61 (1653.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:37:21.610415: step 197210, loss = 0.70 (1660.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:37:22.376196: step 197220, loss = 0.75 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:37:23.142494: step 197230, loss = 0.71 (1670.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:37:23.901379: step 197240, loss = 0.62 (1686.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:24.659587: step 197250, loss = 0.79 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:25.426619: step 197260, loss = 0.74 (1668.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:37:26.192149: step 197270, loss = 0.86 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:37:26.954663: step 197280, loss = 0.63 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:27.720257: step 197290, loss = 0.78 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:37:28.481523: step 197300, loss = 0.77 (1681.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:29.239551: step 197310, loss = 0.63 (1688.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:30.004533: step 197320, loss = 0.65 (1673.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:30.763392: step 197330, loss = 0.66 (1686.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:31.521203: step 197340, loss = 0.55 (1689.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:32.283994: step 197350, loss = 0.71 (1678.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:33.051076: step 197360, loss = 0.66 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:37:33.819141: step 197370, loss = 0.67 (1666.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:37:34.592399: step 197380, loss = 0.73 (1655.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:37:35.365691: step 197390, loss = 0.65 (1655.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:37:36.127743: step 197400, loss = 0.74 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:36.899465: step 197410, loss = 0.53 (1658.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:37:37.671302: step 197420, loss = 0.78 (1658.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:37:38.435646: step 197430, loss = 0.74 (1674.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:39.197845: step 197440, loss = 0.69 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:39.946134: step 197450, loss = 0.88 (1710.6 examples/sec; 0.075 sec/batch)
2017-05-05 21:37:40.710770: step 197460, loss = 0.76 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:41.483378: step 197470, loss = 0.72 (1656.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:37:42.245585: step 197480, loss = 0.61 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:43.017089: step 197490, loss = 0.66 (1659.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:37:43.778133: step 197500, loss = 0.73 (1681.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:44.547647: step 197510, loss = 0.60 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:37:45.310518: step 197520, loss = 0.82 (1677.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:46.075322: step 197530, loss = 0.59 (1673.6 examples/sec; 0.07E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494019552.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 194 events to /tmp/cifar10_train/events.out.tfevents.1494019552.GHC33.GHC.ANDREW.CMU.EDU
6 sec/batch)
2017-05-05 21:37:46.840698: step 197540, loss = 0.69 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:37:47.595694: step 197550, loss = 0.76 (1695.4 examples/sec; 0.075 sec/batch)
2017-05-05 21:37:48.360777: step 197560, loss = 0.73 (1673.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:37:49.120033: step 197570, loss = 0.67 (1685.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:49.889536: step 197580, loss = 0.65 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:37:50.656479: step 197590, loss = 0.59 (1669.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:37:51.414900: step 197600, loss = 0.62 (1687.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:52.175512: step 197610, loss = 0.73 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:52.933501: step 197620, loss = 0.70 (1688.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:53.698764: step 197630, loss = 0.76 (1672.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:37:54.460621: step 197640, loss = 0.64 (1680.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:55.225209: step 197650, loss = 0.69 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:55.978707: step 197660, loss = 0.78 (1698.8 examples/sec; 0.075 sec/batch)
2017-05-05 21:37:56.734491: step 197670, loss = 0.67 (1693.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:57.498142: step 197680, loss = 0.59 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:58.267888: step 197690, loss = 0.59 (1662.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:37:59.030585: step 197700, loss = 0.66 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:37:59.781245: step 197710, loss = 0.56 (1705.2 examples/sec; 0.075 sec/batch)
2017-05-05 21:38:00.540386: step 197720, loss = 0.59 (1686.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:01.303062: step 197730, loss = 0.69 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:02.060388: step 197740, loss = 0.74 (1690.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:02.829031: step 197750, loss = 0.69 (1665.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:38:03.584608: step 197760, loss = 0.72 (1694.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:04.348327: step 197770, loss = 0.78 (1676.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:05.110734: step 197780, loss = 0.60 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:05.877136: step 197790, loss = 0.64 (1670.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:38:06.645188: step 197800, loss = 0.78 (1666.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:38:07.412971: step 197810, loss = 0.66 (1667.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:38:08.173623: step 197820, loss = 0.64 (1682.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:08.940400: step 197830, loss = 0.65 (1669.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:38:09.705129: step 197840, loss = 0.76 (1673.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:10.470077: step 197850, loss = 0.71 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:11.235276: step 197860, loss = 0.83 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:38:11.995449: step 197870, loss = 0.75 (1683.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:12.759880: step 197880, loss = 0.74 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:13.530753: step 197890, loss = 0.86 (1660.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:38:14.298598: step 197900, loss = 0.89 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:38:15.066166: step 197910, loss = 0.82 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:38:15.824484: step 197920, loss = 0.65 (1688.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:16.590515: step 197930, loss = 0.69 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:38:17.355387: step 197940, loss = 0.77 (1673.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:18.127003: step 197950, loss = 0.64 (1658.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:38:18.888722: step 197960, loss = 0.79 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:19.650534: step 197970, loss = 0.69 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:20.414013: step 197980, loss = 0.61 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:21.181252: step 197990, loss = 0.66 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:38:22.044528: step 198000, loss = 0.64 (1482.7 examples/sec; 0.086 sec/batch)
2017-05-05 21:38:22.710208: step 198010, loss = 0.70 (1922.8 examples/sec; 0.067 sec/batch)
2017-05-05 21:38:23.469407: step 198020, loss = 0.89 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:24.228197: step 198030, loss = 0.65 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:24.996053: step 198040, loss = 0.67 (1667.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:38:25.759099: step 198050, loss = 0.70 (1677.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:26.528191: step 198060, loss = 0.64 (1664.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:38:27.291709: step 198070, loss = 0.59 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:28.049093: step 198080, loss = 0.63 (1690.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:28.812644: step 198090, loss = 0.50 (1676.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:29.579242: step 198100, loss = 0.74 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:38:30.346696: step 198110, loss = 0.74 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:38:31.108181: step 198120, loss = 0.75 (1681.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:31.862191: step 198130, loss = 0.55 (1697.6 examples/sec; 0.075 sec/batch)
2017-05-05 21:38:32.631056: step 198140, loss = 0.68 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:38:33.396373: step 198150, loss = 0.60 (1672.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:38:34.159773: step 198160, loss = 0.55 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:34.924784: step 198170, loss = 0.54 (1673.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:38:35.675497: step 198180, loss = 0.59 (1705.1 examples/sec; 0.075 sec/batch)
2017-05-05 21:38:36.437704: step 198190, loss = 0.76 (1679.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:37.204926: step 198200, loss = 0.71 (1668.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:38:37.968238: step 198210, loss = 0.72 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:38.736010: step 198220, loss = 0.81 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:38:39.496621: step 198230, loss = 0.70 (1682.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:40.256547: step 198240, loss = 0.69 (1684.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:41.015544: step 198250, loss = 0.75 (1686.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:41.784613: step 198260, loss = 0.70 (1664.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:38:42.542469: step 198270, loss = 0.71 (1688.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:43.310453: step 198280, loss = 0.72 (1666.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:38:44.067063: step 198290, loss = 0.72 (1691.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:44.830242: step 198300, loss = 0.55 (1677.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:45.594339: step 198310, loss = 0.67 (1675.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:46.357759: step 198320, loss = 0.68 (1676.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:47.121990: step 198330, loss = 0.78 (1674.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:47.874857: step 198340, loss = 0.73 (1700.2 examples/sec; 0.075 sec/batch)
2017-05-05 21:38:48.638119: step 198350, loss = 0.64 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:49.397458: step 198360, loss = 0.64 (1685.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:50.169150: step 198370, loss = 0.76 (1658.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:38:50.927259: step 198380, loss = 0.70 (1688.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:51.684546: step 198390, loss = 0.60 (1690.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:52.448113: step 198400, loss = 0.65 (1676.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:53.213988: step 198410, loss = 0.76 (1671.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:38:53.980578: step 198420, loss = 0.58 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:38:54.753537: step 198430, loss = 0.83 (1656.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:38:55.515518: step 198440, loss = 0.71 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:56.270428: step 198450, loss = 0.61 (1695.6 examples/sec; 0.075 sec/batch)
2017-05-05 21:38:57.035580: step 198460, loss = 0.64 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:38:57.800106: step 198470, loss = 0.69 (1674.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:58.559020: step 198480, loss = 0.74 (1686.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:38:59.322715: step 198490, loss = 0.74 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:00.081826: step 198500, loss = 0.69 (1686.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:00.846260: step 198510, loss = 0.70 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:01.607969: step 198520, loss = 0.75 (1680.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:02.377104: step 198530, loss = 0.66 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:39:03.140880: step 198540, loss = 0.64 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:03.898415: step 198550, loss = 0.66 (1689.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:04.665020: step 198560, loss = 0.69 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:39:05.424573: step 198570, loss = 0.77 (1685.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:06.196884: step 198580, loss = 0.68 (1657.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:39:06.958656: step 198590, loss = 0.69 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:07.715508: step 198600, loss = 0.67 (1691.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:08.482653: step 198610, loss = 0.82 (1668.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:39:09.245435: step 198620, loss = 0.70 (1678.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:10.012005: step 198630, loss = 0.62 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:39:10.773222: step 198640, loss = 0.71 (1681.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:11.533345: step 198650, loss = 0.75 (1683.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:12.295857: step 198660, loss = 0.78 (1678.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:13.063214: step 198670, loss = 0.51 (1668.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:39:13.823618: step 198680, loss = 0.55 (1683.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:14.586912: step 198690, loss = 0.57 (1676.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:15.347636: step 198700, loss = 0.67 (1682.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:16.109201: step 198710, loss = 0.82 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:16.882271: step 198720, loss = 0.70 (1655.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:39:17.647427: step 198730, loss = 0.70 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:39:18.411081: step 198740, loss = 0.55 (1676.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:19.179376: step 198750, loss = 0.84 (1666.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:39:19.936636: step 198760, loss = 0.64 (1690.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:20.703340: step 198770, loss = 0.68 (1669.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:39:21.472145: step 198780, loss = 0.50 (1664.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:39:22.234803: step 198790, loss = 0.73 (1678.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:23.012203: step 198800, loss = 0.67 (1646.5 examples/sec; 0.078 sec/batch)
2017-05-05 21:39:23.764496: step 198810, loss = 0.61 (1701.5 examples/sec; 0.075 sec/batch)
2017-05-05 21:39:24.524767: step 198820, loss = 0.61 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:25.293618: step 198830, loss = 0.67 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:39:26.056900: step 198840, loss = 0.63 (1677.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:26.827253: step 198850, loss = 0.82 (1661.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:39:27.580991: step 198860, loss = 0.69 (1698.2 examples/sec; 0.075 sec/batch)
2017-05-05 21:39:28.352032: step 198870, loss = 0.73 (1660.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:39:29.109583: step 198880, loss = 0.51 (1689.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:29.878378: step 198890, loss = 0.97 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:39:30.638628: step 198900, loss = 0.65 (1683.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:31.403790: step 198910, loss = 0.67 (1672.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:39:32.160779: step 198920, loss = 0.66 (1690.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:32.917122: step 198930, loss = 0.70 (1692.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:33.686080: step 198940, loss = 0.71 (1664.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:39:34.454251: step 198950, loss = 0.76 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:39:35.221563: step 198960, loss = 0.67 (1668.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:39:35.976088: step 198970, loss = 0.62 (1696.4 examples/sec; 0.075 sec/batch)
2017-05-05 21:39:36.740009: step 198980, loss = 0.67 (1675.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:37.606434: step 198990, loss = 0.72 (1477.3 examples/sec; 0.087 sec/batch)
2017-05-05 21:39:38.277873: step 199000, loss = 0.71 (1906.3 examples/sec; 0.067 sec/batch)
2017-05-05 21:39:39.044412: step 199010, loss = 0.81 (1669.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:39:39.797858: step 199020, loss = 0.68 (1698.9 examples/sec; 0.075 sec/batch)
2017-05-05 21:39:40.563302: step 199030, loss = 0.66 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:39:41.324457: step 199040, loss = 0.86 (1681.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:42.092017: step 199050, loss = 0.53 (1667.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:39:42.859763: step 199060, loss = 0.66 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:39:43.618714: step 199070, loss = 0.73 (1686.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:44.384378: step 199080, loss = 0.70 (1671.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:39:45.153205: step 199090, loss = 0.65 (1664.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:39:45.923326: step 199100, loss = 0.83 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:39:46.685868: step 199110, loss = 0.76 (1678.6 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:47.449013: step 199120, loss = 0.76 (1677.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:48.213449: step 199130, loss = 0.65 (1674.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:48.977465: step 199140, loss = 0.75 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:49.746597: step 199150, loss = 0.67 (1664.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:39:50.512158: step 199160, loss = 0.79 (1672.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:39:51.277901: step 199170, loss = 0.73 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:39:52.034235: step 199180, loss = 0.64 (1692.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:52.812744: step 199190, loss = 0.75 (1644.2 examples/sec; 0.078 sec/batch)
2017-05-05 21:39:53.568872: step 199200, loss = 0.64 (1692.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:54.338519: step 199210, loss = 0.62 (1663.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:39:55.107748: step 199220, loss = 0.71 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:39:55.866935: step 199230, loss = 0.62 (1686.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:56.629387: step 199240, loss = 0.63 (1678.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:57.384017: step 199250, loss = 0.63 (1696.2 examples/sec; 0.075 sec/batch)
2017-05-05 21:39:58.154908: step 199260, loss = 0.50 (1660.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:39:58.919530: step 199270, loss = 0.78 (1674.0 examples/sec; 0.076 sec/batch)
2017-05-05 21:39:59.674400: step 199280, loss = 0.72 (1695.7 examples/sec; 0.075 sec/batch)
2017-05-05 21:40:00.433397: step 199290, loss = 0.61 (1686.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:40:01.197179: step 199300, loss = 0.54 (1675.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:40:01.964887: step 199310, loss = 0.67 (1667.3 examples/sec; 0.077 sec/baE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494019552.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 210 events to /tmp/cifar10_train/events.out.tfevents.1494019552.GHC33.GHC.ANDREW.CMU.EDU
tch)
2017-05-05 21:40:02.747616: step 199320, loss = 0.69 (1635.3 examples/sec; 0.078 sec/batch)
2017-05-05 21:40:03.512187: step 199330, loss = 0.65 (1674.1 examples/sec; 0.076 sec/batch)
2017-05-05 21:40:04.273995: step 199340, loss = 0.77 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:40:05.037081: step 199350, loss = 0.72 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:40:05.803010: step 199360, loss = 0.72 (1671.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:06.564597: step 199370, loss = 0.74 (1680.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:40:07.330180: step 199380, loss = 0.80 (1671.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:08.091958: step 199390, loss = 0.66 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:40:08.855445: step 199400, loss = 0.81 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:40:09.621462: step 199410, loss = 0.63 (1671.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:10.390715: step 199420, loss = 0.67 (1664.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:11.158928: step 199430, loss = 0.61 (1666.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:11.908964: step 199440, loss = 0.73 (1706.6 examples/sec; 0.075 sec/batch)
2017-05-05 21:40:12.678485: step 199450, loss = 0.77 (1663.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:13.445555: step 199460, loss = 0.66 (1668.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:14.213737: step 199470, loss = 0.80 (1666.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:14.979529: step 199480, loss = 0.55 (1671.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:15.727734: step 199490, loss = 0.62 (1710.8 examples/sec; 0.075 sec/batch)
2017-05-05 21:40:16.492815: step 199500, loss = 0.59 (1673.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:17.262953: step 199510, loss = 0.76 (1662.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:18.026982: step 199520, loss = 0.78 (1675.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:40:18.792477: step 199530, loss = 0.66 (1672.1 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:19.553263: step 199540, loss = 0.63 (1682.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:40:20.316354: step 199550, loss = 0.79 (1677.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:40:21.083565: step 199560, loss = 0.65 (1668.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:21.850024: step 199570, loss = 0.59 (1670.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:22.615759: step 199580, loss = 0.59 (1671.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:23.383183: step 199590, loss = 0.66 (1667.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:24.148146: step 199600, loss = 0.60 (1673.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:40:24.914328: step 199610, loss = 0.67 (1670.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:25.682195: step 199620, loss = 0.63 (1666.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:26.450979: step 199630, loss = 0.69 (1665.0 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:27.216335: step 199640, loss = 0.66 (1672.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:27.979829: step 199650, loss = 0.56 (1676.5 examples/sec; 0.076 sec/batch)
2017-05-05 21:40:28.741803: step 199660, loss = 0.68 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:40:29.508423: step 199670, loss = 0.75 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:30.270243: step 199680, loss = 0.65 (1680.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:40:31.030936: step 199690, loss = 0.55 (1682.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:40:31.787519: step 199700, loss = 0.64 (1691.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:40:32.553869: step 199710, loss = 0.75 (1670.3 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:33.321608: step 199720, loss = 0.59 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:34.090888: step 199730, loss = 0.76 (1663.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:34.864679: step 199740, loss = 0.67 (1654.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:35.620867: step 199750, loss = 0.77 (1692.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:40:36.389422: step 199760, loE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494019552.GHC33.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 217 events to /tmp/cifar10_train/events.out.tfevents.1494019552.GHC33.GHC.ANDREW.CMU.EDU
ss = 0.72 (1665.5 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:37.157551: step 199770, loss = 0.46 (1666.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:37.921550: step 199780, loss = 0.62 (1675.4 examples/sec; 0.076 sec/batch)
2017-05-05 21:40:38.699690: step 199790, loss = 0.64 (1644.9 examples/sec; 0.078 sec/batch)
2017-05-05 21:40:39.457395: step 199800, loss = 0.76 (1689.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:40:40.216620: step 199810, loss = 0.57 (1685.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:40:40.981784: step 199820, loss = 0.67 (1672.8 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:41.749550: step 199830, loss = 0.72 (1667.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:42.519773: step 199840, loss = 0.59 (1661.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:43.289739: step 199850, loss = 0.71 (1662.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:44.044836: step 199860, loss = 0.56 (1695.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:40:44.810317: step 199870, loss = 0.60 (1672.2 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:45.572070: step 199880, loss = 0.56 (1680.3 examples/sec; 0.076 sec/batch)
2017-05-05 21:40:46.338689: step 199890, loss = 0.63 (1669.7 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:47.109109: step 199900, loss = 0.61 (1661.4 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:47.871511: step 199910, loss = 0.71 (1678.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:40:48.640936: step 199920, loss = 0.70 (1663.6 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:49.402927: step 199930, loss = 0.67 (1679.8 examples/sec; 0.076 sec/batch)
2017-05-05 21:40:50.171261: step 199940, loss = 0.77 (1665.9 examples/sec; 0.077 sec/batch)
2017-05-05 21:40:50.930045: step 199950, loss = 0.71 (1686.9 examples/sec; 0.076 sec/batch)
2017-05-05 21:40:51.688255: step 199960, loss = 0.85 (1688.2 examples/sec; 0.076 sec/batch)
2017-05-05 21:40:52.450276: step 199970, loss = 0.88 (1679.7 examples/sec; 0.076 sec/batch)
2017-05-05 21:40:53.319264: step 199980, loss = 0.66 (1473.0 examples/sec; 0.087 sec/batch)
2017-05-05 21:40:53.984169: step 199990, loss = 0.64 (1925.1 examples/sec; 0.066 sec/batch)
--- 15303.5355899 seconds ---
