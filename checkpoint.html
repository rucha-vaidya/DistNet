<section class="main-content">
      <h2>
<a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a><b>Project Checkpoint</b></h2>

<h3> Deciding Dataset and Platform (April 11 - April 12) </h3>

<p>Dataset:
After searching about datasets we finalized the CIFAR-10 dataset for the project. We made the decision because CIFAR-10 is a image dataset on which we could use convolutional neural networks. And since CIFAR-10 has 32x32 images, the single node version train in a reasonable amount of time</p>

<p>Platform:
Before decided to use CUDA and openMPI, but since our main focus was on distributing machine learning on several nodes, we didn't want to spend a lot of time, creating different layers of the architecture. We chose to use a machine learning framework that would help us with building the network architecture. Since neither of us had experience working on any deep learning framework, we decided to chose one that had good documentation and strong userbase, and hence finalized on Tensorflow</p>
We are using the following software stack
<ul>
	<li> Python API of Tensorflow 1.0 </li>
	<li> cuDNN 5.1 </li>
	<li> CUDA 8.0 </li>
</ul>
<p> We were initially going to test our implementation on the latedays cluster, but decided to use the AWS cloud/Google cloud platform instead</p>


<h3> Deciding Architecture (April 13 - April 16)</h3>

<p>After survey of architectures to experiment on, we decided on Squeezenet as our architecture because of its small size and accuracy similar to Alexnet. Since the number of parameters were minimal, it has a higher chance training speedup due to cluster training. But we were not able to get a the baseline version running on the current version of TensorFlow. Since we were new to TensorFlow, were having to spend lot of time trying to debug the SqueezeNet starter code. To give ourselves more time to work
	on actually building a distributed algorithm, we shifted to a tried and tested Alexnet implementation. We might consider testing our distributed training model for Squeezenet if time permits </p>
	
	
<h3> Single GPU Implementation (April 17) </h3>
<p>We ran the single GPU implementation of AlexNet on GHC machine. This is our baseline to compare our distributed version. The time taken by this implementation is  2.5 hours. </p>

<h3>Designing Experiments (April 18)</h3>
	<p>We went on to design the experiments we plan to run on the AlexNet implementation</p>
	<ul>
	<li>Single Parameter Server, Synchronous updates
		
		</li>
		
	<li>Single Parameter Server, Asynchronous updates - Type 1 
		<ol>
		<li>Workers send the gradients</li>
		<li>Server applies gradients</li>
		<li>Worker gets the updated gradients</li>
		</ol></li>
	<li>Single Parameter Server, Asynchronous updates - Type 2
		<ol>
		<li>Workers send the gradients</li>
		<li>Worker gets the current version of values</li>
		<li>Server applies the gradients</li>
		</ol></li>
	<li>Multiple Parameter Server and Asynchronous updates</li>
	<li>Squeeze Net - can try model parallelism? (super stretch goal)</li>
	</ul>
<h3>Changing TensorFlow flow of control (April 19 - April 23)</h3>
	<p>To implement distributed model, we had to make the flow of control pause at the point where the gradients were computed.
		Also we had to change the code so that the updated values of gradients could be fed into the apply gradients part of TensorFlow. 
		This part took us a long time to figure out since we had to first understand how TensorFlow worked and what each part of the baseline code did. 
		We spent a lot of time trying various methods to do this. We tried using partial_run functionality but it did not seem to work right. 
		We then tried to change the computational graph so that the input to apply_gradients were placeholders. And thus output from compute_gradients would be sent after it was evaluated.
		And new values received from server would be fed into the placeholder into the apply gredients function.</p>

	
<h3>Deciding Communication and Serialization (April 24)</h3>
	<p>Communication :
We decided to use raw TCP sockets to communicate between the worker nodes and the parameter server. It was easy to implement and did not add any overhead</p>
<p>Serialization :
We experimented with JSON and Pickle as our serialization libraries. JSON turned out to be very bulky and thus not suitable after we experimented with actual data created by the worker node. We also tested some compression schemes. However it did not result in very good compression ratios. We are currently sticking to cPickle for the serialization.</p>
	
	<h3>Synchronous Implementation (April 25 - Present)</h3>
	<p>We have implemented the code for Single Parameter Server Synchronous Model and are currently debugging and testing the same.</p>
	
      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/uprasad/CubeRover">CubeRover</a> is maintained by <a href="https://github.com/uprasad">uprasad</a> & <a href="https://github.com/nikithashr">nikithashr</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

  
</section>
