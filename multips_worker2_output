I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.7335
pciBusID 0000:04:00.0
Total memory: 7.92GiB
Free memory: 1.84GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x353eff0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: 
name: Quadro K620
major: 5 minor: 0 memoryClockRate (GHz) 1.124
pciBusID 0000:03:00.0
Total memory: 1.95GiB
Free memory: 1.81GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:04:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:962] Ignoring gpu device (device: 1, name: Quadro K620, pci bus id: 0000:03:00.0) with Cuda multiprocessor count: 3. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Received variable values from ps
Connected to both PSs
2017-05-08 15:32:35.462373: step 0, loss = 4.67 (77.0 examples/sec; 1.663 sec/batch)
2017-05-08 15:32:36.518227: step 10, loss = 4.45 (1212.3 examples/sec; 0.106 sec/batch)
2017-05-08 15:32:37.803759: step 20, loss = 4.46 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:32:39.086735: step 30, loss = 4.46 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:32:40.348440: step 40, loss = 4.29 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-08 15:32:41.630332: step 50, loss = 4.33 (998.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:32:42.908280: step 60, loss = 4.14 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:32:44.188404: step 70, loss = 4.10 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 15:32:45.460223: step 80, loss = 4.15 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-08 15:32:46.755859: step 90, loss = 4.15 (987.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:32:48.137600: step 100, loss = 4.07 (926.4 examples/sec; 0.138 sec/batch)
2017-05-08 15:32:49.349684: step 110, loss = 4.12 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-08 15:32:50.643609: step 120, loss = 3.70 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:32:51.915127: step 130, loss = 3.87 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 15:32:53.201484: step 140, loss = 3.66 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:32:54.485722: step 150, loss = 3.86 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:32:55.744691: step 160, loss = 3.77 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-08 15:32:57.036814: step 170, loss = 4.32 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:32:58.313410: step 180, loss = 3.56 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:32:59.585129: step 190, loss = 3.79 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-08 15:33:00.984321: step 200, loss = 3.58 (914.8 examples/sec; 0.140 sec/batch)
2017-05-08 15:33:02.140423: step 210, loss = 3.78 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-08 15:33:03.417360: step 220, loss = 3.66 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:33:04.722935: step 230, loss = 3.52 (980.4 examples/sec; 0.131 sec/batch)
2017-05-08 15:33:06.027664: step 240, loss = 3.38 (981.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:33:07.322649: step 250, loss = 3.56 (988.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:33:08.599769: step 260, loss = 3.30 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:33:09.883628: step 270, loss = 3.27 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 15:33:11.155291: step 280, loss = 3.18 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-08 15:33:12.425198: step 290, loss = 3.58 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-08 15:33:13.819969: step 300, loss = 3.22 (917.7 examples/sec; 0.139 sec/batch)
2017-05-08 15:33:14.982567: step 310, loss = 3.31 (1101.0 examples/sec; 0.116 sec/batch)
2017-05-08 15:33:16.257438: step 320, loss = 3.06 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-08 15:33:17.531074: step 330, loss = 3.32 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-08 15:33:18.828238: step 340, loss = 3.35 (986.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:33:20.141127: step 350, loss = 3.14 (975.0 examples/sec; 0.131 sec/batch)
2017-05-08 15:33:21.442817: step 360, loss = 2.96 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:33:22.758608: step 370, loss = 3.28 (972.8 examples/sec; 0.132 sec/batch)
2017-05-08 15:33:24.055096: step 380, loss = 2.99 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:33:25.368911: step 390, loss = 2.97 (974.3 examples/sec; 0.131 sec/batch)
2017-05-08 15:33:26.768951: step 400, loss = 3.03 (914.3 examples/sec; 0.140 sec/batch)
2017-05-08 15:33:27.961531: step 410, loss = 2.96 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-08 15:33:29.258032: step 420, loss = 2.94 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:33:30.585704: step 430, loss = 2.88 (964.1 examples/sec; 0.133 sec/batch)
2017-05-08 15:33:31.884318: step 440, loss = 2.78 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:33:33.179805: step 450, loss = 3.18 (988.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:33:34.504343: step 460, loss = 2.77 (966.4 examples/sec; 0.132 sec/batch)
2017-05-08 15:33:35.791753: step 470, loss = 2.65 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:33:37.111605: step 480, loss = 2.85 (969.8 examples/sec; 0.132 sec/batch)
2017-05-08 15:33:38.400737: step 490, loss = 2.80 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:33:39.797323: step 500, loss = 2.70 (916.5 examples/sec; 0.140 sec/batch)
2017-05-08 15:33:40.959801: step 510, loss = 2.64 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-08 15:33:42.293509: step 520, loss = 2.70 (959.7 examples/sec; 0.133 sec/batch)
2017-05-08 15:33:43.606553: step 530, loss = 2.69 (974.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:33:44.916682: step 540, loss = 2.71 (977.0 examples/sec; 0.131 sec/batch)
2017-05-08 15:33:46.197979: step 550, loss = 2.53 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 15:33:47.493613: step 560, loss = 2.69 (987.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:33:48.783313: step 570, loss = 2.51 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:33:50.081381: step 580, loss = 2.42 (986.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:33:51.351103: step 590, loss = 2.54 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-08 15:33:52.742054: step 600, loss = 2.66 (920.2 examples/sec; 0.139 sec/batch)
2017-05-08 15:33:53.932116: step 610, loss = 2.43 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-08 15:33:55.237607: step 620, loss = 2.43 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:33:56.553410: step 630, loss = 2.36 (972.8 examples/sec; 0.132 sec/batch)
2017-05-08 15:33:57.868545: step 640, loss = 2.57 (973.3 examples/sec; 0.132 sec/batch)
2017-05-08 15:33:59.196135: step 650, loss = 2.39 (964.2 examples/sec; 0.133 sec/batch)
2017-05-08 15:34:00.499595: step 660, loss = 2.47 (982.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:34:01.795475: step 670, loss = 2.27 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:34:03.084746: step 680, loss = 2.38 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:34:04.378043: step 690, loss = 2.34 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:34:05.765034: step 700, loss = 2.35 (922.9 examples/sec; 0.139 sec/batch)
2017-05-08 15:34:06.959131: step 710, loss = 2.44 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-08 15:34:08.233762: step 720, loss = 2.49 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-08 15:34:09.504318: step 730, loss = 1.99 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-08 15:34:10.792587: step 740, loss = 2.19 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:34:12.073993: step 750, loss = 2.13 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 15:34:13.367267: step 760, loss = 2.22 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:34:14.659985: step 770, loss = 2.36 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:34:15.963236: step 780, loss = 2.40 (982.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:34:17.287163: step 790, loss = 2.14 (966.8 examples/sec; 0.132 sec/batch)
2017-05-08 15:34:18.680506: step 800, loss = 2.08 (918.7 examples/sec; 0.139 sec/batch)
2017-05-08 15:34:19.883624: step 810, loss = 1.89 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-08 15:34:21.187899: step 820, loss = 1.97 (981.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:34:22.482699: step 830, loss = 2.25 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:34:23.778485: step 840, loss = 2.03 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:34:25.067078: step 850, loss = 1.99 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:34:26.333303: step 860, loss = 2.10 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-08 15:34:27.629262: step 870, loss = 2.05 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:34:28.912038: step 880, loss = 1.98 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:34:30.193959: step 890, loss = 2.10 (998.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:34:31.577848: step 900, loss = 2.12 (924.9 examples/sec; 0.138 sec/batch)
2017-05-08 15:34:32.761985: step 910, loss = 2.34 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-08 15:34:34.081868: step 920, loss = 2.08 (969.8 examples/sec; 0.132 sec/batch)
2017-05-08 15:34:35.351261: step 930, loss = 2.06 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-08 15:34:36.631071: step 940, loss = 1.89 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:34:37.909078: step 950, loss = 1.95 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:34:39.216186: step 960, loss = 1.74 (979.3 examples/sec; 0.131 sec/batch)
2017-05-08 15:34:40.499523: step 970, loss = 1.73 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:34:41.770165: step 980, loss = 2.49 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-08 15:34:43.059315: step 990, loss = 1.75 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:34:44.426636: step 1000, loss = 1.81 (936.1 examples/sec; 0.137 sec/batch)
2017-05-08 15:34:45.611908: step 1010, loss = 1.97 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-08 15:34:46.906421: step 1020, loss = 1.80 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:34:48.191539: step 1030, loss = 1.92 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:34:49.463376: step 1040, loss = 1.92 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-08 15:34:50.745068: step 1050, loss = 1.80 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:34:52.034743: step 1060, loss = 1.86 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:34:53.330149: step 1070, loss = 1.85 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:34:54.620793: step 1080, loss = 1.73 (991.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:34:55.900481: step 1090, loss = 2.01 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:34:57.292699: step 1100, loss = 1.70 (919.4 examples/sec; 0.139 sec/batch)
2017-05-08 15:34:58.453347: step 1110, loss = 1.74 (1102.8 examples/sec; 0.116 sec/batch)
2017-05-08 15:34:59.737819: step 1120, loss = 1.70 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:35:01.000687: step 1130, loss = 1.75 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-08 15:35:02.319416: step 1140, loss = 1.80 (970.6 examples/sec; 0.132 sec/batch)
2017-05-08 15:35:03.604312: step 1150, loss = 2.22 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:35:04.891752: step 1160, loss = 1.72 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:35:06.183163: step 1170, loss = 1.72 (991.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:35:07.493741: step 1180, loss = 1.93 (976.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:35:08.797620: step 1190, loss = 1.53 (981.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:35:10.155474: step 1200, loss = 1.74 (942.7 examples/sec; 0.136 sec/batch)
2017-05-08 15:35:11.346496: step 1210, loss = 1.51 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-08 15:35:12.660788: step 1220, loss = 1.54 (973.9 examples/sec; 0.131 sec/batch)
2017-05-08 15:35:13.946539: step 1230, loss = 1.72 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:35:15.223870: step 1240, loss = 1.78 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:35:16.533290: step 1250, loss = 1.76 (977.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:35:17.816294: step 1260, loss = 1.63 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:35:19.097685: step 1270, loss = 1.74 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 15:35:20.365987: step 1280, loss = 1.70 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-08 15:35:21.691676: step 1290, loss = 1.56 (965.5 examples/sec; 0.133 sec/batch)
2017-05-08 15:35:23.076671: step 1300, loss = 1.69 (924.2 examples/sec; 0.138 sec/batch)
2017-05-08 15:35:24.281888: step 1310, loss = 1.56 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-08 15:35:25.557470: step 1320, loss = 1.73 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:35:26.849134: step 1330, loss = 1.47 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:35:28.134181: step 1340, loss = 1.51 (996.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:35:29.437283: step 1350, loss = 1.61 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:35:30.730262: step 1360, loss = 1.75 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:35:32.007584: step 1370, loss = 1.80 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:35:33.329471: step 1380, loss = 2.02 (968.3 examples/sec; 0.132 sec/batch)
2017-05-08 15:35:34.632980: step 1390, loss = 1.63 (982.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:35:35.993170: step 1400, loss = 1.52 (941.0 examples/sec; 0.136 sec/batch)
2017-05-08 15:35:37.186884: step 1410, loss = 1.68 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-08 15:35:38.475628: step 1420, loss = 1.40 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:35:39.741053: step 1430, loss = 1.59 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-08 15:35:41.037184: step 1440, loss = 1.49 (987.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:35:42.356777: step 1450, loss = 1.73 (970.0 examples/sec; 0.132 sec/batch)
2017-05-08 15:35:43.655595: step 1460, loss = 1.47 (985.5 examples/sec; 0.130 sec/batch)
2017-05-08 15:35:44.966167: step 1470, loss = 1.47 (976.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:35:46.251005: step 1480, loss = 1.46 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:35:47.547957: step 1490, loss = 1.42 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:35:48.931963: step 1500, loss = 1.73 (924.9 examples/sec; 0.138 sec/batch)
2017-05-08 15:35:50.118580: step 1510, loss = 1.66 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-08 15:35:51.418843: step 1520, loss = 1.55 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:35:52.715435: step 1530, loss = 1.51 (987.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:35:53.992842: step 1540, loss = 1.67 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:35:55.259578: step 1550, loss = 1.44 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-08 15:35:56.538360: step 1560, loss = 1.42 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 15:35:57.839032: step 1570, loss = 1.51 (984.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:35:59.136106: step 1580, loss = 1.44 (986.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:36:00.415063: step 1590, loss = 1.43 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:36:01.806725: step 1600, loss = 1.42 (919.8 examples/sec; 0.139 sec/batch)
2017-05-08 15:36:02.989793: step 1610, loss = 1.15 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-08 15:36:04.284564: step 1620, loss = 1.30 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:36:05.572540: step 1630, loss = 1.37 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:36:06.884562: step 1640, loss = 1.52 (975.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:36:08.166758: step 1650, loss = 1.30 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:36:09.460748: step 1660, loss = 1.52 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:36:10.738352: step 1670, loss = 1.53 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 15:36:12.036193: step 1680, loss = 1.38 (986.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:36:13.310697: step 1690, loss = 1.42 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 15:36:14.681834: step 1700, loss = 1.35 (933.5 examples/sec; 0.137 sec/batch)
2017-05-08 15:36:15.879908: step 1710, loss = 1.52 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-08 15:36:17.180390: step 1720, loss = 1.43 (984.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:36:18.476135: step 1730, loss = 1.64 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:36:19.753051: step 1740, loss = 1.49 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:36:21.023042: step 1750, loss = 1.58 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-08 15:36:22.307653: step 1760, loss = 1.24 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:36:23.585955: step 1770, loss = 1.36 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:36:24.880078: step 1780, loss = 1.35 (989.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:36:26.196921: step 1790, loss = 1.38 (972.0 examples/sec; 0.132 sec/batch)
2017-05-08 15:36:27.592528: step 1800, loss = 1.43 (917.2 examples/sec; 0.140 sec/batch)
2017-05-08 15:36:28.770956: step 1810, loss = 1.34 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-08 15:36:30.065852: step 1820, loss = 1.53 (988.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:36:31.349806: step 1830, loss = 1.21 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 15:36:32.633538: step 1840, loss = 1.18 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:36:33.913521: step 1850, loss = 1.55 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 15:36:35.206140: step 1860, loss = 1.31 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:36:36.479508: step 1870, loss = 1.32 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-08 15:36:37.767274: step 1880, loss = 1.32 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:36:39.063436: step 1890, loss = 1.40 (987.5 examples/sec; 0.130 sec/batch)
2017-05-08 15:36:40.475249: step 1900, loss = 1.44 (906.6 examples/sec; 0.141 sec/batch)
2017-05-08 15:36:41.694276: step 1910, loss = 1.32 (1050.0 examples/sec; 0.122 sec/batch)
2017-05-08 15:36:42.979596: step 1920, loss = 1.26 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:36:44.245081: step 1930, loss = 1.57 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-08 15:36:45.523372: step 1940, loss = 1.20 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:36:46.819692: step 1950, loss = 1.44 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:36:48.132313: step 1960, loss = 1.36 (975.1 examples/sec; 0.131 sec/batch)
2017-05-08 15:36:49.438493: step 1970, loss = 1.34 (980.0 examples/sec; 0.131 sec/batch)
2017-05-08 15:36:50.742613: step 1980, loss = 1.52 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 15:36:52.050765: step 1990, loss = 1.51 (978.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:36:53.465260: step 2000, loss = 1.18 (904.9 examples/sec; 0.141 sec/batch)
2017-05-08 15:36:54.677764: step 2010, loss = 1.25 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-08 15:36:55.996206: step 2020, loss = 1.39 (970.8 examples/sec; 0.132 sec/batch)
2017-05-08 15:36:57.310137: step 2030, loss = 1.20 (974.2 examples/sec; 0.131 sec/batch)
2017-05-08 15:36:58.637908: step 2040, loss = 1.43 (964.0 examples/sec; 0.133 sec/batch)
2017-05-08 15:36:59.917022: step 2050, loss = 1.38 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:37:01.214885: step 2060, loss = 1.17 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:37:02.489882: step 2070, loss = 1.21 (1003.9 examples/sec; 0.127 sec/batch)
2017-05-08 15:37:03.772410: step 2080, loss = 1.50 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 15:37:05.080170: step 2090, loss = 1.38 (978.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:37:06.466370: step 2100, loss = 1.48 (923.4 examples/sec; 0.139 sec/batch)
2017-05-08 15:37:07.644173: step 2110, loss = 1.19 (1086.8 examples/sec; 0.118 sec/batch)
2017-05-08 15:37:08.964240: step 2120, loss = 1.29 (969.7 examples/sec; 0.132 sec/batch)
2017-05-08 15:37:10.232092: step 2130, loss = 1.26 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-08 15:37:11.526463: step 2140, loss = 1.23 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:37:12.855238: step 2150, loss = 1.23 (963.3 examples/sec; 0.133 sec/batch)
2017-05-08 15:37:14.168124: step 2160, loss = 1.22 (975.0 examples/sec; 0.131 sec/batch)
2017-05-08 15:37:15.494843: step 2170, loss = 1.25 (964.8 examples/sec; 0.133 sec/batch)
2017-05-08 15:37:16.814986: step 2180, loss = 1.15 (969.6 examples/sec; 0.132 sec/batch)
2017-05-08 15:37:18.104738: step 2190, loss = 1.36 (992.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:37:19.488858: step 2200, loss = 1.44 (924.8 examples/sec; 0.138 sec/batch)
2017-05-08 15:37:20.690153: step 2210, loss = 1.18 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-08 15:37:21.992337: step 2220, loss = 1.15 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:37:23.274235: step 2230, loss = 1.40 (998.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:37:24.566349: step 2240, loss = 1.30 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:37:25.875670: step 2250, loss = 1.25 (977.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:37:27.182292: step 2260, loss = 1.21 (979.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:37:28.479413: step 2270, loss = 1.36 (986.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:37:29.795964: step 2280, loss = 1.24 (972.2 examples/sec; 0.132 sec/batch)
2017-05-08 15:37:31.094157: step 2290, loss = 1.30 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:37:32.500020: step 2300, loss = 1.22 (910.5 examples/sec; 0.141 sec/batch)
2017-05-08 15:37:33.753722: step 2310, loss = 1.34 (1021.0 examples/sec; 0.125 sec/batch)
2017-05-08 15:37:35.081360: step 2320, loss = 1.36 (964.1 examples/sec; 0.133 sec/batch)
2017-05-08 15:37:36.353549: step 2330, loss = 1.06 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-08 15:37:37.672045: step 2340, loss = 1.51 (970.8 examples/sec; 0.132 sec/batch)
2017-05-08 15:37:38.925349: step 2350, loss = 1.22 (1021.3 examples/sec; 0.125 sec/batch)
2017-05-08 15:37:40.235067: step 2360, loss = 1.33 (977.3 examples/sec; 0.131 sec/batch)
2017-05-08 15:37:41.530950: step 2370, loss = 1.22 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:37:42.831282: step 2380, loss = 1.22 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:37:44.123830: step 2390, loss = 1.17 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:37:45.502277: step 2400, loss = 1.19 (928.6 examples/sec; 0.138 sec/batch)
2017-05-08 15:37:46.678889: step 2410, loss = 1.14 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-08 15:37:47.984275: step 2420, loss = 1.52 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:37:49.257155: step 2430, loss = 1.12 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 15:37:50.566060: step 2440, loss = 1.21 (977.9 examples/sec; 0.131 sec/batch)
2017-05-08 15:37:51.829995: step 2450, loss = 1.12 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-08 15:37:53.177887: step 2460, loss = 1.26 (949.6 examples/sec; 0.135 sec/batch)
2017-05-08 15:37:54.497247: step 2470, loss = 1.19 (970.2 examples/sec; 0.132 sec/batch)
2017-05-08 15:37:55.772170: step 2480, loss = 1.12 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-08 15:37:57.084537: step 2490, loss = 1.45 (975.3 examples/sec; 0.131 sec/batch)
2017-05-08 15:37:58.485036: step 2500, loss = 1.01 (914.0 examples/sec; 0.140 sec/batch)
2017-05-08 15:37:59.668177: step 2510, loss = 1.14 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-08 15:38:00.949698: step 2520, loss = 1.11 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:38:02.261682: step 2530, loss = 1.05 (975.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:38:03.557016: step 2540, loss = 1.25 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:38:04.875057: step 2550, loss = 1.11 (971.1 examples/sec; 0.132 sec/batch)
2017-05-08 15:38:06.181485: step 2560, loss = 1.24 (979.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:38:07.474938: step 2570, loss = 1.32 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:38:08.790732: step 2580, loss = 1.09 (972.8 examples/sec; 0.132 sec/batch)
2017-05-08 15:38:10.072126: step 2590, loss = 1.15 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 15:38:11.468241: step 2600, loss = 1.12 (916.8 examples/sec; 0.140 sec/batch)
2017-05-08 15:38:12.680276: step 2610, loss = 1.20 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-08 15:38:14.016069: step 2620, loss = 1.12 (958.2 examples/sec; 0.134 sec/batch)
2017-05-08 15:38:15.313963: step 2630, loss = 1.15 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:38:16.597409: step 2640, loss = 1.23 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:38:17.889545: step 2650, loss = 1.08 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:38:19.182942: step 2660, loss = 0.92 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:38:20.441075: step 2670, loss = 1.05 (1017.4 examples/sec; 0.126 sec/batch)
2017-05-08 15:38:21.737808: step 2680, loss = 1.11 (987.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:38:23.028733: step 2690, loss = 1.28 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:38:24.426193: step 2700, loss = 1.12 (915.9 examples/sec; 0.140 sec/batch)
2017-05-08 15:38:25.614727: step 2710, loss = 1.02 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-08 15:38:26.902603: step 2720, loss = 1.05 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:38:28.199357: step 2730, loss = 1.32 (987.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:38:29.494919: step 2740, loss = 1.25 (988.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:38:30.800954: step 2750, loss = 1.22 (980.1 examples/sec; 0.131 sec/batch)
2017-05-08 15:38:32.120650: step 2760, loss = 1.00 (969.9 examples/sec; 0.132 sec/batch)
2017-05-08 15:38:33.407087: step 2770, loss = 1.09 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:38:34.711961: step 2780, loss = 1.26 (980.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:38:35.999211: step 2790, loss = 1.27 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:38:37.389432: step 2800, loss = 1.00 (920.7 examples/sec; 0.139 sec/batch)
2017-05-08 15:38:38.579690: step 2810, loss = 1.14 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-08 15:38:39.872353: step 2820, loss = 1.08 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:38:41.161277: step 2830, loss = 1.34 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:38:42.460039: step 2840, loss = 1.48 (985.5 examples/sec; 0.130 sec/batch)
2017-05-08 15:38:43.727920: step 2850, loss = 1.12 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-08 15:38:45.016270: step 2860, loss = 1.20 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:38:46.314074: step 2870, loss = 1.04 (986.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:38:47.588732: step 2880, loss = 1.03 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-08 15:38:48.902814: step 2890, loss = 1.25 (974.1 examples/sec; 0.131 sec/batch)
2017-05-08 15:38:50.323617: step 2900, loss = 0.95 (900.9 examples/sec; 0.142 sec/batch)
2017-05-08 15:38:51.541087: step 2910, loss = 1.07 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-08 15:38:52.842137: step 2920, loss = 1.08 (983.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:38:54.123881: step 2930, loss = 1.23 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:38:55.439555: step 2940, loss = 1.15 (972.9 examples/sec; 0.132 sec/batch)
2017-05-08 15:38:56.727319: step 2950, loss = 1.17 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:38:58.022710: step 2960, loss = 1.17 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:38:59.302906: step 2970, loss = 1.10 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:39:00.590815: step 2980, loss = 1.13 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:39:01.891733: step 2990, loss = 0.98 (983.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:39:03.258419: step 3000, loss = 1.11 (936.6 examples/sec; 0.137 sec/batch)
2017-05-08 15:39:04.444781: step 3010, loss = 1.12 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-08 15:39:05.770093: step 3020, loss = 1.26 (965.8 examples/sec; 0.133 sec/batch)
2017-05-08 15:39:07.057005: step 3030, loss = 1.17 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:39:08.371749: step 3040, loss = 1.02 (973.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:39:09.645209: step 3050, loss = 0.95 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 15:39:10.923287: step 3060, loss = 1.08 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:39:12.218362: step 3070, loss = 1.27 (988.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:39:13.489337: step 3080, loss = 1.01 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 15:39:14.759063: step 3090, loss = 1.11 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-08 15:39:16.144257: step 3100, loss = 1.04 (924.1 examples/sec; 0.139 sec/batch)
2017-05-08 15:39:17.355568: step 3110, loss = 1.16 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-08 15:39:18.658542: step 3120, loss = 1.28 (982.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:39:19.927435: step 3130, loss = 1.07 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-08 15:39:21.228557: step 3140, loss = 1.10 (983.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:39:22.538667: step 3150, loss = 0.87 (977.0 examples/sec; 0.131 sec/batch)
2017-05-08 15:39:23.830200: step 3160, loss = 1.22 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:39:25.157191: step 3170, loss = 1.09 (964.6 examples/sec; 0.133 sec/batch)
2017-05-08 15:39:26.443513: step 3180, loss = 1.23 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:39:27.730958: step 3190, loss = 1.12 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:39:29.143191: step 3200, loss = 0.90 (906.4 examples/sec; 0.141 sec/batch)
2017-05-08 15:39:30.349709: step 3210, loss = 1.20 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-08 15:39:31.668187: step 3220, loss = 1.29 (970.8 examples/sec; 0.132 sec/batch)
2017-05-08 15:39:32.998165: step 3230, loss = 1.06 (962.4 examples/sec; 0.133 sec/batch)
2017-05-08 15:39:34.280864: step 3240, loss = 1.05 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 15:39:35.583663: step 3250, loss = 1.13 (982.5 examples/sec; 0.130 sec/batch)
2017-05-08 15:39:36.858587: step 3260, loss = 1.13 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-08 15:39:38.154259: step 3270, loss = 1.05 (987.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:39:39.472255: step 3280, loss = 1.18 (971.2 examples/sec; 0.132 sec/batch)
2017-05-08 15:39:40.779853: step 3290, loss = 1.20 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 15:39:42.189012: step 3300, loss = 1.02 (908.3 examples/sec; 0.141 sec/batch)
2017-05-08 15:39:43.379388: step 3310, loss = 0.95 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-08 15:39:44.701004: step 3320, loss = 1.49 (968.5 examples/sec; 0.132 sec/batch)
2017-05-08 15:39:46.004081: step 3330, loss = 1.14 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:39:47.298090: step 3340, loss = 1.03 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:39:48.598610: step 3350, loss = 1.00 (984.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:39:49.910670: step 3360, loss = 1.23 (975.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:39:51.227125: step 3370, loss = 1.05 (972.3 examples/sec; 0.132 sec/batch)
2017-05-08 15:39:52.544158: step 3380, loss = 1.25 (971.9 examples/sec; 0.132 sec/batch)
2017-05-08 15:39:53.853929: step 3390, loss = 1.17 (977.3 examples/sec; 0.131 sec/batch)
2017-05-08 15:39:55.249372: step 3400, loss = 1.04 (917.3 examples/sec; 0.140 sec/batch)
2017-05-08 15:39:56.436959: step 3410, loss = 1.43 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-08 15:39:57.765723: step 3420, loss = 1.07 (963.3 examples/sec; 0.133 sec/batch)
2017-05-08 15:39:59.082184: step 3430, loss = 1.14 (972.3 examples/sec; 0.132 sec/batch)
2017-05-08 15:40:00.396823: step 3440, loss = 1.19 (973.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:40:01.726492: step 3450, loss = 1.03 (962.6 examples/sec; 0.133 sec/batch)
2017-05-08 15:40:03.039635: step 3460, loss = 1.12 (974.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:40:04.352995: step 3470, loss = 1.07 (974.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:40:05.654884: step 3480, loss = 1.37 (983.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:40:06.950302: step 3490, loss = 1.03 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:40:08.344494: step 3500, loss = 0.98 (918.1 examples/sec; 0.139 sec/batch)
2017-05-08 15:40:09.541907: step 3510, loss = 1.15 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-08 15:40:10.877685: step 3520, loss = 0.97 (958.2 examples/sec; 0.134 sec/batch)
2017-05-08 15:40:12.163631: step 3530, loss = 1.40 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:40:13.467242: step 3540, loss = 1.20 (981.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:40:14.755528: step 3550, loss = 0.96 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:40:16.034923: step 3560, loss = 1.17 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:40:17.337524: step 3570, loss = 1.07 (982.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:40:18.603888: step 3580, loss = 1.03 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-08 15:40:19.911422: step 3590, loss = 1.03 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 15:40:21.333198: step 3600, loss = 1.09 (900.3 examples/sec; 0.142 sec/batch)
2017-05-08 15:40:22.516344: step 3610, loss = 1.05 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-08 15:40:23.809699: step 3620, loss = 0.95 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:40:25.085736: step 3630, loss = 1.18 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:40:26.388573: step 3640, loss = 1.16 (982.5 examples/sec; 0.130 sec/batch)
2017-05-08 15:40:27.676389: step 3650, loss = 1.19 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:40:28.999019: step 3660, loss = 1.17 (967.8 examples/sec; 0.132 sec/batch)
2017-05-08 15:40:30.303798: step 3670, loss = 1.16 (981.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:40:31.609220: step 3680, loss = 1.29 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:40:32.875639: step 3690, loss = 0.79 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-08 15:40:34.273237: step 3700, loss = 1.07 (915.9 examples/sec; 0.140 sec/batch)
2017-05-08 15:40:35.453913: step 3710, loss = 1.12 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-08 15:40:36.740878: step 3720, loss = 1.18 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:40:38.045194: step 3730, loss = 1.15 (981.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:40:39.363944: step 3740, loss = 1.03 (970.6 examples/sec; 0.132 sec/batch)
2017-05-08 15:40:40.654949: step 3750, loss = 1.07 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:40:41.975937: step 3760, loss = 1.06 (969.0 examples/sec; 0.132 sec/batch)
2017-05-08 15:40:43.278107: step 3770, loss = 1.19 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:40:44.585359: step 3780, loss = 1.22 (979.2 examples/sec; 0.131 sec/batch)
2017-05-08 15:40:45.901250: step 3790, loss = 1.18 (972.7 examples/sec; 0.132 sec/batch)
2017-05-08 15:40:47.309305: step 3800, loss = 0.96 (909.1 examples/sec; 0.141 sec/batch)
2017-05-08 15:40:48.498565: step 3810, loss = 1.19 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-08 15:40:49.805040: step 3820, loss = 1.21 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:40:51.086111: step 3830, loss = 0.96 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:40:52.364571: step 3840, loss = 1.09 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:40:53.669604: step 3850, loss = 1.04 (980.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:40:54.982312: step 3860, loss = 1.21 (975.1 examples/sec; 0.131 sec/batch)
2017-05-08 15:40:56.287178: step 3870, loss = 1.10 (980.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:40:57.612775: step 3880, loss = 1.20 (965.6 examples/sec; 0.133 sec/batch)
2017-05-08 15:40:58.896424: step 3890, loss = 1.04 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:41:00.290370: step 3900, loss = 1.01 (918.3 examples/sec; 0.139 sec/batch)
2017-05-08 15:41:01.480615: step 3910, loss = 1.03 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-08 15:41:02.772345: step 3920, loss = 1.15 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:41:04.045913: step 3930, loss = 1.22 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 15:41:05.326583: step 3940, loss = 0.89 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:41:06.613229: step 3950, loss = 0.89 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:41:07.942380: step 3960, loss = 1.09 (963.0 examples/sec; 0.133 sec/batch)
2017-05-08 15:41:09.245513: step 3970, loss = 1.13 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:41:10.542022: step 3980, loss = 1.08 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:41:11.821541: step 3990, loss = 1.53 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:41:13.208061: step 4000, loss = 1.36 (923.2 examples/sec; 0.139 sec/batch)
2017-05-08 15:41:14.433359: step 4010, loss = 1.05 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-08 15:41:15.740676: step 4020, loss = 1.16 (979.1 examples/sec; 0.131 sec/batch)
2017-05-08 15:41:17.066522: step 4030, loss = 1.14 (965.4 examples/sec; 0.133 sec/batch)
2017-05-08 15:41:18.375244: step 4040, loss = 1.06 (978.1 examples/sec; 0.131 sec/batch)
2017-05-08 15:41:19.681425: step 4050, loss = 0.98 (980.0 examples/sec; 0.131 sec/batch)
2017-05-08 15:41:20.969844: step 4060, loss = 0.93 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:41:22.255619: step 4070, loss = 0.98 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:41:23.544687: step 4080, loss = 0.81 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:41:24.823116: step 4090, loss = 0.96 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:41:26.212924: step 4100, loss = 0.99 (921.0 examples/sec; 0.139 sec/batch)
2017-05-08 15:41:27.398213: step 4110, loss = 0.93 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-08 15:41:28.698563: step 4120, loss = 1.15 (984.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:41:30.008307: step 4130, loss = 1.17 (977.3 examples/sec; 0.131 sec/batch)
2017-05-08 15:41:31.302934: step 4140, loss = 1.18 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:41:32.602214: step 4150, loss = 1.24 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:41:33.915763: step 4160, loss = 0.96 (974.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:41:35.224066: step 4170, loss = 1.22 (978.4 examples/sec; 0.131 sec/batch)
2017-05-08 15:41:36.530928: step 4180, loss = 1.12 (979.4 examples/sec; 0.131 sec/batch)
2017-05-08 15:41:37.856139: step 4190, loss = 1.19 (965.9 examples/sec; 0.133 sec/batch)
2017-05-08 15:41:39.239306: step 4200, loss = 1.00 (925.4 examples/sec; 0.138 sec/batch)
2017-05-08 15:41:40.439223: step 4210, loss = 0.92 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-08 15:41:41.714613: step 4220, loss = 1.01 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:41:42.988790: step 4230, loss = 1.11 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 15:41:44.266103: step 4240, loss = 0.93 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:41:45.555857: step 4250, loss = 1.09 (992.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:41:46.850671: step 4260, loss = 0.99 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:41:48.147061: step 4270, loss = 0.96 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:41:49.453104: step 4280, loss = 1.28 (980.1 examples/sec; 0.131 sec/batch)
2017-05-08 15:41:50.763540: step 4290, loss = 1.07 (976.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:41:52.159639: step 4300, loss = 1.07 (916.8 examples/sec; 0.140 sec/batch)
2017-05-08 15:41:53.389405: step 4310, loss = 1.20 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-08 15:41:54.683442: step 4320, loss = 1.03 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:41:55.979173: step 4330, loss = 1.00 (987.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:41:57.261996: step 4340, loss = 1.13 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:41:58.542884: step 4350, loss = 0.86 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:41:59.810367: step 4360, loss = 1.12 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-08 15:42:01.107176: step 4370, loss = 1.05 (987.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:42:02.378887: step 4380, loss = 1.20 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-08 15:42:03.660698: step 4390, loss = 0.99 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:42:05.063858: step 4400, loss = 1.12 (912.2 examples/sec; 0.140 sec/batch)
2017-05-08 15:42:06.258195: step 4410, loss = 0.99 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-08 15:42:07.534171: step 4420, loss = 1.00 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:42:08.819056: step 4430, loss = 0.92 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:42:10.096303: step 4440, loss = 1.05 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:42:11.385492: step 4450, loss = 1.22 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:42:12.676906: step 4460, loss = 0.85 (991.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:42:13.986423: step 4470, loss = 1.21 (977.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:42:15.289015: step 4480, loss = 0.99 (982.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:42:16.576730: step 4490, loss = 1.06 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:42:17.966397: step 4500, loss = 0.99 (921.1 examples/sec; 0.139 sec/batch)
2017-05-08 15:42:19.200039: step 4510, loss = 1.15 (1037.6 examples/sec; 0.123 sec/batch)
2017-05-08 15:42:20.494182: step 4520, loss = 1.28 (989.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:42:21.779822: step 4530, loss = 1.06 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:42:23.084944: step 4540, loss = 1.16 (980.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:42:24.363247: step 4550, loss = 0.92 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:42:25.660619: step 4560, loss = 1.04 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:42:26.978431: step 4570, loss = 1.17 (971.3 examples/sec; 0.132 sec/batch)
2017-05-08 15:42:28.295069: step 4580, loss = 0.85 (972.2 examples/sec; 0.132 sec/batch)
2017-05-08 15:42:29.579684: step 4590, loss = 1.31 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:42:31.002392: step 4600, loss = 1.18 (899.7 examples/sec; 0.142 sec/batch)
2017-05-08 15:42:32.199957: step 4610, loss = 0.91 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-08 15:42:33.479639: step 4620, loss = 1.07 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:42:34.757431: step 4630, loss = 0.95 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:42:36.073624: step 4640, loss = 1.30 (972.5 examples/sec; 0.132 sec/batch)
2017-05-08 15:42:37.373864: step 4650, loss = 1.12 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:42:38.679937: step 4660, loss = 1.00 (980.0 examples/sec; 0.131 sec/batch)
2017-05-08 15:42:40.000823: step 4670, loss = 1.09 (969.1 examples/sec; 0.132 sec/batch)
2017-05-08 15:42:41.324425: step 4680, loss = 1.15 (967.1 examples/sec; 0.132 sec/batch)
2017-05-08 15:42:42.619881: step 4690, loss = 0.95 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:42:44.043085: step 4700, loss = 1.10 (899.4 examples/sec; 0.142 sec/batch)
2017-05-08 15:42:45.280636: step 4710, loss = 0.82 (1034.3 examples/sec; 0.124 sec/batch)
2017-05-08 15:42:46.558553: step 4720, loss = 1.09 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:42:47.833294: step 4730, loss = 0.97 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 15:42:49.142617: step 4740, loss = 1.14 (977.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:42:50.468405: step 4750, loss = 1.01 (965.5 examples/sec; 0.133 sec/batch)
2017-05-08 15:42:51.779980: step 4760, loss = 0.98 (975.9 examples/sec; 0.131 sec/batch)
2017-05-08 15:42:53.064394: step 4770, loss = 1.26 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:42:54.387780: step 4780, loss = 1.12 (967.2 examples/sec; 0.132 sec/batch)
2017-05-08 15:42:55.683774: step 4790, loss = 0.98 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:42:57.078330: step 4800, loss = 1.10 (917.9 examples/sec; 0.139 sec/batch)
2017-05-08 15:42:58.271534: step 4810, loss = 1.00 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-08 15:42:59.554710: step 4820, loss = 1.23 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:43:00.841536: step 4830, loss = 0.96 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:43:02.147735: step 4840, loss = 0.97 (980.0 examples/sec; 0.131 sec/batch)
2017-05-08 15:43:03.425879: step 4850, loss = 1.01 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:43:04.711935: step 4860, loss = 0.94 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:43:06.017967: step 4870, loss = 0.91 (980.1 examples/sec; 0.131 sec/batch)
2017-05-08 15:43:07.310716: step 4880, loss = 1.23 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:43:08.602052: step 4890, loss = 0.94 (991.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:43:10.010997: step 4900, loss = 1.13 (908.5 examples/sec; 0.141 sec/batch)
2017-05-08 15:43:11.210117: step 4910, loss = 1.01 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-08 15:43:12.499545: step 4920, loss = 1.12 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:43:13.818540: step 4930, loss = 1.14 (970.4 examples/sec; 0.132 sec/batch)
2017-05-08 15:43:15.107022: step 4940, loss = 1.15 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:43:16.412106: step 4950, loss = 0.98 (980.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:43:17.723292: step 4960, loss = 1.10 (976.2 examples/sec; 0.131 sec/batch)
2017-05-08 15:43:18.997744: step 4970, loss = 0.96 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-08 15:43:20.281077: step 4980, loss = 1.43 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:43:21.563822: step 4990, loss = 1.21 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 15:43:22.959877: step 5000, loss = 1.06 (916.9 examples/sec; 0.140 sec/batch)
2017-05-08 15:43:24.125421: step 5010, loss = 0.94 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-08 15:43:25.429125: step 5020, loss = 1.15 (981.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:43:26.774786: step 5030, loss = 1.06 (951.2 examples/sec; 0.135 sec/batch)
2017-05-08 15:43:28.075669: step 5040, loss = 1.07 (983.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:43:29.389039: step 5050, loss = 1.20 (974.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:43:30.699561: step 5060, loss = 1.05 (976.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:43:31.987931: step 5070, loss = 0.97 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:43:33.288511: step 5080, loss = 1.07 (984.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:43:34.581358: step 5090, loss = 1.04 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:43:35.982382: step 5100, loss = 1.04 (913.6 examples/sec; 0.140 sec/batch)
2017-05-08 15:43:37.180139: step 5110, loss = 1.02 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-08 15:43:38.454536: step 5120, loss = 0.96 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-08 15:43:39.744083: step 5130, loss = 0.88 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:43:41.010585: step 5140, loss = 1.07 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-08 15:43:42.293268: step 5150, loss = 1.21 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 15:43:43.587040: step 5160, loss = 1.04 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:43:44.886340: step 5170, loss = 1.08 (985.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:43:46.177338: step 5180, loss = 1.03 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:43:47.498182: step 5190, loss = 1.07 (969.1 examples/sec; 0.132 sec/batch)
2017-05-08 15:43:48.866860: step 5200, loss = 0.89 (935.2 examples/sec; 0.137 sec/batch)
2017-05-08 15:43:50.079878: step 5210, loss = 0.89 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-08 15:43:51.398725: step 5220, loss = 1.14 (970.5 examples/sec; 0.132 sec/batch)
2017-05-08 15:43:52.710779: step 5230, loss = 0.95 (975.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:43:53.993110: step 5240, loss = 1.01 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:43:55.290111: step 5250, loss = 1.38 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:43:56.602848: step 5260, loss = 1.11 (975.1 examples/sec; 0.131 sec/batch)
2017-05-08 15:43:57.909841: step 5270, loss = 1.26 (979.3 examples/sec; 0.131 sec/batch)
2017-05-08 15:43:59.195753: step 5280, loss = 1.06 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:44:00.506865: step 5290, loss = 0.99 (976.3 examples/sec; 0.131 sec/batch)
2017-05-08 15:44:01.888739: step 5300, loss = 1.00 (926.3 examples/sec; 0.138 sec/batch)
2017-05-08 15:44:03.093825: step 5310, loss = 1.05 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-08 15:44:04.366991: step 5320, loss = 0.76 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 15:44:05.666036: step 5330, loss = 1.13 (985.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:44:06.943481: step 5340, loss = 1.02 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 15:44:08.223910: step 5350, loss = 0.73 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:44:09.508147: step 5360, loss = 0.83 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:44:10.818596: step 5370, loss = 1.13 (976.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:44:12.095055: step 5380, loss = 1.07 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:44:13.386613: step 5390, loss = 1.16 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:44:14.782869: step 5400, loss = 1.10 (916.7 examples/sec; 0.140 sec/batch)
2017-05-08 15:44:15.967211: step 5410, loss = 1.01 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-08 15:44:17.261564: step 5420, loss = 1.10 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:44:18.585851: step 5430, loss = 0.94 (966.6 examples/sec; 0.132 sec/batch)
2017-05-08 15:44:19.897073: step 5440, loss = 0.83 (976.2 examples/sec; 0.131 sec/batch)
2017-05-08 15:44:21.211904: step 5450, loss = 1.11 (973.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:44:22.502500: step 5460, loss = 0.97 (991.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:44:23.821113: step 5470, loss = 0.92 (970.7 examples/sec; 0.132 sec/batch)
2017-05-08 15:44:25.131633: step 5480, loss = 1.12 (976.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:44:26.408625: step 5490, loss = 1.01 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:44:27.800397: step 5500, loss = 0.96 (919.7 examples/sec; 0.139 sec/batch)
2017-05-08 15:44:29.009054: step 5510, loss = 0.96 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-08 15:44:30.306427: step 5520, loss = 1.03 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:44:31.592708: step 5530, loss = 1.19 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:44:32.888135: step 5540, loss = 1.07 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:44:34.175100: step 5550, loss = 0.98 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:44:35.499862: step 5560, loss = 1.15 (966.2 examples/sec; 0.132 sec/batch)
2017-05-08 15:44:36.793715: step 5570, loss = 1.12 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:44:38.112027: step 5580, loss = 1.08 (970.9 examples/sec; 0.132 sec/batch)
2017-05-08 15:44:39.422945: step 5590, loss = 1.09 (976.4 examples/sec; 0.131 sec/batch)
2017-05-08 15:44:40.803712: step 5600, loss = 0.92 (927.0 examples/sec; 0.138 sec/batch)
2017-05-08 15:44:42.005104: step 5610, loss = 0.92 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-08 15:44:43.275437: step 5620, loss = 1.08 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-08 15:44:44.550769: step 5630, loss = 0.79 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:44:45.840539: step 5640, loss = 0.90 (992.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:44:47.127294: step 5650, loss = 0.95 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:44:48.392097: step 5660, loss = 1.05 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-08 15:44:49.679789: step 5670, loss = 0.80 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:44:50.979556: step 5680, loss = 1.04 (984.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:44:52.283689: step 5690, loss = 0.86 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 15:44:53.664256: step 5700, loss = 1.23 (927.2 examples/sec; 0.138 sec/batch)
2017-05-08 15:44:54.849617: step 5710, loss = 0.92 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-08 15:44:56.120558: step 5720, loss = 0.84 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 15:44:57.416157: step 5730, loss = 1.16 (988.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:44:58.686053: step 5740, loss = 1.02 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-08 15:44:59.982853: step 5750, loss = 1.14 (987.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:45:01.268095: step 5760, loss = 1.03 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:45:02.600873: step 5770, loss = 1.06 (960.4 examples/sec; 0.133 sec/batch)
2017-05-08 15:45:03.911289: step 5780, loss = 1.01 (976.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:45:05.229392: step 5790, loss = 0.87 (971.1 examples/sec; 0.132 sec/batch)
2017-05-08 15:45:06.631196: step 5800, loss = 0.89 (913.1 examples/sec; 0.140 sec/batch)
2017-05-08 15:45:07.859214: step 5810, loss = 0.96 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-08 15:45:09.169274: step 5820, loss = 1.03 (977.0 examples/sec; 0.131 sec/batch)
2017-05-08 15:45:10.462805: step 5830, loss = 1.10 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:45:11.756807: step 5840, loss = 1.16 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:45:13.094423: step 5850, loss = 0.94 (956.9 examples/sec; 0.134 sec/batch)
2017-05-08 15:45:14.406152: step 5860, loss = 1.06 (975.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:45:15.759732: step 5870, loss = 0.99 (945.6 examples/sec; 0.135 sec/batch)
2017-05-08 15:45:17.093104: step 5880, loss = 0.88 (960.0 examples/sec; 0.133 sec/batch)
2017-05-08 15:45:18.397134: step 5890, loss = 0.79 (981.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:45:19.799764: step 5900, loss = 1.05 (912.6 examples/sec; 0.140 sec/batch)
2017-05-08 15:45:21.013101: step 5910, loss = 0.88 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-08 15:45:22.313989: step 5920, loss = 1.02 (984.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:45:23.589672: step 5930, loss = 1.28 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:45:24.897857: step 5940, loss = 1.14 (978.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:45:26.205008: step 5950, loss = 1.04 (979.2 examples/sec; 0.131 sec/batch)
2017-05-08 15:45:27.484283: step 5960, loss = 0.98 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:45:28.758508: step 5970, loss = 1.12 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 15:45:30.037480: step 5980, loss = 0.98 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:45:31.315370: step 5990, loss = 0.98 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:45:32.679621: step 6000, loss = 1.05 (938.2 examples/sec; 0.136 sec/batch)
2017-05-08 15:45:33.873795: step 6010, loss = 1.01 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-08 15:45:35.172896: step 6020, loss = 0.80 (985.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:45:36.464376: step 6030, loss = 1.06 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:45:37.738242: step 6040, loss = 0.95 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 15:45:39.026952: step 6050, loss = 1.10 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:45:40.314825: step 6060, loss = 0.97 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:45:41.640422: step 6070, loss = 1.07 (965.6 examples/sec; 0.133 sec/batch)
2017-05-08 15:45:42.957564: step 6080, loss = 1.09 (971.8 examples/sec; 0.132 sec/batch)
2017-05-08 15:45:44.223473: step 6090, loss = 1.11 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-08 15:45:45.593744: step 6100, loss = 0.97 (934.1 examples/sec; 0.137 sec/batch)
2017-05-08 15:45:46.774770: step 6110, loss = 1.04 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-08 15:45:48.050066: step 6120, loss = 0.95 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:45:49.348752: step 6130, loss = 0.81 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:45:50.645866: step 6140, loss = 0.94 (986.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:45:51.919163: step 6150, loss = 1.17 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 15:45:53.221702: step 6160, loss = 0.96 (982.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:45:54.505792: step 6170, loss = 1.20 (996.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:45:55.778051: step 6180, loss = 1.01 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-08 15:45:57.074138: step 6190, loss = 0.88 (987.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:45:58.467904: step 6200, loss = 0.91 (918.4 examples/sec; 0.139 sec/batch)
2017-05-08 15:45:59.633388: step 6210, loss = 1.16 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-08 15:46:00.911166: step 6220, loss = 0.90 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:46:02.216883: step 6230, loss = 1.09 (980.3 examples/sec; 0.131 sec/batch)
2017-05-08 15:46:03.500478: step 6240, loss = 1.04 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:46:04.787987: step 6250, loss = 0.91 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:46:06.085255: step 6260, loss = 1.05 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:46:07.370693: step 6270, loss = 0.84 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:46:08.671600: step 6280, loss = 0.97 (983.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:46:09.971917: step 6290, loss = 0.84 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:46:11.358579: step 6300, loss = 1.10 (923.1 examples/sec; 0.139 sec/batch)
2017-05-08 15:46:12.550182: step 6310, loss = 1.01 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-08 15:46:13.851002: step 6320, loss = 1.10 (984.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:46:15.130546: step 6330, loss = 0.83 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:46:16.425723: step 6340, loss = 1.01 (988.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:46:17.756138: step 6350, loss = 1.32 (962.1 examples/sec; 0.133 sec/batch)
2017-05-08 15:46:19.056104: step 6360, loss = 1.27 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:46:20.388510: step 6370, loss = 1.08 (960.7 examples/sec; 0.133 sec/batch)
2017-05-08 15:46:21.679492: step 6380, loss = 1.22 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:46:22.987911: step 6390, loss = 1.05 (978.2 examples/sec; 0.131 sec/batch)
2017-05-08 15:46:24.361534: step 6400, loss = 0.96 (931.8 examples/sec; 0.137 sec/batch)
2017-05-08 15:46:25.542683: step 6410, loss = 1.01 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-08 15:46:26.814617: step 6420, loss = 0.93 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-08 15:46:28.097824: step 6430, loss = 0.81 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:46:29.388862: step 6440, loss = 0.86 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:46:30.701321: step 6450, loss = 0.94 (975.3 examples/sec; 0.131 sec/batch)
2017-05-08 15:46:31.995280: step 6460, loss = 1.04 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:46:33.303087: step 6470, loss = 0.87 (978.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:46:34.630917: step 6480, loss = 0.91 (964.0 examples/sec; 0.133 sec/batch)
2017-05-08 15:46:35.947969: step 6490, loss = 0.96 (971.9 examples/sec; 0.132 sec/batch)
2017-05-08 15:46:37.338985: step 6500, loss = 0.88 (920.2 examples/sec; 0.139 sec/batch)
2017-05-08 15:46:38.602181: step 6510, loss = 1.21 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-08 15:46:39.898147: step 6520, loss = 1.01 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:46:41.213596: step 6530, loss = 0.92 (973.0 examples/sec; 0.132 sec/batch)
2017-05-08 15:46:42.485123: step 6540, loss = 1.07 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 15:46:43.789299: step 6550, loss = 0.84 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 15:46:45.094314: step 6560, loss = 1.10 (980.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:46:46.403207: step 6570, loss = 1.02 (977.9 examples/sec; 0.131 sec/batch)
2017-05-08 15:46:47.671901: step 6580, loss = 1.00 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 15:46:48.981782: step 6590, loss = 1.20 (977.2 examples/sec; 0.131 sec/batch)
2017-05-08 15:46:50.358248: step 6600, loss = 1.00 (929.9 examples/sec; 0.138 sec/batch)
2017-05-08 15:46:51.539566: step 6610, loss = 1.02 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-08 15:46:52.813348: step 6620, loss = 0.69 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-08 15:46:54.108223: step 6630, loss = 0.84 (988.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:46:55.391637: step 6640, loss = 0.89 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:46:56.687503: step 6650, loss = 0.91 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:46:58.011665: step 6660, loss = 1.01 (966.7 examples/sec; 0.132 sec/batch)
2017-05-08 15:46:59.339318: step 6670, loss = 1.11 (964.1 examples/sec; 0.133 sec/batch)
2017-05-08 15:47:00.659993: step 6680, loss = 1.09 (969.2 examples/sec; 0.132 sec/batch)
2017-05-08 15:47:01.927691: step 6690, loss = 1.03 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 15:47:03.300822: step 6700, loss = 0.95 (932.2 examples/sec; 0.137 sec/batch)
2017-05-08 15:47:04.491864: step 6710, loss = 1.07 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-08 15:47:05.762278: step 6720, loss = 0.97 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 15:47:07.042050: step 6730, loss = 0.86 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:47:08.327916: step 6740, loss = 1.12 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:47:09.609508: step 6750, loss = 1.29 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:47:10.901251: step 6760, loss = 0.99 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:47:12.222755: step 6770, loss = 0.96 (968.6 examples/sec; 0.132 sec/batch)
2017-05-08 15:47:13.530330: step 6780, loss = 0.91 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 15:47:14.852074: step 6790, loss = 0.97 (968.4 examples/sec; 0.132 sec/batch)
2017-05-08 15:47:16.229521: step 6800, loss = 0.91 (929.2 examples/sec; 0.138 sec/batch)
2017-05-08 15:47:17.468178: step 6810, loss = 0.86 (1033.4 examples/sec; 0.124 sec/batch)
2017-05-08 15:47:18.781346: step 6820, loss = 1.05 (974.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:47:20.037348: step 6830, loss = 1.07 (1019.1 examples/sec; 0.126 sec/batch)
2017-05-08 15:47:21.326024: step 6840, loss = 0.92 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:47:22.651016: step 6850, loss = 1.20 (966.1 examples/sec; 0.132 sec/batch)
2017-05-08 15:47:23.942665: step 6860, loss = 1.01 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:47:25.237760: step 6870, loss = 1.04 (988.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:47:26.546898: step 6880, loss = 0.85 (977.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:47:27.832403: step 6890, loss = 0.78 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:47:29.234574: step 6900, loss = 0.94 (912.9 examples/sec; 0.140 sec/batch)
2017-05-08 15:47:30.447090: step 6910, loss = 0.96 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-08 15:47:31.748094: step 6920, loss = 0.77 (983.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:47:33.078712: step 6930, loss = 1.07 (962.0 examples/sec; 0.133 sec/batch)
2017-05-08 15:47:34.385628: step 6940, loss = 1.08 (979.4 examples/sec; 0.131 sec/batch)
2017-05-08 15:47:35.699988: step 6950, loss = 1.07 (973.9 examples/sec; 0.131 sec/batch)
2017-05-08 15:47:37.013088: step 6960, loss = 0.88 (974.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:47:38.355909: step 6970, loss = 0.84 (953.2 examples/sec; 0.134 sec/batch)
2017-05-08 15:47:39.700215: step 6980, loss = 0.92 (952.2 examples/sec; 0.134 sec/batch)
2017-05-08 15:47:41.007186: step 6990, loss = 0.80 (979.4 examples/sec; 0.131 sec/batch)
2017-05-08 15:47:42.415507: step 7000, loss = 0.99 (908.9 examples/sec; 0.141 sec/batch)
2017-05-08 15:47:43.643844: step 7010, loss = 0.89 (1042.1 examples/sec; 0.123 sec/batch)
2017-05-08 15:47:44.939904: step 7020, loss = 0.93 (987.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:47:46.250132: step 7030, loss = 1.00 (976.9 examples/sec; 0.131 sec/batch)
2017-05-08 15:47:47.539561: step 7040, loss = 1.10 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:47:48.832579: step 7050, loss = 1.09 (989.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:47:50.141829: step 7060, loss = 1.05 (977.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:47:51.442934: step 7070, loss = 0.78 (983.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:47:52.751383: step 7080, loss = 0.93 (978.2 examples/sec; 0.131 sec/batch)
2017-05-08 15:47:54.044600: step 7090, loss = 1.04 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:47:55.445903: step 7100, loss = 1.00 (913.4 examples/sec; 0.140 sec/batch)
2017-05-08 15:47:56.681663: step 7110, loss = 1.02 (1035.8 examples/sec; 0.124 sec/batch)
2017-05-08 15:47:58.016504: step 7120, loss = 0.85 (959.0 examples/sec; 0.133 sec/batch)
2017-05-08 15:47:59.343594: step 7130, loss = 1.03 (964.5 examples/sec; 0.133 sec/batch)
2017-05-08 15:48:00.640615: step 7140, loss = 0.81 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:48:01.922651: step 7150, loss = 0.94 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:48:03.214324: step 7160, loss = 0.92 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:48:04.509866: step 7170, loss = 0.93 (988.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:48:05.792985: step 7180, loss = 0.85 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:48:07.072470: step 7190, loss = 0.72 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:48:08.468018: step 7200, loss = 0.95 (917.2 examples/sec; 0.140 sec/batch)
2017-05-08 15:48:09.685037: step 7210, loss = 1.10 (1051.9 examples/sec; 0.122 sec/batch)
2017-05-08 15:48:10.986904: step 7220, loss = 1.02 (983.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:48:12.278171: step 7230, loss = 1.02 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:48:13.599630: step 7240, loss = 0.90 (968.6 examples/sec; 0.132 sec/batch)
2017-05-08 15:48:14.903285: step 7250, loss = 1.05 (981.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:48:16.191892: step 7260, loss = 0.90 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:48:17.514634: step 7270, loss = 1.00 (967.7 examples/sec; 0.132 sec/batch)
2017-05-08 15:48:18.803704: step 7280, loss = 1.15 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:48:20.085217: step 7290, loss = 0.91 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:48:21.482637: step 7300, loss = 1.04 (916.0 examples/sec; 0.140 sec/batch)
2017-05-08 15:48:22.693630: step 7310, loss = 1.09 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-08 15:48:23.975729: step 7320, loss = 1.04 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:48:25.283195: step 7330, loss = 1.23 (979.0 examples/sec; 0.131 sec/batch)
2017-05-08 15:48:26.579344: step 7340, loss = 0.78 (987.5 examples/sec; 0.130 sec/batch)
2017-05-08 15:48:27.875494: step 7350, loss = 1.02 (987.5 examples/sec; 0.130 sec/batch)
2017-05-08 15:48:29.208372: step 7360, loss = 0.90 (960.3 examples/sec; 0.133 sec/batch)
2017-05-08 15:48:30.522327: step 7370, loss = 0.82 (974.1 examples/sec; 0.131 sec/batch)
2017-05-08 15:48:31.843871: step 7380, loss = 0.82 (968.6 examples/sec; 0.132 sec/batch)
2017-05-08 15:48:33.158541: step 7390, loss = 0.78 (973.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:48:34.573533: step 7400, loss = 0.88 (904.6 examples/sec; 0.141 sec/batch)
2017-05-08 15:48:35.770883: step 7410, loss = 0.92 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-08 15:48:37.073814: step 7420, loss = 1.07 (982.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:48:38.379152: step 7430, loss = 0.90 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:48:39.666468: step 7440, loss = 1.12 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:48:40.965736: step 7450, loss = 0.87 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:48:42.275365: step 7460, loss = 1.12 (977.4 examples/sec; 0.131 sec/batch)
2017-05-08 15:48:43.575254: step 7470, loss = 1.10 (984.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:48:44.846040: step 7480, loss = 0.81 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-08 15:48:46.126223: step 7490, loss = 1.01 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 15:48:47.489563: step 7500, loss = 0.96 (938.9 examples/sec; 0.136 sec/batch)
2017-05-08 15:48:48.689611: step 7510, loss = 0.95 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-08 15:48:49.959052: step 7520, loss = 0.99 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 15:48:51.240987: step 7530, loss = 1.14 (998.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:48:52.517814: step 7540, loss = 1.02 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:48:53.816730: step 7550, loss = 1.16 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:48:55.110503: step 7560, loss = 0.85 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:48:56.394910: step 7570, loss = 1.06 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:48:57.698037: step 7580, loss = 0.89 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:48:58.988107: step 7590, loss = 0.94 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:49:00.363625: step 7600, loss = 1.37 (930.6 examples/sec; 0.138 sec/batch)
2017-05-08 15:49:01.572066: step 7610, loss = 0.85 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-08 15:49:02.856150: step 7620, loss = 0.98 (996.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:49:04.137344: step 7630, loss = 0.84 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:49:05.422207: step 7640, loss = 0.97 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:49:06.710873: step 7650, loss = 0.86 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:49:07.978801: step 7660, loss = 1.17 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-08 15:49:09.284260: step 7670, loss = 1.08 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:49:10.566077: step 7680, loss = 0.86 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:49:11.842114: step 7690, loss = 0.89 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:49:13.236815: step 7700, loss = 1.12 (917.8 examples/sec; 0.139 sec/batch)
2017-05-08 15:49:14.447335: step 7710, loss = 0.88 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-08 15:49:15.749585: step 7720, loss = 1.13 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:49:17.065714: step 7730, loss = 0.87 (972.6 examples/sec; 0.132 sec/batch)
2017-05-08 15:49:18.370190: step 7740, loss = 0.92 (981.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:49:19.659681: step 7750, loss = 0.85 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:49:20.958253: step 7760, loss = 0.94 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:49:22.272697: step 7770, loss = 0.89 (973.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:49:23.548193: step 7780, loss = 0.94 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:49:24.838729: step 7790, loss = 1.05 (991.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:49:26.278295: step 7800, loss = 1.28 (889.2 examples/sec; 0.144 sec/batch)
2017-05-08 15:49:27.500040: step 7810, loss = 0.88 (1047.7 examples/sec; 0.122 sec/batch)
2017-05-08 15:49:28.797185: step 7820, loss = 1.15 (986.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:49:30.089686: step 7830, loss = 0.85 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:49:31.377863: step 7840, loss = 1.07 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:49:32.654378: step 7850, loss = 1.08 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:49:33.953246: step 7860, loss = 0.95 (985.5 examples/sec; 0.130 sec/batch)
2017-05-08 15:49:35.271531: step 7870, loss = 1.02 (971.0 examples/sec; 0.132 sec/batch)
2017-05-08 15:49:36.575443: step 7880, loss = 0.79 (981.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:49:37.870854: step 7890, loss = 1.00 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:49:39.247332: step 7900, loss = 0.84 (929.9 examples/sec; 0.138 sec/batch)
2017-05-08 15:49:40.463007: step 7910, loss = 0.78 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-08 15:49:41.776610: step 7920, loss = 0.95 (974.4 examples/sec; 0.131 sec/batch)
2017-05-08 15:49:43.091252: step 7930, loss = 0.93 (973.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:49:44.392305: step 7940, loss = 1.03 (983.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:49:45.690047: step 7950, loss = 0.89 (986.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:49:46.964046: step 7960, loss = 1.04 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-08 15:49:48.243638: step 7970, loss = 0.79 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:49:49.535944: step 7980, loss = 0.94 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:49:50.830707: step 7990, loss = 0.88 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:49:52.176179: step 8000, loss = 0.97 (951.3 examples/sec; 0.135 sec/batch)
2017-05-08 15:49:53.368068: step 8010, loss = 1.02 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-08 15:49:54.639511: step 8020, loss = 0.98 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 15:49:55.940976: step 8030, loss = 1.23 (983.5 examples/sec; 0.130 sec/batch)
2017-05-08 15:49:57.246286: step 8040, loss = 0.91 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:49:58.541839: step 8050, loss = 1.16 (988.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:49:59.836783: step 8060, loss = 1.01 (988.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:50:01.144071: step 8070, loss = 1.03 (979.1 examples/sec; 0.131 sec/batch)
2017-05-08 15:50:02.458998: step 8080, loss = 0.95 (973.4 examples/sec; 0.131 sec/batch)
2017-05-08 15:50:03.759930: step 8090, loss = 1.19 (983.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:50:05.133502: step 8100, loss = 0.95 (931.9 examples/sec; 0.137 sec/batch)
2017-05-08 15:50:06.360146: step 8110, loss = 1.15 (1043.5 examples/sec; 0.123 sec/batch)
2017-05-08 15:50:07.685972: step 8120, loss = 0.88 (965.4 examples/sec; 0.133 sec/batch)
2017-05-08 15:50:08.980943: step 8130, loss = 0.97 (988.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:50:10.273743: step 8140, loss = 1.20 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:50:11.548236: step 8150, loss = 0.80 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 15:50:12.853642: step 8160, loss = 0.87 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:50:14.152110: step 8170, loss = 0.92 (985.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:50:15.456525: step 8180, loss = 1.22 (981.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:50:16.771778: step 8190, loss = 0.97 (973.2 examples/sec; 0.132 sec/batch)
2017-05-08 15:50:18.199083: step 8200, loss = 1.02 (896.8 examples/sec; 0.143 sec/batch)
2017-05-08 15:50:19.351909: step 8210, loss = 0.91 (1110.3 examples/sec; 0.115 sec/batch)
2017-05-08 15:50:20.627392: step 8220, loss = 0.89 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:50:21.945071: step 8230, loss = 0.94 (971.4 examples/sec; 0.132 sec/batch)
2017-05-08 15:50:23.255569: step 8240, loss = 1.11 (976.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:50:24.564911: step 8250, loss = 1.04 (977.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:50:25.850083: step 8260, loss = 0.93 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:50:27.149746: step 8270, loss = 0.91 (984.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:50:28.423322: step 8280, loss = 0.89 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-08 15:50:29.709749: step 8290, loss = 0.86 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:50:31.091543: step 8300, loss = 0.94 (926.3 examples/sec; 0.138 sec/batch)
2017-05-08 15:50:32.283929: step 8310, loss = 0.95 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-08 15:50:33.589631: step 8320, loss = 0.98 (980.3 examples/sec; 0.131 sec/batch)
2017-05-08 15:50:34.874854: step 8330, loss = 1.07 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:50:36.144580: step 8340, loss = 0.90 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-08 15:50:37.451067: step 8350, loss = 0.87 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:50:38.757339: step 8360, loss = 0.96 (979.9 examples/sec; 0.131 sec/batch)
2017-05-08 15:50:40.046815: step 8370, loss = 0.86 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:50:41.323535: step 8380, loss = 0.85 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:50:42.642972: step 8390, loss = 0.81 (970.1 examples/sec; 0.132 sec/batch)
2017-05-08 15:50:44.020602: step 8400, loss = 0.99 (929.1 examples/sec; 0.138 sec/batch)
2017-05-08 15:50:45.216824: step 8410, loss = 0.97 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-08 15:50:46.522681: step 8420, loss = 0.79 (980.2 examples/sec; 0.131 sec/batch)
2017-05-08 15:50:47.849395: step 8430, loss = 0.94 (964.8 examples/sec; 0.133 sec/batch)
2017-05-08 15:50:49.155739: step 8440, loss = 0.79 (979.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:50:50.463745: step 8450, loss = 1.14 (978.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:50:51.783020: step 8460, loss = 1.08 (970.2 examples/sec; 0.132 sec/batch)
2017-05-08 15:50:53.042090: step 8470, loss = 0.96 (1016.6 examples/sec; 0.126 sec/batch)
2017-05-08 15:50:54.359236: step 8480, loss = 0.99 (971.8 examples/sec; 0.132 sec/batch)
2017-05-08 15:50:55.661883: step 8490, loss = 1.04 (982.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:50:57.077045: step 8500, loss = 0.84 (904.5 examples/sec; 0.142 sec/batch)
2017-05-08 15:50:58.322653: step 8510, loss = 1.11 (1027.6 examples/sec; 0.125 sec/batch)
2017-05-08 15:50:59.600157: step 8520, loss = 0.89 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 15:51:00.896926: step 8530, loss = 1.03 (987.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:51:02.192558: step 8540, loss = 0.93 (987.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:51:03.460701: step 8550, loss = 1.00 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-08 15:51:04.760548: step 8560, loss = 0.84 (984.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:51:06.056637: step 8570, loss = 0.88 (987.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:51:07.376888: step 8580, loss = 1.13 (969.5 examples/sec; 0.132 sec/batch)
2017-05-08 15:51:08.659849: step 8590, loss = 1.14 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:51:10.036137: step 8600, loss = 1.02 (930.0 examples/sec; 0.138 sec/batch)
2017-05-08 15:51:11.241554: step 8610, loss = 0.96 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-08 15:51:12.538507: step 8620, loss = 0.83 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:51:13.843781: step 8630, loss = 1.28 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:51:15.152625: step 8640, loss = 0.97 (978.0 examples/sec; 0.131 sec/batch)
2017-05-08 15:51:16.433986: step 8650, loss = 0.87 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 15:51:17.750347: step 8660, loss = 0.80 (972.4 examples/sec; 0.132 sec/batch)
2017-05-08 15:51:19.045279: step 8670, loss = 1.01 (988.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:51:20.341353: step 8680, loss = 0.85 (987.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:51:21.633376: step 8690, loss = 0.89 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:51:23.011674: step 8700, loss = 0.91 (928.7 examples/sec; 0.138 sec/batch)
2017-05-08 15:51:24.215641: step 8710, loss = 0.85 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-08 15:51:25.515983: step 8720, loss = 0.89 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:51:26.813984: step 8730, loss = 0.80 (986.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:51:28.080748: step 8740, loss = 0.95 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-08 15:51:29.368826: step 8750, loss = 0.96 (993.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:51:30.667511: step 8760, loss = 0.85 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:51:31.954594: step 8770, loss = 0.84 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:51:33.243885: step 8780, loss = 0.91 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:51:34.538119: step 8790, loss = 0.98 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:51:35.902147: step 8800, loss = 0.84 (938.4 examples/sec; 0.136 sec/batch)
2017-05-08 15:51:37.092346: step 8810, loss = 0.96 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-08 15:51:38.376238: step 8820, loss = 0.97 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 15:51:39.653767: step 8830, loss = 1.29 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 15:51:40.949258: step 8840, loss = 1.00 (988.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:51:42.253667: step 8850, loss = 0.84 (981.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:51:43.562361: step 8860, loss = 1.01 (978.1 examples/sec; 0.131 sec/batch)
2017-05-08 15:51:44.875031: step 8870, loss = 1.10 (975.1 examples/sec; 0.131 sec/batch)
2017-05-08 15:51:46.187652: step 8880, loss = 0.93 (975.2 examples/sec; 0.131 sec/batch)
2017-05-08 15:51:47.451308: step 8890, loss = 0.80 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-08 15:51:48.823313: step 8900, loss = 1.11 (932.9 examples/sec; 0.137 sec/batch)
2017-05-08 15:51:50.038843: step 8910, loss = 0.98 (1053.0 examples/sec; 0.122 sec/batch)
2017-05-08 15:51:51.335828: step 8920, loss = 0.91 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:51:52.621798: step 8930, loss = 0.97 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:51:53.890177: step 8940, loss = 0.80 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-08 15:51:55.192207: step 8950, loss = 1.04 (983.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:51:56.488040: step 8960, loss = 1.02 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:51:57.794098: step 8970, loss = 0.95 (980.1 examples/sec; 0.131 sec/batch)
2017-05-08 15:51:59.129533: step 8980, loss = 0.96 (958.6 examples/sec; 0.134 sec/batch)
2017-05-08 15:52:00.414052: step 8990, loss = 0.93 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:52:01.799914: step 9000, loss = 0.86 (923.6 examples/sec; 0.139 sec/batch)
2017-05-08 15:52:03.027721: step 9010, loss = 1.07 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-08 15:52:04.306641: step 9020, loss = 0.95 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:52:05.644382: step 9030, loss = 0.87 (956.8 examples/sec; 0.134 sec/batch)
2017-05-08 15:52:06.960210: step 9040, loss = 0.99 (972.8 examples/sec; 0.132 sec/batch)
2017-05-08 15:52:08.249321: step 9050, loss = 0.93 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:52:09.593430: step 9060, loss = 1.12 (952.3 examples/sec; 0.134 sec/batch)
2017-05-08 15:52:10.889944: step 9070, loss = 0.84 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:52:12.183671: step 9080, loss = 0.81 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:52:13.476271: step 9090, loss = 1.60 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:52:14.879354: step 9100, loss = 1.25 (912.3 examples/sec; 0.140 sec/batch)
2017-05-08 15:52:16.106902: step 9110, loss = 0.77 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-08 15:52:17.401148: step 9120, loss = 1.02 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:52:18.708930: step 9130, loss = 0.85 (978.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:52:19.993713: step 9140, loss = 0.99 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:52:21.319395: step 9150, loss = 0.92 (965.5 examples/sec; 0.133 sec/batch)
2017-05-08 15:52:22.629822: step 9160, loss = 1.06 (976.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:52:23.931127: step 9170, loss = 1.03 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:52:25.229825: step 9180, loss = 1.00 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:52:26.514311: step 9190, loss = 0.93 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:52:27.896840: step 9200, loss = 0.95 (925.8 examples/sec; 0.138 sec/batch)
2017-05-08 15:52:29.082282: step 9210, loss = 0.81 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-08 15:52:30.370936: step 9220, loss = 0.80 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:52:31.674973: step 9230, loss = 0.88 (981.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:52:32.952219: step 9240, loss = 0.92 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:52:34.228142: step 9250, loss = 0.87 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:52:35.574789: step 9260, loss = 0.87 (950.5 examples/sec; 0.135 sec/batch)
2017-05-08 15:52:36.859977: step 9270, loss = 1.01 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:52:38.160311: step 9280, loss = 0.82 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:52:39.432255: step 9290, loss = 0.89 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-08 15:52:40.804735: step 9300, loss = 1.10 (932.6 examples/sec; 0.137 sec/batch)
2017-05-08 15:52:42.038779: step 9310, loss = 0.94 (1037.2 examples/sec; 0.123 sec/batch)
2017-05-08 15:52:43.324372: step 9320, loss = 1.14 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:52:44.648438: step 9330, loss = 0.88 (966.7 examples/sec; 0.132 sec/batch)
2017-05-08 15:52:45.962336: step 9340, loss = 1.16 (974.2 examples/sec; 0.131 sec/batch)
2017-05-08 15:52:47.256177: step 9350, loss = 0.98 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:52:48.544575: step 9360, loss = 0.82 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:52:49.809523: step 9370, loss = 0.96 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-08 15:52:51.129853: step 9380, loss = 1.24 (969.5 examples/sec; 0.132 sec/batch)
2017-05-08 15:52:52.421584: step 9390, loss = 0.98 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:52:53.779775: step 9400, loss = 1.05 (942.4 examples/sec; 0.136 sec/batch)
2017-05-08 15:52:54.978596: step 9410, loss = 0.97 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-08 15:52:56.261022: step 9420, loss = 0.94 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:52:57.539963: step 9430, loss = 0.76 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:52:58.838062: step 9440, loss = 0.96 (986.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:53:00.120839: step 9450, loss = 1.13 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:53:01.420882: step 9460, loss = 1.11 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:53:02.719828: step 9470, loss = 1.11 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:53:03.995590: step 9480, loss = 0.94 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:53:05.293650: step 9490, loss = 0.89 (986.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:53:06.678680: step 9500, loss = 1.03 (924.2 examples/sec; 0.139 sec/batch)
2017-05-08 15:53:07.851533: step 9510, loss = 1.11 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-08 15:53:09.125693: step 9520, loss = 0.93 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 15:53:10.419585: step 9530, loss = 0.87 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:53:11.700633: step 9540, loss = 0.88 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:53:13.001821: step 9550, loss = 1.07 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:53:14.302375: step 9560, loss = 1.03 (984.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:53:15.586911: step 9570, loss = 0.70 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:53:16.879270: step 9580, loss = 0.89 (990.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:53:18.181247: step 9590, loss = 1.02 (983.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:53:19.595333: step 9600, loss = 1.03 (905.2 examples/sec; 0.141 sec/batch)
2017-05-08 15:53:20.782235: step 9610, loss = 0.92 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-08 15:53:22.063434: step 9620, loss = 1.01 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:53:23.365260: step 9630, loss = 0.85 (983.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:53:24.673786: step 9640, loss = 0.90 (978.2 examples/sec; 0.131 sec/batch)
2017-05-08 15:53:26.003197: step 9650, loss = 0.84 (962.8 examples/sec; 0.133 sec/batch)
2017-05-08 15:53:27.298855: step 9660, loss = 1.07 (987.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:53:28.566512: step 9670, loss = 1.03 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 15:53:29.863288: step 9680, loss = 1.01 (987.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:53:31.154559: step 9690, loss = 1.00 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:53:32.543222: step 9700, loss = 0.89 (921.8 examples/sec; 0.139 sec/batch)
2017-05-08 15:53:33.757695: step 9710, loss = 0.82 (1054.0 examples/sec; 0.121 sec/batch)
2017-05-08 15:53:35.051209: step 9720, loss = 1.06 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:53:36.334878: step 9730, loss = 0.81 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:53:37.600502: step 9740, loss = 0.92 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-08 15:53:38.892154: step 9750, loss = 0.95 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:53:40.187876: step 9760, loss = 1.13 (987.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:53:41.474158: step 9770, loss = 0.79 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:53:42.773509: step 9780, loss = 1.10 (985.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:53:44.037412: step 9790, loss = 0.73 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-08 15:53:45.398673: step 9800, loss = 0.92 (940.3 examples/sec; 0.136 sec/batch)
2017-05-08 15:53:46.607421: step 9810, loss = 0.86 (1058.9 examples/sec; 0.121 sec/batch)
2017-05-08 15:53:47.890256: step 9820, loss = 1.08 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:53:49.175601: step 9830, loss = 0.87 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:53:50.463924: step 9840, loss = 1.04 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:53:51.742021: step 9850, loss = 0.93 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:53:53.038782: step 9860, loss = 0.92 (987.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:53:54.322519: step 9870, loss = 0.85 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:53:55.609730: step 9880, loss = 0.92 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:53:56.871639: step 9890, loss = 0.81 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-08 15:53:58.269728: step 9900, loss = 1.10 (915.5 examples/sec; 0.140 sec/batch)
2017-05-08 15:53:59.493507: step 9910, loss = 0.89 (1045.9 examples/sec; 0.122 sec/batch)
2017-05-08 15:54:00.780552: step 9920, loss = 1.03 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:54:02.123228: step 9930, loss = 0.98 (953.3 examples/sec; 0.134 sec/batch)
2017-05-08 15:54:03.398218: step 9940, loss = 0.93 (1003.9 examples/sec; 0.127 sec/batch)
2017-05-08 15:54:04.709093: step 9950, loss = 0.83 (976.4 examples/sec; 0.131 sec/batch)
2017-05-08 15:54:05.995493: step 9960, loss = 1.19 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:54:07.323431: step 9970, loss = 1.06 (963.9 examples/sec; 0.133 sec/batch)
2017-05-08 15:54:08.625671: step 9980, loss = 1.09 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:54:09.917264: step 9990, loss = 1.04 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:54:11.309258: step 10000, loss = 0.87 (919.5 examples/sec; 0.139 sec/batch)
2017-05-08 15:54:12.499095: step 10010, loss = 0.90 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-08 15:54:13.776481: step 10020, loss = 0.78 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:54:15.063358: step 10030, loss = 0.89 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:54:16.348645: step 10040, loss = 0.94 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:54:17.661417: step 10050, loss = 0.86 (975.0 examples/sec; 0.131 sec/batch)
2017-05-08 15:54:18.951614: step 10060, loss = 0.88 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:54:20.264791: step 10070, loss = 0.91 (974.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:54:21.562048: step 10080, loss = 1.07 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:54:22.838822: step 10090, loss = 0.89 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:54:24.217095: step 10100, loss = 1.16 (928.7 examples/sec; 0.138 sec/batch)
2017-05-08 15:54:25.393618: step 10110, loss = 0.88 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-08 15:54:26.683090: step 10120, loss = 0.84 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:54:27.991511: step 10130, loss = 0.79 (978.3 examples/sec; 0.131 sec/batch)
2017-05-08 15:54:29.314033: step 10140, loss = 0.93 (967.8 examples/sec; 0.132 sec/batch)
2017-05-08 15:54:30.629272: step 10150, loss = 0.98 (973.2 examples/sec; 0.132 sec/batch)
2017-05-08 15:54:31.893707: step 10160, loss = 1.16 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-08 15:54:33.163786: step 10170, loss = 0.94 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-08 15:54:34.465170: step 10180, loss = 0.89 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:54:35.754614: step 10190, loss = 0.79 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:54:37.145491: step 10200, loss = 0.98 (920.3 examples/sec; 0.139 sec/batch)
2017-05-08 15:54:38.340701: step 10210, loss = 0.90 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-08 15:54:39.611698: step 10220, loss = 0.86 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 15:54:40.904056: step 10230, loss = 0.75 (990.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:54:42.186986: step 10240, loss = 1.00 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:54:43.456793: step 10250, loss = 0.97 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-08 15:54:44.735704: step 10260, loss = 0.82 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 15:54:46.008423: step 10270, loss = 0.96 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-08 15:54:47.288644: step 10280, loss = 0.97 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:54:48.535607: step 10290, loss = 0.93 (1026.5 examples/sec; 0.125 sec/batch)
2017-05-08 15:54:49.948619: step 10300, loss = 1.17 (905.9 examples/sec; 0.141 sec/batch)
2017-05-08 15:54:51.166868: step 10310, loss = 1.06 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-08 15:54:52.432901: step 10320, loss = 1.00 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 15:54:53.716130: step 10330, loss = 0.94 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:54:54.989303: step 10340, loss = 0.88 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 15:54:56.263894: step 10350, loss = 1.22 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-08 15:54:57.564884: step 10360, loss = 0.88 (983.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:54:58.883322: step 10370, loss = 1.01 (970.8 examples/sec; 0.132 sec/batch)
2017-05-08 15:55:00.168327: step 10380, loss = 1.02 (996.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:55:01.475516: step 10390, loss = 0.85 (979.2 examples/sec; 0.131 sec/batch)
2017-05-08 15:55:02.925321: step 10400, loss = 0.80 (882.9 examples/sec; 0.145 sec/batch)
2017-05-08 15:55:04.099852: step 10410, loss = 1.00 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-08 15:55:05.403610: step 10420, loss = 1.19 (981.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:55:06.691589: step 10430, loss = 0.93 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:55:07.974676: step 10440, loss = 0.98 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:55:09.250814: step 10450, loss = 0.90 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 15:55:10.546440: step 10460, loss = 1.03 (987.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:55:11.821562: step 10470, loss = 0.94 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:55:13.120481: step 10480, loss = 0.67 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:55:14.424650: step 10490, loss = 0.77 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 15:55:15.795194: step 10500, loss = 0.89 (933.9 examples/sec; 0.137 sec/batch)
2017-05-08 15:55:16.990291: step 10510, loss = 0.84 (1071.0 examples/sec; 0.120 sec/batch)
2017-05-08 15:55:18.304040: step 10520, loss = 0.97 (974.3 examples/sec; 0.131 sec/batch)
2017-05-08 15:55:19.579059: step 10530, loss = 0.96 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-08 15:55:20.855126: step 10540, loss = 0.93 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:55:22.130822: step 10550, loss = 0.73 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:55:23.415515: step 10560, loss = 0.88 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:55:24.741097: step 10570, loss = 1.01 (965.6 examples/sec; 0.133 sec/batch)
2017-05-08 15:55:26.010156: step 10580, loss = 0.86 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-08 15:55:27.299007: step 10590, loss = 0.68 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:55:28.700419: step 10600, loss = 0.97 (913.4 examples/sec; 0.140 sec/batch)
2017-05-08 15:55:29.898649: step 10610, loss = 0.98 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-08 15:55:31.204927: step 10620, loss = 1.02 (979.9 examples/sec; 0.131 sec/batch)
2017-05-08 15:55:32.485263: step 10630, loss = 0.91 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:55:33.764601: step 10640, loss = 0.94 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:55:35.050919: step 10650, loss = 0.83 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:55:36.352836: step 10660, loss = 0.97 (983.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:55:37.631425: step 10670, loss = 0.90 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:55:38.914311: step 10680, loss = 0.88 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:55:40.193848: step 10690, loss = 0.95 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:55:41.572588: step 10700, loss = 0.92 (928.4 examples/sec; 0.138 sec/batch)
2017-05-08 15:55:42.770769: step 10710, loss = 0.95 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-08 15:55:44.043888: step 10720, loss = 0.87 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 15:55:45.350287: step 10730, loss = 0.91 (979.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:55:46.633955: step 10740, loss = 1.00 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:55:47.942356: step 10750, loss = 0.96 (978.3 examples/sec; 0.131 sec/batch)
2017-05-08 15:55:49.237671: step 10760, loss = 0.88 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:55:50.506326: step 10770, loss = 0.90 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 15:55:51.792625: step 10780, loss = 0.78 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:55:53.086107: step 10790, loss = 0.96 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:55:54.486349: step 10800, loss = 0.85 (914.1 examples/sec; 0.140 sec/batch)
2017-05-08 15:55:55.672569: step 10810, loss = 0.85 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-08 15:55:56.977078: step 10820, loss = 0.98 (981.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:55:58.265291: step 10830, loss = 1.00 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:55:59.547434: step 10840, loss = 0.86 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:56:00.833387: step 10850, loss = 0.93 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:56:02.146418: step 10860, loss = 0.97 (974.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:56:03.422825: step 10870, loss = 0.81 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:56:04.699693: step 10880, loss = 0.73 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:56:05.999635: step 10890, loss = 0.96 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:56:07.380768: step 10900, loss = 0.94 (926.8 examples/sec; 0.138 sec/batch)
2017-05-08 15:56:08.573981: step 10910, loss = 0.79 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-08 15:56:09.876321: step 10920, loss = 1.01 (982.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:56:11.157194: step 10930, loss = 0.97 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:56:12.441474: step 10940, loss = 1.20 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:56:13.726808: step 10950, loss = 0.86 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:56:15.022249: step 10960, loss = 1.13 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:56:16.314765: step 10970, loss = 0.89 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:56:17.623941: step 10980, loss = 0.93 (977.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:56:18.919085: step 10990, loss = 0.88 (988.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:56:20.291244: step 11000, loss = 0.90 (932.8 examples/sec; 0.137 sec/batch)
2017-05-08 15:56:21.472653: step 11010, loss = 0.94 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-08 15:56:22.787213: step 11020, loss = 1.05 (973.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:56:24.109182: step 11030, loss = 1.04 (968.2 examples/sec; 0.132 sec/batch)
2017-05-08 15:56:25.421667: step 11040, loss = 0.95 (975.3 examples/sec; 0.131 sec/batch)
2017-05-08 15:56:26.718043: step 11050, loss = 0.98 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:56:28.033335: step 11060, loss = 1.03 (973.2 examples/sec; 0.132 sec/batch)
2017-05-08 15:56:29.311030: step 11070, loss = 0.80 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:56:30.614993: step 11080, loss = 0.75 (981.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:56:31.901912: step 11090, loss = 1.11 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:56:33.290413: step 11100, loss = 0.82 (921.9 examples/sec; 0.139 sec/batch)
2017-05-08 15:56:34.492383: step 11110, loss = 0.84 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-08 15:56:35.763377: step 11120, loss = 0.95 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 15:56:37.087431: step 11130, loss = 0.88 (966.7 examples/sec; 0.132 sec/batch)
2017-05-08 15:56:38.388770: step 11140, loss = 1.17 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:56:39.697269: step 11150, loss = 0.94 (978.2 examples/sec; 0.131 sec/batch)
2017-05-08 15:56:41.001405: step 11160, loss = 0.90 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 15:56:42.298793: step 11170, loss = 1.09 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:56:43.592613: step 11180, loss = 1.05 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:56:44.886063: step 11190, loss = 0.83 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:56:46.271559: step 11200, loss = 0.92 (923.9 examples/sec; 0.139 sec/batch)
2017-05-08 15:56:47.491104: step 11210, loss = 0.90 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-08 15:56:48.758223: step 11220, loss = 0.97 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-08 15:56:50.045990: step 11230, loss = 0.93 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:56:51.328405: step 11240, loss = 1.10 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:56:52.624333: step 11250, loss = 0.82 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:56:53.911801: step 11260, loss = 0.84 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:56:55.199184: step 11270, loss = 0.89 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:56:56.480092: step 11280, loss = 0.87 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:56:57.784985: step 11290, loss = 0.94 (980.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:56:59.167960: step 11300, loss = 0.86 (925.5 examples/sec; 0.138 sec/batch)
2017-05-08 15:57:00.332951: step 11310, loss = 0.95 (1098.7 examples/sec; 0.116 sec/batch)
2017-05-08 15:57:01.622760: step 11320, loss = 0.90 (992.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:57:02.941219: step 11330, loss = 0.86 (970.8 examples/sec; 0.132 sec/batch)
2017-05-08 15:57:04.263783: step 11340, loss = 0.96 (967.8 examples/sec; 0.132 sec/batch)
2017-05-08 15:57:05.558116: step 11350, loss = 1.09 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:57:06.871986: step 11360, loss = 0.93 (974.2 examples/sec; 0.131 sec/batch)
2017-05-08 15:57:08.159095: step 11370, loss = 0.71 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:57:09.441385: step 11380, loss = 0.89 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:57:10.723684: step 11390, loss = 0.86 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:57:12.114896: step 11400, loss = 0.92 (920.1 examples/sec; 0.139 sec/batch)
2017-05-08 15:57:13.311428: step 11410, loss = 1.24 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-08 15:57:14.629848: step 11420, loss = 0.93 (970.9 examples/sec; 0.132 sec/batch)
2017-05-08 15:57:15.929074: step 11430, loss = 0.68 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:57:17.213384: step 11440, loss = 1.06 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:57:18.501957: step 11450, loss = 0.81 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:57:19.823893: step 11460, loss = 0.99 (968.3 examples/sec; 0.132 sec/batch)
2017-05-08 15:57:21.126174: step 11470, loss = 1.06 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:57:22.425773: step 11480, loss = 0.80 (984.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:57:23.746377: step 11490, loss = 1.00 (969.3 examples/sec; 0.132 sec/batch)
2017-05-08 15:57:25.134048: step 11500, loss = 0.96 (922.4 examples/sec; 0.139 sec/batch)
2017-05-08 15:57:26.362703: step 11510, loss = 0.94 (1041.8 examples/sec; 0.123 sec/batch)
2017-05-08 15:57:27.658294: step 11520, loss = 1.04 (988.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:57:28.974103: step 11530, loss = 1.06 (972.8 examples/sec; 0.132 sec/batch)
2017-05-08 15:57:30.302560: step 11540, loss = 0.76 (963.5 examples/sec; 0.133 sec/batch)
2017-05-08 15:57:31.584964: step 11550, loss = 0.80 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:57:32.857213: step 11560, loss = 0.79 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-08 15:57:34.138760: step 11570, loss = 0.79 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:57:35.423061: step 11580, loss = 0.98 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:57:36.718378: step 11590, loss = 0.72 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:57:38.100723: step 11600, loss = 0.87 (926.0 examples/sec; 0.138 sec/batch)
2017-05-08 15:57:39.310878: step 11610, loss = 0.94 (1057.7 examples/sec; 0.121 sec/batch)
2017-05-08 15:57:40.603339: step 11620, loss = 0.79 (990.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:57:41.943640: step 11630, loss = 0.96 (955.0 examples/sec; 0.134 sec/batch)
2017-05-08 15:57:43.239856: step 11640, loss = 1.01 (987.5 examples/sec; 0.130 sec/batch)
2017-05-08 15:57:44.569645: step 11650, loss = 0.88 (962.6 examples/sec; 0.133 sec/batch)
2017-05-08 15:57:45.853943: step 11660, loss = 0.87 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:57:47.172144: step 11670, loss = 1.01 (971.0 examples/sec; 0.132 sec/batch)
2017-05-08 15:57:48.461374: step 11680, loss = 1.14 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:57:49.779515: step 11690, loss = 0.91 (971.1 examples/sec; 0.132 sec/batch)
2017-05-08 15:57:51.183827: step 11700, loss = 0.90 (911.5 examples/sec; 0.140 sec/batch)
2017-05-08 15:57:52.365296: step 11710, loss = 0.91 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-08 15:57:53.696777: step 11720, loss = 0.87 (961.3 examples/sec; 0.133 sec/batch)
2017-05-08 15:57:54.981292: step 11730, loss = 0.80 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:57:56.275584: step 11740, loss = 0.83 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:57:57.564420: step 11750, loss = 0.97 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:57:58.862083: step 11760, loss = 0.89 (986.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:58:00.153913: step 11770, loss = 0.98 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:58:01.441662: step 11780, loss = 0.99 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:58:02.769334: step 11790, loss = 0.84 (964.1 examples/sec; 0.133 sec/batch)
2017-05-08 15:58:04.155784: step 11800, loss = 0.84 (923.2 examples/sec; 0.139 sec/batch)
2017-05-08 15:58:05.361285: step 11810, loss = 0.99 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-08 15:58:06.668899: step 11820, loss = 0.80 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 15:58:08.008667: step 11830, loss = 1.11 (955.4 examples/sec; 0.134 sec/batch)
2017-05-08 15:58:09.293995: step 11840, loss = 0.95 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:58:10.582129: step 11850, loss = 0.92 (993.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:58:11.857903: step 11860, loss = 0.89 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:58:13.140957: step 11870, loss = 0.80 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:58:14.427466: step 11880, loss = 0.85 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:58:15.711352: step 11890, loss = 0.88 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 15:58:17.098275: step 11900, loss = 0.91 (923.0 examples/sec; 0.139 sec/batch)
2017-05-08 15:58:18.334300: step 11910, loss = 0.86 (1035.4 examples/sec; 0.124 sec/batch)
2017-05-08 15:58:19.633510: step 11920, loss = 0.92 (985.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:58:20.962057: step 11930, loss = 1.19 (963.4 examples/sec; 0.133 sec/batch)
2017-05-08 15:58:22.255336: step 11940, loss = 1.00 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:58:23.523988: step 11950, loss = 0.95 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 15:58:24.822677: step 11960, loss = 0.87 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:58:26.111638: step 11970, loss = 0.91 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:58:27.402956: step 11980, loss = 1.18 (991.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:58:28.688967: step 11990, loss = 0.78 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:58:30.082641: step 12000, loss = 0.83 (918.4 examples/sec; 0.139 sec/batch)
2017-05-08 15:58:31.288622: step 12010, loss = 0.84 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-08 15:58:32.573320: step 12020, loss = 0.96 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:58:33.882162: step 12030, loss = 1.02 (978.0 examples/sec; 0.131 sec/batch)
2017-05-08 15:58:35.182164: step 12040, loss = 1.08 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:58:36.462909: step 12050, loss = 0.90 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:58:37.796923: step 12060, loss = 0.96 (959.5 examples/sec; 0.133 sec/batch)
2017-05-08 15:58:39.106910: step 12070, loss = 0.87 (977.1 examples/sec; 0.131 sec/batch)
2017-05-08 15:58:40.395507: step 12080, loss = 0.89 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:58:41.692478: step 12090, loss = 0.80 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:58:43.096429: step 12100, loss = 0.89 (911.7 examples/sec; 0.140 sec/batch)
2017-05-08 15:58:44.293576: step 12110, loss = 0.92 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-08 15:58:45.564858: step 12120, loss = 0.79 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 15:58:46.865540: step 12130, loss = 1.03 (984.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:58:48.151275: step 12140, loss = 1.39 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:58:49.453933: step 12150, loss = 0.99 (982.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:58:50.786983: step 12160, loss = 0.93 (960.2 examples/sec; 0.133 sec/batch)
2017-05-08 15:58:52.074312: step 12170, loss = 0.90 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:58:53.377897: step 12180, loss = 0.78 (981.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:58:54.675108: step 12190, loss = 0.86 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:58:56.047952: step 12200, loss = 0.89 (932.4 examples/sec; 0.137 sec/batch)
2017-05-08 15:58:57.252328: step 12210, loss = 1.05 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-08 15:58:58.554165: step 12220, loss = 0.83 (983.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:58:59.839988: step 12230, loss = 0.87 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:59:01.122941: step 12240, loss = 0.98 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:59:02.418030: step 12250, loss = 0.74 (988.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:59:03.691192: step 12260, loss = 1.01 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 15:59:04.977581: step 12270, loss = 1.03 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:59:06.277866: step 12280, loss = 0.67 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:59:07.592382: step 12290, loss = 0.89 (973.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:59:08.963279: step 12300, loss = 0.92 (933.7 examples/sec; 0.137 sec/batch)
2017-05-08 15:59:10.161920: step 12310, loss = 0.95 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-08 15:59:11.472629: step 12320, loss = 0.87 (976.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:59:12.784752: step 12330, loss = 0.98 (975.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:59:14.079019: step 12340, loss = 1.08 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:59:15.373698: step 12350, loss = 0.90 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:59:16.659066: step 12360, loss = 0.81 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:59:17.986850: step 12370, loss = 0.89 (964.0 examples/sec; 0.133 sec/batch)
2017-05-08 15:59:19.277592: step 12380, loss = 0.77 (991.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:59:20.569002: step 12390, loss = 1.23 (991.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:59:21.986890: step 12400, loss = 0.88 (902.8 examples/sec; 0.142 sec/batch)
2017-05-08 15:59:23.186387: step 12410, loss = 0.91 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-08 15:59:24.500886: step 12420, loss = 0.92 (973.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:59:25.826282: step 12430, loss = 0.90 (965.7 examples/sec; 0.133 sec/batch)
2017-05-08 15:59:27.136966: step 12440, loss = 1.00 (976.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:59:28.444521: step 12450, loss = 1.01 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 15:59:29.762524: step 12460, loss = 0.80 (971.2 examples/sec; 0.132 sec/batch)
2017-05-08 15:59:31.048822: step 12470, loss = 0.82 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:59:32.324410: step 12480, loss = 0.99 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:59:33.617810: step 12490, loss = 0.94 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:59:35.004655: step 12500, loss = 1.05 (923.0 examples/sec; 0.139 sec/batch)
2017-05-08 15:59:36.204179: step 12510, loss = 0.90 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-08 15:59:37.545237: step 12520, loss = 0.94 (954.5 examples/sec; 0.134 sec/batch)
2017-05-08 15:59:38.852201: step 12530, loss = 0.94 (979.4 examples/sec; 0.131 sec/batch)
2017-05-08 15:59:40.139754: step 12540, loss = 1.09 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:59:41.433890: step 12550, loss = 1.10 (989.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:59:42.739274: step 12560, loss = 0.98 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:59:44.031933: step 12570, loss = 0.85 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:59:45.303365: step 12580, loss = 0.98 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 15:59:46.576694: step 12590, loss = 0.97 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-08 15:59:47.957985: step 12600, loss = 1.11 (926.7 examples/sec; 0.138 sec/batch)
2017-05-08 15:59:49.214572: step 12610, loss = 0.78 (1018.6 examples/sec; 0.126 sec/batch)
2017-05-08 15:59:50.502021: step 12620, loss = 1.02 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:59:51.801634: step 12630, loss = 0.88 (984.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:59:53.073852: step 12640, loss = 0.99 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-08 15:59:54.405150: step 12650, loss = 0.99 (961.5 examples/sec; 0.133 sec/batch)
2017-05-08 15:59:55.697947: step 12660, loss = 0.92 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:59:57.032261: step 12670, loss = 0.73 (959.3 examples/sec; 0.133 sec/batch)
2017-05-08 15:59:58.319903: step 12680, loss = 0.94 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:59:59.627285: step 12690, loss = 0.98 (979.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:00:01.020518: step 12700, loss = 0.76 (918.7 examples/sec; 0.139 sec/batch)
2017-05-08 16:00:02.255053: step 12710, loss = 0.66 (1036.8 examples/sec; 0.123 sec/batch)
2017-05-08 16:00:03.552127: step 12720, loss = 0.85 (986.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:00:04.847790: step 12730, loss = 0.84 (987.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:00:06.171672: step 12740, loss = 0.83 (966.9 examples/sec; 0.132 sec/batch)
2017-05-08 16:00:07.459377: step 12750, loss = 0.87 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:00:08.770639: step 12760, loss = 0.78 (976.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:00:10.060036: step 12770, loss = 1.01 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:00:11.345544: step 12780, loss = 0.81 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:00:12.649045: step 12790, loss = 0.96 (982.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:00:14.063193: step 12800, loss = 0.86 (905.1 examples/sec; 0.141 sec/batch)
2017-05-08 16:00:15.257971: step 12810, loss = 1.23 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-08 16:00:16.569355: step 12820, loss = 0.91 (976.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:00:17.884017: step 12830, loss = 0.79 (973.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:00:19.197401: step 12840, loss = 0.87 (974.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:00:20.514929: step 12850, loss = 0.92 (971.5 examples/sec; 0.132 sec/batch)
2017-05-08 16:00:21.812065: step 12860, loss = 0.90 (986.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:00:23.118849: step 12870, loss = 0.91 (979.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:00:24.425168: step 12880, loss = 0.96 (979.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:00:25.714716: step 12890, loss = 0.95 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:00:27.102137: step 12900, loss = 0.80 (922.6 examples/sec; 0.139 sec/batch)
2017-05-08 16:00:28.280656: step 12910, loss = 0.72 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-08 16:00:29.570416: step 12920, loss = 0.87 (992.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:00:30.862232: step 12930, loss = 0.83 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:00:32.172132: step 12940, loss = 0.98 (977.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:00:33.489971: step 12950, loss = 0.84 (971.3 examples/sec; 0.132 sec/batch)
2017-05-08 16:00:34.751746: step 12960, loss = 0.89 (1014.4 examples/sec; 0.126 sec/batch)
2017-05-08 16:00:36.069952: step 12970, loss = 0.99 (971.0 examples/sec; 0.132 sec/batch)
2017-05-08 16:00:37.379073: step 12980, loss = 0.85 (977.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:00:38.665586: step 12990, loss = 0.85 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:00:40.069638: step 13000, loss = 0.85 (911.6 examples/sec; 0.140 sec/batch)
2017-05-08 16:00:41.288908: step 13010, loss = 1.09 (1049.8 examples/sec; 0.122 sec/batch)
2017-05-08 16:00:42.602918: step 13020, loss = 1.01 (974.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:00:43.914456: step 13030, loss = 0.95 (976.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:00:45.214350: step 13040, loss = 0.84 (984.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:00:46.513321: step 13050, loss = 0.97 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:00:47.808027: step 13060, loss = 1.03 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:00:49.106893: step 13070, loss = 0.98 (985.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:00:50.399295: step 13080, loss = 0.85 (990.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:00:51.708944: step 13090, loss = 0.74 (977.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:00:53.102281: step 13100, loss = 0.97 (918.7 examples/sec; 0.139 sec/batch)
2017-05-08 16:00:54.310649: step 13110, loss = 0.88 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-08 16:00:55.610985: step 13120, loss = 0.98 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:00:56.941077: step 13130, loss = 0.96 (962.3 examples/sec; 0.133 sec/batch)
2017-05-08 16:00:58.253708: step 13140, loss = 1.04 (975.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:00:59.538809: step 13150, loss = 0.90 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:01:00.822592: step 13160, loss = 0.75 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:01:02.149334: step 13170, loss = 0.72 (964.8 examples/sec; 0.133 sec/batch)
2017-05-08 16:01:03.440312: step 13180, loss = 0.75 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:01:04.746569: step 13190, loss = 0.83 (979.9 examples/sec; 0.131 sec/batch)
2017-05-08 16:01:06.146662: step 13200, loss = 0.93 (914.2 examples/sec; 0.140 sec/batch)
2017-05-08 16:01:07.352011: step 13210, loss = 1.07 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-08 16:01:08.645222: step 13220, loss = 0.96 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:01:09.937408: step 13230, loss = 0.71 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:01:11.247559: step 13240, loss = 0.74 (977.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:01:12.568703: step 13250, loss = 1.01 (968.9 examples/sec; 0.132 sec/batch)
2017-05-08 16:01:13.874866: step 13260, loss = 1.09 (980.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:01:15.166586: step 13270, loss = 0.81 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:01:16.481809: step 13280, loss = 0.91 (973.2 examples/sec; 0.132 sec/batch)
2017-05-08 16:01:17.804946: step 13290, loss = 0.92 (967.4 examples/sec; 0.132 sec/batch)
2017-05-08 16:01:19.180034: step 13300, loss = 1.00 (930.8 examples/sec; 0.138 sec/batch)
2017-05-08 16:01:20.380544: step 13310, loss = 0.99 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-08 16:01:21.690130: step 13320, loss = 0.89 (977.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:01:22.994227: step 13330, loss = 1.03 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:01:24.305416: step 13340, loss = 0.81 (976.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:01:25.597792: step 13350, loss = 0.88 (990.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:01:26.879072: step 13360, loss = 0.80 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:01:28.152238: step 13370, loss = 0.95 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:01:29.466422: step 13380, loss = 0.90 (974.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:01:30.768628: step 13390, loss = 0.85 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:01:32.155525: step 13400, loss = 0.84 (922.9 examples/sec; 0.139 sec/batch)
2017-05-08 16:01:33.354039: step 13410, loss = 1.03 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-08 16:01:34.643786: step 13420, loss = 0.77 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:01:35.934234: step 13430, loss = 0.89 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:01:37.237831: step 13440, loss = 0.74 (981.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:01:38.546721: step 13450, loss = 1.05 (977.9 examples/sec; 0.131 sec/batch)
2017-05-08 16:01:39.833475: step 13460, loss = 1.12 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:01:41.136819: step 13470, loss = 1.04 (982.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:01:42.442018: step 13480, loss = 0.90 (980.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:01:43.756005: step 13490, loss = 0.83 (974.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:01:45.159013: step 13500, loss = 0.80 (912.3 examples/sec; 0.140 sec/batch)
2017-05-08 16:01:46.361888: step 13510, loss = 0.90 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-08 16:01:47.648564: step 13520, loss = 0.92 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:01:48.953021: step 13530, loss = 1.09 (981.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:01:50.246920: step 13540, loss = 1.00 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:01:51.546710: step 13550, loss = 0.82 (984.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:01:52.818734: step 13560, loss = 0.98 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:01:54.102311: step 13570, loss = 0.85 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:01:55.410384: step 13580, loss = 1.06 (978.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:01:56.703551: step 13590, loss = 0.72 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:01:58.105946: step 13600, loss = 0.84 (912.7 examples/sec; 0.140 sec/batch)
2017-05-08 16:01:59.297492: step 13610, loss = 1.00 (1074.3 examples/sec; 0.119 sec/batch)
2017-05-08 16:02:00.591739: step 13620, loss = 0.79 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:02:01.882238: step 13630, loss = 1.13 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:02:03.154788: step 13640, loss = 0.80 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:02:04.424232: step 13650, loss = 0.90 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:02:05.743928: step 13660, loss = 0.94 (969.9 examples/sec; 0.132 sec/batch)
2017-05-08 16:02:07.034617: step 13670, loss = 0.77 (991.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:02:08.324685: step 13680, loss = 0.88 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:02:09.605050: step 13690, loss = 1.00 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:02:10.998238: step 13700, loss = 0.93 (918.8 examples/sec; 0.139 sec/batch)
2017-05-08 16:02:12.187636: step 13710, loss = 0.98 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-08 16:02:13.467003: step 13720, loss = 0.77 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:02:14.770840: step 13730, loss = 0.87 (981.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:02:16.035808: step 13740, loss = 0.95 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-08 16:02:17.311743: step 13750, loss = 0.87 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:02:18.577998: step 13760, loss = 0.92 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:02:19.869150: step 13770, loss = 1.00 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:02:21.171887: step 13780, loss = 0.91 (982.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:02:22.494708: step 13790, loss = 0.85 (967.6 examples/sec; 0.132 sec/batch)
2017-05-08 16:02:23.862134: step 13800, loss = 0.97 (936.1 examples/sec; 0.137 sec/batch)
2017-05-08 16:02:25.071282: step 13810, loss = 0.79 (1058.6 examples/sec; 0.121 sec/batch)
2017-05-08 16:02:26.369592: step 13820, loss = 1.00 (985.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:02:27.683098: step 13830, loss = 0.83 (974.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:02:28.970162: step 13840, loss = 1.09 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:02:30.293000: step 13850, loss = 0.97 (967.6 examples/sec; 0.132 sec/batch)
2017-05-08 16:02:31.597646: step 13860, loss = 0.85 (981.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:02:32.893698: step 13870, loss = 1.05 (987.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:02:34.170952: step 13880, loss = 0.86 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:02:35.521246: step 13890, loss = 0.83 (947.9 examples/sec; 0.135 sec/batch)
2017-05-08 16:02:36.867541: step 13900, loss = 0.86 (950.8 examples/sec; 0.135 sec/batch)
2017-05-08 16:02:38.049741: step 13910, loss = 0.95 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-08 16:02:39.344633: step 13920, loss = 0.89 (988.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:02:40.641934: step 13930, loss = 0.86 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:02:41.951372: step 13940, loss = 0.80 (977.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:02:43.248325: step 13950, loss = 0.90 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:02:44.561431: step 13960, loss = 0.84 (974.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:02:45.859252: step 13970, loss = 1.03 (986.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:02:47.138610: step 13980, loss = 0.84 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:02:48.448242: step 13990, loss = 0.75 (977.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:02:49.828540: step 14000, loss = 0.86 (927.3 examples/sec; 0.138 sec/batch)
2017-05-08 16:02:51.022729: step 14010, loss = 0.98 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-08 16:02:52.323605: step 14020, loss = 0.99 (984.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:02:53.642768: step 14030, loss = 0.76 (970.3 examples/sec; 0.132 sec/batch)
2017-05-08 16:02:54.935117: step 14040, loss = 0.75 (990.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:02:56.250049: step 14050, loss = 0.83 (973.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:02:57.568553: step 14060, loss = 1.04 (970.8 examples/sec; 0.132 sec/batch)
2017-05-08 16:02:58.876579: step 14070, loss = 1.09 (978.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:03:00.185134: step 14080, loss = 0.73 (978.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:03:01.512173: step 14090, loss = 0.85 (964.6 examples/sec; 0.133 sec/batch)
2017-05-08 16:03:02.904345: step 14100, loss = 0.71 (919.4 examples/sec; 0.139 sec/batch)
2017-05-08 16:03:04.101891: step 14110, loss = 0.69 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-08 16:03:05.400357: step 14120, loss = 0.80 (985.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:03:06.731661: step 14130, loss = 0.98 (961.5 examples/sec; 0.133 sec/batch)
2017-05-08 16:03:08.028849: step 14140, loss = 1.05 (986.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:03:09.325289: step 14150, loss = 0.87 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:03:10.621122: step 14160, loss = 0.95 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:03:11.912592: step 14170, loss = 0.83 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:03:13.197411: step 14180, loss = 0.76 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:03:14.516617: step 14190, loss = 0.81 (970.3 examples/sec; 0.132 sec/batch)
2017-05-08 16:03:15.890299: step 14200, loss = 0.74 (931.8 examples/sec; 0.137 sec/batch)
2017-05-08 16:03:17.097857: step 14210, loss = 0.78 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-08 16:03:18.386691: step 14220, loss = 1.00 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:03:19.705850: step 14230, loss = 0.78 (970.3 examples/sec; 0.132 sec/batch)
2017-05-08 16:03:21.017556: step 14240, loss = 0.99 (975.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:03:22.309090: step 14250, loss = 1.19 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:03:23.610691: step 14260, loss = 0.95 (983.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:03:24.896056: step 14270, loss = 0.83 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:03:26.198336: step 14280, loss = 0.78 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:03:27.485784: step 14290, loss = 0.99 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:03:28.878655: step 14300, loss = 1.04 (919.0 examples/sec; 0.139 sec/batch)
2017-05-08 16:03:30.075740: step 14310, loss = 0.80 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-08 16:03:31.358228: step 14320, loss = 0.86 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:03:32.669253: step 14330, loss = 0.94 (976.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:03:33.976322: step 14340, loss = 0.91 (979.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:03:35.283180: step 14350, loss = 1.10 (979.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:03:36.574268: step 14360, loss = 1.04 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:03:37.863053: step 14370, loss = 0.89 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:03:39.163809: step 14380, loss = 0.85 (984.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:03:40.446019: step 14390, loss = 0.84 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:03:41.842620: step 14400, loss = 0.79 (916.5 examples/sec; 0.140 sec/batch)
2017-05-08 16:03:43.056079: step 14410, loss = 0.86 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-08 16:03:44.349415: step 14420, loss = 0.99 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:03:45.663715: step 14430, loss = 0.89 (973.9 examples/sec; 0.131 sec/batch)
2017-05-08 16:03:46.987110: step 14440, loss = 0.93 (967.2 examples/sec; 0.132 sec/batch)
2017-05-08 16:03:48.263648: step 14450, loss = 0.93 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:03:49.557841: step 14460, loss = 0.85 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:03:50.840711: step 14470, loss = 0.80 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:03:52.132153: step 14480, loss = 0.89 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:03:53.434950: step 14490, loss = 0.85 (982.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:03:54.840753: step 14500, loss = 0.86 (910.5 examples/sec; 0.141 sec/batch)
2017-05-08 16:03:56.037492: step 14510, loss = 0.87 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-08 16:03:57.317372: step 14520, loss = 0.84 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:03:58.607789: step 14530, loss = 0.82 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:03:59.861693: step 14540, loss = 0.85 (1020.8 examples/sec; 0.125 sec/batch)
2017-05-08 16:04:01.160400: step 14550, loss = 1.11 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:04:02.454915: step 14560, loss = 0.88 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:04:03.754544: step 14570, loss = 0.86 (984.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:04:05.021553: step 14580, loss = 0.86 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:04:06.308796: step 14590, loss = 0.99 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:04:07.691820: step 14600, loss = 0.78 (925.5 examples/sec; 0.138 sec/batch)
2017-05-08 16:04:08.885256: step 14610, loss = 0.87 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-08 16:04:10.177582: step 14620, loss = 0.99 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:04:11.492612: step 14630, loss = 1.32 (973.4 examples/sec; 0.132 sec/batch)
2017-05-08 16:04:12.793353: step 14640, loss = 0.93 (984.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:04:14.091763: step 14650, loss = 0.90 (985.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:04:15.385089: step 14660, loss = 0.90 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:04:16.668588: step 14670, loss = 0.87 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:04:17.977727: step 14680, loss = 0.87 (977.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:04:19.312035: step 14690, loss = 0.77 (959.3 examples/sec; 0.133 sec/batch)
2017-05-08 16:04:20.699001: step 14700, loss = 0.84 (922.9 examples/sec; 0.139 sec/batch)
2017-05-08 16:04:21.903425: step 14710, loss = 0.92 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-08 16:04:23.195904: step 14720, loss = 1.04 (990.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:04:24.479927: step 14730, loss = 0.86 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:04:25.804641: step 14740, loss = 1.32 (966.2 examples/sec; 0.132 sec/batch)
2017-05-08 16:04:27.134779: step 14750, loss = 1.00 (962.3 examples/sec; 0.133 sec/batch)
2017-05-08 16:04:28.434637: step 14760, loss = 0.90 (984.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:04:29.728252: step 14770, loss = 0.83 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:04:31.007072: step 14780, loss = 0.79 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:04:32.311387: step 14790, loss = 0.86 (981.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:04:33.726638: step 14800, loss = 0.74 (904.4 examples/sec; 0.142 sec/batch)
2017-05-08 16:04:34.947422: step 14810, loss = 0.75 (1048.5 examples/sec; 0.122 sec/batch)
2017-05-08 16:04:36.250138: step 14820, loss = 0.77 (982.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:04:37.538744: step 14830, loss = 0.72 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:04:38.851108: step 14840, loss = 0.97 (975.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:04:40.156363: step 14850, loss = 1.06 (980.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:04:41.466939: step 14860, loss = 0.86 (976.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:04:42.754261: step 14870, loss = 0.83 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:04:44.037861: step 14880, loss = 0.85 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:04:45.332298: step 14890, loss = 0.88 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:04:46.728434: step 14900, loss = 1.17 (916.8 examples/sec; 0.140 sec/batch)
2017-05-08 16:04:47.912338: step 14910, loss = 0.82 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-08 16:04:49.247354: step 14920, loss = 0.91 (958.8 examples/sec; 0.134 sec/batch)
2017-05-08 16:04:50.558240: step 14930, loss = 0.85 (976.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:04:51.879826: step 14940, loss = 1.12 (968.5 examples/sec; 0.132 sec/batch)
2017-05-08 16:04:53.165944: step 14950, loss = 0.78 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:04:54.454836: step 14960, loss = 0.79 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:04:55.719732: step 14970, loss = 0.75 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-08 16:04:57.004827: step 14980, loss = 1.29 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:04:58.301195: step 14990, loss = 0.92 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:04:59.674970: step 15000, loss = 0.95 (931.7 examples/sec; 0.137 sec/batch)
2017-05-08 16:05:00.835279: step 15010, loss = 0.82 (1103.2 examples/sec; 0.116 sec/batch)
2017-05-08 16:05:02.138308: step 15020, loss = 0.92 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:05:03.436970: step 15030, loss = 0.92 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:05:04.752760: step 15040, loss = 0.80 (972.8 examples/sec; 0.132 sec/batch)
2017-05-08 16:05:06.046797: step 15050, loss = 0.93 (989.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:05:07.321600: step 15060, loss = 1.02 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:05:08.621919: step 15070, loss = 0.89 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:05:09.933555: step 15080, loss = 0.98 (975.9 examples/sec; 0.131 sec/batch)
2017-05-08 16:05:11.251291: step 15090, loss = 1.00 (971.4 examples/sec; 0.132 sec/batch)
2017-05-08 16:05:12.626116: step 15100, loss = 0.90 (931.0 examples/sec; 0.137 sec/batch)
2017-05-08 16:05:13.830844: step 15110, loss = 1.01 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-08 16:05:15.119673: step 15120, loss = 0.82 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:05:16.388605: step 15130, loss = 1.00 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:05:17.706689: step 15140, loss = 0.79 (971.1 examples/sec; 0.132 sec/batch)
2017-05-08 16:05:19.011755: step 15150, loss = 0.85 (980.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:05:20.284371: step 15160, loss = 0.83 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:05:21.607282: step 15170, loss = 0.92 (967.6 examples/sec; 0.132 sec/batch)
2017-05-08 16:05:22.881821: step 15180, loss = 0.75 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:05:24.173409: step 15190, loss = 0.88 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:05:25.583530: step 15200, loss = 0.92 (907.7 examples/sec; 0.141 sec/batch)
2017-05-08 16:05:26.779369: step 15210, loss = 0.77 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-08 16:05:28.057052: step 15220, loss = 0.90 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:05:29.366691: step 15230, loss = 0.95 (977.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:05:30.673813: step 15240, loss = 0.91 (979.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:05:32.001030: step 15250, loss = 0.81 (964.4 examples/sec; 0.133 sec/batch)
2017-05-08 16:05:33.319491: step 15260, loss = 0.92 (970.9 examples/sec; 0.132 sec/batch)
2017-05-08 16:05:34.623411: step 15270, loss = 0.96 (981.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:05:35.923492: step 15280, loss = 0.85 (984.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:05:37.209580: step 15290, loss = 0.69 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:05:38.584196: step 15300, loss = 0.77 (931.2 examples/sec; 0.137 sec/batch)
2017-05-08 16:05:39.787597: step 15310, loss = 0.86 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-08 16:05:41.082422: step 15320, loss = 0.90 (988.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:05:42.368490: step 15330, loss = 0.82 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:05:43.675972: step 15340, loss = 0.87 (979.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:05:44.962443: step 15350, loss = 0.77 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:05:46.251323: step 15360, loss = 0.94 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:05:47.533310: step 15370, loss = 0.94 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:05:48.838602: step 15380, loss = 0.81 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:05:50.157805: step 15390, loss = 0.85 (970.3 examples/sec; 0.132 sec/batch)
2017-05-08 16:05:51.552146: step 15400, loss = 0.76 (918.0 examples/sec; 0.139 sec/batch)
2017-05-08 16:05:52.772079: step 15410, loss = 1.09 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-08 16:05:54.060092: step 15420, loss = 0.80 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:05:55.356513: step 15430, loss = 0.85 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:05:56.641886: step 15440, loss = 0.93 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:05:57.947262: step 15450, loss = 0.97 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:05:59.215342: step 15460, loss = 0.82 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:06:00.530240: step 15470, loss = 0.86 (973.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:06:01.792878: step 15480, loss = 0.88 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-08 16:06:03.121935: step 15490, loss = 1.09 (963.1 examples/sec; 0.133 sec/batch)
2017-05-08 16:06:04.514263: step 15500, loss = 0.95 (919.3 examples/sec; 0.139 sec/batch)
2017-05-08 16:06:05.726914: step 15510, loss = 0.81 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-08 16:06:07.053261: step 15520, loss = 0.95 (965.1 examples/sec; 0.133 sec/batch)
2017-05-08 16:06:08.342826: step 15530, loss = 0.86 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:06:09.655020: step 15540, loss = 1.04 (975.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:06:10.943069: step 15550, loss = 0.83 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:06:12.236093: step 15560, loss = 0.79 (989.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:06:13.529854: step 15570, loss = 0.82 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:06:14.837951: step 15580, loss = 0.78 (978.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:06:16.126703: step 15590, loss = 0.75 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:06:17.520303: step 15600, loss = 1.00 (918.5 examples/sec; 0.139 sec/batch)
2017-05-08 16:06:18.716054: step 15610, loss = 0.78 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-08 16:06:20.017116: step 15620, loss = 0.90 (983.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:06:21.345858: step 15630, loss = 0.98 (963.3 examples/sec; 0.133 sec/batch)
2017-05-08 16:06:22.668389: step 15640, loss = 0.98 (967.8 examples/sec; 0.132 sec/batch)
2017-05-08 16:06:23.953892: step 15650, loss = 0.75 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:06:25.232763: step 15660, loss = 0.78 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:06:26.542512: step 15670, loss = 0.78 (977.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:06:27.817161: step 15680, loss = 0.88 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:06:29.115336: step 15690, loss = 0.88 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:06:30.510047: step 15700, loss = 0.80 (917.8 examples/sec; 0.139 sec/batch)
2017-05-08 16:06:31.706925: step 15710, loss = 1.00 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-08 16:06:33.028609: step 15720, loss = 0.83 (968.5 examples/sec; 0.132 sec/batch)
2017-05-08 16:06:34.316848: step 15730, loss = 0.81 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:06:35.633334: step 15740, loss = 0.93 (972.3 examples/sec; 0.132 sec/batch)
2017-05-08 16:06:36.939441: step 15750, loss = 1.01 (980.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:06:38.245626: step 15760, loss = 0.92 (980.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:06:39.542972: step 15770, loss = 0.88 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:06:40.840562: step 15780, loss = 0.88 (986.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:06:42.137503: step 15790, loss = 1.00 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:06:43.507367: step 15800, loss = 0.84 (934.4 examples/sec; 0.137 sec/batch)
2017-05-08 16:06:44.694898: step 15810, loss = 0.88 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-08 16:06:45.974337: step 15820, loss = 0.97 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:06:47.279715: step 15830, loss = 1.00 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:06:48.554923: step 15840, loss = 1.02 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:06:49.851866: step 15850, loss = 0.82 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:06:51.144457: step 15860, loss = 0.79 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:06:52.424035: step 15870, loss = 0.93 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:06:53.702199: step 15880, loss = 0.89 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:06:55.001074: step 15890, loss = 0.79 (985.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:06:56.419753: step 15900, loss = 0.90 (902.2 examples/sec; 0.142 sec/batch)
2017-05-08 16:06:57.622606: step 15910, loss = 0.97 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-08 16:06:58.923589: step 15920, loss = 0.93 (983.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:07:00.216136: step 15930, loss = 0.83 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:07:01.503124: step 15940, loss = 0.84 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:07:02.800563: step 15950, loss = 0.73 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:07:04.134515: step 15960, loss = 0.89 (959.6 examples/sec; 0.133 sec/batch)
2017-05-08 16:07:05.466252: step 15970, loss = 0.82 (961.1 examples/sec; 0.133 sec/batch)
2017-05-08 16:07:06.760141: step 15980, loss = 0.88 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:07:08.057058: step 15990, loss = 0.91 (987.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:07:09.463255: step 16000, loss = 1.23 (910.3 examples/sec; 0.141 sec/batch)
2017-05-08 16:07:10.687386: step 16010, loss = 0.89 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-08 16:07:11.989650: step 16020, loss = 0.89 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:07:13.283587: step 16030, loss = 0.82 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:07:14.584896: step 16040, loss = 0.98 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:07:15.870790: step 16050, loss = 1.01 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:07:17.172753: step 16060, loss = 0.68 (983.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:07:18.491688: step 16070, loss = 0.80 (970.5 examples/sec; 0.132 sec/batch)
2017-05-08 16:07:19.798115: step 16080, loss = 0.97 (979.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:07:21.105623: step 16090, loss = 0.90 (979.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:07:22.499281: step 16100, loss = 0.81 (918.4 examples/sec; 0.139 sec/batch)
2017-05-08 16:07:23.677725: step 16110, loss = 0.98 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-08 16:07:24.993559: step 16120, loss = 0.89 (972.8 examples/sec; 0.132 sec/batch)
2017-05-08 16:07:26.307180: step 16130, loss = 0.98 (974.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:07:27.599698: step 16140, loss = 0.74 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:07:28.924298: step 16150, loss = 0.91 (966.3 examples/sec; 0.132 sec/batch)
2017-05-08 16:07:30.247466: step 16160, loss = 0.80 (967.4 examples/sec; 0.132 sec/batch)
2017-05-08 16:07:31.519027: step 16170, loss = 0.86 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:07:32.877964: step 16180, loss = 0.85 (941.9 examples/sec; 0.136 sec/batch)
2017-05-08 16:07:34.173500: step 16190, loss = 0.98 (988.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:07:35.538975: step 16200, loss = 1.08 (937.4 examples/sec; 0.137 sec/batch)
2017-05-08 16:07:36.726538: step 16210, loss = 0.84 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-08 16:07:37.993673: step 16220, loss = 0.78 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:07:39.309081: step 16230, loss = 0.96 (973.1 examples/sec; 0.132 sec/batch)
2017-05-08 16:07:40.596221: step 16240, loss = 0.78 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:07:41.882798: step 16250, loss = 0.92 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:07:43.187212: step 16260, loss = 0.69 (981.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:07:44.483367: step 16270, loss = 0.83 (987.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:07:45.764851: step 16280, loss = 0.95 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:07:47.055071: step 16290, loss = 1.09 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:07:48.428135: step 16300, loss = 0.80 (932.2 examples/sec; 0.137 sec/batch)
2017-05-08 16:07:49.642439: step 16310, loss = 0.82 (1054.1 examples/sec; 0.121 sec/batch)
2017-05-08 16:07:50.975014: step 16320, loss = 1.05 (960.5 examples/sec; 0.133 sec/batch)
2017-05-08 16:07:52.262808: step 16330, loss = 0.88 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:07:53.557209: step 16340, loss = 0.97 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:07:54.885640: step 16350, loss = 0.89 (963.5 examples/sec; 0.133 sec/batch)
2017-05-08 16:07:56.169373: step 16360, loss = 0.99 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:07:57.508974: step 16370, loss = 1.03 (955.5 examples/sec; 0.134 sec/batch)
2017-05-08 16:07:58.805594: step 16380, loss = 0.82 (987.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:08:00.104439: step 16390, loss = 0.81 (985.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:08:01.503762: step 16400, loss = 0.80 (914.7 examples/sec; 0.140 sec/batch)
2017-05-08 16:08:02.671912: step 16410, loss = 0.82 (1095.7 examples/sec; 0.117 sec/batch)
2017-05-08 16:08:03.946952: step 16420, loss = 0.84 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:08:05.231395: step 16430, loss = 0.98 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:08:06.529245: step 16440, loss = 0.73 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:08:07.811419: step 16450, loss = 1.06 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:08:09.109845: step 16460, loss = 0.99 (985.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:08:10.439094: step 16470, loss = 0.80 (963.0 examples/sec; 0.133 sec/batch)
2017-05-08 16:08:11.725642: step 16480, loss = 0.84 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:08:13.033943: step 16490, loss = 0.79 (978.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:08:14.429614: step 16500, loss = 0.78 (917.1 examples/sec; 0.140 sec/batch)
2017-05-08 16:08:15.609878: step 16510, loss = 0.74 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-08 16:08:16.906533: step 16520, loss = 1.05 (987.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:08:18.228925: step 16530, loss = 0.81 (967.9 examples/sec; 0.132 sec/batch)
2017-05-08 16:08:19.531172: step 16540, loss = 0.77 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:08:20.824412: step 16550, loss = 0.88 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:08:22.142985: step 16560, loss = 0.89 (970.7 examples/sec; 0.132 sec/batch)
2017-05-08 16:08:23.442126: step 16570, loss = 0.70 (985.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:08:24.707743: step 16580, loss = 0.78 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:08:25.996872: step 16590, loss = 0.78 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:08:27.382731: step 16600, loss = 0.84 (923.6 examples/sec; 0.139 sec/batch)
2017-05-08 16:08:28.579368: step 16610, loss = 0.91 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-08 16:08:29.880850: step 16620, loss = 0.73 (983.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:08:31.192920: step 16630, loss = 0.80 (975.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:08:32.514806: step 16640, loss = 1.16 (968.3 examples/sec; 0.132 sec/batch)
2017-05-08 16:08:33.798774: step 16650, loss = 0.84 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:08:35.123378: step 16660, loss = 1.24 (966.3 examples/sec; 0.132 sec/batch)
2017-05-08 16:08:36.440514: step 16670, loss = 0.81 (971.9 examples/sec; 0.132 sec/batch)
2017-05-08 16:08:37.723850: step 16680, loss = 0.77 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:08:39.021120: step 16690, loss = 0.95 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:08:40.396733: step 16700, loss = 0.84 (930.5 examples/sec; 0.138 sec/batch)
2017-05-08 16:08:41.611514: step 16710, loss = 0.90 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-08 16:08:42.906521: step 16720, loss = 0.77 (988.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:08:44.213833: step 16730, loss = 0.94 (979.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:08:45.530901: step 16740, loss = 0.91 (971.9 examples/sec; 0.132 sec/batch)
2017-05-08 16:08:46.845710: step 16750, loss = 0.78 (973.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:08:48.134423: step 16760, loss = 0.83 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:08:49.425409: step 16770, loss = 0.78 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:08:50.717982: step 16780, loss = 0.79 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:08:52.034002: step 16790, loss = 1.12 (972.6 examples/sec; 0.132 sec/batch)
2017-05-08 16:08:53.445188: step 16800, loss = 0.89 (907.0 examples/sec; 0.141 sec/batch)
2017-05-08 16:08:54.642027: step 16810, loss = 0.74 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-08 16:08:55.922813: step 16820, loss = 0.93 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:08:57.217883: step 16830, loss = 0.77 (988.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:08:58.517972: step 16840, loss = 0.78 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:08:59.789391: step 16850, loss = 1.14 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:09:01.075056: step 16860, loss = 0.94 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:09:02.370882: step 16870, loss = 0.92 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:09:03.669742: step 16880, loss = 0.92 (985.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:09:04.961893: step 16890, loss = 0.94 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:09:06.349020: step 16900, loss = 0.70 (922.8 examples/sec; 0.139 sec/batch)
2017-05-08 16:09:07.564916: step 16910, loss = 1.11 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-08 16:09:08.873238: step 16920, loss = 0.79 (978.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:09:10.165242: step 16930, loss = 0.84 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:09:11.468844: step 16940, loss = 0.95 (981.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:09:12.765270: step 16950, loss = 0.90 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:09:14.092518: step 16960, loss = 0.78 (964.4 examples/sec; 0.133 sec/batch)
2017-05-08 16:09:15.384341: step 16970, loss = 1.02 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:09:16.670769: step 16980, loss = 0.95 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:09:17.986005: step 16990, loss = 0.77 (973.2 examples/sec; 0.132 sec/batch)
2017-05-08 16:09:19.370993: step 17000, loss = 0.86 (924.2 examples/sec; 0.138 sec/batch)
2017-05-08 16:09:20.583433: step 17010, loss = 0.88 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-08 16:09:21.855305: step 17020, loss = 0.83 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:09:23.158661: step 17030, loss = 0.88 (982.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:09:24.486631: step 17040, loss = 0.82 (963.9 examples/sec; 0.133 sec/batch)
2017-05-08 16:09:25.779554: step 17050, loss = 0.72 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:09:27.093264: step 17060, loss = 0.82 (974.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:09:28.370981: step 17070, loss = 0.71 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:09:29.696165: step 17080, loss = 0.89 (965.9 examples/sec; 0.133 sec/batch)
2017-05-08 16:09:30.996290: step 17090, loss = 0.93 (984.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:09:32.381364: step 17100, loss = 0.74 (924.1 examples/sec; 0.139 sec/batch)
2017-05-08 16:09:33.566439: step 17110, loss = 0.89 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-08 16:09:34.871399: step 17120, loss = 0.82 (980.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:09:36.160385: step 17130, loss = 0.90 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:09:37.457194: step 17140, loss = 0.81 (987.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:09:38.741782: step 17150, loss = 0.89 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:09:40.022119: step 17160, loss = 0.80 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:09:41.338013: step 17170, loss = 0.79 (972.7 examples/sec; 0.132 sec/batch)
2017-05-08 16:09:42.648509: step 17180, loss = 0.85 (976.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:09:43.924673: step 17190, loss = 0.91 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:09:45.319248: step 17200, loss = 0.84 (917.8 examples/sec; 0.139 sec/batch)
2017-05-08 16:09:46.503553: step 17210, loss = 0.91 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-08 16:09:47.792848: step 17220, loss = 0.81 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:09:49.080074: step 17230, loss = 0.84 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:09:50.370386: step 17240, loss = 0.98 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:09:51.665088: step 17250, loss = 0.90 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:09:52.962613: step 17260, loss = 0.73 (986.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:09:54.250585: step 17270, loss = 0.82 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:09:55.538680: step 17280, loss = 0.91 (993.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:09:56.854271: step 17290, loss = 0.75 (973.0 examples/sec; 0.132 sec/batch)
2017-05-08 16:09:58.228658: step 17300, loss = 0.80 (931.3 examples/sec; 0.137 sec/batch)
2017-05-08 16:09:59.452272: step 17310, loss = 0.89 (1046.1 examples/sec; 0.122 sec/batch)
2017-05-08 16:10:00.741910: step 17320, loss = 0.86 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:10:02.044979: step 17330, loss = 1.00 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:10:03.338748: step 17340, loss = 0.86 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:10:04.619193: step 17350, loss = 0.95 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:10:05.904874: step 17360, loss = 0.85 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:10:07.191675: step 17370, loss = 0.81 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:10:08.493919: step 17380, loss = 0.93 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:10:09.797292: step 17390, loss = 0.87 (982.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:10:11.197825: step 17400, loss = 0.84 (913.9 examples/sec; 0.140 sec/batch)
2017-05-08 16:10:12.390147: step 17410, loss = 0.81 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-08 16:10:13.681614: step 17420, loss = 0.93 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:10:14.974753: step 17430, loss = 0.82 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:10:16.245258: step 17440, loss = 1.00 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:10:17.523737: step 17450, loss = 0.82 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:10:18.811415: step 17460, loss = 0.83 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:10:20.081170: step 17470, loss = 1.08 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:10:21.372120: step 17480, loss = 0.71 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:10:22.644710: step 17490, loss = 0.84 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:10:24.046947: step 17500, loss = 0.94 (912.8 examples/sec; 0.140 sec/batch)
2017-05-08 16:10:25.218646: step 17510, loss = 0.84 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-08 16:10:26.500400: step 17520, loss = 0.73 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:10:27.767157: step 17530, loss = 0.71 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:10:29.060932: step 17540, loss = 0.83 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:10:30.355600: step 17550, loss = 0.91 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:10:31.648928: step 17560, loss = 0.80 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:10:32.949693: step 17570, loss = 1.15 (984.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:10:34.257979: step 17580, loss = 1.16 (978.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:10:35.559650: step 17590, loss = 0.87 (983.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:10:36.956253: step 17600, loss = 0.96 (916.5 examples/sec; 0.140 sec/batch)
2017-05-08 16:10:38.166637: step 17610, loss = 0.81 (1057.5 examples/sec; 0.121 sec/batch)
2017-05-08 16:10:39.471286: step 17620, loss = 0.97 (981.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:10:40.805747: step 17630, loss = 0.85 (959.2 examples/sec; 0.133 sec/batch)
2017-05-08 16:10:42.094064: step 17640, loss = 0.77 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:10:43.406224: step 17650, loss = 0.80 (975.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:10:44.705564: step 17660, loss = 0.82 (985.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:10:46.003478: step 17670, loss = 0.90 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:10:47.296099: step 17680, loss = 0.96 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:10:48.588724: step 17690, loss = 0.91 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:10:49.969155: step 17700, loss = 0.98 (927.2 examples/sec; 0.138 sec/batch)
2017-05-08 16:10:51.158247: step 17710, loss = 0.85 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-08 16:10:52.452161: step 17720, loss = 1.00 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:10:53.742835: step 17730, loss = 0.83 (991.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:10:55.047977: step 17740, loss = 0.95 (980.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:10:56.339254: step 17750, loss = 1.07 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:10:57.637689: step 17760, loss = 0.83 (985.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:10:58.952836: step 17770, loss = 0.98 (973.3 examples/sec; 0.132 sec/batch)
2017-05-08 16:11:00.241782: step 17780, loss = 0.87 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:11:01.534517: step 17790, loss = 0.70 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:11:02.915950: step 17800, loss = 0.95 (926.6 examples/sec; 0.138 sec/batch)
2017-05-08 16:11:04.095803: step 17810, loss = 0.72 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-08 16:11:05.384112: step 17820, loss = 0.92 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:11:06.681647: step 17830, loss = 0.84 (986.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:11:07.957441: step 17840, loss = 0.92 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:11:09.255359: step 17850, loss = 0.86 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:11:10.535718: step 17860, loss = 0.90 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:11:11.848195: step 17870, loss = 1.05 (975.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:11:13.151432: step 17880, loss = 0.80 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:11:14.482310: step 17890, loss = 0.85 (961.8 examples/sec; 0.133 sec/batch)
2017-05-08 16:11:15.868981: step 17900, loss = 0.77 (923.1 examples/sec; 0.139 sec/batch)
2017-05-08 16:11:17.077655: step 17910, loss = 0.86 (1059.0 examples/sec; 0.121 sec/batch)
2017-05-08 16:11:18.379559: step 17920, loss = 0.85 (983.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:11:19.678745: step 17930, loss = 0.86 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:11:20.948613: step 17940, loss = 0.71 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:11:22.230162: step 17950, loss = 0.97 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:11:23.506228: step 17960, loss = 0.87 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:11:24.773905: step 17970, loss = 0.86 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:11:26.060793: step 17980, loss = 0.72 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:11:27.374036: step 17990, loss = 0.82 (974.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:11:28.754978: step 18000, loss = 0.97 (926.9 examples/sec; 0.138 sec/batch)
2017-05-08 16:11:29.977215: step 18010, loss = 0.78 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-08 16:11:31.286676: step 18020, loss = 0.93 (977.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:11:32.592028: step 18030, loss = 1.00 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:11:33.875842: step 18040, loss = 0.90 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:11:35.168040: step 18050, loss = 0.77 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:11:36.444908: step 18060, loss = 0.66 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:11:37.736548: step 18070, loss = 0.84 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:11:39.020210: step 18080, loss = 0.79 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:11:40.314514: step 18090, loss = 0.95 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:11:41.685094: step 18100, loss = 0.79 (933.9 examples/sec; 0.137 sec/batch)
2017-05-08 16:11:42.890865: step 18110, loss = 0.99 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-08 16:11:44.187910: step 18120, loss = 0.77 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:11:45.475417: step 18130, loss = 0.84 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:11:46.744185: step 18140, loss = 0.84 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:11:48.039431: step 18150, loss = 1.07 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:11:49.327251: step 18160, loss = 0.74 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:11:50.625593: step 18170, loss = 1.00 (985.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:11:51.917418: step 18180, loss = 0.85 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:11:53.209074: step 18190, loss = 0.88 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:11:54.567388: step 18200, loss = 0.77 (942.3 examples/sec; 0.136 sec/batch)
2017-05-08 16:11:55.764534: step 18210, loss = 1.04 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-08 16:11:57.054582: step 18220, loss = 0.85 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:11:58.354568: step 18230, loss = 1.02 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:11:59.620615: step 18240, loss = 0.87 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:12:00.928159: step 18250, loss = 0.88 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 16:12:02.209852: step 18260, loss = 0.76 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:12:03.503198: step 18270, loss = 0.66 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:12:04.801428: step 18280, loss = 0.66 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:12:06.104733: step 18290, loss = 0.73 (982.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:12:07.490595: step 18300, loss = 0.69 (923.6 examples/sec; 0.139 sec/batch)
2017-05-08 16:12:08.669430: step 18310, loss = 0.73 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-08 16:12:09.952529: step 18320, loss = 1.09 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:12:11.246133: step 18330, loss = 1.01 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:12:12.521664: step 18340, loss = 1.07 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:12:13.852867: step 18350, loss = 0.82 (961.5 examples/sec; 0.133 sec/batch)
2017-05-08 16:12:15.139983: step 18360, loss = 0.89 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:12:16.435378: step 18370, loss = 0.86 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:12:17.716695: step 18380, loss = 0.85 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:12:18.997614: step 18390, loss = 0.65 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:12:20.377968: step 18400, loss = 1.06 (927.3 examples/sec; 0.138 sec/batch)
2017-05-08 16:12:21.600046: step 18410, loss = 1.00 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-08 16:12:22.933603: step 18420, loss = 0.93 (959.8 examples/sec; 0.133 sec/batch)
2017-05-08 16:12:24.216503: step 18430, loss = 0.94 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:12:25.511268: step 18440, loss = 1.08 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:12:26.830621: step 18450, loss = 0.93 (970.2 examples/sec; 0.132 sec/batch)
2017-05-08 16:12:28.151860: step 18460, loss = 0.91 (968.8 examples/sec; 0.132 sec/batch)
2017-05-08 16:12:29.433101: step 18470, loss = 0.80 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:12:30.711618: step 18480, loss = 0.81 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:12:31.994722: step 18490, loss = 0.94 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:12:33.387459: step 18500, loss = 0.88 (919.1 examples/sec; 0.139 sec/batch)
2017-05-08 16:12:34.572343: step 18510, loss = 0.78 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-08 16:12:35.913552: step 18520, loss = 0.82 (954.4 examples/sec; 0.134 sec/batch)
2017-05-08 16:12:37.199723: step 18530, loss = 0.99 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:12:38.501724: step 18540, loss = 0.88 (983.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:12:39.790417: step 18550, loss = 0.86 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:12:41.085353: step 18560, loss = 0.82 (988.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:12:42.383600: step 18570, loss = 0.81 (985.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:12:43.663267: step 18580, loss = 0.92 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:12:44.934791: step 18590, loss = 0.86 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:12:46.341901: step 18600, loss = 1.01 (909.7 examples/sec; 0.141 sec/batch)
2017-05-08 16:12:47.545835: step 18610, loss = 0.71 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-08 16:12:48.824606: step 18620, loss = 1.04 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:12:50.132079: step 18630, loss = 0.87 (979.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:12:51.416353: step 18640, loss = 1.00 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:12:52.689781: step 18650, loss = 0.77 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:12:53.990384: step 18660, loss = 0.70 (984.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:12:55.255165: step 18670, loss = 0.87 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-08 16:12:56.540325: step 18680, loss = 0.77 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:12:57.871024: step 18690, loss = 0.96 (961.9 examples/sec; 0.133 sec/batch)
2017-05-08 16:12:59.242162: step 18700, loss = 0.75 (933.5 examples/sec; 0.137 sec/batch)
2017-05-08 16:13:00.428065: step 18710, loss = 0.84 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-08 16:13:01.714179: step 18720, loss = 1.00 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:13:03.003733: step 18730, loss = 0.81 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:13:04.306378: step 18740, loss = 0.89 (982.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:13:05.585949: step 18750, loss = 0.87 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:13:06.892798: step 18760, loss = 0.88 (979.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:13:08.184418: step 18770, loss = 1.10 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:13:09.503485: step 18780, loss = 0.82 (970.4 examples/sec; 0.132 sec/batch)
2017-05-08 16:13:10.804758: step 18790, loss = 0.86 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:13:12.169934: step 18800, loss = 0.90 (937.6 examples/sec; 0.137 sec/batch)
2017-05-08 16:13:13.368637: step 18810, loss = 0.98 (1067.8 examples/sec; 0.120 sec/batch)
2017-05-08 16:13:14.653901: step 18820, loss = 0.89 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:13:15.924212: step 18830, loss = 0.89 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:13:17.220144: step 18840, loss = 0.85 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:13:18.491074: step 18850, loss = 0.80 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:13:19.749315: step 18860, loss = 0.88 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-08 16:13:21.023880: step 18870, loss = 0.84 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:13:22.347022: step 18880, loss = 0.91 (967.4 examples/sec; 0.132 sec/batch)
2017-05-08 16:13:23.610556: step 18890, loss = 0.71 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-08 16:13:24.998207: step 18900, loss = 0.78 (922.4 examples/sec; 0.139 sec/batch)
2017-05-08 16:13:26.189819: step 18910, loss = 1.03 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-08 16:13:27.478430: step 18920, loss = 0.85 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:13:28.755672: step 18930, loss = 0.84 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:13:30.049096: step 18940, loss = 0.73 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:13:31.325165: step 18950, loss = 0.93 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:13:32.592926: step 18960, loss = 0.92 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:13:33.884233: step 18970, loss = 0.83 (991.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:13:35.202511: step 18980, loss = 0.98 (971.0 examples/sec; 0.132 sec/batch)
2017-05-08 16:13:36.495959: step 18990, loss = 0.75 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:13:37.901932: step 19000, loss = 0.89 (910.4 examples/sec; 0.141 sec/batch)
2017-05-08 16:13:39.131614: step 19010, loss = 0.77 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-08 16:13:40.416409: step 19020, loss = 0.83 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:13:41.708818: step 19030, loss = 0.82 (990.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:13:42.996689: step 19040, loss = 0.75 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:13:44.272366: step 19050, loss = 0.88 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:13:45.573654: step 19060, loss = 0.73 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:13:46.854744: step 19070, loss = 0.94 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:13:48.151210: step 19080, loss = 0.87 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:13:49.443875: step 19090, loss = 0.89 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:13:50.833887: step 19100, loss = 1.05 (920.9 examples/sec; 0.139 sec/batch)
2017-05-08 16:13:52.052509: step 19110, loss = 0.65 (1050.4 examples/sec; 0.122 sec/batch)
2017-05-08 16:13:53.362602: step 19120, loss = 1.05 (977.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:13:54.670524: step 19130, loss = 0.85 (978.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:13:55.958376: step 19140, loss = 0.71 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:13:57.237321: step 19150, loss = 1.10 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:13:58.531557: step 19160, loss = 0.67 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:13:59.825842: step 19170, loss = 0.80 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:14:01.114396: step 19180, loss = 0.86 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:14:02.410277: step 19190, loss = 0.72 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:14:03.773575: step 19200, loss = 0.77 (938.9 examples/sec; 0.136 sec/batch)
2017-05-08 16:14:04.966721: step 19210, loss = 1.04 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-08 16:14:06.258958: step 19220, loss = 0.85 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:14:07.540167: step 19230, loss = 0.95 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:14:08.827194: step 19240, loss = 1.00 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:14:10.118223: step 19250, loss = 1.00 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:14:11.392848: step 19260, loss = 0.94 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:14:12.670409: step 19270, loss = 0.69 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:14:13.978240: step 19280, loss = 0.76 (978.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:14:15.268296: step 19290, loss = 1.09 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:14:16.631357: step 19300, loss = 0.81 (939.1 examples/sec; 0.136 sec/batch)
2017-05-08 16:14:17.822686: step 19310, loss = 1.16 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-08 16:14:19.118698: step 19320, loss = 0.85 (987.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:14:20.402408: step 19330, loss = 0.86 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:14:21.695897: step 19340, loss = 0.78 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:14:22.993903: step 19350, loss = 0.73 (986.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:14:24.264655: step 19360, loss = 0.96 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:14:25.555630: step 19370, loss = 0.98 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:14:26.864894: step 19380, loss = 0.82 (977.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:14:28.140121: step 19390, loss = 0.98 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:14:29.517639: step 19400, loss = 0.84 (929.2 examples/sec; 0.138 sec/batch)
2017-05-08 16:14:30.715970: step 19410, loss = 0.86 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-08 16:14:31.970332: step 19420, loss = 0.76 (1020.4 examples/sec; 0.125 sec/batch)
2017-05-08 16:14:33.271767: step 19430, loss = 0.85 (983.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:14:34.571737: step 19440, loss = 0.96 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:14:35.866815: step 19450, loss = 0.85 (988.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:14:37.140147: step 19460, loss = 0.81 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:14:38.429086: step 19470, loss = 0.98 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:14:39.693749: step 19480, loss = 0.92 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-08 16:14:41.003540: step 19490, loss = 0.73 (977.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:14:42.368981: step 19500, loss = 0.68 (937.4 examples/sec; 0.137 sec/batch)
2017-05-08 16:14:43.542898: step 19510, loss = 0.68 (1090.4 examples/sec; 0.117 sec/batch)
2017-05-08 16:14:44.828123: step 19520, loss = 0.72 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:14:46.120986: step 19530, loss = 0.73 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:14:47.402367: step 19540, loss = 0.89 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:14:48.683571: step 19550, loss = 0.84 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:14:49.967371: step 19560, loss = 0.86 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:14:51.240650: step 19570, loss = 0.88 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:14:52.542032: step 19580, loss = 0.78 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:14:53.819375: step 19590, loss = 1.00 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:14:55.231619: step 19600, loss = 0.90 (906.3 examples/sec; 0.141 sec/batch)
2017-05-08 16:14:56.437334: step 19610, loss = 0.88 (1061.6 examples/sec; 0.121 sec/batch)
2017-05-08 16:14:57.729146: step 19620, loss = 0.77 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:14:59.053433: step 19630, loss = 1.00 (966.6 examples/sec; 0.132 sec/batch)
2017-05-08 16:15:00.360995: step 19640, loss = 0.93 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 16:15:01.653818: step 19650, loss = 0.91 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:15:02.933379: step 19660, loss = 0.92 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:15:04.207704: step 19670, loss = 1.00 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:15:05.485314: step 19680, loss = 1.01 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:15:06.762744: step 19690, loss = 0.80 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:15:08.137176: step 19700, loss = 0.93 (931.3 examples/sec; 0.137 sec/batch)
2017-05-08 16:15:09.327395: step 19710, loss = 0.90 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-08 16:15:10.605662: step 19720, loss = 0.72 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:15:11.921930: step 19730, loss = 0.81 (972.4 examples/sec; 0.132 sec/batch)
2017-05-08 16:15:13.203362: step 19740, loss = 0.86 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:15:14.489850: step 19750, loss = 0.84 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:15:15.778730: step 19760, loss = 0.81 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:15:17.076243: step 19770, loss = 0.97 (986.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:15:18.360207: step 19780, loss = 0.98 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:15:19.634516: step 19790, loss = 1.01 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:15:21.002141: step 19800, loss = 0.73 (935.9 examples/sec; 0.137 sec/batch)
2017-05-08 16:15:22.170026: step 19810, loss = 0.67 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-08 16:15:23.480676: step 19820, loss = 1.17 (976.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:15:24.779187: step 19830, loss = 0.89 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:15:26.067508: step 19840, loss = 0.81 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:15:27.364018: step 19850, loss = 0.85 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:15:28.638008: step 19860, loss = 0.89 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:15:29.924440: step 19870, loss = 0.88 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:15:31.193158: step 19880, loss = 0.79 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:15:32.472255: step 19890, loss = 0.85 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:15:33.840922: step 19900, loss = 0.84 (935.2 examples/sec; 0.137 sec/batch)
2017-05-08 16:15:35.030938: step 19910, loss = 0.89 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-08 16:15:36.332228: step 19920, loss = 1.00 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:15:37.626580: step 19930, loss = 0.89 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:15:38.934767: step 19940, loss = 1.05 (978.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:15:40.223961: step 19950, loss = 1.01 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:15:41.521541: step 19960, loss = 0.85 (986.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:15:42.816799: step 19970, loss = 0.84 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:15:44.065548: step 19980, loss = 0.98 (1025.0 examples/sec; 0.125 sec/batch)
2017-05-08 16:15:45.361404: step 19990, loss = 0.82 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:15:46.773238: step 20000, loss = 0.74 (906.6 examples/sec; 0.141 sec/batch)
2017-05-08 16:15:47.957800: step 20010, loss = 0.86 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-08 16:15:49.232113: step 20020, loss = 0.94 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:15:50.509922: step 20030, loss = 1.06 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:15:51.789742: step 20040, loss = 1.08 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:15:53.075120: step 20050, loss = 0.86 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:15:54.366242: step 20060, loss = 0.80 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:15:55.645499: step 20070, loss = 0.75 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:15:56.927139: step 20080, loss = 0.85 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:15:58.202369: step 20090, loss = 0.81 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:15:59.580275: step 20100, loss = 0.96 (928.9 examples/sec; 0.138 sec/batch)
2017-05-08 16:16:00.774247: step 20110, loss = 0.80 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-08 16:16:02.042951: step 20120, loss = 1.02 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:16:03.329046: step 20130, loss = 0.95 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:16:04.595626: step 20140, loss = 0.96 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:16:05.863328: step 20150, loss = 0.95 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:16:07.163631: step 20160, loss = 0.88 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:16:08.445062: step 20170, loss = 1.00 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:16:09.719664: step 20180, loss = 0.85 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:16:10.995080: step 20190, loss = 0.83 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:16:12.359825: step 20200, loss = 0.89 (937.9 examples/sec; 0.136 sec/batch)
2017-05-08 16:16:13.554091: step 20210, loss = 0.94 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-08 16:16:14.830080: step 20220, loss = 0.90 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:16:16.128224: step 20230, loss = 0.96 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:16:17.417014: step 20240, loss = 1.01 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:16:18.726942: step 20250, loss = 0.89 (977.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:16:20.030571: step 20260, loss = 1.10 (981.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:16:21.311368: step 20270, loss = 0.99 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:16:22.594354: step 20280, loss = 0.83 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:16:23.893933: step 20290, loss = 0.92 (984.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:16:25.263623: step 20300, loss = 0.91 (934.5 examples/sec; 0.137 sec/batch)
2017-05-08 16:16:26.472213: step 20310, loss = 0.74 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-08 16:16:27.745516: step 20320, loss = 0.92 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:16:29.039102: step 20330, loss = 1.00 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:16:30.330357: step 20340, loss = 1.09 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:16:31.626118: step 20350, loss = 0.92 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:16:32.939354: step 20360, loss = 0.85 (974.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:16:34.242668: step 20370, loss = 0.89 (982.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:16:35.517030: step 20380, loss = 0.85 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:16:36.809328: step 20390, loss = 0.77 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:16:38.214384: step 20400, loss = 1.12 (911.0 examples/sec; 0.141 sec/batch)
2017-05-08 16:16:39.420747: step 20410, loss = 0.84 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-08 16:16:40.697630: step 20420, loss = 0.81 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:16:41.966032: step 20430, loss = 0.90 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:16:43.246292: step 20440, loss = 0.82 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:16:44.521746: step 20450, loss = 0.76 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:16:45.791880: step 20460, loss = 0.98 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:16:47.072395: step 20470, loss = 0.89 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:16:48.356532: step 20480, loss = 1.07 (996.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:16:49.634093: step 20490, loss = 0.78 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:16:51.018753: step 20500, loss = 1.00 (924.4 examples/sec; 0.138 sec/batch)
2017-05-08 16:16:52.203853: step 20510, loss = 1.06 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-08 16:16:53.466127: step 20520, loss = 0.93 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-08 16:16:54.752159: step 20530, loss = 0.89 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:16:56.014623: step 20540, loss = 0.77 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-08 16:16:57.297935: step 20550, loss = 0.97 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:16:58.579324: step 20560, loss = 0.91 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:16:59.862841: step 20570, loss = 0.80 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:17:01.134086: step 20580, loss = 1.02 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:17:02.435639: step 20590, loss = 0.79 (983.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:17:03.826700: step 20600, loss = 0.82 (920.2 examples/sec; 0.139 sec/batch)
2017-05-08 16:17:05.053606: step 20610, loss = 0.85 (1043.3 examples/sec; 0.123 sec/batch)
2017-05-08 16:17:06.360602: step 20620, loss = 0.96 (979.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:17:07.631152: step 20630, loss = 0.93 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:17:08.914501: step 20640, loss = 0.86 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:17:10.216013: step 20650, loss = 0.95 (983.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:17:11.485304: step 20660, loss = 0.95 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:17:12.763997: step 20670, loss = 0.77 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:17:14.061807: step 20680, loss = 0.84 (986.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:17:15.355777: step 20690, loss = 0.64 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:17:16.728663: step 20700, loss = 0.97 (932.3 examples/sec; 0.137 sec/batch)
2017-05-08 16:17:17.919778: step 20710, loss = 0.70 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-08 16:17:19.196061: step 20720, loss = 0.84 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:17:20.480223: step 20730, loss = 0.93 (996.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:17:21.773273: step 20740, loss = 0.84 (989.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:17:23.055680: step 20750, loss = 0.75 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:17:24.344302: step 20760, loss = 0.86 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:17:25.617440: step 20770, loss = 0.79 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:17:26.924392: step 20780, loss = 0.81 (979.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:17:28.200013: step 20790, loss = 0.89 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:17:29.584544: step 20800, loss = 0.82 (924.5 examples/sec; 0.138 sec/batch)
2017-05-08 16:17:30.774052: step 20810, loss = 0.94 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-08 16:17:32.049506: step 20820, loss = 0.78 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:17:33.331920: step 20830, loss = 0.83 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:17:34.649509: step 20840, loss = 0.80 (971.5 examples/sec; 0.132 sec/batch)
2017-05-08 16:17:35.925073: step 20850, loss = 0.94 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:17:37.240474: step 20860, loss = 0.93 (973.1 examples/sec; 0.132 sec/batch)
2017-05-08 16:17:38.549131: step 20870, loss = 0.97 (978.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:17:39.856714: step 20880, loss = 1.00 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 16:17:41.115847: step 20890, loss = 0.85 (1016.6 examples/sec; 0.126 sec/batch)
2017-05-08 16:17:42.500273: step 20900, loss = 0.84 (924.6 examples/sec; 0.138 sec/batch)
2017-05-08 16:17:43.675643: step 20910, loss = 0.88 (1089.0 examples/sec; 0.118 sec/batch)
2017-05-08 16:17:44.969990: step 20920, loss = 0.98 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:17:46.247122: step 20930, loss = 0.81 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:17:47.530541: step 20940, loss = 0.89 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:17:48.830551: step 20950, loss = 0.82 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:17:50.116553: step 20960, loss = 0.96 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:17:51.388167: step 20970, loss = 0.74 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:17:52.655636: step 20980, loss = 1.00 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:17:53.926862: step 20990, loss = 1.00 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:17:55.317585: step 21000, loss = 0.85 (920.4 examples/sec; 0.139 sec/batch)
2017-05-08 16:17:56.514325: step 21010, loss = 1.19 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-08 16:17:57.815172: step 21020, loss = 1.17 (984.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:17:59.106420: step 21030, loss = 1.02 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:18:00.394989: step 21040, loss = 0.68 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:18:01.678569: step 21050, loss = 0.96 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:18:02.974936: step 21060, loss = 0.87 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:18:04.293612: step 21070, loss = 0.80 (970.7 examples/sec; 0.132 sec/batch)
2017-05-08 16:18:05.579295: step 21080, loss = 1.02 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:18:06.853813: step 21090, loss = 0.75 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:18:08.265943: step 21100, loss = 0.75 (906.4 examples/sec; 0.141 sec/batch)
2017-05-08 16:18:09.449909: step 21110, loss = 0.87 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-08 16:18:10.734292: step 21120, loss = 0.66 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:18:12.014127: step 21130, loss = 0.82 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:18:13.307292: step 21140, loss = 0.83 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:18:14.627000: step 21150, loss = 0.86 (969.9 examples/sec; 0.132 sec/batch)
2017-05-08 16:18:15.910328: step 21160, loss = 0.71 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:18:17.198991: step 21170, loss = 0.91 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:18:18.516877: step 21180, loss = 0.83 (971.3 examples/sec; 0.132 sec/batch)
2017-05-08 16:18:19.814577: step 21190, loss = 0.67 (986.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:18:21.196062: step 21200, loss = 0.92 (926.5 examples/sec; 0.138 sec/batch)
2017-05-08 16:18:22.384742: step 21210, loss = 0.89 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-08 16:18:23.651576: step 21220, loss = 0.85 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:18:24.916846: step 21230, loss = 0.95 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:18:26.196803: step 21240, loss = 0.86 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:18:27.461026: step 21250, loss = 0.78 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-08 16:18:28.721744: step 21260, loss = 1.00 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-08 16:18:30.003397: step 21270, loss = 0.77 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:18:31.284387: step 21280, loss = 0.95 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:18:32.558120: step 21290, loss = 0.78 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:18:33.930838: step 21300, loss = 0.85 (932.5 examples/sec; 0.137 sec/batch)
2017-05-08 16:18:35.124288: step 21310, loss = 0.81 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-08 16:18:36.371992: step 21320, loss = 0.67 (1025.9 examples/sec; 0.125 sec/batch)
2017-05-08 16:18:37.647832: step 21330, loss = 0.99 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:18:38.925664: step 21340, loss = 0.81 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:18:40.211099: step 21350, loss = 0.70 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:18:41.519312: step 21360, loss = 1.13 (978.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:18:42.833320: step 21370, loss = 0.94 (974.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:18:44.132011: step 21380, loss = 0.98 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:18:45.398290: step 21390, loss = 0.90 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:18:46.779376: step 21400, loss = 1.08 (926.8 examples/sec; 0.138 sec/batch)
2017-05-08 16:18:47.961144: step 21410, loss = 0.86 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-08 16:18:49.243926: step 21420, loss = 0.90 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:18:50.533956: step 21430, loss = 0.79 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:18:51.824499: step 21440, loss = 0.88 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:18:53.149839: step 21450, loss = 0.83 (965.8 examples/sec; 0.133 sec/batch)
2017-05-08 16:18:54.416069: step 21460, loss = 0.89 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:18:55.683975: step 21470, loss = 0.83 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:18:56.958639: step 21480, loss = 0.79 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:18:58.266030: step 21490, loss = 0.83 (979.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:18:59.632882: step 21500, loss = 0.82 (936.5 examples/sec; 0.137 sec/batch)
2017-05-08 16:19:00.822065: step 21510, loss = 0.86 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-08 16:19:02.126614: step 21520, loss = 0.73 (981.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:19:03.413727: step 21530, loss = 0.85 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:19:04.733995: step 21540, loss = 0.92 (969.5 examples/sec; 0.132 sec/batch)
2017-05-08 16:19:06.015063: step 21550, loss = 0.84 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:19:07.292432: step 21560, loss = 0.82 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:19:08.574367: step 21570, loss = 1.20 (998.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:19:09.885888: step 21580, loss = 0.85 (976.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:19:11.182276: step 21590, loss = 0.76 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:19:12.544049: step 21600, loss = 0.77 (939.9 examples/sec; 0.136 sec/batch)
2017-05-08 16:19:13.754619: step 21610, loss = 0.73 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-08 16:19:15.024785: step 21620, loss = 0.83 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:19:16.287649: step 21630, loss = 1.01 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-08 16:19:17.567330: step 21640, loss = 0.90 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:19:18.854220: step 21650, loss = 0.82 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:19:20.129773: step 21660, loss = 0.80 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:19:21.403680: step 21670, loss = 0.99 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:19:22.704426: step 21680, loss = 0.98 (984.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:19:24.015575: step 21690, loss = 0.90 (976.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:19:25.395499: step 21700, loss = 0.86 (927.6 examples/sec; 0.138 sec/batch)
2017-05-08 16:19:26.635633: step 21710, loss = 0.96 (1032.2 examples/sec; 0.124 sec/batch)
2017-05-08 16:19:27.916875: step 21720, loss = 1.02 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:19:29.204977: step 21730, loss = 0.90 (993.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:19:30.512726: step 21740, loss = 0.84 (978.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:19:31.804131: step 21750, loss = 0.87 (991.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:19:33.105572: step 21760, loss = 0.85 (983.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:19:34.380926: step 21770, loss = 0.71 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:19:35.671008: step 21780, loss = 0.88 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:19:36.942725: step 21790, loss = 0.86 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:19:38.309079: step 21800, loss = 0.89 (936.8 examples/sec; 0.137 sec/batch)
2017-05-08 16:19:39.498614: step 21810, loss = 0.80 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-08 16:19:40.803880: step 21820, loss = 0.71 (980.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:19:42.091502: step 21830, loss = 0.87 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:19:43.409783: step 21840, loss = 0.82 (971.0 examples/sec; 0.132 sec/batch)
2017-05-08 16:19:44.707838: step 21850, loss = 0.65 (986.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:19:46.007243: step 21860, loss = 0.83 (985.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:19:47.297829: step 21870, loss = 1.01 (991.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:19:48.575224: step 21880, loss = 0.70 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:19:49.905290: step 21890, loss = 0.78 (962.4 examples/sec; 0.133 sec/batch)
2017-05-08 16:19:51.289092: step 21900, loss = 0.80 (925.0 examples/sec; 0.138 sec/batch)
2017-05-08 16:19:52.466535: step 21910, loss = 1.01 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-08 16:19:53.773061: step 21920, loss = 1.04 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:19:55.044890: step 21930, loss = 0.87 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:19:56.330506: step 21940, loss = 0.88 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:19:57.608743: step 21950, loss = 0.76 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:19:58.895083: step 21960, loss = 0.83 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:20:00.189510: step 21970, loss = 0.88 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:20:01.477372: step 21980, loss = 0.79 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:20:02.760817: step 21990, loss = 0.81 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:20:04.121667: step 22000, loss = 0.84 (940.6 examples/sec; 0.136 sec/batch)
2017-05-08 16:20:05.315252: step 22010, loss = 0.97 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-08 16:20:06.595707: step 22020, loss = 0.87 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:20:07.848818: step 22030, loss = 0.79 (1021.5 examples/sec; 0.125 sec/batch)
2017-05-08 16:20:09.144029: step 22040, loss = 0.86 (988.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:20:10.442641: step 22050, loss = 0.83 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:20:11.755663: step 22060, loss = 0.95 (974.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:20:13.047590: step 22070, loss = 1.00 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:20:14.326961: step 22080, loss = 0.76 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:20:15.613179: step 22090, loss = 0.80 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:20:16.996314: step 22100, loss = 0.82 (925.4 examples/sec; 0.138 sec/batch)
2017-05-08 16:20:18.209670: step 22110, loss = 0.89 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-08 16:20:19.486289: step 22120, loss = 1.16 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:20:20.759208: step 22130, loss = 0.85 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:20:22.033940: step 22140, loss = 0.62 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:20:23.376147: step 22150, loss = 0.96 (953.7 examples/sec; 0.134 sec/batch)
2017-05-08 16:20:24.638694: step 22160, loss = 0.82 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-08 16:20:25.930305: step 22170, loss = 0.75 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:20:27.231856: step 22180, loss = 0.74 (983.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:20:28.508797: step 22190, loss = 0.75 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:20:29.891034: step 22200, loss = 0.76 (926.0 examples/sec; 0.138 sec/batch)
2017-05-08 16:20:31.104189: step 22210, loss = 0.68 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-08 16:20:32.393757: step 22220, loss = 1.13 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:20:33.698978: step 22230, loss = 0.91 (980.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:20:34.989179: step 22240, loss = 0.89 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:20:36.266914: step 22250, loss = 0.96 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:20:37.561860: step 22260, loss = 1.02 (988.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:20:38.851264: step 22270, loss = 0.80 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:20:40.146841: step 22280, loss = 0.77 (988.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:20:41.433238: step 22290, loss = 0.77 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:20:42.809496: step 22300, loss = 0.88 (930.1 examples/sec; 0.138 sec/batch)
2017-05-08 16:20:43.973422: step 22310, loss = 0.77 (1099.7 examples/sec; 0.116 sec/batch)
2017-05-08 16:20:45.239726: step 22320, loss = 0.89 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:20:46.537264: step 22330, loss = 0.82 (986.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:20:47.813542: step 22340, loss = 0.88 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:20:49.115279: step 22350, loss = 0.92 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:20:50.415589: step 22360, loss = 0.87 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:20:51.689285: step 22370, loss = 1.13 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:20:52.994433: step 22380, loss = 0.87 (980.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:20:54.285435: step 22390, loss = 0.66 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:20:55.670111: step 22400, loss = 0.90 (924.4 examples/sec; 0.138 sec/batch)
2017-05-08 16:20:56.852827: step 22410, loss = 1.00 (1082.3 examples/sec; 0.118 sec/batch)
2017-05-08 16:20:58.135687: step 22420, loss = 0.71 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:20:59.423931: step 22430, loss = 0.85 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:21:00.702671: step 22440, loss = 0.93 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:21:01.972713: step 22450, loss = 0.87 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:21:03.249141: step 22460, loss = 0.97 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:21:04.526181: step 22470, loss = 0.84 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:21:05.805372: step 22480, loss = 0.76 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:21:07.074618: step 22490, loss = 0.99 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:21:08.440536: step 22500, loss = 0.94 (937.1 examples/sec; 0.137 sec/batch)
2017-05-08 16:21:09.671835: step 22510, loss = 0.82 (1039.6 examples/sec; 0.123 sec/batch)
2017-05-08 16:21:10.996946: step 22520, loss = 0.91 (966.0 examples/sec; 0.133 sec/batch)
2017-05-08 16:21:12.280561: step 22530, loss = 0.98 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:21:13.550777: step 22540, loss = 0.90 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:21:14.850794: step 22550, loss = 0.79 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:21:16.153066: step 22560, loss = 0.67 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:21:17.434964: step 22570, loss = 0.66 (998.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:21:18.738909: step 22580, loss = 0.83 (981.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:21:20.009567: step 22590, loss = 0.75 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:21:21.375152: step 22600, loss = 0.92 (937.3 examples/sec; 0.137 sec/batch)
2017-05-08 16:21:22.554016: step 22610, loss = 0.93 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-08 16:21:23.837390: step 22620, loss = 0.87 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:21:25.137890: step 22630, loss = 0.83 (984.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:21:26.433421: step 22640, loss = 0.84 (988.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:21:27.735782: step 22650, loss = 0.92 (982.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:21:29.043858: step 22660, loss = 0.96 (978.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:21:30.342038: step 22670, loss = 0.85 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:21:31.605537: step 22680, loss = 0.85 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-08 16:21:32.885827: step 22690, loss = 0.95 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:21:34.289874: step 22700, loss = 0.83 (911.6 examples/sec; 0.140 sec/batch)
2017-05-08 16:21:35.486938: step 22710, loss = 0.62 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-08 16:21:36.773232: step 22720, loss = 1.03 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:21:38.103317: step 22730, loss = 0.93 (962.3 examples/sec; 0.133 sec/batch)
2017-05-08 16:21:39.389395: step 22740, loss = 0.79 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:21:40.678437: step 22750, loss = 0.96 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:21:41.966400: step 22760, loss = 0.89 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:21:43.252372: step 22770, loss = 0.94 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:21:44.549971: step 22780, loss = 0.78 (986.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:21:45.847533: step 22790, loss = 1.00 (986.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:21:47.233960: step 22800, loss = 0.91 (923.2 examples/sec; 0.139 sec/batch)
2017-05-08 16:21:48.449100: step 22810, loss = 0.86 (1053.4 examples/sec; 0.122 sec/batch)
2017-05-08 16:21:49.735107: step 22820, loss = 0.75 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:21:51.005170: step 22830, loss = 0.83 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:21:52.263547: step 22840, loss = 0.92 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-08 16:21:53.575172: step 22850, loss = 1.02 (975.9 examples/sec; 0.131 sec/batch)
2017-05-08 16:21:54.874685: step 22860, loss = 0.89 (985.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:21:56.153643: step 22870, loss = 0.84 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:21:57.453733: step 22880, loss = 0.79 (984.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:21:58.740067: step 22890, loss = 0.85 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:22:00.101171: step 22900, loss = 0.80 (940.4 examples/sec; 0.136 sec/batch)
2017-05-08 16:22:01.285784: step 22910, loss = 0.83 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-08 16:22:02.578349: step 22920, loss = 0.80 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:22:03.851095: step 22930, loss = 0.85 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:22:05.135653: step 22940, loss = 0.81 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:22:06.429646: step 22950, loss = 0.96 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:22:07.731597: step 22960, loss = 0.78 (983.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:22:09.030181: step 22970, loss = 0.82 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:22:10.306582: step 22980, loss = 0.81 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:22:11.598341: step 22990, loss = 0.86 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:22:13.015998: step 23000, loss = 0.91 (902.9 examples/sec; 0.142 sec/batch)
2017-05-08 16:22:14.186104: step 23010, loss = 0.73 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-08 16:22:15.468876: step 23020, loss = 0.80 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:22:16.740738: step 23030, loss = 0.66 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:22:17.995532: step 23040, loss = 0.88 (1020.1 examples/sec; 0.125 sec/batch)
2017-05-08 16:22:19.272337: step 23050, loss = 0.87 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:22:20.540427: step 23060, loss = 0.76 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:22:21.818166: step 23070, loss = 0.94 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:22:23.096130: step 23080, loss = 0.74 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:22:24.362406: step 23090, loss = 0.78 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:22:25.735719: step 23100, loss = 0.69 (932.1 examples/sec; 0.137 sec/batch)
2017-05-08 16:22:26.928633: step 23110, loss = 0.83 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-08 16:22:28.211267: step 23120, loss = 0.77 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:22:29.496208: step 23130, loss = 1.13 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:22:30.797505: step 23140, loss = 0.94 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:22:32.123060: step 23150, loss = 0.84 (965.6 examples/sec; 0.133 sec/batch)
2017-05-08 16:22:33.389731: step 23160, loss = 0.79 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:22:34.648678: step 23170, loss = 0.70 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-08 16:22:35.945806: step 23180, loss = 0.91 (986.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:22:37.251752: step 23190, loss = 0.95 (980.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:22:38.661391: step 23200, loss = 0.93 (908.0 examples/sec; 0.141 sec/batch)
2017-05-08 16:22:39.855405: step 23210, loss = 0.63 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-08 16:22:41.130886: step 23220, loss = 0.90 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:22:42.421739: step 23230, loss = 0.81 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:22:43.689290: step 23240, loss = 0.80 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:22:44.991520: step 23250, loss = 1.02 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:22:46.280815: step 23260, loss = 0.98 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:22:47.585944: step 23270, loss = 0.89 (980.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:22:48.897004: step 23280, loss = 0.81 (976.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:22:50.187822: step 23290, loss = 0.91 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:22:51.561844: step 23300, loss = 0.78 (931.6 examples/sec; 0.137 sec/batch)
2017-05-08 16:22:52.733202: step 23310, loss = 0.60 (1092.7 examples/sec; 0.117 sec/batch)
2017-05-08 16:22:54.012111: step 23320, loss = 0.93 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:22:55.299771: step 23330, loss = 0.80 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:22:56.574982: step 23340, loss = 1.24 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:22:57.864601: step 23350, loss = 1.04 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:22:59.178477: step 23360, loss = 0.99 (974.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:23:00.463943: step 23370, loss = 0.85 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:23:01.771433: step 23380, loss = 0.91 (979.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:23:03.065767: step 23390, loss = 0.87 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:23:04.459135: step 23400, loss = 0.69 (918.6 examples/sec; 0.139 sec/batch)
2017-05-08 16:23:05.670717: step 23410, loss = 1.08 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-08 16:23:06.976557: step 23420, loss = 0.83 (980.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:23:08.264954: step 23430, loss = 0.94 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:23:09.557488: step 23440, loss = 0.90 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:23:10.870829: step 23450, loss = 0.89 (974.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:23:12.175203: step 23460, loss = 0.87 (981.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:23:13.465859: step 23470, loss = 0.82 (991.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:23:14.759683: step 23480, loss = 0.91 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:23:16.083662: step 23490, loss = 1.04 (966.8 examples/sec; 0.132 sec/batch)
2017-05-08 16:23:17.473210: step 23500, loss = 0.75 (921.2 examples/sec; 0.139 sec/batch)
2017-05-08 16:23:18.676507: step 23510, loss = 0.87 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-08 16:23:19.951278: step 23520, loss = 0.81 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:23:21.237260: step 23530, loss = 0.73 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:23:22.510362: step 23540, loss = 0.85 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:23:23.828496: step 23550, loss = 0.83 (971.1 examples/sec; 0.132 sec/batch)
2017-05-08 16:23:25.099290: step 23560, loss = 1.16 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:23:26.423194: step 23570, loss = 0.88 (966.8 examples/sec; 0.132 sec/batch)
2017-05-08 16:23:27.722671: step 23580, loss = 1.06 (985.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:23:29.010892: step 23590, loss = 0.91 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:23:30.392133: step 23600, loss = 0.92 (926.7 examples/sec; 0.138 sec/batch)
2017-05-08 16:23:31.580504: step 23610, loss = 0.77 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-08 16:23:32.861736: step 23620, loss = 0.89 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:23:34.157063: step 23630, loss = 0.76 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:23:35.449174: step 23640, loss = 0.85 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:23:36.734271: step 23650, loss = 0.72 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:23:37.997259: step 23660, loss = 0.96 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-08 16:23:39.281530: step 23670, loss = 0.80 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:23:40.566719: step 23680, loss = 0.87 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:23:41.849351: step 23690, loss = 0.82 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:23:43.227849: step 23700, loss = 0.69 (928.5 examples/sec; 0.138 sec/batch)
2017-05-08 16:23:44.403873: step 23710, loss = 0.88 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-08 16:23:45.671419: step 23720, loss = 0.83 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:23:46.952354: step 23730, loss = 0.79 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:23:48.237028: step 23740, loss = 0.95 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:23:49.528357: step 23750, loss = 0.80 (991.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:23:50.816981: step 23760, loss = 0.85 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:23:52.093931: step 23770, loss = 0.76 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:23:53.362762: step 23780, loss = 0.92 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:23:54.644133: step 23790, loss = 0.96 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:23:56.025014: step 23800, loss = 0.94 (926.9 examples/sec; 0.138 sec/batch)
2017-05-08 16:23:57.238159: step 23810, loss = 0.74 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-08 16:23:58.505600: step 23820, loss = 0.89 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:23:59.767520: step 23830, loss = 0.88 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-08 16:24:01.053923: step 23840, loss = 0.73 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:24:02.340550: step 23850, loss = 0.79 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:24:03.629254: step 23860, loss = 0.82 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:24:04.916112: step 23870, loss = 0.86 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:24:06.185730: step 23880, loss = 0.82 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:24:07.455524: step 23890, loss = 0.89 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:24:08.803073: step 23900, loss = 0.81 (949.9 examples/sec; 0.135 sec/batch)
2017-05-08 16:24:10.005730: step 23910, loss = 0.91 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-08 16:24:11.306936: step 23920, loss = 0.92 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:24:12.591834: step 23930, loss = 0.84 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:24:13.875195: step 23940, loss = 0.64 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:24:15.155697: step 23950, loss = 0.65 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:24:16.437000: step 23960, loss = 0.84 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:24:17.732301: step 23970, loss = 0.85 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:24:19.013089: step 23980, loss = 0.72 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:24:20.272762: step 23990, loss = 1.17 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-08 16:24:21.651324: step 24000, loss = 0.74 (928.5 examples/sec; 0.138 sec/batch)
2017-05-08 16:24:22.844795: step 24010, loss = 1.02 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-08 16:24:24.122723: step 24020, loss = 0.94 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:24:25.402662: step 24030, loss = 0.78 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:24:26.682823: step 24040, loss = 0.80 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:24:27.953143: step 24050, loss = 0.79 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:24:29.245551: step 24060, loss = 0.81 (990.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:24:30.524799: step 24070, loss = 0.90 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:24:31.814923: step 24080, loss = 1.04 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:24:33.092169: step 24090, loss = 0.77 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:24:34.463238: step 24100, loss = 0.91 (933.6 examples/sec; 0.137 sec/batch)
2017-05-08 16:24:35.629861: step 24110, loss = 0.95 (1097.2 examples/sec; 0.117 sec/batch)
2017-05-08 16:24:36.915587: step 24120, loss = 0.77 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:24:38.184322: step 24130, loss = 0.91 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:24:39.479584: step 24140, loss = 0.88 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:24:40.762419: step 24150, loss = 0.74 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:24:42.055023: step 24160, loss = 0.93 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:24:43.371758: step 24170, loss = 0.83 (972.1 examples/sec; 0.132 sec/batch)
2017-05-08 16:24:44.656747: step 24180, loss = 0.94 (996.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:24:45.955261: step 24190, loss = 0.78 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:24:47.365944: step 24200, loss = 0.82 (907.4 examples/sec; 0.141 sec/batch)
2017-05-08 16:24:48.550626: step 24210, loss = 0.79 (1080.5 examples/sec; 0.118 sec/batch)
2017-05-08 16:24:49.829493: step 24220, loss = 0.92 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:24:51.092300: step 24230, loss = 0.79 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-08 16:24:52.380205: step 24240, loss = 1.02 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:24:53.696015: step 24250, loss = 0.77 (972.8 examples/sec; 0.132 sec/batch)
2017-05-08 16:24:54.971025: step 24260, loss = 0.73 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:24:56.257982: step 24270, loss = 0.82 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:24:57.562693: step 24280, loss = 0.88 (981.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:24:58.858059: step 24290, loss = 0.77 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:25:00.262491: step 24300, loss = 0.83 (911.4 examples/sec; 0.140 sec/batch)
2017-05-08 16:25:01.465698: step 24310, loss = 0.73 (1063.8 examples/sec; 0.120 sec/batch)
2017-05-08 16:25:02.781480: step 24320, loss = 0.86 (972.8 examples/sec; 0.132 sec/batch)
2017-05-08 16:25:04.060213: step 24330, loss = 0.83 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:25:05.337067: step 24340, loss = 0.90 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:25:06.619705: step 24350, loss = 0.73 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:25:07.910907: step 24360, loss = 1.14 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:25:09.197060: step 24370, loss = 0.90 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:25:10.469437: step 24380, loss = 0.69 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:25:11.761040: step 24390, loss = 0.81 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:25:13.136023: step 24400, loss = 0.82 (930.9 examples/sec; 0.137 sec/batch)
2017-05-08 16:25:14.343983: step 24410, loss = 0.75 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-08 16:25:15.625119: step 24420, loss = 0.70 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:25:16.914505: step 24430, loss = 0.86 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:25:18.172872: step 24440, loss = 0.71 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-08 16:25:19.458943: step 24450, loss = 0.71 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:25:20.730368: step 24460, loss = 0.90 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:25:22.020222: step 24470, loss = 0.69 (992.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:25:23.283432: step 24480, loss = 0.95 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-08 16:25:24.561653: step 24490, loss = 0.92 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:25:25.916868: step 24500, loss = 1.02 (944.5 examples/sec; 0.136 sec/batch)
2017-05-08 16:25:27.100972: step 24510, loss = 0.84 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-08 16:25:28.384047: step 24520, loss = 0.83 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:25:29.678853: step 24530, loss = 0.65 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:25:30.950352: step 24540, loss = 0.82 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:25:32.235182: step 24550, loss = 0.87 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:25:33.536865: step 24560, loss = 0.90 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:25:34.853365: step 24570, loss = 0.78 (972.3 examples/sec; 0.132 sec/batch)
2017-05-08 16:25:36.135528: step 24580, loss = 0.74 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:25:37.441660: step 24590, loss = 0.78 (980.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:25:38.815858: step 24600, loss = 0.76 (931.5 examples/sec; 0.137 sec/batch)
2017-05-08 16:25:40.007742: step 24610, loss = 0.77 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-08 16:25:41.288074: step 24620, loss = 0.79 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:25:42.570078: step 24630, loss = 0.93 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:25:43.869156: step 24640, loss = 0.82 (985.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:25:45.136094: step 24650, loss = 0.83 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:25:46.416388: step 24660, loss = 0.88 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:25:47.690916: step 24670, loss = 0.88 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:25:49.009583: step 24680, loss = 0.76 (970.7 examples/sec; 0.132 sec/batch)
2017-05-08 16:25:50.329027: step 24690, loss = 1.05 (970.1 examples/sec; 0.132 sec/batch)
2017-05-08 16:25:51.716186: step 24700, loss = 0.96 (922.7 examples/sec; 0.139 sec/batch)
2017-05-08 16:25:52.896789: step 24710, loss = 0.84 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-08 16:25:54.194266: step 24720, loss = 0.68 (986.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:25:55.477099: step 24730, loss = 0.86 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:25:56.785191: step 24740, loss = 0.79 (978.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:25:58.062770: step 24750, loss = 1.08 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:25:59.348397: step 24760, loss = 1.04 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:26:00.659340: step 24770, loss = 1.03 (976.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:26:01.957555: step 24780, loss = 0.84 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:26:03.229513: step 24790, loss = 0.89 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:26:04.615010: step 24800, loss = 1.16 (923.9 examples/sec; 0.139 sec/batch)
2017-05-08 16:26:05.813551: step 24810, loss = 0.81 (1068.0 examples/sec; 0.120 sec/batch)
2017-05-08 16:26:07.118861: step 24820, loss = 0.84 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:26:08.375721: step 24830, loss = 0.86 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-08 16:26:09.643516: step 24840, loss = 0.67 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:26:10.925080: step 24850, loss = 0.91 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:26:12.216763: step 24860, loss = 0.98 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:26:13.512680: step 24870, loss = 0.91 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:26:14.829295: step 24880, loss = 0.80 (972.2 examples/sec; 0.132 sec/batch)
2017-05-08 16:26:16.143341: step 24890, loss = 0.87 (974.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:26:17.524883: step 24900, loss = 0.71 (926.5 examples/sec; 0.138 sec/batch)
2017-05-08 16:26:18.726214: step 24910, loss = 1.02 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-08 16:26:20.003353: step 24920, loss = 0.83 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:26:21.301398: step 24930, loss = 1.04 (986.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:26:22.618806: step 24940, loss = 1.02 (971.6 examples/sec; 0.132 sec/batch)
2017-05-08 16:26:23.923541: step 24950, loss = 0.97 (981.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:26:25.243199: step 24960, loss = 0.77 (969.9 examples/sec; 0.132 sec/batch)
2017-05-08 16:26:26.502205: step 24970, loss = 0.81 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-08 16:26:27.782884: step 24980, loss = 0.72 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:26:29.061087: step 24990, loss = 0.73 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:26:30.429749: step 25000, loss = 0.91 (935.2 examples/sec; 0.137 sec/batch)
2017-05-08 16:26:31.598412: step 25010, loss = 0.93 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-08 16:26:32.893730: step 25020, loss = 0.90 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:26:34.184065: step 25030, loss = 0.77 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:26:35.481162: step 25040, loss = 0.76 (986.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:26:36.763802: step 25050, loss = 0.74 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:26:38.036741: step 25060, loss = 0.79 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:26:39.317599: step 25070, loss = 0.80 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:26:40.596040: step 25080, loss = 0.85 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:26:41.874405: step 25090, loss = 0.67 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:26:43.232533: step 25100, loss = 0.88 (942.5 examples/sec; 0.136 sec/batch)
2017-05-08 16:26:44.438027: step 25110, loss = 0.71 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-08 16:26:45.731792: step 25120, loss = 0.82 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:26:46.998956: step 25130, loss = 1.03 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:26:48.288996: step 25140, loss = 0.71 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:26:49.575537: step 25150, loss = 0.95 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:26:50.882413: step 25160, loss = 0.73 (979.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:26:52.181775: step 25170, loss = 0.79 (985.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:26:53.469338: step 25180, loss = 0.84 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:26:54.757739: step 25190, loss = 0.82 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:26:56.110137: step 25200, loss = 0.71 (946.5 examples/sec; 0.135 sec/batch)
2017-05-08 16:26:57.323950: step 25210, loss = 0.92 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-08 16:26:58.629182: step 25220, loss = 0.93 (980.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:26:59.933182: step 25230, loss = 1.12 (981.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:27:01.225807: step 25240, loss = 0.88 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:27:02.523237: step 25250, loss = 0.79 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:27:03.794077: step 25260, loss = 0.93 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:27:05.060112: step 25270, loss = 0.72 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:27:06.373866: step 25280, loss = 0.83 (974.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:27:07.677623: step 25290, loss = 1.06 (981.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:27:09.054039: step 25300, loss = 0.86 (930.0 examples/sec; 0.138 sec/batch)
2017-05-08 16:27:10.236141: step 25310, loss = 0.77 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-08 16:27:11.538801: step 25320, loss = 0.76 (982.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:27:12.809207: step 25330, loss = 0.73 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:27:14.117484: step 25340, loss = 0.90 (978.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:27:15.399921: step 25350, loss = 0.71 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:27:16.676688: step 25360, loss = 0.80 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:27:17.937655: step 25370, loss = 0.78 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-08 16:27:19.215586: step 25380, loss = 0.73 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:27:20.497795: step 25390, loss = 0.78 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:27:21.877188: step 25400, loss = 0.89 (928.0 examples/sec; 0.138 sec/batch)
2017-05-08 16:27:23.076213: step 25410, loss = 0.82 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-08 16:27:24.349866: step 25420, loss = 0.91 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:27:25.642396: step 25430, loss = 0.85 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:27:26.916492: step 25440, loss = 0.89 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:27:28.179969: step 25450, loss = 0.97 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-08 16:27:29.458346: step 25460, loss = 0.87 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:27:30.751801: step 25470, loss = 1.08 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:27:32.060647: step 25480, loss = 0.77 (978.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:27:33.365213: step 25490, loss = 1.05 (981.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:27:34.708236: step 25500, loss = 0.73 (953.1 examples/sec; 0.134 sec/batch)
2017-05-08 16:27:35.935030: step 25510, loss = 0.89 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-08 16:27:37.232405: step 25520, loss = 0.70 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:27:38.526553: step 25530, loss = 0.87 (989.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:27:39.789029: step 25540, loss = 0.89 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-08 16:27:41.068942: step 25550, loss = 0.77 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:27:42.355712: step 25560, loss = 0.79 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:27:43.638713: step 25570, loss = 0.87 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:27:44.924699: step 25580, loss = 0.80 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:27:46.214986: step 25590, loss = 0.75 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:27:47.558222: step 25600, loss = 0.66 (952.9 examples/sec; 0.134 sec/batch)
2017-05-08 16:27:48.730123: step 25610, loss = 0.90 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-08 16:27:50.041833: step 25620, loss = 1.04 (975.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:27:51.348498: step 25630, loss = 0.72 (979.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:27:52.635238: step 25640, loss = 0.89 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:27:53.923390: step 25650, loss = 0.96 (993.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:27:55.217019: step 25660, loss = 0.83 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:27:56.488977: step 25670, loss = 0.92 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:27:57.778067: step 25680, loss = 0.75 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:27:59.052582: step 25690, loss = 0.80 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:28:00.439770: step 25700, loss = 0.91 (922.7 examples/sec; 0.139 sec/batch)
2017-05-08 16:28:01.628537: step 25710, loss = 0.81 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-08 16:28:02.946644: step 25720, loss = 0.87 (971.1 examples/sec; 0.132 sec/batch)
2017-05-08 16:28:04.216600: step 25730, loss = 0.89 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:28:05.477758: step 25740, loss = 0.78 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-08 16:28:06.747243: step 25750, loss = 0.74 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:28:08.008358: step 25760, loss = 0.89 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-08 16:28:09.309186: step 25770, loss = 1.18 (984.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:28:10.596988: step 25780, loss = 0.90 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:28:11.851388: step 25790, loss = 0.92 (1020.4 examples/sec; 0.125 sec/batch)
2017-05-08 16:28:13.220854: step 25800, loss = 0.72 (934.7 examples/sec; 0.137 sec/batch)
2017-05-08 16:28:14.406705: step 25810, loss = 0.68 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-08 16:28:15.696372: step 25820, loss = 0.95 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:28:16.987413: step 25830, loss = 0.84 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:28:18.252837: step 25840, loss = 0.88 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:28:19.525640: step 25850, loss = 0.74 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:28:20.798413: step 25860, loss = 0.75 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:28:22.085370: step 25870, loss = 0.86 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:28:23.372372: step 25880, loss = 0.76 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:28:24.647147: step 25890, loss = 0.87 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:28:26.026105: step 25900, loss = 0.92 (928.2 examples/sec; 0.138 sec/batch)
2017-05-08 16:28:27.229784: step 25910, loss = 0.94 (1063.4 examples/sec; 0.120 sec/batch)
2017-05-08 16:28:28.513568: step 25920, loss = 0.88 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:28:29.809361: step 25930, loss = 0.81 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:28:31.097627: step 25940, loss = 0.73 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:28:32.358274: step 25950, loss = 0.78 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-08 16:28:33.625925: step 25960, loss = 1.06 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:28:34.906537: step 25970, loss = 0.89 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:28:36.181499: step 25980, loss = 0.85 (1003.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:28:37.461754: step 25990, loss = 0.70 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:28:38.823169: step 26000, loss = 0.67 (940.2 examples/sec; 0.136 sec/batch)
2017-05-08 16:28:40.009378: step 26010, loss = 0.77 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-08 16:28:41.303570: step 26020, loss = 0.76 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:28:42.571226: step 26030, loss = 0.87 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:28:43.848281: step 26040, loss = 0.85 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:28:45.160511: step 26050, loss = 0.86 (975.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:28:46.468397: step 26060, loss = 0.78 (978.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:28:47.757002: step 26070, loss = 0.79 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:28:49.029923: step 26080, loss = 0.92 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:28:50.348731: step 26090, loss = 0.93 (970.6 examples/sec; 0.132 sec/batch)
2017-05-08 16:28:51.714830: step 26100, loss = 0.80 (937.0 examples/sec; 0.137 sec/batch)
2017-05-08 16:28:52.887645: step 26110, loss = 0.95 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-08 16:28:54.186977: step 26120, loss = 0.88 (985.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:28:55.490716: step 26130, loss = 1.03 (981.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:28:56.788904: step 26140, loss = 1.13 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:28:58.094150: step 26150, loss = 0.99 (980.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:28:59.415470: step 26160, loss = 0.64 (968.7 examples/sec; 0.132 sec/batch)
2017-05-08 16:29:00.702794: step 26170, loss = 0.95 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:29:01.993104: step 26180, loss = 0.94 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:29:03.270989: step 26190, loss = 0.75 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:29:04.634946: step 26200, loss = 0.91 (938.4 examples/sec; 0.136 sec/batch)
2017-05-08 16:29:05.826544: step 26210, loss = 0.76 (1074.2 examples/sec; 0.119 sec/batch)
2017-05-08 16:29:07.096501: step 26220, loss = 0.79 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:29:08.385181: step 26230, loss = 0.75 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:29:09.679318: step 26240, loss = 0.71 (989.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:29:10.977551: step 26250, loss = 0.77 (985.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:29:12.260812: step 26260, loss = 0.80 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:29:13.547041: step 26270, loss = 0.76 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:29:14.837847: step 26280, loss = 0.88 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:29:16.112113: step 26290, loss = 0.83 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:29:17.478371: step 26300, loss = 0.83 (936.9 examples/sec; 0.137 sec/batch)
2017-05-08 16:29:18.688959: step 26310, loss = 0.68 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-08 16:29:20.008029: step 26320, loss = 0.87 (970.4 examples/sec; 0.132 sec/batch)
2017-05-08 16:29:21.332664: step 26330, loss = 0.73 (966.3 examples/sec; 0.132 sec/batch)
2017-05-08 16:29:22.638212: step 26340, loss = 0.89 (980.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:29:23.949993: step 26350, loss = 0.95 (975.9 examples/sec; 0.131 sec/batch)
2017-05-08 16:29:25.239226: step 26360, loss = 0.81 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:29:26.530787: step 26370, loss = 0.84 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:29:27.815037: step 26380, loss = 0.80 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:29:29.107803: step 26390, loss = 0.65 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:29:30.494590: step 26400, loss = 1.09 (923.0 examples/sec; 0.139 sec/batch)
2017-05-08 16:29:31.695990: step 26410, loss = 0.88 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-08 16:29:32.981616: step 26420, loss = 0.75 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:29:34.296894: step 26430, loss = 0.93 (973.2 examples/sec; 0.132 sec/batch)
2017-05-08 16:29:35.576928: step 26440, loss = 0.87 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:29:36.874892: step 26450, loss = 0.75 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:29:38.162095: step 26460, loss = 0.74 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:29:39.456723: step 26470, loss = 0.95 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:29:40.731002: step 26480, loss = 0.74 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:29:42.032663: step 26490, loss = 0.83 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:29:43.432335: step 26500, loss = 0.82 (914.5 examples/sec; 0.140 sec/batch)
2017-05-08 16:29:44.642866: step 26510, loss = 0.80 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-08 16:29:45.924319: step 26520, loss = 0.84 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:29:47.219839: step 26530, loss = 0.66 (988.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:29:48.475714: step 26540, loss = 0.74 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-08 16:29:49.760660: step 26550, loss = 0.89 (996.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:29:51.084614: step 26560, loss = 0.92 (966.8 examples/sec; 0.132 sec/batch)
2017-05-08 16:29:52.383025: step 26570, loss = 1.02 (985.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:29:53.680907: step 26580, loss = 1.08 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:29:54.976321: step 26590, loss = 0.77 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:29:56.360683: step 26600, loss = 0.88 (924.6 examples/sec; 0.138 sec/batch)
2017-05-08 16:29:57.556815: step 26610, loss = 0.82 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-08 16:29:58.854920: step 26620, loss = 0.85 (986.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:30:00.165505: step 26630, loss = 0.84 (976.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:30:01.493880: step 26640, loss = 0.91 (963.6 examples/sec; 0.133 sec/batch)
2017-05-08 16:30:02.798150: step 26650, loss = 0.83 (981.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:30:04.089734: step 26660, loss = 0.88 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:30:05.356549: step 26670, loss = 0.83 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:30:06.656934: step 26680, loss = 0.86 (984.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:30:07.955423: step 26690, loss = 0.95 (985.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:30:09.327337: step 26700, loss = 0.82 (933.0 examples/sec; 0.137 sec/batch)
2017-05-08 16:30:10.551972: step 26710, loss = 0.86 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-08 16:30:11.829020: step 26720, loss = 0.85 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:30:13.116734: step 26730, loss = 0.71 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:30:14.416426: step 26740, loss = 0.83 (984.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:30:15.716628: step 26750, loss = 1.28 (984.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:30:17.016198: step 26760, loss = 1.04 (984.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:30:18.325817: step 26770, loss = 0.77 (977.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:30:19.615957: step 26780, loss = 0.84 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:30:20.942937: step 26790, loss = 0.71 (964.6 examples/sec; 0.133 sec/batch)
2017-05-08 16:30:22.321957: step 26800, loss = 0.77 (928.2 examples/sec; 0.138 sec/batch)
2017-05-08 16:30:23.539648: step 26810, loss = 0.95 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-08 16:30:24.820968: step 26820, loss = 0.94 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:30:26.101550: step 26830, loss = 0.68 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:30:27.378873: step 26840, loss = 0.82 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:30:28.669664: step 26850, loss = 0.93 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:30:29.957234: step 26860, loss = 0.81 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:30:31.239260: step 26870, loss = 0.83 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:30:32.511087: step 26880, loss = 0.72 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:30:33.803709: step 26890, loss = 0.76 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:30:35.184264: step 26900, loss = 0.88 (927.2 examples/sec; 0.138 sec/batch)
2017-05-08 16:30:36.394300: step 26910, loss = 1.06 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-08 16:30:37.671116: step 26920, loss = 0.75 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:30:38.988751: step 26930, loss = 0.94 (971.4 examples/sec; 0.132 sec/batch)
2017-05-08 16:30:40.264720: step 26940, loss = 1.03 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:30:41.584981: step 26950, loss = 0.90 (969.5 examples/sec; 0.132 sec/batch)
2017-05-08 16:30:42.866519: step 26960, loss = 0.77 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:30:44.168277: step 26970, loss = 0.79 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:30:45.440834: step 26980, loss = 0.84 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:30:46.737746: step 26990, loss = 0.80 (987.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:30:48.095787: step 27000, loss = 0.74 (942.5 examples/sec; 0.136 sec/batch)
2017-05-08 16:30:49.286732: step 27010, loss = 0.79 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-08 16:30:50.563170: step 27020, loss = 0.83 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:30:51.852577: step 27030, loss = 0.84 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:30:53.126626: step 27040, loss = 0.80 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:30:54.409646: step 27050, loss = 0.76 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:30:55.698484: step 27060, loss = 0.73 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:30:57.013832: step 27070, loss = 0.99 (973.1 examples/sec; 0.132 sec/batch)
2017-05-08 16:30:58.303396: step 27080, loss = 0.75 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:30:59.595685: step 27090, loss = 0.81 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:31:00.953264: step 27100, loss = 0.81 (942.9 examples/sec; 0.136 sec/batch)
2017-05-08 16:31:02.144460: step 27110, loss = 0.80 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-08 16:31:03.411354: step 27120, loss = 0.85 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:31:04.715865: step 27130, loss = 1.07 (981.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:31:06.022823: step 27140, loss = 0.90 (979.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:31:07.292583: step 27150, loss = 0.81 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:31:08.572636: step 27160, loss = 0.82 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:31:09.868487: step 27170, loss = 0.75 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:31:11.168841: step 27180, loss = 0.79 (984.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:31:12.469916: step 27190, loss = 0.88 (983.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:31:13.870253: step 27200, loss = 0.95 (914.1 examples/sec; 0.140 sec/batch)
2017-05-08 16:31:15.055139: step 27210, loss = 0.69 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-08 16:31:16.341148: step 27220, loss = 0.91 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:31:17.653832: step 27230, loss = 0.91 (975.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:31:18.963255: step 27240, loss = 0.99 (977.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:31:20.262845: step 27250, loss = 0.91 (984.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:31:21.567213: step 27260, loss = 0.75 (981.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:31:22.847437: step 27270, loss = 0.77 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:31:24.131565: step 27280, loss = 0.72 (996.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:31:25.410337: step 27290, loss = 0.75 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:31:26.792564: step 27300, loss = 0.85 (926.0 examples/sec; 0.138 sec/batch)
2017-05-08 16:31:27.993280: step 27310, loss = 0.83 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-08 16:31:29.257893: step 27320, loss = 0.68 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 16:31:30.551571: step 27330, loss = 0.93 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:31:31.840171: step 27340, loss = 0.95 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:31:33.158826: step 27350, loss = 0.79 (970.7 examples/sec; 0.132 sec/batch)
2017-05-08 16:31:34.435295: step 27360, loss = 0.89 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:31:35.700064: step 27370, loss = 0.84 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-08 16:31:36.977523: step 27380, loss = 0.70 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:31:38.249719: step 27390, loss = 0.74 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:31:39.631896: step 27400, loss = 0.86 (926.1 examples/sec; 0.138 sec/batch)
2017-05-08 16:31:40.842755: step 27410, loss = 0.91 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-08 16:31:42.137384: step 27420, loss = 0.79 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:31:43.408184: step 27430, loss = 0.74 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:31:44.678681: step 27440, loss = 0.66 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:31:45.961580: step 27450, loss = 0.76 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:31:47.247847: step 27460, loss = 0.71 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:31:48.543812: step 27470, loss = 0.82 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:31:49.836090: step 27480, loss = 0.88 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:31:51.123066: step 27490, loss = 0.74 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:31:52.480294: step 27500, loss = 0.85 (943.1 examples/sec; 0.136 sec/batch)
2017-05-08 16:31:53.671259: step 27510, loss = 0.93 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-08 16:31:54.934883: step 27520, loss = 0.88 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-08 16:31:56.202982: step 27530, loss = 0.87 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:31:57.510544: step 27540, loss = 0.85 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 16:31:58.817900: step 27550, loss = 0.88 (979.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:32:00.104007: step 27560, loss = 0.83 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:32:01.388251: step 27570, loss = 0.95 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:32:02.641983: step 27580, loss = 0.67 (1020.9 examples/sec; 0.125 sec/batch)
2017-05-08 16:32:03.908998: step 27590, loss = 0.79 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:32:05.277309: step 27600, loss = 0.92 (935.5 examples/sec; 0.137 sec/batch)
2017-05-08 16:32:06.464776: step 27610, loss = 0.72 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-08 16:32:07.735665: step 27620, loss = 0.82 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:32:09.023644: step 27630, loss = 0.99 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:32:10.305425: step 27640, loss = 0.89 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:32:11.589098: step 27650, loss = 0.74 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:32:12.864327: step 27660, loss = 0.79 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:32:14.149244: step 27670, loss = 0.93 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:32:15.451357: step 27680, loss = 0.83 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:32:16.731508: step 27690, loss = 0.78 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:32:18.103990: step 27700, loss = 0.70 (932.6 examples/sec; 0.137 sec/batch)
2017-05-08 16:32:19.321210: step 27710, loss = 0.87 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-08 16:32:20.593028: step 27720, loss = 0.73 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:32:21.905055: step 27730, loss = 1.02 (975.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:32:23.224796: step 27740, loss = 0.80 (969.9 examples/sec; 0.132 sec/batch)
2017-05-08 16:32:24.499046: step 27750, loss = 0.97 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:32:25.793840: step 27760, loss = 0.89 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:32:27.072594: step 27770, loss = 0.74 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:32:28.396394: step 27780, loss = 1.00 (966.9 examples/sec; 0.132 sec/batch)
2017-05-08 16:32:29.688108: step 27790, loss = 0.72 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:32:31.062245: step 27800, loss = 0.77 (931.5 examples/sec; 0.137 sec/batch)
2017-05-08 16:32:32.235682: step 27810, loss = 0.78 (1090.8 examples/sec; 0.117 sec/batch)
2017-05-08 16:32:33.522640: step 27820, loss = 0.62 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:32:34.808184: step 27830, loss = 0.73 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:32:36.105350: step 27840, loss = 0.89 (986.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:32:37.372099: step 27850, loss = 0.86 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:32:38.659896: step 27860, loss = 0.66 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:32:39.942866: step 27870, loss = 0.79 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:32:41.222611: step 27880, loss = 1.04 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:32:42.528291: step 27890, loss = 0.78 (980.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:32:43.911619: step 27900, loss = 0.88 (925.3 examples/sec; 0.138 sec/batch)
2017-05-08 16:32:45.117773: step 27910, loss = 0.84 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-08 16:32:46.446010: step 27920, loss = 0.84 (963.7 examples/sec; 0.133 sec/batch)
2017-05-08 16:32:47.730196: step 27930, loss = 0.77 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:32:49.028426: step 27940, loss = 0.89 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:32:50.333990: step 27950, loss = 0.88 (980.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:32:51.619615: step 27960, loss = 0.92 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:32:52.928263: step 27970, loss = 0.81 (978.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:32:54.236131: step 27980, loss = 0.87 (978.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:32:55.553223: step 27990, loss = 0.83 (971.8 examples/sec; 0.132 sec/batch)
2017-05-08 16:32:56.952733: step 28000, loss = 0.97 (914.6 examples/sec; 0.140 sec/batch)
2017-05-08 16:32:58.136624: step 28010, loss = 0.85 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-08 16:32:59.448321: step 28020, loss = 0.77 (975.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:33:00.754391: step 28030, loss = 0.76 (980.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:33:02.062162: step 28040, loss = 0.91 (978.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:33:03.349088: step 28050, loss = 1.16 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:33:04.619005: step 28060, loss = 0.97 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:33:05.925000: step 28070, loss = 0.79 (980.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:33:07.222517: step 28080, loss = 0.71 (986.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:33:08.521196: step 28090, loss = 0.83 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:33:09.931134: step 28100, loss = 0.75 (907.8 examples/sec; 0.141 sec/batch)
2017-05-08 16:33:11.110185: step 28110, loss = 0.93 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-08 16:33:12.415805: step 28120, loss = 0.81 (980.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:33:13.727813: step 28130, loss = 0.84 (975.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:33:15.006544: step 28140, loss = 0.73 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:33:16.292950: step 28150, loss = 0.77 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:33:17.562337: step 28160, loss = 0.93 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:33:18.851502: step 28170, loss = 1.02 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:33:20.147666: step 28180, loss = 0.76 (987.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:33:21.450797: step 28190, loss = 0.83 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:33:22.832165: step 28200, loss = 0.95 (926.6 examples/sec; 0.138 sec/batch)
2017-05-08 16:33:24.061817: step 28210, loss = 0.91 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-08 16:33:25.353494: step 28220, loss = 0.87 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:33:26.674343: step 28230, loss = 0.77 (969.1 examples/sec; 0.132 sec/batch)
2017-05-08 16:33:27.974718: step 28240, loss = 0.81 (984.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:33:29.289128: step 28250, loss = 0.78 (973.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:33:30.600694: step 28260, loss = 0.89 (975.9 examples/sec; 0.131 sec/batch)
2017-05-08 16:33:31.869578: step 28270, loss = 0.77 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:33:33.157360: step 28280, loss = 0.83 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:33:34.428329: step 28290, loss = 0.89 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:33:35.783072: step 28300, loss = 1.01 (944.8 examples/sec; 0.135 sec/batch)
2017-05-08 16:33:36.972027: step 28310, loss = 0.84 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-08 16:33:38.251243: step 28320, loss = 0.75 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:33:39.539255: step 28330, loss = 0.73 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:33:40.833729: step 28340, loss = 0.78 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:33:42.105690: step 28350, loss = 0.98 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:33:43.387029: step 28360, loss = 0.87 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:33:44.662415: step 28370, loss = 0.87 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:33:45.950228: step 28380, loss = 0.80 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:33:47.235339: step 28390, loss = 0.90 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:33:48.600195: step 28400, loss = 0.81 (937.8 examples/sec; 0.136 sec/batch)
2017-05-08 16:33:49.800913: step 28410, loss = 0.76 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-08 16:33:51.085354: step 28420, loss = 0.80 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:33:52.355905: step 28430, loss = 0.76 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:33:53.621965: step 28440, loss = 0.88 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:33:54.905563: step 28450, loss = 0.93 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:33:56.195043: step 28460, loss = 0.96 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:33:57.478157: step 28470, loss = 0.88 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:33:58.763719: step 28480, loss = 0.78 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:34:00.055580: step 28490, loss = 0.66 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:34:01.472799: step 28500, loss = 0.79 (903.2 examples/sec; 0.142 sec/batch)
2017-05-08 16:34:02.671987: step 28510, loss = 0.79 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-08 16:34:03.950759: step 28520, loss = 0.71 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:34:05.224862: step 28530, loss = 0.68 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:34:06.529868: step 28540, loss = 0.76 (980.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:34:07.803608: step 28550, loss = 0.88 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:34:09.084481: step 28560, loss = 0.81 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:34:10.372295: step 28570, loss = 0.84 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:34:11.647651: step 28580, loss = 0.91 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:34:12.919880: step 28590, loss = 0.83 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:34:14.288869: step 28600, loss = 0.81 (935.0 examples/sec; 0.137 sec/batch)
2017-05-08 16:34:15.504776: step 28610, loss = 0.89 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-08 16:34:16.812321: step 28620, loss = 0.92 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 16:34:18.119650: step 28630, loss = 0.86 (979.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:34:19.385418: step 28640, loss = 0.86 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:34:20.657611: step 28650, loss = 0.93 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:34:21.931167: step 28660, loss = 0.81 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:34:23.209760: step 28670, loss = 0.80 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:34:24.485973: step 28680, loss = 0.83 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:34:25.763829: step 28690, loss = 0.86 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:34:27.125035: step 28700, loss = 0.89 (940.3 examples/sec; 0.136 sec/batch)
2017-05-08 16:34:28.327543: step 28710, loss = 1.04 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-08 16:34:29.640639: step 28720, loss = 0.74 (974.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:34:30.924318: step 28730, loss = 0.79 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:34:32.191265: step 28740, loss = 0.66 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:34:33.482030: step 28750, loss = 0.74 (991.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:34:34.761807: step 28760, loss = 0.94 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:34:36.023095: step 28770, loss = 0.83 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-08 16:34:37.297481: step 28780, loss = 0.88 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:34:38.591328: step 28790, loss = 0.75 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:34:39.966319: step 28800, loss = 0.81 (930.9 examples/sec; 0.137 sec/batch)
2017-05-08 16:34:41.160026: step 28810, loss = 0.96 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-08 16:34:42.430357: step 28820, loss = 0.83 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:34:43.695335: step 28830, loss = 0.77 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-08 16:34:44.974821: step 28840, loss = 0.78 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:34:46.251993: step 28850, loss = 0.94 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:34:47.548782: step 28860, loss = 0.97 (987.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:34:48.835995: step 28870, loss = 0.78 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:34:50.142344: step 28880, loss = 0.95 (979.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:34:51.436689: step 28890, loss = 0.74 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:34:52.819705: step 28900, loss = 0.87 (925.5 examples/sec; 0.138 sec/batch)
2017-05-08 16:34:53.992569: step 28910, loss = 0.84 (1091.4 examples/sec; 0.117 sec/batch)
2017-05-08 16:34:55.287031: step 28920, loss = 0.71 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:34:56.568193: step 28930, loss = 0.78 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:34:57.855973: step 28940, loss = 1.01 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:34:59.116178: step 28950, loss = 0.83 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-08 16:35:00.401554: step 28960, loss = 0.83 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:35:01.704552: step 28970, loss = 0.72 (982.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:35:03.000377: step 28980, loss = 0.80 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:35:04.270299: step 28990, loss = 0.91 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:35:05.637835: step 29000, loss = 0.75 (936.0 examples/sec; 0.137 sec/batch)
2017-05-08 16:35:06.828773: step 29010, loss = 0.88 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-08 16:35:08.105428: step 29020, loss = 0.88 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:35:09.426012: step 29030, loss = 0.75 (969.3 examples/sec; 0.132 sec/batch)
2017-05-08 16:35:10.748718: step 29040, loss = 0.83 (967.7 examples/sec; 0.132 sec/batch)
2017-05-08 16:35:12.028518: step 29050, loss = 0.79 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:35:13.335033: step 29060, loss = 0.85 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:35:14.620482: step 29070, loss = 0.92 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:35:15.919780: step 29080, loss = 1.19 (985.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:35:17.216032: step 29090, loss = 0.87 (987.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:35:18.583760: step 29100, loss = 0.75 (935.9 examples/sec; 0.137 sec/batch)
2017-05-08 16:35:19.801190: step 29110, loss = 0.85 (1051.4 examples/sec; 0.122 sec/batch)
2017-05-08 16:35:21.106821: step 29120, loss = 0.85 (980.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:35:22.406682: step 29130, loss = 0.94 (984.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:35:23.724621: step 29140, loss = 1.02 (971.2 examples/sec; 0.132 sec/batch)
2017-05-08 16:35:25.004437: step 29150, loss = 0.87 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:35:26.274889: step 29160, loss = 0.89 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:35:27.552980: step 29170, loss = 0.80 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:35:28.834948: step 29180, loss = 0.85 (998.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:35:30.106179: step 29190, loss = 0.78 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:35:31.475301: step 29200, loss = 0.96 (934.9 examples/sec; 0.137 sec/batch)
2017-05-08 16:35:32.651343: step 29210, loss = 0.84 (1088.4 examples/sec; 0.118 sec/batch)
2017-05-08 16:35:33.941048: step 29220, loss = 0.85 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:35:35.216962: step 29230, loss = 0.94 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:35:36.484130: step 29240, loss = 0.98 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:35:37.761439: step 29250, loss = 0.76 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:35:39.052954: step 29260, loss = 0.88 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:35:40.341239: step 29270, loss = 0.76 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:35:41.638343: step 29280, loss = 0.88 (986.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:35:42.939121: step 29290, loss = 0.91 (984.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:35:44.347475: step 29300, loss = 0.80 (908.9 examples/sec; 0.141 sec/batch)
2017-05-08 16:35:45.526870: step 29310, loss = 0.84 (1085.3 examples/sec; 0.118 sec/batch)
2017-05-08 16:35:46.810863: step 29320, loss = 0.95 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:35:48.070877: step 29330, loss = 0.92 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-08 16:35:49.364312: step 29340, loss = 0.90 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:35:50.649185: step 29350, loss = 0.95 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:35:51.912749: step 29360, loss = 0.76 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-08 16:35:53.170792: step 29370, loss = 1.00 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-08 16:35:54.459849: step 29380, loss = 0.77 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:35:55.742804: step 29390, loss = 0.99 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:35:57.102420: step 29400, loss = 0.75 (941.4 examples/sec; 0.136 sec/batch)
2017-05-08 16:35:58.304748: step 29410, loss = 0.99 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-08 16:35:59.572934: step 29420, loss = 0.80 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:36:00.869081: step 29430, loss = 0.79 (987.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:36:02.142364: step 29440, loss = 0.78 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:36:03.435206: step 29450, loss = 0.80 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:36:04.709914: step 29460, loss = 0.70 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:36:05.999364: step 29470, loss = 0.78 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:36:07.299394: step 29480, loss = 0.94 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:36:08.561322: step 29490, loss = 0.68 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-08 16:36:09.962916: step 29500, loss = 0.72 (913.2 examples/sec; 0.140 sec/batch)
2017-05-08 16:36:11.170748: step 29510, loss = 0.82 (1059.8 examples/sec; 0.121 sec/batch)
2017-05-08 16:36:12.463753: step 29520, loss = 0.85 (989.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:36:13.762781: step 29530, loss = 0.92 (985.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:36:15.046986: step 29540, loss = 0.90 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:36:16.302823: step 29550, loss = 0.86 (1019.2 examples/sec; 0.126 sec/batch)
2017-05-08 16:36:17.612847: step 29560, loss = 0.83 (977.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:36:18.906267: step 29570, loss = 0.80 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:36:20.207013: step 29580, loss = 0.81 (984.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:36:21.479720: step 29590, loss = 0.90 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:36:22.844573: step 29600, loss = 0.95 (937.8 examples/sec; 0.136 sec/batch)
2017-05-08 16:36:24.044693: step 29610, loss = 0.87 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-08 16:36:25.321440: step 29620, loss = 0.76 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:36:26.591113: step 29630, loss = 0.72 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:36:27.861408: step 29640, loss = 0.84 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:36:29.131104: step 29650, loss = 0.82 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:36:30.397138: step 29660, loss = 0.90 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:36:31.674723: step 29670, loss = 0.88 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:36:32.977382: step 29680, loss = 0.79 (982.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:36:34.283214: step 29690, loss = 0.71 (980.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:36:35.680014: step 29700, loss = 0.81 (916.4 examples/sec; 0.140 sec/batch)
2017-05-08 16:36:36.874119: step 29710, loss = 0.85 (1071.9 examples/sec; 0.119 sec/batch)
2017-05-08 16:36:38.180572: step 29720, loss = 0.75 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:36:39.456981: step 29730, loss = 0.96 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:36:40.739196: step 29740, loss = 0.93 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:36:42.017164: step 29750, loss = 0.73 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:36:43.314567: step 29760, loss = 0.96 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:36:44.614353: step 29770, loss = 0.87 (984.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:36:45.926034: step 29780, loss = 0.86 (975.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:36:47.249655: step 29790, loss = 0.99 (967.0 examples/sec; 0.132 sec/batch)
2017-05-08 16:36:48.655847: step 29800, loss = 1.04 (910.3 examples/sec; 0.141 sec/batch)
2017-05-08 16:36:49.877301: step 29810, loss = 0.72 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-08 16:36:51.167673: step 29820, loss = 0.83 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:36:52.461865: step 29830, loss = 0.71 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:36:53.732421: step 29840, loss = 0.76 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:36:55.019037: step 29850, loss = 0.89 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:36:56.289882: step 29860, loss = 0.75 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:36:57.553276: step 29870, loss = 0.81 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-08 16:36:58.865387: step 29880, loss = 0.96 (975.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:37:00.148641: step 29890, loss = 0.60 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:37:01.499612: step 29900, loss = 0.85 (947.5 examples/sec; 0.135 sec/batch)
2017-05-08 16:37:02.678436: step 29910, loss = 0.77 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-08 16:37:03.950718: step 29920, loss = 0.77 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:37:05.251021: step 29930, loss = 0.77 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:37:06.526655: step 29940, loss = 0.80 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:37:07.800661: step 29950, loss = 0.68 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:37:09.094098: step 29960, loss = 0.76 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:37:10.387842: step 29970, loss = 0.91 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:37:11.657825: step 29980, loss = 0.79 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:37:12.945864: step 29990, loss = 0.65 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:37:14.335912: step 30000, loss = 0.93 (920.8 examples/sec; 0.139 sec/batch)
2017-05-08 16:37:15.503509: step 30010, loss = 0.75 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-08 16:37:16.784581: step 30020, loss = 0.89 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:37:18.071152: step 30030, loss = 0.62 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:37:19.367618: step 30040, loss = 0.85 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:37:20.646620: step 30050, loss = 0.86 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:37:21.923022: step 30060, loss = 0.88 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:37:23.199904: step 30070, loss = 0.78 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:37:24.467134: step 30080, loss = 0.66 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:37:25.731380: step 30090, loss = 0.72 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-08 16:37:27.097573: step 30100, loss = 0.69 (936.9 examples/sec; 0.137 sec/batch)
2017-05-08 16:37:28.271254: step 30110, loss = 0.86 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-08 16:37:29.561377: step 30120, loss = 0.89 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:37:30.862924: step 30130, loss = 0.80 (983.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:37:32.141703: step 30140, loss = 0.73 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:37:33.440990: step 30150, loss = 0.84 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:37:34.701832: step 30160, loss = 0.86 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-08 16:37:35.979057: step 30170, loss = 0.76 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:37:37.268550: step 30180, loss = 0.85 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:37:38.551381: step 30190, loss = 0.78 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:37:39.940475: step 30200, loss = 0.78 (921.5 examples/sec; 0.139 sec/batch)
2017-05-08 16:37:41.127873: step 30210, loss = 0.74 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-08 16:37:42.423674: step 30220, loss = 0.72 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:37:43.695623: step 30230, loss = 0.77 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:37:44.961483: step 30240, loss = 0.64 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:37:46.245302: step 30250, loss = 0.85 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:37:47.495807: step 30260, loss = 0.74 (1023.6 examples/sec; 0.125 sec/batch)
2017-05-08 16:37:48.797938: step 30270, loss = 0.81 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:37:50.089462: step 30280, loss = 0.86 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:37:51.380785: step 30290, loss = 0.89 (991.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:37:52.750524: step 30300, loss = 1.07 (934.5 examples/sec; 0.137 sec/batch)
2017-05-08 16:37:53.923567: step 30310, loss = 0.88 (1091.2 examples/sec; 0.117 sec/batch)
2017-05-08 16:37:55.193102: step 30320, loss = 0.83 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:37:56.474543: step 30330, loss = 0.73 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:37:57.779106: step 30340, loss = 0.88 (981.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:37:59.055520: step 30350, loss = 1.00 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:38:00.341489: step 30360, loss = 0.66 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:38:01.633390: step 30370, loss = 0.62 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:38:02.922432: step 30380, loss = 0.77 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:38:04.201305: step 30390, loss = 0.73 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:38:05.597762: step 30400, loss = 0.72 (916.6 examples/sec; 0.140 sec/batch)
2017-05-08 16:38:06.796088: step 30410, loss = 0.96 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-08 16:38:08.103116: step 30420, loss = 1.18 (979.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:38:09.387354: step 30430, loss = 0.82 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:38:10.652722: step 30440, loss = 0.88 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:38:11.921745: step 30450, loss = 0.97 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:38:13.195680: step 30460, loss = 0.71 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:38:14.494427: step 30470, loss = 0.69 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:38:15.758854: step 30480, loss = 0.84 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-08 16:38:17.059140: step 30490, loss = 0.87 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:38:18.430666: step 30500, loss = 0.83 (933.3 examples/sec; 0.137 sec/batch)
2017-05-08 16:38:19.602156: step 30510, loss = 0.66 (1092.6 examples/sec; 0.117 sec/batch)
2017-05-08 16:38:20.885804: step 30520, loss = 0.79 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:38:22.160475: step 30530, loss = 0.87 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:38:23.445274: step 30540, loss = 0.81 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:38:24.732319: step 30550, loss = 0.77 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:38:26.015120: step 30560, loss = 0.93 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:38:27.291091: step 30570, loss = 0.84 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:38:28.576245: step 30580, loss = 0.78 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:38:29.851071: step 30590, loss = 0.81 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:38:31.220709: step 30600, loss = 0.78 (934.6 examples/sec; 0.137 sec/batch)
2017-05-08 16:38:32.411600: step 30610, loss = 0.73 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-08 16:38:33.688024: step 30620, loss = 0.96 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:38:34.975270: step 30630, loss = 0.74 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:38:36.231622: step 30640, loss = 0.77 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-08 16:38:37.511755: step 30650, loss = 0.88 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:38:38.795487: step 30660, loss = 0.82 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:38:40.093378: step 30670, loss = 0.84 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:38:41.358851: step 30680, loss = 0.68 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:38:42.648211: step 30690, loss = 0.74 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:38:44.022619: step 30700, loss = 0.69 (931.3 examples/sec; 0.137 sec/batch)
2017-05-08 16:38:45.213171: step 30710, loss = 0.78 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-08 16:38:46.489361: step 30720, loss = 0.74 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:38:47.746693: step 30730, loss = 0.71 (1018.0 examples/sec; 0.126 sec/batch)
2017-05-08 16:38:49.029436: step 30740, loss = 0.72 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:38:50.334351: step 30750, loss = 0.95 (980.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:38:51.610033: step 30760, loss = 0.81 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:38:52.929990: step 30770, loss = 0.72 (969.7 examples/sec; 0.132 sec/batch)
2017-05-08 16:38:54.212221: step 30780, loss = 0.80 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:38:55.475957: step 30790, loss = 0.68 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-08 16:38:56.859463: step 30800, loss = 0.92 (925.2 examples/sec; 0.138 sec/batch)
2017-05-08 16:38:58.055675: step 30810, loss = 0.81 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-08 16:38:59.353088: step 30820, loss = 0.81 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:39:00.657936: step 30830, loss = 0.71 (981.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:39:01.936555: step 30840, loss = 0.77 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:39:03.222989: step 30850, loss = 0.89 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:39:04.521275: step 30860, loss = 0.74 (985.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:39:05.805547: step 30870, loss = 0.96 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:39:07.119384: step 30880, loss = 0.68 (974.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:39:08.413250: step 30890, loss = 0.80 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:39:09.792105: step 30900, loss = 0.87 (928.3 examples/sec; 0.138 sec/batch)
2017-05-08 16:39:11.031736: step 30910, loss = 0.89 (1032.6 examples/sec; 0.124 sec/batch)
2017-05-08 16:39:12.299745: step 30920, loss = 0.81 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:39:13.582321: step 30930, loss = 0.79 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:39:14.875133: step 30940, loss = 0.72 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:39:16.150270: step 30950, loss = 0.77 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:39:17.433894: step 30960, loss = 0.80 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:39:18.705475: step 30970, loss = 0.68 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:39:19.979369: step 30980, loss = 0.80 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:39:21.265531: step 30990, loss = 0.77 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:39:22.645841: step 31000, loss = 0.92 (927.3 examples/sec; 0.138 sec/batch)
2017-05-08 16:39:23.852718: step 31010, loss = 0.77 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-08 16:39:25.157003: step 31020, loss = 1.05 (981.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:39:26.466270: step 31030, loss = 0.78 (977.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:39:27.750092: step 31040, loss = 0.78 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:39:29.054762: step 31050, loss = 0.87 (981.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:39:30.359945: step 31060, loss = 0.82 (980.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:39:31.646365: step 31070, loss = 0.78 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:39:32.949601: step 31080, loss = 0.84 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:39:34.207322: step 31090, loss = 0.88 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-08 16:39:35.566125: step 31100, loss = 0.64 (942.0 examples/sec; 0.136 sec/batch)
2017-05-08 16:39:36.797862: step 31110, loss = 0.66 (1039.2 examples/sec; 0.123 sec/batch)
2017-05-08 16:39:38.054922: step 31120, loss = 0.76 (1018.2 examples/sec; 0.126 sec/batch)
2017-05-08 16:39:39.334162: step 31130, loss = 0.80 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:39:40.615895: step 31140, loss = 0.69 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:39:41.911846: step 31150, loss = 0.91 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:39:43.181267: step 31160, loss = 0.80 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:39:44.441687: step 31170, loss = 0.77 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-08 16:39:45.722412: step 31180, loss = 0.86 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:39:47.011719: step 31190, loss = 0.77 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:39:48.393045: step 31200, loss = 0.79 (926.6 examples/sec; 0.138 sec/batch)
2017-05-08 16:39:49.598333: step 31210, loss = 0.83 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-08 16:39:50.869526: step 31220, loss = 0.82 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:39:52.138158: step 31230, loss = 0.81 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:39:53.443563: step 31240, loss = 0.73 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:39:54.719919: step 31250, loss = 1.03 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:39:55.996802: step 31260, loss = 0.75 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:39:57.262218: step 31270, loss = 0.74 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:39:58.534990: step 31280, loss = 0.66 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:39:59.811625: step 31290, loss = 0.87 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:40:01.185339: step 31300, loss = 0.79 (931.8 examples/sec; 0.137 sec/batch)
2017-05-08 16:40:02.406950: step 31310, loss = 0.71 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-08 16:40:03.681793: step 31320, loss = 0.74 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:40:04.967280: step 31330, loss = 0.83 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:40:06.248925: step 31340, loss = 0.80 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:40:07.532730: step 31350, loss = 0.73 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:40:08.819863: step 31360, loss = 0.87 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:40:10.118877: step 31370, loss = 0.88 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:40:11.415969: step 31380, loss = 0.59 (986.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:40:12.705076: step 31390, loss = 0.79 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:40:14.125050: step 31400, loss = 0.86 (901.4 examples/sec; 0.142 sec/batch)
2017-05-08 16:40:15.336339: step 31410, loss = 0.90 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-08 16:40:16.632292: step 31420, loss = 0.74 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:40:17.925059: step 31430, loss = 0.91 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:40:19.223063: step 31440, loss = 0.97 (986.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:40:20.511718: step 31450, loss = 0.80 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:40:21.824385: step 31460, loss = 0.80 (975.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:40:23.110732: step 31470, loss = 0.75 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:40:24.389391: step 31480, loss = 0.84 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:40:25.689427: step 31490, loss = 0.98 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:40:27.055736: step 31500, loss = 0.78 (936.8 examples/sec; 0.137 sec/batch)
2017-05-08 16:40:28.242095: step 31510, loss = 0.90 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-08 16:40:29.522662: step 31520, loss = 0.81 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:40:30.802252: step 31530, loss = 0.75 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:40:32.096691: step 31540, loss = 0.63 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:40:33.394882: step 31550, loss = 0.88 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:40:34.681289: step 31560, loss = 0.80 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:40:35.995945: step 31570, loss = 0.84 (973.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:40:37.295827: step 31580, loss = 0.85 (984.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:40:38.598638: step 31590, loss = 1.10 (982.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:40:39.970193: step 31600, loss = 0.73 (933.2 examples/sec; 0.137 sec/batch)
2017-05-08 16:40:41.188550: step 31610, loss = 0.98 (1050.6 examples/sec; 0.122 sec/batch)
2017-05-08 16:40:42.468479: step 31620, loss = 0.71 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:40:43.749100: step 31630, loss = 0.81 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:40:45.022395: step 31640, loss = 0.77 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:40:46.317277: step 31650, loss = 0.78 (988.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:40:47.620567: step 31660, loss = 0.94 (982.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:40:48.895619: step 31670, loss = 0.73 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:40:50.171765: step 31680, loss = 0.81 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:40:51.437782: step 31690, loss = 0.88 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:40:52.804120: step 31700, loss = 0.71 (936.8 examples/sec; 0.137 sec/batch)
2017-05-08 16:40:53.999204: step 31710, loss = 0.62 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-08 16:40:55.284121: step 31720, loss = 0.80 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:40:56.559057: step 31730, loss = 0.89 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:40:57.847104: step 31740, loss = 0.92 (993.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:40:59.116345: step 31750, loss = 0.78 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:41:00.385646: step 31760, loss = 0.81 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:41:01.657551: step 31770, loss = 0.77 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:41:02.968096: step 31780, loss = 1.03 (976.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:41:04.278559: step 31790, loss = 0.87 (976.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:41:05.674281: step 31800, loss = 0.88 (917.1 examples/sec; 0.140 sec/batch)
2017-05-08 16:41:06.849342: step 31810, loss = 0.83 (1089.3 examples/sec; 0.118 sec/batch)
2017-05-08 16:41:08.106530: step 31820, loss = 0.70 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-08 16:41:09.382649: step 31830, loss = 0.85 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:41:10.664540: step 31840, loss = 0.78 (998.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:41:11.949375: step 31850, loss = 1.04 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:41:13.237974: step 31860, loss = 0.77 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:41:14.540787: step 31870, loss = 0.97 (982.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:41:15.811290: step 31880, loss = 0.59 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:41:17.079818: step 31890, loss = 0.63 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:41:18.454097: step 31900, loss = 0.74 (931.4 examples/sec; 0.137 sec/batch)
2017-05-08 16:41:19.640528: step 31910, loss = 0.84 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-08 16:41:20.914290: step 31920, loss = 0.78 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:41:22.180181: step 31930, loss = 0.97 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:41:23.486798: step 31940, loss = 0.91 (979.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:41:24.791219: step 31950, loss = 0.87 (981.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:41:26.057400: step 31960, loss = 1.00 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:41:27.383495: step 31970, loss = 0.84 (965.3 examples/sec; 0.133 sec/batch)
2017-05-08 16:41:28.669859: step 31980, loss = 0.61 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:41:29.941616: step 31990, loss = 0.84 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:41:31.313023: step 32000, loss = 0.82 (933.4 examples/sec; 0.137 sec/batch)
2017-05-08 16:41:32.505694: step 32010, loss = 0.83 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-08 16:41:33.800037: step 32020, loss = 0.85 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:41:35.107537: step 32030, loss = 0.92 (979.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:41:36.380551: step 32040, loss = 0.90 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:41:37.664619: step 32050, loss = 0.76 (996.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:41:38.964432: step 32060, loss = 0.67 (984.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:41:40.255800: step 32070, loss = 0.77 (991.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:41:41.543321: step 32080, loss = 0.77 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:41:42.826624: step 32090, loss = 0.71 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:41:44.195297: step 32100, loss = 0.75 (935.2 examples/sec; 0.137 sec/batch)
2017-05-08 16:41:45.378560: step 32110, loss = 0.77 (1081.8 examples/sec; 0.118 sec/batch)
2017-05-08 16:41:46.655300: step 32120, loss = 0.71 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:41:47.926367: step 32130, loss = 0.88 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:41:49.197073: step 32140, loss = 0.70 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:41:50.473506: step 32150, loss = 0.93 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:41:51.790769: step 32160, loss = 0.82 (971.7 examples/sec; 0.132 sec/batch)
2017-05-08 16:41:53.092240: step 32170, loss = 0.89 (983.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:41:54.386172: step 32180, loss = 0.88 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:41:55.668128: step 32190, loss = 0.77 (998.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:41:57.044721: step 32200, loss = 0.81 (929.8 examples/sec; 0.138 sec/batch)
2017-05-08 16:41:58.253737: step 32210, loss = 0.71 (1058.7 examples/sec; 0.121 sec/batch)
2017-05-08 16:41:59.545905: step 32220, loss = 0.95 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:42:00.827140: step 32230, loss = 0.92 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:42:02.148828: step 32240, loss = 0.84 (968.5 examples/sec; 0.132 sec/batch)
2017-05-08 16:42:03.442383: step 32250, loss = 0.72 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:42:04.738679: step 32260, loss = 0.99 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:42:06.037210: step 32270, loss = 1.03 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:42:07.322380: step 32280, loss = 0.81 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:42:08.620807: step 32290, loss = 0.82 (985.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:42:09.987146: step 32300, loss = 0.71 (936.8 examples/sec; 0.137 sec/batch)
2017-05-08 16:42:11.209322: step 32310, loss = 0.94 (1047.3 examples/sec; 0.122 sec/batch)
2017-05-08 16:42:12.496236: step 32320, loss = 1.05 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:42:13.791362: step 32330, loss = 0.79 (988.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:42:15.083025: step 32340, loss = 0.93 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:42:16.367749: step 32350, loss = 0.72 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:42:17.664347: step 32360, loss = 0.67 (987.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:42:18.938473: step 32370, loss = 0.64 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:42:20.238157: step 32380, loss = 0.80 (984.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:42:21.537415: step 32390, loss = 0.84 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:42:22.891556: step 32400, loss = 0.81 (945.2 examples/sec; 0.135 sec/batch)
2017-05-08 16:42:24.073066: step 32410, loss = 0.64 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-08 16:42:25.351209: step 32420, loss = 0.66 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:42:26.627601: step 32430, loss = 0.78 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:42:27.903115: step 32440, loss = 0.79 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:42:29.187207: step 32450, loss = 0.85 (996.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:42:30.452495: step 32460, loss = 0.80 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:42:31.722952: step 32470, loss = 0.72 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:42:33.007043: step 32480, loss = 0.81 (996.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:42:34.277093: step 32490, loss = 0.85 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:42:35.749262: step 32500, loss = 0.62 (869.5 examples/sec; 0.147 sec/batch)
2017-05-08 16:42:36.873305: step 32510, loss = 0.75 (1138.7 examples/sec; 0.112 sec/batch)
2017-05-08 16:42:38.165939: step 32520, loss = 1.01 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:42:39.455983: step 32530, loss = 0.81 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:42:40.745571: step 32540, loss = 0.87 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:42:42.047760: step 32550, loss = 0.88 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:42:43.339046: step 32560, loss = 0.82 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:42:44.636912: step 32570, loss = 0.98 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:42:45.957936: step 32580, loss = 0.91 (968.9 examples/sec; 0.132 sec/batch)
2017-05-08 16:42:47.262671: step 32590, loss = 0.80 (981.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:42:48.642958: step 32600, loss = 0.85 (927.3 examples/sec; 0.138 sec/batch)
2017-05-08 16:42:49.825536: step 32610, loss = 0.77 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-08 16:42:51.102591: step 32620, loss = 0.92 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:42:52.388276: step 32630, loss = 0.74 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:42:53.678056: step 32640, loss = 0.92 (992.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:42:54.956241: step 32650, loss = 0.69 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:42:56.212588: step 32660, loss = 0.74 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-08 16:42:57.481951: step 32670, loss = 1.04 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:42:58.768326: step 32680, loss = 0.87 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:43:00.040377: step 32690, loss = 0.84 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:43:01.405831: step 32700, loss = 0.97 (937.4 examples/sec; 0.137 sec/batch)
2017-05-08 16:43:02.588861: step 32710, loss = 0.82 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-08 16:43:03.864641: step 32720, loss = 0.78 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:43:05.198683: step 32730, loss = 0.79 (959.5 examples/sec; 0.133 sec/batch)
2017-05-08 16:43:06.499913: step 32740, loss = 0.77 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:43:07.807396: step 32750, loss = 0.83 (979.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:43:09.103884: step 32760, loss = 0.89 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:43:10.394931: step 32770, loss = 0.82 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:43:11.679646: step 32780, loss = 0.92 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:43:12.968649: step 32790, loss = 0.71 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:43:14.343870: step 32800, loss = 0.83 (930.8 examples/sec; 0.138 sec/batch)
2017-05-08 16:43:15.564998: step 32810, loss = 0.90 (1048.2 examples/sec; 0.122 sec/batch)
2017-05-08 16:43:16.834227: step 32820, loss = 0.84 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:43:18.147064: step 32830, loss = 0.79 (975.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:43:19.434667: step 32840, loss = 0.84 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:43:20.745828: step 32850, loss = 0.92 (976.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:43:22.032849: step 32860, loss = 0.77 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:43:23.324080: step 32870, loss = 0.79 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:43:24.589237: step 32880, loss = 0.74 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:43:25.865911: step 32890, loss = 0.60 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:43:27.259580: step 32900, loss = 0.84 (918.4 examples/sec; 0.139 sec/batch)
2017-05-08 16:43:28.455901: step 32910, loss = 0.75 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-08 16:43:29.756813: step 32920, loss = 0.92 (983.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:43:31.044602: step 32930, loss = 0.72 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:43:32.352362: step 32940, loss = 0.98 (978.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:43:33.636759: step 32950, loss = 0.82 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:43:34.920548: step 32960, loss = 0.85 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:43:36.204826: step 32970, loss = 0.80 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:43:37.491044: step 32980, loss = 0.87 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:43:38.763598: step 32990, loss = 0.75 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:43:40.107863: step 33000, loss = 0.80 (952.2 examples/sec; 0.134 sec/batch)
2017-05-08 16:43:41.308092: step 33010, loss = 0.73 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-08 16:43:42.576824: step 33020, loss = 0.99 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:43:43.842231: step 33030, loss = 0.79 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:43:45.130904: step 33040, loss = 0.63 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:43:46.443498: step 33050, loss = 0.99 (975.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:43:47.740678: step 33060, loss = 0.62 (986.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:43:49.055771: step 33070, loss = 1.21 (973.3 examples/sec; 0.132 sec/batch)
2017-05-08 16:43:50.373986: step 33080, loss = 0.87 (971.0 examples/sec; 0.132 sec/batch)
2017-05-08 16:43:51.647043: step 33090, loss = 0.79 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:43:53.049581: step 33100, loss = 0.78 (912.6 examples/sec; 0.140 sec/batch)
2017-05-08 16:43:54.245917: step 33110, loss = 0.77 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-08 16:43:55.525908: step 33120, loss = 0.73 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:43:56.794279: step 33130, loss = 0.84 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:43:58.107894: step 33140, loss = 0.91 (974.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:43:59.384824: step 33150, loss = 1.04 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:44:00.655562: step 33160, loss = 0.65 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:44:01.960985: step 33170, loss = 0.74 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:44:03.254661: step 33180, loss = 0.90 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:44:04.554562: step 33190, loss = 0.82 (984.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:44:05.933835: step 33200, loss = 0.80 (928.0 examples/sec; 0.138 sec/batch)
2017-05-08 16:44:07.176656: step 33210, loss = 0.73 (1029.9 examples/sec; 0.124 sec/batch)
2017-05-08 16:44:08.479187: step 33220, loss = 0.74 (982.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:44:09.805948: step 33230, loss = 0.85 (964.8 examples/sec; 0.133 sec/batch)
2017-05-08 16:44:11.085250: step 33240, loss = 0.91 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:44:12.371771: step 33250, loss = 0.89 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:44:13.670682: step 33260, loss = 0.90 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:44:14.980192: step 33270, loss = 0.71 (977.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:44:16.270417: step 33280, loss = 0.69 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:44:17.560676: step 33290, loss = 0.90 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:44:18.959129: step 33300, loss = 0.79 (915.3 examples/sec; 0.140 sec/batch)
2017-05-08 16:44:20.143481: step 33310, loss = 0.74 (1080.8 examples/sec; 0.118 sec/batch)
2017-05-08 16:44:21.444573: step 33320, loss = 0.95 (983.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:44:22.762863: step 33330, loss = 0.90 (971.0 examples/sec; 0.132 sec/batch)
2017-05-08 16:44:24.022715: step 33340, loss = 0.82 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-08 16:44:25.302630: step 33350, loss = 0.99 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:44:26.577486: step 33360, loss = 0.74 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:44:27.854363: step 33370, loss = 0.79 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:44:29.129022: step 33380, loss = 0.65 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:44:30.414836: step 33390, loss = 0.87 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:44:31.777430: step 33400, loss = 0.89 (939.4 examples/sec; 0.136 sec/batch)
2017-05-08 16:44:32.954333: step 33410, loss = 0.91 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-08 16:44:34.235410: step 33420, loss = 0.79 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:44:35.538319: step 33430, loss = 0.88 (982.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:44:36.807038: step 33440, loss = 0.88 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:44:38.081080: step 33450, loss = 0.77 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:44:39.362676: step 33460, loss = 0.76 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:44:40.635863: step 33470, loss = 0.99 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:44:41.922385: step 33480, loss = 0.70 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:44:43.206249: step 33490, loss = 0.76 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:44:44.572916: step 33500, loss = 0.70 (936.6 examples/sec; 0.137 sec/batch)
2017-05-08 16:44:45.763315: step 33510, loss = 0.76 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-08 16:44:47.036190: step 33520, loss = 0.95 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:44:48.320151: step 33530, loss = 0.83 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:44:49.587126: step 33540, loss = 0.93 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:44:50.877380: step 33550, loss = 0.83 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:44:52.170629: step 33560, loss = 0.91 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:44:53.445759: step 33570, loss = 0.62 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:44:54.744370: step 33580, loss = 0.76 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:44:56.029528: step 33590, loss = 0.89 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:44:57.428770: step 33600, loss = 0.88 (914.8 examples/sec; 0.140 sec/batch)
2017-05-08 16:44:58.627675: step 33610, loss = 0.75 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-08 16:44:59.887850: step 33620, loss = 0.91 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-08 16:45:01.183666: step 33630, loss = 0.88 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:45:02.462741: step 33640, loss = 0.71 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:45:03.745482: step 33650, loss = 0.80 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:45:05.019198: step 33660, loss = 0.96 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:45:06.296516: step 33670, loss = 0.91 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:45:07.583363: step 33680, loss = 0.82 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:45:08.892039: step 33690, loss = 0.81 (978.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:45:10.281503: step 33700, loss = 0.86 (921.3 examples/sec; 0.139 sec/batch)
2017-05-08 16:45:11.474734: step 33710, loss = 0.72 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-08 16:45:12.796888: step 33720, loss = 0.86 (968.1 examples/sec; 0.132 sec/batch)
2017-05-08 16:45:14.081013: step 33730, loss = 0.82 (996.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:45:15.358907: step 33740, loss = 0.77 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:45:16.662481: step 33750, loss = 0.97 (981.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:45:17.954439: step 33760, loss = 0.92 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:45:19.260802: step 33770, loss = 0.65 (979.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:45:20.531126: step 33780, loss = 0.72 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:45:21.793629: step 33790, loss = 0.74 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-08 16:45:23.185053: step 33800, loss = 1.21 (919.9 examples/sec; 0.139 sec/batch)
2017-05-08 16:45:24.351873: step 33810, loss = 0.71 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-08 16:45:25.634716: step 33820, loss = 0.79 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:45:26.929897: step 33830, loss = 0.77 (988.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:45:28.192329: step 33840, loss = 0.71 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-08 16:45:29.473730: step 33850, loss = 0.84 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:45:30.749840: step 33860, loss = 0.78 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:45:32.032646: step 33870, loss = 0.68 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:45:33.326829: step 33880, loss = 0.87 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:45:34.612301: step 33890, loss = 0.91 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:45:35.990832: step 33900, loss = 0.83 (928.5 examples/sec; 0.138 sec/batch)
2017-05-08 16:45:37.177536: step 33910, loss = 0.74 (1078.6 examples/sec; 0.119 sec/batch)
2017-05-08 16:45:38.480947: step 33920, loss = 0.76 (982.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:45:39.743326: step 33930, loss = 0.78 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-08 16:45:41.005724: step 33940, loss = 0.59 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-08 16:45:42.293915: step 33950, loss = 0.96 (993.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:45:43.564349: step 33960, loss = 0.82 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:45:44.850683: step 33970, loss = 0.99 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:45:46.185063: step 33980, loss = 0.78 (959.2 examples/sec; 0.133 sec/batch)
2017-05-08 16:45:47.470608: step 33990, loss = 0.87 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:45:48.837180: step 34000, loss = 0.75 (936.7 examples/sec; 0.137 sec/batch)
2017-05-08 16:45:50.011486: step 34010, loss = 0.70 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-08 16:45:51.290491: step 34020, loss = 0.92 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:45:52.559802: step 34030, loss = 0.69 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:45:53.867920: step 34040, loss = 0.85 (978.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:45:55.158642: step 34050, loss = 0.75 (991.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:45:56.440277: step 34060, loss = 1.00 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:45:57.740486: step 34070, loss = 0.84 (984.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:45:59.038765: step 34080, loss = 0.81 (985.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:46:00.299686: step 34090, loss = 0.69 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-08 16:46:01.701833: step 34100, loss = 0.71 (912.9 examples/sec; 0.140 sec/batch)
2017-05-08 16:46:02.894517: step 34110, loss = 0.83 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-08 16:46:04.156166: step 34120, loss = 0.75 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-08 16:46:05.443428: step 34130, loss = 1.01 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:46:06.726101: step 34140, loss = 0.64 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:46:07.994350: step 34150, loss = 0.87 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:46:09.288212: step 34160, loss = 0.63 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:46:10.556930: step 34170, loss = 0.82 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:46:11.850209: step 34180, loss = 0.76 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:46:13.125088: step 34190, loss = 0.99 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:46:14.512743: step 34200, loss = 0.77 (922.4 examples/sec; 0.139 sec/batch)
2017-05-08 16:46:15.737411: step 34210, loss = 0.82 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-08 16:46:17.042768: step 34220, loss = 0.82 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:46:18.315906: step 34230, loss = 0.81 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:46:19.642227: step 34240, loss = 0.77 (965.1 examples/sec; 0.133 sec/batch)
2017-05-08 16:46:20.911250: step 34250, loss = 0.80 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:46:22.201151: step 34260, loss = 0.70 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:46:23.471929: step 34270, loss = 0.89 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:46:24.751766: step 34280, loss = 0.83 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:46:26.012022: step 34290, loss = 0.86 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-08 16:46:27.402746: step 34300, loss = 0.79 (920.4 examples/sec; 0.139 sec/batch)
2017-05-08 16:46:28.591984: step 34310, loss = 0.77 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-08 16:46:29.882770: step 34320, loss = 0.72 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:46:31.159691: step 34330, loss = 0.79 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:46:32.420878: step 34340, loss = 0.95 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-08 16:46:33.712352: step 34350, loss = 0.58 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:46:35.003959: step 34360, loss = 0.83 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:46:36.273485: step 34370, loss = 0.80 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:46:37.544288: step 34380, loss = 0.91 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:46:38.814539: step 34390, loss = 0.80 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:46:40.186700: step 34400, loss = 0.83 (932.8 examples/sec; 0.137 sec/batch)
2017-05-08 16:46:41.365376: step 34410, loss = 0.74 (1086.0 examples/sec; 0.118 sec/batch)
2017-05-08 16:46:42.658616: step 34420, loss = 0.82 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:46:43.939399: step 34430, loss = 0.74 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:46:45.218606: step 34440, loss = 0.80 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:46:46.508630: step 34450, loss = 0.90 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:46:47.818504: step 34460, loss = 0.82 (977.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:46:49.121718: step 34470, loss = 0.80 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:46:50.424828: step 34480, loss = 0.86 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:46:51.744055: step 34490, loss = 1.20 (970.3 examples/sec; 0.132 sec/batch)
2017-05-08 16:46:53.132898: step 34500, loss = 0.85 (921.6 examples/sec; 0.139 sec/batch)
2017-05-08 16:46:54.339165: step 34510, loss = 0.76 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-08 16:46:55.609624: step 34520, loss = 0.81 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:46:56.918215: step 34530, loss = 0.77 (978.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:46:58.201074: step 34540, loss = 0.72 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:46:59.483818: step 34550, loss = 0.83 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:47:00.757691: step 34560, loss = 0.91 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:47:02.077550: step 34570, loss = 0.84 (969.8 examples/sec; 0.132 sec/batch)
2017-05-08 16:47:03.363165: step 34580, loss = 0.72 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:47:04.671105: step 34590, loss = 0.74 (978.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:47:06.064298: step 34600, loss = 0.84 (918.8 examples/sec; 0.139 sec/batch)
2017-05-08 16:47:07.256414: step 34610, loss = 0.97 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-08 16:47:08.578839: step 34620, loss = 0.79 (967.9 examples/sec; 0.132 sec/batch)
2017-05-08 16:47:09.870050: step 34630, loss = 0.68 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:47:11.176830: step 34640, loss = 0.92 (979.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:47:12.465563: step 34650, loss = 0.76 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:47:13.781912: step 34660, loss = 0.75 (972.4 examples/sec; 0.132 sec/batch)
2017-05-08 16:47:15.080421: step 34670, loss = 0.90 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:47:16.346903: step 34680, loss = 0.80 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:47:17.634220: step 34690, loss = 0.78 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:47:19.002515: step 34700, loss = 0.72 (935.5 examples/sec; 0.137 sec/batch)
2017-05-08 16:47:20.193379: step 34710, loss = 0.76 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-08 16:47:21.475265: step 34720, loss = 0.75 (998.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:47:22.742898: step 34730, loss = 0.89 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:47:24.016011: step 34740, loss = 0.67 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:47:25.305830: step 34750, loss = 0.87 (992.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:47:26.561807: step 34760, loss = 0.91 (1019.1 examples/sec; 0.126 sec/batch)
2017-05-08 16:47:27.826746: step 34770, loss = 0.69 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-08 16:47:29.104003: step 34780, loss = 0.65 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:47:30.384348: step 34790, loss = 0.85 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:47:31.753696: step 34800, loss = 0.75 (934.8 examples/sec; 0.137 sec/batch)
2017-05-08 16:47:32.944869: step 34810, loss = 0.71 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-08 16:47:34.233070: step 34820, loss = 0.65 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:47:35.510056: step 34830, loss = 0.72 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:47:36.795277: step 34840, loss = 0.68 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:47:38.077476: step 34850, loss = 0.76 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:47:39.354334: step 34860, loss = 0.78 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:47:40.631326: step 34870, loss = 0.75 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:47:41.906903: step 34880, loss = 0.86 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:47:43.176098: step 34890, loss = 0.78 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:47:44.540114: step 34900, loss = 0.75 (938.4 examples/sec; 0.136 sec/batch)
2017-05-08 16:47:45.728958: step 34910, loss = 0.80 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-08 16:47:47.010700: step 34920, loss = 0.69 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:47:48.318175: step 34930, loss = 0.95 (979.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:47:49.614905: step 34940, loss = 0.98 (987.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:47:50.918672: step 34950, loss = 0.92 (981.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:47:52.195049: step 34960, loss = 0.89 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:47:53.472705: step 34970, loss = 0.80 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:47:54.781998: step 34980, loss = 0.78 (977.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:47:56.066014: step 34990, loss = 0.74 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:47:57.432104: step 35000, loss = 0.80 (937.0 examples/sec; 0.137 sec/batch)
2017-05-08 16:47:58.625310: step 35010, loss = 0.78 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-08 16:47:59.894593: step 35020, loss = 0.79 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:48:01.171217: step 35030, loss = 0.75 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:48:02.442055: step 35040, loss = 0.80 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:48:03.719058: step 35050, loss = 0.76 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:48:04.987024: step 35060, loss = 0.73 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:48:06.261354: step 35070, loss = 0.90 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:48:07.528641: step 35080, loss = 0.75 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:48:08.807265: step 35090, loss = 0.87 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:48:10.184142: step 35100, loss = 0.94 (929.6 examples/sec; 0.138 sec/batch)
2017-05-08 16:48:11.413295: step 35110, loss = 0.89 (1041.4 examples/sec; 0.123 sec/batch)
2017-05-08 16:48:12.738880: step 35120, loss = 0.93 (965.6 examples/sec; 0.133 sec/batch)
2017-05-08 16:48:14.012086: step 35130, loss = 0.92 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:48:15.313729: step 35140, loss = 0.93 (983.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:48:16.607004: step 35150, loss = 0.89 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:48:17.919571: step 35160, loss = 1.02 (975.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:48:19.205085: step 35170, loss = 0.84 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:48:20.490959: step 35180, loss = 0.85 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:48:21.767441: step 35190, loss = 0.72 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:48:23.141561: step 35200, loss = 0.79 (931.5 examples/sec; 0.137 sec/batch)
2017-05-08 16:48:24.320054: step 35210, loss = 0.87 (1086.1 examples/sec; 0.118 sec/batch)
2017-05-08 16:48:25.611088: step 35220, loss = 0.80 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:48:26.895373: step 35230, loss = 0.87 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:48:28.176638: step 35240, loss = 0.79 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:48:29.462653: step 35250, loss = 0.81 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:48:30.751781: step 35260, loss = 0.93 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:48:32.069075: step 35270, loss = 0.68 (971.7 examples/sec; 0.132 sec/batch)
2017-05-08 16:48:33.382274: step 35280, loss = 0.86 (974.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:48:34.643908: step 35290, loss = 0.80 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-08 16:48:36.042561: step 35300, loss = 0.65 (915.2 examples/sec; 0.140 sec/batch)
2017-05-08 16:48:37.240863: step 35310, loss = 0.84 (1068.2 examples/sec; 0.120 sec/batch)
2017-05-08 16:48:38.530830: step 35320, loss = 1.02 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:48:39.810506: step 35330, loss = 0.68 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:48:41.081376: step 35340, loss = 0.63 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:48:42.346418: step 35350, loss = 0.85 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:48:43.633246: step 35360, loss = 0.92 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:48:44.915331: step 35370, loss = 0.75 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:48:46.182774: step 35380, loss = 0.80 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:48:47.443149: step 35390, loss = 0.75 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-08 16:48:48.824189: step 35400, loss = 0.80 (926.8 examples/sec; 0.138 sec/batch)
2017-05-08 16:48:50.024258: step 35410, loss = 0.73 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-08 16:48:51.339012: step 35420, loss = 0.79 (973.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:48:52.629065: step 35430, loss = 0.91 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:48:53.932326: step 35440, loss = 0.88 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:48:55.217154: step 35450, loss = 0.83 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:48:56.491579: step 35460, loss = 0.70 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:48:57.763963: step 35470, loss = 0.85 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:48:59.037533: step 35480, loss = 0.66 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:49:00.350221: step 35490, loss = 0.85 (975.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:49:01.731413: step 35500, loss = 1.09 (926.7 examples/sec; 0.138 sec/batch)
2017-05-08 16:49:02.951552: step 35510, loss = 0.84 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-08 16:49:04.227975: step 35520, loss = 0.71 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:49:05.512913: step 35530, loss = 0.93 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:49:06.827518: step 35540, loss = 0.84 (973.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:49:08.115732: step 35550, loss = 0.78 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:49:09.425690: step 35560, loss = 0.68 (977.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:49:10.726683: step 35570, loss = 1.04 (983.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:49:12.030251: step 35580, loss = 0.85 (981.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:49:13.304090: step 35590, loss = 0.79 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:49:14.693167: step 35600, loss = 0.96 (921.5 examples/sec; 0.139 sec/batch)
2017-05-08 16:49:15.911649: step 35610, loss = 0.80 (1050.5 examples/sec; 0.122 sec/batch)
2017-05-08 16:49:17.223070: step 35620, loss = 0.80 (976.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:49:18.508022: step 35630, loss = 0.92 (996.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:49:19.789341: step 35640, loss = 0.77 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:49:21.073957: step 35650, loss = 0.71 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:49:22.367634: step 35660, loss = 0.71 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:49:23.676880: step 35670, loss = 0.84 (977.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:49:25.015646: step 35680, loss = 0.66 (956.1 examples/sec; 0.134 sec/batch)
2017-05-08 16:49:26.295349: step 35690, loss = 0.78 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:49:27.675336: step 35700, loss = 0.80 (927.5 examples/sec; 0.138 sec/batch)
2017-05-08 16:49:28.867592: step 35710, loss = 0.88 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-08 16:49:30.168895: step 35720, loss = 0.71 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:49:31.448830: step 35730, loss = 0.69 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:49:32.722478: step 35740, loss = 0.73 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:49:33.991585: step 35750, loss = 0.68 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:49:35.272005: step 35760, loss = 0.65 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:49:36.556149: step 35770, loss = 0.84 (996.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:49:37.848364: step 35780, loss = 0.78 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:49:39.162242: step 35790, loss = 0.89 (974.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:49:40.526205: step 35800, loss = 0.78 (938.4 examples/sec; 0.136 sec/batch)
2017-05-08 16:49:41.720483: step 35810, loss = 0.78 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-08 16:49:43.006538: step 35820, loss = 0.85 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:49:44.276917: step 35830, loss = 0.90 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:49:45.557040: step 35840, loss = 0.87 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:49:46.839196: step 35850, loss = 0.78 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:49:48.116305: step 35860, loss = 0.88 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:49:49.397698: step 35870, loss = 1.03 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:49:50.681157: step 35880, loss = 0.84 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:49:51.951411: step 35890, loss = 0.69 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:49:53.323529: step 35900, loss = 0.64 (932.9 examples/sec; 0.137 sec/batch)
2017-05-08 16:49:54.490076: step 35910, loss = 0.88 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-08 16:49:55.780249: step 35920, loss = 1.03 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:49:57.078355: step 35930, loss = 0.83 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:49:58.392244: step 35940, loss = 0.70 (974.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:49:59.697192: step 35950, loss = 0.85 (980.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:50:01.000671: step 35960, loss = 0.79 (982.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:50:02.288099: step 35970, loss = 0.72 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:50:03.575152: step 35980, loss = 0.95 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:50:04.872176: step 35990, loss = 0.86 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:50:06.249577: step 36000, loss = 0.81 (929.3 examples/sec; 0.138 sec/batch)
2017-05-08 16:50:07.438167: step 36010, loss = 0.97 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-08 16:50:08.712444: step 36020, loss = 0.74 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:50:09.992417: step 36030, loss = 0.66 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:50:11.303194: step 36040, loss = 0.70 (976.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:50:12.569350: step 36050, loss = 0.69 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:50:13.845744: step 36060, loss = 0.77 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:50:15.109378: step 36070, loss = 0.82 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-08 16:50:16.385150: step 36080, loss = 0.88 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:50:17.672628: step 36090, loss = 0.73 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:50:19.036067: step 36100, loss = 0.73 (938.8 examples/sec; 0.136 sec/batch)
2017-05-08 16:50:20.216407: step 36110, loss = 0.84 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-08 16:50:21.504168: step 36120, loss = 0.69 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:50:22.795751: step 36130, loss = 0.97 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:50:24.048071: step 36140, loss = 0.84 (1022.1 examples/sec; 0.125 sec/batch)
2017-05-08 16:50:25.317002: step 36150, loss = 0.84 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:50:26.565705: step 36160, loss = 0.89 (1025.1 examples/sec; 0.125 sec/batch)
2017-05-08 16:50:27.839537: step 36170, loss = 0.90 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:50:29.120614: step 36180, loss = 0.82 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:50:30.385701: step 36190, loss = 0.76 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:50:31.747861: step 36200, loss = 0.77 (939.7 examples/sec; 0.136 sec/batch)
2017-05-08 16:50:32.935858: step 36210, loss = 1.00 (1077.4 examples/sec; 0.119 sec/batch)
2017-05-08 16:50:34.226034: step 36220, loss = 0.91 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:50:35.488197: step 36230, loss = 0.62 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-08 16:50:36.770039: step 36240, loss = 0.65 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:50:38.046361: step 36250, loss = 0.82 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:50:39.318057: step 36260, loss = 0.89 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:50:40.595830: step 36270, loss = 0.86 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:50:41.882329: step 36280, loss = 0.85 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:50:43.163096: step 36290, loss = 0.90 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:50:44.529758: step 36300, loss = 0.76 (936.6 examples/sec; 0.137 sec/batch)
2017-05-08 16:50:45.716752: step 36310, loss = 0.66 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-08 16:50:47.030338: step 36320, loss = 0.79 (974.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:50:48.288409: step 36330, loss = 0.79 (1017.4 examples/sec; 0.126 sec/batch)
2017-05-08 16:50:49.593049: step 36340, loss = 0.78 (981.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:50:50.888888: step 36350, loss = 0.88 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:50:52.204415: step 36360, loss = 0.97 (973.0 examples/sec; 0.132 sec/batch)
2017-05-08 16:50:53.500796: step 36370, loss = 0.73 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:50:54.792039: step 36380, loss = 0.86 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:50:56.079721: step 36390, loss = 0.81 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:50:57.452682: step 36400, loss = 0.93 (932.3 examples/sec; 0.137 sec/batch)
2017-05-08 16:50:58.646067: step 36410, loss = 0.79 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-08 16:50:59.903917: step 36420, loss = 0.69 (1017.6 examples/sec; 0.126 sec/batch)
2017-05-08 16:51:01.170014: step 36430, loss = 1.00 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:51:02.447102: step 36440, loss = 0.82 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:51:03.722029: step 36450, loss = 0.83 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:51:05.014134: step 36460, loss = 0.71 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:51:06.320893: step 36470, loss = 0.88 (979.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:51:07.600932: step 36480, loss = 0.87 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:51:08.860421: step 36490, loss = 0.70 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-08 16:51:10.260685: step 36500, loss = 0.90 (914.1 examples/sec; 0.140 sec/batch)
2017-05-08 16:51:11.426143: step 36510, loss = 0.77 (1098.3 examples/sec; 0.117 sec/batch)
2017-05-08 16:51:12.704055: step 36520, loss = 0.86 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:51:13.994234: step 36530, loss = 0.69 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:51:15.274739: step 36540, loss = 0.80 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:51:16.577883: step 36550, loss = 0.99 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:51:17.893095: step 36560, loss = 0.80 (973.2 examples/sec; 0.132 sec/batch)
2017-05-08 16:51:19.208234: step 36570, loss = 0.76 (973.3 examples/sec; 0.132 sec/batch)
2017-05-08 16:51:20.489303: step 36580, loss = 0.91 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:51:21.799688: step 36590, loss = 1.03 (976.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:51:23.173591: step 36600, loss = 0.77 (931.6 examples/sec; 0.137 sec/batch)
2017-05-08 16:51:24.377528: step 36610, loss = 1.01 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-08 16:51:25.694302: step 36620, loss = 0.82 (972.1 examples/sec; 0.132 sec/batch)
2017-05-08 16:51:26.980584: step 36630, loss = 0.93 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:51:28.264115: step 36640, loss = 0.87 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:51:29.547791: step 36650, loss = 0.97 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:51:30.842892: step 36660, loss = 0.90 (988.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:51:32.128333: step 36670, loss = 1.01 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:51:33.429113: step 36680, loss = 0.80 (984.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:51:34.692570: step 36690, loss = 0.78 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-08 16:51:36.068375: step 36700, loss = 0.86 (930.4 examples/sec; 0.138 sec/batch)
2017-05-08 16:51:37.237689: step 36710, loss = 0.89 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-08 16:51:38.541418: step 36720, loss = 0.79 (981.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:51:39.827949: step 36730, loss = 0.78 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:51:41.120708: step 36740, loss = 0.61 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:51:42.401917: step 36750, loss = 0.77 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:51:43.670767: step 36760, loss = 0.83 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:51:44.953561: step 36770, loss = 0.89 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:51:46.229475: step 36780, loss = 0.79 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:51:47.508161: step 36790, loss = 0.90 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:51:48.883585: step 36800, loss = 0.86 (930.6 examples/sec; 0.138 sec/batch)
2017-05-08 16:51:50.057204: step 36810, loss = 0.67 (1090.6 examples/sec; 0.117 sec/batch)
2017-05-08 16:51:51.342997: step 36820, loss = 0.69 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:51:52.612844: step 36830, loss = 0.79 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:51:53.905370: step 36840, loss = 0.73 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:51:55.189265: step 36850, loss = 1.02 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:51:56.463595: step 36860, loss = 0.72 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:51:57.758836: step 36870, loss = 0.96 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:51:59.051776: step 36880, loss = 0.68 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:52:00.310031: step 36890, loss = 0.72 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-08 16:52:01.677782: step 36900, loss = 1.03 (935.8 examples/sec; 0.137 sec/batch)
2017-05-08 16:52:02.874197: step 36910, loss = 0.72 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-08 16:52:04.152679: step 36920, loss = 0.70 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:52:05.449068: step 36930, loss = 0.72 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:52:06.768304: step 36940, loss = 0.81 (970.3 examples/sec; 0.132 sec/batch)
2017-05-08 16:52:08.055993: step 36950, loss = 0.98 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:52:09.374795: step 36960, loss = 0.86 (970.6 examples/sec; 0.132 sec/batch)
2017-05-08 16:52:10.667012: step 36970, loss = 0.76 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:52:11.930941: step 36980, loss = 0.88 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-08 16:52:13.235132: step 36990, loss = 0.73 (981.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:52:14.615063: step 37000, loss = 0.97 (927.6 examples/sec; 0.138 sec/batch)
2017-05-08 16:52:15.782671: step 37010, loss = 0.85 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-08 16:52:17.089379: step 37020, loss = 0.70 (979.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:52:18.374470: step 37030, loss = 0.98 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:52:19.649662: step 37040, loss = 0.87 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:52:20.923785: step 37050, loss = 0.81 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:52:22.222996: step 37060, loss = 0.92 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:52:23.515730: step 37070, loss = 0.91 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:52:24.789817: step 37080, loss = 0.79 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:52:26.073386: step 37090, loss = 0.82 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:52:27.454140: step 37100, loss = 0.79 (927.0 examples/sec; 0.138 sec/batch)
2017-05-08 16:52:28.660583: step 37110, loss = 0.75 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-08 16:52:29.963889: step 37120, loss = 0.85 (982.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:52:31.265840: step 37130, loss = 0.76 (983.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:52:32.552782: step 37140, loss = 0.79 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:52:33.857131: step 37150, loss = 1.01 (981.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:52:35.139890: step 37160, loss = 0.84 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:52:36.426424: step 37170, loss = 0.76 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:52:37.705487: step 37180, loss = 0.79 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:52:38.975366: step 37190, loss = 0.80 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:52:40.322142: step 37200, loss = 0.82 (950.4 examples/sec; 0.135 sec/batch)
2017-05-08 16:52:41.523627: step 37210, loss = 0.86 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-08 16:52:42.814502: step 37220, loss = 0.75 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:52:44.126026: step 37230, loss = 1.01 (976.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:52:45.422373: step 37240, loss = 0.71 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:52:46.701548: step 37250, loss = 0.74 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:52:47.978696: step 37260, loss = 0.86 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:52:49.282941: step 37270, loss = 0.85 (981.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:52:50.588655: step 37280, loss = 0.79 (980.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:52:51.879516: step 37290, loss = 0.87 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:52:53.281343: step 37300, loss = 0.88 (913.1 examples/sec; 0.140 sec/batch)
2017-05-08 16:52:54.521872: step 37310, loss = 0.89 (1031.8 examples/sec; 0.124 sec/batch)
2017-05-08 16:52:55.814160: step 37320, loss = 0.90 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:52:57.102895: step 37330, loss = 0.78 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:52:58.385677: step 37340, loss = 0.86 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:52:59.690128: step 37350, loss = 0.74 (981.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:53:01.006186: step 37360, loss = 0.87 (972.6 examples/sec; 0.132 sec/batch)
2017-05-08 16:53:02.299503: step 37370, loss = 0.78 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:53:03.600910: step 37380, loss = 0.86 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:53:04.906627: step 37390, loss = 0.77 (980.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:53:06.307087: step 37400, loss = 0.59 (914.0 examples/sec; 0.140 sec/batch)
2017-05-08 16:53:07.505199: step 37410, loss = 0.82 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-08 16:53:08.808215: step 37420, loss = 0.90 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:53:10.101720: step 37430, loss = 0.83 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:53:11.406190: step 37440, loss = 0.78 (981.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:53:12.715025: step 37450, loss = 0.76 (978.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:53:14.007775: step 37460, loss = 0.91 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:53:15.311229: step 37470, loss = 0.81 (982.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:53:16.626440: step 37480, loss = 0.73 (973.2 examples/sec; 0.132 sec/batch)
2017-05-08 16:53:17.925032: step 37490, loss = 0.84 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:53:19.303656: step 37500, loss = 0.74 (928.5 examples/sec; 0.138 sec/batch)
2017-05-08 16:53:20.478257: step 37510, loss = 0.71 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-08 16:53:21.765853: step 37520, loss = 0.81 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:53:23.087339: step 37530, loss = 0.87 (968.6 examples/sec; 0.132 sec/batch)
2017-05-08 16:53:24.380530: step 37540, loss = 0.83 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:53:25.638636: step 37550, loss = 0.70 (1017.4 examples/sec; 0.126 sec/batch)
2017-05-08 16:53:26.922910: step 37560, loss = 0.81 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:53:28.226988: step 37570, loss = 0.91 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:53:29.522162: step 37580, loss = 0.88 (988.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:53:30.801498: step 37590, loss = 0.86 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:53:32.198928: step 37600, loss = 0.84 (916.0 examples/sec; 0.140 sec/batch)
2017-05-08 16:53:33.415986: step 37610, loss = 0.92 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-08 16:53:34.716232: step 37620, loss = 0.90 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:53:36.023583: step 37630, loss = 0.77 (979.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:53:37.290821: step 37640, loss = 0.83 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:53:38.591512: step 37650, loss = 0.68 (984.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:53:39.863655: step 37660, loss = 0.81 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:53:41.145259: step 37670, loss = 0.72 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:53:42.418172: step 37680, loss = 0.94 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:53:43.704299: step 37690, loss = 0.80 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:53:45.059536: step 37700, loss = 0.80 (944.5 examples/sec; 0.136 sec/batch)
2017-05-08 16:53:46.239280: step 37710, loss = 0.89 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-08 16:53:47.514968: step 37720, loss = 0.76 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:53:48.789615: step 37730, loss = 0.82 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:53:50.084317: step 37740, loss = 0.89 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:53:51.381105: step 37750, loss = 0.89 (987.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:53:52.661007: step 37760, loss = 0.91 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:53:53.941655: step 37770, loss = 0.68 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:53:55.212408: step 37780, loss = 0.89 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:53:56.485848: step 37790, loss = 0.66 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:53:57.855923: step 37800, loss = 0.80 (934.3 examples/sec; 0.137 sec/batch)
2017-05-08 16:53:59.067908: step 37810, loss = 0.67 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-08 16:54:00.349007: step 37820, loss = 0.82 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:54:01.664441: step 37830, loss = 0.79 (973.1 examples/sec; 0.132 sec/batch)
2017-05-08 16:54:02.945599: step 37840, loss = 0.96 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:54:04.225432: step 37850, loss = 0.84 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:54:05.511105: step 37860, loss = 0.71 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:54:06.825054: step 37870, loss = 0.71 (974.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:54:08.134810: step 37880, loss = 0.82 (977.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:54:09.444836: step 37890, loss = 0.82 (977.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:54:10.826154: step 37900, loss = 1.02 (926.6 examples/sec; 0.138 sec/batch)
2017-05-08 16:54:12.044349: step 37910, loss = 0.82 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-08 16:54:13.361974: step 37920, loss = 0.80 (971.4 examples/sec; 0.132 sec/batch)
2017-05-08 16:54:14.658533: step 37930, loss = 0.98 (987.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:54:15.948171: step 37940, loss = 0.78 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:54:17.266837: step 37950, loss = 0.76 (970.7 examples/sec; 0.132 sec/batch)
2017-05-08 16:54:18.570140: step 37960, loss = 1.00 (982.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:54:19.842442: step 37970, loss = 0.79 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:54:21.142194: step 37980, loss = 0.67 (984.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:54:22.422529: step 37990, loss = 0.71 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:54:23.796653: step 38000, loss = 0.86 (931.5 examples/sec; 0.137 sec/batch)
2017-05-08 16:54:25.018208: step 38010, loss = 0.72 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-08 16:54:26.297580: step 38020, loss = 0.80 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:54:27.574027: step 38030, loss = 0.97 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:54:28.823936: step 38040, loss = 0.91 (1024.1 examples/sec; 0.125 sec/batch)
2017-05-08 16:54:30.153315: step 38050, loss = 0.82 (962.9 examples/sec; 0.133 sec/batch)
2017-05-08 16:54:31.418292: step 38060, loss = 0.74 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-08 16:54:32.704636: step 38070, loss = 0.79 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:54:33.992113: step 38080, loss = 0.84 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:54:35.282603: step 38090, loss = 0.75 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:54:36.635143: step 38100, loss = 0.88 (946.4 examples/sec; 0.135 sec/batch)
2017-05-08 16:54:37.813027: step 38110, loss = 0.70 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-08 16:54:39.121581: step 38120, loss = 0.82 (978.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:54:40.407230: step 38130, loss = 0.87 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:54:41.679148: step 38140, loss = 0.69 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:54:42.985909: step 38150, loss = 0.69 (979.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:54:44.259023: step 38160, loss = 0.79 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:54:45.564036: step 38170, loss = 0.83 (980.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:54:46.868367: step 38180, loss = 0.90 (981.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:54:48.186662: step 38190, loss = 0.72 (971.0 examples/sec; 0.132 sec/batch)
2017-05-08 16:54:49.584667: step 38200, loss = 0.75 (915.6 examples/sec; 0.140 sec/batch)
2017-05-08 16:54:50.780935: step 38210, loss = 0.83 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-08 16:54:52.058994: step 38220, loss = 0.80 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:54:53.370665: step 38230, loss = 0.83 (975.9 examples/sec; 0.131 sec/batch)
2017-05-08 16:54:54.675690: step 38240, loss = 0.95 (980.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:54:55.974662: step 38250, loss = 0.91 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:54:57.284157: step 38260, loss = 0.80 (977.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:54:58.575160: step 38270, loss = 0.87 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:54:59.881634: step 38280, loss = 1.02 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:55:01.171366: step 38290, loss = 0.80 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:55:02.578586: step 38300, loss = 0.75 (909.6 examples/sec; 0.141 sec/batch)
2017-05-08 16:55:03.751819: step 38310, loss = 0.93 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-08 16:55:05.074076: step 38320, loss = 0.69 (968.0 examples/sec; 0.132 sec/batch)
2017-05-08 16:55:06.351469: step 38330, loss = 0.71 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:55:07.638144: step 38340, loss = 0.78 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:55:08.916379: step 38350, loss = 0.83 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:55:10.193719: step 38360, loss = 0.63 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:55:11.505258: step 38370, loss = 0.76 (976.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:55:12.783294: step 38380, loss = 0.66 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:55:14.101401: step 38390, loss = 0.87 (971.1 examples/sec; 0.132 sec/batch)
2017-05-08 16:55:15.490076: step 38400, loss = 0.74 (921.7 examples/sec; 0.139 sec/batch)
2017-05-08 16:55:16.707895: step 38410, loss = 0.96 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-08 16:55:18.016760: step 38420, loss = 0.90 (978.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:55:19.288265: step 38430, loss = 0.65 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:55:20.566374: step 38440, loss = 0.72 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:55:21.848903: step 38450, loss = 0.75 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:55:23.136253: step 38460, loss = 0.98 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:55:24.403936: step 38470, loss = 0.65 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:55:25.713958: step 38480, loss = 0.80 (977.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:55:26.994256: step 38490, loss = 0.80 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:55:28.389898: step 38500, loss = 0.65 (917.1 examples/sec; 0.140 sec/batch)
2017-05-08 16:55:29.587732: step 38510, loss = 0.73 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-08 16:55:30.880939: step 38520, loss = 1.05 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:55:32.146174: step 38530, loss = 0.88 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:55:33.417874: step 38540, loss = 0.73 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:55:34.678629: step 38550, loss = 0.68 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-08 16:55:35.955829: step 38560, loss = 0.77 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:55:37.238749: step 38570, loss = 0.81 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:55:38.528120: step 38580, loss = 0.77 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:55:39.811641: step 38590, loss = 0.78 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:55:41.185685: step 38600, loss = 0.84 (931.6 examples/sec; 0.137 sec/batch)
2017-05-08 16:55:42.406164: step 38610, loss = 0.87 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-08 16:55:43.728062: step 38620, loss = 0.85 (968.3 examples/sec; 0.132 sec/batch)
2017-05-08 16:55:45.051301: step 38630, loss = 1.01 (967.3 examples/sec; 0.132 sec/batch)
2017-05-08 16:55:46.362667: step 38640, loss = 0.73 (976.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:55:47.667245: step 38650, loss = 0.73 (981.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:55:48.975743: step 38660, loss = 0.85 (978.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:55:50.279651: step 38670, loss = 0.73 (981.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:55:51.576123: step 38680, loss = 0.86 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:55:52.840934: step 38690, loss = 0.86 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-08 16:55:54.203858: step 38700, loss = 0.98 (939.2 examples/sec; 0.136 sec/batch)
2017-05-08 16:55:55.410093: step 38710, loss = 0.76 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-08 16:55:56.708647: step 38720, loss = 0.96 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:55:57.985752: step 38730, loss = 0.75 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:55:59.271007: step 38740, loss = 0.78 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:56:00.560010: step 38750, loss = 0.74 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:56:01.878545: step 38760, loss = 0.76 (970.8 examples/sec; 0.132 sec/batch)
2017-05-08 16:56:03.170698: step 38770, loss = 0.82 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:56:04.483051: step 38780, loss = 0.84 (975.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:56:05.761511: step 38790, loss = 0.95 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:56:07.116203: step 38800, loss = 0.72 (944.9 examples/sec; 0.135 sec/batch)
2017-05-08 16:56:08.312787: step 38810, loss = 0.96 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-08 16:56:09.582811: step 38820, loss = 0.72 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:56:10.879632: step 38830, loss = 0.64 (987.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:56:12.163013: step 38840, loss = 1.09 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:56:13.473208: step 38850, loss = 0.90 (977.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:56:14.734569: step 38860, loss = 0.90 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-08 16:56:16.029218: step 38870, loss = 0.87 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:56:17.332422: step 38880, loss = 0.83 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:56:18.667023: step 38890, loss = 0.89 (959.1 examples/sec; 0.133 sec/batch)
2017-05-08 16:56:20.031129: step 38900, loss = 0.81 (938.3 examples/sec; 0.136 sec/batch)
2017-05-08 16:56:21.225739: step 38910, loss = 0.77 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-08 16:56:22.511518: step 38920, loss = 0.76 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:56:23.785069: step 38930, loss = 0.81 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:56:25.053740: step 38940, loss = 0.97 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:56:26.344435: step 38950, loss = 0.72 (991.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:56:27.614011: step 38960, loss = 0.87 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:56:28.899870: step 38970, loss = 0.72 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:56:30.188188: step 38980, loss = 0.98 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:56:31.486997: step 38990, loss = 0.87 (985.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:56:32.865694: step 39000, loss = 0.81 (928.4 examples/sec; 0.138 sec/batch)
2017-05-08 16:56:34.052073: step 39010, loss = 0.82 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-08 16:56:35.365878: step 39020, loss = 0.76 (974.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:56:36.632621: step 39030, loss = 0.97 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:56:37.904791: step 39040, loss = 0.82 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:56:39.205168: step 39050, loss = 0.88 (984.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:56:40.518334: step 39060, loss = 0.84 (974.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:56:41.801073: step 39070, loss = 0.71 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:56:43.076876: step 39080, loss = 0.76 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:56:44.376887: step 39090, loss = 0.85 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:56:45.761206: step 39100, loss = 0.70 (924.6 examples/sec; 0.138 sec/batch)
2017-05-08 16:56:46.959039: step 39110, loss = 0.76 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-08 16:56:48.270750: step 39120, loss = 0.71 (975.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:56:49.595534: step 39130, loss = 0.83 (966.2 examples/sec; 0.132 sec/batch)
2017-05-08 16:56:50.855091: step 39140, loss = 0.86 (1016.2 examples/sec; 0.126 sec/batch)
2017-05-08 16:56:52.130416: step 39150, loss = 0.92 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:56:53.428703: step 39160, loss = 0.81 (985.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:56:54.735128: step 39170, loss = 0.70 (979.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:56:56.008269: step 39180, loss = 0.71 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:56:57.299618: step 39190, loss = 0.88 (991.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:56:58.685818: step 39200, loss = 0.86 (923.4 examples/sec; 0.139 sec/batch)
2017-05-08 16:56:59.897424: step 39210, loss = 1.00 (1056.4 examples/sec; 0.121 sec/batch)
2017-05-08 16:57:01.214979: step 39220, loss = 0.73 (971.5 examples/sec; 0.132 sec/batch)
2017-05-08 16:57:02.492839: step 39230, loss = 0.84 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:57:03.803235: step 39240, loss = 0.90 (976.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:57:05.115185: step 39250, loss = 1.02 (975.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:57:06.420942: step 39260, loss = 0.82 (980.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:57:07.707800: step 39270, loss = 0.92 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:57:08.980043: step 39280, loss = 0.88 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:57:10.270885: step 39290, loss = 0.86 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:57:11.636956: step 39300, loss = 0.76 (937.0 examples/sec; 0.137 sec/batch)
2017-05-08 16:57:12.830063: step 39310, loss = 0.73 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-08 16:57:14.113378: step 39320, loss = 0.90 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:57:15.380943: step 39330, loss = 0.74 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:57:16.646188: step 39340, loss = 0.71 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:57:17.925490: step 39350, loss = 0.81 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:57:19.189113: step 39360, loss = 0.76 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-08 16:57:20.479714: step 39370, loss = 0.69 (991.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:57:21.752051: step 39380, loss = 0.96 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:57:23.056141: step 39390, loss = 0.75 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:57:24.410798: step 39400, loss = 0.89 (944.9 examples/sec; 0.135 sec/batch)
2017-05-08 16:57:25.602770: step 39410, loss = 0.74 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-08 16:57:26.916604: step 39420, loss = 0.90 (974.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:57:28.191494: step 39430, loss = 0.98 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:57:29.460290: step 39440, loss = 0.79 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:57:30.746281: step 39450, loss = 0.83 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:57:32.007538: step 39460, loss = 0.93 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-08 16:57:33.304290: step 39470, loss = 0.72 (987.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:57:34.593081: step 39480, loss = 0.76 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:57:35.878626: step 39490, loss = 0.68 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:57:37.246606: step 39500, loss = 0.65 (935.7 examples/sec; 0.137 sec/batch)
2017-05-08 16:57:38.417951: step 39510, loss = 0.78 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-08 16:57:39.707150: step 39520, loss = 0.85 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:57:41.010076: step 39530, loss = 0.68 (982.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:57:42.308500: step 39540, loss = 0.77 (985.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:57:43.599757: step 39550, loss = 0.71 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:57:44.871204: step 39560, loss = 0.82 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:57:46.165984: step 39570, loss = 0.83 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:57:47.454658: step 39580, loss = 0.75 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:57:48.725822: step 39590, loss = 0.96 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:57:50.102498: step 39600, loss = 0.81 (929.8 examples/sec; 0.138 sec/batch)
2017-05-08 16:57:51.289060: step 39610, loss = 0.74 (1078.7 examples/sec; 0.119 sec/batch)
2017-05-08 16:57:52.568032: step 39620, loss = 0.74 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:57:53.875126: step 39630, loss = 1.01 (979.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:57:55.148387: step 39640, loss = 0.79 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:57:56.424630: step 39650, loss = 0.88 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:57:57.688953: step 39660, loss = 0.89 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-08 16:57:58.986021: step 39670, loss = 0.85 (986.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:58:00.245027: step 39680, loss = 0.75 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-08 16:58:01.544207: step 39690, loss = 1.04 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:58:02.894393: step 39700, loss = 0.69 (948.0 examples/sec; 0.135 sec/batch)
2017-05-08 16:58:04.088338: step 39710, loss = 0.97 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-08 16:58:05.401712: step 39720, loss = 0.75 (974.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:58:06.722380: step 39730, loss = 0.89 (969.2 examples/sec; 0.132 sec/batch)
2017-05-08 16:58:07.989172: step 39740, loss = 0.68 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:58:09.287209: step 39750, loss = 0.77 (986.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:58:10.605501: step 39760, loss = 1.01 (970.9 examples/sec; 0.132 sec/batch)
2017-05-08 16:58:11.878772: step 39770, loss = 0.79 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:58:13.165432: step 39780, loss = 0.66 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:58:14.426734: step 39790, loss = 0.97 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-08 16:58:15.796814: step 39800, loss = 0.66 (934.3 examples/sec; 0.137 sec/batch)
2017-05-08 16:58:16.972907: step 39810, loss = 0.90 (1088.3 examples/sec; 0.118 sec/batch)
2017-05-08 16:58:18.276370: step 39820, loss = 0.75 (982.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:58:19.555179: step 39830, loss = 0.72 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:58:20.840306: step 39840, loss = 0.77 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:58:22.117495: step 39850, loss = 0.84 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:58:23.414813: step 39860, loss = 0.91 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:58:24.714759: step 39870, loss = 0.83 (984.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:58:26.037660: step 39880, loss = 0.84 (967.6 examples/sec; 0.132 sec/batch)
2017-05-08 16:58:27.350950: step 39890, loss = 0.71 (974.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:58:28.711208: step 39900, loss = 0.64 (941.0 examples/sec; 0.136 sec/batch)
2017-05-08 16:58:29.941416: step 39910, loss = 0.85 (1040.5 examples/sec; 0.123 sec/batch)
2017-05-08 16:58:31.214357: step 39920, loss = 0.80 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:58:32.487884: step 39930, loss = 0.79 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:58:33.780067: step 39940, loss = 0.89 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:58:35.070289: step 39950, loss = 0.70 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:58:36.366512: step 39960, loss = 0.88 (987.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:58:37.675814: step 39970, loss = 0.77 (977.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:58:38.946832: step 39980, loss = 0.75 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:58:40.242107: step 39990, loss = 0.93 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:58:41.628094: step 40000, loss = 0.65 (923.5 examples/sec; 0.139 sec/batch)
2017-05-08 16:58:42.861807: step 40010, loss = 0.85 (1037.5 examples/sec; 0.123 sec/batch)
2017-05-08 16:58:44.148369: step 40020, loss = 1.03 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:58:45.439839: step 40030, loss = 0.76 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:58:46.734327: step 40040, loss = 0.86 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:58:48.034867: step 40050, loss = 1.05 (984.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:58:49.327220: step 40060, loss = 0.72 (990.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:58:50.631015: step 40070, loss = 0.97 (981.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:58:51.924643: step 40080, loss = 0.81 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:58:53.221654: step 40090, loss = 0.94 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:58:54.625398: step 40100, loss = 0.72 (911.8 examples/sec; 0.140 sec/batch)
2017-05-08 16:58:55.830919: step 40110, loss = 0.88 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-08 16:58:57.110296: step 40120, loss = 0.81 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:58:58.410072: step 40130, loss = 0.72 (984.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:58:59.731024: step 40140, loss = 0.79 (969.0 examples/sec; 0.132 sec/batch)
2017-05-08 16:59:01.009165: step 40150, loss = 0.94 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:59:02.298039: step 40160, loss = 0.80 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:59:03.575713: step 40170, loss = 0.84 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:59:04.836138: step 40180, loss = 0.67 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-08 16:59:06.105694: step 40190, loss = 0.84 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:59:07.490823: step 40200, loss = 0.87 (924.1 examples/sec; 0.139 sec/batch)
2017-05-08 16:59:08.682176: step 40210, loss = 0.79 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-08 16:59:09.958655: step 40220, loss = 0.80 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:59:11.258286: step 40230, loss = 0.83 (984.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:59:12.556942: step 40240, loss = 0.81 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:59:13.862439: step 40250, loss = 0.72 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:59:15.148771: step 40260, loss = 0.79 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:59:16.467042: step 40270, loss = 0.86 (971.0 examples/sec; 0.132 sec/batch)
2017-05-08 16:59:17.752421: step 40280, loss = 0.95 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:59:19.048415: step 40290, loss = 0.79 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:59:20.424207: step 40300, loss = 0.85 (930.4 examples/sec; 0.138 sec/batch)
2017-05-08 16:59:21.595795: step 40310, loss = 0.66 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-08 16:59:22.872191: step 40320, loss = 0.80 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:59:24.143886: step 40330, loss = 0.73 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:59:25.453330: step 40340, loss = 0.78 (977.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:59:26.728523: step 40350, loss = 0.91 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:59:28.010187: step 40360, loss = 0.67 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:59:29.301279: step 40370, loss = 0.86 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:59:30.572355: step 40380, loss = 0.78 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:59:31.848627: step 40390, loss = 0.67 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:59:33.235376: step 40400, loss = 0.74 (923.0 examples/sec; 0.139 sec/batch)
2017-05-08 16:59:34.430849: step 40410, loss = 1.00 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-08 16:59:35.707569: step 40420, loss = 0.90 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:59:36.988626: step 40430, loss = 0.77 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:59:38.271773: step 40440, loss = 0.72 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:59:39.555012: step 40450, loss = 0.85 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:59:40.840500: step 40460, loss = 0.75 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:59:42.170064: step 40470, loss = 0.75 (962.7 examples/sec; 0.133 sec/batch)
2017-05-08 16:59:43.466267: step 40480, loss = 1.14 (987.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:59:44.759479: step 40490, loss = 0.88 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:59:46.170602: step 40500, loss = 0.85 (907.1 examples/sec; 0.141 sec/batch)
2017-05-08 16:59:47.334942: step 40510, loss = 0.67 (1099.3 examples/sec; 0.116 sec/batch)
2017-05-08 16:59:48.653566: step 40520, loss = 0.71 (970.7 examples/sec; 0.132 sec/batch)
2017-05-08 16:59:49.954733: step 40530, loss = 0.64 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:59:51.265824: step 40540, loss = 0.89 (976.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:59:52.572886: step 40550, loss = 1.35 (979.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:59:53.897251: step 40560, loss = 0.63 (966.5 examples/sec; 0.132 sec/batch)
2017-05-08 16:59:55.187354: step 40570, loss = 0.88 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:59:56.495156: step 40580, loss = 0.77 (978.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:59:57.801099: step 40590, loss = 0.87 (980.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:59:59.191492: step 40600, loss = 0.87 (920.6 examples/sec; 0.139 sec/batch)
2017-05-08 17:00:00.371852: step 40610, loss = 0.65 (1084.4 examples/sec; 0.118 sec/batch)
2017-05-08 17:00:01.675908: step 40620, loss = 0.63 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 17:00:02.977940: step 40630, loss = 0.75 (983.1 examples/sec; 0.130 sec/batch)
2017-05-08 17:00:04.258499: step 40640, loss = 0.88 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 17:00:05.564027: step 40650, loss = 0.82 (980.4 examples/sec; 0.131 sec/batch)
2017-05-08 17:00:06.887195: step 40660, loss = 0.71 (967.4 examples/sec; 0.132 sec/batch)
2017-05-08 17:00:08.182651: step 40670, loss = 0.86 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 17:00:09.489377: step 40680, loss = 1.06 (979.5 examples/sec; 0.131 sec/batch)
2017-05-08 17:00:10.762672: step 40690, loss = 0.79 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 17:00:12.161246: step 40700, loss = 0.90 (915.2 examples/sec; 0.140 sec/batch)
2017-05-08 17:00:13.361194: step 40710, loss = 0.80 (1066.7 examples/sec; 0.120 sec/batch)
2017-05-08 17:00:14.677767: step 40720, loss = 0.95 (972.2 examples/sec; 0.132 sec/batch)
2017-05-08 17:00:15.969685: step 40730, loss = 0.70 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:00:17.266621: step 40740, loss = 0.77 (987.0 examples/sec; 0.130 sec/batch)
2017-05-08 17:00:18.553235: step 40750, loss = 0.80 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:00:19.813852: step 40760, loss = 0.91 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-08 17:00:21.092415: step 40770, loss = 0.76 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:00:22.378717: step 40780, loss = 0.87 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:00:23.691949: step 40790, loss = 0.66 (974.7 examples/sec; 0.131 sec/batch)
2017-05-08 17:00:25.064432: step 40800, loss = 0.75 (932.6 examples/sec; 0.137 sec/batch)
2017-05-08 17:00:26.261961: step 40810, loss = 0.77 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-08 17:00:27.558315: step 40820, loss = 0.67 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 17:00:28.824347: step 40830, loss = 0.73 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 17:00:30.130657: step 40840, loss = 1.01 (979.9 examples/sec; 0.131 sec/batch)
2017-05-08 17:00:31.426332: step 40850, loss = 0.75 (987.9 examples/sec; 0.130 sec/batch)
2017-05-08 17:00:32.699735: step 40860, loss = 0.83 (1005.2 examples/sec; 0.127 sec/batch)
2017-05-08 17:00:33.987374: step 40870, loss = 0.88 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:00:35.296803: step 40880, loss = 0.71 (977.5 examples/sec; 0.131 sec/batch)
2017-05-08 17:00:36.584652: step 40890, loss = 0.82 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:00:37.961004: step 40900, loss = 0.74 (930.0 examples/sec; 0.138 sec/batch)
2017-05-08 17:00:39.165145: step 40910, loss = 0.77 (1063.0 examples/sec; 0.120 sec/batch)
2017-05-08 17:00:40.416315: step 40920, loss = 0.79 (1023.0 examples/sec; 0.125 sec/batch)
2017-05-08 17:00:41.739518: step 40930, loss = 0.72 (967.3 examples/sec; 0.132 sec/batch)
2017-05-08 17:00:43.038751: step 40940, loss = 0.89 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 17:00:44.313808: step 40950, loss = 0.80 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-08 17:00:45.600481: step 40960, loss = 0.75 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:00:46.915889: step 40970, loss = 0.75 (973.1 examples/sec; 0.132 sec/batch)
2017-05-08 17:00:48.188562: step 40980, loss = 0.75 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-08 17:00:49.471966: step 40990, loss = 0.72 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:00:50.886816: step 41000, loss = 0.79 (904.7 examples/sec; 0.141 sec/batch)
2017-05-08 17:00:52.060411: step 41010, loss = 0.66 (1090.7 examples/sec; 0.117 sec/batch)
2017-05-08 17:00:53.345326: step 41020, loss = 0.85 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 17:00:54.632366: step 41030, loss = 0.88 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:00:55.896175: step 41040, loss = 0.81 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-08 17:00:57.191292: step 41050, loss = 0.72 (988.3 examples/sec; 0.130 sec/batch)
2017-05-08 17:00:58.469156: step 41060, loss = 0.83 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:00:59.736643: step 41070, loss = 0.91 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-08 17:01:01.046091: step 41080, loss = 0.84 (977.5 examples/sec; 0.131 sec/batch)
2017-05-08 17:01:02.333768: step 41090, loss = 0.76 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:01:03.702748: step 41100, loss = 0.97 (935.0 examples/sec; 0.137 sec/batch)
2017-05-08 17:01:04.913428: step 41110, loss = 0.84 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-08 17:01:06.227895: step 41120, loss = 1.03 (973.8 examples/sec; 0.131 sec/batch)
2017-05-08 17:01:07.496187: step 41130, loss = 0.76 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-08 17:01:08.813136: step 41140, loss = 0.84 (971.9 examples/sec; 0.132 sec/batch)
2017-05-08 17:01:10.109390: step 41150, loss = 0.71 (987.5 examples/sec; 0.130 sec/batch)
2017-05-08 17:01:11.389257: step 41160, loss = 0.69 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:01:12.672971: step 41170, loss = 0.79 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:01:13.951274: step 41180, loss = 0.83 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:01:15.250198: step 41190, loss = 0.78 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 17:01:16.613753: step 41200, loss = 0.83 (938.7 examples/sec; 0.136 sec/batch)
2017-05-08 17:01:17.837769: step 41210, loss = 0.71 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-08 17:01:19.150387: step 41220, loss = 0.70 (975.1 examples/sec; 0.131 sec/batch)
2017-05-08 17:01:20.454044: step 41230, loss = 0.78 (981.9 examples/sec; 0.130 sec/batch)
2017-05-08 17:01:21.752095: step 41240, loss = 0.92 (986.1 examples/sec; 0.130 sec/batch)
2017-05-08 17:01:23.044300: step 41250, loss = 0.88 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:01:24.348369: step 41260, loss = 0.83 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 17:01:25.634097: step 41270, loss = 0.91 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:01:26.939840: step 41280, loss = 0.84 (980.3 examples/sec; 0.131 sec/batch)
2017-05-08 17:01:28.221751: step 41290, loss = 0.77 (998.5 examples/sec; 0.128 sec/batch)
2017-05-08 17:01:29.602588: step 41300, loss = 0.92 (927.0 examples/sec; 0.138 sec/batch)
2017-05-08 17:01:30.806635: step 41310, loss = 0.97 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-08 17:01:32.085106: step 41320, loss = 0.83 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 17:01:33.353197: step 41330, loss = 0.87 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-08 17:01:34.627265: step 41340, loss = 0.71 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-08 17:01:35.902743: step 41350, loss = 0.67 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 17:01:37.168206: step 41360, loss = 0.80 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-08 17:01:38.442390: step 41370, loss = 0.87 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 17:01:39.695201: step 41380, loss = 0.95 (1021.7 examples/sec; 0.125 sec/batch)
2017-05-08 17:01:40.988073: step 41390, loss = 0.74 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:01:42.374926: step 41400, loss = 0.85 (923.0 examples/sec; 0.139 sec/batch)
2017-05-08 17:01:43.570307: step 41410, loss = 0.87 (1070.8 examples/sec; 0.120 sec/batch)
2017-05-08 17:01:44.851418: step 41420, loss = 0.80 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:01:46.115638: step 41430, loss = 0.71 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-08 17:01:47.417945: step 41440, loss = 0.65 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 17:01:48.701100: step 41450, loss = 0.76 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 17:01:49.999358: step 41460, loss = 1.06 (985.9 examples/sec; 0.130 sec/batch)
2017-05-08 17:01:51.281115: step 41470, loss = 0.67 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 17:01:52.550356: step 41480, loss = 0.81 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-08 17:01:53.832679: step 41490, loss = 0.85 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 17:01:55.216023: step 41500, loss = 0.65 (925.3 examples/sec; 0.138 sec/batch)
2017-05-08 17:01:56.403662: step 41510, loss = 0.76 (1077.8 examples/sec; 0.119 sec/batch)
2017-05-08 17:01:57.672800: step 41520, loss = 0.92 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-08 17:01:58.967420: step 41530, loss = 0.69 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 17:02:00.260270: step 41540, loss = 0.79 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:02:01.547770: step 41550, loss = 0.74 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 17:02:02.861304: step 41560, loss = 1.01 (974.5 examples/sec; 0.131 sec/batch)
2017-05-08 17:02:04.170233: step 41570, loss = 0.83 (977.9 examples/sec; 0.131 sec/batch)
2017-05-08 17:02:05.469985: step 41580, loss = 0.87 (984.8 examples/sec; 0.130 sec/batch)
2017-05-08 17:02:06.737902: step 41590, loss = 0.90 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-08 17:02:08.085711: step 41600, loss = 0.60 (949.7 examples/sec; 0.135 sec/batch)
2017-05-08 17:02:09.287151: step 41610, loss = 0.77 (1065.4 examples/sec; 0.120 sec/batch)
2017-05-08 17:02:10.576739: step 41620, loss = 0.74 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:02:11.859725: step 41630, loss = 0.77 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:02:13.131403: step 41640, loss = 0.75 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-08 17:02:14.418887: step 41650, loss = 0.82 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 17:02:15.724696: step 41660, loss = 0.83 (980.2 examples/sec; 0.131 sec/batch)
2017-05-08 17:02:17.030864: step 41670, loss = 0.89 (980.0 examples/sec; 0.131 sec/batch)
2017-05-08 17:02:18.349744: step 41680, loss = 0.89 (970.5 examples/sec; 0.132 sec/batch)
2017-05-08 17:02:19.631685: step 41690, loss = 0.85 (998.5 examples/sec; 0.128 sec/batch)
2017-05-08 17:02:21.015381: step 41700, loss = 0.70 (925.1 examples/sec; 0.138 sec/batch)
2017-05-08 17:02:22.196952: step 41710, loss = 0.71 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-08 17:02:23.469249: step 41720, loss = 0.89 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-08 17:02:24.745819: step 41730, loss = 0.67 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:02:26.019409: step 41740, loss = 0.75 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-08 17:02:27.302623: step 41750, loss = 0.89 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 17:02:28.571080: step 41760, loss = 0.99 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-08 17:02:29.851391: step 41770, loss = 0.75 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:02:31.131848: step 41780, loss = 0.79 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 17:02:32.398746: step 41790, loss = 0.68 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-08 17:02:33.757291: step 41800, loss = 0.93 (942.2 examples/sec; 0.136 sec/batch)
2017-05-08 17:02:34.958094: step 41810, loss = 0.78 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-08 17:02:36.258687: step 41820, loss = 0.70 (984.2 examples/sec; 0.130 sec/batch)
2017-05-08 17:02:37.531214: step 41830, loss = 0.71 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-08 17:02:38.824549: step 41840, loss = 0.67 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 17:02:40.123491: step 41850, loss = 0.73 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 17:02:41.456558: step 41860, loss = 0.80 (960.2 examples/sec; 0.133 sec/batch)
2017-05-08 17:02:42.735295: step 41870, loss = 0.87 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 17:02:44.020220: step 41880, loss = 0.86 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 17:02:45.331487: step 41890, loss = 0.78 (976.2 examples/sec; 0.131 sec/batch)
2017-05-08 17:02:46.710152: step 41900, loss = 0.74 (928.4 examples/sec; 0.138 sec/batch)
2017-05-08 17:02:47.913719: step 41910, loss = 0.82 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-08 17:02:49.238410: step 41920, loss = 1.05 (966.3 examples/sec; 0.132 sec/batch)
2017-05-08 17:02:50.533007: step 41930, loss = 0.89 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 17:02:51.837331: step 41940, loss = 0.74 (981.4 examples/sec; 0.130 sec/batch)
2017-05-08 17:02:53.237437: step 41950, loss = 0.69 (914.2 examples/sec; 0.140 sec/batch)
2017-05-08 17:02:54.508028: step 41960, loss = 0.80 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-08 17:02:55.791731: step 41970, loss = 0.89 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:02:57.078854: step 41980, loss = 0.85 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:02:58.366679: step 41990, loss = 0.76 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:02:59.827782: step 42000, loss = 0.74 (876.1 examples/sec; 0.146 sec/batch)
2017-05-08 17:03:01.032102: step 42010, loss = 0.79 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-08 17:03:02.313379: step 42020, loss = 0.85 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 17:03:03.579610: step 42030, loss = 0.82 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-08 17:03:04.867937: step 42040, loss = 0.77 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:03:06.145875: step 42050, loss = 0.82 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 17:03:07.419050: step 42060, loss = 0.74 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 17:03:08.710525: step 42070, loss = 0.73 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:03:09.988940: step 42080, loss = 0.66 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 17:03:11.250986: step 42090, loss = 0.92 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-08 17:03:12.625011: step 42100, loss = 0.78 (931.6 examples/sec; 0.137 sec/batch)
2017-05-08 17:03:13.823576: step 42110, loss = 0.78 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-08 17:03:15.105671: step 42120, loss = 0.82 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:03:16.404995: step 42130, loss = 0.92 (985.1 examples/sec; 0.130 sec/batch)
2017-05-08 17:03:17.698347: step 42140, loss = 0.93 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 17:03:19.026300: step 42150, loss = 0.69 (963.9 examples/sec; 0.133 sec/batch)
2017-05-08 17:03:20.320699: step 42160, loss = 0.88 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:03:21.613715: step 42170, loss = 0.83 (989.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:03:22.902730: step 42180, loss = 0.86 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:03:24.193928: step 42190, loss = 0.74 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 17:03:25.585597: step 42200, loss = 0.79 (919.8 examples/sec; 0.139 sec/batch)
2017-05-08 17:03:26.764551: step 42210, loss = 0.75 (1085.7 examples/sec; 0.118 sec/batch)
2017-05-08 17:03:28.059016: step 42220, loss = 0.76 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:03:29.363435: step 42230, loss = 0.85 (981.3 examples/sec; 0.130 sec/batch)
2017-05-08 17:03:30.682565: step 42240, loss = 0.85 (970.3 examples/sec; 0.132 sec/batch)
2017-05-08 17:03:31.984780: step 42250, loss = 0.96 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 17:03:33.293696: step 42260, loss = 0.80 (977.9 examples/sec; 0.131 sec/batch)
2017-05-08 17:03:34.591807: step 42270, loss = 0.67 (986.1 examples/sec; 0.130 sec/batch)
2017-05-08 17:03:35.875082: step 42280, loss = 0.92 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 17:03:37.172602: step 42290, loss = 0.81 (986.5 examples/sec; 0.130 sec/batch)
2017-05-08 17:03:38.579044: step 42300, loss = 0.95 (910.1 examples/sec; 0.141 sec/batch)
2017-05-08 17:03:39.762068: step 42310, loss = 0.62 (1082.0 examples/sec; 0.118 sec/batch)
2017-05-08 17:03:41.013489: step 42320, loss = 0.78 (1022.8 examples/sec; 0.125 sec/batch)
2017-05-08 17:03:42.317893: step 42330, loss = 0.80 (981.3 examples/sec; 0.130 sec/batch)
2017-05-08 17:03:43.588011: step 42340, loss = 0.67 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-08 17:03:44.884700: step 42350, loss = 0.72 (987.1 examples/sec; 0.130 sec/batch)
2017-05-08 17:03:46.177924: step 42360, loss = 0.83 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:03:47.502061: step 42370, loss = 0.91 (966.7 examples/sec; 0.132 sec/batch)
2017-05-08 17:03:48.787615: step 42380, loss = 0.77 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 17:03:50.080386: step 42390, loss = 0.72 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:03:51.454097: step 42400, loss = 0.87 (931.8 examples/sec; 0.137 sec/batch)
2017-05-08 17:03:52.640532: step 42410, loss = 0.68 (1078.9 examples/sec; 0.119 sec/batch)
2017-05-08 17:03:53.933162: step 42420, loss = 0.62 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 17:03:55.207709: step 42430, loss = 0.82 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 17:03:56.490657: step 42440, loss = 0.77 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:03:57.764653: step 42450, loss = 0.73 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-08 17:03:59.033961: step 42460, loss = 0.84 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-08 17:04:00.293599: step 42470, loss = 0.74 (1016.2 examples/sec; 0.126 sec/batch)
2017-05-08 17:04:01.585478: step 42480, loss = 1.03 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:04:02.879362: step 42490, loss = 0.93 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 17:04:04.252317: step 42500, loss = 0.97 (932.3 examples/sec; 0.137 sec/batch)
2017-05-08 17:04:05.447332: step 42510, loss = 0.86 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-08 17:04:06.731808: step 42520, loss = 0.75 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 17:04:08.047670: step 42530, loss = 0.78 (972.7 examples/sec; 0.132 sec/batch)
2017-05-08 17:04:09.363230: step 42540, loss = 0.79 (973.0 examples/sec; 0.132 sec/batch)
2017-05-08 17:04:10.639249: step 42550, loss = 1.04 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:04:11.959126: step 42560, loss = 0.85 (969.8 examples/sec; 0.132 sec/batch)
2017-05-08 17:04:13.236272: step 42570, loss = 0.83 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 17:04:14.505613: step 42580, loss = 0.71 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-08 17:04:15.764105: step 42590, loss = 0.68 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-08 17:04:17.146926: step 42600, loss = 0.79 (925.6 examples/sec; 0.138 sec/batch)
2017-05-08 17:04:18.334749: step 42610, loss = 0.72 (1077.6 examples/sec; 0.119 sec/batch)
2017-05-08 17:04:19.625127: step 42620, loss = 0.90 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:04:20.911914: step 42630, loss = 0.77 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 17:04:22.224727: step 42640, loss = 0.62 (975.0 examples/sec; 0.131 sec/batch)
2017-05-08 17:04:23.506205: step 42650, loss = 0.82 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 17:04:24.757641: step 42660, loss = 0.68 (1022.8 examples/sec; 0.125 sec/batch)
2017-05-08 17:04:26.033555: step 42670, loss = 0.56 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-08 17:04:27.318965: step 42680, loss = 0.77 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:04:28.596286: step 42690, loss = 0.70 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:04:29.944289: step 42700, loss = 0.65 (949.6 examples/sec; 0.135 sec/batch)
2017-05-08 17:04:31.126479: step 42710, loss = 0.66 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-08 17:04:32.420499: step 42720, loss = 0.85 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 17:04:33.708788: step 42730, loss = 0.86 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:04:34.985723: step 42740, loss = 0.69 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:04:36.266548: step 42750, loss = 0.73 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:04:37.563543: step 42760, loss = 0.79 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 17:04:38.859904: step 42770, loss = 0.74 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 17:04:40.129254: step 42780, loss = 0.79 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-08 17:04:41.412389: step 42790, loss = 0.71 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 17:04:42.807995: step 42800, loss = 0.78 (917.2 examples/sec; 0.140 sec/batch)
2017-05-08 17:04:43.982419: step 42810, loss = 0.87 (1089.9 examples/sec; 0.117 sec/batch)
2017-05-08 17:04:45.271134: step 42820, loss = 0.82 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 17:04:46.593631: step 42830, loss = 0.79 (967.9 examples/sec; 0.132 sec/batch)
2017-05-08 17:04:47.877357: step 42840, loss = 0.71 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:04:49.183777: step 42850, loss = 0.89 (979.8 examples/sec; 0.131 sec/batch)
2017-05-08 17:04:50.464630: step 42860, loss = 0.72 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:04:51.738662: step 42870, loss = 0.84 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-08 17:04:53.069339: step 42880, loss = 0.97 (961.9 examples/sec; 0.133 sec/batch)
2017-05-08 17:04:54.337899: step 42890, loss = 0.73 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-08 17:04:55.737575: step 42900, loss = 0.91 (914.5 examples/sec; 0.140 sec/batch)
2017-05-08 17:04:56.927025: step 42910, loss = 0.61 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-08 17:04:58.263953: step 42920, loss = 0.85 (957.4 examples/sec; 0.134 sec/batch)
2017-05-08 17:04:59.566622: step 42930, loss = 0.86 (982.6 examples/sec; 0.130 sec/batch)
2017-05-08 17:05:00.845043: step 42940, loss = 0.69 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 17:05:02.125868: step 42950, loss = 0.81 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:05:03.443404: step 42960, loss = 0.90 (971.5 examples/sec; 0.132 sec/batch)
2017-05-08 17:05:04.803881: step 42970, loss = 0.96 (940.8 examples/sec; 0.136 sec/batch)
2017-05-08 17:05:06.137393: step 42980, loss = 0.62 (959.9 examples/sec; 0.133 sec/batch)
2017-05-08 17:05:07.445925: step 42990, loss = 0.90 (978.2 examples/sec; 0.131 sec/batch)
2017-05-08 17:05:08.806348: step 43000, loss = 0.79 (940.9 examples/sec; 0.136 sec/batch)
2017-05-08 17:05:09.991726: step 43010, loss = 0.65 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-08 17:05:11.261563: step 43020, loss = 0.88 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-08 17:05:12.552621: step 43030, loss = 0.75 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 17:05:13.843191: step 43040, loss = 0.69 (991.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:05:15.151658: step 43050, loss = 0.87 (978.2 examples/sec; 0.131 sec/batch)
2017-05-08 17:05:16.462648: step 43060, loss = 1.06 (976.4 examples/sec; 0.131 sec/batch)
2017-05-08 17:05:17.763322: step 43070, loss = 0.65 (984.1 examples/sec; 0.130 sec/batch)
2017-05-08 17:05:19.036587: step 43080, loss = 0.75 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 17:05:20.371955: step 43090, loss = 0.87 (958.5 examples/sec; 0.134 sec/batch)
2017-05-08 17:05:21.783715: step 43100, loss = 0.72 (906.7 examples/sec; 0.141 sec/batch)
2017-05-08 17:05:22.964378: step 43110, loss = 0.72 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-08 17:05:24.299659: step 43120, loss = 0.65 (958.6 examples/sec; 0.134 sec/batch)
2017-05-08 17:05:25.619518: step 43130, loss = 0.78 (969.8 examples/sec; 0.132 sec/batch)
2017-05-08 17:05:26.947293: step 43140, loss = 0.87 (964.0 examples/sec; 0.133 sec/batch)
2017-05-08 17:05:28.261946: step 43150, loss = 0.79 (973.6 examples/sec; 0.131 sec/batch)
2017-05-08 17:05:29.565126: step 43160, loss = 0.71 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 17:05:30.859511: step 43170, loss = 0.97 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:05:32.184896: step 43180, loss = 0.90 (965.8 examples/sec; 0.133 sec/batch)
2017-05-08 17:05:33.472510: step 43190, loss = 0.74 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:05:34.852053: step 43200, loss = 0.80 (927.8 examples/sec; 0.138 sec/batch)
2017-05-08 17:05:36.039552: step 43210, loss = 0.81 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-08 17:05:37.398594: step 43220, loss = 0.65 (941.8 examples/sec; 0.136 sec/batch)
2017-05-08 17:05:38.685594: step 43230, loss = 0.70 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:05:39.947934: step 43240, loss = 0.85 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-08 17:05:41.265626: step 43250, loss = 0.98 (971.4 examples/sec; 0.132 sec/batch)
2017-05-08 17:05:42.596993: step 43260, loss = 0.85 (961.4 examples/sec; 0.133 sec/batch)
2017-05-08 17:05:43.911551: step 43270, loss = 0.96 (973.7 examples/sec; 0.131 sec/batch)
2017-05-08 17:05:45.212608: step 43280, loss = 0.85 (983.8 examples/sec; 0.130 sec/batch)
2017-05-08 17:05:46.509043: step 43290, loss = 0.77 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 17:05:47.873739: step 43300, loss = 1.01 (937.9 examples/sec; 0.136 sec/batch)
2017-05-08 17:05:49.091485: step 43310, loss = 0.79 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-08 17:05:50.364303: step 43320, loss = 0.73 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 17:05:51.629931: step 43330, loss = 0.71 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-08 17:05:52.922319: step 43340, loss = 0.90 (990.4 examples/sec; 0.129 sec/batch)
2017-05-08 17:05:54.208239: step 43350, loss = 0.72 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 17:05:55.494074: step 43360, loss = 0.72 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:05:56.779033: step 43370, loss = 0.76 (996.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:05:58.064474: step 43380, loss = 0.79 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:05:59.347932: step 43390, loss = 0.79 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:06:00.730476: step 43400, loss = 0.96 (925.8 examples/sec; 0.138 sec/batch)
2017-05-08 17:06:01.909973: step 43410, loss = 0.70 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-08 17:06:03.238824: step 43420, loss = 0.75 (963.2 examples/sec; 0.133 sec/batch)
2017-05-08 17:06:04.524091: step 43430, loss = 0.67 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:06:05.807402: step 43440, loss = 0.64 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:06:07.116799: step 43450, loss = 0.76 (977.5 examples/sec; 0.131 sec/batch)
2017-05-08 17:06:08.397897: step 43460, loss = 0.85 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:06:09.663394: step 43470, loss = 0.90 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-08 17:06:10.989083: step 43480, loss = 0.70 (965.5 examples/sec; 0.133 sec/batch)
2017-05-08 17:06:12.316970: step 43490, loss = 0.74 (964.0 examples/sec; 0.133 sec/batch)
2017-05-08 17:06:13.700629: step 43500, loss = 0.97 (925.1 examples/sec; 0.138 sec/batch)
2017-05-08 17:06:14.886213: step 43510, loss = 0.64 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-08 17:06:16.174317: step 43520, loss = 0.66 (993.7 examples/sec; 0.129 sec/batch)
2017-05-08 17:06:17.472585: step 43530, loss = 0.99 (985.9 examples/sec; 0.130 sec/batch)
2017-05-08 17:06:18.787364: step 43540, loss = 0.90 (973.5 examples/sec; 0.131 sec/batch)
2017-05-08 17:06:20.080495: step 43550, loss = 0.80 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:06:21.395547: step 43560, loss = 0.67 (973.3 examples/sec; 0.132 sec/batch)
2017-05-08 17:06:22.687721: step 43570, loss = 0.68 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:06:23.945221: step 43580, loss = 0.79 (1017.9 examples/sec; 0.126 sec/batch)
2017-05-08 17:06:25.236757: step 43590, loss = 0.81 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:06:26.623049: step 43600, loss = 0.82 (923.3 examples/sec; 0.139 sec/batch)
2017-05-08 17:06:27.794000: step 43610, loss = 0.97 (1093.1 examples/sec; 0.117 sec/batch)
2017-05-08 17:06:29.119232: step 43620, loss = 0.75 (965.9 examples/sec; 0.133 sec/batch)
2017-05-08 17:06:30.396208: step 43630, loss = 0.68 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:06:31.709391: step 43640, loss = 0.92 (974.7 examples/sec; 0.131 sec/batch)
2017-05-08 17:06:32.994054: step 43650, loss = 0.91 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:06:34.294528: step 43660, loss = 0.77 (984.3 examples/sec; 0.130 sec/batch)
2017-05-08 17:06:35.588412: step 43670, loss = 1.21 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 17:06:36.858133: step 43680, loss = 0.67 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-08 17:06:38.143877: step 43690, loss = 0.95 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:06:39.537485: step 43700, loss = 0.81 (918.5 examples/sec; 0.139 sec/batch)
2017-05-08 17:06:40.740613: step 43710, loss = 0.86 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-08 17:06:42.037275: step 43720, loss = 0.62 (987.2 examples/sec; 0.130 sec/batch)
2017-05-08 17:06:43.324239: step 43730, loss = 0.72 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:06:44.641201: step 43740, loss = 0.70 (971.9 examples/sec; 0.132 sec/batch)
2017-05-08 17:06:45.957251: step 43750, loss = 0.81 (972.6 examples/sec; 0.132 sec/batch)
2017-05-08 17:06:47.256730: step 43760, loss = 0.84 (985.0 examples/sec; 0.130 sec/batch)
2017-05-08 17:06:48.542878: step 43770, loss = 0.79 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 17:06:49.832077: step 43780, loss = 0.88 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:06:51.111544: step 43790, loss = 0.83 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:06:52.497763: step 43800, loss = 0.84 (923.4 examples/sec; 0.139 sec/batch)
2017-05-08 17:06:53.720199: step 43810, loss = 0.76 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-08 17:06:55.014882: step 43820, loss = 0.73 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 17:06:56.309187: step 43830, loss = 0.64 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:06:57.580799: step 43840, loss = 0.73 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-08 17:06:58.866700: step 43850, loss = 0.77 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 17:07:00.139653: step 43860, loss = 0.64 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 17:07:01.466154: step 43870, loss = 0.74 (964.9 examples/sec; 0.133 sec/batch)
2017-05-08 17:07:02.751424: step 43880, loss = 0.90 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:07:04.041371: step 43890, loss = 0.68 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 17:07:05.433381: step 43900, loss = 0.85 (919.5 examples/sec; 0.139 sec/batch)
2017-05-08 17:07:06.636179: step 43910, loss = 0.79 (1064.2 examples/sec; 0.120 sec/batch)
2017-05-08 17:07:07.897762: step 43920, loss = 0.79 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-08 17:07:09.171238: step 43930, loss = 0.85 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 17:07:10.451695: step 43940, loss = 0.97 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 17:07:11.722621: step 43950, loss = 0.80 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 17:07:12.997060: step 43960, loss = 0.78 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-08 17:07:14.292018: step 43970, loss = 0.82 (988.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:07:15.583800: step 43980, loss = 0.90 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:07:16.864762: step 43990, loss = 0.97 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 17:07:18.251783: step 44000, loss = 0.70 (922.8 examples/sec; 0.139 sec/batch)
2017-05-08 17:07:19.452795: step 44010, loss = 0.83 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-08 17:07:20.731011: step 44020, loss = 0.74 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:07:22.015739: step 44030, loss = 0.72 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:07:23.333418: step 44040, loss = 0.86 (971.4 examples/sec; 0.132 sec/batch)
2017-05-08 17:07:24.606727: step 44050, loss = 0.98 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 17:07:25.907886: step 44060, loss = 0.63 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 17:07:27.212478: step 44070, loss = 0.76 (981.2 examples/sec; 0.130 sec/batch)
2017-05-08 17:07:28.499709: step 44080, loss = 0.87 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 17:07:29.792595: step 44090, loss = 0.73 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:07:31.193214: step 44100, loss = 1.00 (913.9 examples/sec; 0.140 sec/batch)
2017-05-08 17:07:32.381720: step 44110, loss = 0.72 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-08 17:07:33.651248: step 44120, loss = 0.69 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 17:07:34.926275: step 44130, loss = 0.83 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-08 17:07:36.224497: step 44140, loss = 0.67 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 17:07:37.492804: step 44150, loss = 0.78 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-08 17:07:38.766425: step 44160, loss = 0.80 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-08 17:07:40.051949: step 44170, loss = 0.93 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 17:07:41.367174: step 44180, loss = 0.88 (973.2 examples/sec; 0.132 sec/batch)
2017-05-08 17:07:42.670621: step 44190, loss = 0.96 (982.0 examples/sec; 0.130 sec/batch)
2017-05-08 17:07:44.053024: step 44200, loss = 0.89 (925.9 examples/sec; 0.138 sec/batch)
2017-05-08 17:07:45.271167: step 44210, loss = 0.82 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-08 17:07:46.544866: step 44220, loss = 0.73 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-08 17:07:47.832158: step 44230, loss = 0.89 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 17:07:49.143379: step 44240, loss = 0.74 (976.2 examples/sec; 0.131 sec/batch)
2017-05-08 17:07:50.434341: step 44250, loss = 0.99 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:07:51.698887: step 44260, loss = 0.75 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 17:07:52.997562: step 44270, loss = 1.04 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 17:07:54.311681: step 44280, loss = 0.71 (974.0 examples/sec; 0.131 sec/batch)
2017-05-08 17:07:55.579761: step 44290, loss = 0.91 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-08 17:07:56.967759: step 44300, loss = 0.93 (922.2 examples/sec; 0.139 sec/batch)
2017-05-08 17:07:58.168863: step 44310, loss = 0.61 (1065.7 examples/sec; 0.120 sec/batch)
2017-05-08 17:07:59.454922: step 44320, loss = 0.80 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 17:08:00.719893: step 44330, loss = 0.77 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-08 17:08:01.998317: step 44340, loss = 0.72 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 17:08:03.324818: step 44350, loss = 0.94 (964.9 examples/sec; 0.133 sec/batch)
2017-05-08 17:08:04.619771: step 44360, loss = 0.76 (988.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:08:05.913839: step 44370, loss = 0.72 (989.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:08:07.222938: step 44380, loss = 0.83 (977.8 examples/sec; 0.131 sec/batch)
2017-05-08 17:08:08.513285: step 44390, loss = 0.78 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:08:09.896151: step 44400, loss = 0.72 (925.6 examples/sec; 0.138 sec/batch)
2017-05-08 17:08:11.113230: step 44410, loss = 0.88 (1051.7 examples/sec; 0.122 sec/batch)
2017-05-08 17:08:12.422599: step 44420, loss = 0.86 (977.6 examples/sec; 0.131 sec/batch)
2017-05-08 17:08:13.722868: step 44430, loss = 0.72 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 17:08:15.007193: step 44440, loss = 0.70 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 17:08:16.295137: step 44450, loss = 0.85 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:08:17.608947: step 44460, loss = 0.80 (974.3 examples/sec; 0.131 sec/batch)
2017-05-08 17:08:18.893029: step 44470, loss = 1.02 (996.8 examples/sec; 0.128 sec/batch)
2017-05-08 17:08:20.186672: step 44480, loss = 0.83 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 17:08:21.497364: step 44490, loss = 0.81 (976.6 examples/sec; 0.131 sec/batch)
2017-05-08 17:08:22.882883: step 44500, loss = 0.94 (923.8 examples/sec; 0.139 sec/batch)
2017-05-08 17:08:24.052168: step 44510, loss = 0.59 (1094.7 examples/sec; 0.117 sec/batch)
2017-05-08 17:08:25.340090: step 44520, loss = 0.73 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:08:26.631688: step 44530, loss = 0.65 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:08:27.921537: step 44540, loss = 0.70 (992.4 examples/sec; 0.129 sec/batch)
2017-05-08 17:08:29.217530: step 44550, loss = 0.90 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 17:08:30.492864: step 44560, loss = 0.71 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:08:31.782725: step 44570, loss = 1.00 (992.4 examples/sec; 0.129 sec/batch)
2017-05-08 17:08:33.046499: step 44580, loss = 0.92 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-08 17:08:34.356397: step 44590, loss = 0.81 (977.2 examples/sec; 0.131 sec/batch)
2017-05-08 17:08:35.718723: step 44600, loss = 0.80 (939.6 examples/sec; 0.136 sec/batch)
2017-05-08 17:08:36.905592: step 44610, loss = 0.85 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-08 17:08:38.207313: step 44620, loss = 0.75 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 17:08:39.504561: step 44630, loss = 0.80 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 17:08:40.784935: step 44640, loss = 0.82 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:08:42.067786: step 44650, loss = 0.62 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 17:08:43.333900: step 44660, loss = 0.93 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 17:08:44.604315: step 44670, loss = 0.69 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 17:08:45.894539: step 44680, loss = 0.62 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:08:47.194829: step 44690, loss = 0.84 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 17:08:48.570356: step 44700, loss = 0.85 (930.6 examples/sec; 0.138 sec/batch)
2017-05-08 17:08:49.744417: step 44710, loss = 0.76 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-08 17:08:51.049668: step 44720, loss = 0.71 (980.7 examples/sec; 0.131 sec/batch)
2017-05-08 17:08:52.353367: step 44730, loss = 0.99 (981.8 examples/sec; 0.130 sec/batch)
2017-05-08 17:08:53.641089: step 44740, loss = 0.74 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:08:54.962792: step 44750, loss = 0.90 (968.4 examples/sec; 0.132 sec/batch)
2017-05-08 17:08:56.240088: step 44760, loss = 0.96 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:08:57.571843: step 44770, loss = 0.71 (961.1 examples/sec; 0.133 sec/batch)
2017-05-08 17:08:58.860231: step 44780, loss = 0.74 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:09:00.140720: step 44790, loss = 0.78 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 17:09:01.509627: step 44800, loss = 0.75 (935.1 examples/sec; 0.137 sec/batch)
2017-05-08 17:09:02.700734: step 44810, loss = 0.70 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-08 17:09:03.968842: step 44820, loss = 0.71 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-08 17:09:05.246112: step 44830, loss = 0.95 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:09:06.547899: step 44840, loss = 0.97 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 17:09:07.827724: step 44850, loss = 0.78 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:09:09.087136: step 44860, loss = 0.80 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-08 17:09:10.393963: step 44870, loss = 0.83 (979.5 examples/sec; 0.131 sec/batch)
2017-05-08 17:09:11.665960: step 44880, loss = 0.83 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-08 17:09:12.950424: step 44890, loss = 0.90 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 17:09:14.352828: step 44900, loss = 0.73 (912.7 examples/sec; 0.140 sec/batch)
2017-05-08 17:09:15.550396: step 44910, loss = 0.82 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-08 17:09:16.825869: step 44920, loss = 0.71 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 17:09:18.103423: step 44930, loss = 0.70 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 17:09:19.390227: step 44940, loss = 0.58 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:09:20.652151: step 44950, loss = 0.84 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-08 17:09:21.966841: step 44960, loss = 1.00 (973.6 examples/sec; 0.131 sec/batch)
2017-05-08 17:09:23.223593: step 44970, loss = 0.74 (1018.5 examples/sec; 0.126 sec/batch)
2017-05-08 17:09:24.493713: step 44980, loss = 0.78 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-08 17:09:25.779425: step 44990, loss = 0.62 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:09:27.154103: step 45000, loss = 0.73 (931.1 examples/sec; 0.137 sec/batch)
2017-05-08 17:09:28.322749: step 45010, loss = 0.65 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-08 17:09:29.616311: step 45020, loss = 0.67 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:09:30.916427: step 45030, loss = 1.10 (984.5 examples/sec; 0.130 sec/batch)
2017-05-08 17:09:32.208099: step 45040, loss = 0.97 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:09:33.530152: step 45050, loss = 0.81 (968.2 examples/sec; 0.132 sec/batch)
2017-05-08 17:09:34.815442: step 45060, loss = 0.68 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:09:36.117985: step 45070, loss = 0.79 (982.7 examples/sec; 0.130 sec/batch)
2017-05-08 17:09:37.427922: step 45080, loss = 0.94 (977.1 examples/sec; 0.131 sec/batch)
2017-05-08 17:09:38.731638: step 45090, loss = 0.73 (981.8 examples/sec; 0.130 sec/batch)
2017-05-08 17:09:40.120607: step 45100, loss = 0.77 (921.5 examples/sec; 0.139 sec/batch)
2017-05-08 17:09:41.343113: step 45110, loss = 1.00 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-08 17:09:42.579658: step 45120, loss = 0.68 (1035.1 examples/sec; 0.124 sec/batch)
2017-05-08 17:09:43.920946: step 45130, loss = 0.84 (954.3 examples/sec; 0.134 sec/batch)
2017-05-08 17:09:45.240477: step 45140, loss = 0.74 (970.1 examples/sec; 0.132 sec/batch)
2017-05-08 17:09:46.593848: step 45150, loss = 0.87 (945.8 examples/sec; 0.135 sec/batch)
2017-05-08 17:09:47.863494: step 45160, loss = 0.75 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-08 17:09:49.163579: step 45170, loss = 0.82 (984.5 examples/sec; 0.130 sec/batch)
2017-05-08 17:09:50.474659: step 45180, loss = 0.67 (976.3 examples/sec; 0.131 sec/batch)
2017-05-08 17:09:51.803937: step 45190, loss = 0.85 (962.9 examples/sec; 0.133 sec/batch)
2017-05-08 17:09:53.176774: step 45200, loss = 0.84 (932.4 examples/sec; 0.137 sec/batch)
2017-05-08 17:09:54.344790: step 45210, loss = 0.66 (1095.9 examples/sec; 0.117 sec/batch)
2017-05-08 17:09:55.616817: step 45220, loss = 0.86 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-08 17:09:56.995962: step 45230, loss = 0.84 (928.1 examples/sec; 0.138 sec/batch)
2017-05-08 17:09:58.314623: step 45240, loss = 0.81 (970.7 examples/sec; 0.132 sec/batch)
2017-05-08 17:09:59.587397: step 45250, loss = 0.75 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-08 17:10:00.894105: step 45260, loss = 0.79 (979.6 examples/sec; 0.131 sec/batch)
2017-05-08 17:10:02.199763: step 45270, loss = 0.82 (980.3 examples/sec; 0.131 sec/batch)
2017-05-08 17:10:03.546228: step 45280, loss = 0.82 (950.6 examples/sec; 0.135 sec/batch)
2017-05-08 17:10:04.852922: step 45290, loss = 0.85 (979.6 examples/sec; 0.131 sec/batch)
2017-05-08 17:10:06.228820: step 45300, loss = 0.78 (930.3 examples/sec; 0.138 sec/batch)
2017-05-08 17:10:07.411955: step 45310, loss = 0.85 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-08 17:10:08.677996: step 45320, loss = 0.67 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 17:10:09.975206: step 45330, loss = 0.84 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 17:10:11.269384: step 45340, loss = 0.78 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:10:12.540858: step 45350, loss = 0.98 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 17:10:13.852407: step 45360, loss = 0.96 (975.9 examples/sec; 0.131 sec/batch)
2017-05-08 17:10:15.161742: step 45370, loss = 1.04 (977.6 examples/sec; 0.131 sec/batch)
2017-05-08 17:10:16.452572: step 45380, loss = 0.84 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:10:17.731577: step 45390, loss = 0.81 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 17:10:19.128527: step 45400, loss = 0.82 (916.3 examples/sec; 0.140 sec/batch)
2017-05-08 17:10:20.324353: step 45410, loss = 0.85 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-08 17:10:21.613124: step 45420, loss = 0.74 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 17:10:22.918390: step 45430, loss = 0.91 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 17:10:24.235512: step 45440, loss = 0.88 (971.8 examples/sec; 0.132 sec/batch)
2017-05-08 17:10:25.529177: step 45450, loss = 0.80 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 17:10:26.797494: step 45460, loss = 0.81 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-08 17:10:28.051952: step 45470, loss = 0.90 (1020.4 examples/sec; 0.125 sec/batch)
2017-05-08 17:10:29.318500: step 45480, loss = 0.72 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 17:10:30.625402: step 45490, loss = 0.79 (979.4 examples/sec; 0.131 sec/batch)
2017-05-08 17:10:32.017616: step 45500, loss = 0.81 (919.4 examples/sec; 0.139 sec/batch)
2017-05-08 17:10:33.224308: step 45510, loss = 0.68 (1060.7 examples/sec; 0.121 sec/batch)
2017-05-08 17:10:34.524382: step 45520, loss = 0.74 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 17:10:35.814374: step 45530, loss = 0.82 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 17:10:37.121720: step 45540, loss = 0.91 (979.1 examples/sec; 0.131 sec/batch)
2017-05-08 17:10:38.418279: step 45550, loss = 0.86 (987.2 examples/sec; 0.130 sec/batch)
2017-05-08 17:10:39.728600: step 45560, loss = 0.95 (976.9 examples/sec; 0.131 sec/batch)
2017-05-08 17:10:41.003772: step 45570, loss = 0.79 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 17:10:42.303192: step 45580, loss = 0.79 (985.1 examples/sec; 0.130 sec/batch)
2017-05-08 17:10:43.607356: step 45590, loss = 0.75 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 17:10:45.003883: step 45600, loss = 0.75 (916.6 examples/sec; 0.140 sec/batch)
2017-05-08 17:10:46.200933: step 45610, loss = 0.72 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-08 17:10:47.500330: step 45620, loss = 0.79 (985.1 examples/sec; 0.130 sec/batch)
2017-05-08 17:10:48.794656: step 45630, loss = 0.90 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:10:50.114698: step 45640, loss = 0.86 (969.7 examples/sec; 0.132 sec/batch)
2017-05-08 17:10:51.423660: step 45650, loss = 0.70 (977.9 examples/sec; 0.131 sec/batch)
2017-05-08 17:10:52.699277: step 45660, loss = 0.73 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:10:54.005678: step 45670, loss = 0.60 (979.8 examples/sec; 0.131 sec/batch)
2017-05-08 17:10:55.283826: step 45680, loss = 0.77 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:10:56.572120: step 45690, loss = 0.65 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:10:57.936041: step 45700, loss = 0.88 (938.5 examples/sec; 0.136 sec/batch)
2017-05-08 17:10:59.131550: step 45710, loss = 0.79 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-08 17:11:00.421987: step 45720, loss = 0.77 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:11:01.698939: step 45730, loss = 0.75 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:11:03.010941: step 45740, loss = 0.83 (975.6 examples/sec; 0.131 sec/batch)
2017-05-08 17:11:04.282277: step 45750, loss = 0.72 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-08 17:11:05.549659: step 45760, loss = 0.85 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-08 17:11:06.864213: step 45770, loss = 0.60 (973.7 examples/sec; 0.131 sec/batch)
2017-05-08 17:11:08.167026: step 45780, loss = 1.01 (982.5 examples/sec; 0.130 sec/batch)
2017-05-08 17:11:09.437013: step 45790, loss = 0.87 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-08 17:11:10.830023: step 45800, loss = 0.69 (918.9 examples/sec; 0.139 sec/batch)
2017-05-08 17:11:12.018366: step 45810, loss = 0.86 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-08 17:11:13.339244: step 45820, loss = 0.79 (969.1 examples/sec; 0.132 sec/batch)
2017-05-08 17:11:14.637683: step 45830, loss = 0.66 (985.8 examples/sec; 0.130 sec/batch)
2017-05-08 17:11:15.950855: step 45840, loss = 0.75 (974.7 examples/sec; 0.131 sec/batch)
2017-05-08 17:11:17.260439: step 45850, loss = 0.97 (977.4 examples/sec; 0.131 sec/batch)
2017-05-08 17:11:18.550631: step 45860, loss = 0.82 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:11:19.828412: step 45870, loss = 0.89 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:11:21.110549: step 45880, loss = 0.68 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:11:22.398425: step 45890, loss = 0.85 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:11:23.755212: step 45900, loss = 0.75 (943.4 examples/sec; 0.136 sec/batch)
2017-05-08 17:11:24.961866: step 45910, loss = 0.69 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-08 17:11:26.264394: step 45920, loss = 0.79 (982.7 examples/sec; 0.130 sec/batch)
2017-05-08 17:11:27.522031: step 45930, loss = 0.88 (1017.8 examples/sec; 0.126 sec/batch)
2017-05-08 17:11:28.823331: step 45940, loss = 0.81 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 17:11:30.129205: step 45950, loss = 0.76 (980.2 examples/sec; 0.131 sec/batch)
2017-05-08 17:11:31.416376: step 45960, loss = 0.77 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 17:11:32.719890: step 45970, loss = 0.67 (982.0 examples/sec; 0.130 sec/batch)
2017-05-08 17:11:34.039641: step 45980, loss = 0.86 (969.9 examples/sec; 0.132 sec/batch)
2017-05-08 17:11:35.345902: step 45990, loss = 0.77 (979.9 examples/sec; 0.131 sec/batch)
2017-05-08 17:11:36.719773: step 46000, loss = 0.79 (931.7 examples/sec; 0.137 sec/batch)
2017-05-08 17:11:37.931627: step 46010, loss = 0.73 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-08 17:11:39.213542: step 46020, loss = 0.71 (998.5 examples/sec; 0.128 sec/batch)
2017-05-08 17:11:40.481753: step 46030, loss = 0.67 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-08 17:11:41.765259: step 46040, loss = 0.73 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:11:43.027764: step 46050, loss = 0.82 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-08 17:11:44.316645: step 46060, loss = 0.70 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:11:45.619068: step 46070, loss = 0.78 (982.8 examples/sec; 0.130 sec/batch)
2017-05-08 17:11:46.933963: step 46080, loss = 0.71 (973.5 examples/sec; 0.131 sec/batch)
2017-05-08 17:11:48.197304: step 46090, loss = 0.72 (1013.2 examples/sec; 0.126 sec/batch)
2017-05-08 17:11:49.567348: step 46100, loss = 0.88 (934.3 examples/sec; 0.137 sec/batch)
2017-05-08 17:11:50.758034: step 46110, loss = 0.77 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-08 17:11:52.055827: step 46120, loss = 0.86 (986.3 examples/sec; 0.130 sec/batch)
2017-05-08 17:11:53.319553: step 46130, loss = 0.72 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-08 17:11:54.604355: step 46140, loss = 0.78 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:11:55.919438: step 46150, loss = 0.81 (973.3 examples/sec; 0.132 sec/batch)
2017-05-08 17:11:57.209052: step 46160, loss = 0.68 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:11:58.521275: step 46170, loss = 0.78 (975.4 examples/sec; 0.131 sec/batch)
2017-05-08 17:11:59.875239: step 46180, loss = 0.87 (945.4 examples/sec; 0.135 sec/batch)
2017-05-08 17:12:01.174977: step 46190, loss = 0.71 (984.8 examples/sec; 0.130 sec/batch)
2017-05-08 17:12:02.597055: step 46200, loss = 0.61 (900.1 examples/sec; 0.142 sec/batch)
2017-05-08 17:12:03.818408: step 46210, loss = 0.69 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-08 17:12:05.113970: step 46220, loss = 0.88 (988.0 examples/sec; 0.130 sec/batch)
2017-05-08 17:12:06.422195: step 46230, loss = 0.86 (978.4 examples/sec; 0.131 sec/batch)
2017-05-08 17:12:07.695074: step 46240, loss = 0.75 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 17:12:09.113345: step 46250, loss = 0.72 (902.5 examples/sec; 0.142 sec/batch)
2017-05-08 17:12:10.403029: step 46260, loss = 0.71 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:12:11.669787: step 46270, loss = 0.69 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-08 17:12:12.948646: step 46280, loss = 0.89 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 17:12:14.268012: step 46290, loss = 0.71 (970.2 examples/sec; 0.132 sec/batch)
2017-05-08 17:12:15.747970: step 46300, loss = 0.80 (864.9 examples/sec; 0.148 sec/batch)
2017-05-08 17:12:16.944185: step 46310, loss = 0.91 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-08 17:12:18.219854: step 46320, loss = 0.86 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:12:19.518565: step 46330, loss = 1.03 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 17:12:20.809862: step 46340, loss = 0.66 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 17:12:22.097813: step 46350, loss = 0.77 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:12:23.381114: step 46360, loss = 0.82 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:12:24.671402: step 46370, loss = 0.81 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:12:25.967399: step 46380, loss = 0.72 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 17:12:27.263859: step 46390, loss = 0.72 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 17:12:28.647327: step 46400, loss = 0.75 (925.2 examples/sec; 0.138 sec/batch)
2017-05-08 17:12:29.867280: step 46410, loss = 0.72 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-08 17:12:31.156275: step 46420, loss = 0.75 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:12:32.421506: step 46430, loss = 0.82 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-08 17:12:33.679717: step 46440, loss = 0.91 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-08 17:12:34.974541: step 46450, loss = 0.86 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:12:36.275117: step 46460, loss = 0.83 (984.2 examples/sec; 0.130 sec/batch)
2017-05-08 17:12:37.548891: step 46470, loss = 0.66 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-08 17:12:38.847869: step 46480, loss = 0.83 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 17:12:40.094720: step 46490, loss = 0.65 (1026.6 examples/sec; 0.125 sec/batch)
2017-05-08 17:12:41.555029: step 46500, loss = 0.65 (876.5 examples/sec; 0.146 sec/batch)
2017-05-08 17:12:42.660136: step 46510, loss = 0.80 (1158.3 examples/sec; 0.111 sec/batch)
2017-05-08 17:12:43.968979: step 46520, loss = 0.75 (978.0 examples/sec; 0.131 sec/batch)
2017-05-08 17:12:45.263511: step 46530, loss = 0.75 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:12:46.568957: step 46540, loss = 0.66 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 17:12:47.840043: step 46550, loss = 0.83 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 17:12:49.194286: step 46560, loss = 0.92 (945.2 examples/sec; 0.135 sec/batch)
2017-05-08 17:12:50.482596: step 46570, loss = 0.75 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:12:51.764335: step 46580, loss = 0.81 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:12:53.111166: step 46590, loss = 0.62 (950.4 examples/sec; 0.135 sec/batch)
2017-05-08 17:12:54.483077: step 46600, loss = 0.80 (933.0 examples/sec; 0.137 sec/batch)
2017-05-08 17:12:55.672646: step 46610, loss = 0.67 (1076.0 examples/sec; 0.119 sec/batch)
2017-05-08 17:12:56.977294: step 46620, loss = 0.70 (981.1 examples/sec; 0.130 sec/batch)
2017-05-08 17:12:58.316662: step 46630, loss = 0.77 (955.7 examples/sec; 0.134 sec/batch)
2017-05-08 17:12:59.651682: step 46640, loss = 1.02 (958.8 examples/sec; 0.134 sec/batch)
2017-05-08 17:13:00.952859: step 46650, loss = 0.82 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 17:13:02.260150: step 46660, loss = 0.81 (979.1 examples/sec; 0.131 sec/batch)
2017-05-08 17:13:03.510250: step 46670, loss = 0.77 (1023.9 examples/sec; 0.125 sec/batch)
2017-05-08 17:13:04.817099: step 46680, loss = 0.80 (979.5 examples/sec; 0.131 sec/batch)
2017-05-08 17:13:06.160708: step 46690, loss = 0.89 (952.7 examples/sec; 0.134 sec/batch)
2017-05-08 17:13:07.540107: step 46700, loss = 0.84 (927.9 examples/sec; 0.138 sec/batch)
2017-05-08 17:13:08.726079: step 46710, loss = 0.76 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-08 17:13:10.019781: step 46720, loss = 0.74 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 17:13:11.328813: step 46730, loss = 1.01 (977.8 examples/sec; 0.131 sec/batch)
2017-05-08 17:13:12.617287: step 46740, loss = 0.67 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 17:13:13.918268: step 46750, loss = 0.73 (983.9 examples/sec; 0.130 sec/batch)
2017-05-08 17:13:15.204056: step 46760, loss = 0.82 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:13:16.475655: step 46770, loss = 0.85 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-08 17:13:17.790091: step 46780, loss = 0.89 (973.8 examples/sec; 0.131 sec/batch)
2017-05-08 17:13:19.109152: step 46790, loss = 0.77 (970.4 examples/sec; 0.132 sec/batch)
2017-05-08 17:13:20.487105: step 46800, loss = 0.81 (928.9 examples/sec; 0.138 sec/batch)
2017-05-08 17:13:21.670749: step 46810, loss = 0.71 (1081.4 examples/sec; 0.118 sec/batch)
2017-05-08 17:13:22.953217: step 46820, loss = 0.75 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:13:24.226513: step 46830, loss = 0.73 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 17:13:25.483716: step 46840, loss = 0.71 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-08 17:13:26.776682: step 46850, loss = 0.61 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:13:28.052002: step 46860, loss = 0.89 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:13:29.323484: step 46870, loss = 0.81 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 17:13:30.598319: step 46880, loss = 0.60 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 17:13:31.866675: step 46890, loss = 0.74 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-08 17:13:33.262749: step 46900, loss = 0.74 (916.9 examples/sec; 0.140 sec/batch)
2017-05-08 17:13:34.483232: step 46910, loss = 0.85 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-08 17:13:35.772708: step 46920, loss = 0.92 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 17:13:37.057657: step 46930, loss = 0.74 (996.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:13:38.348190: step 46940, loss = 0.64 (991.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:13:39.646393: step 46950, loss = 0.83 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 17:13:40.913678: step 46960, loss = 0.80 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-08 17:13:42.227190: step 46970, loss = 0.88 (974.5 examples/sec; 0.131 sec/batch)
2017-05-08 17:13:43.507575: step 46980, loss = 0.65 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:13:44.792442: step 46990, loss = 0.67 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 17:13:46.157419: step 47000, loss = 0.66 (937.7 examples/sec; 0.136 sec/batch)
2017-05-08 17:13:47.369577: step 47010, loss = 0.74 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-08 17:13:48.658641: step 47020, loss = 0.78 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:13:49.956826: step 47030, loss = 0.78 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 17:13:51.229400: step 47040, loss = 0.82 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-08 17:13:52.515548: step 47050, loss = 0.84 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 17:13:53.819848: step 47060, loss = 0.81 (981.4 examples/sec; 0.130 sec/batch)
2017-05-08 17:13:55.105349: step 47070, loss = 0.88 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 17:13:56.359954: step 47080, loss = 0.74 (1020.2 examples/sec; 0.125 sec/batch)
2017-05-08 17:13:57.644068: step 47090, loss = 0.68 (996.8 examples/sec; 0.128 sec/batch)
2017-05-08 17:13:58.997664: step 47100, loss = 0.87 (945.6 examples/sec; 0.135 sec/batch)
2017-05-08 17:14:00.177750: step 47110, loss = 0.72 (1084.7 examples/sec; 0.118 sec/batch)
2017-05-08 17:14:01.455077: step 47120, loss = 0.67 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:14:02.775917: step 47130, loss = 0.86 (969.1 examples/sec; 0.132 sec/batch)
2017-05-08 17:14:04.071820: step 47140, loss = 0.82 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 17:14:05.358726: step 47150, loss = 0.74 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:14:06.655244: step 47160, loss = 0.77 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 17:14:07.938618: step 47170, loss = 0.70 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:14:09.230533: step 47180, loss = 0.74 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:14:10.537373: step 47190, loss = 0.73 (979.5 examples/sec; 0.131 sec/batch)
2017-05-08 17:14:11.905880: step 47200, loss = 0.80 (935.3 examples/sec; 0.137 sec/batch)
2017-05-08 17:14:13.099903: step 47210, loss = 0.58 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-08 17:14:14.422908: step 47220, loss = 0.88 (967.5 examples/sec; 0.132 sec/batch)
2017-05-08 17:14:15.743088: step 47230, loss = 0.71 (969.6 examples/sec; 0.132 sec/batch)
2017-05-08 17:14:17.045196: step 47240, loss = 0.88 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 17:14:18.346124: step 47250, loss = 0.76 (983.9 examples/sec; 0.130 sec/batch)
2017-05-08 17:14:19.628335: step 47260, loss = 0.77 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:14:20.945347: step 47270, loss = 0.72 (971.9 examples/sec; 0.132 sec/batch)
2017-05-08 17:14:22.246509: step 47280, loss = 1.01 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 17:14:23.514290: step 47290, loss = 0.73 (1009.6 examples/sec; 0.127 sec/batch)
2017-05-08 17:14:24.899820: step 47300, loss = 0.86 (923.8 examples/sec; 0.139 sec/batch)
2017-05-08 17:14:26.105739: step 47310, loss = 0.72 (1061.4 examples/sec; 0.121 sec/batch)
2017-05-08 17:14:27.372012: step 47320, loss = 0.81 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-08 17:14:28.636703: step 47330, loss = 0.77 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-08 17:14:29.921859: step 47340, loss = 0.67 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:14:31.192313: step 47350, loss = 0.75 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 17:14:32.468070: step 47360, loss = 0.58 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:14:33.757765: step 47370, loss = 0.77 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:14:35.080343: step 47380, loss = 0.92 (967.8 examples/sec; 0.132 sec/batch)
2017-05-08 17:14:36.348763: step 47390, loss = 0.77 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-08 17:14:37.726779: step 47400, loss = 0.59 (928.9 examples/sec; 0.138 sec/batch)
2017-05-08 17:14:38.937540: step 47410, loss = 0.65 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-08 17:14:40.248947: step 47420, loss = 0.82 (976.0 examples/sec; 0.131 sec/batch)
2017-05-08 17:14:41.518771: step 47430, loss = 0.83 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-08 17:14:42.823352: step 47440, loss = 0.78 (981.2 examples/sec; 0.130 sec/batch)
2017-05-08 17:14:44.132166: step 47450, loss = 0.73 (978.0 examples/sec; 0.131 sec/batch)
2017-05-08 17:14:45.413533: step 47460, loss = 0.63 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 17:14:46.690264: step 47470, loss = 0.74 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-08 17:14:47.958434: step 47480, loss = 0.84 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-08 17:14:49.225339: step 47490, loss = 0.75 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-08 17:14:50.605228: step 47500, loss = 0.89 (927.6 examples/sec; 0.138 sec/batch)
2017-05-08 17:14:51.777291: step 47510, loss = 0.76 (1092.1 examples/sec; 0.117 sec/batch)
2017-05-08 17:14:53.055198: step 47520, loss = 0.86 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 17:14:54.329239: step 47530, loss = 0.80 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-08 17:14:55.613477: step 47540, loss = 0.86 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:14:56.917740: step 47550, loss = 0.71 (981.4 examples/sec; 0.130 sec/batch)
2017-05-08 17:14:58.207428: step 47560, loss = 0.77 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:14:59.492082: step 47570, loss = 0.79 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:15:00.780231: step 47580, loss = 0.78 (993.7 examples/sec; 0.129 sec/batch)
2017-05-08 17:15:02.077486: step 47590, loss = 0.72 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 17:15:03.467002: step 47600, loss = 0.85 (921.2 examples/sec; 0.139 sec/batch)
2017-05-08 17:15:04.655320: step 47610, loss = 0.76 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-08 17:15:05.954565: step 47620, loss = 0.58 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 17:15:07.259899: step 47630, loss = 0.71 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 17:15:08.532479: step 47640, loss = 0.64 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-08 17:15:09.839307: step 47650, loss = 0.75 (979.5 examples/sec; 0.131 sec/batch)
2017-05-08 17:15:11.151069: step 47660, loss = 0.70 (975.8 examples/sec; 0.131 sec/batch)
2017-05-08 17:15:12.415148: step 47670, loss = 0.80 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-08 17:15:13.715102: step 47680, loss = 0.70 (984.7 examples/sec; 0.130 sec/batch)
2017-05-08 17:15:15.015772: step 47690, loss = 0.88 (984.1 examples/sec; 0.130 sec/batch)
2017-05-08 17:15:16.398720: step 47700, loss = 0.71 (925.6 examples/sec; 0.138 sec/batch)
2017-05-08 17:15:17.576900: step 47710, loss = 0.94 (1086.4 examples/sec; 0.118 sec/batch)
2017-05-08 17:15:18.885235: step 47720, loss = 0.79 (978.3 examples/sec; 0.131 sec/batch)
2017-05-08 17:15:20.187593: step 47730, loss = 0.85 (982.8 examples/sec; 0.130 sec/batch)
2017-05-08 17:15:21.505660: step 47740, loss = 0.72 (971.1 examples/sec; 0.132 sec/batch)
2017-05-08 17:15:22.810305: step 47750, loss = 0.62 (981.1 examples/sec; 0.130 sec/batch)
2017-05-08 17:15:24.122749: step 47760, loss = 0.78 (975.3 examples/sec; 0.131 sec/batch)
2017-05-08 17:15:25.394574: step 47770, loss = 0.78 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-08 17:15:26.691790: step 47780, loss = 0.84 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 17:15:27.980035: step 47790, loss = 0.75 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:15:29.371029: step 47800, loss = 0.73 (920.2 examples/sec; 0.139 sec/batch)
2017-05-08 17:15:30.539886: step 47810, loss = 0.99 (1095.1 examples/sec; 0.117 sec/batch)
2017-05-08 17:15:31.809778: step 47820, loss = 0.88 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-08 17:15:33.076062: step 47830, loss = 0.67 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-08 17:15:34.392499: step 47840, loss = 0.68 (972.4 examples/sec; 0.132 sec/batch)
2017-05-08 17:15:35.673942: step 47850, loss = 0.80 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 17:15:36.971917: step 47860, loss = 0.69 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 17:15:38.266724: step 47870, loss = 0.85 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:15:39.549637: step 47880, loss = 0.71 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:15:40.863434: step 47890, loss = 0.64 (974.3 examples/sec; 0.131 sec/batch)
2017-05-08 17:15:42.262616: step 47900, loss = 0.72 (914.8 examples/sec; 0.140 sec/batch)
2017-05-08 17:15:43.476158: step 47910, loss = 0.82 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-08 17:15:44.757234: step 47920, loss = 1.09 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 17:15:46.063706: step 47930, loss = 0.69 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 17:15:47.346194: step 47940, loss = 0.78 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:15:48.602270: step 47950, loss = 0.76 (1019.0 examples/sec; 0.126 sec/batch)
2017-05-08 17:15:49.883541: step 47960, loss = 0.76 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 17:15:51.192828: step 47970, loss = 0.72 (977.6 examples/sec; 0.131 sec/batch)
2017-05-08 17:15:52.465794: step 47980, loss = 0.74 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 17:15:53.772661: step 47990, loss = 0.79 (979.4 examples/sec; 0.131 sec/batch)
2017-05-08 17:15:55.173948: step 48000, loss = 0.64 (913.4 examples/sec; 0.140 sec/batch)
2017-05-08 17:15:56.382286: step 48010, loss = 0.75 (1059.3 examples/sec; 0.121 sec/batch)
2017-05-08 17:15:57.679141: step 48020, loss = 0.84 (987.0 examples/sec; 0.130 sec/batch)
2017-05-08 17:15:58.987096: step 48030, loss = 0.75 (978.6 examples/sec; 0.131 sec/batch)
2017-05-08 17:16:00.276421: step 48040, loss = 0.80 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:16:01.561021: step 48050, loss = 0.98 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:16:02.843937: step 48060, loss = 0.79 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:16:04.144700: step 48070, loss = 0.80 (984.0 examples/sec; 0.130 sec/batch)
2017-05-08 17:16:05.432640: step 48080, loss = 0.67 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:16:06.744647: step 48090, loss = 0.87 (975.6 examples/sec; 0.131 sec/batch)
2017-05-08 17:16:08.141695: step 48100, loss = 0.62 (916.2 examples/sec; 0.140 sec/batch)
2017-05-08 17:16:09.352653: step 48110, loss = 0.75 (1057.0 examples/sec; 0.121 sec/batch)
2017-05-08 17:16:10.613913: step 48120, loss = 0.77 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-08 17:16:11.867807: step 48130, loss = 0.85 (1020.8 examples/sec; 0.125 sec/batch)
2017-05-08 17:16:13.162692: step 48140, loss = 0.70 (988.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:16:14.451384: step 48150, loss = 0.87 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 17:16:15.735854: step 48160, loss = 0.74 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 17:16:17.016388: step 48170, loss = 0.74 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 17:16:18.307853: step 48180, loss = 0.92 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:16:19.624725: step 48190, loss = 0.74 (972.0 examples/sec; 0.132 sec/batch)
2017-05-08 17:16:21.021778: step 48200, loss = 0.87 (916.2 examples/sec; 0.140 sec/batch)
2017-05-08 17:16:22.223749: step 48210, loss = 0.89 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-08 17:16:23.516491: step 48220, loss = 0.79 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 17:16:24.831702: step 48230, loss = 0.77 (973.2 examples/sec; 0.132 sec/batch)
2017-05-08 17:16:26.155135: step 48240, loss = 0.82 (967.2 examples/sec; 0.132 sec/batch)
2017-05-08 17:16:27.425234: step 48250, loss = 0.87 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-08 17:16:28.737758: step 48260, loss = 0.70 (975.2 examples/sec; 0.131 sec/batch)
2017-05-08 17:16:30.020136: step 48270, loss = 0.90 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:16:31.311213: step 48280, loss = 0.82 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 17:16:32.579351: step 48290, loss = 1.02 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-08 17:16:33.927254: step 48300, loss = 0.77 (949.6 examples/sec; 0.135 sec/batch)
2017-05-08 17:16:35.137519: step 48310, loss = 0.93 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-08 17:16:36.420501: step 48320, loss = 0.76 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:16:37.712416: step 48330, loss = 0.93 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 17:16:39.011116: step 48340, loss = 0.85 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 17:16:40.302243: step 48350, loss = 0.90 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 17:16:41.592308: step 48360, loss = 0.70 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 17:16:42.902901: step 48370, loss = 0.73 (976.7 examples/sec; 0.131 sec/batch)
2017-05-08 17:16:44.179302: step 48380, loss = 0.80 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 17:16:45.475555: step 48390, loss = 0.73 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 17:16:46.850711: step 48400, loss = 0.72 (930.8 examples/sec; 0.138 sec/batch)
2017-05-08 17:16:48.066595: step 48410, loss = 0.76 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-08 17:16:49.392367: step 48420, loss = 0.64 (965.5 examples/sec; 0.133 sec/batch)
2017-05-08 17:16:50.696515: step 48430, loss = 0.90 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 17:16:51.993864: step 48440, loss = 0.92 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 17:16:53.291250: step 48450, loss = 0.73 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 17:16:54.547414: step 48460, loss = 0.84 (1019.0 examples/sec; 0.126 sec/batch)
2017-05-08 17:16:55.814352: step 48470, loss = 0.78 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-08 17:16:57.116761: step 48480, loss = 0.71 (982.8 examples/sec; 0.130 sec/batch)
2017-05-08 17:16:58.384778: step 48490, loss = 0.80 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-08 17:16:59.789659: step 48500, loss = 0.78 (911.1 examples/sec; 0.140 sec/batch)
2017-05-08 17:17:00.994489: step 48510, loss = 0.65 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-08 17:17:02.291546: step 48520, loss = 0.84 (986.8 examples/sec; 0.130 sec/batch)
2017-05-08 17:17:03.576149: step 48530, loss = 0.73 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:17:04.895183: step 48540, loss = 0.92 (970.4 examples/sec; 0.132 sec/batch)
2017-05-08 17:17:06.195703: step 48550, loss = 0.79 (984.2 examples/sec; 0.130 sec/batch)
2017-05-08 17:17:07.483657: step 48560, loss = 0.79 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:17:08.767195: step 48570, loss = 0.70 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 17:17:10.089819: step 48580, loss = 0.90 (967.8 examples/sec; 0.132 sec/batch)
2017-05-08 17:17:11.391551: step 48590, loss = 0.85 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 17:17:12.761311: step 48600, loss = 0.76 (934.5 examples/sec; 0.137 sec/batch)
2017-05-08 17:17:13.968613: step 48610, loss = 0.75 (1060.2 examples/sec; 0.121 sec/batch)
2017-05-08 17:17:15.267621: step 48620, loss = 0.84 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 17:17:16.556187: step 48630, loss = 0.75 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 17:17:17.854341: step 48640, loss = 0.78 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 17:17:19.177704: step 48650, loss = 0.71 (967.2 examples/sec; 0.132 sec/batch)
2017-05-08 17:17:20.487782: step 48660, loss = 0.79 (977.0 examples/sec; 0.131 sec/batch)
2017-05-08 17:17:21.758899: step 48670, loss = 0.74 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 17:17:23.054174: step 48680, loss = 0.77 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 17:17:24.353698: step 48690, loss = 0.76 (985.0 examples/sec; 0.130 sec/batch)
2017-05-08 17:17:25.747140: step 48700, loss = 0.73 (918.6 examples/sec; 0.139 sec/batch)
2017-05-08 17:17:26.927810: step 48710, loss = 0.84 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-08 17:17:28.207294: step 48720, loss = 0.81 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:17:29.489772: step 48730, loss = 0.84 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:17:30.779348: step 48740, loss = 0.71 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:17:32.067067: step 48750, loss = 0.78 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:17:33.374525: step 48760, loss = 0.79 (979.0 examples/sec; 0.131 sec/batch)
2017-05-08 17:17:34.676075: step 48770, loss = 0.86 (983.4 examples/sec; 0.130 sec/batch)
2017-05-08 17:17:35.950784: step 48780, loss = 0.78 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-08 17:17:37.228936: step 48790, loss = 0.85 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:17:38.595973: step 48800, loss = 0.80 (936.3 examples/sec; 0.137 sec/batch)
2017-05-08 17:17:39.791778: step 48810, loss = 0.83 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-08 17:17:41.071936: step 48820, loss = 0.86 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 17:17:42.358608: step 48830, loss = 0.73 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:17:43.658198: step 48840, loss = 0.77 (984.9 examples/sec; 0.130 sec/batch)
2017-05-08 17:17:44.966645: step 48850, loss = 0.74 (978.3 examples/sec; 0.131 sec/batch)
2017-05-08 17:17:46.258472: step 48860, loss = 0.67 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:17:47.570645: step 48870, loss = 0.77 (975.5 examples/sec; 0.131 sec/batch)
2017-05-08 17:17:48.850821: step 48880, loss = 0.98 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 17:17:50.168259: step 48890, loss = 0.96 (971.6 examples/sec; 0.132 sec/batch)
2017-05-08 17:17:51.568216: step 48900, loss = 0.89 (914.3 examples/sec; 0.140 sec/batch)
2017-05-08 17:17:52.769571: step 48910, loss = 0.74 (1065.5 examples/sec; 0.120 sec/batch)
2017-05-08 17:17:54.049918: step 48920, loss = 0.81 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:17:55.343764: step 48930, loss = 0.75 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 17:17:56.616129: step 48940, loss = 0.73 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-08 17:17:57.893675: step 48950, loss = 0.67 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 17:17:59.187869: step 48960, loss = 0.79 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:18:00.495162: step 48970, loss = 0.65 (979.1 examples/sec; 0.131 sec/batch)
2017-05-08 17:18:01.793755: step 48980, loss = 1.17 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 17:18:03.073307: step 48990, loss = 0.87 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:18:04.460114: step 49000, loss = 0.88 (923.0 examples/sec; 0.139 sec/batch)
2017-05-08 17:18:05.650129: step 49010, loss = 0.87 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-08 17:18:06.960037: step 49020, loss = 0.83 (977.2 examples/sec; 0.131 sec/batch)
2017-05-08 17:18:08.251651: step 49030, loss = 0.84 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:18:09.557152: step 49040, loss = 0.92 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 17:18:10.869513: step 49050, loss = 0.73 (975.3 examples/sec; 0.131 sec/batch)
2017-05-08 17:18:12.148027: step 49060, loss = 0.76 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 17:18:13.423645: step 49070, loss = 0.90 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:18:14.710389: step 49080, loss = 0.71 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:18:16.015247: step 49090, loss = 0.85 (980.9 examples/sec; 0.130 sec/batch)
2017-05-08 17:18:17.395411: step 49100, loss = 0.77 (927.4 examples/sec; 0.138 sec/batch)
2017-05-08 17:18:18.612377: step 49110, loss = 0.70 (1051.8 examples/sec; 0.122 sec/batch)
2017-05-08 17:18:19.902805: step 49120, loss = 0.77 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:18:21.214734: step 49130, loss = 0.83 (975.7 examples/sec; 0.131 sec/batch)
2017-05-08 17:18:22.493849: step 49140, loss = 0.70 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:18:23.768663: step 49150, loss = 0.77 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 17:18:25.060468: step 49160, loss = 0.78 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:18:26.321603: step 49170, loss = 0.59 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-08 17:18:27.618413: step 49180, loss = 0.86 (987.0 examples/sec; 0.130 sec/batch)
2017-05-08 17:18:28.917940: step 49190, loss = 0.88 (985.0 examples/sec; 0.130 sec/batch)
2017-05-08 17:18:30.305107: step 49200, loss = 0.83 (922.7 examples/sec; 0.139 sec/batch)
2017-05-08 17:18:31.516438: step 49210, loss = 0.71 (1056.7 examples/sec; 0.121 sec/batch)
2017-05-08 17:18:32.804103: step 49220, loss = 0.78 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:18:34.078011: step 49230, loss = 0.68 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 17:18:35.369285: step 49240, loss = 0.68 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 17:18:36.654301: step 49250, loss = 0.68 (996.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:18:37.946371: step 49260, loss = 0.82 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:18:39.225118: step 49270, loss = 0.78 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 17:18:40.504337: step 49280, loss = 0.71 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-08 17:18:41.778078: step 49290, loss = 0.82 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-08 17:18:43.146760: step 49300, loss = 0.70 (935.2 examples/sec; 0.137 sec/batch)
2017-05-08 17:18:44.335216: step 49310, loss = 0.79 (1077.0 examples/sec; 0.119 sec/batch)
2017-05-08 17:18:45.628696: step 49320, loss = 0.73 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:18:46.923175: step 49330, loss = 0.82 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:18:48.220374: step 49340, loss = 0.81 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 17:18:49.476864: step 49350, loss = 0.75 (1018.7 examples/sec; 0.126 sec/batch)
2017-05-08 17:18:50.754244: step 49360, loss = 0.64 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 17:18:52.021516: step 49370, loss = 0.67 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-08 17:18:53.302315: step 49380, loss = 0.73 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:18:54.606971: step 49390, loss = 0.87 (981.1 examples/sec; 0.130 sec/batch)
2017-05-08 17:18:55.982712: step 49400, loss = 0.70 (930.4 examples/sec; 0.138 sec/batch)
2017-05-08 17:18:57.158286: step 49410, loss = 0.83 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-08 17:18:58.451050: step 49420, loss = 0.87 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:18:59.719044: step 49430, loss = 0.76 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-08 17:19:01.053256: step 49440, loss = 0.82 (959.4 examples/sec; 0.133 sec/batch)
2017-05-08 17:19:02.337511: step 49450, loss = 0.82 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:19:03.639290: step 49460, loss = 0.91 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 17:19:04.943127: step 49470, loss = 0.82 (981.7 examples/sec; 0.130 sec/batch)
2017-05-08 17:19:06.209162: step 49480, loss = 0.84 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 17:19:07.467789: step 49490, loss = 0.79 (1017.0 examples/sec; 0.126 sec/batch)
2017-05-08 17:19:08.862356: step 49500, loss = 0.75 (917.8 examples/sec; 0.139 sec/batch)
2017-05-08 17:19:10.090241: step 49510, loss = 0.84 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-08 17:19:11.368390: step 49520, loss = 0.78 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:19:12.665796: step 49530, loss = 0.71 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 17:19:13.946064: step 49540, loss = 0.77 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 17:19:15.251591: step 49550, loss = 0.65 (980.4 examples/sec; 0.131 sec/batch)
2017-05-08 17:19:16.528152: step 49560, loss = 0.65 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:19:17.795850: step 49570, loss = 0.80 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 17:19:19.075246: step 49580, loss = 0.75 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-08 17:19:20.391136: step 49590, loss = 0.69 (972.7 examples/sec; 0.132 sec/batch)
2017-05-08 17:19:21.765046: step 49600, loss = 0.85 (931.6 examples/sec; 0.137 sec/batch)
2017-05-08 17:19:23.003729: step 49610, loss = 1.00 (1033.4 examples/sec; 0.124 sec/batch)
2017-05-08 17:19:24.302534: step 49620, loss = 0.78 (985.5 examples/sec; 0.130 sec/batch)
2017-05-08 17:19:25.615382: step 49630, loss = 0.83 (975.0 examples/sec; 0.131 sec/batch)
2017-05-08 17:19:26.918762: step 49640, loss = 0.78 (982.1 examples/sec; 0.130 sec/batch)
2017-05-08 17:19:28.202366: step 49650, loss = 0.70 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 17:19:29.481378: step 49660, loss = 0.81 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 17:19:30.781731: step 49670, loss = 0.78 (984.3 examples/sec; 0.130 sec/batch)
2017-05-08 17:19:32.097358: step 49680, loss = 0.90 (972.9 examples/sec; 0.132 sec/batch)
2017-05-08 17:19:33.409481: step 49690, loss = 0.80 (975.5 examples/sec; 0.131 sec/batch)
2017-05-08 17:19:34.780222: step 49700, loss = 0.87 (933.8 examples/sec; 0.137 sec/batch)
2017-05-08 17:19:35.971870: step 49710, loss = 0.78 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-08 17:19:37.248051: step 49720, loss = 0.85 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 17:19:38.538214: step 49730, loss = 0.59 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:19:39.814639: step 49740, loss = 0.82 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 17:19:41.101223: step 49750, loss = 0.84 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:19:42.404107: step 49760, loss = 0.83 (982.4 examples/sec; 0.130 sec/batch)
2017-05-08 17:19:43.695374: step 49770, loss = 0.83 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 17:19:45.018607: step 49780, loss = 0.68 (967.3 examples/sec; 0.132 sec/batch)
2017-05-08 17:19:46.302871: step 49790, loss = 0.76 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:19:47.685030: step 49800, loss = 0.88 (926.1 examples/sec; 0.138 sec/batch)
2017-05-08 17:19:48.896191: step 49810, loss = 0.79 (1056.8 examples/sec; 0.121 sec/batch)
2017-05-08 17:19:50.192631: step 49820, loss = 0.55 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 17:19:51.482411: step 49830, loss = 0.70 (992.4 examples/sec; 0.129 sec/batch)
2017-05-08 17:19:52.800431: step 49840, loss = 1.04 (971.2 examples/sec; 0.132 sec/batch)
2017-05-08 17:19:54.098530: step 49850, loss = 0.89 (986.1 examples/sec; 0.130 sec/batch)
2017-05-08 17:19:55.377530: step 49860, loss = 0.71 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 17:19:56.703091: step 49870, loss = 0.76 (965.6 examples/sec; 0.133 sec/batch)
2017-05-08 17:19:58.008585: step 49880, loss = 0.81 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 17:19:59.301432: step 49890, loss = 0.92 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:20:00.670654: step 49900, loss = 0.84 (934.8 examples/sec; 0.137 sec/batch)
2017-05-08 17:20:01.940812: step 49910, loss = 0.81 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-08 17:20:03.221539: step 49920, loss = 0.87 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:20:04.483851: step 49930, loss = 0.74 (1014.0 examples/sec; 0.126 sec/batch)
2017-05-08 17:20:05.778586: step 49940, loss = 1.05 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:20:07.083045: step 49950, loss = 0.60 (981.2 examples/sec; 0.130 sec/batch)
2017-05-08 17:20:08.059004: step 49960, loss = 0.63 (1311.5 examples/sec; 0.098 sec/batch)
2017-05-08 17:20:08.922987: step 49970, loss = 0.72 (1481.5 examples/sec; 0.086 sec/batch)
2017-05-08 17:20:09.769815: step 49980, loss = 0.79 (1511.5 examples/sec; 0.085 sec/batch)
2017-05-08 17:20:10.638890: step 49990, loss = 0.77 (1472.8 examples/sec; 0.087 sec/batch)
2017-05-08 17:20:11.590652: step 50000, loss = 0.75 (1344.9 examples/sec; 0.095 sec/batch)
--- 6473.33298516 seconds ---
