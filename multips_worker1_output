I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.7335
pciBusID 0000:04:00.0
Total memory: 7.92GiB
Free memory: 3.57GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x47a5f00
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: 
name: Quadro K620
major: 5 minor: 0 memoryClockRate (GHz) 1.124
pciBusID 0000:03:00.0
Total memory: 1.95GiB
Free memory: 1.82GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:04:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:962] Ignoring gpu device (device: 1, name: Quadro K620, pci bus id: 0000:03:00.0) with Cuda multiprocessor count: 3. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Received variable values from ps
Connected to both PSs
2017-05-08 15:32:33.306544: step 0, loss = 4.67 (78.6 examples/sec; 1.629 sec/batch)
2017-05-08 15:32:34.243274: step 10, loss = 4.62 (1366.5 examples/sec; 0.094 sec/batch)
2017-05-08 15:32:35.271288: step 20, loss = 4.57 (1245.1 examples/sec; 0.103 sec/batch)
2017-05-08 15:32:36.563933: step 30, loss = 4.58 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:32:37.847603: step 40, loss = 4.41 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:32:39.132741: step 50, loss = 4.65 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:32:40.424751: step 60, loss = 4.28 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:32:41.710534: step 70, loss = 4.28 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:32:42.993012: step 80, loss = 4.19 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:32:44.274603: step 90, loss = 4.06 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:32:45.688489: step 100, loss = 4.25 (905.3 examples/sec; 0.141 sec/batch)
2017-05-08 15:32:46.861582: step 110, loss = 4.11 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-08 15:32:48.164224: step 120, loss = 3.90 (982.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:32:49.494688: step 130, loss = 4.02 (962.1 examples/sec; 0.133 sec/batch)
2017-05-08 15:32:50.794968: step 140, loss = 3.89 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:32:52.078340: step 150, loss = 3.75 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:32:53.368554: step 160, loss = 4.08 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:32:54.646958: step 170, loss = 3.76 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:32:55.931553: step 180, loss = 3.77 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:32:57.208189: step 190, loss = 3.92 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:32:58.619904: step 200, loss = 3.64 (906.7 examples/sec; 0.141 sec/batch)
2017-05-08 15:32:59.774827: step 210, loss = 3.71 (1108.3 examples/sec; 0.115 sec/batch)
2017-05-08 15:33:01.071323: step 220, loss = 3.49 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:33:02.365253: step 230, loss = 3.68 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:33:03.673364: step 240, loss = 3.50 (978.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:33:04.976511: step 250, loss = 3.44 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:33:06.308389: step 260, loss = 3.51 (961.1 examples/sec; 0.133 sec/batch)
2017-05-08 15:33:07.587963: step 270, loss = 3.43 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:33:08.888110: step 280, loss = 3.27 (984.5 examples/sec; 0.130 sec/batch)
2017-05-08 15:33:10.189779: step 290, loss = 3.31 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:33:11.576956: step 300, loss = 3.35 (922.7 examples/sec; 0.139 sec/batch)
2017-05-08 15:33:12.766012: step 310, loss = 3.72 (1076.5 examples/sec; 0.119 sec/batch)
2017-05-08 15:33:14.051504: step 320, loss = 3.25 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:33:15.335195: step 330, loss = 3.33 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:33:16.625398: step 340, loss = 3.08 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:33:17.943528: step 350, loss = 3.33 (971.1 examples/sec; 0.132 sec/batch)
2017-05-08 15:33:19.236818: step 360, loss = 3.24 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:33:20.564996: step 370, loss = 3.06 (963.7 examples/sec; 0.133 sec/batch)
2017-05-08 15:33:21.853659: step 380, loss = 3.25 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:33:23.173488: step 390, loss = 3.19 (969.8 examples/sec; 0.132 sec/batch)
2017-05-08 15:33:24.554586: step 400, loss = 2.91 (926.8 examples/sec; 0.138 sec/batch)
2017-05-08 15:33:25.753247: step 410, loss = 2.95 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-08 15:33:27.040575: step 420, loss = 3.07 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:33:28.372939: step 430, loss = 2.99 (960.7 examples/sec; 0.133 sec/batch)
2017-05-08 15:33:29.655517: step 440, loss = 2.97 (998.0 examples/sec; 0.128 sE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 23 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
ec/batch)
2017-05-08 15:33:30.971237: step 450, loss = 2.93 (972.9 examples/sec; 0.132 sec/batch)
2017-05-08 15:33:32.277075: step 460, loss = 2.88 (980.2 examples/sec; 0.131 sec/batch)
2017-05-08 15:33:33.575519: step 470, loss = 2.87 (985.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:33:34.899984: step 480, loss = 2.70 (966.4 examples/sec; 0.132 sec/batch)
2017-05-08 15:33:36.198932: step 490, loss = 2.66 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:33:37.597985: step 500, loss = 2.73 (914.9 examples/sec; 0.140 sec/batch)
2017-05-08 15:33:38.811213: step 510, loss = 2.72 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-08 15:33:40.093192: step 520, loss = 2.57 (998.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:33:41.385884: step 530, loss = 2.62 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:33:42.698527: step 540, loss = 2.79 (975.1 examples/sec; 0.131 sec/batch)
2017-05-08 15:33:44.003066: step 550, loss = 2.86 (981.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:33:45.329996: step 560, loss = 2.73 (964.6 examples/sec; 0.133 sec/batch)
2017-05-08 15:33:46.614354: step 570, loss = 2.56 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:33:47.895129: step 580, loss = 2.65 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:33:49.197253: step 590, loss = 2.61 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:33:50.603544: step 600, loss = 2.69 (910.2 examples/sec; 0.141 sec/batch)
2017-05-08 15:33:51.785640: step 610, loss = 2.50 (1082.8 examples/sec; 0.118 sec/batch)
2017-05-08 15:33:53.089215: step 620, loss = 2.67 (981.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:33:54.361604: step 630, loss = 2.40 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-08 15:33:55.647287: step 640, loss = 2.53 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:33:56.945850: step 650, loss = 2.56 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:33:58.263228: step 660, loss = 2.50 (971.6 examples/sec; 0.132 sec/batch)
2017-05-08 15:33:59.549651: step 670, loss = 2.32 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:34:00.835980: step 680, loss = 2.24 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:34:02.128400: step 690, loss = 2.38 (990.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:34:03.514828: step 700, loss = 2.28 (923.2 examples/sec; 0.139 sec/batch)
2017-05-08 15:34:04.703757: step 710, loss = 2.41 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-08 15:34:05.992376: step 720, loss = 2.19 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:34:07.287731: step 730, loss = 2.14 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:34:08.574794: step 740, loss = 2.55 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:34:09.860996: step 750, loss = 2.29 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:34:11.146135: step 760, loss = 2.40 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:34:12.425768: step 770, loss = 2.35 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:34:13.707014: step 780, loss = 2.16 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 15:34:14.994418: step 790, loss = 2.22 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:34:16.397630: step 800, loss = 2.19 (912.2 examples/sec; 0.140 sec/batch)
2017-05-08 15:34:17.621762: step 810, loss = 2.18 (1045.6 examples/sec; 0.122 sec/batch)
2017-05-08 15:34:18.936436: step 820, loss = 2.25 (973.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:34:20.230769: step 830, loss = 2.16 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:34:21.541302: step 840, loss = 1.90 (976.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:34:22.826369: step 850, loss = 2.26 (996.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:34:24.106141: step 860, loss = 2.07 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:34:25.409946: step 870, loss = 2.07 (981.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:34:26.703655: step 880, loss = 2.03 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:34:27.983895: step 890, loss = 1.96 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:34:29.378913: step 900, loss = 2.12 (917.6 examples/sec; 0.140 sec/batch)
2017-05-08 15:34:30.556082: step 910, loss = 2.07 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-08 15:34:31.857995: step 920, loss = 2.20 (983.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:34:33.182940: step 930, loss = 2.37 (966.1 examples/sec; 0.132 sec/batch)
2017-05-08 15:34:34.499427: step 940, loss = 2.11 (972.3 examples/sec; 0.132 sec/batch)
2017-05-08 15:34:35.768469: step 950, loss = 2.00 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-08 15:34:37.050719: step 960, loss = 1.85 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:34:38.327058: step 970, loss = 1.82 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 15:34:39.624707: step 980, loss = 1.95 (986.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:34:40.905209: step 990, loss = 1.77 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:34:42.289841: step 1000, loss = 1.94 (924.4 examples/sec; 0.138 sec/batch)
2017-05-08 15:34:43.487167: step 1010, loss = 1.85 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-08 15:34:44.765228: step 1020, loss = 1.91 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:34:46.082834: step 1030, loss = 2.12 (971.5 examples/sec; 0.132 sec/batch)
2017-05-08 15:34:47.362488: step 1040, loss = 1.95 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:34:48.659333: step 1050, loss = 1.84 (987.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:34:49.938554: step 1060, loss = 1.94 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:34:51.221864: step 1070, loss = 1.78 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:34:52.504017: step 1080, loss = 2.00 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:34:53.781482: step 1090, loss = 1.73 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 15:34:55.153897: step 1100, loss = 1.96 (932.6 examples/sec; 0.137 sec/batch)
2017-05-08 15:34:56.329169: step 1110, loss = 1.65 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-08 15:34:57.600267: step 1120, loss = 1.71 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 15:34:58.878493: step 1130, loss = 1.95 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:35:00.155836: step 1140, loss = 1.84 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 15:35:01.488789: step 1150, loss = 1.81 (960.3 examples/sec; 0.133 sec/batch)
2017-05-08 15:35:02.767383: step 1160, loss = 2.00 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:35:04.049563: step 1170, loss = 1.88 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:35:05.351806: step 1180, loss = 1.95 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:35:06.641711: step 1190, loss = 1.80 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:35:08.023309: step 1200, loss = 1.83 (926.5 examples/sec; 0.138 sec/batch)
2017-05-08 15:35:09.261025: step 1210, loss = 1.72 (1034.2 examples/sec; 0.124 sec/batch)
2017-05-08 15:35:10.565012: step 1220, loss = 1.71 (981.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:35:11.879675: step 1230, loss = 1.91 (973.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:35:13.187665: step 1240, loss = 1.92 (978.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:35:14.489745: step 1250, loss = 1.66 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:35:15.749187: step 1260, loss = 1.74 (1016.3 examples/sec; 0.126 sec/batch)
2017-05-08 15:35:17.056697: step 1270, loss = 1.54 (979.0 examples/sec; 0.131 sec/batch)
2017-05-08 15:35:18.371557: step 1280, loss = 1.65 (973.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:35:19.642581: step 1290, loss = 1.45 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 15:35:21.023283: step 1300, loss = 1.48 (927.1 examples/sec; 0.138 sec/batch)
2017-05-08 15:35:22.225530: step 1310, loss = 1.50 (1064.7 examples/sec; 0.120 sec/batch)
2017-05-08 15:35:23.517525: step 1320, loss = 1.60 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:35:24.826757: step 1330, loss = 1.65 (977.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:35:26.122125: step 1340, loss = 1.44 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:35:27.416284: step 1350, loss = 1.78 (989.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:35:28.683980: step 1360, loss = 1.44 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 15:35:29.977576: step 1370, loss = 1.47 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:35:31.260440: step 1380, loss = 1.72 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:35:32.546773: step 1390, loss = 1.78 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:35:33.971517: step 1400, loss = 1.52 (898.4 examples/sec; 0.142 sec/batch)
2017-05-08 15:35:35.187182: step 1410, loss = 1.54 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-08 15:35:36.464411: step 1420, loss = 1.61 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:35:37.772376: step 1430, loss = 1.52 (978.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:35:39.073358: step 1440, loss = 1.36 (983.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:35:40.376149: step 1450, loss = 1.78 (982.5 examples/sec; 0.130 sec/batch)
2017-05-08 15:35:41.695618: step 1460, loss = 1.64 (970.1 examples/sec; 0.132 sec/batch)
2017-05-08 15:35:43.002238: step 1470, loss = 1.69 (979.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:35:44.288368: step 1480, loss = 1.61 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:35:45.565023: step 1490, loss = 1.44 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:35:46.944740: step 1500, loss = 1.39 (927.7 examples/sec; 0.138 sec/batch)
2017-05-08 15:35:48.117004: step 1510, loss = 1.51 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-08 15:35:49.398308: step 1520, loss = 1.41 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 15:35:50.689323: step 1530, loss = 1.66 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:35:51.988861: step 1540, loss = 1.76 (985.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:35:53.290564: step 1550, loss = 1.50 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:35:54.592719: step 1560, loss = 1.40 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:35:55.875215: step 1570, loss = 1.55 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:35:57.167753: step 1580, loss = 1.71 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:35:58.459144: step 1590, loss = 1.54 (991.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:35:59.840631: step 1600, loss = 1.64 (926.5 examples/sec; 0.138 sec/batch)
2017-05-08 15:36:01.068303: step 1610, loss = 1.60 (1042.6 examples/sec; 0.123 sec/batch)
2017-05-08 15:36:02.388946: step 1620, loss = 1.38 (969.2 examples/sec; 0.132 sec/batch)
2017-05-08 15:36:03.661732: step 1630, loss = 1.38 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-08 15:36:04.940176: step 1640, loss = 1.46 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:36:06.240598: step 1650, loss = 1.43 (984.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:36:07.558534: step 1660, loss = 1.68 (971.2 examples/sec; 0.132 sec/batch)
2017-05-08 15:36:08.847283: step 1670, loss = 1.41 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:36:10.149935: step 1680, loss = 1.68 (982.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:36:11.445378: step 1690, loss = 1.28 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:36:12.841079: step 1700, loss = 1.53 (917.1 examples/sec; 0.140 sec/batch)
2017-05-08 15:36:14.059834: step 1710, loss = 1.29 (1050.3 examples/sec; 0.122 sec/batch)
2017-05-08 15:36:15.336790: step 1720, loss = 1.53 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:36:16.648676: step 1730, loss = 1.38 (975.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:36:17.945739: step 1740, loss = 1.59 (986.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:36:19.268520: step 1750, loss = 1.44 (967.7 examples/sec; 0.132 sec/batch)
2017-05-08 15:36:20.548817: step 1760, loss = 1.51 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:36:21.836970: step 1770, loss = 1.16 (993.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:36:23.129236: step 1780, loss = 1.27 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:36:24.393969: step 1790, loss = 1.43 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-08 15:36:25.765863: step 1800, loss = 1.37 (933.0 examples/sec; 0.137 sec/batch)
2017-05-08 15:36:26.967710: step 1810, loss = 1.41 (1065.0 examples/sec; 0.120 sec/batch)
2017-05-08 15:36:28.287042: step 1820, loss = 1.60 (970.2 examples/sec; 0.132 sec/batch)
2017-05-08 15:3E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 43 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
6:29.571910: step 1830, loss = 1.30 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:36:30.879195: step 1840, loss = 1.53 (979.1 examples/sec; 0.131 sec/batch)
2017-05-08 15:36:32.156115: step 1850, loss = 1.43 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:36:33.439896: step 1860, loss = 1.49 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:36:34.725178: step 1870, loss = 1.37 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:36:36.010760: step 1880, loss = 1.37 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:36:37.297150: step 1890, loss = 1.36 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:36:38.661436: step 1900, loss = 1.32 (938.2 examples/sec; 0.136 sec/batch)
2017-05-08 15:36:39.850216: step 1910, loss = 1.28 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-08 15:36:41.137140: step 1920, loss = 1.47 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:36:42.422512: step 1930, loss = 1.35 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:36:43.691505: step 1940, loss = 1.33 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-08 15:36:44.972697: step 1950, loss = 1.45 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:36:46.291661: step 1960, loss = 1.16 (970.5 examples/sec; 0.132 sec/batch)
2017-05-08 15:36:47.589832: step 1970, loss = 1.35 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:36:48.906993: step 1980, loss = 1.26 (971.8 examples/sec; 0.132 sec/batch)
2017-05-08 15:36:50.223098: step 1990, loss = 1.71 (972.6 examples/sec; 0.132 sec/batch)
2017-05-08 15:36:51.634925: step 2000, loss = 1.37 (906.6 examples/sec; 0.141 sec/batch)
2017-05-08 15:36:52.866198: step 2010, loss = 1.57 (1039.6 examples/sec; 0.123 sec/batch)
2017-05-08 15:36:54.175812: step 2020, loss = 1.78 (977.4 examples/sec; 0.131 sec/batch)
2017-05-08 15:36:55.478842: step 2030, loss = 1.24 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:36:56.764623: step 2040, loss = 1.35 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:36:58.103215: step 2050, loss = 1.36 (956.2 examples/sec; 0.134 sec/batch)
2017-05-08 15:36:59.381965: step 2060, loss = 1.27 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 15:37:00.650488: step 2070, loss = 1.32 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-08 15:37:01.928008: step 2080, loss = 1.18 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 15:37:03.243074: step 2090, loss = 1.38 (973.3 examples/sec; 0.132 sec/batch)
2017-05-08 15:37:04.660841: step 2100, loss = 1.39 (902.8 examples/sec; 0.142 sec/batch)
2017-05-08 15:37:05.874241: step 2110, loss = 1.15 (1054.9 examples/sec; 0.121 sec/batch)
2017-05-08 15:37:07.166549: step 2120, loss = 1.17 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:37:08.437930: step 2130, loss = 1.12 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-08 15:37:09.745325: step 2140, loss = 1.45 (979.0 examples/sec; 0.131 sec/batch)
2017-05-08 15:37:11.030768: step 2150, loss = 1.11 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:37:12.339863: step 2160, loss = 1.10 (977.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:37:13.644897: step 2170, loss = 1.15 (980.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:37:14.962404: step 2180, loss = 1.06 (971.5 examples/sec; 0.132 sec/batch)
2017-05-08 15:37:16.299692: step 2190, loss = 1.18 (957.2 examples/sec; 0.134 sec/batch)
2017-05-08 15:37:17.708515: step 2200, loss = 1.27 (908.6 examples/sec; 0.141 sec/batch)
2017-05-08 15:37:18.939285: step 2210, loss = 1.23 (1040.0 examples/sec; 0.123 sec/batch)
2017-05-08 15:37:20.245097: step 2220, loss = 1.15 (980.2 examples/sec; 0.131 sec/batch)
2017-05-08 15:37:21.531532: step 2230, loss = 1.24 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:37:22.859105: step 2240, loss = 1.49 (964.2 examples/sec; 0.133 sec/batch)
2017-05-08 15:37:24.159237: step 2250, loss = 1.25 (984.5 examples/sec; 0.130 sec/batch)
2017-05-08 15:37:25.463523: step 2260, loss = 1.37 (981.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:37:26.787417: step 2270, loss = 1.15 (966.8 examples/sec; 0.132 sec/batch)
2017-05-08 15:37:28.092035: step 2280, loss = 1.40 (981.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:37:29.399050: step 2290, loss = 1.35 (979.3 examples/sec; 0.131 sec/batch)
2017-05-08 15:37:30.779403: step 2300, loss = 1.55 (927.3 examples/sec; 0.138 sec/batch)
2017-05-08 15:37:32.013750: step 2310, loss = 1.34 (1037.0 examples/sec; 0.123 sec/batch)
2017-05-08 15:37:33.348327: step 2320, loss = 1.19 (959.1 examples/sec; 0.133 sec/batch)
2017-05-08 15:37:34.647943: step 2330, loss = 1.34 (984.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:37:35.972010: step 2340, loss = 1.18 (966.7 examples/sec; 0.132 sec/batch)
2017-05-08 15:37:37.286199: step 2350, loss = 1.24 (974.0 examples/sec; 0.131 sec/batch)
2017-05-08 15:37:38.592070: step 2360, loss = 1.18 (980.2 examples/sec; 0.131 sec/batch)
2017-05-08 15:37:39.888996: step 2370, loss = 1.25 (987.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:37:41.185117: step 2380, loss = 1.23 (987.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:37:42.502014: step 2390, loss = 1.20 (972.0 examples/sec; 0.132 sec/batch)
2017-05-08 15:37:43.895906: step 2400, loss = 1.19 (918.3 examples/sec; 0.139 sec/batch)
2017-05-08 15:37:45.086326: step 2410, loss = 1.07 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-08 15:37:46.417656: step 2420, loss = 1.19 (961.6 examples/sec; 0.133 sec/batch)
2017-05-08 15:37:47.708371: step 2430, loss = 1.74 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:37:49.030357: step 2440, loss = 1.57 (968.2 examples/sec; 0.132 sec/batch)
2017-05-08 15:37:50.321054: step 2450, loss = 1.30 (991.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:37:51.614586: step 2460, loss = 1.23 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:37:52.906388: step 2470, loss = 1.13 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:37:54.222035: step 2480, loss = 1.36 (972.9 examples/sec; 0.132 sec/batch)
2017-05-08 15:37:55.501004: step 2490, loss = 1.18 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:37:56.904432: step 2500, loss = 1.25 (912.1 examples/sec; 0.140 sec/batch)
2017-05-08 15:37:58.111895: step 2510, loss = 1.11 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-08 15:37:59.420407: step 2520, loss = 1.24 (978.2 examples/sec; 0.131 sec/batch)
2017-05-08 15:38:00.726181: step 2530, loss = 1.09 (980.3 examples/sec; 0.131 sec/batch)
2017-05-08 15:38:02.011489: step 2540, loss = 1.25 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:38:03.300410: step 2550, loss = 1.21 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:38:04.607962: step 2560, loss = 1.18 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 15:38:05.918033: step 2570, loss = 1.18 (977.1 examples/sec; 0.131 sec/batch)
2017-05-08 15:38:07.250065: step 2580, loss = 1.30 (960.9 examples/sec; 0.133 sec/batch)
2017-05-08 15:38:08.539755: step 2590, loss = 1.10 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:38:09.942822: step 2600, loss = 1.39 (912.3 examples/sec; 0.140 sec/batch)
2017-05-08 15:38:11.128960: step 2610, loss = 1.25 (1079.1 examples/sec; 0.119 sec/batch)
2017-05-08 15:38:12.400044: step 2620, loss = 1.26 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 15:38:13.693825: step 2630, loss = 1.04 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:38:15.020096: step 2640, loss = 1.10 (965.1 examples/sec; 0.133 sec/batch)
2017-05-08 15:38:16.351896: step 2650, loss = 1.25 (961.1 examples/sec; 0.133 sec/batch)
2017-05-08 15:38:17.660700: step 2660, loss = 1.58 (978.0 examples/sec; 0.131 sec/batch)
2017-05-08 15:38:18.950365: step 2670, loss = 1.19 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:38:20.242567: step 2680, loss = 1.24 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:38:21.534180: step 2690, loss = 1.04 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:38:22.930889: step 2700, loss = 1.17 (916.4 examples/sec; 0.140 sec/batch)
2017-05-08 15:38:24.127001: step 2710, loss = 1.30 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-08 15:38:25.423141: step 2720, loss = 1.08 (987.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:38:26.699873: step 2730, loss = 1.21 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:38:27.980487: step 2740, loss = 1.07 (999.5 examples/sec; 0.128 sec/batch)
2E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 63 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
017-05-08 15:38:29.271799: step 2750, loss = 1.38 (991.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:38:30.565753: step 2760, loss = 1.33 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:38:31.845099: step 2770, loss = 1.27 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:38:33.177733: step 2780, loss = 1.02 (960.5 examples/sec; 0.133 sec/batch)
2017-05-08 15:38:34.473639: step 2790, loss = 1.44 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:38:35.886122: step 2800, loss = 1.33 (906.2 examples/sec; 0.141 sec/batch)
2017-05-08 15:38:37.082992: step 2810, loss = 1.03 (1069.5 examples/sec; 0.120 sec/batch)
2017-05-08 15:38:38.366727: step 2820, loss = 1.25 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:38:39.658488: step 2830, loss = 1.32 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:38:40.963722: step 2840, loss = 1.11 (980.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:38:42.242764: step 2850, loss = 1.32 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:38:43.518585: step 2860, loss = 1.06 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:38:44.806529: step 2870, loss = 1.25 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:38:46.088182: step 2880, loss = 1.17 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:38:47.386864: step 2890, loss = 1.07 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:38:48.790316: step 2900, loss = 1.26 (912.0 examples/sec; 0.140 sec/batch)
2017-05-08 15:38:50.017106: step 2910, loss = 1.12 (1043.4 examples/sec; 0.123 sec/batch)
2017-05-08 15:38:51.334415: step 2920, loss = 1.29 (971.7 examples/sec; 0.132 sec/batch)
2017-05-08 15:38:52.650599: step 2930, loss = 0.98 (972.5 examples/sec; 0.132 sec/batch)
2017-05-08 15:38:53.950879: step 2940, loss = 1.35 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:38:55.240931: step 2950, loss = 0.99 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:38:56.522976: step 2960, loss = 1.24 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:38:57.809072: step 2970, loss = 1.08 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:38:59.109657: step 2980, loss = 1.00 (984.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:39:00.390872: step 2990, loss = 0.99 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:39:01.783867: step 3000, loss = 1.02 (918.9 examples/sec; 0.139 sec/batch)
2017-05-08 15:39:02.994535: step 3010, loss = 0.99 (1057.3 examples/sec; 0.121 sec/batch)
2017-05-08 15:39:04.290711: step 3020, loss = 1.36 (987.5 examples/sec; 0.130 sec/batch)
2017-05-08 15:39:05.622760: step 3030, loss = 1.15 (960.9 examples/sec; 0.133 sec/batch)
2017-05-08 15:39:06.911617: step 3040, loss = 0.99 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:39:08.234435: step 3050, loss = 1.14 (967.6 examples/sec; 0.132 sec/batch)
2017-05-08 15:39:09.559030: step 3060, loss = 0.97 (966.3 examples/sec; 0.132 sec/batch)
2017-05-08 15:39:10.841247: step 3070, loss = 1.27 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:39:12.134640: step 3080, loss = 1.26 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:39:13.436380: step 3090, loss = 1.14 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:39:14.827171: step 3100, loss = 1.11 (920.3 examples/sec; 0.139 sec/batch)
2017-05-08 15:39:16.021836: step 3110, loss = 1.05 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-08 15:39:17.340279: step 3120, loss = 0.97 (970.8 examples/sec; 0.132 sec/batch)
2017-05-08 15:39:18.619817: step 3130, loss = 1.32 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:39:19.894626: step 3140, loss = 1.02 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 15:39:21.192363: step 3150, loss = 1.24 (986.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:39:22.495226: step 3160, loss = 1.17 (982.5 examples/sec; 0.130 sec/batch)
2017-05-08 15:39:23.794164: step 3170, loss = 1.28 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:39:25.105250: step 3180, loss = 1.14 (976.3 examples/sec; 0.131 sec/batch)
2017-05-08 15:39:26.418358: step 3190, loss = 1.13 (974.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:39:27.811786: step 3200, loss = 1.19 (918.6 examples/sec; 0.139 sec/batch)
2017-05-08 15:39:29.025060: step 3210, loss = 1.00 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-08 15:39:30.347602: step 3220, loss = 1.42 (967.8 examples/sec; 0.132 sec/batch)
2017-05-08 15:39:31.653922: step 3230, loss = 1.28 (979.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:39:32.956249: step 3240, loss = 1.22 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:39:34.259542: step 3250, loss = 1.32 (982.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:39:35.541831: step 3260, loss = 1.14 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:39:36.843837: step 3270, loss = 1.13 (983.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:39:38.187572: step 3280, loss = 0.94 (952.6 examples/sec; 0.134 sec/batch)
2017-05-08 15:39:39.462713: step 3290, loss = 1.07 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:39:40.862871: step 3300, loss = 1.49 (914.2 examples/sec; 0.140 sec/batch)
2017-05-08 15:39:42.078890: step 3310, loss = 1.12 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-08 15:39:43.379375: step 3320, loss = 1.19 (984.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:39:44.699848: step 3330, loss = 1.24 (969.4 examples/sec; 0.132 sec/batch)
2017-05-08 15:39:45.977834: step 3340, loss = 1.11 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:39:47.272615: step 3350, loss = 1.18 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:39:48.582181: step 3360, loss = 1.12 (977.4 examples/sec; 0.131 sec/batch)
2017-05-08 15:39:49.886660: step 3370, loss = 1.21 (981.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:39:51.225727: step 3380, loss = 1.09 (955.9 examples/sec; 0.134 sec/batch)
2017-05-08 15:39:52.540035: step 3390, loss = 1.30 (973.9 examples/sec; 0.131 sec/batch)
2017-05-08 15:39:53.936497: step 3400, loss = 1.19 (916.6 examples/sec; 0.140 sec/batch)
2017-05-08 15:39:55.136190: step 3410, loss = 1.06 (1066.9 examples/sec; 0.120 sec/batch)
2017-05-08 15:39:56.452575: step 3420, loss = 1.27 (972.4 examples/sec; 0.132 sec/batch)
2017-05-08 15:39:57.764787: step 3430, loss = 1.15 (975.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:39:59.085092: step 3440, loss = 1.16 (969.5 examples/sec; 0.132 sec/batch)
2017-05-08 15:40:00.417860: step 3450, loss = 1.09 (960.4 examples/sec; 0.133 sec/batch)
2017-05-08 15:40:01.742158: step 3460, loss = 1.15 (966.6 examples/sec; 0.132 sec/batch)
2017-05-08 15:40:03.036968: step 3470, loss = 1.06 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:40:04.332698: step 3480, loss = 1.14 (987.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:40:05.634697: step 3490, loss = 1.26 (983.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:40:07.040974: step 3500, loss = 1.15 (910.2 examples/sec; 0.141 sec/batch)
2017-05-08 15:40:08.255706: step 3510, loss = 0.89 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-08 15:40:09.557253: step 3520, loss = 1.05 (983.5 examples/sec; 0.130 sec/batch)
2017-05-08 15:40:10.874878: step 3530, loss = 1.08 (971.4 examples/sec; 0.132 sec/batch)
2017-05-08 15:40:12.175783: step 3540, loss = 1.32 (983.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:40:13.483250: step 3550, loss = 1.08 (979.0 examples/sec; 0.131 sec/batch)
2017-05-08 15:40:14.784591: step 3560, loss = 1.00 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:40:16.078235: step 3570, loss = 1.12 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:40:17.353221: step 3580, loss = 0.99 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-08 15:40:18.651750: step 3590, loss = 1.04 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:40:20.032586: step 3600, loss = 1.09 (927.0 examples/sec; 0.138 sec/batch)
2017-05-08 15:40:21.249135: step 3610, loss = 1.17 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-08 15:40:22.553423: step 3620, loss = 1.24 (981.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:40:23.813051: step 3630, loss = 0.92 (1016.2 examples/sec; 0.126 sec/batch)
2017-05-08 15:40:25.121738: step 3640, loss = 1.01 (978.1 examples/sec; 0.131 sec/batch)
2017-05-08 15:40:26.401990: step 3650, loss = 1.01 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:40:27.699226: step 3660, loss = 1.08 (986.7 examples/sec; 0.13E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 83 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
0 sec/batch)
2017-05-08 15:40:28.991418: step 3670, loss = 1.08 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:40:30.329141: step 3680, loss = 1.27 (956.9 examples/sec; 0.134 sec/batch)
2017-05-08 15:40:31.631737: step 3690, loss = 1.02 (982.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:40:33.026829: step 3700, loss = 1.03 (917.5 examples/sec; 0.140 sec/batch)
2017-05-08 15:40:34.197118: step 3710, loss = 1.14 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-08 15:40:35.479441: step 3720, loss = 1.04 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:40:36.773598: step 3730, loss = 1.07 (989.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:40:38.046088: step 3740, loss = 1.09 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-08 15:40:39.370953: step 3750, loss = 1.09 (966.1 examples/sec; 0.132 sec/batch)
2017-05-08 15:40:40.661806: step 3760, loss = 0.92 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:40:41.977289: step 3770, loss = 1.26 (973.0 examples/sec; 0.132 sec/batch)
2017-05-08 15:40:43.289002: step 3780, loss = 1.12 (975.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:40:44.594410: step 3790, loss = 1.39 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:40:45.992438: step 3800, loss = 1.04 (915.6 examples/sec; 0.140 sec/batch)
2017-05-08 15:40:47.198650: step 3810, loss = 1.04 (1061.2 examples/sec; 0.121 sec/batch)
2017-05-08 15:40:48.510992: step 3820, loss = 1.18 (975.3 examples/sec; 0.131 sec/batch)
2017-05-08 15:40:49.826870: step 3830, loss = 1.24 (972.7 examples/sec; 0.132 sec/batch)
2017-05-08 15:40:51.124188: step 3840, loss = 1.27 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:40:52.400540: step 3850, loss = 0.93 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 15:40:53.684269: step 3860, loss = 0.85 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:40:55.006525: step 3870, loss = 1.11 (968.0 examples/sec; 0.132 sec/batch)
2017-05-08 15:40:56.307664: step 3880, loss = 0.93 (983.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:40:57.627112: step 3890, loss = 1.24 (970.1 examples/sec; 0.132 sec/batch)
2017-05-08 15:40:59.036286: step 3900, loss = 1.06 (908.3 examples/sec; 0.141 sec/batch)
2017-05-08 15:41:00.233766: step 3910, loss = 0.97 (1068.9 examples/sec; 0.120 sec/batch)
2017-05-08 15:41:01.548337: step 3920, loss = 1.02 (973.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:41:02.862515: step 3930, loss = 1.04 (974.0 examples/sec; 0.131 sec/batch)
2017-05-08 15:41:04.139136: step 3940, loss = 1.22 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:41:05.422293: step 3950, loss = 1.01 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:41:06.749566: step 3960, loss = 1.03 (964.4 examples/sec; 0.133 sec/batch)
2017-05-08 15:41:08.025798: step 3970, loss = 1.12 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 15:41:09.332134: step 3980, loss = 1.06 (979.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:41:10.646237: step 3990, loss = 1.08 (974.0 examples/sec; 0.131 sec/batch)
2017-05-08 15:41:12.052642: step 4000, loss = 1.48 (910.1 examples/sec; 0.141 sec/batch)
2017-05-08 15:41:13.255121: step 4010, loss = 1.07 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-08 15:41:14.544973: step 4020, loss = 0.95 (992.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:41:15.865353: step 4030, loss = 1.09 (969.4 examples/sec; 0.132 sec/batch)
2017-05-08 15:41:17.178290: step 4040, loss = 1.09 (974.9 examples/sec; 0.131 sec/batch)
2017-05-08 15:41:18.519925: step 4050, loss = 1.10 (954.1 examples/sec; 0.134 sec/batch)
2017-05-08 15:41:19.856165: step 4060, loss = 1.13 (957.9 examples/sec; 0.134 sec/batch)
2017-05-08 15:41:21.155535: step 4070, loss = 1.05 (985.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:41:22.461210: step 4080, loss = 1.16 (980.3 examples/sec; 0.131 sec/batch)
2017-05-08 15:41:23.751859: step 4090, loss = 1.04 (991.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:41:25.127783: step 4100, loss = 1.08 (930.3 examples/sec; 0.138 sec/batch)
2017-05-08 15:41:26.343145: step 4110, loss = 0.94 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-08 15:41:27.637545: step 4120, loss = 1.04 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:41:28.964628: step 4130, loss = 1.16 (964.5 examples/sec; 0.133 sec/batch)
2017-05-08 15:41:30.282943: step 4140, loss = 1.07 (970.9 examples/sec; 0.132 sec/batch)
2017-05-08 15:41:31.575720: step 4150, loss = 1.06 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:41:32.864824: step 4160, loss = 1.19 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:41:34.178154: step 4170, loss = 1.09 (974.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:41:35.465066: step 4180, loss = 1.34 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:41:36.793813: step 4190, loss = 0.89 (963.3 examples/sec; 0.133 sec/batch)
2017-05-08 15:41:38.212546: step 4200, loss = 1.20 (902.2 examples/sec; 0.142 sec/batch)
2017-05-08 15:41:39.432899: step 4210, loss = 1.00 (1048.9 examples/sec; 0.122 sec/batch)
2017-05-08 15:41:40.725148: step 4220, loss = 0.92 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:41:42.020288: step 4230, loss = 1.14 (988.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:41:43.294085: step 4240, loss = 1.12 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-08 15:41:44.560078: step 4250, loss = 0.82 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-08 15:41:45.844084: step 4260, loss = 1.00 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 15:41:47.150758: step 4270, loss = 0.92 (979.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:41:48.433960: step 4280, loss = 1.10 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:41:49.734588: step 4290, loss = 1.11 (984.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:41:51.109344: step 4300, loss = 1.00 (931.1 examples/sec; 0.137 sec/batch)
2017-05-08 15:41:52.321376: step 4310, loss = 0.95 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-08 15:41:53.625666: step 4320, loss = 1.15 (981.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:41:54.917973: step 4330, loss = 1.17 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:41:56.180697: step 4340, loss = 0.92 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-08 15:41:57.459936: step 4350, loss = 0.96 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:41:58.755330: step 4360, loss = 1.03 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:42:00.038989: step 4370, loss = 1.08 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:42:01.343171: step 4380, loss = 1.17 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 15:42:02.624401: step 4390, loss = 1.17 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 15:42:04.037111: step 4400, loss = 0.95 (906.1 examples/sec; 0.141 sec/batch)
2017-05-08 15:42:05.244116: step 4410, loss = 1.00 (1060.5 examples/sec; 0.121 sec/batch)
2017-05-08 15:42:06.551147: step 4420, loss = 1.00 (979.3 examples/sec; 0.131 sec/batch)
2017-05-08 15:42:07.833210: step 4430, loss = 1.15 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:42:09.117625: step 4440, loss = 0.97 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:42:10.398324: step 4450, loss = 0.96 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:42:11.713661: step 4460, loss = 1.03 (973.1 examples/sec; 0.132 sec/batch)
2017-05-08 15:42:13.003659: step 4470, loss = 1.06 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:42:14.312288: step 4480, loss = 1.00 (978.1 examples/sec; 0.131 sec/batch)
2017-05-08 15:42:15.573540: step 4490, loss = 0.94 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-08 15:42:16.984965: step 4500, loss = 1.17 (906.9 examples/sec; 0.141 sec/batch)
2017-05-08 15:42:18.176676: step 4510, loss = 0.83 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-08 15:42:19.474282: step 4520, loss = 1.00 (986.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:42:20.768961: step 4530, loss = 1.24 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:42:22.069246: step 4540, loss = 1.06 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:42:23.363551: step 4550, loss = 1.12 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:42:24.656107: step 4560, loss = 1.10 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:42:25.943924: step 4570, loss = 1.02 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:42:27.235014: step 4580, loss = 1.24 (991.4 examE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 104 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
ples/sec; 0.129 sec/batch)
2017-05-08 15:42:28.557257: step 4590, loss = 1.34 (968.0 examples/sec; 0.132 sec/batch)
2017-05-08 15:42:29.949660: step 4600, loss = 1.09 (919.3 examples/sec; 0.139 sec/batch)
2017-05-08 15:42:31.138585: step 4610, loss = 0.93 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-08 15:42:32.435272: step 4620, loss = 0.99 (987.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:42:33.762078: step 4630, loss = 0.96 (964.7 examples/sec; 0.133 sec/batch)
2017-05-08 15:42:35.054279: step 4640, loss = 0.98 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:42:36.341601: step 4650, loss = 1.13 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:42:37.639063: step 4660, loss = 1.16 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:42:38.963712: step 4670, loss = 0.98 (966.3 examples/sec; 0.132 sec/batch)
2017-05-08 15:42:40.269159: step 4680, loss = 1.11 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:42:41.591537: step 4690, loss = 1.23 (968.0 examples/sec; 0.132 sec/batch)
2017-05-08 15:42:43.000128: step 4700, loss = 0.97 (908.7 examples/sec; 0.141 sec/batch)
2017-05-08 15:42:44.206764: step 4710, loss = 1.17 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-08 15:42:45.515184: step 4720, loss = 0.93 (978.3 examples/sec; 0.131 sec/batch)
2017-05-08 15:42:46.839262: step 4730, loss = 0.97 (966.7 examples/sec; 0.132 sec/batch)
2017-05-08 15:42:48.136709: step 4740, loss = 0.90 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:42:49.420058: step 4750, loss = 1.13 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:42:50.738511: step 4760, loss = 1.00 (970.8 examples/sec; 0.132 sec/batch)
2017-05-08 15:42:52.044804: step 4770, loss = 1.19 (979.9 examples/sec; 0.131 sec/batch)
2017-05-08 15:42:53.323369: step 4780, loss = 1.01 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:42:54.618264: step 4790, loss = 0.90 (988.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:42:56.020887: step 4800, loss = 1.18 (912.6 examples/sec; 0.140 sec/batch)
2017-05-08 15:42:57.195148: step 4810, loss = 0.90 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-08 15:42:58.478104: step 4820, loss = 0.87 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:42:59.744432: step 4830, loss = 0.91 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-08 15:43:01.031693: step 4840, loss = 0.98 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:43:02.338661: step 4850, loss = 1.03 (979.4 examples/sec; 0.131 sec/batch)
2017-05-08 15:43:03.624084: step 4860, loss = 1.07 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:43:04.910191: step 4870, loss = 1.03 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:43:06.180177: step 4880, loss = 1.03 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-08 15:43:07.470814: step 4890, loss = 1.06 (991.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:43:08.844437: step 4900, loss = 1.02 (931.8 examples/sec; 0.137 sec/batch)
2017-05-08 15:43:10.037543: step 4910, loss = 1.24 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-08 15:43:11.345518: step 4920, loss = 0.90 (978.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:43:12.643631: step 4930, loss = 1.10 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:43:13.950827: step 4940, loss = 1.22 (979.2 examples/sec; 0.131 sec/batch)
2017-05-08 15:43:15.261711: step 4950, loss = 1.06 (976.4 examples/sec; 0.131 sec/batch)
2017-05-08 15:43:16.538673: step 4960, loss = 0.87 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:43:17.817261: step 4970, loss = 0.87 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:43:19.115434: step 4980, loss = 0.98 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:43:20.434792: step 4990, loss = 1.16 (970.2 examples/sec; 0.132 sec/batch)
2017-05-08 15:43:21.807050: step 5000, loss = 1.01 (932.8 examples/sec; 0.137 sec/batch)
2017-05-08 15:43:23.012844: step 5010, loss = 1.07 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-08 15:43:24.313590: step 5020, loss = 0.91 (984.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:43:25.604580: step 5030, loss = 0.96 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:43:26.908641: step 5040, loss = 1.16 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 15:43:28.220063: step 5050, loss = 1.33 (976.0 examples/sec; 0.131 sec/batch)
2017-05-08 15:43:29.505722: step 5060, loss = 0.93 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:43:30.816431: step 5070, loss = 1.15 (976.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:43:32.150986: step 5080, loss = 1.06 (959.1 examples/sec; 0.133 sec/batch)
2017-05-08 15:43:33.440544: step 5090, loss = 1.07 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:43:34.811615: step 5100, loss = 1.20 (933.6 examples/sec; 0.137 sec/batch)
2017-05-08 15:43:35.974095: step 5110, loss = 1.07 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-08 15:43:37.267278: step 5120, loss = 0.98 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:43:38.540480: step 5130, loss = 0.94 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 15:43:39.815744: step 5140, loss = 1.10 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:43:41.115958: step 5150, loss = 1.29 (984.5 examples/sec; 0.130 sec/batch)
2017-05-08 15:43:42.409094: step 5160, loss = 0.95 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:43:43.698267: step 5170, loss = 1.22 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:43:45.032286: step 5180, loss = 1.14 (959.5 examples/sec; 0.133 sec/batch)
2017-05-08 15:43:46.333361: step 5190, loss = 1.17 (983.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:43:47.730932: step 5200, loss = 1.13 (915.9 examples/sec; 0.140 sec/batch)
2017-05-08 15:43:48.914886: step 5210, loss = 1.05 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-08 15:43:50.221381: step 5220, loss = 0.95 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:43:51.529366: step 5230, loss = 0.98 (978.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:43:52.818273: step 5240, loss = 0.94 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:43:54.135402: step 5250, loss = 1.15 (971.8 examples/sec; 0.132 sec/batch)
2017-05-08 15:43:55.430581: step 5260, loss = 1.08 (988.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:43:56.717635: step 5270, loss = 1.05 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:43:58.011078: step 5280, loss = 1.09 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:43:59.331780: step 5290, loss = 1.12 (969.2 examples/sec; 0.132 sec/batch)
2017-05-08 15:44:00.730712: step 5300, loss = 1.10 (915.0 examples/sec; 0.140 sec/batch)
2017-05-08 15:44:01.936333: step 5310, loss = 1.02 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-08 15:44:03.240603: step 5320, loss = 0.92 (981.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:44:04.546824: step 5330, loss = 0.90 (979.9 examples/sec; 0.131 sec/batch)
2017-05-08 15:44:05.831880: step 5340, loss = 1.15 (996.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:44:07.113065: step 5350, loss = 0.90 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:44:08.397547: step 5360, loss = 1.10 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:44:09.684598: step 5370, loss = 0.99 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:44:10.982045: step 5380, loss = 1.06 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:44:12.287021: step 5390, loss = 0.95 (980.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:44:13.681029: step 5400, loss = 0.83 (918.2 examples/sec; 0.139 sec/batch)
2017-05-08 15:44:14.858693: step 5410, loss = 1.03 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-08 15:44:16.145989: step 5420, loss = 0.97 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:44:17.410280: step 5430, loss = 1.19 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-08 15:44:18.717910: step 5440, loss = 1.19 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 15:44:20.015372: step 5450, loss = 0.87 (986.5 examples/sec; 0.130 sec/batch)
2017-05-08 15:44:21.325229: step 5460, loss = 1.06 (977.2 examples/sec; 0.131 sec/batch)
2017-05-08 15:44:22.631225: step 5470, loss = 0.92 (980.1 examples/sec; 0.131 sec/batch)
2017-05-08 15:44:23.939369: step 5480, loss = 0.98 (978.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:44:25.229330: step 5490, loss = 0.94 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:44:26.616491: step 5500, loss = 0.9E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 124 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
4 (922.8 examples/sec; 0.139 sec/batch)
2017-05-08 15:44:27.795529: step 5510, loss = 1.32 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-08 15:44:29.134349: step 5520, loss = 1.04 (956.1 examples/sec; 0.134 sec/batch)
2017-05-08 15:44:30.457131: step 5530, loss = 0.96 (967.7 examples/sec; 0.132 sec/batch)
2017-05-08 15:44:31.740942: step 5540, loss = 0.97 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 15:44:33.053703: step 5550, loss = 1.24 (975.0 examples/sec; 0.131 sec/batch)
2017-05-08 15:44:34.336725: step 5560, loss = 0.98 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:44:35.642115: step 5570, loss = 0.89 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:44:36.923737: step 5580, loss = 1.09 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:44:38.244852: step 5590, loss = 1.23 (968.9 examples/sec; 0.132 sec/batch)
2017-05-08 15:44:39.654918: step 5600, loss = 1.13 (907.8 examples/sec; 0.141 sec/batch)
2017-05-08 15:44:40.848816: step 5610, loss = 0.99 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-08 15:44:42.164071: step 5620, loss = 1.03 (973.2 examples/sec; 0.132 sec/batch)
2017-05-08 15:44:43.447776: step 5630, loss = 1.25 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:44:44.733507: step 5640, loss = 0.84 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:44:46.020204: step 5650, loss = 0.93 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:44:47.300632: step 5660, loss = 1.03 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:44:48.595031: step 5670, loss = 0.89 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:44:49.872919: step 5680, loss = 1.06 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:44:51.174727: step 5690, loss = 1.15 (983.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:44:52.554672: step 5700, loss = 0.92 (927.6 examples/sec; 0.138 sec/batch)
2017-05-08 15:44:53.724665: step 5710, loss = 0.86 (1094.0 examples/sec; 0.117 sec/batch)
2017-05-08 15:44:55.025140: step 5720, loss = 1.16 (984.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:44:56.325654: step 5730, loss = 1.08 (984.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:44:57.620344: step 5740, loss = 0.86 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:44:58.919478: step 5750, loss = 1.18 (985.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:45:00.210025: step 5760, loss = 1.00 (991.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:45:01.526601: step 5770, loss = 1.03 (972.2 examples/sec; 0.132 sec/batch)
2017-05-08 15:45:02.857725: step 5780, loss = 1.08 (961.6 examples/sec; 0.133 sec/batch)
2017-05-08 15:45:04.179348: step 5790, loss = 0.96 (968.5 examples/sec; 0.132 sec/batch)
2017-05-08 15:45:05.576216: step 5800, loss = 1.13 (916.3 examples/sec; 0.140 sec/batch)
2017-05-08 15:45:06.814702: step 5810, loss = 1.14 (1033.5 examples/sec; 0.124 sec/batch)
2017-05-08 15:45:08.126576: step 5820, loss = 1.13 (975.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:45:09.437027: step 5830, loss = 1.11 (976.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:45:10.738066: step 5840, loss = 1.08 (983.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:45:12.039209: step 5850, loss = 0.84 (983.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:45:13.371784: step 5860, loss = 0.78 (960.5 examples/sec; 0.133 sec/batch)
2017-05-08 15:45:14.685948: step 5870, loss = 1.19 (974.0 examples/sec; 0.131 sec/batch)
2017-05-08 15:45:15.996769: step 5880, loss = 1.13 (976.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:45:17.302213: step 5890, loss = 1.06 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:45:18.715076: step 5900, loss = 1.11 (906.0 examples/sec; 0.141 sec/batch)
2017-05-08 15:45:19.912934: step 5910, loss = 1.06 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-08 15:45:21.235786: step 5920, loss = 1.13 (967.6 examples/sec; 0.132 sec/batch)
2017-05-08 15:45:22.529108: step 5930, loss = 1.08 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:45:23.850584: step 5940, loss = 1.10 (968.6 examples/sec; 0.132 sec/batch)
2017-05-08 15:45:25.174204: step 5950, loss = 0.97 (967.1 examples/sec; 0.132 sec/batch)
2017-05-08 15:45:26.482810: step 5960, loss = 1.01 (978.1 examples/sec; 0.131 sec/batch)
2017-05-08 15:45:27.774530: step 5970, loss = 1.02 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:45:29.066854: step 5980, loss = 1.09 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:45:30.346470: step 5990, loss = 0.95 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:45:31.742007: step 6000, loss = 1.08 (917.2 examples/sec; 0.140 sec/batch)
2017-05-08 15:45:32.942549: step 6010, loss = 1.02 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-08 15:45:34.256645: step 6020, loss = 1.10 (974.1 examples/sec; 0.131 sec/batch)
2017-05-08 15:45:35.520991: step 6030, loss = 0.92 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-08 15:45:36.807590: step 6040, loss = 0.93 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:45:38.111890: step 6050, loss = 0.95 (981.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:45:39.432362: step 6060, loss = 1.35 (969.3 examples/sec; 0.132 sec/batch)
2017-05-08 15:45:40.742615: step 6070, loss = 0.96 (976.9 examples/sec; 0.131 sec/batch)
2017-05-08 15:45:42.021631: step 6080, loss = 1.07 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:45:43.358079: step 6090, loss = 0.95 (957.8 examples/sec; 0.134 sec/batch)
2017-05-08 15:45:44.750532: step 6100, loss = 0.91 (919.2 examples/sec; 0.139 sec/batch)
2017-05-08 15:45:45.945250: step 6110, loss = 0.89 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-08 15:45:47.238402: step 6120, loss = 1.03 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:45:48.508247: step 6130, loss = 0.80 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-08 15:45:49.821254: step 6140, loss = 1.18 (974.9 examples/sec; 0.131 sec/batch)
2017-05-08 15:45:51.127090: step 6150, loss = 1.24 (980.2 examples/sec; 0.131 sec/batch)
2017-05-08 15:45:52.407056: step 6160, loss = 0.91 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 15:45:53.686506: step 6170, loss = 0.96 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:45:54.974339: step 6180, loss = 0.86 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:45:56.257185: step 6190, loss = 0.96 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:45:57.634036: step 6200, loss = 0.84 (929.7 examples/sec; 0.138 sec/batch)
2017-05-08 15:45:58.825994: step 6210, loss = 0.90 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-08 15:46:00.119040: step 6220, loss = 1.03 (989.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:46:01.416820: step 6230, loss = 1.05 (986.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:46:02.728620: step 6240, loss = 0.95 (975.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:46:04.043234: step 6250, loss = 0.96 (973.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:46:05.370032: step 6260, loss = 0.98 (964.7 examples/sec; 0.133 sec/batch)
2017-05-08 15:46:06.674353: step 6270, loss = 0.86 (981.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:46:07.973522: step 6280, loss = 1.00 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:46:09.265580: step 6290, loss = 0.91 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:46:10.661510: step 6300, loss = 0.90 (917.0 examples/sec; 0.140 sec/batch)
2017-05-08 15:46:11.847049: step 6310, loss = 1.01 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-08 15:46:13.140526: step 6320, loss = 0.95 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:46:14.449800: step 6330, loss = 1.04 (977.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:46:15.715357: step 6340, loss = 1.03 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-08 15:46:16.983409: step 6350, loss = 0.91 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-08 15:46:18.281234: step 6360, loss = 1.18 (986.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:46:19.583635: step 6370, loss = 1.11 (982.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:46:20.887992: step 6380, loss = 1.01 (981.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:46:22.206771: step 6390, loss = 0.95 (970.6 examples/sec; 0.132 sec/batch)
2017-05-08 15:46:23.636609: step 6400, loss = 0.94 (895.2 examples/sec; 0.143 sec/batch)
2017-05-08 15:46:24.807795: step 6410, loss = 1.15 (1092.9 examples/sec; 0.117 sec/batch)
2017-05-08 15:46:26.077053: step 6420, loss = 0.98 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-08 15:46:27.363364: step 6430, loss = 0.89 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:46:28.655342: step 6440, loss = 0.96 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:46:29.969304: step 6450, loss = 0.90 (974.2 examples/sec; 0.131 sec/batch)
2017-05-08 15:46:31.289024: step 6460, loss = 1.00 (969.9 examples/sec; 0.132 sec/batch)
2017-05-08 15:46:32.618986: step 6470, loss = 0.84 (962.4 examples/sec; 0.133 sec/batch)
2017-05-08 15:46:33.975566: step 6480, loss = 1.03 (943.6 examples/sec; 0.136 sec/batch)
2017-05-08 15:46:35.286221: step 6490, loss = 1.10 (976.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:46:36.733397: step 6500, loss = 0.90 (884.5 examples/sec; 0.145 sec/batch)
2017-05-08 15:46:37.934403: step 6510, loss = 0.78 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-08 15:46:39.232614: step 6520, loss = 0.93 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:46:40.543772: step 6530, loss = 0.91 (976.2 examples/sec; 0.131 sec/batch)
2017-05-08 15:46:41.865932: step 6540, loss = 1.02 (968.1 examples/sec; 0.132 sec/batch)
2017-05-08 15:46:43.153304: step 6550, loss = 1.02 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:46:44.457779: step 6560, loss = 0.93 (981.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:46:45.759650: step 6570, loss = 1.15 (983.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:46:47.064115: step 6580, loss = 1.01 (981.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:46:48.343984: step 6590, loss = 1.41 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:46:49.747141: step 6600, loss = 1.01 (912.2 examples/sec; 0.140 sec/batch)
2017-05-08 15:46:50.944872: step 6610, loss = 0.80 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-08 15:46:52.220108: step 6620, loss = 1.09 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:46:53.502976: step 6630, loss = 0.95 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:46:54.793196: step 6640, loss = 1.09 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:46:56.065167: step 6650, loss = 1.11 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-08 15:46:57.354892: step 6660, loss = 0.97 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:46:58.676074: step 6670, loss = 1.07 (968.8 examples/sec; 0.132 sec/batch)
2017-05-08 15:46:59.982536: step 6680, loss = 0.82 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:47:01.334867: step 6690, loss = 1.00 (946.5 examples/sec; 0.135 sec/batch)
2017-05-08 15:47:02.703270: step 6700, loss = 1.00 (935.4 examples/sec; 0.137 sec/batch)
2017-05-08 15:47:03.878111: step 6710, loss = 0.83 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-08 15:47:05.159051: step 6720, loss = 0.98 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:47:06.471961: step 6730, loss = 0.95 (974.9 examples/sec; 0.131 sec/batch)
2017-05-08 15:47:07.778320: step 6740, loss = 0.94 (979.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:47:09.068513: step 6750, loss = 0.91 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:47:10.367257: step 6760, loss = 1.06 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:47:11.688148: step 6770, loss = 0.93 (969.0 examples/sec; 0.132 sec/batch)
2017-05-08 15:47:13.013440: step 6780, loss = 1.00 (965.8 examples/sec; 0.133 sec/batch)
2017-05-08 15:47:14.309383: step 6790, loss = 1.11 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:47:15.731812: step 6800, loss = 1.34 (899.9 examples/sec; 0.142 sec/batch)
2017-05-08 15:47:16.937272: step 6810, loss = 1.09 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-08 15:47:18.232294: step 6820, loss = 0.81 (988.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:47:19.505396: step 6830, loss = 0.74 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 15:47:20.814437: step 6840, loss = 0.99 (977.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:47:22.123621: step 6850, loss = 1.13 (977.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:47:23.426024: step 6860, loss = 0.99 (982.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:47:24.732976: step 6870, loss = 1.00 (979.4 examples/sec; 0.131 sec/batch)
2017-05-08 15:47:26.050291:E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 144 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
 step 6880, loss = 0.90 (971.7 examples/sec; 0.132 sec/batch)
2017-05-08 15:47:27.338112: step 6890, loss = 1.00 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:47:28.727486: step 6900, loss = 0.90 (921.3 examples/sec; 0.139 sec/batch)
2017-05-08 15:47:29.939383: step 6910, loss = 0.97 (1056.2 examples/sec; 0.121 sec/batch)
2017-05-08 15:47:31.235787: step 6920, loss = 0.95 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:47:32.538711: step 6930, loss = 1.06 (982.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:47:33.869861: step 6940, loss = 1.23 (961.6 examples/sec; 0.133 sec/batch)
2017-05-08 15:47:35.195395: step 6950, loss = 0.90 (965.7 examples/sec; 0.133 sec/batch)
2017-05-08 15:47:36.523381: step 6960, loss = 1.11 (963.9 examples/sec; 0.133 sec/batch)
2017-05-08 15:47:37.819291: step 6970, loss = 1.12 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:47:39.155899: step 6980, loss = 0.91 (957.7 examples/sec; 0.134 sec/batch)
2017-05-08 15:47:40.456016: step 6990, loss = 1.13 (984.5 examples/sec; 0.130 sec/batch)
2017-05-08 15:47:41.873935: step 7000, loss = 0.85 (902.7 examples/sec; 0.142 sec/batch)
2017-05-08 15:47:43.087721: step 7010, loss = 1.04 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-08 15:47:44.372327: step 7020, loss = 0.75 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:47:45.707054: step 7030, loss = 0.97 (959.0 examples/sec; 0.133 sec/batch)
2017-05-08 15:47:47.016051: step 7040, loss = 0.98 (977.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:47:48.323878: step 7050, loss = 0.92 (978.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:47:49.644408: step 7060, loss = 1.06 (969.3 examples/sec; 0.132 sec/batch)
2017-05-08 15:47:50.960674: step 7070, loss = 0.87 (972.4 examples/sec; 0.132 sec/batch)
2017-05-08 15:47:52.284048: step 7080, loss = 1.10 (967.2 examples/sec; 0.132 sec/batch)
2017-05-08 15:47:53.604427: step 7090, loss = 0.84 (969.4 examples/sec; 0.132 sec/batch)
2017-05-08 15:47:55.043167: step 7100, loss = 0.89 (889.7 examples/sec; 0.144 sec/batch)
2017-05-08 15:47:56.283829: step 7110, loss = 1.04 (1031.7 examples/sec; 0.124 sec/batch)
2017-05-08 15:47:57.619939: step 7120, loss = 0.89 (958.0 examples/sec; 0.134 sec/batch)
2017-05-08 15:47:58.959564: step 7130, loss = 1.22 (955.5 examples/sec; 0.134 sec/batch)
2017-05-08 15:48:00.285138: step 7140, loss = 0.90 (965.6 examples/sec; 0.133 sec/batch)
2017-05-08 15:48:01.563845: step 7150, loss = 1.01 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 15:48:02.848906: step 7160, loss = 0.85 (996.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:48:04.136927: step 7170, loss = 1.04 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:48:05.430114: step 7180, loss = 1.02 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:48:06.735567: step 7190, loss = 0.81 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:48:08.116235: step 7200, loss = 0.94 (927.1 examples/sec; 0.138 sec/batch)
2017-05-08 15:48:09.333548: step 7210, loss = 0.78 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-08 15:48:10.648226: step 7220, loss = 0.91 (973.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:48:11.944035: step 7230, loss = 0.80 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:48:13.248612: step 7240, loss = 1.02 (981.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:48:14.559638: step 7250, loss = 0.93 (976.3 examples/sec; 0.131 sec/batch)
2017-05-08 15:48:15.861750: step 7260, loss = 0.83 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:48:17.189772: step 7270, loss = 0.85 (963.8 examples/sec; 0.133 sec/batch)
2017-05-08 15:48:18.499322: step 7280, loss = 1.25 (977.4 examples/sec; 0.131 sec/batch)
2017-05-08 15:48:19.785914: step 7290, loss = 0.91 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:48:21.165665: step 7300, loss = 0.79 (927.7 examples/sec; 0.138 sec/batch)
2017-05-08 15:48:22.368168: step 7310, loss = 0.96 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-08 15:48:23.651532: step 7320, loss = 1.03 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:48:24.952423: step 7330, loss = 1.02 (983.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:48:26.258140: step 7340, loss = 0.94 (980.3 examples/sec; 0.131 sec/batch)
2017-05-08 15:48:27.566808: step 7350, loss = 1.10 (978.1 examples/sec; 0.131 sec/batch)
2017-05-08 15:48:28.886275: step 7360, loss = 0.76 (970.1 examples/sec; 0.132 sec/batch)
2017-05-08 15:48:30.196760: step 7370, loss = 0.84 (976.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:48:31.496622: step 7380, loss = 0.89 (984.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:48:32.818391: step 7390, loss = 0.89 (968.4 examples/sec; 0.132 sec/batch)
2017-05-08 15:48:34.228694: step 7400, loss = 0.96 (907.6 examples/sec; 0.141 sec/batch)
2017-05-08 15:48:35.456697: step 7410, loss = 0.89 (1042.3 examples/sec; 0.123 sec/batch)
2017-05-08 15:48:36.759719: step 7420, loss = 0.81 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:48:38.096208: step 7430, loss = 0.99 (957.7 examples/sec; 0.134 sec/batch)
2017-05-08 15:48:39.380861: step 7440, loss = 1.03 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:48:40.680182: step 7450, loss = 1.07 (985.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:48:41.980548: step 7460, loss = 1.06 (984.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:48:43.305389: step 7470, loss = 0.97 (966.2 examples/sec; 0.132 sec/batch)
2017-05-08 15:48:44.615573: step 7480, loss = 0.84 (977.0 examples/sec; 0.131 sec/batch)
2017-05-08 15:48:45.916988: step 7490, loss = 0.86 (983.5 examples/sec; 0.130 sec/batch)
2017-05-08 15:48:47.324841: step 7500, loss = 0.97 (909.2 examples/sec; 0.141 sec/batch)
2017-05-08 15:48:48.514670: step 7510, loss = 0.98 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-08 15:48:49.795427: step 7520, loss = 1.07 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:48:51.084536: step 7530, loss = 0.93 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:48:52.386673: step 7540, loss = 1.30 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:48:53.646489: step 7550, loss = 1.12 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-08 15:48:54.924592: step 7560, loss = 1.10 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:48:56.210689: step 7570, loss = 0.95 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:48:57.502591: step 7580, loss = 1.25 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:48:58.792745: step 7590, loss = 0.92 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:49:00.152229: step 7600, loss = 0.87 (941.5 examples/sec; 0.136 sec/batch)
2017-05-08 15:49:01.341009: step 7610, loss = 1.05 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-08 15:49:02.637037: step 7620, loss = 1.01 (987.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:49:03.920960: step 7630, loss = 0.94 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 15:49:05.217570: step 7640, loss = 0.87 (987.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:49:06.520116: step 7650, loss = 0.90 (982.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:49:07.818939: step 7660, loss = 1.01 (985.5 examples/sec; 0.130 sec/batch)
2017-05-08 15:49:09.121098: step 7670, loss = 1.51 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:49:10.411658: step 7680, loss = 0.96 (991.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:49:11.695829: step 7690, loss = 1.01 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:49:13.089244: step 7700, loss = 0.82 (918.6 examples/sec; 0.139 sec/batch)
2017-05-08 15:49:14.301207: step 7710, loss = 1.23 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-08 15:49:15.616036: step 7720, loss = 1.10 (973.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:49:16.929203: step 7730, loss = 0.89 (974.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:49:18.214399: step 7740, loss = 0.88 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:49:19.529743: step 7750, loss = 0.92 (973.1 examples/sec; 0.132 sec/batch)
2017-05-08 15:49:20.839246: step 7760, loss = 0.88 (977.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:49:22.126316: step 7770, loss = 1.22 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:49:23.459367: step 7780, loss = 1.03 (960.2 examples/sec; 0.133 sec/batch)
2017-05-08 15:49:24.732233: step 7790, loss = 1.00 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 15:49:2E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 164 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
6.145413: step 7800, loss = 1.05 (905.8 examples/sec; 0.141 sec/batch)
2017-05-08 15:49:27.370671: step 7810, loss = 0.91 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-08 15:49:28.677438: step 7820, loss = 1.10 (979.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:49:29.995929: step 7830, loss = 0.95 (970.8 examples/sec; 0.132 sec/batch)
2017-05-08 15:49:31.303653: step 7840, loss = 0.89 (978.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:49:32.580659: step 7850, loss = 0.90 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:49:33.843149: step 7860, loss = 0.91 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-08 15:49:35.146109: step 7870, loss = 1.02 (982.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:49:36.475431: step 7880, loss = 0.88 (962.9 examples/sec; 0.133 sec/batch)
2017-05-08 15:49:37.772986: step 7890, loss = 0.93 (986.5 examples/sec; 0.130 sec/batch)
2017-05-08 15:49:39.152489: step 7900, loss = 0.86 (927.9 examples/sec; 0.138 sec/batch)
2017-05-08 15:49:40.349594: step 7910, loss = 1.09 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-08 15:49:41.655084: step 7920, loss = 1.12 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:49:42.956629: step 7930, loss = 1.23 (983.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:49:44.272227: step 7940, loss = 0.78 (972.9 examples/sec; 0.132 sec/batch)
2017-05-08 15:49:45.587081: step 7950, loss = 0.93 (973.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:49:46.886329: step 7960, loss = 0.87 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:49:48.180958: step 7970, loss = 0.91 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:49:49.481827: step 7980, loss = 0.86 (984.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:49:50.788645: step 7990, loss = 1.01 (979.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:49:52.167075: step 8000, loss = 0.86 (928.6 examples/sec; 0.138 sec/batch)
2017-05-08 15:49:53.382989: step 8010, loss = 1.01 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-08 15:49:54.673858: step 8020, loss = 1.14 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:49:55.957289: step 8030, loss = 1.09 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:49:57.251283: step 8040, loss = 0.91 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:49:58.553543: step 8050, loss = 0.86 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:49:59.860246: step 8060, loss = 0.98 (979.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:50:01.147508: step 8070, loss = 0.91 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:50:02.457663: step 8080, loss = 1.13 (977.0 examples/sec; 0.131 sec/batch)
2017-05-08 15:50:03.761993: step 8090, loss = 1.29 (981.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:50:05.156254: step 8100, loss = 1.16 (918.0 examples/sec; 0.139 sec/batch)
2017-05-08 15:50:06.345081: step 8110, loss = 1.25 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-08 15:50:07.661363: step 8120, loss = 0.90 (972.4 examples/sec; 0.132 sec/batch)
2017-05-08 15:50:08.948291: step 8130, loss = 1.02 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:50:10.243044: step 8140, loss = 1.07 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:50:11.538046: step 8150, loss = 0.91 (988.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:50:12.820255: step 8160, loss = 1.01 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:50:14.152669: step 8170, loss = 1.05 (960.7 examples/sec; 0.133 sec/batch)
2017-05-08 15:50:15.445246: step 8180, loss = 1.17 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:50:16.751104: step 8190, loss = 1.09 (980.2 examples/sec; 0.131 sec/batch)
2017-05-08 15:50:18.168353: step 8200, loss = 1.06 (903.2 examples/sec; 0.142 sec/batch)
2017-05-08 15:50:19.319316: step 8210, loss = 0.94 (1112.1 examples/sec; 0.115 sec/batch)
2017-05-08 15:50:20.615837: step 8220, loss = 0.92 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:50:21.931191: step 8230, loss = 1.12 (973.1 examples/sec; 0.132 sec/batch)
2017-05-08 15:50:23.252881: step 8240, loss = 0.80 (968.5 examples/sec; 0.132 sec/batch)
2017-05-08 15:50:24.547513: step 8250, loss = 1.21 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:50:25.863568: step 8260, loss = 0.92 (972.6 examples/sec; 0.132 sec/batch)
2017-05-08 15:50:27.164821: step 8270, loss = 0.88 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:50:28.450968: step 8280, loss = 0.93 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:50:29.750237: step 8290, loss = 0.98 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:50:31.135887: step 8300, loss = 1.03 (923.7 examples/sec; 0.139 sec/batch)
2017-05-08 15:50:32.325083: step 8310, loss = 1.00 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-08 15:50:33.601139: step 8320, loss = 0.93 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:50:34.910799: step 8330, loss = 1.14 (977.3 examples/sec; 0.131 sec/batch)
2017-05-08 15:50:36.191091: step 8340, loss = 0.82 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:50:37.477060: step 8350, loss = 1.07 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:50:38.771404: step 8360, loss = 1.07 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:50:40.058567: step 8370, loss = 1.05 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:50:41.354018: step 8380, loss = 0.73 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:50:42.641566: step 8390, loss = 0.85 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:50:44.041504: step 8400, loss = 0.88 (914.4 examples/sec; 0.140 sec/batch)
2017-05-08 15:50:45.245005: step 8410, loss = 1.07 (1063.5 examples/sec; 0.120 sec/batch)
2017-05-08 15:50:46.534305: step 8420, loss = 0.83 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:50:47.849223: step 8430, loss = 1.08 (973.4 examples/sec; 0.131 sec/batch)
2017-05-08 15:50:49.157535: step 8440, loss = 1.03 (978.4 examples/sec; 0.131 sec/batch)
2017-05-08 15:50:50.454642: step 8450, loss = 1.35 (986.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:50:51.787987: step 8460, loss = 1.01 (960.0 examples/sec; 0.133 sec/batch)
2017-05-08 15:50:53.088869: step 8470, loss = 0.98 (983.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:50:54.368881: step 8480, loss = 0.94 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 15:50:55.675231: step 8490, loss = 0.91 (979.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:50:57.070079: step 8500, loss = 1.06 (917.7 examples/sec; 0.139 sec/batch)
2017-05-08 15:50:58.303176: step 8510, loss = 1.10 (1038.0 examples/sec; 0.123 sec/batch)
2017-05-08 15:50:59.615776: step 8520, loss = 0.90 (975.2 examples/sec; 0.131 sec/batch)
2017-05-08 15:51:00.900658: step 8530, loss = 0.87 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:51:02.214399: step 8540, loss = 0.92 (974.3 examples/sec; 0.131 sec/batch)
2017-05-08 15:51:03.504153: step 8550, loss = 0.90 (992.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:51:04.787992: step 8560, loss = 0.90 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 15:51:06.079283: step 8570, loss = 0.96 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:51:07.361224: step 8580, loss = 1.26 (998.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:51:08.683079: step 8590, loss = 1.09 (968.3 examples/sec; 0.132 sec/batch)
2017-05-08 15:51:10.071793: step 8600, loss = 0.90 (921.7 examples/sec; 0.139 sec/batch)
2017-05-08 15:51:11.254065: step 8610, loss = 0.86 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-08 15:51:12.561961: step 8620, loss = 1.06 (978.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:51:13.840022: step 8630, loss = 1.08 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:51:15.127832: step 8640, loss = 0.90 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:51:16.422716: step 8650, loss = 1.02 (988.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:51:17.724990: step 8660, loss = 0.97 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:51:19.003560: step 8670, loss = 1.01 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:51:20.285330: step 8680, loss = 0.88 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:51:21.566028: step 8690, loss = 1.12 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:51:22.962932: step 8700, loss = 1.00 (916.3 examples/sec; 0.140 sec/batch)
2017-05-08 15:51:24.137737: step 8710, loss = 1.21 (1089.5 examples/sec; 0.117 sec/batch)
2017-0E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 184 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
5-08 15:51:25.403501: step 8720, loss = 0.87 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-08 15:51:26.707861: step 8730, loss = 1.03 (981.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:51:27.988707: step 8740, loss = 1.15 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:51:29.276996: step 8750, loss = 0.89 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:51:30.578251: step 8760, loss = 0.98 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:51:31.871516: step 8770, loss = 1.05 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:51:33.184669: step 8780, loss = 1.15 (974.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:51:34.475330: step 8790, loss = 0.89 (991.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:51:35.860317: step 8800, loss = 0.95 (924.2 examples/sec; 0.138 sec/batch)
2017-05-08 15:51:37.071081: step 8810, loss = 0.67 (1057.2 examples/sec; 0.121 sec/batch)
2017-05-08 15:51:38.378032: step 8820, loss = 0.95 (979.4 examples/sec; 0.131 sec/batch)
2017-05-08 15:51:39.676959: step 8830, loss = 1.02 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:51:40.977505: step 8840, loss = 0.96 (984.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:51:42.258547: step 8850, loss = 0.97 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:51:43.570156: step 8860, loss = 1.11 (975.9 examples/sec; 0.131 sec/batch)
2017-05-08 15:51:44.904124: step 8870, loss = 1.20 (959.5 examples/sec; 0.133 sec/batch)
2017-05-08 15:51:46.200078: step 8880, loss = 0.98 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:51:47.510186: step 8890, loss = 0.93 (977.0 examples/sec; 0.131 sec/batch)
2017-05-08 15:51:48.887840: step 8900, loss = 0.92 (929.1 examples/sec; 0.138 sec/batch)
2017-05-08 15:51:50.092618: step 8910, loss = 1.04 (1062.4 examples/sec; 0.120 sec/batch)
2017-05-08 15:51:51.387882: step 8920, loss = 1.02 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:51:52.679951: step 8930, loss = 0.83 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:51:53.990773: step 8940, loss = 0.98 (976.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:51:55.291908: step 8950, loss = 0.94 (983.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:51:56.620131: step 8960, loss = 1.10 (963.7 examples/sec; 0.133 sec/batch)
2017-05-08 15:51:57.933766: step 8970, loss = 0.91 (974.4 examples/sec; 0.131 sec/batch)
2017-05-08 15:51:59.249820: step 8980, loss = 1.16 (972.6 examples/sec; 0.132 sec/batch)
2017-05-08 15:52:00.564629: step 8990, loss = 0.87 (973.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:52:01.942334: step 9000, loss = 0.69 (929.1 examples/sec; 0.138 sec/batch)
2017-05-08 15:52:03.157285: step 9010, loss = 1.03 (1053.5 examples/sec; 0.121 sec/batch)
2017-05-08 15:52:04.448986: step 9020, loss = 0.74 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:52:05.777009: step 9030, loss = 0.87 (963.8 examples/sec; 0.133 sec/batch)
2017-05-08 15:52:07.109551: step 9040, loss = 1.02 (960.6 examples/sec; 0.133 sec/batch)
2017-05-08 15:52:08.399244: step 9050, loss = 0.94 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:52:09.699905: step 9060, loss = 1.00 (984.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:52:11.018071: step 9070, loss = 0.97 (971.1 examples/sec; 0.132 sec/batch)
2017-05-08 15:52:12.289247: step 9080, loss = 1.03 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 15:52:13.612522: step 9090, loss = 0.95 (967.3 examples/sec; 0.132 sec/batch)
2017-05-08 15:52:15.006930: step 9100, loss = 0.97 (918.0 examples/sec; 0.139 sec/batch)
2017-05-08 15:52:16.236917: step 9110, loss = 0.96 (1040.7 examples/sec; 0.123 sec/batch)
2017-05-08 15:52:17.509331: step 9120, loss = 0.84 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-08 15:52:18.805769: step 9130, loss = 1.00 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:52:20.129977: step 9140, loss = 0.72 (966.6 examples/sec; 0.132 sec/batch)
2017-05-08 15:52:21.430005: step 9150, loss = 1.03 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:52:22.703755: step 9160, loss = 0.96 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-08 15:52:23.998054: step 9170, loss = 0.91 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:52:25.273280: step 9180, loss = 1.17 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:52:26.565512: step 9190, loss = 0.97 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:52:27.920398: step 9200, loss = 0.69 (944.7 examples/sec; 0.135 sec/batch)
2017-05-08 15:52:29.147086: step 9210, loss = 0.97 (1043.5 examples/sec; 0.123 sec/batch)
2017-05-08 15:52:30.450611: step 9220, loss = 0.91 (981.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:52:31.740642: step 9230, loss = 0.85 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:52:33.048826: step 9240, loss = 0.84 (978.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:52:34.386488: step 9250, loss = 1.01 (956.9 examples/sec; 0.134 sec/batch)
2017-05-08 15:52:35.667870: step 9260, loss = 1.27 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 15:52:36.942077: step 9270, loss = 1.02 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 15:52:38.229525: step 9280, loss = 0.74 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:52:39.528800: step 9290, loss = 0.73 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:52:40.940761: step 9300, loss = 0.82 (906.5 examples/sec; 0.141 sec/batch)
2017-05-08 15:52:42.159708: step 9310, loss = 0.98 (1050.1 examples/sec; 0.122 sec/batch)
2017-05-08 15:52:43.466012: step 9320, loss = 1.05 (979.9 examples/sec; 0.131 sec/batch)
2017-05-08 15:52:44.771407: step 9330, loss = 1.09 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:52:46.058241: step 9340, loss = 0.88 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:52:47.344976: step 9350, loss = 0.89 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:52:48.625425: step 9360, loss = 0.94 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:52:49.950193: step 9370, loss = 0.87 (966.2 examples/sec; 0.132 sec/batch)
2017-05-08 15:52:51.260145: step 9380, loss = 0.90 (977.1 examples/sec; 0.131 sec/batch)
2017-05-08 15:52:52.587890: step 9390, loss = 0.95 (964.6 examples/sec; 0.133 sec/batch)
2017-05-08 15:52:53.943678: step 9400, loss = 0.98 (943.6 examples/sec; 0.136 sec/batch)
2017-05-08 15:52:55.151911: step 9410, loss = 1.15 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-08 15:52:56.442141: step 9420, loss = 0.90 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:52:57.726977: step 9430, loss = 0.94 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:52:59.007258: step 9440, loss = 0.92 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:53:00.293273: step 9450, loss = 1.10 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:53:01.604269: step 9460, loss = 0.95 (976.4 examples/sec; 0.131 sec/batch)
2017-05-08 15:53:02.895438: step 9470, loss = 0.91 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:53:04.199505: step 9480, loss = 0.99 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 15:53:05.484903: step 9490, loss = 0.83 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:53:06.879529: step 9500, loss = 0.81 (917.8 examples/sec; 0.139 sec/batch)
2017-05-08 15:53:08.060512: step 9510, loss = 0.79 (1083.8 examples/sec; 0.118 sec/batch)
2017-05-08 15:53:09.346698: step 9520, loss = 1.13 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:53:10.635665: step 9530, loss = 0.95 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:53:11.907903: step 9540, loss = 0.94 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-08 15:53:13.188240: step 9550, loss = 0.87 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:53:14.481831: step 9560, loss = 1.00 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:53:15.764308: step 9570, loss = 0.86 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:53:17.039649: step 9580, loss = 0.85 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:53:18.338050: step 9590, loss = 0.95 (985.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:53:19.726143: step 9600, loss = 1.14 (922.1 examples/sec; 0.139 sec/batch)
2017-05-08 15:53:20.958529: step 9610, loss = 0.88 (1038.6 examples/sec; 0.123 sec/batch)
2017-05-08 15:53:22.234009: step 9620, loss = 0.94 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:53:23.518740: step 9630, loss = 1.13 (996.3 examples/sec; 0.128 secE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 205 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
/batch)
2017-05-08 15:53:24.813701: step 9640, loss = 0.88 (988.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:53:26.099255: step 9650, loss = 0.83 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:53:27.378692: step 9660, loss = 1.02 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:53:28.648401: step 9670, loss = 0.84 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-08 15:53:29.929620: step 9680, loss = 1.01 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 15:53:31.213952: step 9690, loss = 0.96 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:53:32.590237: step 9700, loss = 1.16 (930.0 examples/sec; 0.138 sec/batch)
2017-05-08 15:53:33.779144: step 9710, loss = 0.96 (1076.6 examples/sec; 0.119 sec/batch)
2017-05-08 15:53:35.068910: step 9720, loss = 0.97 (992.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:53:36.364763: step 9730, loss = 0.88 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:53:37.643028: step 9740, loss = 1.05 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:53:38.933866: step 9750, loss = 0.81 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:53:40.231140: step 9760, loss = 1.03 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:53:41.511840: step 9770, loss = 0.94 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:53:42.789685: step 9780, loss = 0.91 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:53:44.072030: step 9790, loss = 0.73 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:53:45.444209: step 9800, loss = 0.86 (932.8 examples/sec; 0.137 sec/batch)
2017-05-08 15:53:46.659534: step 9810, loss = 0.97 (1053.2 examples/sec; 0.122 sec/batch)
2017-05-08 15:53:47.959216: step 9820, loss = 0.92 (984.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:53:49.240282: step 9830, loss = 0.95 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:53:50.520636: step 9840, loss = 0.99 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:53:51.799571: step 9850, loss = 0.77 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:53:53.093049: step 9860, loss = 1.09 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:53:54.395032: step 9870, loss = 0.84 (983.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:53:55.689492: step 9880, loss = 0.79 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:53:57.013833: step 9890, loss = 0.82 (966.5 examples/sec; 0.132 sec/batch)
2017-05-08 15:53:58.395989: step 9900, loss = 0.89 (926.1 examples/sec; 0.138 sec/batch)
2017-05-08 15:53:59.602527: step 9910, loss = 1.13 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-08 15:54:00.922372: step 9920, loss = 0.98 (969.8 examples/sec; 0.132 sec/batch)
2017-05-08 15:54:02.221003: step 9930, loss = 0.97 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:54:03.547379: step 9940, loss = 0.95 (965.0 examples/sec; 0.133 sec/batch)
2017-05-08 15:54:04.865110: step 9950, loss = 0.82 (971.4 examples/sec; 0.132 sec/batch)
2017-05-08 15:54:06.157539: step 9960, loss = 0.81 (990.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:54:07.430774: step 9970, loss = 0.84 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 15:54:08.758360: step 9980, loss = 1.02 (964.2 examples/sec; 0.133 sec/batch)
2017-05-08 15:54:10.073359: step 9990, loss = 1.01 (973.4 examples/sec; 0.132 sec/batch)
2017-05-08 15:54:11.459140: step 10000, loss = 0.92 (923.7 examples/sec; 0.139 sec/batch)
2017-05-08 15:54:12.665508: step 10010, loss = 0.83 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-08 15:54:13.948785: step 10020, loss = 1.09 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:54:15.232762: step 10030, loss = 0.91 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 15:54:16.498806: step 10040, loss = 0.87 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 15:54:17.794390: step 10050, loss = 0.95 (988.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:54:19.089722: step 10060, loss = 1.00 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:54:20.402331: step 10070, loss = 1.03 (975.2 examples/sec; 0.131 sec/batch)
2017-05-08 15:54:21.703116: step 10080, loss = 0.98 (984.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:54:23.019425: step 10090, loss = 0.91 (972.4 examples/sec; 0.132 sec/batch)
2017-05-08 15:54:24.381563: step 10100, loss = 1.09 (939.7 examples/sec; 0.136 sec/batch)
2017-05-08 15:54:25.576787: step 10110, loss = 0.83 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-08 15:54:26.867210: step 10120, loss = 0.80 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:54:28.133772: step 10130, loss = 0.82 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 15:54:29.431968: step 10140, loss = 0.82 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:54:30.758554: step 10150, loss = 0.97 (964.9 examples/sec; 0.133 sec/batch)
2017-05-08 15:54:32.055968: step 10160, loss = 1.03 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:54:33.357252: step 10170, loss = 0.90 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:54:34.656089: step 10180, loss = 0.98 (985.5 examples/sec; 0.130 sec/batch)
2017-05-08 15:54:35.935600: step 10190, loss = 0.89 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:54:37.331343: step 10200, loss = 1.01 (917.1 examples/sec; 0.140 sec/batch)
2017-05-08 15:54:38.512594: step 10210, loss = 1.06 (1083.6 examples/sec; 0.118 sec/batch)
2017-05-08 15:54:39.799776: step 10220, loss = 0.86 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:54:41.078398: step 10230, loss = 0.97 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:54:42.358219: step 10240, loss = 1.00 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:54:43.638763: step 10250, loss = 1.03 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:54:44.928210: step 10260, loss = 0.76 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:54:46.212956: step 10270, loss = 0.91 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:54:47.488110: step 10280, loss = 0.86 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:54:48.782103: step 10290, loss = 0.88 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:54:50.187134: step 10300, loss = 0.94 (911.0 examples/sec; 0.141 sec/batch)
2017-05-08 15:54:51.383159: step 10310, loss = 0.71 (1070.2 examples/sec; 0.120 sec/batch)
2017-05-08 15:54:52.648584: step 10320, loss = 0.93 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-08 15:54:53.933770: step 10330, loss = 0.82 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:54:55.225486: step 10340, loss = 1.08 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:54:56.550931: step 10350, loss = 1.27 (965.7 examples/sec; 0.133 sec/batch)
2017-05-08 15:54:57.844665: step 10360, loss = 0.89 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 15:54:59.150938: step 10370, loss = 1.04 (979.9 examples/sec; 0.131 sec/batch)
2017-05-08 15:55:00.473856: step 10380, loss = 1.11 (967.6 examples/sec; 0.132 sec/batch)
2017-05-08 15:55:01.757400: step 10390, loss = 1.05 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:55:03.159602: step 10400, loss = 0.93 (912.8 examples/sec; 0.140 sec/batch)
2017-05-08 15:55:04.375559: step 10410, loss = 1.07 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-08 15:55:05.688061: step 10420, loss = 0.93 (975.2 examples/sec; 0.131 sec/batch)
2017-05-08 15:55:06.978628: step 10430, loss = 0.82 (991.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:55:08.274214: step 10440, loss = 0.86 (988.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:55:09.570346: step 10450, loss = 0.91 (987.5 examples/sec; 0.130 sec/batch)
2017-05-08 15:55:10.861944: step 10460, loss = 0.83 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:55:12.165103: step 10470, loss = 0.85 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:55:13.440345: step 10480, loss = 0.91 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:55:14.742261: step 10490, loss = 0.96 (983.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:55:16.130438: step 10500, loss = 0.84 (922.1 examples/sec; 0.139 sec/batch)
2017-05-08 15:55:17.314974: step 10510, loss = 0.77 (1080.6 examples/sec; 0.118 sec/batch)
2017-05-08 15:55:18.602073: step 10520, loss = 0.94 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:55:19.873239: step 10530, loss = 1.00 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 15:55:21.183300: step 10540, loss = 1.01 (977.1 examples/sec; 0.131 sec/batch)
2017-0E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 225 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
5-08 15:55:22.482791: step 10550, loss = 0.89 (985.0 examples/sec; 0.130 sec/batch)
2017-05-08 15:55:23.789244: step 10560, loss = 1.14 (979.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:55:25.082222: step 10570, loss = 0.94 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:55:26.367883: step 10580, loss = 0.98 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:55:27.657819: step 10590, loss = 0.92 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:55:29.042632: step 10600, loss = 0.83 (924.3 examples/sec; 0.138 sec/batch)
2017-05-08 15:55:30.234506: step 10610, loss = 0.88 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-08 15:55:31.500729: step 10620, loss = 1.00 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-08 15:55:32.779049: step 10630, loss = 0.89 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:55:34.063687: step 10640, loss = 0.90 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:55:35.354145: step 10650, loss = 0.74 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:55:36.651346: step 10660, loss = 0.93 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:55:37.949937: step 10670, loss = 0.89 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:55:39.243304: step 10680, loss = 1.03 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:55:40.534177: step 10690, loss = 0.97 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:55:41.914346: step 10700, loss = 0.77 (927.4 examples/sec; 0.138 sec/batch)
2017-05-08 15:55:43.115309: step 10710, loss = 1.14 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-08 15:55:44.397534: step 10720, loss = 0.91 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:55:45.667915: step 10730, loss = 1.07 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-08 15:55:46.976771: step 10740, loss = 0.85 (978.0 examples/sec; 0.131 sec/batch)
2017-05-08 15:55:48.271676: step 10750, loss = 0.84 (988.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:55:49.545984: step 10760, loss = 0.96 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 15:55:50.838983: step 10770, loss = 0.84 (989.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:55:52.136062: step 10780, loss = 0.89 (986.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:55:53.425270: step 10790, loss = 0.85 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:55:54.813790: step 10800, loss = 0.70 (921.8 examples/sec; 0.139 sec/batch)
2017-05-08 15:55:56.009613: step 10810, loss = 0.95 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-08 15:55:57.287821: step 10820, loss = 1.12 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:55:58.558962: step 10830, loss = 0.74 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 15:55:59.839937: step 10840, loss = 0.93 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:56:01.113414: step 10850, loss = 1.10 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 15:56:02.430136: step 10860, loss = 0.92 (972.1 examples/sec; 0.132 sec/batch)
2017-05-08 15:56:03.715765: step 10870, loss = 0.80 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:56:05.014956: step 10880, loss = 0.83 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:56:06.307592: step 10890, loss = 0.95 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:56:07.697903: step 10900, loss = 0.90 (920.7 examples/sec; 0.139 sec/batch)
2017-05-08 15:56:08.894040: step 10910, loss = 0.78 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-08 15:56:10.175463: step 10920, loss = 0.88 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 15:56:11.458100: step 10930, loss = 0.93 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 15:56:12.747550: step 10940, loss = 1.05 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:56:14.044020: step 10950, loss = 0.98 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:56:15.334078: step 10960, loss = 1.03 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:56:16.610362: step 10970, loss = 1.09 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 15:56:17.867577: step 10980, loss = 0.92 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-08 15:56:19.155749: step 10990, loss = 0.91 (993.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:56:20.541086: step 11000, loss = 0.82 (924.0 examples/sec; 0.139 sec/batch)
2017-05-08 15:56:21.759373: step 11010, loss = 0.98 (1050.7 examples/sec; 0.122 sec/batch)
2017-05-08 15:56:23.050108: step 11020, loss = 0.97 (991.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:56:24.354016: step 11030, loss = 0.90 (981.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:56:25.660633: step 11040, loss = 1.00 (979.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:56:26.981342: step 11050, loss = 0.90 (969.2 examples/sec; 0.132 sec/batch)
2017-05-08 15:56:28.258276: step 11060, loss = 0.93 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:56:29.553620: step 11070, loss = 0.93 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:56:30.837280: step 11080, loss = 0.96 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:56:32.119629: step 11090, loss = 1.04 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:56:33.483375: step 11100, loss = 0.88 (938.6 examples/sec; 0.136 sec/batch)
2017-05-08 15:56:34.666685: step 11110, loss = 0.91 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-08 15:56:35.942382: step 11120, loss = 0.95 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 15:56:37.223979: step 11130, loss = 0.84 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:56:38.508229: step 11140, loss = 0.90 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:56:39.814716: step 11150, loss = 0.91 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 15:56:41.112984: step 11160, loss = 0.82 (985.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:56:42.408032: step 11170, loss = 0.92 (988.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:56:43.691167: step 11180, loss = 1.10 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 15:56:45.024821: step 11190, loss = 1.17 (959.8 examples/sec; 0.133 sec/batch)
2017-05-08 15:56:46.402222: step 11200, loss = 0.96 (929.3 examples/sec; 0.138 sec/batch)
2017-05-08 15:56:47.577942: step 11210, loss = 0.89 (1088.7 examples/sec; 0.118 sec/batch)
2017-05-08 15:56:48.859323: step 11220, loss = 1.11 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 15:56:50.142153: step 11230, loss = 0.87 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:56:51.414185: step 11240, loss = 0.87 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-08 15:56:52.692501: step 11250, loss = 0.81 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:56:53.971057: step 11260, loss = 0.75 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:56:55.270091: step 11270, loss = 0.74 (985.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:56:56.553537: step 11280, loss = 0.90 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:56:57.831435: step 11290, loss = 0.84 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:56:59.213151: step 11300, loss = 0.86 (926.4 examples/sec; 0.138 sec/batch)
2017-05-08 15:57:00.390864: step 11310, loss = 1.01 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-08 15:57:01.658137: step 11320, loss = 0.90 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-08 15:57:02.916431: step 11330, loss = 1.14 (1017.2 examples/sec; 0.126 sec/batch)
2017-05-08 15:57:04.241879: step 11340, loss = 0.91 (965.7 examples/sec; 0.133 sec/batch)
2017-05-08 15:57:05.560499: step 11350, loss = 1.07 (970.7 examples/sec; 0.132 sec/batch)
2017-05-08 15:57:06.827099: step 11360, loss = 0.93 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 15:57:08.108041: step 11370, loss = 0.81 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:57:09.414846: step 11380, loss = 0.85 (979.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:57:10.719275: step 11390, loss = 1.11 (981.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:57:12.090670: step 11400, loss = 0.85 (933.4 examples/sec; 0.137 sec/batch)
2017-05-08 15:57:13.307066: step 11410, loss = 1.24 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-08 15:57:14.624362: step 11420, loss = 0.82 (971.7 examples/sec; 0.132 sec/batch)
2017-05-08 15:57:15.914930: step 11430, loss = 0.86 (991.8 examples/sec; 0.129 sec/batch)
2017-05-08 15:57:17.248498: step 11440, loss = 1.10 (959.9 examples/sec; 0.133 sec/batch)
2017-05-08 15:57:18.534260: step 11450, loss = 0.91 (995.5 examples/secE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 245 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
; 0.129 sec/batch)
2017-05-08 15:57:19.825102: step 11460, loss = 0.91 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:57:21.134972: step 11470, loss = 1.10 (977.2 examples/sec; 0.131 sec/batch)
2017-05-08 15:57:22.440966: step 11480, loss = 1.06 (980.1 examples/sec; 0.131 sec/batch)
2017-05-08 15:57:23.739970: step 11490, loss = 0.87 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:57:25.153175: step 11500, loss = 0.77 (905.7 examples/sec; 0.141 sec/batch)
2017-05-08 15:57:26.367127: step 11510, loss = 0.87 (1054.4 examples/sec; 0.121 sec/batch)
2017-05-08 15:57:27.665239: step 11520, loss = 1.13 (986.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:57:28.969095: step 11530, loss = 1.07 (981.7 examples/sec; 0.130 sec/batch)
2017-05-08 15:57:30.268246: step 11540, loss = 0.97 (985.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:57:31.549987: step 11550, loss = 0.93 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:57:32.816516: step 11560, loss = 0.92 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 15:57:34.093008: step 11570, loss = 0.68 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:57:35.418090: step 11580, loss = 0.89 (966.0 examples/sec; 0.133 sec/batch)
2017-05-08 15:57:36.697251: step 11590, loss = 0.91 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:57:38.087639: step 11600, loss = 0.81 (920.6 examples/sec; 0.139 sec/batch)
2017-05-08 15:57:39.331342: step 11610, loss = 1.02 (1029.2 examples/sec; 0.124 sec/batch)
2017-05-08 15:57:40.616553: step 11620, loss = 0.70 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:57:41.941211: step 11630, loss = 1.00 (966.3 examples/sec; 0.132 sec/batch)
2017-05-08 15:57:43.255528: step 11640, loss = 0.96 (973.9 examples/sec; 0.131 sec/batch)
2017-05-08 15:57:44.540640: step 11650, loss = 0.93 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:57:45.846117: step 11660, loss = 0.89 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:57:47.160763: step 11670, loss = 1.14 (973.6 examples/sec; 0.131 sec/batch)
2017-05-08 15:57:48.473061: step 11680, loss = 0.91 (975.4 examples/sec; 0.131 sec/batch)
2017-05-08 15:57:49.767752: step 11690, loss = 0.73 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:57:51.156228: step 11700, loss = 0.98 (921.9 examples/sec; 0.139 sec/batch)
2017-05-08 15:57:52.350182: step 11710, loss = 0.75 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-08 15:57:53.662531: step 11720, loss = 1.10 (975.3 examples/sec; 0.131 sec/batch)
2017-05-08 15:57:54.933558: step 11730, loss = 1.04 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 15:57:56.207399: step 11740, loss = 0.86 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 15:57:57.506511: step 11750, loss = 0.98 (985.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:57:58.800759: step 11760, loss = 0.80 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:58:00.077066: step 11770, loss = 0.85 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 15:58:01.348561: step 11780, loss = 0.85 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 15:58:02.675235: step 11790, loss = 0.93 (964.8 examples/sec; 0.133 sec/batch)
2017-05-08 15:58:04.045329: step 11800, loss = 1.10 (934.2 examples/sec; 0.137 sec/batch)
2017-05-08 15:58:05.277058: step 11810, loss = 0.99 (1039.2 examples/sec; 0.123 sec/batch)
2017-05-08 15:58:06.557257: step 11820, loss = 0.99 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:58:07.861489: step 11830, loss = 1.01 (981.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:58:09.110203: step 11840, loss = 1.09 (1025.1 examples/sec; 0.125 sec/batch)
2017-05-08 15:58:10.374251: step 11850, loss = 0.96 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-08 15:58:11.646892: step 11860, loss = 0.93 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-08 15:58:12.951224: step 11870, loss = 0.91 (981.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:58:14.239802: step 11880, loss = 0.76 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:58:15.521338: step 11890, loss = 1.07 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:58:16.923729: step 11900, loss = 1.07 (912.7 examples/sec; 0.140 sec/batch)
2017-05-08 15:58:18.097867: step 11910, loss = 0.93 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-08 15:58:19.402191: step 11920, loss = 1.26 (981.3 examples/sec; 0.130 sec/batch)
2017-05-08 15:58:20.688543: step 11930, loss = 1.17 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:58:21.968935: step 11940, loss = 0.90 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:58:23.224960: step 11950, loss = 0.89 (1019.1 examples/sec; 0.126 sec/batch)
2017-05-08 15:58:24.499311: step 11960, loss = 0.89 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-08 15:58:25.784594: step 11970, loss = 0.72 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 15:58:27.084927: step 11980, loss = 0.97 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 15:58:28.376436: step 11990, loss = 0.78 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:58:29.756586: step 12000, loss = 0.97 (927.4 examples/sec; 0.138 sec/batch)
2017-05-08 15:58:30.958923: step 12010, loss = 0.89 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-08 15:58:32.228512: step 12020, loss = 1.10 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-08 15:58:33.527189: step 12030, loss = 0.94 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:58:34.809649: step 12040, loss = 0.97 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:58:36.100526: step 12050, loss = 0.83 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:58:37.401431: step 12060, loss = 0.90 (983.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:58:38.702351: step 12070, loss = 0.90 (983.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:58:39.980050: step 12080, loss = 0.87 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:58:41.286830: step 12090, loss = 0.77 (979.5 examples/sec; 0.131 sec/batch)
2017-05-08 15:58:42.688190: step 12100, loss = 0.83 (913.4 examples/sec; 0.140 sec/batch)
2017-05-08 15:58:43.862123: step 12110, loss = 0.75 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-08 15:58:45.157526: step 12120, loss = 0.82 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:58:46.480200: step 12130, loss = 0.79 (967.7 examples/sec; 0.132 sec/batch)
2017-05-08 15:58:47.778940: step 12140, loss = 1.22 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:58:49.067948: step 12150, loss = 0.89 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 15:58:50.390057: step 12160, loss = 0.89 (968.2 examples/sec; 0.132 sec/batch)
2017-05-08 15:58:51.678822: step 12170, loss = 0.70 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 15:58:52.972687: step 12180, loss = 1.11 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:58:54.247757: step 12190, loss = 0.77 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-08 15:58:55.610764: step 12200, loss = 0.96 (939.1 examples/sec; 0.136 sec/batch)
2017-05-08 15:58:56.812439: step 12210, loss = 0.94 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-08 15:58:58.083178: step 12220, loss = 0.96 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-08 15:58:59.386892: step 12230, loss = 1.05 (981.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:59:00.658104: step 12240, loss = 0.82 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 15:59:01.941829: step 12250, loss = 0.74 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 15:59:03.224722: step 12260, loss = 1.03 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 15:59:04.497809: step 12270, loss = 0.89 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 15:59:05.779998: step 12280, loss = 0.86 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 15:59:07.083993: step 12290, loss = 1.06 (981.6 examples/sec; 0.130 sec/batch)
2017-05-08 15:59:08.473592: step 12300, loss = 0.86 (921.1 examples/sec; 0.139 sec/batch)
2017-05-08 15:59:09.673147: step 12310, loss = 1.17 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-08 15:59:10.945015: step 12320, loss = 0.87 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-08 15:59:12.258862: step 12330, loss = 0.93 (974.2 examples/sec; 0.131 sec/batch)
2017-05-08 15:59:13.527937: step 12340, loss = 0.93 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-08 15:59:14.832933: step 12350, loss = 0.75 (980.8 examples/sec; 0.130 sec/batch)
2017-05-08 15:59:16.142498: step 12360, loss = 0.90 (977.4 examples/sec; 0.131 sec/batch)
2017-05-08 15:59:17.445694: step 12370, loss = 0.91 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:59:18.742261: step 12380, loss = 0.92 (987.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:59:20.065394: step 12390, loss = 0.94 (967.4 examples/sec; 0.132 sec/batch)
2017-05-08 15:59:21.460807: step 12400, loss = 1.22 (917.3 examples/sec; 0.140 sec/batch)
2017-05-08 15:59:22.688131: step 12410, loss = 0.96 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-08 15:59:23.978897: step 12420, loss = 0.88 (991.7 examples/sec; 0.129 sec/batch)
2017-05-08 15:59:25.290645: step 12430, loss = 0.77 (975.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:59:26.632597: step 12440, loss = 0.94 (953.8 examples/sec; 0.134 sec/batch)
2017-05-08 15:59:27.922547: step 12450, loss = 0.80 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 15:59:29.249111: step 12460, loss = 1.09 (964.9 examples/sec; 0.133 sec/batch)
2017-05-08 15:59:30.549806: step 12470, loss = 1.12 (984.1 examples/sec; 0.130 sec/batch)
2017-05-08 15:59:31.868320: step 12480, loss = 0.89 (970.8 examples/sec; 0.132 sec/batch)
2017-05-08 15:59:33.150596: step 12490, loss = 0.88 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 15:59:34.533592: step 12500, loss = 0.98 (925.5 examples/sec; 0.138 sec/batch)
2017-05-08 15:59:35.717999: step 12510, loss = 0.76 (1080.7 examples/sec; 0.118 sec/batch)
2017-05-08 15:59:37.007566: step 12520, loss = 0.96 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 15:59:38.319366: step 12530, loss = 1.00 (975.8 examples/sec; 0.131 sec/batch)
2017-05-08 15:59:39.583943: step 12540, loss = 1.15 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 15:59:40.892695: step 12550, loss = 0.82 (978.0 examples/sec; 0.131 sec/batch)
2017-05-08 15:59:42.193649: step 12560, loss = 1.05 (983.9 examples/sec; 0.130 sec/batch)
2017-05-08 15:59:43.485093: step 12570, loss = 0.86 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 15:59:44.766661: step 12580, loss = 0.84 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 15:59:46.047106: step 12590, loss = 0.75 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 15:59:47.466261: step 12600, loss = 0.72 (901.9 examples/sec; 0.142 sec/batch)
2017-05-08 15:59:48.663331: step 12610, loss = 0.81 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-08 15:59:49.984025: step 12620, loss = 1.01 (969.2 examples/sec; 0.132 sec/batch)
2017-05-08 15:59:51.308812: step 12630, loss = 1.02 (966.2 examples/sec; 0.132 sec/batch)
2017-05-08 15:59:52.580977: step 12640, loss = 0.91 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-08 15:59:53.851327: step 12650, loss = 0.92 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-08 15:59:55.180398: step 12660, loss = 0.87 (963.1 examples/sec; 0.133 sec/batch)
2017-05-08 15:59:56.482301: step 12670, loss = 0.98 (983.2 examples/sec; 0.130 sec/batch)
2017-05-08 15:59:57.777211: step 12680, loss = 0.81 (988.5 examples/sec; 0.129 sec/batch)
2017-05-08 15:59:59.093375: step 12690, loss = 1.12 (972.5 examples/sec; 0.132 sec/batch)
2017-05-08 16:00:00.513179: step 12700, loss = 0.90 (901.5 examples/sec; 0.142 sec/batch)
2017-05-08 16:00:01.723404: step 12710, loss = 0.91 (1057.6 examples/sec; 0.121 sec/batch)
2017-05-08 16:00:03.014938: step 12720, loss = 0.94 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:00:04.292298: step 12730, loss = 0.95 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:00:05.585189: step 12740, loss = 0.81 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:00:06.858895: step 12750, loss = 0.96 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:00:08.130131: step 12760, loss = 0.87 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:00:09.431926: step 12770, loss = 0.92 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:00:10.730565: step 12780, loss = 0.88 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:00:12.021137: step 12790, loss = 1.20 (991.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:00:13.397224: step 12800, loss = 0.97 (930.2 examples/sec; 0.138 sec/batch)
2017-05-08 16:00:14.595913: step 12810, loss = 0.98 (1067.8 examples/sec; 0.120 E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 265 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
sec/batch)
2017-05-08 16:00:15.912767: step 12820, loss = 1.04 (972.0 examples/sec; 0.132 sec/batch)
2017-05-08 16:00:17.216623: step 12830, loss = 1.12 (981.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:00:18.528174: step 12840, loss = 0.90 (975.9 examples/sec; 0.131 sec/batch)
2017-05-08 16:00:19.846855: step 12850, loss = 0.93 (970.7 examples/sec; 0.132 sec/batch)
2017-05-08 16:00:21.125483: step 12860, loss = 1.01 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:00:22.404195: step 12870, loss = 1.05 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:00:23.702794: step 12880, loss = 0.89 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:00:24.983807: step 12890, loss = 0.99 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:00:26.366873: step 12900, loss = 0.86 (925.5 examples/sec; 0.138 sec/batch)
2017-05-08 16:00:27.552813: step 12910, loss = 0.83 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-08 16:00:28.825800: step 12920, loss = 0.81 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:00:30.117876: step 12930, loss = 0.93 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:00:31.396060: step 12940, loss = 0.99 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:00:32.692203: step 12950, loss = 1.01 (987.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:00:33.963734: step 12960, loss = 0.99 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:00:35.274477: step 12970, loss = 0.73 (976.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:00:36.576140: step 12980, loss = 0.97 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:00:37.855650: step 12990, loss = 0.81 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:00:39.228614: step 13000, loss = 0.81 (932.3 examples/sec; 0.137 sec/batch)
2017-05-08 16:00:40.404596: step 13010, loss = 0.86 (1088.5 examples/sec; 0.118 sec/batch)
2017-05-08 16:00:41.679321: step 13020, loss = 0.88 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:00:42.982395: step 13030, loss = 0.88 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:00:44.246986: step 13040, loss = 1.33 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 16:00:45.530971: step 13050, loss = 0.87 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:00:46.817907: step 13060, loss = 0.88 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:00:48.091185: step 13070, loss = 1.01 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:00:49.381754: step 13080, loss = 0.88 (991.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:00:50.683905: step 13090, loss = 0.78 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:00:52.059425: step 13100, loss = 0.92 (930.6 examples/sec; 0.138 sec/batch)
2017-05-08 16:00:53.244611: step 13110, loss = 1.15 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-08 16:00:54.555072: step 13120, loss = 0.96 (976.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:00:55.877421: step 13130, loss = 0.79 (968.0 examples/sec; 0.132 sec/batch)
2017-05-08 16:00:57.198382: step 13140, loss = 0.84 (969.0 examples/sec; 0.132 sec/batch)
2017-05-08 16:00:58.495873: step 13150, loss = 0.79 (986.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:00:59.758546: step 13160, loss = 0.88 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-08 16:01:01.032818: step 13170, loss = 0.88 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:01:02.337571: step 13180, loss = 0.93 (981.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:01:03.616732: step 13190, loss = 0.90 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:01:04.980070: step 13200, loss = 0.91 (938.9 examples/sec; 0.136 sec/batch)
2017-05-08 16:01:06.181648: step 13210, loss = 0.95 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-08 16:01:07.508090: step 13220, loss = 0.95 (965.0 examples/sec; 0.133 sec/batch)
2017-05-08 16:01:08.788113: step 13230, loss = 1.02 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:01:10.085872: step 13240, loss = 0.83 (986.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:01:11.384354: step 13250, loss = 0.88 (985.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:01:12.689753: step 13260, loss = 0.96 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:01:14.007076: step 13270, loss = 0.85 (971.7 examples/sec; 0.132 sec/batch)
2017-05-08 16:01:15.317934: step 13280, loss = 0.85 (976.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:01:16.597300: step 13290, loss = 0.81 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:01:18.010089: step 13300, loss = 1.01 (906.0 examples/sec; 0.141 sec/batch)
2017-05-08 16:01:19.222103: step 13310, loss = 1.06 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-08 16:01:20.512163: step 13320, loss = 0.90 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:01:21.804534: step 13330, loss = 0.80 (990.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:01:23.125197: step 13340, loss = 1.03 (969.2 examples/sec; 0.132 sec/batch)
2017-05-08 16:01:24.394385: step 13350, loss = 0.68 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:01:25.686375: step 13360, loss = 0.93 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:01:26.972232: step 13370, loss = 0.96 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:01:28.248657: step 13380, loss = 0.98 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:01:29.536050: step 13390, loss = 0.92 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:01:30.917926: step 13400, loss = 0.79 (926.3 examples/sec; 0.138 sec/batch)
2017-05-08 16:01:32.110092: step 13410, loss = 0.92 (1073.7 examples/sec; 0.119 sec/batch)
2017-05-08 16:01:33.408385: step 13420, loss = 0.79 (985.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:01:34.701556: step 13430, loss = 0.85 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:01:35.971115: step 13440, loss = 0.77 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:01:37.249989: step 13450, loss = 0.83 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:01:38.553876: step 13460, loss = 1.02 (981.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:01:39.859073: step 13470, loss = 0.94 (980.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:01:41.176569: step 13480, loss = 0.90 (971.5 examples/sec; 0.132 sec/batch)
2017-05-08 16:01:42.451073: step 13490, loss = 0.85 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:01:43.831986: step 13500, loss = 1.18 (926.9 examples/sec; 0.138 sec/batch)
2017-05-08 16:01:45.034134: step 13510, loss = 0.89 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-08 16:01:46.335096: step 13520, loss = 0.91 (983.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:01:47.618930: step 13530, loss = 0.78 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:01:48.932268: step 13540, loss = 0.98 (974.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:01:50.227932: step 13550, loss = 0.92 (987.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:01:51.510161: step 13560, loss = 0.89 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:01:52.794315: step 13570, loss = 0.89 (996.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:01:54.084695: step 13580, loss = 0.91 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:01:55.351326: step 13590, loss = 0.99 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:01:56.739279: step 13600, loss = 0.75 (922.2 examples/sec; 0.139 sec/batch)
2017-05-08 16:01:57.940937: step 13610, loss = 0.96 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-08 16:01:59.236813: step 13620, loss = 1.13 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:02:00.523002: step 13630, loss = 0.86 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:02:01.806707: step 13640, loss = 1.05 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:02:03.104655: step 13650, loss = 0.86 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:02:04.387368: step 13660, loss = 0.94 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:02:05.667830: step 13670, loss = 0.92 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:02:06.961777: step 13680, loss = 0.93 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:02:08.275796: step 13690, loss = 0.78 (974.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:02:09.663075: step 13700, loss = 1.06 (922.7 examples/sec; 0.139 sec/batch)
2017-05-08 16:02:10.857129: step 13710, loss = 0.89 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-08 16:02:12.128007: step 13720, loss = 0.80 E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 285 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
(1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:02:13.435267: step 13730, loss = 1.05 (979.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:02:14.717076: step 13740, loss = 1.03 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:02:16.001669: step 13750, loss = 0.91 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:02:17.288832: step 13760, loss = 0.92 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:02:18.605699: step 13770, loss = 0.79 (972.0 examples/sec; 0.132 sec/batch)
2017-05-08 16:02:19.891822: step 13780, loss = 0.89 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:02:21.193922: step 13790, loss = 1.05 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:02:22.594308: step 13800, loss = 0.73 (914.0 examples/sec; 0.140 sec/batch)
2017-05-08 16:02:23.781607: step 13810, loss = 0.94 (1078.1 examples/sec; 0.119 sec/batch)
2017-05-08 16:02:25.067890: step 13820, loss = 1.00 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:02:26.393081: step 13830, loss = 0.98 (965.9 examples/sec; 0.133 sec/batch)
2017-05-08 16:02:27.647703: step 13840, loss = 0.98 (1020.2 examples/sec; 0.125 sec/batch)
2017-05-08 16:02:28.970703: step 13850, loss = 0.83 (967.5 examples/sec; 0.132 sec/batch)
2017-05-08 16:02:30.265943: step 13860, loss = 0.95 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:02:31.540784: step 13870, loss = 0.79 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:02:32.808967: step 13880, loss = 0.89 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:02:34.123650: step 13890, loss = 0.79 (973.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:02:35.495705: step 13900, loss = 0.89 (932.9 examples/sec; 0.137 sec/batch)
2017-05-08 16:02:36.670326: step 13910, loss = 0.84 (1089.7 examples/sec; 0.117 sec/batch)
2017-05-08 16:02:37.964740: step 13920, loss = 0.99 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:02:39.250790: step 13930, loss = 1.07 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:02:40.547297: step 13940, loss = 0.87 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:02:41.838722: step 13950, loss = 0.92 (991.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:02:43.141588: step 13960, loss = 0.78 (982.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:02:44.415569: step 13970, loss = 0.88 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:02:45.706914: step 13980, loss = 0.90 (991.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:02:47.004354: step 13990, loss = 0.80 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:02:48.382162: step 14000, loss = 1.16 (929.0 examples/sec; 0.138 sec/batch)
2017-05-08 16:02:49.571538: step 14010, loss = 0.90 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-08 16:02:50.893827: step 14020, loss = 0.81 (968.0 examples/sec; 0.132 sec/batch)
2017-05-08 16:02:52.200895: step 14030, loss = 0.78 (979.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:02:53.515614: step 14040, loss = 0.93 (973.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:02:54.812096: step 14050, loss = 0.82 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:02:56.108691: step 14060, loss = 0.95 (987.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:02:57.418763: step 14070, loss = 0.92 (977.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:02:58.723517: step 14080, loss = 0.98 (981.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:03:00.050737: step 14090, loss = 0.84 (964.4 examples/sec; 0.133 sec/batch)
2017-05-08 16:03:01.442411: step 14100, loss = 0.83 (919.8 examples/sec; 0.139 sec/batch)
2017-05-08 16:03:02.628739: step 14110, loss = 0.83 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-08 16:03:03.912495: step 14120, loss = 0.77 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:03:05.191577: step 14130, loss = 0.87 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:03:06.465761: step 14140, loss = 0.93 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:03:07.745879: step 14150, loss = 1.28 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:03:09.022754: step 14160, loss = 0.83 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:03:10.309594: step 14170, loss = 1.22 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:03:11.586148: step 14180, loss = 0.69 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:03:12.896032: step 14190, loss = 0.80 (977.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:03:14.276132: step 14200, loss = 0.89 (927.5 examples/sec; 0.138 sec/batch)
2017-05-08 16:03:15.468534: step 14210, loss = 0.92 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-08 16:03:16.756274: step 14220, loss = 0.89 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:03:18.039112: step 14230, loss = 0.87 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:03:19.315510: step 14240, loss = 0.92 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:03:20.605434: step 14250, loss = 0.88 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:03:21.879519: step 14260, loss = 1.02 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:03:23.159528: step 14270, loss = 0.76 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:03:24.435376: step 14280, loss = 0.82 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:03:25.738765: step 14290, loss = 0.89 (982.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:03:27.118193: step 14300, loss = 1.00 (927.9 examples/sec; 0.138 sec/batch)
2017-05-08 16:03:28.318442: step 14310, loss = 0.86 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-08 16:03:29.595931: step 14320, loss = 0.94 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:03:30.863078: step 14330, loss = 0.71 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:03:32.146693: step 14340, loss = 0.99 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:03:33.453853: step 14350, loss = 1.10 (979.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:03:34.736088: step 14360, loss = 0.88 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:03:36.019354: step 14370, loss = 1.00 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:03:37.292583: step 14380, loss = 1.02 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:03:38.581809: step 14390, loss = 0.82 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:03:39.939396: step 14400, loss = 0.93 (942.9 examples/sec; 0.136 sec/batch)
2017-05-08 16:03:41.135566: step 14410, loss = 0.68 (1070.1 examples/sec; 0.120 sec/batch)
2017-05-08 16:03:42.422404: step 14420, loss = 0.83 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:03:43.693490: step 14430, loss = 0.90 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:03:45.010827: step 14440, loss = 0.95 (971.7 examples/sec; 0.132 sec/batch)
2017-05-08 16:03:46.308036: step 14450, loss = 1.01 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:03:47.597536: step 14460, loss = 0.71 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:03:48.886325: step 14470, loss = 0.84 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:03:50.172213: step 14480, loss = 1.09 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:03:51.463570: step 14490, loss = 0.88 (991.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:03:52.854963: step 14500, loss = 0.91 (919.9 examples/sec; 0.139 sec/batch)
2017-05-08 16:03:54.036116: step 14510, loss = 0.64 (1083.7 examples/sec; 0.118 sec/batch)
2017-05-08 16:03:55.321356: step 14520, loss = 0.87 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:03:56.605877: step 14530, loss = 0.92 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:03:57.905850: step 14540, loss = 1.02 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:03:59.207100: step 14550, loss = 0.89 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:04:00.494121: step 14560, loss = 0.90 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:04:01.790400: step 14570, loss = 0.97 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:04:03.114687: step 14580, loss = 0.91 (966.6 examples/sec; 0.132 sec/batch)
2017-05-08 16:04:04.425822: step 14590, loss = 0.82 (976.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:04:05.813816: step 14600, loss = 0.97 (922.2 examples/sec; 0.139 sec/batch)
2017-05-08 16:04:06.980538: step 14610, loss = 0.95 (1097.1 examples/sec; 0.117 sec/batch)
2017-05-08 16:04:08.274237: step 14620, loss = 1.04 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:04:09.570155: E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 306 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
step 14630, loss = 0.91 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:04:10.854572: step 14640, loss = 0.88 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:04:12.126043: step 14650, loss = 1.59 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:04:13.410935: step 14660, loss = 0.95 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:04:14.682952: step 14670, loss = 0.90 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:04:15.943438: step 14680, loss = 0.78 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-08 16:04:17.210468: step 14690, loss = 0.74 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:04:18.610919: step 14700, loss = 1.10 (914.0 examples/sec; 0.140 sec/batch)
2017-05-08 16:04:19.814827: step 14710, loss = 0.82 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-08 16:04:21.111278: step 14720, loss = 1.20 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:04:22.423475: step 14730, loss = 1.00 (975.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:04:23.728008: step 14740, loss = 0.85 (981.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:04:25.028389: step 14750, loss = 0.99 (984.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:04:26.289765: step 14760, loss = 1.05 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-08 16:04:27.584495: step 14770, loss = 0.68 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:04:28.869891: step 14780, loss = 1.00 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:04:30.131330: step 14790, loss = 0.76 (1014.7 examples/sec; 0.126 sec/batch)
2017-05-08 16:04:31.496308: step 14800, loss = 0.98 (937.7 examples/sec; 0.136 sec/batch)
2017-05-08 16:04:32.693873: step 14810, loss = 0.96 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-08 16:04:33.986993: step 14820, loss = 0.89 (989.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:04:35.268715: step 14830, loss = 0.94 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:04:36.543224: step 14840, loss = 0.86 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:04:37.818688: step 14850, loss = 0.95 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:04:39.112135: step 14860, loss = 1.00 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:04:40.405067: step 14870, loss = 1.03 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:04:41.714469: step 14880, loss = 0.85 (977.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:04:42.983087: step 14890, loss = 0.77 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:04:44.378888: step 14900, loss = 0.79 (917.0 examples/sec; 0.140 sec/batch)
2017-05-08 16:04:45.594710: step 14910, loss = 0.99 (1052.8 examples/sec; 0.122 sec/batch)
2017-05-08 16:04:46.879850: step 14920, loss = 1.11 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:04:48.156615: step 14930, loss = 0.76 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:04:49.433675: step 14940, loss = 0.95 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:04:50.699692: step 14950, loss = 0.69 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:04:51.978576: step 14960, loss = 0.67 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:04:53.267534: step 14970, loss = 0.97 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:04:54.528009: step 14980, loss = 0.70 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-08 16:04:55.806789: step 14990, loss = 0.83 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:04:57.188600: step 15000, loss = 1.00 (926.3 examples/sec; 0.138 sec/batch)
2017-05-08 16:04:58.382569: step 15010, loss = 0.78 (1072.0 examples/sec; 0.119 sec/batch)
2017-05-08 16:04:59.642997: step 15020, loss = 0.83 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-08 16:05:00.940059: step 15030, loss = 0.81 (986.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:05:02.245313: step 15040, loss = 0.97 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:05:03.553364: step 15050, loss = 0.89 (978.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:05:04.854845: step 15060, loss = 0.78 (983.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:05:06.134522: step 15070, loss = 1.13 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:05:07.447148: step 15080, loss = 0.84 (975.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:05:08.759835: step 15090, loss = 0.98 (975.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:05:10.115261: step 15100, loss = 0.87 (944.4 examples/sec; 0.136 sec/batch)
2017-05-08 16:05:11.292902: step 15110, loss = 0.90 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-08 16:05:12.593368: step 15120, loss = 0.93 (984.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:05:13.882167: step 15130, loss = 0.94 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:05:15.169155: step 15140, loss = 0.92 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:05:16.435592: step 15150, loss = 0.87 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:05:17.708369: step 15160, loss = 0.77 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:05:18.980533: step 15170, loss = 0.95 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:05:20.268654: step 15180, loss = 0.95 (993.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:05:21.576919: step 15190, loss = 1.05 (978.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:05:22.945853: step 15200, loss = 0.91 (935.0 examples/sec; 0.137 sec/batch)
2017-05-08 16:05:24.152441: step 15210, loss = 0.92 (1060.8 examples/sec; 0.121 sec/batch)
2017-05-08 16:05:25.427014: step 15220, loss = 0.84 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:05:26.719152: step 15230, loss = 0.94 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:05:27.978817: step 15240, loss = 0.70 (1016.2 examples/sec; 0.126 sec/batch)
2017-05-08 16:05:29.236992: step 15250, loss = 0.74 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-08 16:05:30.541255: step 15260, loss = 1.07 (981.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:05:31.870091: step 15270, loss = 1.01 (963.2 examples/sec; 0.133 sec/batch)
2017-05-08 16:05:33.184936: step 15280, loss = 0.96 (973.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:05:34.491099: step 15290, loss = 1.06 (980.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:05:35.866402: step 15300, loss = 0.86 (930.7 examples/sec; 0.138 sec/batch)
2017-05-08 16:05:37.049170: step 15310, loss = 0.96 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-08 16:05:38.320307: step 15320, loss = 0.90 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:05:39.594491: step 15330, loss = 0.85 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:05:40.893260: step 15340, loss = 0.76 (985.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:05:42.162692: step 15350, loss = 0.87 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:05:43.439600: step 15360, loss = 0.77 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:05:44.744419: step 15370, loss = 0.84 (981.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:05:46.014768: step 15380, loss = 0.94 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:05:47.303651: step 15390, loss = 0.99 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:05:48.657357: step 15400, loss = 0.97 (945.5 examples/sec; 0.135 sec/batch)
2017-05-08 16:05:49.815868: step 15410, loss = 0.98 (1104.9 examples/sec; 0.116 sec/batch)
2017-05-08 16:05:51.099105: step 15420, loss = 0.95 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:05:52.354847: step 15430, loss = 0.92 (1019.3 examples/sec; 0.126 sec/batch)
2017-05-08 16:05:53.645337: step 15440, loss = 0.85 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:05:54.917068: step 15450, loss = 0.79 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:05:56.192381: step 15460, loss = 0.92 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:05:57.486817: step 15470, loss = 0.95 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:05:58.746375: step 15480, loss = 0.83 (1016.2 examples/sec; 0.126 sec/batch)
2017-05-08 16:06:00.006351: step 15490, loss = 0.72 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-08 16:06:01.377033: step 15500, loss = 1.15 (933.8 examples/sec; 0.137 sec/batch)
2017-05-08 16:06:02.599155: step 15510, loss = 1.07 (1047.4 examples/sec; 0.122 sec/batch)
2017-05-08 16:06:03.888904: step 15520, loss = 1.01 (992.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:06:05.202068: step 15530, loss = 0.98 (974.7 examples/sec; 0.131E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 326 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
 sec/batch)
2017-05-08 16:06:06.521650: step 15540, loss = 0.79 (970.0 examples/sec; 0.132 sec/batch)
2017-05-08 16:06:07.794528: step 15550, loss = 0.91 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:06:09.096837: step 15560, loss = 1.02 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:06:10.377282: step 15570, loss = 0.93 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:06:11.655038: step 15580, loss = 0.87 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:06:12.948857: step 15590, loss = 0.93 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:06:14.356989: step 15600, loss = 1.04 (909.0 examples/sec; 0.141 sec/batch)
2017-05-08 16:06:15.547771: step 15610, loss = 0.89 (1074.9 examples/sec; 0.119 sec/batch)
2017-05-08 16:06:16.831116: step 15620, loss = 0.85 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:06:18.122008: step 15630, loss = 0.92 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:06:19.404548: step 15640, loss = 0.79 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:06:20.691143: step 15650, loss = 0.93 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:06:22.000369: step 15660, loss = 1.02 (977.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:06:23.283364: step 15670, loss = 0.84 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:06:24.557952: step 15680, loss = 0.77 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:06:25.817968: step 15690, loss = 1.01 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-08 16:06:27.207074: step 15700, loss = 0.84 (921.5 examples/sec; 0.139 sec/batch)
2017-05-08 16:06:28.373587: step 15710, loss = 0.82 (1097.3 examples/sec; 0.117 sec/batch)
2017-05-08 16:06:29.664420: step 15720, loss = 0.94 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:06:30.929092: step 15730, loss = 0.98 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-08 16:06:32.237143: step 15740, loss = 1.03 (978.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:06:33.567146: step 15750, loss = 0.90 (962.4 examples/sec; 0.133 sec/batch)
2017-05-08 16:06:34.848578: step 15760, loss = 0.86 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:06:36.159859: step 15770, loss = 0.94 (976.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:06:37.465508: step 15780, loss = 0.87 (980.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:06:38.775228: step 15790, loss = 1.03 (977.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:06:40.160686: step 15800, loss = 1.16 (923.9 examples/sec; 0.139 sec/batch)
2017-05-08 16:06:41.341564: step 15810, loss = 0.94 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-08 16:06:42.612688: step 15820, loss = 0.96 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:06:43.879434: step 15830, loss = 0.96 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:06:45.159256: step 15840, loss = 0.79 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:06:46.450440: step 15850, loss = 0.77 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:06:47.725431: step 15860, loss = 0.80 (1003.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:06:49.020946: step 15870, loss = 0.83 (988.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:06:50.301695: step 15880, loss = 0.78 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:06:51.574334: step 15890, loss = 1.08 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:06:52.941428: step 15900, loss = 0.88 (936.3 examples/sec; 0.137 sec/batch)
2017-05-08 16:06:54.121105: step 15910, loss = 0.75 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-08 16:06:55.397479: step 15920, loss = 0.81 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:06:56.693767: step 15930, loss = 0.79 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:06:57.967898: step 15940, loss = 0.71 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:06:59.253039: step 15950, loss = 0.82 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:07:00.534700: step 15960, loss = 0.90 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:07:01.814516: step 15970, loss = 0.82 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:07:03.083606: step 15980, loss = 1.05 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:07:04.370563: step 15990, loss = 0.98 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:07:05.746793: step 16000, loss = 0.84 (930.1 examples/sec; 0.138 sec/batch)
2017-05-08 16:07:06.939230: step 16010, loss = 0.85 (1073.4 examples/sec; 0.119 sec/batch)
2017-05-08 16:07:08.203361: step 16020, loss = 0.95 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-08 16:07:09.500082: step 16030, loss = 0.90 (987.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:07:10.796878: step 16040, loss = 0.89 (987.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:07:12.073605: step 16050, loss = 1.04 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:07:13.356554: step 16060, loss = 0.91 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:07:14.643638: step 16070, loss = 0.91 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:07:15.922966: step 16080, loss = 0.82 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:07:17.215864: step 16090, loss = 0.76 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:07:18.589779: step 16100, loss = 0.91 (931.6 examples/sec; 0.137 sec/batch)
2017-05-08 16:07:19.824970: step 16110, loss = 1.09 (1036.3 examples/sec; 0.124 sec/batch)
2017-05-08 16:07:21.116029: step 16120, loss = 0.85 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:07:22.422693: step 16130, loss = 0.98 (979.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:07:23.693884: step 16140, loss = 0.95 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:07:24.992516: step 16150, loss = 0.69 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:07:26.305545: step 16160, loss = 1.01 (974.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:07:27.611931: step 16170, loss = 0.85 (979.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:07:28.873889: step 16180, loss = 0.86 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-08 16:07:30.164823: step 16190, loss = 0.85 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:07:31.534766: step 16200, loss = 0.83 (934.3 examples/sec; 0.137 sec/batch)
2017-05-08 16:07:32.728309: step 16210, loss = 0.99 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-08 16:07:34.002245: step 16220, loss = 1.01 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:07:35.280961: step 16230, loss = 0.79 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:07:36.563029: step 16240, loss = 0.96 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:07:37.842591: step 16250, loss = 1.07 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:07:39.147312: step 16260, loss = 0.78 (981.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:07:40.416825: step 16270, loss = 0.83 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:07:41.702751: step 16280, loss = 0.95 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:07:42.974545: step 16290, loss = 1.04 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:07:44.366539: step 16300, loss = 0.83 (919.5 examples/sec; 0.139 sec/batch)
2017-05-08 16:07:45.546118: step 16310, loss = 0.88 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-08 16:07:46.827816: step 16320, loss = 0.72 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:07:48.105399: step 16330, loss = 0.74 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:07:49.408490: step 16340, loss = 1.04 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:07:50.689398: step 16350, loss = 0.91 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:07:51.983957: step 16360, loss = 0.85 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:07:53.303048: step 16370, loss = 0.89 (970.4 examples/sec; 0.132 sec/batch)
2017-05-08 16:07:54.605273: step 16380, loss = 0.92 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:07:55.931116: step 16390, loss = 1.00 (965.4 examples/sec; 0.133 sec/batch)
2017-05-08 16:07:57.331404: step 16400, loss = 0.92 (914.1 examples/sec; 0.140 sec/batch)
2017-05-08 16:07:58.497628: step 16410, loss = 1.00 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-08 16:07:59.779668: step 16420, loss = 1.04 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:08:01.074818: step 16430, loss = 0.83 (988.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:08:02.379854: step 16440, loss = 0.83 (980.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:08:03.649722: step 16450, loss = 0.79 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:08:04.930372: step 16460, loss = 0.73 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:08:06.223709: step 16470, loss = 0.84 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:08:07.530749: step 16480, loss = 1.06 (979.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:08:08.857346: step 16490, loss = 0.94 (964.9 examples/sec; 0.133 sec/batch)
2017-05-08 16:08:10.251055: step 16500, loss = 0.88 (918.4 examples/sec; 0.139 sec/batch)
2017-05-08 16:08:11.422664: step 16510, loss = 0.96 (1092.5 examples/sec; 0.117 sec/batch)
2017-05-08 16:08:12.712311: step 16520, loss = 0.93 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:08:13.977360: step 16530, loss = 0.85 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:08:15.271960: step 16540, loss = 0.90 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:08:16.549382: step 16550, loss = 0.90 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:08:17.836767: step 16560, loss = 0.80 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:08:19.130076: step 16570, loss = 1.01 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:08:20.442963: step 16580, loss = 0.98 (975.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:08:21.727237: step 16590, loss = 0.92 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:08:23.111641: step 16600, loss = 0.86 (924.6 examples/sec; 0.138 sec/batch)
2017-05-08 16:08:24.281959: step 16610, loss = 0.94 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-08 16:08:25.573866: step 16620, loss = 0.88 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:08:26.873113: step 16630, loss = 0.72 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:08:28.138308: step 16640, loss = 0.77 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:08:29.412195: step 16650, loss = 0.95 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:08:30.675248: step 16660, loss = 0.91 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-08 16:08:31.977380: step 16670, loss = 0.95 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:08:33.262083: step 16680, loss = 0.94 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:08:34.593654: step 16690, loss = 0.81 (961.3 examples/sec; 0.133 sec/batch)
2017-05-08 16:08:35.998412: step 16700, loss = 0.83 (911.2 examples/sec; 0.140 sec/batch)
2017-05-08 16:08:37.170979: step 16710, loss = 0.77 (1091.6 examples/sec; 0.117 sec/batch)
2017-05-08 16:08:38.452183: step 16720, loss = 0.90 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:08:39.725786: step 16730, loss = 0.75 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:08:41.017668: step 16740, loss = 0.89 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:08:42.310138: step 16750, loss = 1.09 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:08:43.570155: step 16760, loss = 0.80 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-08 16:08:44.870360: step 16770, loss = 0.99 (984.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:08:46.144835: step 16780, loss = 1.00 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:08:47.424680: step 16790, loss = 0.91 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:08:48.802275: step 16800, loss = 0.82 (929.2 examples/sec; 0.138 sec/batch)
2017-05-08 16:08:49.972414: step 16810, loss = 0.81 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-08 16:08:51.247414: step 16820, loss = 0.91 (1003.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:08:52.533373: step 16830, loss = 1.13 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:08:53.802071: step 16840, loss = 0.74 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:08:55.081877: step 16850, loss = 0.73 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:08:56.389945: step 16860, loss = 0.81 (978.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:08:57.700736: step 16870, loss = 0.83 (976.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:08:58.992542: step 16880, loss = 0.86 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:09:00.270461: step 16890, loss = 0.87 (1001.6 examples/sec; 0E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 346 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
.128 sec/batch)
2017-05-08 16:09:01.652433: step 16900, loss = 0.79 (926.2 examples/sec; 0.138 sec/batch)
2017-05-08 16:09:02.850530: step 16910, loss = 0.92 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-08 16:09:04.123351: step 16920, loss = 0.92 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:09:05.390059: step 16930, loss = 0.73 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:09:06.652806: step 16940, loss = 0.94 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-08 16:09:07.944985: step 16950, loss = 1.01 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:09:09.248124: step 16960, loss = 0.78 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:09:10.565627: step 16970, loss = 0.93 (971.5 examples/sec; 0.132 sec/batch)
2017-05-08 16:09:11.877521: step 16980, loss = 0.82 (975.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:09:13.175383: step 16990, loss = 0.87 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:09:14.554037: step 17000, loss = 0.98 (928.4 examples/sec; 0.138 sec/batch)
2017-05-08 16:09:15.758723: step 17010, loss = 1.02 (1062.5 examples/sec; 0.120 sec/batch)
2017-05-08 16:09:17.080481: step 17020, loss = 1.14 (968.4 examples/sec; 0.132 sec/batch)
2017-05-08 16:09:18.363690: step 17030, loss = 0.91 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:09:19.668204: step 17040, loss = 0.97 (981.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:09:20.996720: step 17050, loss = 0.70 (963.5 examples/sec; 0.133 sec/batch)
2017-05-08 16:09:22.281235: step 17060, loss = 0.87 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:09:23.552181: step 17070, loss = 1.06 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:09:24.854966: step 17080, loss = 0.85 (982.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:09:26.150665: step 17090, loss = 1.01 (987.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:09:27.534844: step 17100, loss = 0.72 (924.7 examples/sec; 0.138 sec/batch)
2017-05-08 16:09:28.762442: step 17110, loss = 1.12 (1042.7 examples/sec; 0.123 sec/batch)
2017-05-08 16:09:30.042242: step 17120, loss = 0.83 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:09:31.327492: step 17130, loss = 0.82 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:09:32.601921: step 17140, loss = 0.81 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:09:33.901826: step 17150, loss = 0.95 (984.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:09:35.194907: step 17160, loss = 0.92 (989.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:09:36.490232: step 17170, loss = 0.86 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:09:37.756805: step 17180, loss = 0.69 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:09:39.046918: step 17190, loss = 0.74 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:09:40.407424: step 17200, loss = 0.83 (940.8 examples/sec; 0.136 sec/batch)
2017-05-08 16:09:41.579180: step 17210, loss = 0.98 (1092.4 examples/sec; 0.117 sec/batch)
2017-05-08 16:09:42.858422: step 17220, loss = 0.89 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:09:44.131714: step 17230, loss = 0.86 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:09:45.428409: step 17240, loss = 1.05 (987.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:09:46.723190: step 17250, loss = 0.76 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:09:48.010593: step 17260, loss = 0.77 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:09:49.311817: step 17270, loss = 0.69 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:09:50.580061: step 17280, loss = 0.89 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:09:51.861608: step 17290, loss = 0.61 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:09:53.234479: step 17300, loss = 0.91 (932.4 examples/sec; 0.137 sec/batch)
2017-05-08 16:09:54.431370: step 17310, loss = 0.84 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-08 16:09:55.683048: step 17320, loss = 0.88 (1022.6 examples/sec; 0.125 sec/batch)
2017-05-08 16:09:56.998644: step 17330, loss = 0.98 (972.9 examples/sec; 0.132 sec/batch)
2017-05-08 16:09:58.274608: step 17340, loss = 0.81 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:09:59.560147: step 17350, loss = 0.90 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:10:00.860330: step 17360, loss = 0.92 (984.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:10:02.156847: step 17370, loss = 0.89 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:10:03.445551: step 17380, loss = 0.97 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:10:04.719174: step 17390, loss = 0.87 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:10:06.097299: step 17400, loss = 0.71 (928.8 examples/sec; 0.138 sec/batch)
2017-05-08 16:10:07.314491: step 17410, loss = 0.92 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-08 16:10:08.599935: step 17420, loss = 1.08 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:10:09.927197: step 17430, loss = 1.00 (964.4 examples/sec; 0.133 sec/batch)
2017-05-08 16:10:11.196987: step 17440, loss = 0.99 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:10:12.466026: step 17450, loss = 0.75 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:10:13.762382: step 17460, loss = 0.69 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:10:15.034957: step 17470, loss = 0.83 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:10:16.321537: step 17480, loss = 0.77 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:10:17.620549: step 17490, loss = 0.94 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:10:19.002473: step 17500, loss = 0.90 (926.2 examples/sec; 0.138 sec/batch)
2017-05-08 16:10:20.178061: step 17510, loss = 0.84 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-08 16:10:21.447215: step 17520, loss = 0.90 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:10:22.730755: step 17530, loss = 0.93 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:10:24.016823: step 17540, loss = 0.89 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:10:25.302806: step 17550, loss = 0.73 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:10:26.569932: step 17560, loss = 0.93 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:10:27.852232: step 17570, loss = 0.87 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:10:29.134373: step 17580, loss = 0.96 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:10:30.397417: step 17590, loss = 0.87 (1013.4 examples/sec; 0.126 sec/batch)
2017-05-08 16:10:31.774953: step 17600, loss = 0.92 (929.2 examples/sec; 0.138 sec/batch)
2017-05-08 16:10:32.960891: step 17610, loss = 1.20 (1079.3 examples/sec; 0.119 sec/batch)
2017-05-08 16:10:34.256314: step 17620, loss = 1.19 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:10:35.517186: step 17630, loss = 0.85 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-08 16:10:36.784843: step 17640, loss = 0.97 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:10:38.062249: step 17650, loss = 0.95 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:10:39.347277: step 17660, loss = 0.90 (996.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:10:40.649510: step 17670, loss = 0.93 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:10:41.951410: step 17680, loss = 0.83 (983.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:10:43.260573: step 17690, loss = 1.06 (977.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:10:44.648365: step 17700, loss = 0.91 (922.3 examples/sec; 0.139 sec/batch)
2017-05-08 16:10:45.843351: step 17710, loss = 0.77 (1071.1 examples/sec; 0.119 sec/batch)
2017-05-08 16:10:47.159109: step 17720, loss = 0.89 (972.8 examples/sec; 0.132 sec/batch)
2017-05-08 16:10:48.441491: step 17730, loss = 0.90 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:10:49.708692: step 17740, loss = 0.80 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:10:51.018393: step 17750, loss = 1.10 (977.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:10:52.323835: step 17760, loss = 1.10 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:10:53.601598: step 17770, loss = 0.73 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:10:54.885892: step 17780, loss = 0.75 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:10:56.206688: step 17790, loss = 0.97 (969.1 examples/sec; 0.132 sec/batch)
2017-05-08 16:10:57.622962: step 17800, losE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 366 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
s = 0.97 (903.8 examples/sec; 0.142 sec/batch)
2017-05-08 16:10:58.828587: step 17810, loss = 0.83 (1061.7 examples/sec; 0.121 sec/batch)
2017-05-08 16:11:00.135777: step 17820, loss = 0.82 (979.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:11:01.439431: step 17830, loss = 0.83 (981.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:11:02.744523: step 17840, loss = 0.91 (980.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:11:04.023503: step 17850, loss = 0.83 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:11:05.306978: step 17860, loss = 0.83 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:11:06.604774: step 17870, loss = 0.90 (986.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:11:07.863755: step 17880, loss = 0.83 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-08 16:11:09.153332: step 17890, loss = 0.99 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:11:10.512086: step 17900, loss = 0.77 (942.0 examples/sec; 0.136 sec/batch)
2017-05-08 16:11:11.721657: step 17910, loss = 0.92 (1058.2 examples/sec; 0.121 sec/batch)
2017-05-08 16:11:13.031919: step 17920, loss = 1.05 (976.9 examples/sec; 0.131 sec/batch)
2017-05-08 16:11:14.352780: step 17930, loss = 1.17 (969.1 examples/sec; 0.132 sec/batch)
2017-05-08 16:11:15.625780: step 17940, loss = 0.89 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:11:16.933352: step 17950, loss = 0.82 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 16:11:18.213800: step 17960, loss = 0.97 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:11:19.489386: step 17970, loss = 0.87 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:11:20.786331: step 17980, loss = 0.82 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:11:22.057661: step 17990, loss = 0.91 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:11:23.430884: step 18000, loss = 0.78 (932.1 examples/sec; 0.137 sec/batch)
2017-05-08 16:11:24.610648: step 18010, loss = 0.96 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-08 16:11:25.883735: step 18020, loss = 0.75 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:11:27.146244: step 18030, loss = 0.87 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-08 16:11:28.412146: step 18040, loss = 0.98 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:11:29.719236: step 18050, loss = 0.83 (979.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:11:31.036914: step 18060, loss = 0.92 (971.4 examples/sec; 0.132 sec/batch)
2017-05-08 16:11:32.308370: step 18070, loss = 1.15 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:11:33.580321: step 18080, loss = 0.75 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:11:34.851980: step 18090, loss = 0.78 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:11:36.249498: step 18100, loss = 0.81 (915.9 examples/sec; 0.140 sec/batch)
2017-05-08 16:11:37.451110: step 18110, loss = 0.88 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-08 16:11:38.719197: step 18120, loss = 0.88 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:11:40.012385: step 18130, loss = 0.91 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:11:41.303315: step 18140, loss = 0.76 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:11:42.609863: step 18150, loss = 0.99 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:11:43.876282: step 18160, loss = 0.92 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:11:45.145964: step 18170, loss = 0.89 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:11:46.423871: step 18180, loss = 0.91 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:11:47.709752: step 18190, loss = 0.92 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:11:49.112770: step 18200, loss = 0.83 (912.3 examples/sec; 0.140 sec/batch)
2017-05-08 16:11:50.315059: step 18210, loss = 1.08 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-08 16:11:51.597934: step 18220, loss = 0.96 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:11:52.896913: step 18230, loss = 0.84 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:11:54.203078: step 18240, loss = 0.87 (980.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:11:55.488596: step 18250, loss = 0.88 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:11:56.772771: step 18260, loss = 0.86 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:11:58.060192: step 18270, loss = 0.92 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:11:59.357193: step 18280, loss = 0.89 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:12:00.623747: step 18290, loss = 1.00 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:12:01.995688: step 18300, loss = 0.84 (933.0 examples/sec; 0.137 sec/batch)
2017-05-08 16:12:03.187043: step 18310, loss = 0.79 (1074.4 examples/sec; 0.119 sec/batch)
2017-05-08 16:12:04.458367: step 18320, loss = 0.74 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:12:05.772370: step 18330, loss = 0.85 (974.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:12:07.065157: step 18340, loss = 0.94 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:12:08.343867: step 18350, loss = 0.93 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:12:09.620756: step 18360, loss = 0.94 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:12:10.885708: step 18370, loss = 0.85 (1011.9 examples/sec; 0.126 sec/batch)
2017-05-08 16:12:12.163996: step 18380, loss = 0.91 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:12:13.436932: step 18390, loss = 0.93 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:12:14.848318: step 18400, loss = 0.83 (906.9 examples/sec; 0.141 sec/batch)
2017-05-08 16:12:16.013889: step 18410, loss = 0.94 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-08 16:12:17.342113: step 18420, loss = 0.84 (963.7 examples/sec; 0.133 sec/batch)
2017-05-08 16:12:18.641583: step 18430, loss = 0.90 (985.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:12:19.916026: step 18440, loss = 1.09 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:12:21.227412: step 18450, loss = 0.94 (976.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:12:22.550915: step 18460, loss = 0.81 (967.1 examples/sec; 0.132 sec/batch)
2017-05-08 16:12:23.830940: step 18470, loss = 0.97 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:12:25.127339: step 18480, loss = 0.96 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:12:26.442300: step 18490, loss = 1.03 (973.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:12:27.817426: step 18500, loss = 0.92 (930.8 examples/sec; 0.138 sec/batch)
2017-05-08 16:12:28.994819: step 18510, loss = 0.73 (1087.1 examples/sec; 0.118 sec/batch)
2017-05-08 16:12:30.263536: step 18520, loss = 0.79 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:12:31.546961: step 18530, loss = 0.85 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:12:32.844704: step 18540, loss = 0.77 (986.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:12:34.147607: step 18550, loss = 0.90 (982.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:12:35.399138: step 18560, loss = 0.94 (1022.7 examples/sec; 0.125 sec/batch)
2017-05-08 16:12:36.679915: step 18570, loss = 0.90 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:12:37.986373: step 18580, loss = 0.79 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:12:39.282129: step 18590, loss = 0.82 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:12:40.645095: step 18600, loss = 0.94 (939.1 examples/sec; 0.136 sec/batch)
2017-05-08 16:12:41.817757: step 18610, loss = 1.00 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-08 16:12:43.104296: step 18620, loss = 0.76 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:12:44.396192: step 18630, loss = 0.81 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:12:45.725947: step 18640, loss = 0.80 (962.6 examples/sec; 0.133 sec/batch)
2017-05-08 16:12:47.004122: step 18650, loss = 1.04 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:12:48.299290: step 18660, loss = 0.83 (988.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:12:49.593809: step 18670, loss = 0.93 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:12:50.899760: step 18680, loss = 0.73 (980.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:12:52.202968: step 18690, loss = 1.02 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:12:53.585666: step 18700, loss = 0.97 (925.7 examples/sec; 0.138 sec/batch)
2017-05-0E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 387 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
8 16:12:54.779070: step 18710, loss = 1.02 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-08 16:12:56.050011: step 18720, loss = 0.77 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:12:57.325391: step 18730, loss = 0.87 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:12:58.598946: step 18740, loss = 0.77 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:12:59.869135: step 18750, loss = 0.78 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:13:01.132959: step 18760, loss = 0.91 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-08 16:13:02.437267: step 18770, loss = 0.70 (981.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:13:03.710208: step 18780, loss = 0.85 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:13:04.978544: step 18790, loss = 0.99 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:13:06.354634: step 18800, loss = 0.86 (930.2 examples/sec; 0.138 sec/batch)
2017-05-08 16:13:07.539947: step 18810, loss = 0.70 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-08 16:13:08.827317: step 18820, loss = 1.25 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:13:10.141083: step 18830, loss = 0.65 (974.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:13:11.415584: step 18840, loss = 0.91 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:13:12.691216: step 18850, loss = 0.76 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:13:13.967015: step 18860, loss = 0.80 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:13:15.262703: step 18870, loss = 0.81 (987.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:13:16.539746: step 18880, loss = 0.82 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:13:17.815280: step 18890, loss = 0.88 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:13:19.207838: step 18900, loss = 0.68 (919.2 examples/sec; 0.139 sec/batch)
2017-05-08 16:13:20.361555: step 18910, loss = 0.80 (1109.5 examples/sec; 0.115 sec/batch)
2017-05-08 16:13:21.670558: step 18920, loss = 0.76 (977.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:13:22.944621: step 18930, loss = 0.90 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:13:24.205156: step 18940, loss = 1.03 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-08 16:13:25.530701: step 18950, loss = 0.89 (965.7 examples/sec; 0.133 sec/batch)
2017-05-08 16:13:26.803647: step 18960, loss = 0.95 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:13:28.063901: step 18970, loss = 0.68 (1015.7 examples/sec; 0.126 sec/batch)
2017-05-08 16:13:29.352171: step 18980, loss = 0.94 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:13:30.648934: step 18990, loss = 0.72 (987.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:13:32.017925: step 19000, loss = 0.84 (935.0 examples/sec; 0.137 sec/batch)
2017-05-08 16:13:33.214378: step 19010, loss = 0.91 (1069.8 examples/sec; 0.120 sec/batch)
2017-05-08 16:13:34.512928: step 19020, loss = 0.99 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:13:35.770929: step 19030, loss = 0.86 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-08 16:13:37.049425: step 19040, loss = 0.91 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:13:38.330566: step 19050, loss = 0.89 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:13:39.594287: step 19060, loss = 0.88 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-08 16:13:40.875582: step 19070, loss = 0.76 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:13:42.157720: step 19080, loss = 0.76 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:13:43.452997: step 19090, loss = 0.86 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:13:44.834375: step 19100, loss = 0.93 (926.6 examples/sec; 0.138 sec/batch)
2017-05-08 16:13:46.018252: step 19110, loss = 0.68 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-08 16:13:47.307739: step 19120, loss = 0.95 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:13:48.577396: step 19130, loss = 0.85 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:13:49.838773: step 19140, loss = 0.76 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-08 16:13:51.149673: step 19150, loss = 0.83 (976.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:13:52.436671: step 19160, loss = 0.91 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:13:53.740484: step 19170, loss = 1.17 (981.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:13:55.031893: step 19180, loss = 0.68 (991.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:13:56.319967: step 19190, loss = 0.82 (993.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:13:57.681172: step 19200, loss = 1.01 (940.3 examples/sec; 0.136 sec/batch)
2017-05-08 16:13:58.867997: step 19210, loss = 0.91 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-08 16:14:00.154046: step 19220, loss = 0.78 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:14:01.439927: step 19230, loss = 0.74 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:14:02.716748: step 19240, loss = 0.94 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:14:03.997675: step 19250, loss = 0.78 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:14:05.277565: step 19260, loss = 0.92 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:14:06.557237: step 19270, loss = 0.89 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:14:07.847517: step 19280, loss = 0.79 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:14:09.132154: step 19290, loss = 0.96 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:14:10.526141: step 19300, loss = 0.77 (918.2 examples/sec; 0.139 sec/batch)
2017-05-08 16:14:11.707902: step 19310, loss = 0.85 (1083.1 examples/sec; 0.118 sec/batch)
2017-05-08 16:14:12.994629: step 19320, loss = 0.71 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:14:14.286867: step 19330, loss = 0.76 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:14:15.564112: step 19340, loss = 0.91 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:14:16.859379: step 19350, loss = 0.78 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:14:18.142992: step 19360, loss = 1.08 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:14:19.418462: step 19370, loss = 0.91 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:14:20.728164: step 19380, loss = 0.83 (977.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:14:22.014198: step 19390, loss = 0.89 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:14:23.381308: step 19400, loss = 0.81 (936.3 examples/sec; 0.137 sec/batch)
2017-05-08 16:14:24.572581: step 19410, loss = 0.99 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-08 16:14:25.862213: step 19420, loss = 0.89 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:14:27.154309: step 19430, loss = 0.77 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:14:28.454817: step 19440, loss = 0.81 (984.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:14:29.754959: step 19450, loss = 0.88 (984.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:14:31.027203: step 19460, loss = 0.87 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:14:32.321730: step 19470, loss = 0.96 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:14:33.607214: step 19480, loss = 1.07 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:14:34.887801: step 19490, loss = 0.89 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:14:36.240943: step 19500, loss = 0.73 (945.9 examples/sec; 0.135 sec/batch)
2017-05-08 16:14:37.432837: step 19510, loss = 0.74 (1073.9 examples/sec; 0.119 sec/batch)
2017-05-08 16:14:38.724256: step 19520, loss = 0.90 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:14:39.987759: step 19530, loss = 1.09 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-08 16:14:41.284408: step 19540, loss = 0.93 (987.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:14:42.585186: step 19550, loss = 0.65 (984.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:14:43.870430: step 19560, loss = 0.83 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:14:45.149858: step 19570, loss = 0.70 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:14:46.429277: step 19580, loss = 0.81 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:14:47.725524: step 19590, loss = 0.97 (987.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:14:49.126618: step 19600, loss = 0.77 (913.6 examples/sec; 0.140 sec/batch)
2017-05-08 16:14:50.306646: step 19610, loss = 0.81 (1084.7 exE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 407 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
amples/sec; 0.118 sec/batch)
2017-05-08 16:14:51.569418: step 19620, loss = 0.87 (1013.6 examples/sec; 0.126 sec/batch)
2017-05-08 16:14:52.828280: step 19630, loss = 0.85 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-08 16:14:54.105707: step 19640, loss = 0.88 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:14:55.400221: step 19650, loss = 0.88 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:14:56.696660: step 19660, loss = 0.89 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:14:58.013038: step 19670, loss = 0.92 (972.4 examples/sec; 0.132 sec/batch)
2017-05-08 16:14:59.308374: step 19680, loss = 0.97 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:15:00.569382: step 19690, loss = 0.93 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-08 16:15:01.960379: step 19700, loss = 0.84 (920.2 examples/sec; 0.139 sec/batch)
2017-05-08 16:15:03.157503: step 19710, loss = 0.76 (1069.2 examples/sec; 0.120 sec/batch)
2017-05-08 16:15:04.425780: step 19720, loss = 0.95 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:15:05.692309: step 19730, loss = 0.73 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:15:06.964559: step 19740, loss = 0.78 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:15:08.254671: step 19750, loss = 0.86 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:15:09.552140: step 19760, loss = 0.96 (986.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:15:10.857505: step 19770, loss = 0.84 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:15:12.158750: step 19780, loss = 0.83 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:15:13.437365: step 19790, loss = 0.95 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:15:14.801131: step 19800, loss = 0.71 (938.6 examples/sec; 0.136 sec/batch)
2017-05-08 16:15:15.984706: step 19810, loss = 0.84 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-08 16:15:17.291410: step 19820, loss = 0.94 (979.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:15:18.588246: step 19830, loss = 0.96 (987.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:15:19.854579: step 19840, loss = 1.08 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:15:21.129700: step 19850, loss = 0.73 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:15:22.416433: step 19860, loss = 0.89 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:15:23.707295: step 19870, loss = 1.07 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:15:24.988044: step 19880, loss = 0.85 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:15:26.263107: step 19890, loss = 0.82 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:15:27.639402: step 19900, loss = 0.98 (930.0 examples/sec; 0.138 sec/batch)
2017-05-08 16:15:28.841097: step 19910, loss = 0.77 (1065.2 examples/sec; 0.120 sec/batch)
2017-05-08 16:15:30.117566: step 19920, loss = 0.78 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:15:31.418061: step 19930, loss = 0.93 (984.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:15:32.687127: step 19940, loss = 0.84 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:15:33.980123: step 19950, loss = 0.77 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:15:35.259299: step 19960, loss = 0.93 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:15:36.516901: step 19970, loss = 0.90 (1017.8 examples/sec; 0.126 sec/batch)
2017-05-08 16:15:37.769506: step 19980, loss = 0.80 (1021.9 examples/sec; 0.125 sec/batch)
2017-05-08 16:15:39.060634: step 19990, loss = 1.02 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:15:40.452080: step 20000, loss = 1.00 (919.9 examples/sec; 0.139 sec/batch)
2017-05-08 16:15:41.631247: step 20010, loss = 0.78 (1085.5 examples/sec; 0.118 sec/batch)
2017-05-08 16:15:42.890918: step 20020, loss = 0.74 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-08 16:15:44.162691: step 20030, loss = 0.80 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:15:45.479865: step 20040, loss = 0.90 (971.8 examples/sec; 0.132 sec/batch)
2017-05-08 16:15:46.772539: step 20050, loss = 0.77 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:15:48.046086: step 20060, loss = 0.96 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:15:49.326762: step 20070, loss = 0.84 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:15:50.597968: step 20080, loss = 0.95 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:15:51.878964: step 20090, loss = 0.74 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:15:53.259771: step 20100, loss = 0.80 (927.0 examples/sec; 0.138 sec/batch)
2017-05-08 16:15:54.445409: step 20110, loss = 0.85 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-08 16:15:55.733321: step 20120, loss = 0.82 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:15:57.008382: step 20130, loss = 0.71 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:15:58.285722: step 20140, loss = 0.87 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:15:59.547056: step 20150, loss = 1.01 (1014.8 examples/sec; 0.126 sec/batch)
2017-05-08 16:16:00.836218: step 20160, loss = 0.88 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:16:02.121159: step 20170, loss = 0.73 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:16:03.415966: step 20180, loss = 0.81 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:16:04.692006: step 20190, loss = 0.83 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:16:06.048917: step 20200, loss = 0.71 (943.3 examples/sec; 0.136 sec/batch)
2017-05-08 16:16:07.236409: step 20210, loss = 0.97 (1077.9 examples/sec; 0.119 sec/batch)
2017-05-08 16:16:08.519880: step 20220, loss = 0.87 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:16:09.799952: step 20230, loss = 0.90 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:16:11.084890: step 20240, loss = 0.75 (996.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:16:12.352874: step 20250, loss = 0.75 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:16:13.659885: step 20260, loss = 0.87 (979.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:16:14.960216: step 20270, loss = 0.92 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:16:16.261617: step 20280, loss = 0.94 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:16:17.568659: step 20290, loss = 1.16 (979.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:16:18.938885: step 20300, loss = 0.89 (934.2 examples/sec; 0.137 sec/batch)
2017-05-08 16:16:20.138069: step 20310, loss = 0.92 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-08 16:16:21.404328: step 20320, loss = 0.84 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:16:22.715878: step 20330, loss = 0.86 (975.9 examples/sec; 0.131 sec/batch)
2017-05-08 16:16:24.004288: step 20340, loss = 0.83 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:16:25.312233: step 20350, loss = 0.91 (978.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:16:26.611700: step 20360, loss = 0.95 (985.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:16:27.915739: step 20370, loss = 0.75 (981.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:16:29.206641: step 20380, loss = 0.80 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:16:30.473975: step 20390, loss = 0.70 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:16:31.856201: step 20400, loss = 0.79 (926.0 examples/sec; 0.138 sec/batch)
2017-05-08 16:16:33.047099: step 20410, loss = 0.87 (1074.8 examples/sec; 0.119 sec/batch)
2017-05-08 16:16:34.341351: step 20420, loss = 0.89 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:16:35.635158: step 20430, loss = 0.83 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:16:36.944153: step 20440, loss = 1.01 (977.9 examples/sec; 0.131 sec/batch)
2017-05-08 16:16:38.250613: step 20450, loss = 1.00 (979.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:16:39.557769: step 20460, loss = 0.95 (979.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:16:40.856801: step 20470, loss = 0.77 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:16:42.149775: step 20480, loss = 0.63 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:16:43.418859: step 20490, loss = 1.03 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:16:44.813712: step 20500, loss = 0.79 (917.7 examples/sec; 0.139 sec/batch)
2017-05-08 16:16:46.005369: step 20510, loss = 0.76 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-08 16:16:47.32118E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 427 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
8: step 20520, loss = 0.89 (972.8 examples/sec; 0.132 sec/batch)
2017-05-08 16:16:48.624405: step 20530, loss = 0.95 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:16:49.925749: step 20540, loss = 0.95 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:16:51.228167: step 20550, loss = 0.94 (982.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:16:52.503574: step 20560, loss = 0.90 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:16:53.802563: step 20570, loss = 0.80 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:16:55.086709: step 20580, loss = 0.85 (996.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:16:56.351778: step 20590, loss = 0.66 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:16:57.754144: step 20600, loss = 0.85 (912.8 examples/sec; 0.140 sec/batch)
2017-05-08 16:16:58.944576: step 20610, loss = 0.72 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-08 16:17:00.227944: step 20620, loss = 0.74 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:17:01.538129: step 20630, loss = 0.91 (977.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:17:02.838433: step 20640, loss = 0.82 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:17:04.136320: step 20650, loss = 0.80 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:17:05.417711: step 20660, loss = 0.76 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:17:06.692613: step 20670, loss = 0.80 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:17:07.966971: step 20680, loss = 0.83 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:17:09.247504: step 20690, loss = 0.81 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:17:10.614050: step 20700, loss = 0.82 (936.7 examples/sec; 0.137 sec/batch)
2017-05-08 16:17:11.797112: step 20710, loss = 0.85 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-08 16:17:13.087947: step 20720, loss = 0.82 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:17:14.384565: step 20730, loss = 0.86 (987.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:17:15.662370: step 20740, loss = 0.82 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:17:16.948776: step 20750, loss = 0.82 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:17:18.221907: step 20760, loss = 0.93 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:17:19.508215: step 20770, loss = 0.83 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:17:20.785693: step 20780, loss = 0.78 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:17:22.076962: step 20790, loss = 0.80 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:17:23.464242: step 20800, loss = 0.82 (922.7 examples/sec; 0.139 sec/batch)
2017-05-08 16:17:24.642662: step 20810, loss = 0.80 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-08 16:17:25.925159: step 20820, loss = 0.93 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:17:27.209047: step 20830, loss = 0.83 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:17:28.476760: step 20840, loss = 0.66 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:17:29.763343: step 20850, loss = 0.90 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:17:31.063368: step 20860, loss = 0.79 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:17:32.334226: step 20870, loss = 0.77 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:17:33.609614: step 20880, loss = 0.78 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:17:34.916300: step 20890, loss = 0.90 (979.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:17:36.300133: step 20900, loss = 0.72 (925.0 examples/sec; 0.138 sec/batch)
2017-05-08 16:17:37.485541: step 20910, loss = 0.92 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-08 16:17:38.804895: step 20920, loss = 0.87 (970.2 examples/sec; 0.132 sec/batch)
2017-05-08 16:17:40.071937: step 20930, loss = 0.90 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:17:41.354339: step 20940, loss = 0.97 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:17:42.636517: step 20950, loss = 0.93 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:17:43.916846: step 20960, loss = 0.78 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:17:45.180333: step 20970, loss = 0.67 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-08 16:17:46.458882: step 20980, loss = 0.76 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:17:47.741791: step 20990, loss = 0.79 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:17:49.111356: step 21000, loss = 0.80 (934.6 examples/sec; 0.137 sec/batch)
2017-05-08 16:17:50.286340: step 21010, loss = 0.84 (1089.4 examples/sec; 0.117 sec/batch)
2017-05-08 16:17:51.552963: step 21020, loss = 0.87 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:17:52.816179: step 21030, loss = 0.91 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-08 16:17:54.098579: step 21040, loss = 0.72 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:17:55.361160: step 21050, loss = 0.84 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-08 16:17:56.664086: step 21060, loss = 0.95 (982.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:17:57.961206: step 21070, loss = 0.85 (986.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:17:59.269417: step 21080, loss = 0.85 (978.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:18:00.546220: step 21090, loss = 0.76 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:18:01.893998: step 21100, loss = 1.01 (949.7 examples/sec; 0.135 sec/batch)
2017-05-08 16:18:03.100886: step 21110, loss = 0.74 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-08 16:18:04.407539: step 21120, loss = 1.00 (979.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:18:05.721615: step 21130, loss = 0.94 (974.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:18:07.016829: step 21140, loss = 0.76 (988.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:18:08.309483: step 21150, loss = 0.68 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:18:09.611072: step 21160, loss = 0.94 (983.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:18:10.900948: step 21170, loss = 0.78 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:18:12.181256: step 21180, loss = 0.76 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:18:13.457569: step 21190, loss = 0.89 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:18:14.850712: step 21200, loss = 0.77 (918.8 examples/sec; 0.139 sec/batch)
2017-05-08 16:18:16.064442: step 21210, loss = 0.93 (1054.6 examples/sec; 0.121 sec/batch)
2017-05-08 16:18:17.343658: step 21220, loss = 0.84 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:18:18.660589: step 21230, loss = 0.76 (972.0 examples/sec; 0.132 sec/batch)
2017-05-08 16:18:19.948349: step 21240, loss = 0.92 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:18:21.266570: step 21250, loss = 0.87 (971.0 examples/sec; 0.132 sec/batch)
2017-05-08 16:18:22.556181: step 21260, loss = 0.67 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:18:23.815345: step 21270, loss = 0.74 (1016.5 examples/sec; 0.126 sec/batch)
2017-05-08 16:18:25.107288: step 21280, loss = 0.77 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:18:26.385332: step 21290, loss = 0.82 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:18:27.770659: step 21300, loss = 1.02 (924.0 examples/sec; 0.139 sec/batch)
2017-05-08 16:18:28.950936: step 21310, loss = 0.74 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-08 16:18:30.215567: step 21320, loss = 1.01 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-08 16:18:31.506555: step 21330, loss = 0.79 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:18:32.778423: step 21340, loss = 0.83 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:18:34.078796: step 21350, loss = 0.80 (984.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:18:35.397794: step 21360, loss = 0.87 (970.4 examples/sec; 0.132 sec/batch)
2017-05-08 16:18:36.683732: step 21370, loss = 0.79 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:18:37.946212: step 21380, loss = 0.84 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-08 16:18:39.210545: step 21390, loss = 0.89 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-08 16:18:40.588312: step 21400, loss = 0.84 (929.0 examples/sec; 0.138 sec/batch)
2017-05-08 16:18:41.779320: step 21410, loss = 1.03 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-08 16:18:43.103326: step 21420, loss = 0.87 (966.8 examples/sec; 0.132 sec/batch)
2017-05-08 16:18:44.409145: step 21430, loss = 1.19 (980.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:18:45.695782: step 21440, loss = 0.78 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:18:46.985405: step 21450, loss = 0.92 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:18:48.264748: step 21460, loss = 0.79 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:18:49.532667: step 21470, loss = 0.73 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:18:50.811216: step 21480, loss = 0.76 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:18:52.076838: step 21490, loss = 0.89 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:18:53.486372: step 21500, loss = 1.02 (908.1 examples/sec; 0.141 sec/batch)
2017-05-08 16:18:54.708892: step 21510, loss = 0.91 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-08 16:18:55.974116: step 21520, loss = 0.88 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:18:57.262753: step 21530, loss = 0.67 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:18:58.537437: step 21540, loss = 0.96 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:18:59.839391: step 21550, loss = 0.70 (983.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:19:01.128246: step 21560, loss = 0.96 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:19:02.402631: step 21570, loss = 0.70 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:19:03.679318: step 21580, loss = 0.87 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:19:04.945875: step 21590, loss = 0.87 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:19:06.337736: step 21600, loss = 0.85 (919.6 examples/sec; 0.139 sec/batch)
2017-05-08 16:19:07.528069: step 21610, loss = 0.86 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-08 16:19:08.835871: step 21620, loss = 0.93 (978.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:19:10.102982: step 21630, loss = 0.97 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:19:11.380004: step 21640, loss = 0.70 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:19:12.652062: step 21650, loss = 1.00 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:19:13.911391: step 21660, loss = 0.96 (1016.4 examples/sec; 0.126 sec/batch)
2017-05-08 16:19:15.201658: step 21670, loss = 0.87 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:19:16.474003: step 21680, loss = 0.85 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:19:17.743512: step 21690, loss = 0.91 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:19:19.106891: step 21700, loss = 0.61 (938.9 examples/sec; 0.136 sec/batch)
2017-05-08 16:19:20.285319: step 21710, loss = 0.83 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-08 16:19:21.558593: step 21720, loss = 0.82 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:19:22.842482: step 21730, loss = 0.87 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:19:24.159774: step 21740, loss = 1.05 (971.7 examples/sec; 0.132 sec/batch)
2017-05-08 16:19:25.445384: step 21750, loss = 0.89 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:19:26.743075: step 21760, loss = 0.92 (986.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:19:28.057948: step 21770, loss = 0.86 (973.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:19:29.344069: step 21780, loss = 0.87 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:19:30.645624: step 21790, loss = 0.91 (983.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:19:32.017641: step 21800, loss = 1.00 (932.9 examples/sec; 0.137 sec/batch)
2017-05-08 16:19:33.212540: step 21810, loss = 0.98 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-08 16:19:34.522374: step 21820, loss = 0.96 (977.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:19:35.815321: step 21830, loss = 0.85 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:19:37.116876: step 21840, loss = 0.81 (983.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:19:38.389694: step 21850, loss = 0.95 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:19:39.644938: step 21860, loss = 0.93 (1019.7 examples/sec; 0.126 sec/batch)
2017-05-08 16:19:40.943359: step 21870, loss = 1.04 (985.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:19:E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 447 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
42.235512: step 21880, loss = 0.91 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:19:43.534706: step 21890, loss = 0.72 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:19:44.946993: step 21900, loss = 0.85 (906.3 examples/sec; 0.141 sec/batch)
2017-05-08 16:19:46.137354: step 21910, loss = 0.89 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-08 16:19:47.411737: step 21920, loss = 0.87 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:19:48.715059: step 21930, loss = 0.85 (982.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:19:50.023536: step 21940, loss = 1.02 (978.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:19:51.295174: step 21950, loss = 0.77 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:19:52.580734: step 21960, loss = 0.88 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:19:53.876354: step 21970, loss = 0.85 (987.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:19:55.141754: step 21980, loss = 1.00 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:19:56.445790: step 21990, loss = 0.86 (981.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:19:57.854832: step 22000, loss = 0.81 (908.4 examples/sec; 0.141 sec/batch)
2017-05-08 16:19:59.036226: step 22010, loss = 0.86 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-08 16:20:00.333016: step 22020, loss = 0.80 (987.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:20:01.625570: step 22030, loss = 0.99 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:20:02.912349: step 22040, loss = 0.90 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:20:04.186112: step 22050, loss = 0.86 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:20:05.460378: step 22060, loss = 0.91 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:20:06.764086: step 22070, loss = 0.68 (981.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:20:08.051301: step 22080, loss = 0.89 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:20:09.321130: step 22090, loss = 0.81 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:20:10.682830: step 22100, loss = 0.77 (940.0 examples/sec; 0.136 sec/batch)
2017-05-08 16:20:11.864847: step 22110, loss = 0.82 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-08 16:20:13.148427: step 22120, loss = 0.89 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:20:14.437331: step 22130, loss = 0.92 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:20:15.736821: step 22140, loss = 0.83 (985.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:20:17.015317: step 22150, loss = 0.87 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:20:18.322856: step 22160, loss = 0.78 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 16:20:19.635936: step 22170, loss = 0.99 (974.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:20:20.923431: step 22180, loss = 0.85 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:20:22.204159: step 22190, loss = 0.85 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:20:23.565990: step 22200, loss = 0.95 (939.9 examples/sec; 0.136 sec/batch)
2017-05-08 16:20:24.765356: step 22210, loss = 0.90 (1067.2 examples/sec; 0.120 sec/batch)
2017-05-08 16:20:26.049514: step 22220, loss = 0.82 (996.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:20:27.345471: step 22230, loss = 0.83 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:20:28.659426: step 22240, loss = 0.66 (974.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:20:29.943747: step 22250, loss = 0.61 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:20:31.221024: step 22260, loss = 0.87 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:20:32.542711: step 22270, loss = 1.18 (968.5 examples/sec; 0.132 sec/batch)
2017-05-08 16:20:33.842054: step 22280, loss = 0.85 (985.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:20:35.123550: step 22290, loss = 0.94 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:20:36.520914: step 22300, loss = 0.82 (916.0 examples/sec; 0.140 sec/batch)
2017-05-08 16:20:37.715553: step 22310, loss = 0.85 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-08 16:20:39.008831: step 22320, loss = 0.76 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:20:40.298066: step 22330, loss = 0.86 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:20:41.584652: step 22340, loss = 0.71 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:20:42.858608: step 22350, loss = 0.73 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:20:44.181913: step 22360, loss = 0.87 (967.3 examples/sec; 0.132 sec/batch)
2017-05-08 16:20:45.474900: step 22370, loss = 0.86 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:20:46.799837: step 22380, loss = 0.97 (966.1 examples/sec; 0.132 sec/batch)
2017-05-08 16:20:48.088814: step 22390, loss = 0.90 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:20:49.476739: step 22400, loss = 0.94 (922.2 examples/sec; 0.139 sec/batch)
2017-05-08 16:20:50.689729: step 22410, loss = 1.02 (1055.2 examples/sec; 0.121 sec/batch)
2017-05-08 16:20:51.970174: step 22420, loss = 0.93 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:20:53.271275: step 22430, loss = 0.83 (983.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:20:54.568662: step 22440, loss = 0.88 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:20:55.878050: step 22450, loss = 0.81 (977.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:20:57.179569: step 22460, loss = 0.79 (983.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:20:58.453808: step 22470, loss = 0.71 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:20:59.734021: step 22480, loss = 0.68 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:21:01.016550: step 22490, loss = 0.85 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:21:02.415811: step 22500, loss = 0.89 (914.8 examples/sec; 0.140 sec/batch)
2017-05-08 16:21:03.607073: step 22510, loss = 0.83 (1074.5 examples/sec; 0.119 sec/batch)
2017-05-08 16:21:04.892070: step 22520, loss = 0.81 (996.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:21:06.172826: step 22530, loss = 0.70 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:21:07.486804: step 22540, loss = 0.81 (974.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:21:08.773518: step 22550, loss = 0.80 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:21:10.067386: step 22560, loss = 0.84 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:21:11.349209: step 22570, loss = 0.89 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:21:12.611936: step 22580, loss = 1.26 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-08 16:21:13.881281: step 22590, loss = 0.88 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:21:15.263200: step 22600, loss = 0.76 (926.3 examples/sec; 0.138 sec/batch)
2017-05-08 16:21:16.433613: step 22610, loss = 0.72 (1093.6 examples/sec; 0.117 sec/batch)
2017-05-08 16:21:17.715289: step 22620, loss = 0.66 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:21:19.018545: step 22630, loss = 0.90 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:21:20.294368: step 22640, loss = 0.82 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:21:21.583918: step 22650, loss = 0.84 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:21:22.865296: step 22660, loss = 0.72 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:21:24.155735: step 22670, loss = 0.80 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:21:25.418258: step 22680, loss = 0.78 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-08 16:21:26.707021: step 22690, loss = 0.89 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:21:28.085879: step 22700, loss = 0.91 (928.3 examples/sec; 0.138 sec/batch)
2017-05-08 16:21:29.313694: step 22710, loss = 0.92 (1042.5 examples/sec; 0.123 sec/batch)
2017-05-08 16:21:30.605487: step 22720, loss = 0.96 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:21:31.896540: step 22730, loss = 0.65 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:21:33.182567: step 22740, loss = 1.00 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:21:34.457322: step 22750, loss = 0.74 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:21:35.756520: step 22760, loss = 0.80 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:21:37.052774: step 22770, loss = 0.81 (987.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:21:38.346145: step 22780, loss = 0.95 (989.7 examples/sec; 0.129 sec/baE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 467 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
tch)
2017-05-08 16:21:39.627294: step 22790, loss = 0.75 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:21:41.014673: step 22800, loss = 0.86 (922.6 examples/sec; 0.139 sec/batch)
2017-05-08 16:21:42.198827: step 22810, loss = 0.81 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-08 16:21:43.508256: step 22820, loss = 0.97 (977.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:21:44.788644: step 22830, loss = 0.75 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:21:46.088874: step 22840, loss = 1.02 (984.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:21:47.393806: step 22850, loss = 0.84 (980.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:21:48.658377: step 22860, loss = 0.71 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 16:21:49.938570: step 22870, loss = 0.81 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:21:51.236112: step 22880, loss = 0.94 (986.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:21:52.532466: step 22890, loss = 0.86 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:21:53.930295: step 22900, loss = 0.85 (915.7 examples/sec; 0.140 sec/batch)
2017-05-08 16:21:55.139621: step 22910, loss = 0.80 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-08 16:21:56.426549: step 22920, loss = 0.86 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:21:57.725657: step 22930, loss = 0.93 (985.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:21:59.020337: step 22940, loss = 0.84 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:22:00.308152: step 22950, loss = 0.86 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:22:01.581018: step 22960, loss = 0.62 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:22:02.873636: step 22970, loss = 0.82 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:22:04.146687: step 22980, loss = 0.67 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:22:05.424764: step 22990, loss = 0.70 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:22:06.807478: step 23000, loss = 0.81 (925.7 examples/sec; 0.138 sec/batch)
2017-05-08 16:22:07.996639: step 23010, loss = 1.16 (1076.4 examples/sec; 0.119 sec/batch)
2017-05-08 16:22:09.301114: step 23020, loss = 0.83 (981.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:22:10.593672: step 23030, loss = 0.83 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:22:11.870055: step 23040, loss = 1.07 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:22:13.179177: step 23050, loss = 1.07 (977.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:22:14.493644: step 23060, loss = 0.85 (973.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:22:15.778889: step 23070, loss = 0.91 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:22:17.078593: step 23080, loss = 0.91 (984.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:22:18.348719: step 23090, loss = 0.83 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:22:19.754162: step 23100, loss = 0.92 (910.7 examples/sec; 0.141 sec/batch)
2017-05-08 16:22:20.953015: step 23110, loss = 0.79 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-08 16:22:22.237866: step 23120, loss = 0.88 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:22:23.515536: step 23130, loss = 0.75 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:22:24.791297: step 23140, loss = 0.83 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:22:26.074131: step 23150, loss = 0.91 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:22:27.366113: step 23160, loss = 0.77 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:22:28.636919: step 23170, loss = 1.07 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:22:29.925542: step 23180, loss = 0.84 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:22:31.195823: step 23190, loss = 0.98 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:22:32.566671: step 23200, loss = 0.71 (933.7 examples/sec; 0.137 sec/batch)
2017-05-08 16:22:33.798151: step 23210, loss = 0.83 (1039.4 examples/sec; 0.123 sec/batch)
2017-05-08 16:22:35.074751: step 23220, loss = 0.78 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:22:36.345699: step 23230, loss = 0.87 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:22:37.651934: step 23240, loss = 1.07 (979.9 examples/sec; 0.131 sec/batch)
2017-05-08 16:22:38.942470: step 23250, loss = 0.83 (991.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:22:40.213626: step 23260, loss = 0.81 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:22:41.495296: step 23270, loss = 0.91 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:22:42.796805: step 23280, loss = 0.86 (983.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:22:44.087829: step 23290, loss = 0.74 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:22:45.480897: step 23300, loss = 1.00 (918.8 examples/sec; 0.139 sec/batch)
2017-05-08 16:22:46.677312: step 23310, loss = 1.12 (1069.9 examples/sec; 0.120 sec/batch)
2017-05-08 16:22:47.963583: step 23320, loss = 0.70 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:22:49.259959: step 23330, loss = 0.76 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:22:50.556425: step 23340, loss = 1.17 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:22:51.863998: step 23350, loss = 0.95 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 16:22:53.141571: step 23360, loss = 0.86 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:22:54.427276: step 23370, loss = 0.96 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:22:55.709893: step 23380, loss = 0.82 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:22:56.980163: step 23390, loss = 0.84 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:22:58.364793: step 23400, loss = 0.76 (924.4 examples/sec; 0.138 sec/batch)
2017-05-08 16:22:59.538069: step 23410, loss = 0.91 (1091.0 examples/sec; 0.117 sec/batch)
2017-05-08 16:23:00.850155: step 23420, loss = 0.91 (975.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:23:02.155641: step 23430, loss = 0.83 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:23:03.466205: step 23440, loss = 0.85 (976.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:23:04.769360: step 23450, loss = 0.82 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:23:06.086292: step 23460, loss = 0.82 (972.0 examples/sec; 0.132 sec/batch)
2017-05-08 16:23:07.386583: step 23470, loss = 0.93 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:23:08.665750: step 23480, loss = 0.75 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:23:09.969286: step 23490, loss = 0.86 (981.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:23:11.350763: step 23500, loss = 0.80 (926.5 examples/sec; 0.138 sec/batch)
2017-05-08 16:23:12.545062: step 23510, loss = 0.78 (1071.8 examples/sec; 0.119 sec/batch)
2017-05-08 16:23:13.870803: step 23520, loss = 1.12 (965.5 examples/sec; 0.133 sec/batch)
2017-05-08 16:23:15.164872: step 23530, loss = 0.84 (989.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:23:16.479292: step 23540, loss = 0.93 (973.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:23:17.786819: step 23550, loss = 0.87 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 16:23:19.087340: step 23560, loss = 0.70 (984.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:23:20.387650: step 23570, loss = 0.87 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:23:21.663169: step 23580, loss = 0.82 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:23:22.943272: step 23590, loss = 0.89 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:23:24.285482: step 23600, loss = 0.70 (953.7 examples/sec; 0.134 sec/batch)
2017-05-08 16:23:25.504958: step 23610, loss = 0.91 (1049.6 examples/sec; 0.122 sec/batch)
2017-05-08 16:23:26.814354: step 23620, loss = 0.89 (977.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:23:28.110272: step 23630, loss = 1.00 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:23:29.421247: step 23640, loss = 0.84 (976.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:23:30.724147: step 23650, loss = 0.74 (982.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:23:31.989664: step 23660, loss = 0.88 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:23:33.263867: step 23670, loss = 0.70 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:23:34.564202: step 23680, loss = 0.73 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:23:35.855169: step 23690, loss = 0.74 (991.5 E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 488 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
examples/sec; 0.129 sec/batch)
2017-05-08 16:23:37.254085: step 23700, loss = 1.07 (915.0 examples/sec; 0.140 sec/batch)
2017-05-08 16:23:38.434515: step 23710, loss = 0.89 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-08 16:23:39.718657: step 23720, loss = 0.89 (996.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:23:41.016528: step 23730, loss = 0.79 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:23:42.306437: step 23740, loss = 0.89 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:23:43.567047: step 23750, loss = 0.81 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-08 16:23:44.857334: step 23760, loss = 1.01 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:23:46.140340: step 23770, loss = 0.66 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:23:47.412764: step 23780, loss = 1.00 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:23:48.687074: step 23790, loss = 0.92 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:23:50.067342: step 23800, loss = 0.88 (927.4 examples/sec; 0.138 sec/batch)
2017-05-08 16:23:51.254220: step 23810, loss = 0.96 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-08 16:23:52.556531: step 23820, loss = 0.82 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:23:53.847410: step 23830, loss = 0.78 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:23:55.129214: step 23840, loss = 1.07 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:23:56.377958: step 23850, loss = 0.76 (1025.0 examples/sec; 0.125 sec/batch)
2017-05-08 16:23:57.656976: step 23860, loss = 0.70 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:23:58.930911: step 23870, loss = 0.73 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:24:00.208562: step 23880, loss = 0.96 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:24:01.501429: step 23890, loss = 0.70 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:24:02.877583: step 23900, loss = 0.89 (930.1 examples/sec; 0.138 sec/batch)
2017-05-08 16:24:04.074195: step 23910, loss = 0.94 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-08 16:24:05.363170: step 23920, loss = 0.83 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:24:06.639567: step 23930, loss = 1.07 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:24:07.915277: step 23940, loss = 0.80 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:24:09.205566: step 23950, loss = 0.79 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:24:10.522744: step 23960, loss = 0.85 (971.8 examples/sec; 0.132 sec/batch)
2017-05-08 16:24:11.836996: step 23970, loss = 1.17 (973.9 examples/sec; 0.131 sec/batch)
2017-05-08 16:24:13.153503: step 23980, loss = 0.85 (972.3 examples/sec; 0.132 sec/batch)
2017-05-08 16:24:14.446236: step 23990, loss = 0.93 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:24:15.823586: step 24000, loss = 0.79 (929.3 examples/sec; 0.138 sec/batch)
2017-05-08 16:24:17.007750: step 24010, loss = 0.98 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-08 16:24:18.279249: step 24020, loss = 1.02 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:24:19.571625: step 24030, loss = 0.94 (990.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:24:20.832837: step 24040, loss = 0.86 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-08 16:24:22.115574: step 24050, loss = 0.97 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:24:23.394515: step 24060, loss = 0.75 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:24:24.664598: step 24070, loss = 0.93 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:24:25.954925: step 24080, loss = 0.86 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:24:27.243191: step 24090, loss = 0.81 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:24:28.607269: step 24100, loss = 0.89 (938.4 examples/sec; 0.136 sec/batch)
2017-05-08 16:24:29.805695: step 24110, loss = 0.74 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-08 16:24:31.098176: step 24120, loss = 0.80 (990.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:24:32.401946: step 24130, loss = 0.66 (981.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:24:33.684500: step 24140, loss = 0.90 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:24:34.948201: step 24150, loss = 0.78 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-08 16:24:36.234342: step 24160, loss = 0.75 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:24:37.510090: step 24170, loss = 0.91 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:24:38.822928: step 24180, loss = 0.86 (975.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:24:40.096443: step 24190, loss = 1.00 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:24:41.504877: step 24200, loss = 0.96 (908.8 examples/sec; 0.141 sec/batch)
2017-05-08 16:24:42.707228: step 24210, loss = 0.78 (1064.6 examples/sec; 0.120 sec/batch)
2017-05-08 16:24:43.985935: step 24220, loss = 0.99 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:24:45.295125: step 24230, loss = 0.81 (977.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:24:46.606116: step 24240, loss = 0.76 (976.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:24:47.874724: step 24250, loss = 0.67 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:24:49.148841: step 24260, loss = 0.74 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:24:50.443547: step 24270, loss = 0.78 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:24:51.715334: step 24280, loss = 0.79 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:24:53.030757: step 24290, loss = 0.95 (973.1 examples/sec; 0.132 sec/batch)
2017-05-08 16:24:54.399503: step 24300, loss = 1.03 (935.2 examples/sec; 0.137 sec/batch)
2017-05-08 16:24:55.605297: step 24310, loss = 0.77 (1061.5 examples/sec; 0.121 sec/batch)
2017-05-08 16:24:56.913218: step 24320, loss = 1.00 (978.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:24:58.198540: step 24330, loss = 1.08 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:24:59.495178: step 24340, loss = 0.85 (987.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:25:00.794522: step 24350, loss = 0.74 (985.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:25:02.093802: step 24360, loss = 0.88 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:25:03.382048: step 24370, loss = 0.68 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:25:04.666752: step 24380, loss = 0.82 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:25:05.962782: step 24390, loss = 0.87 (987.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:25:07.336617: step 24400, loss = 0.90 (931.7 examples/sec; 0.137 sec/batch)
2017-05-08 16:25:08.502922: step 24410, loss = 0.90 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-08 16:25:09.777048: step 24420, loss = 0.85 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:25:11.054692: step 24430, loss = 0.86 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:25:12.318695: step 24440, loss = 0.87 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-08 16:25:13.610308: step 24450, loss = 1.09 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:25:14.877811: step 24460, loss = 0.86 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:25:16.173012: step 24470, loss = 0.79 (988.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:25:17.458294: step 24480, loss = 0.74 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:25:18.746297: step 24490, loss = 0.84 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:25:20.098237: step 24500, loss = 0.73 (946.8 examples/sec; 0.135 sec/batch)
2017-05-08 16:25:21.284717: step 24510, loss = 0.90 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-08 16:25:22.554198: step 24520, loss = 0.73 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:25:23.841345: step 24530, loss = 0.87 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:25:25.116341: step 24540, loss = 0.77 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:25:26.406251: step 24550, loss = 0.90 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:25:27.686937: step 24560, loss = 0.83 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:25:28.971600: step 24570, loss = 0.77 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:25:30.282856: step 24580, loss = 0.79 (976.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:25:31.565818: step 24590, loss = 0.92 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:25:32.961913E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 508 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
: step 24600, loss = 0.95 (916.8 examples/sec; 0.140 sec/batch)
2017-05-08 16:25:34.190325: step 24610, loss = 0.97 (1042.0 examples/sec; 0.123 sec/batch)
2017-05-08 16:25:35.471339: step 24620, loss = 0.73 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:25:36.771224: step 24630, loss = 0.99 (984.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:25:38.081298: step 24640, loss = 0.91 (977.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:25:39.378625: step 24650, loss = 0.72 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:25:40.676578: step 24660, loss = 0.78 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:25:41.958892: step 24670, loss = 0.75 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:25:43.248301: step 24680, loss = 0.97 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:25:44.541285: step 24690, loss = 0.83 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:25:45.921218: step 24700, loss = 0.82 (927.6 examples/sec; 0.138 sec/batch)
2017-05-08 16:25:47.102526: step 24710, loss = 0.76 (1083.5 examples/sec; 0.118 sec/batch)
2017-05-08 16:25:48.368432: step 24720, loss = 0.72 (1011.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:25:49.644996: step 24730, loss = 1.05 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:25:50.939929: step 24740, loss = 0.93 (988.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:25:52.206146: step 24750, loss = 0.89 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:25:53.485287: step 24760, loss = 0.92 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:25:54.742606: step 24770, loss = 0.80 (1018.0 examples/sec; 0.126 sec/batch)
2017-05-08 16:25:56.024213: step 24780, loss = 0.60 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:25:57.310269: step 24790, loss = 1.11 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:25:58.704278: step 24800, loss = 0.83 (918.2 examples/sec; 0.139 sec/batch)
2017-05-08 16:25:59.890606: step 24810, loss = 1.13 (1079.0 examples/sec; 0.119 sec/batch)
2017-05-08 16:26:01.187898: step 24820, loss = 0.92 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:26:02.456016: step 24830, loss = 0.80 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:26:03.740901: step 24840, loss = 0.88 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:26:05.019901: step 24850, loss = 0.87 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:26:06.318436: step 24860, loss = 0.80 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:26:07.617843: step 24870, loss = 0.92 (985.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:26:08.922795: step 24880, loss = 0.93 (980.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:26:10.196890: step 24890, loss = 0.84 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:26:11.579484: step 24900, loss = 0.73 (925.8 examples/sec; 0.138 sec/batch)
2017-05-08 16:26:12.778540: step 24910, loss = 0.90 (1067.5 examples/sec; 0.120 sec/batch)
2017-05-08 16:26:14.047640: step 24920, loss = 0.72 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:26:15.345498: step 24930, loss = 0.91 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:26:16.636267: step 24940, loss = 0.94 (991.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:26:17.927757: step 24950, loss = 1.05 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:26:19.242685: step 24960, loss = 0.84 (973.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:26:20.545839: step 24970, loss = 0.77 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:26:21.826193: step 24980, loss = 1.00 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:26:23.158286: step 24990, loss = 0.92 (960.9 examples/sec; 0.133 sec/batch)
2017-05-08 16:26:24.538881: step 25000, loss = 0.88 (927.1 examples/sec; 0.138 sec/batch)
2017-05-08 16:26:25.705804: step 25010, loss = 0.87 (1096.9 examples/sec; 0.117 sec/batch)
2017-05-08 16:26:26.963932: step 25020, loss = 0.65 (1017.4 examples/sec; 0.126 sec/batch)
2017-05-08 16:26:28.240874: step 25030, loss = 0.79 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:26:29.517426: step 25040, loss = 0.78 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:26:30.806283: step 25050, loss = 0.91 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:26:32.086804: step 25060, loss = 1.03 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:26:33.385439: step 25070, loss = 0.74 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:26:34.647407: step 25080, loss = 0.62 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-08 16:26:35.936843: step 25090, loss = 0.92 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:26:37.303904: step 25100, loss = 0.85 (936.3 examples/sec; 0.137 sec/batch)
2017-05-08 16:26:38.498500: step 25110, loss = 0.81 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-08 16:26:39.773075: step 25120, loss = 0.87 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:26:41.042951: step 25130, loss = 0.83 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:26:42.342205: step 25140, loss = 0.85 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:26:43.635079: step 25150, loss = 0.83 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:26:44.919095: step 25160, loss = 0.85 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:26:46.199601: step 25170, loss = 0.78 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:26:47.495078: step 25180, loss = 0.75 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:26:48.782684: step 25190, loss = 0.84 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:26:50.163610: step 25200, loss = 0.87 (926.9 examples/sec; 0.138 sec/batch)
2017-05-08 16:26:51.372579: step 25210, loss = 0.81 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-08 16:26:52.651481: step 25220, loss = 0.87 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:26:53.923383: step 25230, loss = 0.82 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:26:55.213114: step 25240, loss = 0.91 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:26:56.469583: step 25250, loss = 0.83 (1018.7 examples/sec; 0.126 sec/batch)
2017-05-08 16:26:57.764507: step 25260, loss = 0.96 (988.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:26:59.024037: step 25270, loss = 0.90 (1016.2 examples/sec; 0.126 sec/batch)
2017-05-08 16:27:00.311919: step 25280, loss = 0.87 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:27:01.586979: step 25290, loss = 0.72 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:27:02.943045: step 25300, loss = 0.84 (943.9 examples/sec; 0.136 sec/batch)
2017-05-08 16:27:04.132721: step 25310, loss = 0.86 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-08 16:27:05.422407: step 25320, loss = 0.80 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:27:06.723793: step 25330, loss = 0.87 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:27:08.005538: step 25340, loss = 0.73 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:27:09.290859: step 25350, loss = 0.68 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:27:10.571767: step 25360, loss = 0.84 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:27:11.848933: step 25370, loss = 0.81 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:27:13.134807: step 25380, loss = 0.81 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:27:14.424510: step 25390, loss = 0.79 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:27:15.792879: step 25400, loss = 0.64 (935.4 examples/sec; 0.137 sec/batch)
2017-05-08 16:27:16.965110: step 25410, loss = 0.86 (1091.9 examples/sec; 0.117 sec/batch)
2017-05-08 16:27:18.244765: step 25420, loss = 0.79 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:27:19.531853: step 25430, loss = 0.71 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:27:20.820248: step 25440, loss = 0.80 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:27:22.103105: step 25450, loss = 0.75 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:27:23.409602: step 25460, loss = 0.84 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:27:24.679508: step 25470, loss = 0.73 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:27:25.962940: step 25480, loss = 0.82 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:27:27.259542: step 25490, loss = 0.93 (987.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:27:28.654606: step 25500, loss = 1.09 (917.5 examples/sec; 0.140 sec/batE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 528 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
ch)
2017-05-08 16:27:29.864699: step 25510, loss = 0.97 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-08 16:27:31.160605: step 25520, loss = 0.60 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:27:32.438506: step 25530, loss = 0.73 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:27:33.719240: step 25540, loss = 0.84 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:27:35.015106: step 25550, loss = 0.75 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:27:36.309190: step 25560, loss = 0.94 (989.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:27:37.585242: step 25570, loss = 0.93 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:27:38.864874: step 25580, loss = 0.79 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:27:40.143622: step 25590, loss = 0.86 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:27:41.520138: step 25600, loss = 0.91 (929.9 examples/sec; 0.138 sec/batch)
2017-05-08 16:27:42.701633: step 25610, loss = 0.87 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-08 16:27:43.971693: step 25620, loss = 0.95 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:27:45.251125: step 25630, loss = 0.88 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:27:46.537126: step 25640, loss = 0.76 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:27:47.812623: step 25650, loss = 0.84 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:27:49.125979: step 25660, loss = 0.79 (974.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:27:50.431289: step 25670, loss = 0.88 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:27:51.707494: step 25680, loss = 0.91 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:27:53.012753: step 25690, loss = 1.01 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:27:54.363132: step 25700, loss = 0.93 (947.9 examples/sec; 0.135 sec/batch)
2017-05-08 16:27:55.542818: step 25710, loss = 0.77 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-08 16:27:56.846287: step 25720, loss = 0.79 (982.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:27:58.120506: step 25730, loss = 0.77 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:27:59.394846: step 25740, loss = 0.88 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:28:00.672585: step 25750, loss = 0.66 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:28:01.985970: step 25760, loss = 0.68 (974.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:28:03.285743: step 25770, loss = 0.87 (984.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:28:04.562607: step 25780, loss = 0.84 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:28:05.817067: step 25790, loss = 0.93 (1020.3 examples/sec; 0.125 sec/batch)
2017-05-08 16:28:07.194580: step 25800, loss = 0.86 (929.2 examples/sec; 0.138 sec/batch)
2017-05-08 16:28:08.397277: step 25810, loss = 0.77 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-08 16:28:09.713938: step 25820, loss = 1.33 (972.2 examples/sec; 0.132 sec/batch)
2017-05-08 16:28:11.000759: step 25830, loss = 0.81 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:28:12.283920: step 25840, loss = 0.80 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:28:13.576726: step 25850, loss = 0.77 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:28:14.853320: step 25860, loss = 0.81 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:28:16.106682: step 25870, loss = 0.72 (1021.3 examples/sec; 0.125 sec/batch)
2017-05-08 16:28:17.391612: step 25880, loss = 0.70 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:28:18.686425: step 25890, loss = 0.89 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:28:20.055524: step 25900, loss = 0.81 (934.9 examples/sec; 0.137 sec/batch)
2017-05-08 16:28:21.261006: step 25910, loss = 0.96 (1061.8 examples/sec; 0.121 sec/batch)
2017-05-08 16:28:22.544564: step 25920, loss = 0.96 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:28:23.822329: step 25930, loss = 0.89 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:28:25.111417: step 25940, loss = 0.90 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:28:26.399188: step 25950, loss = 0.70 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:28:27.673434: step 25960, loss = 0.84 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:28:28.965836: step 25970, loss = 0.71 (990.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:28:30.237906: step 25980, loss = 0.91 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:28:31.513156: step 25990, loss = 1.01 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:28:32.876044: step 26000, loss = 0.88 (939.2 examples/sec; 0.136 sec/batch)
2017-05-08 16:28:34.059980: step 26010, loss = 0.78 (1081.1 examples/sec; 0.118 sec/batch)
2017-05-08 16:28:35.328402: step 26020, loss = 0.88 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:28:36.614732: step 26030, loss = 0.89 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:28:37.913419: step 26040, loss = 0.91 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:28:39.208700: step 26050, loss = 0.72 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:28:40.494260: step 26060, loss = 0.80 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:28:41.769236: step 26070, loss = 0.71 (1003.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:28:43.060887: step 26080, loss = 0.91 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:28:44.378387: step 26090, loss = 0.81 (971.5 examples/sec; 0.132 sec/batch)
2017-05-08 16:28:45.760938: step 26100, loss = 1.08 (925.8 examples/sec; 0.138 sec/batch)
2017-05-08 16:28:46.951250: step 26110, loss = 0.89 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-08 16:28:48.251276: step 26120, loss = 0.79 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:28:49.523616: step 26130, loss = 0.84 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:28:50.818187: step 26140, loss = 0.84 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:28:52.088704: step 26150, loss = 0.75 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:28:53.388145: step 26160, loss = 0.78 (985.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:28:54.699594: step 26170, loss = 0.89 (976.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:28:55.991346: step 26180, loss = 0.95 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:28:57.302114: step 26190, loss = 1.18 (976.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:28:58.702476: step 26200, loss = 1.06 (914.1 examples/sec; 0.140 sec/batch)
2017-05-08 16:28:59.909291: step 26210, loss = 0.84 (1060.6 examples/sec; 0.121 sec/batch)
2017-05-08 16:29:01.197928: step 26220, loss = 0.79 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:29:02.468047: step 26230, loss = 0.71 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:29:03.728941: step 26240, loss = 1.01 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-08 16:29:05.010635: step 26250, loss = 0.60 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:29:06.295893: step 26260, loss = 0.92 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:29:07.580343: step 26270, loss = 0.75 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:29:08.861631: step 26280, loss = 1.06 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:29:10.133868: step 26290, loss = 0.80 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:29:11.506952: step 26300, loss = 0.88 (932.2 examples/sec; 0.137 sec/batch)
2017-05-08 16:29:12.687161: step 26310, loss = 0.75 (1084.6 examples/sec; 0.118 sec/batch)
2017-05-08 16:29:13.954728: step 26320, loss = 0.85 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:29:15.256624: step 26330, loss = 1.14 (983.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:29:16.528646: step 26340, loss = 0.75 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:29:17.809931: step 26350, loss = 0.95 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:29:19.094609: step 26360, loss = 0.98 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:29:20.392255: step 26370, loss = 1.02 (986.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:29:21.700864: step 26380, loss = 0.73 (978.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:29:23.031763: step 26390, loss = 0.85 (961.8 examples/sec; 0.133 sec/batch)
2017-05-08 16:29:24.393538: step 26400, loss = 1.05 (939.9 examples/sec; 0.136 sec/batch)
2017-05-08 16:29:25.601759: step 26410, loss = 0.74 (1059.4 examples/sec; 0.121 sec/batch)
2017-05-08 16:29:26.881695: step 26420, loss = 0.88 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:29:28.176418: step 26430, loss = 0.85 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:29:29.487512: step 26440, loss = 1.11 (976.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:29:30.779317: step 26450, loss = 0.90 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:29:32.089706: step 26460, loss = 0.83 (976.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:29:33.388663: step 26470, loss = 0.81 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:29:34.669515: step 26480, loss = 0.91 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:29:35.950627: step 26490, loss = 0.76 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:29:37.328009: step 26500, loss = 0.76 (929.3 examples/sec; 0.138 sec/batch)
2017-05-08 16:29:38.535522: step 26510, loss = 0.86 (1060.0 examples/sec; 0.121 sec/batch)
2017-05-08 16:29:39.854141: step 26520, loss = 0.95 (970.7 examples/sec; 0.132 sec/batch)
2017-05-08 16:29:41.140050: step 26530, loss = 0.95 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:29:42.441259: step 26540, loss = 0.96 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:29:43.744693: step 26550, loss = 0.91 (982.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:29:45.044671: step 26560, loss = 0.73 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:29:46.349902: step 26570, loss = 0.72 (980.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:29:47.612658: step 26580, loss = 0.86 (1013.7 examples/sec; 0.126 sec/batch)
2017-05-08 16:29:48.909548: step 26590, loss = 0.92 (987.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:29:50.273367: step 26600, loss = 0.90 (938.5 examples/sec; 0.136 sec/batch)
2017-05-08 16:29:51.470084: step 26610, loss = 0.82 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-08 16:29:52.771763: step 26620, loss = 0.81 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:29:54.063389: step 26630, loss = 1.30 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:29:55.339109: step 26640, loss = 0.93 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:29:56.658937: step 26650, loss = 0.98 (969.8 examples/sec; 0.132 sec/batch)
2017-05-08 16:29:57.958614: step 26660, loss = 0.86 (984.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:29:59.254944: step 26670, loss = 0.87 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:30:00.562968: step 26680, loss = 1.01 (978.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:30:01.859067: step 26690, loss = 0.85 (987.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:30:03.289259: step 26700, loss = 0.76 (895.0 examples/sec; 0.143 sec/batch)
2017-05-08 16:30:04.484095: step 26710, loss = 0.71 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-08 16:30:05.741870: step 26720, loss = 0.80 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-08 16:30:07.072378: step 26730, loss = 0.81 (962.0 examples/sec; 0.133 sec/batch)
2017-05-08 16:30:08.361872: step 26740, loss = 0.93 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:30:09.660825: step 26750, loss = 0.84 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:30:10.954680: step 26760, loss = 0.82 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:30:12.235043: step 26770, loss = 0.96 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:30:13.521126: step 26780, loss = 0.74 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:30:14.835592: step 26790, loss = 0.96 (973.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:30:16.203320: step 26800, loss = 1.04 (935.9 examples/sec; 0.137 sec/batch)
2017-05-08 16:30:17.407207: step 26810, loss = 0.81 (1063.2 examples/sec; 0.120 sec/batch)
2017-05-08 16:30:18.731666: step 26820, loss = 0.79 (966.4 examples/sec; 0.132 sec/batch)
2017-05-08 16:30:20.015194: step 26830, loss = 0.83 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:30:21.322094: step 26840, loss = 0.81 (979.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:30:22.606686: step 26850, loss = 0.78 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:30:23.867120: step 26860, loss = 0.67 (1015.5 examples/sec; 0.126 sec/batcE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 548 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
h)
2017-05-08 16:30:25.147058: step 26870, loss = 0.85 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:30:26.427774: step 26880, loss = 0.68 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:30:27.716130: step 26890, loss = 0.71 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:30:29.071228: step 26900, loss = 0.86 (944.6 examples/sec; 0.136 sec/batch)
2017-05-08 16:30:30.262371: step 26910, loss = 0.86 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-08 16:30:31.546148: step 26920, loss = 0.85 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:30:32.820374: step 26930, loss = 0.72 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:30:34.089228: step 26940, loss = 1.03 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:30:35.387678: step 26950, loss = 0.83 (985.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:30:36.671111: step 26960, loss = 0.79 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:30:37.956819: step 26970, loss = 0.88 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:30:39.225704: step 26980, loss = 0.99 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:30:40.536739: step 26990, loss = 0.93 (976.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:30:41.910648: step 27000, loss = 0.89 (931.6 examples/sec; 0.137 sec/batch)
2017-05-08 16:30:43.139320: step 27010, loss = 0.81 (1041.8 examples/sec; 0.123 sec/batch)
2017-05-08 16:30:44.454385: step 27020, loss = 0.99 (973.3 examples/sec; 0.132 sec/batch)
2017-05-08 16:30:45.713302: step 27030, loss = 0.77 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-08 16:30:47.011202: step 27040, loss = 1.00 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:30:48.292374: step 27050, loss = 0.78 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:30:49.558013: step 27060, loss = 0.87 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:30:50.837929: step 27070, loss = 0.91 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:30:52.153039: step 27080, loss = 0.79 (973.3 examples/sec; 0.132 sec/batch)
2017-05-08 16:30:53.433034: step 27090, loss = 0.86 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:30:54.799917: step 27100, loss = 0.78 (936.4 examples/sec; 0.137 sec/batch)
2017-05-08 16:30:55.979670: step 27110, loss = 0.85 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-08 16:30:57.246056: step 27120, loss = 0.92 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:30:58.528242: step 27130, loss = 0.69 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:30:59.810283: step 27140, loss = 0.76 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:31:01.089321: step 27150, loss = 0.75 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:31:02.372934: step 27160, loss = 0.70 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:31:03.677668: step 27170, loss = 0.88 (981.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:31:04.995425: step 27180, loss = 1.03 (971.3 examples/sec; 0.132 sec/batch)
2017-05-08 16:31:06.302076: step 27190, loss = 0.87 (979.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:31:07.672905: step 27200, loss = 0.76 (933.7 examples/sec; 0.137 sec/batch)
2017-05-08 16:31:08.848553: step 27210, loss = 0.78 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-08 16:31:10.159576: step 27220, loss = 0.73 (976.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:31:11.420181: step 27230, loss = 0.82 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-08 16:31:12.719032: step 27240, loss = 0.82 (985.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:31:14.013046: step 27250, loss = 0.73 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:31:15.326201: step 27260, loss = 0.71 (974.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:31:16.626778: step 27270, loss = 0.90 (984.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:31:17.902324: step 27280, loss = 0.87 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:31:19.225862: step 27290, loss = 0.76 (967.1 examples/sec; 0.132 sec/batch)
2017-05-08 16:31:20.619759: step 27300, loss = 0.86 (918.3 examples/sec; 0.139 sec/batch)
2017-05-08 16:31:21.843817: step 27310, loss = 0.65 (1045.7 examples/sec; 0.122 sec/batch)
2017-05-08 16:31:23.137191: step 27320, loss = 0.77 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:31:24.404513: step 27330, loss = 0.90 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:31:25.711586: step 27340, loss = 0.69 (979.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:31:27.009841: step 27350, loss = 0.76 (985.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:31:28.305583: step 27360, loss = 0.72 (987.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:31:29.599644: step 27370, loss = 0.81 (989.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:31:30.875265: step 27380, loss = 0.82 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:31:32.165711: step 27390, loss = 0.76 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:31:33.537790: step 27400, loss = 0.75 (932.9 examples/sec; 0.137 sec/batch)
2017-05-08 16:31:34.740915: step 27410, loss = 0.70 (1063.9 examples/sec; 0.120 sec/batch)
2017-05-08 16:31:36.023986: step 27420, loss = 0.83 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:31:37.316240: step 27430, loss = 0.62 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:31:38.606594: step 27440, loss = 0.83 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:31:39.879662: step 27450, loss = 0.89 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:31:41.146887: step 27460, loss = 0.87 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:31:42.435581: step 27470, loss = 0.85 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:31:43.711294: step 27480, loss = 0.89 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:31:44.985410: step 27490, loss = 0.76 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:31:46.363201: step 27500, loss = 0.92 (929.0 examples/sec; 0.138 sec/batch)
2017-05-08 16:31:47.539876: step 27510, loss = 0.91 (1087.8 examples/sec; 0.118 sec/batch)
2017-05-08 16:31:48.850894: step 27520, loss = 0.81 (976.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:31:50.159858: step 27530, loss = 0.91 (977.9 examples/sec; 0.131 sec/batch)
2017-05-08 16:31:51.451724: step 27540, loss = 0.74 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:31:52.728239: step 27550, loss = 0.78 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:31:54.010593: step 27560, loss = 0.86 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:31:55.297966: step 27570, loss = 0.77 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:31:56.605290: step 27580, loss = 0.90 (979.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:31:57.906601: step 27590, loss = 0.82 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:31:59.308996: step 27600, loss = 0.83 (912.7 examples/sec; 0.140 sec/batch)
2017-05-08 16:32:00.502634: step 27610, loss = 0.88 (1072.4 examples/sec; 0.119 sec/batch)
2017-05-08 16:32:01.804001: step 27620, loss = 0.86 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:32:03.089785: step 27630, loss = 0.83 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:32:04.366963: step 27640, loss = 0.90 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:32:05.647682: step 27650, loss = 0.89 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:32:06.919335: step 27660, loss = 0.66 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:32:08.196537: step 27670, loss = 0.82 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:32:09.465221: step 27680, loss = 0.74 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:32:10.755297: step 27690, loss = 0.93 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:32:12.097008: step 27700, loss = 0.91 (954.0 examples/sec; 0.134 sec/batch)
2017-05-08 16:32:13.293961: step 27710, loss = 1.04 (1069.4 examples/sec; 0.120 sec/batch)
2017-05-08 16:32:14.581813: step 27720, loss = 0.82 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:32:15.863232: step 27730, loss = 0.83 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:32:17.158773: step 27740, loss = 0.74 (988.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:32:18.436988: step 27750, loss = 0.81 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:32:19.713401: step 27760, loss = 0.93 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:32:21.007294: step 27770, loss = 0.86 (98E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 568 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
9.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:32:22.300926: step 27780, loss = 0.95 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:32:23.585306: step 27790, loss = 0.74 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:32:24.974414: step 27800, loss = 0.87 (921.5 examples/sec; 0.139 sec/batch)
2017-05-08 16:32:26.170375: step 27810, loss = 0.99 (1070.3 examples/sec; 0.120 sec/batch)
2017-05-08 16:32:27.465678: step 27820, loss = 0.88 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:32:28.760880: step 27830, loss = 0.97 (988.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:32:30.033148: step 27840, loss = 0.78 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:32:31.303637: step 27850, loss = 0.84 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:32:32.573663: step 27860, loss = 0.70 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:32:33.867278: step 27870, loss = 0.80 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:32:35.136326: step 27880, loss = 0.78 (1008.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:32:36.440195: step 27890, loss = 0.86 (981.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:32:37.816740: step 27900, loss = 0.91 (929.9 examples/sec; 0.138 sec/batch)
2017-05-08 16:32:39.019173: step 27910, loss = 0.61 (1064.5 examples/sec; 0.120 sec/batch)
2017-05-08 16:32:40.357075: step 27920, loss = 0.65 (956.7 examples/sec; 0.134 sec/batch)
2017-05-08 16:32:41.647634: step 27930, loss = 0.85 (991.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:32:42.937304: step 27940, loss = 0.70 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:32:44.224792: step 27950, loss = 0.94 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:32:45.519311: step 27960, loss = 0.91 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:32:46.808303: step 27970, loss = 0.96 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:32:48.100317: step 27980, loss = 0.66 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:32:49.410136: step 27990, loss = 0.80 (977.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:32:50.812184: step 28000, loss = 0.75 (912.9 examples/sec; 0.140 sec/batch)
2017-05-08 16:32:52.016623: step 28010, loss = 0.94 (1062.7 examples/sec; 0.120 sec/batch)
2017-05-08 16:32:53.326861: step 28020, loss = 0.91 (976.9 examples/sec; 0.131 sec/batch)
2017-05-08 16:32:54.629576: step 28030, loss = 0.89 (982.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:32:55.938748: step 28040, loss = 0.96 (977.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:32:57.248104: step 28050, loss = 0.73 (977.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:32:58.536970: step 28060, loss = 0.97 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:32:59.847561: step 28070, loss = 0.77 (976.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:33:01.149477: step 28080, loss = 0.78 (983.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:33:02.453583: step 28090, loss = 0.97 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:33:03.844378: step 28100, loss = 0.72 (920.3 examples/sec; 0.139 sec/batch)
2017-05-08 16:33:05.018572: step 28110, loss = 0.92 (1090.1 examples/sec; 0.117 sec/batch)
2017-05-08 16:33:06.316632: step 28120, loss = 0.79 (986.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:33:07.612741: step 28130, loss = 0.83 (987.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:33:08.911800: step 28140, loss = 0.94 (985.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:33:10.186064: step 28150, loss = 0.74 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:33:11.507497: step 28160, loss = 0.75 (968.6 examples/sec; 0.132 sec/batch)
2017-05-08 16:33:12.816015: step 28170, loss = 1.10 (978.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:33:14.079175: step 28180, loss = 0.83 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-08 16:33:15.352350: step 28190, loss = 0.88 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:33:16.734745: step 28200, loss = 0.67 (925.9 examples/sec; 0.138 sec/batch)
2017-05-08 16:33:17.925323: step 28210, loss = 0.95 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-08 16:33:19.236414: step 28220, loss = 1.02 (976.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:33:20.529841: step 28230, loss = 0.92 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:33:21.825611: step 28240, loss = 0.78 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:33:23.151987: step 28250, loss = 0.86 (965.0 examples/sec; 0.133 sec/batch)
2017-05-08 16:33:24.460829: step 28260, loss = 0.84 (978.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:33:25.765127: step 28270, loss = 0.80 (981.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:33:27.073241: step 28280, loss = 0.96 (978.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:33:28.355959: step 28290, loss = 0.81 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:33:29.767794: step 28300, loss = 0.79 (906.6 examples/sec; 0.141 sec/batch)
2017-05-08 16:33:30.950531: step 28310, loss = 0.91 (1082.2 examples/sec; 0.118 sec/batch)
2017-05-08 16:33:32.217535: step 28320, loss = 0.81 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:33:33.494039: step 28330, loss = 0.76 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:33:34.800499: step 28340, loss = 0.70 (979.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:33:36.121183: step 28350, loss = 0.80 (969.2 examples/sec; 0.132 sec/batch)
2017-05-08 16:33:37.410811: step 28360, loss = 0.62 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:33:38.711935: step 28370, loss = 0.97 (983.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:33:39.988481: step 28380, loss = 0.81 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:33:41.265905: step 28390, loss = 0.70 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:33:42.657432: step 28400, loss = 0.78 (919.8 examples/sec; 0.139 sec/batch)
2017-05-08 16:33:43.833675: step 28410, loss = 0.87 (1088.2 examples/sec; 0.118 sec/batch)
2017-05-08 16:33:45.118395: step 28420, loss = 0.91 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:33:46.397730: step 28430, loss = 0.84 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:33:47.658541: step 28440, loss = 0.87 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-08 16:33:48.947293: step 28450, loss = 0.75 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:33:50.229736: step 28460, loss = 0.72 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:33:51.512519: step 28470, loss = 0.68 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:33:52.777573: step 28480, loss = 0.83 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:33:54.060689: step 28490, loss = 0.69 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:33:55.428394: step 28500, loss = 0.82 (935.9 examples/sec; 0.137 sec/batch)
2017-05-08 16:33:56.604861: step 28510, loss = 0.82 (1088.0 examples/sec; 0.118 sec/batch)
2017-05-08 16:33:57.899877: step 28520, loss = 0.71 (988.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:33:59.171297: step 28530, loss = 1.06 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:34:00.441250: step 28540, loss = 0.94 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:34:01.741266: step 28550, loss = 0.69 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:34:03.017556: step 28560, loss = 0.75 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:34:04.294881: step 28570, loss = 0.76 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:34:05.597347: step 28580, loss = 1.08 (982.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:34:06.903166: step 28590, loss = 0.87 (980.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:34:08.307114: step 28600, loss = 0.79 (911.7 examples/sec; 0.140 sec/batch)
2017-05-08 16:34:09.510019: step 28610, loss = 0.92 (1064.1 examples/sec; 0.120 sec/batch)
2017-05-08 16:34:10.791599: step 28620, loss = 0.95 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:34:12.051314: step 28630, loss = 0.64 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-08 16:34:13.341660: step 28640, loss = 0.80 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:34:14.628060: step 28650, loss = 0.84 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:34:15.900258: step 28660, loss = 0.78 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:34:17.185356: step 28670, loss = 0.82 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:34:18.461927: E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 587 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
step 28680, loss = 0.87 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:34:19.768755: step 28690, loss = 0.72 (979.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:34:21.163376: step 28700, loss = 0.64 (917.8 examples/sec; 0.139 sec/batch)
2017-05-08 16:34:22.364894: step 28710, loss = 0.91 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-08 16:34:23.659680: step 28720, loss = 0.74 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:34:24.948988: step 28730, loss = 0.67 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:34:26.271871: step 28740, loss = 0.96 (967.6 examples/sec; 0.132 sec/batch)
2017-05-08 16:34:27.558994: step 28750, loss = 0.86 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:34:28.862660: step 28760, loss = 0.88 (981.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:34:30.135666: step 28770, loss = 0.74 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:34:31.402620: step 28780, loss = 0.80 (1010.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:34:32.668675: step 28790, loss = 0.86 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:34:34.025362: step 28800, loss = 0.75 (943.5 examples/sec; 0.136 sec/batch)
2017-05-08 16:34:35.229710: step 28810, loss = 0.73 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-08 16:34:36.507671: step 28820, loss = 0.82 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:34:37.775073: step 28830, loss = 0.81 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:34:39.068280: step 28840, loss = 0.78 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:34:40.336502: step 28850, loss = 0.80 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:34:41.616659: step 28860, loss = 0.80 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:34:42.885535: step 28870, loss = 0.95 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:34:44.163976: step 28880, loss = 0.84 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:34:45.455032: step 28890, loss = 0.71 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:34:46.811727: step 28900, loss = 0.58 (943.5 examples/sec; 0.136 sec/batch)
2017-05-08 16:34:47.979235: step 28910, loss = 0.81 (1096.3 examples/sec; 0.117 sec/batch)
2017-05-08 16:34:49.253127: step 28920, loss = 0.80 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:34:50.559840: step 28930, loss = 0.68 (979.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:34:51.835147: step 28940, loss = 0.94 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:34:53.141866: step 28950, loss = 0.76 (979.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:34:54.430852: step 28960, loss = 0.80 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:34:55.713372: step 28970, loss = 0.81 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:34:57.006883: step 28980, loss = 0.94 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:34:58.279125: step 28990, loss = 0.81 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:34:59.660066: step 29000, loss = 0.80 (926.9 examples/sec; 0.138 sec/batch)
2017-05-08 16:35:00.851769: step 29010, loss = 0.80 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-08 16:35:02.150367: step 29020, loss = 0.86 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:35:03.440537: step 29030, loss = 0.87 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:35:04.691046: step 29040, loss = 0.96 (1023.6 examples/sec; 0.125 sec/batch)
2017-05-08 16:35:05.967491: step 29050, loss = 0.96 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:35:07.251490: step 29060, loss = 0.80 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:35:08.524320: step 29070, loss = 0.84 (1005.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:35:09.818359: step 29080, loss = 0.82 (989.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:35:11.106833: step 29090, loss = 0.87 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:35:12.503745: step 29100, loss = 0.79 (916.3 examples/sec; 0.140 sec/batch)
2017-05-08 16:35:13.700509: step 29110, loss = 0.78 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-08 16:35:15.011787: step 29120, loss = 0.89 (976.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:35:16.323130: step 29130, loss = 0.79 (976.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:35:17.624343: step 29140, loss = 0.67 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:35:18.912383: step 29150, loss = 1.00 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:35:20.181864: step 29160, loss = 0.94 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:35:21.501935: step 29170, loss = 0.74 (969.6 examples/sec; 0.132 sec/batch)
2017-05-08 16:35:22.815225: step 29180, loss = 0.91 (974.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:35:24.092475: step 29190, loss = 0.88 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:35:25.485689: step 29200, loss = 0.93 (918.7 examples/sec; 0.139 sec/batch)
2017-05-08 16:35:26.700425: step 29210, loss = 0.72 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-08 16:35:27.960756: step 29220, loss = 0.85 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-08 16:35:29.256529: step 29230, loss = 0.77 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:35:30.527986: step 29240, loss = 1.03 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:35:31.806138: step 29250, loss = 0.75 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:35:33.070278: step 29260, loss = 0.94 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-08 16:35:34.341660: step 29270, loss = 0.71 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:35:35.635901: step 29280, loss = 0.88 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:35:36.925381: step 29290, loss = 0.72 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:35:38.293296: step 29300, loss = 0.79 (935.7 examples/sec; 0.137 sec/batch)
2017-05-08 16:35:39.479805: step 29310, loss = 0.76 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-08 16:35:40.746677: step 29320, loss = 0.87 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:35:42.038989: step 29330, loss = 0.85 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:35:43.309364: step 29340, loss = 0.84 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:35:44.593793: step 29350, loss = 0.89 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:35:45.874545: step 29360, loss = 0.71 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:35:47.144206: step 29370, loss = 0.81 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:35:48.410657: step 29380, loss = 0.83 (1010.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:35:49.691193: step 29390, loss = 0.88 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:35:51.070081: step 29400, loss = 0.73 (928.3 examples/sec; 0.138 sec/batch)
2017-05-08 16:35:52.254320: step 29410, loss = 0.62 (1080.9 examples/sec; 0.118 sec/batch)
2017-05-08 16:35:53.529187: step 29420, loss = 0.84 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:35:54.800982: step 29430, loss = 0.82 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:35:56.074049: step 29440, loss = 0.66 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:35:57.334440: step 29450, loss = 0.73 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-08 16:35:58.589643: step 29460, loss = 0.76 (1019.8 examples/sec; 0.126 sec/batch)
2017-05-08 16:35:59.881666: step 29470, loss = 0.84 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:36:01.166604: step 29480, loss = 0.79 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:36:02.459265: step 29490, loss = 0.86 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:36:03.823649: step 29500, loss = 0.71 (938.2 examples/sec; 0.136 sec/batch)
2017-05-08 16:36:05.032259: step 29510, loss = 0.90 (1059.1 examples/sec; 0.121 sec/batch)
2017-05-08 16:36:06.297692: step 29520, loss = 0.80 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:36:07.578439: step 29530, loss = 0.85 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:36:08.866959: step 29540, loss = 0.75 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:36:10.115999: step 29550, loss = 0.85 (1024.8 examples/sec; 0.125 sec/batch)
2017-05-08 16:36:11.414777: step 29560, loss = 0.79 (985.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:36:12.715549: step 29570, loss = 0.92 (984.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:36:13.980629: step 29580, loss = 0.95 (1011.8 examples/sec; 0.12E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 607 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
7 sec/batch)
2017-05-08 16:36:15.259530: step 29590, loss = 0.67 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:36:16.632356: step 29600, loss = 0.58 (932.4 examples/sec; 0.137 sec/batch)
2017-05-08 16:36:17.815294: step 29610, loss = 0.73 (1082.1 examples/sec; 0.118 sec/batch)
2017-05-08 16:36:19.111703: step 29620, loss = 0.72 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:36:20.376239: step 29630, loss = 0.74 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 16:36:21.650482: step 29640, loss = 0.77 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:36:22.935234: step 29650, loss = 0.68 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:36:24.218151: step 29660, loss = 0.83 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:36:25.517897: step 29670, loss = 0.89 (984.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:36:26.796995: step 29680, loss = 0.84 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:36:28.076064: step 29690, loss = 0.64 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:36:29.437402: step 29700, loss = 0.76 (940.3 examples/sec; 0.136 sec/batch)
2017-05-08 16:36:30.633628: step 29710, loss = 0.74 (1070.0 examples/sec; 0.120 sec/batch)
2017-05-08 16:36:31.921388: step 29720, loss = 0.85 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:36:33.244072: step 29730, loss = 0.88 (967.7 examples/sec; 0.132 sec/batch)
2017-05-08 16:36:34.520933: step 29740, loss = 0.80 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:36:35.822574: step 29750, loss = 0.87 (983.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:36:37.122861: step 29760, loss = 0.84 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:36:38.415153: step 29770, loss = 0.89 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:36:39.693656: step 29780, loss = 0.89 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:36:40.993426: step 29790, loss = 0.84 (984.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:36:42.387521: step 29800, loss = 0.83 (918.2 examples/sec; 0.139 sec/batch)
2017-05-08 16:36:43.562800: step 29810, loss = 1.03 (1089.1 examples/sec; 0.118 sec/batch)
2017-05-08 16:36:44.890602: step 29820, loss = 0.90 (964.0 examples/sec; 0.133 sec/batch)
2017-05-08 16:36:46.185081: step 29830, loss = 0.88 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:36:47.502488: step 29840, loss = 1.26 (971.6 examples/sec; 0.132 sec/batch)
2017-05-08 16:36:48.809681: step 29850, loss = 0.92 (979.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:36:50.075390: step 29860, loss = 0.82 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:36:51.363422: step 29870, loss = 0.86 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:36:52.662628: step 29880, loss = 0.82 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:36:53.943683: step 29890, loss = 0.74 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:36:55.320289: step 29900, loss = 0.84 (929.8 examples/sec; 0.138 sec/batch)
2017-05-08 16:36:56.507142: step 29910, loss = 0.83 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-08 16:36:57.819216: step 29920, loss = 0.78 (975.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:36:59.133439: step 29930, loss = 0.82 (974.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:37:00.426415: step 29940, loss = 0.97 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:37:01.704800: step 29950, loss = 0.75 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:37:02.970338: step 29960, loss = 0.78 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:37:04.243624: step 29970, loss = 0.90 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:37:05.518659: step 29980, loss = 0.80 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:37:06.818864: step 29990, loss = 0.83 (984.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:37:08.176813: step 30000, loss = 0.84 (942.6 examples/sec; 0.136 sec/batch)
2017-05-08 16:37:09.355643: step 30010, loss = 0.82 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-08 16:37:10.657952: step 30020, loss = 0.88 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:37:11.959718: step 30030, loss = 0.86 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:37:13.242595: step 30040, loss = 0.75 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:37:14.539055: step 30050, loss = 1.01 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:37:15.826014: step 30060, loss = 0.89 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:37:17.118599: step 30070, loss = 0.82 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:37:18.396495: step 30080, loss = 0.86 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:37:19.679028: step 30090, loss = 0.76 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:37:21.056535: step 30100, loss = 0.70 (929.2 examples/sec; 0.138 sec/batch)
2017-05-08 16:37:22.225317: step 30110, loss = 0.87 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-08 16:37:23.523139: step 30120, loss = 0.79 (986.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:37:24.794638: step 30130, loss = 0.80 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:37:26.070336: step 30140, loss = 0.66 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:37:27.354751: step 30150, loss = 0.84 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:37:28.629909: step 30160, loss = 0.62 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:37:29.905885: step 30170, loss = 0.77 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:37:31.177561: step 30180, loss = 0.81 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:37:32.466879: step 30190, loss = 0.84 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:37:33.856246: step 30200, loss = 0.63 (921.3 examples/sec; 0.139 sec/batch)
2017-05-08 16:37:35.046269: step 30210, loss = 0.70 (1075.6 examples/sec; 0.119 sec/batch)
2017-05-08 16:37:36.329920: step 30220, loss = 0.92 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:37:37.604056: step 30230, loss = 0.72 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:37:38.875787: step 30240, loss = 0.74 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:37:40.163615: step 30250, loss = 0.81 (993.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:37:41.444069: step 30260, loss = 0.73 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:37:42.723934: step 30270, loss = 0.69 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:37:43.995071: step 30280, loss = 0.66 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:37:45.267769: step 30290, loss = 0.77 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:37:46.659756: step 30300, loss = 0.95 (919.5 examples/sec; 0.139 sec/batch)
2017-05-08 16:37:47.856493: step 30310, loss = 0.96 (1069.6 examples/sec; 0.120 sec/batch)
2017-05-08 16:37:49.126180: step 30320, loss = 0.95 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:37:50.409998: step 30330, loss = 0.80 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:37:51.691405: step 30340, loss = 0.67 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:37:52.974742: step 30350, loss = 0.76 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:37:54.265999: step 30360, loss = 0.73 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:37:55.525131: step 30370, loss = 0.87 (1016.6 examples/sec; 0.126 sec/batch)
2017-05-08 16:37:56.810057: step 30380, loss = 0.79 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:37:58.075861: step 30390, loss = 0.84 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:37:59.461185: step 30400, loss = 0.92 (924.0 examples/sec; 0.139 sec/batch)
2017-05-08 16:38:00.660738: step 30410, loss = 0.81 (1067.1 examples/sec; 0.120 sec/batch)
2017-05-08 16:38:01.938336: step 30420, loss = 0.71 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:38:03.234589: step 30430, loss = 0.78 (987.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:38:04.506570: step 30440, loss = 0.97 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:38:05.799944: step 30450, loss = 0.76 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:38:07.070786: step 30460, loss = 0.81 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:38:08.361916: step 30470, loss = 1.18 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:38:09.670098: step 30480, loss = 0.82 (978.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:38:10.946786: step 30490, lE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 627 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
oss = 0.63 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:38:12.305938: step 30500, loss = 0.82 (941.8 examples/sec; 0.136 sec/batch)
2017-05-08 16:38:13.483575: step 30510, loss = 0.83 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-08 16:38:14.767301: step 30520, loss = 0.81 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:38:16.044653: step 30530, loss = 0.80 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:38:17.344604: step 30540, loss = 0.95 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:38:18.627262: step 30550, loss = 0.79 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:38:19.920726: step 30560, loss = 0.67 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:38:21.213825: step 30570, loss = 0.94 (989.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:38:22.498931: step 30580, loss = 0.76 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:38:23.769856: step 30590, loss = 1.09 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:38:25.169909: step 30600, loss = 0.87 (914.2 examples/sec; 0.140 sec/batch)
2017-05-08 16:38:26.355381: step 30610, loss = 0.82 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-08 16:38:27.641416: step 30620, loss = 0.85 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:38:28.918409: step 30630, loss = 0.86 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:38:30.220045: step 30640, loss = 0.72 (983.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:38:31.521852: step 30650, loss = 0.76 (983.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:38:32.815619: step 30660, loss = 0.88 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:38:34.108929: step 30670, loss = 0.91 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:38:35.385608: step 30680, loss = 0.75 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:38:36.664290: step 30690, loss = 0.76 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:38:38.045188: step 30700, loss = 0.79 (926.9 examples/sec; 0.138 sec/batch)
2017-05-08 16:38:39.242761: step 30710, loss = 0.76 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-08 16:38:40.520218: step 30720, loss = 0.97 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:38:41.771982: step 30730, loss = 0.93 (1022.6 examples/sec; 0.125 sec/batch)
2017-05-08 16:38:43.049215: step 30740, loss = 0.83 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:38:44.341492: step 30750, loss = 0.67 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:38:45.630929: step 30760, loss = 0.88 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:38:46.917428: step 30770, loss = 0.83 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:38:48.196262: step 30780, loss = 0.67 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:38:49.472878: step 30790, loss = 0.96 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:38:50.819689: step 30800, loss = 0.91 (950.4 examples/sec; 0.135 sec/batch)
2017-05-08 16:38:52.001171: step 30810, loss = 0.74 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-08 16:38:53.281181: step 30820, loss = 0.74 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:38:54.545639: step 30830, loss = 0.87 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-08 16:38:55.828861: step 30840, loss = 0.65 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:38:57.133602: step 30850, loss = 0.85 (981.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:38:58.435193: step 30860, loss = 0.94 (983.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:38:59.732187: step 30870, loss = 0.79 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:39:01.018641: step 30880, loss = 0.74 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:39:02.317761: step 30890, loss = 0.72 (985.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:39:03.693607: step 30900, loss = 0.69 (930.3 examples/sec; 0.138 sec/batch)
2017-05-08 16:39:04.898505: step 30910, loss = 0.67 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-08 16:39:06.193313: step 30920, loss = 0.96 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:39:07.496839: step 30930, loss = 0.72 (981.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:39:08.801065: step 30940, loss = 0.83 (981.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:39:10.102830: step 30950, loss = 0.80 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:39:11.391641: step 30960, loss = 0.83 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:39:12.656696: step 30970, loss = 0.86 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:39:13.950908: step 30980, loss = 0.71 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:39:15.231889: step 30990, loss = 0.78 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:39:16.623249: step 31000, loss = 0.87 (920.0 examples/sec; 0.139 sec/batch)
2017-05-08 16:39:17.848483: step 31010, loss = 0.75 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-08 16:39:19.128753: step 31020, loss = 0.76 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:39:20.378073: step 31030, loss = 0.76 (1024.5 examples/sec; 0.125 sec/batch)
2017-05-08 16:39:21.673937: step 31040, loss = 0.80 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:39:22.953300: step 31050, loss = 0.86 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:39:24.256714: step 31060, loss = 0.69 (982.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:39:25.560737: step 31070, loss = 1.08 (981.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:39:26.843841: step 31080, loss = 0.87 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:39:28.152396: step 31090, loss = 0.89 (978.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:39:29.531376: step 31100, loss = 0.89 (928.2 examples/sec; 0.138 sec/batch)
2017-05-08 16:39:30.736779: step 31110, loss = 0.79 (1061.9 examples/sec; 0.121 sec/batch)
2017-05-08 16:39:32.009809: step 31120, loss = 0.82 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:39:33.284137: step 31130, loss = 0.91 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:39:34.566203: step 31140, loss = 0.84 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:39:35.852848: step 31150, loss = 0.96 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:39:37.126928: step 31160, loss = 0.81 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:39:38.396940: step 31170, loss = 0.78 (1007.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:39:39.696996: step 31180, loss = 0.85 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:39:41.003156: step 31190, loss = 0.81 (980.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:39:42.399281: step 31200, loss = 1.01 (916.8 examples/sec; 0.140 sec/batch)
2017-05-08 16:39:43.615969: step 31210, loss = 0.70 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-08 16:39:44.882199: step 31220, loss = 0.78 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:39:46.176630: step 31230, loss = 0.97 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:39:47.454036: step 31240, loss = 0.91 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:39:48.736318: step 31250, loss = 0.66 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:39:50.032947: step 31260, loss = 0.70 (987.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:39:51.310589: step 31270, loss = 0.80 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:39:52.562905: step 31280, loss = 0.82 (1022.1 examples/sec; 0.125 sec/batch)
2017-05-08 16:39:53.847108: step 31290, loss = 0.77 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:39:55.229931: step 31300, loss = 1.11 (925.6 examples/sec; 0.138 sec/batch)
2017-05-08 16:39:56.418653: step 31310, loss = 0.88 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-08 16:39:57.705269: step 31320, loss = 0.76 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:39:58.965982: step 31330, loss = 0.77 (1015.3 examples/sec; 0.126 sec/batch)
2017-05-08 16:40:00.237113: step 31340, loss = 0.86 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:40:01.539088: step 31350, loss = 0.64 (983.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:40:02.861198: step 31360, loss = 0.92 (968.2 examples/sec; 0.132 sec/batch)
2017-05-08 16:40:04.136544: step 31370, loss = 0.78 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:40:05.431978: step 31380, loss = 0.83 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:40:06.716770: step 31390, loss = 0.98 (996.3 examples/sec; 0.128 sec/batch)
2017-05E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 647 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
-08 16:40:08.108630: step 31400, loss = 0.94 (919.6 examples/sec; 0.139 sec/batch)
2017-05-08 16:40:09.288227: step 31410, loss = 0.85 (1085.1 examples/sec; 0.118 sec/batch)
2017-05-08 16:40:10.567628: step 31420, loss = 0.78 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:40:11.834955: step 31430, loss = 0.69 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:40:13.114041: step 31440, loss = 0.68 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:40:14.410547: step 31450, loss = 0.95 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:40:15.735683: step 31460, loss = 0.97 (965.9 examples/sec; 0.133 sec/batch)
2017-05-08 16:40:17.050075: step 31470, loss = 0.89 (973.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:40:18.334875: step 31480, loss = 0.81 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:40:19.614847: step 31490, loss = 0.72 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:40:20.977783: step 31500, loss = 0.74 (939.2 examples/sec; 0.136 sec/batch)
2017-05-08 16:40:22.176143: step 31510, loss = 0.92 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-08 16:40:23.460909: step 31520, loss = 0.85 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:40:24.786832: step 31530, loss = 0.87 (965.4 examples/sec; 0.133 sec/batch)
2017-05-08 16:40:26.099704: step 31540, loss = 0.77 (975.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:40:27.404984: step 31550, loss = 0.67 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:40:28.668926: step 31560, loss = 0.79 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-08 16:40:29.954091: step 31570, loss = 0.78 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:40:31.224498: step 31580, loss = 0.78 (1007.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:40:32.502507: step 31590, loss = 0.72 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:40:33.878384: step 31600, loss = 0.67 (930.3 examples/sec; 0.138 sec/batch)
2017-05-08 16:40:35.094389: step 31610, loss = 0.82 (1052.6 examples/sec; 0.122 sec/batch)
2017-05-08 16:40:36.395714: step 31620, loss = 0.88 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:40:37.685431: step 31630, loss = 0.89 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:40:38.963997: step 31640, loss = 1.08 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:40:40.220177: step 31650, loss = 0.69 (1019.0 examples/sec; 0.126 sec/batch)
2017-05-08 16:40:41.504293: step 31660, loss = 0.67 (996.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:40:42.779724: step 31670, loss = 0.91 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:40:44.044555: step 31680, loss = 0.73 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-08 16:40:45.331533: step 31690, loss = 0.85 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:40:46.685891: step 31700, loss = 0.98 (945.1 examples/sec; 0.135 sec/batch)
2017-05-08 16:40:47.887077: step 31710, loss = 0.82 (1065.6 examples/sec; 0.120 sec/batch)
2017-05-08 16:40:49.188538: step 31720, loss = 0.72 (983.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:40:50.471697: step 31730, loss = 0.79 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:40:51.769203: step 31740, loss = 0.96 (986.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:40:53.064117: step 31750, loss = 0.78 (988.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:40:54.337972: step 31760, loss = 0.84 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:40:55.623973: step 31770, loss = 0.81 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:40:56.890045: step 31780, loss = 0.86 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:40:58.179420: step 31790, loss = 0.73 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:40:59.540840: step 31800, loss = 0.91 (940.2 examples/sec; 0.136 sec/batch)
2017-05-08 16:41:00.738169: step 31810, loss = 0.86 (1069.0 examples/sec; 0.120 sec/batch)
2017-05-08 16:41:02.031265: step 31820, loss = 0.92 (989.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:41:03.361754: step 31830, loss = 1.10 (962.1 examples/sec; 0.133 sec/batch)
2017-05-08 16:41:04.634346: step 31840, loss = 0.96 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:41:05.913595: step 31850, loss = 0.88 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:41:07.189526: step 31860, loss = 0.94 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:41:08.477913: step 31870, loss = 0.78 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:41:09.747348: step 31880, loss = 0.81 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:41:11.031632: step 31890, loss = 0.88 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:41:12.401144: step 31900, loss = 1.00 (934.6 examples/sec; 0.137 sec/batch)
2017-05-08 16:41:13.578260: step 31910, loss = 0.75 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-08 16:41:14.836272: step 31920, loss = 0.87 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-08 16:41:16.114239: step 31930, loss = 0.73 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:41:17.397173: step 31940, loss = 0.85 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:41:18.687754: step 31950, loss = 0.92 (991.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:41:19.960515: step 31960, loss = 0.90 (1005.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:41:21.258637: step 31970, loss = 0.77 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:41:22.557064: step 31980, loss = 0.88 (985.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:41:23.867556: step 31990, loss = 0.66 (976.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:41:25.267906: step 32000, loss = 0.99 (914.1 examples/sec; 0.140 sec/batch)
2017-05-08 16:41:26.466533: step 32010, loss = 0.89 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-08 16:41:27.736711: step 32020, loss = 0.77 (1007.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:41:29.022271: step 32030, loss = 0.72 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:41:30.295834: step 32040, loss = 0.99 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:41:31.605622: step 32050, loss = 0.85 (977.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:41:32.899314: step 32060, loss = 0.81 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:41:34.202879: step 32070, loss = 0.97 (981.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:41:35.470178: step 32080, loss = 0.83 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:41:36.750369: step 32090, loss = 0.87 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:41:38.128154: step 32100, loss = 0.78 (929.0 examples/sec; 0.138 sec/batch)
2017-05-08 16:41:39.315890: step 32110, loss = 0.76 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-08 16:41:40.584669: step 32120, loss = 0.81 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:41:41.864789: step 32130, loss = 0.75 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:41:43.160159: step 32140, loss = 0.86 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:41:44.439255: step 32150, loss = 0.72 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:41:45.721281: step 32160, loss = 0.69 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:41:46.987874: step 32170, loss = 0.76 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:41:48.269897: step 32180, loss = 0.71 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:41:49.615655: step 32190, loss = 0.59 (951.1 examples/sec; 0.135 sec/batch)
2017-05-08 16:41:50.968509: step 32200, loss = 0.86 (946.1 examples/sec; 0.135 sec/batch)
2017-05-08 16:41:52.184863: step 32210, loss = 0.91 (1052.3 examples/sec; 0.122 sec/batch)
2017-05-08 16:41:53.457142: step 32220, loss = 0.76 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:41:54.766032: step 32230, loss = 0.84 (977.9 examples/sec; 0.131 sec/batch)
2017-05-08 16:41:56.079348: step 32240, loss = 0.77 (974.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:41:57.378719: step 32250, loss = 0.84 (985.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:41:58.656531: step 32260, loss = 0.68 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:41:59.961477: step 32270, loss = 0.88 (980.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:42:01.235604: step 32280, loss = 0.74 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:42:02.519603: step 32290, loss = 0.90 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:42:03.915347: step 32300, loss = 0.80 (917.1 examples/sec; 0.140 sec/batch)
2017-05-08 16:42:05.100676: step 32310, loss = 0.77 (1079.9 examples/sec; 0.119 sec/batch)
2017-05-08 16:42:06.392888: step 32320, loss = 1.00 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:42:07.708885: step 32330, loss = 0.72 (972.6 examples/sec; 0.132 sec/batch)
2017-05-08 16:42:08.991758: step 32340, loss = 0.81 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:42:10.294958: step 32350, loss = 0.72 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:42:11.572568: step 32360, loss = 0.91 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:42:12.841036: step 32370, loss = 0.82 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:42:14.119908: step 32380, loss = 0.75 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:42:15.419702: step 32390, loss = 0.81 (984.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:42:16.785159: step 32400, loss = 0.74 (937.4 examples/sec; 0.137 sec/batch)
2017-05-08 16:42:17.947112: step 32410, loss = 0.79 (1101.6 examples/sec; 0.116 sec/batch)
2017-05-08 16:42:19.220643: step 32420, loss = 0.77 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:42:20.499107: step 32430, loss = 0.90 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:42:21.808199: step 32440, loss = 0.74 (977.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:42:23.104054: step 32450, loss = 0.88 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:42:24.383962: step 32460, loss = 0.80 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:42:25.660369: step 32470, loss = 0.77 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:42:26.942722: step 32480, loss = 0.75 (998.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:42:28.221921: step 32490, loss = 0.87 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:42:29.602947: step 32500, loss = 0.69 (926.8 examples/sec; 0.138 sec/batch)
2017-05-08 16:42:30.792689: step 32510, loss = 0.74 (1075.9 examples/sec; 0.119 sec/batch)
2017-05-08 16:42:32.066998: step 32520, loss = 0.89 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:42:33.352324: step 32530, loss = 0.91 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:42:34.682728: step 32540, loss = 0.92 (962.1 examples/sec; 0.133 sec/batch)
2017-05-08 16:42:35.938413: step 32550, loss = 0.78 (1019.4 examples/sec; 0.126 sec/batch)
2017-05-08 16:42:37.240514: step 32560, loss = 0.76 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:42:38.538528: step 32570, loss = 0.95 (986.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:42:39.842665: step 32580, loss = 0.96 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:42:41.144679: step 32590, loss = 0.80 (983.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:42:42.538683: step 32600, loss = 0.98 (918.2 examples/sec; 0.139 sec/batch)
2017-05-08 16:42:43.737651: step 32610, loss = 0.82 (1067.6 examples/sec; 0.120 sec/batch)
2017-05-08 16:42:45.028088: step 32620, loss = 1.06 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:42:46.356383: step 32630, loss = 1.04 (963.6 examples/sec; 0.133 sec/batch)
2017-05-08 16:42:47.659918: step 32640, loss = 0.81 (981.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:42:48.957817: step 32650, loss = 0.69 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:42:50.234337: step 32660, loss = 0.85 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:42:51.510892: step 32670, loss = 0.68 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:42:52.797808: step 32680, loss = 0.78 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:42:54.087476: step 32690, loss = 0.74 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:42:55.462613: step 32700, loss = 0.73 (930.8 examples/sec; 0.138 sec/batch)
2017-05-08 16:42:56.646730: step 32710, loss = 0.77 (1081.0 examples/sec; 0.118 sec/batch)
2017-05-08 16:42:57.935005: step 32720, loss = 0.72 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:42:59.222124: step 32730, loss = 0.69 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:43:00.511159: step 32740, loss = 0.74 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:43:01.783257: step 32750, loss = 1.02 (1006.2 examples/sec; 0.127 sec/batch)E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 668 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU

2017-05-08 16:43:03.094342: step 32760, loss = 0.66 (976.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:43:04.410044: step 32770, loss = 0.91 (972.9 examples/sec; 0.132 sec/batch)
2017-05-08 16:43:05.725006: step 32780, loss = 0.77 (973.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:43:07.022760: step 32790, loss = 0.77 (986.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:43:08.416594: step 32800, loss = 0.79 (918.3 examples/sec; 0.139 sec/batch)
2017-05-08 16:43:09.618618: step 32810, loss = 0.73 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-08 16:43:10.883810: step 32820, loss = 0.81 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:43:12.174488: step 32830, loss = 0.82 (991.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:43:13.507039: step 32840, loss = 0.90 (960.5 examples/sec; 0.133 sec/batch)
2017-05-08 16:43:14.783416: step 32850, loss = 0.82 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:43:16.082052: step 32860, loss = 0.87 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:43:17.369652: step 32870, loss = 0.92 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:43:18.681796: step 32880, loss = 0.92 (975.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:43:19.968799: step 32890, loss = 0.84 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:43:21.343350: step 32900, loss = 0.71 (931.2 examples/sec; 0.137 sec/batch)
2017-05-08 16:43:22.555366: step 32910, loss = 0.85 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-08 16:43:23.856519: step 32920, loss = 1.00 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:43:25.138758: step 32930, loss = 0.80 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:43:26.441732: step 32940, loss = 0.68 (982.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:43:27.729068: step 32950, loss = 0.80 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:43:29.002995: step 32960, loss = 0.80 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:43:30.286800: step 32970, loss = 0.84 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:43:31.552911: step 32980, loss = 0.85 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:43:32.842255: step 32990, loss = 0.83 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:43:34.253094: step 33000, loss = 0.84 (907.3 examples/sec; 0.141 sec/batch)
2017-05-08 16:43:35.453667: step 33010, loss = 0.76 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-08 16:43:36.734385: step 33020, loss = 0.91 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:43:38.012377: step 33030, loss = 0.97 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:43:39.306575: step 33040, loss = 0.76 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:43:40.571740: step 33050, loss = 0.63 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:43:41.861701: step 33060, loss = 0.82 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:43:43.127796: step 33070, loss = 0.86 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:43:44.429143: step 33080, loss = 0.70 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:43:45.719411: step 33090, loss = 0.89 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:43:47.096906: step 33100, loss = 0.87 (929.2 examples/sec; 0.138 sec/batch)
2017-05-08 16:43:48.278410: step 33110, loss = 0.94 (1083.4 examples/sec; 0.118 sec/batch)
2017-05-08 16:43:49.586575: step 33120, loss = 0.85 (978.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:43:50.894161: step 33130, loss = 0.70 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 16:43:52.177077: step 33140, loss = 0.74 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:43:53.457904: step 33150, loss = 0.92 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:43:54.720295: step 33160, loss = 0.87 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-08 16:43:55.992880: step 33170, loss = 0.80 (1005.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:43:57.292030: step 33180, loss = 0.97 (985.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:43:58.582499: step 33190, loss = 0.99 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:43:59.953400: step 33200, loss = 0.69 (933.7 examples/sec; 0.137 sec/batch)
2017-05-08 16:44:01.179264: step 33210, loss = 0.89 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-08 16:44:02.486360: step 33220, loss = 0.71 (979.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:44:03.779195: step 33230, loss = 0.83 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:44:05.088529: step 33240, loss = 0.76 (977.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:44:06.385921: step 33250, loss = 0.70 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:44:07.691761: step 33260, loss = 0.86 (980.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:44:09.000277: step 33270, loss = 0.80 (978.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:44:10.302956: step 33280, loss = 0.85 (982.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:44:11.581575: step 33290, loss = 0.82 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:44:12.970405: step 33300, loss = 1.00 (921.6 examples/sec; 0.139 sec/batch)
2017-05-08 16:44:14.191487: step 33310, loss = 0.78 (1048.3 examples/sec; 0.122 sec/batch)
2017-05-08 16:44:15.490068: step 33320, loss = 0.97 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:44:16.781007: step 33330, loss = 1.24 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:44:18.081916: step 33340, loss = 1.01 (983.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:44:19.401927: step 33350, loss = 0.72 (969.7 examples/sec; 0.132 sec/batch)
2017-05-08 16:44:20.690094: step 33360, loss = 0.79 (993.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:44:21.980755: step 33370, loss = 0.88 (991.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:44:23.291377: step 33380, loss = 0.96 (976.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:44:24.558968: step 33390, loss = 0.71 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:44:25.924977: step 33400, loss = 0.73 (937.0 examples/sec; 0.137 sec/batch)
2017-05-08 16:44:27.119966: step 33410, loss = 0.72 (1071.1 examples/sec; 0.119 sec/batch)
2017-05-08 16:44:28.400851: step 33420, loss = 0.80 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:44:29.669428: step 33430, loss = 0.84 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:44:30.959140: step 33440, loss = 0.84 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:44:32.244442: step 33450, loss = 0.86 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:44:33.504836: step 33460, loss = 0.78 (1015.6 examples/sec; 0.126 sec/batch)
2017-05-08 16:44:34.770044: step 33470, loss = 0.72 (1011.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:44:36.052417: step 33480, loss = 0.80 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:44:37.362292: step 33490, loss = 0.90 (977.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:44:38.757350: step 33500, loss = 0.81 (917.5 examples/sec; 0.140 sec/batch)
2017-05-08 16:44:39.949904: step 33510, loss = 0.69 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-08 16:44:41.235191: step 33520, loss = 0.85 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:44:42.522838: step 33530, loss = 0.78 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:44:43.801228: step 33540, loss = 1.04 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:44:45.090547: step 33550, loss = 0.92 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:44:46.360099: step 33560, loss = 0.95 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:44:47.646893: step 33570, loss = 0.85 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:44:48.920939: step 33580, loss = 0.67 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:44:50.205798: step 33590, loss = 0.81 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:44:51.608934: step 33600, loss = 1.08 (912.2 examples/sec; 0.140 sec/batch)
2017-05-08 16:44:52.772975: step 33610, loss = 0.77 (1099.6 examples/sec; 0.116 sec/batch)
2017-05-08 16:44:54.055171: step 33620, loss = 0.87 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:44:55.337372: step 33630, loss = 0.84 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:44:56.615405: step 33640, loss = 0.85 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:44:57.917122: step 33650, loss = 0.74 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:44:59.199308: step 33660, loss = 0.87 (998.3 exampE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 688 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
les/sec; 0.128 sec/batch)
2017-05-08 16:45:00.480720: step 33670, loss = 0.89 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:45:01.771957: step 33680, loss = 0.69 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:45:03.074924: step 33690, loss = 0.91 (982.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:45:04.447095: step 33700, loss = 0.87 (932.8 examples/sec; 0.137 sec/batch)
2017-05-08 16:45:05.664903: step 33710, loss = 0.73 (1051.1 examples/sec; 0.122 sec/batch)
2017-05-08 16:45:06.962202: step 33720, loss = 0.76 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:45:08.239959: step 33730, loss = 0.75 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:45:09.545137: step 33740, loss = 1.19 (980.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:45:10.831879: step 33750, loss = 0.84 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:45:12.130198: step 33760, loss = 0.76 (985.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:45:13.437750: step 33770, loss = 0.78 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 16:45:14.737745: step 33780, loss = 0.69 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:45:16.029516: step 33790, loss = 0.70 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:45:17.405895: step 33800, loss = 0.72 (930.0 examples/sec; 0.138 sec/batch)
2017-05-08 16:45:18.609887: step 33810, loss = 0.87 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-08 16:45:19.906389: step 33820, loss = 0.75 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:45:21.211704: step 33830, loss = 0.89 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:45:22.478529: step 33840, loss = 0.79 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:45:23.759631: step 33850, loss = 0.72 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:45:25.055977: step 33860, loss = 0.71 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:45:26.347819: step 33870, loss = 0.77 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:45:27.631969: step 33880, loss = 0.87 (996.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:45:28.924641: step 33890, loss = 0.87 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:45:30.280837: step 33900, loss = 0.86 (943.8 examples/sec; 0.136 sec/batch)
2017-05-08 16:45:31.458107: step 33910, loss = 0.81 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-08 16:45:32.758507: step 33920, loss = 0.81 (984.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:45:34.055706: step 33930, loss = 0.96 (986.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:45:35.358776: step 33940, loss = 1.00 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:45:36.650028: step 33950, loss = 1.00 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:45:37.972959: step 33960, loss = 0.95 (967.5 examples/sec; 0.132 sec/batch)
2017-05-08 16:45:39.267810: step 33970, loss = 0.77 (988.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:45:40.535105: step 33980, loss = 0.82 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:45:41.808095: step 33990, loss = 0.79 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:45:43.185830: step 34000, loss = 0.80 (929.0 examples/sec; 0.138 sec/batch)
2017-05-08 16:45:44.363209: step 34010, loss = 0.66 (1087.3 examples/sec; 0.118 sec/batch)
2017-05-08 16:45:45.638012: step 34020, loss = 0.72 (1003.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:45:46.925638: step 34030, loss = 0.96 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:45:48.191347: step 34040, loss = 0.75 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:45:49.461979: step 34050, loss = 0.95 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:45:50.779258: step 34060, loss = 0.85 (971.7 examples/sec; 0.132 sec/batch)
2017-05-08 16:45:52.057787: step 34070, loss = 0.83 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:45:53.361167: step 34080, loss = 0.78 (982.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:45:54.651218: step 34090, loss = 0.83 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:45:56.028417: step 34100, loss = 0.80 (929.4 examples/sec; 0.138 sec/batch)
2017-05-08 16:45:57.215778: step 34110, loss = 0.97 (1078.0 examples/sec; 0.119 sec/batch)
2017-05-08 16:45:58.524909: step 34120, loss = 0.68 (977.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:45:59.814519: step 34130, loss = 0.83 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:46:01.082782: step 34140, loss = 0.78 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:46:02.376713: step 34150, loss = 0.77 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:46:03.665083: step 34160, loss = 0.92 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:46:04.959899: step 34170, loss = 0.80 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:46:06.240691: step 34180, loss = 0.96 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:46:07.531680: step 34190, loss = 0.75 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:46:08.888516: step 34200, loss = 0.72 (943.4 examples/sec; 0.136 sec/batch)
2017-05-08 16:46:10.092777: step 34210, loss = 0.81 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-08 16:46:11.349921: step 34220, loss = 0.78 (1018.2 examples/sec; 0.126 sec/batch)
2017-05-08 16:46:12.647917: step 34230, loss = 0.79 (986.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:46:13.944901: step 34240, loss = 0.66 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:46:15.222888: step 34250, loss = 0.89 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:46:16.515690: step 34260, loss = 0.80 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:46:17.799791: step 34270, loss = 0.81 (996.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:46:19.115547: step 34280, loss = 0.89 (972.8 examples/sec; 0.132 sec/batch)
2017-05-08 16:46:20.441828: step 34290, loss = 1.13 (965.1 examples/sec; 0.133 sec/batch)
2017-05-08 16:46:21.836595: step 34300, loss = 0.76 (917.7 examples/sec; 0.139 sec/batch)
2017-05-08 16:46:23.027317: step 34310, loss = 0.67 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-08 16:46:24.331880: step 34320, loss = 0.82 (981.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:46:25.602812: step 34330, loss = 0.73 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:46:26.863334: step 34340, loss = 0.68 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-08 16:46:28.136606: step 34350, loss = 0.87 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:46:29.417389: step 34360, loss = 0.73 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:46:30.702391: step 34370, loss = 0.74 (996.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:46:31.964114: step 34380, loss = 0.76 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-08 16:46:33.269002: step 34390, loss = 0.79 (980.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:46:34.639541: step 34400, loss = 0.77 (933.9 examples/sec; 0.137 sec/batch)
2017-05-08 16:46:35.814065: step 34410, loss = 0.77 (1089.8 examples/sec; 0.117 sec/batch)
2017-05-08 16:46:37.082839: step 34420, loss = 0.75 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:46:38.383920: step 34430, loss = 0.74 (983.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:46:39.669839: step 34440, loss = 0.94 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:46:40.963702: step 34450, loss = 0.85 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:46:42.284338: step 34460, loss = 0.72 (969.2 examples/sec; 0.132 sec/batch)
2017-05-08 16:46:43.588944: step 34470, loss = 0.81 (981.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:46:44.876023: step 34480, loss = 0.86 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:46:46.156205: step 34490, loss = 0.89 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:46:47.524285: step 34500, loss = 0.89 (935.6 examples/sec; 0.137 sec/batch)
2017-05-08 16:46:48.736259: step 34510, loss = 0.67 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-08 16:46:50.045050: step 34520, loss = 0.77 (978.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:46:51.360932: step 34530, loss = 1.03 (972.7 examples/sec; 0.132 sec/batch)
2017-05-08 16:46:52.678844: step 34540, loss = 0.88 (971.2 examples/sec; 0.132 sec/batch)
2017-05-08 16:46:53.966917: step 34550, loss = 0.88 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:46:55.254536: step 34560, loss = 0.92 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:46:56.521413: step 34570, lE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 708 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
oss = 0.78 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:46:57.822283: step 34580, loss = 0.79 (984.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:46:59.121669: step 34590, loss = 0.84 (985.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:47:00.487643: step 34600, loss = 0.73 (937.1 examples/sec; 0.137 sec/batch)
2017-05-08 16:47:01.683129: step 34610, loss = 0.82 (1070.7 examples/sec; 0.120 sec/batch)
2017-05-08 16:47:02.974368: step 34620, loss = 0.85 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:47:04.276585: step 34630, loss = 0.86 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:47:05.564034: step 34640, loss = 0.80 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:47:06.879649: step 34650, loss = 0.92 (972.9 examples/sec; 0.132 sec/batch)
2017-05-08 16:47:08.165200: step 34660, loss = 0.78 (995.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:47:09.452505: step 34670, loss = 0.78 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:47:10.775061: step 34680, loss = 0.87 (967.8 examples/sec; 0.132 sec/batch)
2017-05-08 16:47:12.074123: step 34690, loss = 0.92 (985.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:47:13.491230: step 34700, loss = 0.77 (903.3 examples/sec; 0.142 sec/batch)
2017-05-08 16:47:14.694611: step 34710, loss = 0.91 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-08 16:47:15.973634: step 34720, loss = 0.82 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:47:17.276056: step 34730, loss = 0.90 (982.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:47:18.557641: step 34740, loss = 0.81 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:47:19.841902: step 34750, loss = 0.86 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:47:21.124694: step 34760, loss = 0.81 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:47:22.398165: step 34770, loss = 0.84 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:47:23.679052: step 34780, loss = 0.76 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:47:24.939844: step 34790, loss = 0.80 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-08 16:47:26.327869: step 34800, loss = 0.70 (922.2 examples/sec; 0.139 sec/batch)
2017-05-08 16:47:27.518274: step 34810, loss = 1.02 (1075.3 examples/sec; 0.119 sec/batch)
2017-05-08 16:47:28.807925: step 34820, loss = 0.81 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:47:30.088240: step 34830, loss = 0.75 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:47:31.392863: step 34840, loss = 0.79 (981.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:47:32.660751: step 34850, loss = 0.71 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:47:33.929491: step 34860, loss = 0.94 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:47:35.207772: step 34870, loss = 0.76 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:47:36.489555: step 34880, loss = 0.99 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:47:37.781281: step 34890, loss = 0.74 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:47:39.146656: step 34900, loss = 0.93 (937.5 examples/sec; 0.137 sec/batch)
2017-05-08 16:47:40.333155: step 34910, loss = 0.96 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-08 16:47:41.613542: step 34920, loss = 0.87 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:47:42.957141: step 34930, loss = 0.73 (952.7 examples/sec; 0.134 sec/batch)
2017-05-08 16:47:44.216356: step 34940, loss = 0.84 (1016.5 examples/sec; 0.126 sec/batch)
2017-05-08 16:47:45.509009: step 34950, loss = 0.82 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:47:46.767102: step 34960, loss = 0.88 (1017.4 examples/sec; 0.126 sec/batch)
2017-05-08 16:47:48.059135: step 34970, loss = 0.75 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:47:49.357950: step 34980, loss = 0.82 (985.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:47:50.631929: step 34990, loss = 1.02 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:47:51.987700: step 35000, loss = 0.67 (944.1 examples/sec; 0.136 sec/batch)
2017-05-08 16:47:53.180839: step 35010, loss = 0.61 (1072.8 examples/sec; 0.119 sec/batch)
2017-05-08 16:47:54.467718: step 35020, loss = 0.97 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:47:55.743699: step 35030, loss = 0.78 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:47:57.022579: step 35040, loss = 0.77 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:47:58.312585: step 35050, loss = 0.74 (992.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:47:59.590355: step 35060, loss = 0.80 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:48:00.878654: step 35070, loss = 0.81 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:48:02.145026: step 35080, loss = 0.87 (1010.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:48:03.434010: step 35090, loss = 0.90 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:48:04.810314: step 35100, loss = 0.83 (930.0 examples/sec; 0.138 sec/batch)
2017-05-08 16:48:06.030134: step 35110, loss = 0.81 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-08 16:48:07.318990: step 35120, loss = 0.91 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:48:08.589665: step 35130, loss = 0.82 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:48:09.866536: step 35140, loss = 0.71 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:48:11.145074: step 35150, loss = 0.75 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:48:12.459879: step 35160, loss = 0.84 (973.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:48:13.734640: step 35170, loss = 0.73 (1004.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:48:15.075355: step 35180, loss = 0.87 (954.7 examples/sec; 0.134 sec/batch)
2017-05-08 16:48:16.362375: step 35190, loss = 0.76 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:48:17.743904: step 35200, loss = 0.86 (926.5 examples/sec; 0.138 sec/batch)
2017-05-08 16:48:18.969685: step 35210, loss = 0.83 (1044.2 examples/sec; 0.123 sec/batch)
2017-05-08 16:48:20.256178: step 35220, loss = 0.81 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:48:21.550133: step 35230, loss = 0.93 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:48:22.838700: step 35240, loss = 0.81 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:48:24.110363: step 35250, loss = 0.89 (1006.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:48:25.384813: step 35260, loss = 0.85 (1004.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:48:26.695675: step 35270, loss = 0.72 (976.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:48:27.966188: step 35280, loss = 0.85 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:48:29.250376: step 35290, loss = 0.79 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:48:30.603829: step 35300, loss = 0.82 (945.7 examples/sec; 0.135 sec/batch)
2017-05-08 16:48:31.783819: step 35310, loss = 1.01 (1084.8 examples/sec; 0.118 sec/batch)
2017-05-08 16:48:33.086724: step 35320, loss = 0.92 (982.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:48:34.362359: step 35330, loss = 0.76 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:48:35.629979: step 35340, loss = 1.03 (1009.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:48:36.923124: step 35350, loss = 0.65 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:48:38.201345: step 35360, loss = 0.78 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:48:39.493619: step 35370, loss = 0.72 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:48:40.761661: step 35380, loss = 0.63 (1009.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:48:42.065672: step 35390, loss = 0.76 (981.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:48:43.437128: step 35400, loss = 0.83 (933.3 examples/sec; 0.137 sec/batch)
2017-05-08 16:48:44.622196: step 35410, loss = 0.93 (1080.1 examples/sec; 0.119 sec/batch)
2017-05-08 16:48:45.922855: step 35420, loss = 0.66 (984.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:48:47.237783: step 35430, loss = 0.67 (973.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:48:48.511992: step 35440, loss = 0.73 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:48:49.810030: step 35450, loss = 0.90 (986.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:48:51.075660: step 35460, loss = 0.84 (1011.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:48:52.358272: step 35470, loss = 1.09 (998.0 examples/sec; 0.128 sec/batch)
2017-05E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 728 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
-08 16:48:53.652081: step 35480, loss = 0.83 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:48:54.932374: step 35490, loss = 0.94 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:48:56.293097: step 35500, loss = 0.82 (940.7 examples/sec; 0.136 sec/batch)
2017-05-08 16:48:57.483693: step 35510, loss = 0.82 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-08 16:48:58.770626: step 35520, loss = 0.80 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:49:00.083490: step 35530, loss = 0.74 (975.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:49:01.399311: step 35540, loss = 0.84 (972.8 examples/sec; 0.132 sec/batch)
2017-05-08 16:49:02.692113: step 35550, loss = 0.90 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:49:03.988916: step 35560, loss = 0.65 (987.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:49:05.277388: step 35570, loss = 0.86 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:49:06.557365: step 35580, loss = 0.82 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:49:07.848855: step 35590, loss = 0.84 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:49:09.236496: step 35600, loss = 0.91 (922.4 examples/sec; 0.139 sec/batch)
2017-05-08 16:49:10.458022: step 35610, loss = 0.85 (1047.9 examples/sec; 0.122 sec/batch)
2017-05-08 16:49:11.746129: step 35620, loss = 0.85 (993.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:49:13.030958: step 35630, loss = 0.87 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:49:14.353367: step 35640, loss = 0.74 (967.9 examples/sec; 0.132 sec/batch)
2017-05-08 16:49:15.650208: step 35650, loss = 0.79 (987.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:49:16.968499: step 35660, loss = 0.79 (971.0 examples/sec; 0.132 sec/batch)
2017-05-08 16:49:18.229287: step 35670, loss = 0.85 (1015.2 examples/sec; 0.126 sec/batch)
2017-05-08 16:49:19.493875: step 35680, loss = 0.73 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 16:49:20.758363: step 35690, loss = 0.82 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-08 16:49:22.138783: step 35700, loss = 0.63 (927.3 examples/sec; 0.138 sec/batch)
2017-05-08 16:49:23.321927: step 35710, loss = 0.83 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-08 16:49:24.612010: step 35720, loss = 0.70 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:49:25.891068: step 35730, loss = 1.15 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:49:27.186556: step 35740, loss = 0.88 (988.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:49:28.490646: step 35750, loss = 0.84 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:49:29.798123: step 35760, loss = 0.91 (979.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:49:31.080331: step 35770, loss = 0.80 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:49:32.376050: step 35780, loss = 0.74 (987.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:49:33.667243: step 35790, loss = 0.71 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:49:35.046199: step 35800, loss = 0.81 (928.2 examples/sec; 0.138 sec/batch)
2017-05-08 16:49:36.239609: step 35810, loss = 0.72 (1072.6 examples/sec; 0.119 sec/batch)
2017-05-08 16:49:37.525016: step 35820, loss = 0.68 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:49:38.833267: step 35830, loss = 0.65 (978.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:49:40.108877: step 35840, loss = 0.85 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:49:41.420393: step 35850, loss = 0.70 (976.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:49:42.700142: step 35860, loss = 0.90 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:49:43.978935: step 35870, loss = 0.89 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:49:45.245487: step 35880, loss = 0.80 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:49:46.523788: step 35890, loss = 0.65 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:49:47.877500: step 35900, loss = 0.78 (945.5 examples/sec; 0.135 sec/batch)
2017-05-08 16:49:49.076720: step 35910, loss = 0.96 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-08 16:49:50.360415: step 35920, loss = 0.83 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:49:51.631514: step 35930, loss = 0.69 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:49:52.917566: step 35940, loss = 0.87 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:49:54.217425: step 35950, loss = 0.68 (984.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:49:55.538625: step 35960, loss = 0.91 (968.8 examples/sec; 0.132 sec/batch)
2017-05-08 16:49:56.847215: step 35970, loss = 0.95 (978.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:49:58.137854: step 35980, loss = 1.02 (991.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:49:59.428646: step 35990, loss = 0.96 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:50:00.808242: step 36000, loss = 0.74 (927.8 examples/sec; 0.138 sec/batch)
2017-05-08 16:50:02.003204: step 36010, loss = 0.75 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-08 16:50:03.294317: step 36020, loss = 0.77 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:50:04.593215: step 36030, loss = 0.91 (985.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:50:05.909546: step 36040, loss = 0.69 (972.4 examples/sec; 0.132 sec/batch)
2017-05-08 16:50:07.215363: step 36050, loss = 0.80 (980.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:50:08.502714: step 36060, loss = 0.85 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:50:09.766878: step 36070, loss = 0.68 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-08 16:50:11.048873: step 36080, loss = 0.87 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:50:12.344336: step 36090, loss = 0.83 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:50:13.710928: step 36100, loss = 0.78 (936.6 examples/sec; 0.137 sec/batch)
2017-05-08 16:50:14.897925: step 36110, loss = 0.88 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-08 16:50:16.183271: step 36120, loss = 0.83 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:50:17.481017: step 36130, loss = 0.92 (986.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:50:18.787765: step 36140, loss = 0.78 (979.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:50:20.055191: step 36150, loss = 0.97 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:50:21.344593: step 36160, loss = 0.75 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:50:22.608758: step 36170, loss = 1.01 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-08 16:50:23.870499: step 36180, loss = 0.86 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-08 16:50:25.149000: step 36190, loss = 0.83 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:50:26.561046: step 36200, loss = 0.73 (906.5 examples/sec; 0.141 sec/batch)
2017-05-08 16:50:27.740563: step 36210, loss = 0.78 (1085.2 examples/sec; 0.118 sec/batch)
2017-05-08 16:50:29.018236: step 36220, loss = 0.81 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:50:30.292083: step 36230, loss = 0.79 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:50:31.575797: step 36240, loss = 0.70 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:50:32.852700: step 36250, loss = 0.57 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:50:34.128252: step 36260, loss = 0.83 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:50:35.419993: step 36270, loss = 0.74 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:50:36.681916: step 36280, loss = 0.65 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-08 16:50:37.987748: step 36290, loss = 0.78 (980.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:50:39.372160: step 36300, loss = 0.79 (924.5 examples/sec; 0.138 sec/batch)
2017-05-08 16:50:40.572971: step 36310, loss = 0.89 (1065.9 examples/sec; 0.120 sec/batch)
2017-05-08 16:50:41.840324: step 36320, loss = 0.76 (1010.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:50:43.121589: step 36330, loss = 0.86 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:50:44.391236: step 36340, loss = 0.85 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:50:45.668830: step 36350, loss = 0.88 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:50:46.963414: step 36360, loss = 0.83 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:50:48.268856: step 36370, loss = 0.78 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:50:49.561189: step 36380, loss = 0.75 (990.5 exampleE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 748 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
s/sec; 0.129 sec/batch)
2017-05-08 16:50:50.911009: step 36390, loss = 0.96 (948.3 examples/sec; 0.135 sec/batch)
2017-05-08 16:50:52.286576: step 36400, loss = 0.81 (930.5 examples/sec; 0.138 sec/batch)
2017-05-08 16:50:53.494061: step 36410, loss = 0.73 (1060.1 examples/sec; 0.121 sec/batch)
2017-05-08 16:50:54.816035: step 36420, loss = 0.81 (968.2 examples/sec; 0.132 sec/batch)
2017-05-08 16:50:56.104859: step 36430, loss = 0.85 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:50:57.389039: step 36440, loss = 1.04 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:50:58.681382: step 36450, loss = 0.68 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:50:59.970686: step 36460, loss = 0.73 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:51:01.271900: step 36470, loss = 0.78 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:51:02.549403: step 36480, loss = 0.88 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:51:03.850515: step 36490, loss = 0.92 (983.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:51:05.229343: step 36500, loss = 0.74 (928.3 examples/sec; 0.138 sec/batch)
2017-05-08 16:51:06.426956: step 36510, loss = 0.74 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-08 16:51:07.690160: step 36520, loss = 0.77 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-08 16:51:08.983792: step 36530, loss = 0.89 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:51:10.281732: step 36540, loss = 0.82 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:51:11.586304: step 36550, loss = 0.87 (981.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:51:12.889162: step 36560, loss = 0.79 (982.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:51:14.165304: step 36570, loss = 0.96 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:51:15.415961: step 36580, loss = 0.88 (1023.5 examples/sec; 0.125 sec/batch)
2017-05-08 16:51:16.726405: step 36590, loss = 0.96 (976.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:51:18.105769: step 36600, loss = 0.83 (928.0 examples/sec; 0.138 sec/batch)
2017-05-08 16:51:19.323893: step 36610, loss = 0.88 (1050.8 examples/sec; 0.122 sec/batch)
2017-05-08 16:51:20.625985: step 36620, loss = 0.77 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:51:21.911211: step 36630, loss = 0.93 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:51:23.196000: step 36640, loss = 0.82 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:51:24.499438: step 36650, loss = 0.74 (982.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:51:25.792089: step 36660, loss = 0.70 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:51:27.073679: step 36670, loss = 0.60 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:51:28.334272: step 36680, loss = 0.91 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-08 16:51:29.625225: step 36690, loss = 1.01 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:51:30.983479: step 36700, loss = 0.62 (942.4 examples/sec; 0.136 sec/batch)
2017-05-08 16:51:32.186960: step 36710, loss = 0.83 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-08 16:51:33.472098: step 36720, loss = 0.78 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:51:34.744551: step 36730, loss = 0.82 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:51:36.032987: step 36740, loss = 0.67 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:51:37.329421: step 36750, loss = 0.72 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:51:38.605712: step 36760, loss = 0.81 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:51:39.889641: step 36770, loss = 0.65 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:51:41.168289: step 36780, loss = 0.79 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:51:42.462472: step 36790, loss = 0.82 (989.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:51:43.845975: step 36800, loss = 0.85 (925.2 examples/sec; 0.138 sec/batch)
2017-05-08 16:51:45.035227: step 36810, loss = 0.78 (1076.3 examples/sec; 0.119 sec/batch)
2017-05-08 16:51:46.321331: step 36820, loss = 0.86 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:51:47.590696: step 36830, loss = 0.75 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:51:48.863714: step 36840, loss = 0.73 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:51:50.149500: step 36850, loss = 0.94 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:51:51.437008: step 36860, loss = 0.91 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:51:52.729950: step 36870, loss = 0.69 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:51:54.006900: step 36880, loss = 0.81 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:51:55.309315: step 36890, loss = 0.97 (982.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:51:56.708624: step 36900, loss = 0.85 (914.7 examples/sec; 0.140 sec/batch)
2017-05-08 16:51:57.910151: step 36910, loss = 0.90 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-08 16:51:59.201079: step 36920, loss = 0.77 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:52:00.464808: step 36930, loss = 0.71 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-08 16:52:01.752371: step 36940, loss = 0.71 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:52:03.048296: step 36950, loss = 0.86 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:52:04.311932: step 36960, loss = 0.77 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-08 16:52:05.581838: step 36970, loss = 0.85 (1008.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:52:06.874487: step 36980, loss = 1.00 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:52:08.194405: step 36990, loss = 1.09 (969.8 examples/sec; 0.132 sec/batch)
2017-05-08 16:52:09.573776: step 37000, loss = 0.72 (928.0 examples/sec; 0.138 sec/batch)
2017-05-08 16:52:10.771886: step 37010, loss = 0.88 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-08 16:52:12.053536: step 37020, loss = 0.81 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:52:13.350313: step 37030, loss = 0.89 (987.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:52:14.665692: step 37040, loss = 0.75 (973.1 examples/sec; 0.132 sec/batch)
2017-05-08 16:52:15.948774: step 37050, loss = 0.89 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:52:17.240116: step 37060, loss = 0.80 (991.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:52:18.523994: step 37070, loss = 0.77 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:52:19.797990: step 37080, loss = 0.83 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:52:21.085502: step 37090, loss = 0.68 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:52:22.461479: step 37100, loss = 0.66 (930.3 examples/sec; 0.138 sec/batch)
2017-05-08 16:52:23.643934: step 37110, loss = 0.99 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-08 16:52:24.942773: step 37120, loss = 0.95 (985.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:52:26.214076: step 37130, loss = 0.89 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:52:27.503931: step 37140, loss = 0.61 (992.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:52:28.810253: step 37150, loss = 0.80 (979.9 examples/sec; 0.131 sec/batch)
2017-05-08 16:52:30.111621: step 37160, loss = 1.00 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:52:31.417218: step 37170, loss = 0.80 (980.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:52:32.686629: step 37180, loss = 0.74 (1008.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:52:34.026616: step 37190, loss = 0.80 (955.2 examples/sec; 0.134 sec/batch)
2017-05-08 16:52:35.415227: step 37200, loss = 0.64 (921.8 examples/sec; 0.139 sec/batch)
2017-05-08 16:52:36.612958: step 37210, loss = 0.80 (1068.7 examples/sec; 0.120 sec/batch)
2017-05-08 16:52:37.875151: step 37220, loss = 0.66 (1014.1 examples/sec; 0.126 sec/batch)
2017-05-08 16:52:39.132941: step 37230, loss = 0.94 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-08 16:52:40.418242: step 37240, loss = 0.95 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:52:41.696907: step 37250, loss = 0.75 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:52:42.964106: step 37260, loss = 0.68 (1010.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:52:44.252436: step 37270, loss = 0.92 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:52:45.560975: step 37280, loss = 0.76 (978.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:52:46.855265: step 37290, loss = 0.76 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:52:48.208188: step 37300, loss = 0.88 (946.1 examples/sec; 0.135 sec/batch)
2017-05-08 16:52:49.424124: step 37310, loss = 0.83 (1052.7 examples/sec; 0.122 sec/batch)
2017-05-08 16:52:50.719055: step 37320, loss = 0.91 (988.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:52:52.003577: step 37330, loss = 0.90 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:52:53.319100: step 37340, loss = 0.78 (973.0 examples/sec; 0.132 sec/batch)
2017-05-08 16:52:54.636808: step 37350, loss = 0.77 (971.4 examples/sec; 0.132 sec/batch)
2017-05-08 16:52:55.934102: step 37360, loss = 0.88 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:52:57.239810: step 37370, loss = 0.82 (980.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:52:58.534532: step 37380, loss = 0.90 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:52:59.830693: step 37390, loss = 0.79 (987.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:53:01.232607: step 37400, loss = 0.78 (913.0 examples/sec; 0.140 sec/batch)
2017-05-08 16:53:02.423119: step 37410, loss = 0.94 (1075.2 examples/sec; 0.119 sec/batch)
2017-05-08 16:53:03.736847: step 37420, loss = 0.91 (974.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:53:05.037343: step 37430, loss = 0.83 (984.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:53:06.337518: step 37440, loss = 0.81 (984.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:53:07.649719: step 37450, loss = 1.00 (975.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:53:08.936862: step 37460, loss = 0.77 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:53:10.226291: step 37470, loss = 0.77 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:53:11.523407: step 37480, loss = 0.75 (986.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:53:12.819537: step 37490, loss = 0.87 (987.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:53:14.215362: step 37500, loss = 0.76 (917.0 examples/sec; 0.140 sec/batch)
2017-05-08 16:53:15.441044: step 37510, loss = 0.85 (1044.3 examples/sec; 0.123 sec/batch)
2017-05-08 16:53:16.736101: step 37520, loss = 0.77 (988.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:53:18.018101: step 37530, loss = 0.96 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:53:19.295213: step 37540, loss = 0.85 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:53:20.586775: step 37550, loss = 0.71 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:53:21.889018: step 37560, loss = 1.08 (982.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:53:23.202403: step 37570, loss = 0.89 (974.6 examples/sec; 0.131 sec/batch)
2017-05-08 16:53:24.468056: step 37580, loss = 0.82 (1011.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:53:25.743399: step 37590, loss = 0.78 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:53:27.144673: step 37600, loss = 0.73 (913.5 examples/sec; 0.140 sec/batch)
2017-05-08 16:53:28.335816: step 37610, loss = 0.72 (1074.6 examples/sec; 0.119 sec/batch)
2017-05-08 16:53:29.638791: step 37620, loss = 0.83 (982.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:53:30.949661: step 37630, loss = 0.93 (976.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:53:32.222932: step 37640, loss = 0.89 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:53:33.546172: step 37650, loss = 0.72 (967.3 examples/sec; 0.132 sec/batch)
2017-05-08 16:53:34.846854: step 37660, loss = 0.79 (984.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:53:36.115688: step 37670, loss = 0.67 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:53:37.399410: step 37680, loss = 0.82 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:53:38.685678: step 37690, loss = 0.77 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:53:40.074349: step 37700, loss = 0.66 (921.7 examples/sec; 0.139 sec/batch)
2017-05-08 16:53:41.309550: step 37710, loss = 0.83 (1036.3 examples/sec; 0.124 sec/batch)
2017-05-08 16:53:42.593046: step 37720, loss = 0.87 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:53:43.868930: step 37730, loss = 0.82 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:53:45.151913: step 37740, loss = 0.76 (997.7 examples/sec; E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 769 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
0.128 sec/batch)
2017-05-08 16:53:46.434149: step 37750, loss = 0.80 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:53:47.719310: step 37760, loss = 0.80 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:53:48.992583: step 37770, loss = 0.84 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:53:50.252293: step 37780, loss = 0.71 (1016.1 examples/sec; 0.126 sec/batch)
2017-05-08 16:53:51.517737: step 37790, loss = 0.86 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:53:52.890466: step 37800, loss = 1.00 (932.5 examples/sec; 0.137 sec/batch)
2017-05-08 16:53:54.092470: step 37810, loss = 0.73 (1064.9 examples/sec; 0.120 sec/batch)
2017-05-08 16:53:55.376259: step 37820, loss = 0.91 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:53:56.651281: step 37830, loss = 0.94 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:53:57.930000: step 37840, loss = 0.64 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:53:59.209908: step 37850, loss = 0.85 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:54:00.493057: step 37860, loss = 0.95 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:54:01.800115: step 37870, loss = 0.89 (979.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:54:03.086724: step 37880, loss = 0.96 (994.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:54:04.385718: step 37890, loss = 0.83 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:54:05.747525: step 37900, loss = 0.75 (939.9 examples/sec; 0.136 sec/batch)
2017-05-08 16:54:06.952545: step 37910, loss = 0.92 (1062.2 examples/sec; 0.121 sec/batch)
2017-05-08 16:54:08.245049: step 37920, loss = 0.83 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:54:09.564689: step 37930, loss = 0.67 (970.0 examples/sec; 0.132 sec/batch)
2017-05-08 16:54:10.872853: step 37940, loss = 0.71 (978.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:54:12.186632: step 37950, loss = 0.70 (974.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:54:13.483380: step 37960, loss = 0.91 (987.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:54:14.801984: step 37970, loss = 0.85 (970.7 examples/sec; 0.132 sec/batch)
2017-05-08 16:54:16.079014: step 37980, loss = 0.81 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:54:17.402735: step 37990, loss = 0.97 (967.0 examples/sec; 0.132 sec/batch)
2017-05-08 16:54:18.804141: step 38000, loss = 0.94 (913.4 examples/sec; 0.140 sec/batch)
2017-05-08 16:54:19.999155: step 38010, loss = 0.83 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-08 16:54:21.282260: step 38020, loss = 0.88 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:54:22.595757: step 38030, loss = 0.76 (974.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:54:23.860428: step 38040, loss = 0.70 (1012.1 examples/sec; 0.126 sec/batch)
2017-05-08 16:54:25.179416: step 38050, loss = 0.75 (970.5 examples/sec; 0.132 sec/batch)
2017-05-08 16:54:26.442998: step 38060, loss = 0.85 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-08 16:54:27.722734: step 38070, loss = 0.86 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:54:28.989441: step 38080, loss = 0.67 (1010.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:54:30.256534: step 38090, loss = 0.76 (1010.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:54:31.596909: step 38100, loss = 0.66 (955.0 examples/sec; 0.134 sec/batch)
2017-05-08 16:54:32.785512: step 38110, loss = 0.95 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-08 16:54:34.072174: step 38120, loss = 0.96 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:54:35.357610: step 38130, loss = 0.79 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:54:36.635694: step 38140, loss = 0.85 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:54:37.969199: step 38150, loss = 0.76 (959.9 examples/sec; 0.133 sec/batch)
2017-05-08 16:54:39.237351: step 38160, loss = 0.73 (1009.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:54:40.541685: step 38170, loss = 0.84 (981.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:54:41.850306: step 38180, loss = 0.96 (978.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:54:43.118865: step 38190, loss = 0.76 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-08 16:54:44.503642: step 38200, loss = 0.79 (924.3 examples/sec; 0.138 sec/batch)
2017-05-08 16:54:45.715224: step 38210, loss = 0.82 (1056.5 examples/sec; 0.121 sec/batch)
2017-05-08 16:54:47.001132: step 38220, loss = 0.82 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:54:48.302303: step 38230, loss = 0.87 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:54:49.595286: step 38240, loss = 0.70 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:54:50.900188: step 38250, loss = 0.88 (980.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:54:52.205371: step 38260, loss = 0.73 (980.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:54:53.508571: step 38270, loss = 0.91 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:54:54.806951: step 38280, loss = 0.77 (985.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:54:56.091319: step 38290, loss = 0.89 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:54:57.479157: step 38300, loss = 0.84 (922.3 examples/sec; 0.139 sec/batch)
2017-05-08 16:54:58.700435: step 38310, loss = 0.70 (1048.1 examples/sec; 0.122 sec/batch)
2017-05-08 16:54:59.992901: step 38320, loss = 1.04 (990.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:55:01.299923: step 38330, loss = 0.72 (979.3 examples/sec; 0.131 sec/batch)
2017-05-08 16:55:02.622562: step 38340, loss = 0.63 (967.8 examples/sec; 0.132 sec/batch)
2017-05-08 16:55:03.902539: step 38350, loss = 0.71 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:55:05.176582: step 38360, loss = 0.91 (1004.7 examples/sec; 0.127 sec/batch)
2017-05-08 16:55:06.456374: step 38370, loss = 0.79 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:55:07.719293: step 38380, loss = 0.81 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-08 16:55:09.011008: step 38390, loss = 0.82 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:55:10.377874: step 38400, loss = 0.91 (936.5 examples/sec; 0.137 sec/batch)
2017-05-08 16:55:11.553507: step 38410, loss = 0.75 (1088.8 examples/sec; 0.118 sec/batch)
2017-05-08 16:55:12.811094: step 38420, loss = 0.82 (1017.8 examples/sec; 0.126 sec/batch)
2017-05-08 16:55:14.104632: step 38430, loss = 0.70 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:55:15.381552: step 38440, loss = 0.98 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:55:16.673855: step 38450, loss = 0.94 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:55:17.967193: step 38460, loss = 0.73 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:55:19.245661: step 38470, loss = 0.67 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:55:20.514312: step 38480, loss = 0.84 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:55:21.796026: step 38490, loss = 0.68 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:55:23.182268: step 38500, loss = 0.71 (923.4 examples/sec; 0.139 sec/batch)
2017-05-08 16:55:24.370964: step 38510, loss = 0.71 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-08 16:55:25.669337: step 38520, loss = 0.77 (985.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:55:26.961910: step 38530, loss = 0.94 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:55:28.243297: step 38540, loss = 0.70 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:55:29.530980: step 38550, loss = 0.86 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:55:30.806831: step 38560, loss = 0.99 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:55:32.074822: step 38570, loss = 0.88 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:55:33.367420: step 38580, loss = 0.78 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:55:34.670491: step 38590, loss = 0.82 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:55:36.076286: step 38600, loss = 0.99 (910.5 examples/sec; 0.141 sec/batch)
2017-05-08 16:55:37.293524: step 38610, loss = 0.86 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-08 16:55:38.567856: step 38620, loss = 0.81 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:55:39.855603: step 38630, loss = 0.71 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:55:41.121850: step 38640, loss = 0.87 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:55:42.406941: step 38650,E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 789 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
 loss = 0.84 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:55:43.724643: step 38660, loss = 1.00 (971.4 examples/sec; 0.132 sec/batch)
2017-05-08 16:55:45.034122: step 38670, loss = 0.97 (977.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:55:46.356242: step 38680, loss = 0.73 (968.1 examples/sec; 0.132 sec/batch)
2017-05-08 16:55:47.665897: step 38690, loss = 0.81 (977.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:55:49.055361: step 38700, loss = 0.85 (921.2 examples/sec; 0.139 sec/batch)
2017-05-08 16:55:50.263788: step 38710, loss = 0.79 (1059.2 examples/sec; 0.121 sec/batch)
2017-05-08 16:55:51.528250: step 38720, loss = 0.80 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-08 16:55:52.815576: step 38730, loss = 0.81 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:55:54.078198: step 38740, loss = 0.88 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-08 16:55:55.370229: step 38750, loss = 0.82 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:55:56.655541: step 38760, loss = 0.79 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:55:57.959111: step 38770, loss = 0.97 (981.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:55:59.280628: step 38780, loss = 0.81 (968.6 examples/sec; 0.132 sec/batch)
2017-05-08 16:56:00.590923: step 38790, loss = 0.92 (976.9 examples/sec; 0.131 sec/batch)
2017-05-08 16:56:01.965156: step 38800, loss = 0.74 (931.4 examples/sec; 0.137 sec/batch)
2017-05-08 16:56:03.174595: step 38810, loss = 0.74 (1058.4 examples/sec; 0.121 sec/batch)
2017-05-08 16:56:04.478373: step 38820, loss = 0.83 (981.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:56:05.791881: step 38830, loss = 0.75 (974.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:56:07.067550: step 38840, loss = 0.86 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:56:08.332138: step 38850, loss = 0.83 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 16:56:09.615987: step 38860, loss = 0.87 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 16:56:10.887336: step 38870, loss = 0.81 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:56:12.176816: step 38880, loss = 0.81 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:56:13.478168: step 38890, loss = 1.00 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:56:14.861258: step 38900, loss = 0.93 (925.5 examples/sec; 0.138 sec/batch)
2017-05-08 16:56:16.049876: step 38910, loss = 0.81 (1076.9 examples/sec; 0.119 sec/batch)
2017-05-08 16:56:17.347185: step 38920, loss = 0.88 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:56:18.665112: step 38930, loss = 0.88 (971.2 examples/sec; 0.132 sec/batch)
2017-05-08 16:56:19.954984: step 38940, loss = 0.73 (992.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:56:21.259223: step 38950, loss = 0.93 (981.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:56:22.552057: step 38960, loss = 0.84 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:56:23.826615: step 38970, loss = 0.84 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:56:25.096187: step 38980, loss = 0.86 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:56:26.364671: step 38990, loss = 0.77 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-08 16:56:27.735167: step 39000, loss = 0.69 (934.0 examples/sec; 0.137 sec/batch)
2017-05-08 16:56:28.927807: step 39010, loss = 0.81 (1073.3 examples/sec; 0.119 sec/batch)
2017-05-08 16:56:30.221749: step 39020, loss = 0.86 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:56:31.490103: step 39030, loss = 0.89 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:56:32.781860: step 39040, loss = 0.98 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:56:34.086811: step 39050, loss = 0.76 (980.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:56:35.351897: step 39060, loss = 0.81 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:56:36.658480: step 39070, loss = 0.83 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:56:37.939415: step 39080, loss = 0.73 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:56:39.221878: step 39090, loss = 0.79 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:56:40.600086: step 39100, loss = 0.91 (928.7 examples/sec; 0.138 sec/batch)
2017-05-08 16:56:41.769599: step 39110, loss = 0.73 (1094.5 examples/sec; 0.117 sec/batch)
2017-05-08 16:56:43.054691: step 39120, loss = 0.90 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:56:44.350662: step 39130, loss = 0.72 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:56:45.637098: step 39140, loss = 0.77 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:56:46.950356: step 39150, loss = 0.92 (974.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:56:48.268763: step 39160, loss = 0.85 (970.9 examples/sec; 0.132 sec/batch)
2017-05-08 16:56:49.568581: step 39170, loss = 0.87 (984.8 examples/sec; 0.130 sec/batch)
2017-05-08 16:56:50.888923: step 39180, loss = 0.81 (969.4 examples/sec; 0.132 sec/batch)
2017-05-08 16:56:52.166165: step 39190, loss = 0.64 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:56:53.529557: step 39200, loss = 0.75 (938.8 examples/sec; 0.136 sec/batch)
2017-05-08 16:56:54.715212: step 39210, loss = 0.83 (1079.6 examples/sec; 0.119 sec/batch)
2017-05-08 16:56:55.993487: step 39220, loss = 0.72 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:56:57.301224: step 39230, loss = 0.76 (978.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:56:58.611620: step 39240, loss = 0.87 (976.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:56:59.899383: step 39250, loss = 0.96 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:57:01.217730: step 39260, loss = 0.64 (970.9 examples/sec; 0.132 sec/batch)
2017-05-08 16:57:02.512828: step 39270, loss = 0.80 (988.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:57:03.801791: step 39280, loss = 0.79 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:57:05.088667: step 39290, loss = 0.82 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:57:06.462226: step 39300, loss = 0.80 (931.9 examples/sec; 0.137 sec/batch)
2017-05-08 16:57:07.636174: step 39310, loss = 0.82 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-08 16:57:08.912788: step 39320, loss = 0.75 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:57:10.199199: step 39330, loss = 0.72 (995.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:57:11.459826: step 39340, loss = 0.85 (1015.4 examples/sec; 0.126 sec/batch)
2017-05-08 16:57:12.744416: step 39350, loss = 0.74 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:57:14.024245: step 39360, loss = 0.92 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:57:15.306973: step 39370, loss = 0.67 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:57:16.613480: step 39380, loss = 0.83 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:57:17.939379: step 39390, loss = 0.68 (965.4 examples/sec; 0.133 sec/batch)
2017-05-08 16:57:19.335568: step 39400, loss = 0.75 (916.8 examples/sec; 0.140 sec/batch)
2017-05-08 16:57:20.527302: step 39410, loss = 0.64 (1074.1 examples/sec; 0.119 sec/batch)
2017-05-08 16:57:21.810010: step 39420, loss = 0.86 (997.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:57:23.083160: step 39430, loss = 0.78 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:57:24.344694: step 39440, loss = 1.04 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-08 16:57:25.629570: step 39450, loss = 0.82 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:57:26.917802: step 39460, loss = 0.76 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:57:28.225249: step 39470, loss = 0.90 (979.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:57:29.511200: step 39480, loss = 0.78 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 16:57:30.808574: step 39490, loss = 1.00 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:57:32.190077: step 39500, loss = 0.85 (926.5 examples/sec; 0.138 sec/batch)
2017-05-08 16:57:33.390203: step 39510, loss = 0.98 (1066.6 examples/sec; 0.120 sec/batch)
2017-05-08 16:57:34.668274: step 39520, loss = 0.79 (1001.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:57:35.934121: step 39530, loss = 0.71 (1011.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:57:37.222986: step 39540, loss = 0.74 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:57:38.524241: step 39550, loss = 0.79 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 16E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 809 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
:57:39.822890: step 39560, loss = 0.85 (985.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:57:41.115813: step 39570, loss = 0.81 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:57:42.412806: step 39580, loss = 0.70 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:57:43.684972: step 39590, loss = 0.99 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-08 16:57:45.098083: step 39600, loss = 0.85 (905.8 examples/sec; 0.141 sec/batch)
2017-05-08 16:57:46.280668: step 39610, loss = 0.78 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-08 16:57:47.564589: step 39620, loss = 1.01 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:57:48.869663: step 39630, loss = 0.66 (980.8 examples/sec; 0.131 sec/batch)
2017-05-08 16:57:50.190847: step 39640, loss = 0.63 (968.8 examples/sec; 0.132 sec/batch)
2017-05-08 16:57:51.482603: step 39650, loss = 0.85 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:57:52.776551: step 39660, loss = 0.83 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:57:54.047131: step 39670, loss = 0.84 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-08 16:57:55.326655: step 39680, loss = 0.78 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:57:56.587140: step 39690, loss = 0.75 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-08 16:57:57.967236: step 39700, loss = 0.91 (927.5 examples/sec; 0.138 sec/batch)
2017-05-08 16:57:59.151145: step 39710, loss = 0.93 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-08 16:58:00.433084: step 39720, loss = 0.82 (998.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:58:01.757525: step 39730, loss = 0.77 (966.4 examples/sec; 0.132 sec/batch)
2017-05-08 16:58:03.060610: step 39740, loss = 0.76 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:58:04.363202: step 39750, loss = 0.94 (982.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:58:05.669783: step 39760, loss = 0.89 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:58:06.991436: step 39770, loss = 0.78 (968.5 examples/sec; 0.132 sec/batch)
2017-05-08 16:58:08.271501: step 39780, loss = 0.58 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:58:09.553118: step 39790, loss = 0.74 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 16:58:10.967902: step 39800, loss = 0.77 (904.7 examples/sec; 0.141 sec/batch)
2017-05-08 16:58:12.165545: step 39810, loss = 0.79 (1068.8 examples/sec; 0.120 sec/batch)
2017-05-08 16:58:13.455824: step 39820, loss = 0.92 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:58:14.753466: step 39830, loss = 0.78 (986.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:58:16.029096: step 39840, loss = 0.87 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 16:58:17.311292: step 39850, loss = 0.93 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:58:18.598726: step 39860, loss = 0.72 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:58:19.882687: step 39870, loss = 0.81 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:58:21.179431: step 39880, loss = 0.69 (987.1 examples/sec; 0.130 sec/batch)
2017-05-08 16:58:22.502179: step 39890, loss = 0.81 (967.7 examples/sec; 0.132 sec/batch)
2017-05-08 16:58:23.896173: step 39900, loss = 1.06 (918.2 examples/sec; 0.139 sec/batch)
2017-05-08 16:58:25.108924: step 39910, loss = 0.98 (1055.5 examples/sec; 0.121 sec/batch)
2017-05-08 16:58:26.436511: step 39920, loss = 0.93 (964.2 examples/sec; 0.133 sec/batch)
2017-05-08 16:58:27.741914: step 39930, loss = 0.87 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:58:28.999551: step 39940, loss = 0.79 (1017.8 examples/sec; 0.126 sec/batch)
2017-05-08 16:58:30.279755: step 39950, loss = 0.73 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:58:31.558110: step 39960, loss = 0.93 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:58:32.833101: step 39970, loss = 0.82 (1003.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:58:34.114561: step 39980, loss = 0.82 (998.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:58:35.411855: step 39990, loss = 0.61 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 16:58:36.783545: step 40000, loss = 0.82 (933.2 examples/sec; 0.137 sec/batch)
2017-05-08 16:58:37.976599: step 40010, loss = 0.86 (1072.9 examples/sec; 0.119 sec/batch)
2017-05-08 16:58:39.258133: step 40020, loss = 0.70 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:58:40.532604: step 40030, loss = 0.58 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:58:41.823076: step 40040, loss = 0.70 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 16:58:43.104588: step 40050, loss = 0.86 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:58:44.420614: step 40060, loss = 0.77 (972.6 examples/sec; 0.132 sec/batch)
2017-05-08 16:58:45.708979: step 40070, loss = 0.75 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 16:58:47.001687: step 40080, loss = 0.73 (990.2 examples/sec; 0.129 sec/batch)
2017-05-08 16:58:48.305628: step 40090, loss = 0.89 (981.6 examples/sec; 0.130 sec/batch)
2017-05-08 16:58:49.682058: step 40100, loss = 0.95 (929.9 examples/sec; 0.138 sec/batch)
2017-05-08 16:58:50.904628: step 40110, loss = 0.80 (1047.0 examples/sec; 0.122 sec/batch)
2017-05-08 16:58:52.207854: step 40120, loss = 1.01 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:58:53.496504: step 40130, loss = 0.78 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 16:58:54.770458: step 40140, loss = 0.76 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:58:56.053862: step 40150, loss = 0.75 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:58:57.362114: step 40160, loss = 0.89 (978.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:58:58.668940: step 40170, loss = 0.77 (979.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:58:59.972542: step 40180, loss = 0.69 (981.9 examples/sec; 0.130 sec/batch)
2017-05-08 16:59:01.283286: step 40190, loss = 0.88 (976.5 examples/sec; 0.131 sec/batch)
2017-05-08 16:59:02.663436: step 40200, loss = 0.77 (927.4 examples/sec; 0.138 sec/batch)
2017-05-08 16:59:03.856687: step 40210, loss = 0.79 (1072.7 examples/sec; 0.119 sec/batch)
2017-05-08 16:59:05.132220: step 40220, loss = 0.70 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 16:59:06.424262: step 40230, loss = 0.83 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 16:59:07.705822: step 40240, loss = 0.79 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 16:59:08.992707: step 40250, loss = 0.76 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:59:10.264261: step 40260, loss = 0.85 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-08 16:59:11.528275: step 40270, loss = 0.70 (1012.6 examples/sec; 0.126 sec/batch)
2017-05-08 16:59:12.827438: step 40280, loss = 0.69 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 16:59:14.139257: step 40290, loss = 0.75 (975.7 examples/sec; 0.131 sec/batch)
2017-05-08 16:59:15.502859: step 40300, loss = 0.67 (938.7 examples/sec; 0.136 sec/batch)
2017-05-08 16:59:16.724826: step 40310, loss = 1.07 (1047.5 examples/sec; 0.122 sec/batch)
2017-05-08 16:59:18.035064: step 40320, loss = 1.08 (976.9 examples/sec; 0.131 sec/batch)
2017-05-08 16:59:19.325940: step 40330, loss = 0.67 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 16:59:20.604596: step 40340, loss = 0.73 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:59:21.883050: step 40350, loss = 0.83 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 16:59:23.167074: step 40360, loss = 0.90 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:59:24.478481: step 40370, loss = 0.88 (976.1 examples/sec; 0.131 sec/batch)
2017-05-08 16:59:25.757041: step 40380, loss = 0.77 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 16:59:27.028465: step 40390, loss = 0.81 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-08 16:59:28.389491: step 40400, loss = 0.60 (940.5 examples/sec; 0.136 sec/batch)
2017-05-08 16:59:29.586059: step 40410, loss = 0.61 (1069.7 examples/sec; 0.120 sec/batch)
2017-05-08 16:59:30.883555: step 40420, loss = 0.96 (986.5 examples/sec; 0.130 sec/batch)
2017-05-08 16:59:32.160668: step 40430, loss = 0.72 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 16:59:33.447009: step 40440, loss = 0.79 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 16:59:34.713272: step 40450, loss = 0.73 (1010.9 examples/sec; 0.127 sec/batch)
2017-05-08 16:59:35.994267: step 40460, loss = 0.84 (999.2 examples/sec; 0.12E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 829 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
8 sec/batch)
2017-05-08 16:59:37.290555: step 40470, loss = 0.80 (987.4 examples/sec; 0.130 sec/batch)
2017-05-08 16:59:38.563775: step 40480, loss = 0.80 (1005.3 examples/sec; 0.127 sec/batch)
2017-05-08 16:59:39.836772: step 40490, loss = 0.76 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 16:59:41.201730: step 40500, loss = 0.70 (937.8 examples/sec; 0.136 sec/batch)
2017-05-08 16:59:42.430732: step 40510, loss = 0.92 (1041.5 examples/sec; 0.123 sec/batch)
2017-05-08 16:59:43.733837: step 40520, loss = 0.99 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 16:59:45.035967: step 40530, loss = 0.87 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 16:59:46.348184: step 40540, loss = 0.82 (975.4 examples/sec; 0.131 sec/batch)
2017-05-08 16:59:47.624927: step 40550, loss = 0.86 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-08 16:59:48.902497: step 40560, loss = 0.75 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 16:59:50.215382: step 40570, loss = 0.88 (975.0 examples/sec; 0.131 sec/batch)
2017-05-08 16:59:51.523868: step 40580, loss = 0.92 (978.2 examples/sec; 0.131 sec/batch)
2017-05-08 16:59:52.840190: step 40590, loss = 0.98 (972.4 examples/sec; 0.132 sec/batch)
2017-05-08 16:59:54.228901: step 40600, loss = 0.87 (921.7 examples/sec; 0.139 sec/batch)
2017-05-08 16:59:55.463713: step 40610, loss = 0.82 (1036.6 examples/sec; 0.123 sec/batch)
2017-05-08 16:59:56.757894: step 40620, loss = 0.81 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 16:59:58.043326: step 40630, loss = 1.04 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 16:59:59.322865: step 40640, loss = 0.69 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:00:00.622331: step 40650, loss = 0.81 (985.0 examples/sec; 0.130 sec/batch)
2017-05-08 17:00:01.960581: step 40660, loss = 0.71 (956.5 examples/sec; 0.134 sec/batch)
2017-05-08 17:00:03.270153: step 40670, loss = 0.96 (977.4 examples/sec; 0.131 sec/batch)
2017-05-08 17:00:04.524187: step 40680, loss = 0.85 (1020.7 examples/sec; 0.125 sec/batch)
2017-05-08 17:00:05.829971: step 40690, loss = 0.84 (980.3 examples/sec; 0.131 sec/batch)
2017-05-08 17:00:07.228591: step 40700, loss = 0.68 (915.2 examples/sec; 0.140 sec/batch)
2017-05-08 17:00:08.438013: step 40710, loss = 0.83 (1058.3 examples/sec; 0.121 sec/batch)
2017-05-08 17:00:09.719120: step 40720, loss = 1.03 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:00:11.008725: step 40730, loss = 0.70 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:00:12.314210: step 40740, loss = 0.76 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 17:00:13.631698: step 40750, loss = 0.72 (971.5 examples/sec; 0.132 sec/batch)
2017-05-08 17:00:14.911434: step 40760, loss = 0.77 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 17:00:16.204933: step 40770, loss = 0.89 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:00:17.495138: step 40780, loss = 0.69 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:00:18.803180: step 40790, loss = 0.68 (978.6 examples/sec; 0.131 sec/batch)
2017-05-08 17:00:20.182339: step 40800, loss = 0.87 (928.1 examples/sec; 0.138 sec/batch)
2017-05-08 17:00:21.374563: step 40810, loss = 0.84 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-08 17:00:22.658984: step 40820, loss = 0.87 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 17:00:23.951935: step 40830, loss = 0.90 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:00:25.283103: step 40840, loss = 0.89 (961.6 examples/sec; 0.133 sec/batch)
2017-05-08 17:00:26.571435: step 40850, loss = 0.73 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:00:27.848966: step 40860, loss = 0.89 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 17:00:29.119481: step 40870, loss = 0.92 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 17:00:30.404277: step 40880, loss = 0.99 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 17:00:31.697975: step 40890, loss = 0.90 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 17:00:33.076568: step 40900, loss = 0.79 (928.5 examples/sec; 0.138 sec/batch)
2017-05-08 17:00:34.266353: step 40910, loss = 0.84 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-08 17:00:35.538592: step 40920, loss = 0.80 (1006.1 examples/sec; 0.127 sec/batch)
2017-05-08 17:00:36.815661: step 40930, loss = 1.10 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:00:38.099281: step 40940, loss = 0.78 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 17:00:39.389383: step 40950, loss = 0.79 (992.2 examples/sec; 0.129 sec/batch)
2017-05-08 17:00:40.690039: step 40960, loss = 0.75 (984.1 examples/sec; 0.130 sec/batch)
2017-05-08 17:00:42.000829: step 40970, loss = 0.72 (976.5 examples/sec; 0.131 sec/batch)
2017-05-08 17:00:43.310236: step 40980, loss = 0.87 (977.5 examples/sec; 0.131 sec/batch)
2017-05-08 17:00:44.590685: step 40990, loss = 0.78 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 17:00:45.961187: step 41000, loss = 0.85 (934.0 examples/sec; 0.137 sec/batch)
2017-05-08 17:00:47.173335: step 41010, loss = 0.87 (1056.0 examples/sec; 0.121 sec/batch)
2017-05-08 17:00:48.464199: step 41020, loss = 0.79 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:00:49.767284: step 41030, loss = 0.69 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 17:00:51.072860: step 41040, loss = 0.83 (980.4 examples/sec; 0.131 sec/batch)
2017-05-08 17:00:52.336844: step 41050, loss = 0.86 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-08 17:00:53.641613: step 41060, loss = 0.91 (981.0 examples/sec; 0.130 sec/batch)
2017-05-08 17:00:54.917223: step 41070, loss = 0.67 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:00:56.202986: step 41080, loss = 0.75 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:00:57.483265: step 41090, loss = 0.69 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 17:00:58.873138: step 41100, loss = 0.72 (921.0 examples/sec; 0.139 sec/batch)
2017-05-08 17:01:00.079666: step 41110, loss = 0.83 (1060.9 examples/sec; 0.121 sec/batch)
2017-05-08 17:01:01.395486: step 41120, loss = 0.96 (972.8 examples/sec; 0.132 sec/batch)
2017-05-08 17:01:02.716615: step 41130, loss = 0.66 (968.8 examples/sec; 0.132 sec/batch)
2017-05-08 17:01:04.014621: step 41140, loss = 0.79 (986.1 examples/sec; 0.130 sec/batch)
2017-05-08 17:01:05.296234: step 41150, loss = 0.88 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:01:06.580704: step 41160, loss = 1.10 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 17:01:07.883675: step 41170, loss = 0.78 (982.4 examples/sec; 0.130 sec/batch)
2017-05-08 17:01:09.199561: step 41180, loss = 0.94 (972.7 examples/sec; 0.132 sec/batch)
2017-05-08 17:01:10.513706: step 41190, loss = 0.62 (974.0 examples/sec; 0.131 sec/batch)
2017-05-08 17:01:11.892883: step 41200, loss = 0.77 (928.1 examples/sec; 0.138 sec/batch)
2017-05-08 17:01:13.087501: step 41210, loss = 0.74 (1071.5 examples/sec; 0.119 sec/batch)
2017-05-08 17:01:14.355202: step 41220, loss = 0.68 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 17:01:15.649164: step 41230, loss = 0.81 (989.2 examples/sec; 0.129 sec/batch)
2017-05-08 17:01:16.937980: step 41240, loss = 0.77 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 17:01:18.228771: step 41250, loss = 0.80 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:01:19.551552: step 41260, loss = 0.86 (967.7 examples/sec; 0.132 sec/batch)
2017-05-08 17:01:20.845304: step 41270, loss = 0.75 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 17:01:22.138388: step 41280, loss = 0.73 (989.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:01:23.434356: step 41290, loss = 1.19 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 17:01:24.833701: step 41300, loss = 0.90 (914.7 examples/sec; 0.140 sec/batch)
2017-05-08 17:01:26.036295: step 41310, loss = 0.72 (1064.4 examples/sec; 0.120 sec/batch)
2017-05-08 17:01:27.343434: step 41320, loss = 0.76 (979.2 examples/sec; 0.131 sec/batch)
2017-05-08 17:01:28.619643: step 41330, loss = 0.73 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 17:01:29.908692: step 41340, loss = 0.86 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:01:31.205911: step 41350, loss = 0.75 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 17:01:32.501560: step 41360, loss = 0.86 (987.9 examples/sec; 0.130 sec/batch)
2017-05-08 17:01:33.775932: step 41370, loss = 0.75 (10E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 849 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
04.4 examples/sec; 0.127 sec/batch)
2017-05-08 17:01:35.048295: step 41380, loss = 0.68 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-08 17:01:36.322801: step 41390, loss = 0.72 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 17:01:37.695730: step 41400, loss = 0.64 (932.3 examples/sec; 0.137 sec/batch)
2017-05-08 17:01:38.878193: step 41410, loss = 0.87 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-08 17:01:40.162940: step 41420, loss = 0.82 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:01:41.440327: step 41430, loss = 0.82 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 17:01:42.733770: step 41440, loss = 0.64 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:01:44.015091: step 41450, loss = 1.15 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 17:01:45.312836: step 41460, loss = 0.87 (986.3 examples/sec; 0.130 sec/batch)
2017-05-08 17:01:46.577871: step 41470, loss = 0.75 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-08 17:01:47.836108: step 41480, loss = 0.65 (1017.3 examples/sec; 0.126 sec/batch)
2017-05-08 17:01:49.112914: step 41490, loss = 0.92 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 17:01:50.526614: step 41500, loss = 0.94 (905.4 examples/sec; 0.141 sec/batch)
2017-05-08 17:01:51.718941: step 41510, loss = 0.73 (1073.5 examples/sec; 0.119 sec/batch)
2017-05-08 17:01:53.001939: step 41520, loss = 0.70 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:01:54.288114: step 41530, loss = 0.68 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 17:01:55.566664: step 41540, loss = 0.80 (1001.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:01:56.842417: step 41550, loss = 0.83 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:01:58.127295: step 41560, loss = 0.96 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 17:01:59.394093: step 41570, loss = 0.88 (1010.4 examples/sec; 0.127 sec/batch)
2017-05-08 17:02:00.673268: step 41580, loss = 0.64 (1000.6 examples/sec; 0.128 sec/batch)
2017-05-08 17:02:01.964167: step 41590, loss = 0.70 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:02:03.346689: step 41600, loss = 0.82 (925.8 examples/sec; 0.138 sec/batch)
2017-05-08 17:02:04.567618: step 41610, loss = 0.96 (1048.4 examples/sec; 0.122 sec/batch)
2017-05-08 17:02:05.876434: step 41620, loss = 0.79 (978.0 examples/sec; 0.131 sec/batch)
2017-05-08 17:02:07.160391: step 41630, loss = 0.74 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 17:02:08.445509: step 41640, loss = 0.81 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:02:09.724327: step 41650, loss = 0.79 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 17:02:11.013719: step 41660, loss = 0.84 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 17:02:12.283375: step 41670, loss = 0.79 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-08 17:02:13.560064: step 41680, loss = 0.76 (1002.6 examples/sec; 0.128 sec/batch)
2017-05-08 17:02:14.821742: step 41690, loss = 0.82 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-08 17:02:16.189096: step 41700, loss = 0.90 (936.1 examples/sec; 0.137 sec/batch)
2017-05-08 17:02:17.420883: step 41710, loss = 0.94 (1039.1 examples/sec; 0.123 sec/batch)
2017-05-08 17:02:18.716862: step 41720, loss = 0.78 (987.7 examples/sec; 0.130 sec/batch)
2017-05-08 17:02:20.014299: step 41730, loss = 0.88 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 17:02:21.325809: step 41740, loss = 0.76 (976.0 examples/sec; 0.131 sec/batch)
2017-05-08 17:02:22.632492: step 41750, loss = 0.91 (979.6 examples/sec; 0.131 sec/batch)
2017-05-08 17:02:23.900195: step 41760, loss = 0.67 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 17:02:25.177228: step 41770, loss = 0.84 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:02:26.455708: step 41780, loss = 0.57 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-08 17:02:27.722326: step 41790, loss = 0.75 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 17:02:29.113680: step 41800, loss = 0.79 (920.0 examples/sec; 0.139 sec/batch)
2017-05-08 17:02:30.317172: step 41810, loss = 0.74 (1063.6 examples/sec; 0.120 sec/batch)
2017-05-08 17:02:31.567881: step 41820, loss = 0.74 (1023.4 examples/sec; 0.125 sec/batch)
2017-05-08 17:02:32.825010: step 41830, loss = 0.82 (1018.2 examples/sec; 0.126 sec/batch)
2017-05-08 17:02:34.104695: step 41840, loss = 0.66 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-08 17:02:35.406833: step 41850, loss = 0.82 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 17:02:36.706867: step 41860, loss = 0.88 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 17:02:37.983885: step 41870, loss = 0.76 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:02:39.281635: step 41880, loss = 0.73 (986.3 examples/sec; 0.130 sec/batch)
2017-05-08 17:02:40.581747: step 41890, loss = 0.82 (984.5 examples/sec; 0.130 sec/batch)
2017-05-08 17:02:41.944862: step 41900, loss = 0.78 (939.0 examples/sec; 0.136 sec/batch)
2017-05-08 17:02:43.139284: step 41910, loss = 0.88 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-08 17:02:44.429410: step 41920, loss = 0.95 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:02:45.751001: step 41930, loss = 0.79 (968.5 examples/sec; 0.132 sec/batch)
2017-05-08 17:02:47.037782: step 41940, loss = 0.90 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 17:02:48.353829: step 41950, loss = 0.74 (972.6 examples/sec; 0.132 sec/batch)
2017-05-08 17:02:49.638353: step 41960, loss = 0.87 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 17:02:50.941425: step 41970, loss = 0.86 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 17:02:52.248757: step 41980, loss = 0.74 (979.1 examples/sec; 0.131 sec/batch)
2017-05-08 17:02:53.577099: step 41990, loss = 0.61 (963.6 examples/sec; 0.133 sec/batch)
2017-05-08 17:02:54.952273: step 42000, loss = 0.69 (930.8 examples/sec; 0.138 sec/batch)
2017-05-08 17:02:56.118495: step 42010, loss = 0.86 (1097.6 examples/sec; 0.117 sec/batch)
2017-05-08 17:02:57.419729: step 42020, loss = 0.82 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 17:02:58.740949: step 42030, loss = 0.91 (968.8 examples/sec; 0.132 sec/batch)
2017-05-08 17:03:00.068442: step 42040, loss = 0.88 (964.2 examples/sec; 0.133 sec/batch)
2017-05-08 17:03:01.349517: step 42050, loss = 0.78 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 17:03:02.604581: step 42060, loss = 0.90 (1019.9 examples/sec; 0.126 sec/batch)
2017-05-08 17:03:03.881415: step 42070, loss = 0.77 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 17:03:05.176038: step 42080, loss = 0.63 (988.7 examples/sec; 0.129 sec/batch)
2017-05-08 17:03:06.438012: step 42090, loss = 0.70 (1014.3 examples/sec; 0.126 sec/batch)
2017-05-08 17:03:07.826541: step 42100, loss = 0.83 (921.8 examples/sec; 0.139 sec/batch)
2017-05-08 17:03:09.024671: step 42110, loss = 0.89 (1068.3 examples/sec; 0.120 sec/batch)
2017-05-08 17:03:10.312874: step 42120, loss = 0.73 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:03:11.617977: step 42130, loss = 0.82 (980.8 examples/sec; 0.131 sec/batch)
2017-05-08 17:03:12.905433: step 42140, loss = 0.78 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 17:03:14.176833: step 42150, loss = 0.75 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-08 17:03:15.501898: step 42160, loss = 0.71 (966.0 examples/sec; 0.133 sec/batch)
2017-05-08 17:03:16.805561: step 42170, loss = 0.76 (981.9 examples/sec; 0.130 sec/batch)
2017-05-08 17:03:18.110435: step 42180, loss = 0.97 (980.9 examples/sec; 0.130 sec/batch)
2017-05-08 17:03:19.407800: step 42190, loss = 0.62 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 17:03:20.779691: step 42200, loss = 0.83 (933.0 examples/sec; 0.137 sec/batch)
2017-05-08 17:03:21.980244: step 42210, loss = 0.63 (1066.2 examples/sec; 0.120 sec/batch)
2017-05-08 17:03:23.270741: step 42220, loss = 0.97 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:03:24.563314: step 42230, loss = 0.78 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 17:03:25.847832: step 42240, loss = 0.69 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 17:03:27.128717: step 42250, loss = 0.76 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:03:28.434432: step 42260, loss = 0.66 (980.3 examples/sec; 0.131 sec/batch)
2017-05-08 17:03:29.756559: step 42270, loss = 0.90 (968.1 examples/sec; 0.132 sec/batch)
2017-05-08 17:03:31.061808: step 42280, loss = 0.76 (980.7 examples/sec; 0.131 sec/batch)
2017-05-08 17:03:32.362819: step 42290, loss = 0.91 (983.9 examples/sec; 0.130 sec/batch)
2017-05-08 17:03:33.765964: step 42300, loss = 0.80 (912.2 examples/sec; 0.140 sec/batch)
2017-05-08 17:03:34.960847: step 42310, loss = 0.75 (1071.2 examples/sec; 0.119 sec/batch)
2017-05-08 17:03:36.262609: step 42320, loss = 0.89 (983.3 examples/sec; 0.130 sec/batch)
2017-05-08 17:03:37.549936: step 42330, loss = 0.74 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 17:03:38.817656: step 42340, loss = 0.91 (1009.7 examples/sec; 0.127 sec/batch)
2017-05-08 17:03:40.093823: step 42350, loss = 0.75 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 17:03:41.401975: step 42360, loss = 0.81 (978.5 examples/sec; 0.131 sec/batch)
2017-05-08 17:03:42.669413: step 42370, loss = 0.84 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-08 17:03:43.985510: step 42380, loss = 0.90 (972.6 examples/sec; 0.132 sec/batch)
2017-05-08 17:03:45.265690: step 42390, loss = 0.89 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 17:03:46.644138: step 42400, loss = 0.96 (928.6 examples/sec; 0.138 sec/batch)
2017-05-08 17:03:47.858038: step 42410, loss = 0.67 (1054.5 examples/sec; 0.121 sec/batch)
2017-05-08 17:03:49.125437: step 42420, loss = 0.63 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-08 17:03:50.401781: step 42430, loss = 0.98 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 17:03:51.677508: step 42440, loss = 0.81 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:03:52.967232: step 42450, loss = 0.81 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:03:54.241140: step 42460, loss = 0.70 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 17:03:55.519420: step 42470, loss = 0.75 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:03:56.807205: step 42480, loss = 0.87 (994.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:03:58.097369: step 42490, loss = 0.86 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:03:59.469784: step 42500, loss = 0.73 (932.7 examples/sec; 0.137 sec/batch)
2017-05-08 17:04:00.668380: step 42510, loss = 0.88 (1067.9 examples/sec; 0.120 sec/batch)
2017-05-08 17:04:01.999632: step 42520, loss = 0.79 (961.5 examples/sec; 0.133 sec/batch)
2017-05-08 17:04:03.292870: step 42530, loss = 0.79 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:04:04.575884: step 42540, loss = 0.70 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 17:04:05.856229: step 42550, loss = 0.97 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:04:07.142996: step 42560, loss = 0.85 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 17:04:08.443175: step 42570, loss = 0.90 (984.5 examples/sec; 0.130 sec/batch)
2017-05-08 17:04:09.733380: step 42580, loss = 0.84 (992.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:04:11.032300: step 42590, loss = 0.69 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 17:04:12.417635: step 42600, loss = 0.78 (924.0 examples/sec; 0.139 sec/batch)
2017-05-08 17:04:13.642607: step 42610, loss = 1.00 (1044.9 examples/sec; 0.122 sec/batch)
2017-05-08 17:04:14.927109: step 42620, loss = 0.74 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 17:04:16.200792: step 42630, loss = 0.83 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-08 17:04:17.485474: step 42640, loss = 0.88 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:04:18.747048: step 42650, loss = 0.75 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-08 17:04:20.039505: step 42660, loss = 0.80 (990.4 examples/sec; 0.129 sec/batch)
2017-05-08 17:04:21.313137: step 42670, loss = 0.82 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-08 17:04:22.616872: step 42680, loss = 0.83 (981.8 examples/sec; 0.130 sec/batch)
2017-05-08 17:04:23.918196: step 42690, loss = 0.91 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 17:04:25.283552: step 42700, loss = 0.73 (937.5 examples/sec; 0.137 sec/batch)
2017-05-08 17:04:26.467034: step 42710, loss = 0.83 (1081.6 examples/sec; 0.118 sec/batch)
2017-05-08 17:04:27.760392: step 42720, loss = 0.66 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 17:04:29.047153: step 42730, loss = 0.73E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 870 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
 (994.7 examples/sec; 0.129 sec/batch)
2017-05-08 17:04:30.346548: step 42740, loss = 0.77 (985.1 examples/sec; 0.130 sec/batch)
2017-05-08 17:04:31.632544: step 42750, loss = 0.71 (995.3 examples/sec; 0.129 sec/batch)
2017-05-08 17:04:32.910377: step 42760, loss = 0.93 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:04:34.179188: step 42770, loss = 0.71 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-08 17:04:35.467569: step 42780, loss = 0.63 (993.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:04:36.767312: step 42790, loss = 0.89 (984.8 examples/sec; 0.130 sec/batch)
2017-05-08 17:04:38.149912: step 42800, loss = 0.95 (925.8 examples/sec; 0.138 sec/batch)
2017-05-08 17:04:39.327863: step 42810, loss = 0.83 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-08 17:04:40.598739: step 42820, loss = 0.68 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 17:04:41.874009: step 42830, loss = 0.83 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:04:43.161532: step 42840, loss = 0.71 (994.2 examples/sec; 0.129 sec/batch)
2017-05-08 17:04:44.447127: step 42850, loss = 0.71 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:04:45.724374: step 42860, loss = 0.99 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-08 17:04:47.000957: step 42870, loss = 0.86 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:04:48.293781: step 42880, loss = 0.71 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:04:49.615501: step 42890, loss = 0.86 (968.4 examples/sec; 0.132 sec/batch)
2017-05-08 17:04:50.971704: step 42900, loss = 0.69 (943.8 examples/sec; 0.136 sec/batch)
2017-05-08 17:04:52.191538: step 42910, loss = 0.93 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-08 17:04:53.539272: step 42920, loss = 0.66 (949.7 examples/sec; 0.135 sec/batch)
2017-05-08 17:04:54.796215: step 42930, loss = 0.76 (1018.3 examples/sec; 0.126 sec/batch)
2017-05-08 17:04:56.091695: step 42940, loss = 0.86 (988.0 examples/sec; 0.130 sec/batch)
2017-05-08 17:04:57.390716: step 42950, loss = 0.72 (985.4 examples/sec; 0.130 sec/batch)
2017-05-08 17:04:58.783258: step 42960, loss = 0.88 (919.2 examples/sec; 0.139 sec/batch)
2017-05-08 17:05:00.060336: step 42970, loss = 0.66 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:05:01.339840: step 42980, loss = 0.69 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:05:02.668327: step 42990, loss = 0.83 (963.5 examples/sec; 0.133 sec/batch)
2017-05-08 17:05:04.075077: step 43000, loss = 0.81 (909.9 examples/sec; 0.141 sec/batch)
2017-05-08 17:05:05.368426: step 43010, loss = 0.86 (989.7 examples/sec; 0.129 sec/batch)
2017-05-08 17:05:06.676099: step 43020, loss = 0.89 (978.8 examples/sec; 0.131 sec/batch)
2017-05-08 17:05:07.998684: step 43030, loss = 0.79 (967.8 examples/sec; 0.132 sec/batch)
2017-05-08 17:05:09.279788: step 43040, loss = 0.76 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:05:10.581008: step 43050, loss = 0.72 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 17:05:11.856191: step 43060, loss = 0.84 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 17:05:13.137979: step 43070, loss = 0.73 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 17:05:14.359608: step 43080, loss = 0.71 (1047.8 examples/sec; 0.122 sec/batch)
2017-05-08 17:05:15.690260: step 43090, loss = 0.95 (961.9 examples/sec; 0.133 sec/batch)
2017-05-08 17:05:17.067936: step 43100, loss = 0.73 (929.1 examples/sec; 0.138 sec/batch)
2017-05-08 17:05:18.260721: step 43110, loss = 0.95 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-08 17:05:19.569182: step 43120, loss = 0.75 (978.3 examples/sec; 0.131 sec/batch)
2017-05-08 17:05:20.899514: step 43130, loss = 0.73 (962.2 examples/sec; 0.133 sec/batch)
2017-05-08 17:05:22.196854: step 43140, loss = 0.77 (986.6 examples/sec; 0.130 sec/batch)
2017-05-08 17:05:23.477750: step 43150, loss = 0.79 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:05:24.841814: step 43160, loss = 0.60 (938.4 examples/sec; 0.136 sec/batch)
2017-05-08 17:05:26.149349: step 43170, loss = 0.89 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 17:05:27.449969: step 43180, loss = 0.79 (984.1 examples/sec; 0.130 sec/batch)
2017-05-08 17:05:28.754135: step 43190, loss = 0.76 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 17:05:30.112759: step 43200, loss = 0.77 (942.1 examples/sec; 0.136 sec/batch)
2017-05-08 17:05:31.328410: step 43210, loss = 0.54 (1052.9 examples/sec; 0.122 sec/batch)
2017-05-08 17:05:32.608413: step 43220, loss = 0.71 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 17:05:33.876405: step 43230, loss = 0.89 (1009.5 examples/sec; 0.127 sec/batch)
2017-05-08 17:05:35.182897: step 43240, loss = 0.67 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 17:05:36.500388: step 43250, loss = 0.57 (971.5 examples/sec; 0.132 sec/batch)
2017-05-08 17:05:37.813067: step 43260, loss = 0.66 (975.1 examples/sec; 0.131 sec/batch)
2017-05-08 17:05:39.097585: step 43270, loss = 0.82 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 17:05:40.353200: step 43280, loss = 0.61 (1019.4 examples/sec; 0.126 sec/batch)
2017-05-08 17:05:41.662264: step 43290, loss = 0.87 (977.8 examples/sec; 0.131 sec/batch)
2017-05-08 17:05:43.117295: step 43300, loss = 0.67 (879.7 examples/sec; 0.146 sec/batch)
2017-05-08 17:05:44.344292: step 43310, loss = 0.98 (1043.2 examples/sec; 0.123 sec/batch)
2017-05-08 17:05:45.628251: step 43320, loss = 0.58 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 17:05:46.912250: step 43330, loss = 0.83 (996.9 examples/sec; 0.128 sec/batch)
2017-05-08 17:05:48.191136: step 43340, loss = 0.81 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 17:05:49.510560: step 43350, loss = 0.73 (970.1 examples/sec; 0.132 sec/batch)
2017-05-08 17:05:50.784216: step 43360, loss = 0.73 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-08 17:05:52.041409: step 43370, loss = 0.84 (1018.1 examples/sec; 0.126 sec/batch)
2017-05-08 17:05:53.339098: step 43380, loss = 0.84 (986.4 examples/sec; 0.130 sec/batch)
2017-05-08 17:05:54.620023: step 43390, loss = 0.80 (999.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:05:55.994304: step 43400, loss = 0.75 (931.4 examples/sec; 0.137 sec/batch)
2017-05-08 17:05:57.189017: step 43410, loss = 0.79 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-08 17:05:58.481051: step 43420, loss = 0.62 (990.7 examples/sec; 0.129 sec/batch)
2017-05-08 17:05:59.739064: step 43430, loss = 0.77 (1017.5 examples/sec; 0.126 sec/batch)
2017-05-08 17:06:01.039298: step 43440, loss = 0.84 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 17:06:02.330427: step 43450, loss = 0.72 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 17:06:03.601606: step 43460, loss = 0.81 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 17:06:04.882661: step 43470, loss = 0.74 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 17:06:06.186291: step 43480, loss = 0.84 (981.9 examples/sec; 0.130 sec/batch)
2017-05-08 17:06:07.489703: step 43490, loss = 0.78 (982.0 examples/sec; 0.130 sec/batch)
2017-05-08 17:06:08.880753: step 43500, loss = 0.79 (920.2 examples/sec; 0.139 sec/batch)
2017-05-08 17:06:10.067680: step 43510, loss = 0.67 (1078.4 examples/sec; 0.119 sec/batch)
2017-05-08 17:06:11.365851: step 43520, loss = 0.85 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 17:06:12.652559: step 43530, loss = 0.91 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:06:13.930315: step 43540, loss = 0.68 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 17:06:15.207758: step 43550, loss = 0.78 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-08 17:06:16.488082: step 43560, loss = 0.74 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 17:06:17.765739: step 43570, loss = 0.73 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 17:06:19.030274: step 43580, loss = 0.74 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-08 17:06:20.341867: step 43590, loss = 0.78 (975.9 examples/sec; 0.131 sec/batch)
2017-05-08 17:06:21.743013: step 43600, loss = 0.83 (913.5 examples/sec; 0.140 sec/batch)
2017-05-08 17:06:22.904201: step 43610, loss = 0.90 (1102.3 examples/sec; 0.116 sec/batch)
2017-05-08 17:06:24.187292: step 43620, loss = 0.86 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 17:06:25.481811: step 43630, loss = 0.83 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:06:26.77E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 890 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
0262: step 43640, loss = 0.56 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 17:06:28.055907: step 43650, loss = 0.75 (995.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:06:29.349401: step 43660, loss = 0.78 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:06:30.652161: step 43670, loss = 0.93 (982.5 examples/sec; 0.130 sec/batch)
2017-05-08 17:06:31.935535: step 43680, loss = 0.86 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:06:33.238743: step 43690, loss = 0.77 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 17:06:34.647983: step 43700, loss = 0.79 (908.3 examples/sec; 0.141 sec/batch)
2017-05-08 17:06:35.846320: step 43710, loss = 0.96 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-08 17:06:37.159300: step 43720, loss = 0.88 (974.9 examples/sec; 0.131 sec/batch)
2017-05-08 17:06:38.430232: step 43730, loss = 0.99 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 17:06:39.684208: step 43740, loss = 1.00 (1020.8 examples/sec; 0.125 sec/batch)
2017-05-08 17:06:40.949047: step 43750, loss = 0.78 (1012.0 examples/sec; 0.126 sec/batch)
2017-05-08 17:06:42.225854: step 43760, loss = 0.70 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 17:06:43.517625: step 43770, loss = 0.76 (990.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:06:44.770284: step 43780, loss = 0.59 (1021.8 examples/sec; 0.125 sec/batch)
2017-05-08 17:06:46.062404: step 43790, loss = 0.80 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:06:47.411974: step 43800, loss = 0.92 (948.4 examples/sec; 0.135 sec/batch)
2017-05-08 17:06:48.596920: step 43810, loss = 0.56 (1080.2 examples/sec; 0.118 sec/batch)
2017-05-08 17:06:49.865580: step 43820, loss = 0.65 (1008.9 examples/sec; 0.127 sec/batch)
2017-05-08 17:06:51.148816: step 43830, loss = 0.79 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 17:06:52.416323: step 43840, loss = 0.77 (1009.9 examples/sec; 0.127 sec/batch)
2017-05-08 17:06:53.702609: step 43850, loss = 1.05 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:06:55.021479: step 43860, loss = 0.77 (970.5 examples/sec; 0.132 sec/batch)
2017-05-08 17:06:56.314031: step 43870, loss = 0.62 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 17:06:57.623171: step 43880, loss = 0.73 (977.7 examples/sec; 0.131 sec/batch)
2017-05-08 17:06:58.897108: step 43890, loss = 0.72 (1004.8 examples/sec; 0.127 sec/batch)
2017-05-08 17:07:00.256187: step 43900, loss = 0.66 (941.8 examples/sec; 0.136 sec/batch)
2017-05-08 17:07:01.445571: step 43910, loss = 0.75 (1076.2 examples/sec; 0.119 sec/batch)
2017-05-08 17:07:02.716823: step 43920, loss = 0.81 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 17:07:03.989857: step 43930, loss = 0.82 (1005.5 examples/sec; 0.127 sec/batch)
2017-05-08 17:07:05.281357: step 43940, loss = 0.73 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:07:06.577090: step 43950, loss = 0.72 (987.8 examples/sec; 0.130 sec/batch)
2017-05-08 17:07:07.850280: step 43960, loss = 0.80 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 17:07:09.128556: step 43970, loss = 0.83 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:07:10.421768: step 43980, loss = 0.80 (989.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:07:11.695988: step 43990, loss = 0.82 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 17:07:13.073958: step 44000, loss = 0.77 (928.9 examples/sec; 0.138 sec/batch)
2017-05-08 17:07:14.264180: step 44010, loss = 0.82 (1075.4 examples/sec; 0.119 sec/batch)
2017-05-08 17:07:15.548482: step 44020, loss = 0.99 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:07:16.852722: step 44030, loss = 0.76 (981.4 examples/sec; 0.130 sec/batch)
2017-05-08 17:07:18.163044: step 44040, loss = 0.78 (976.9 examples/sec; 0.131 sec/batch)
2017-05-08 17:07:19.413503: step 44050, loss = 0.79 (1023.6 examples/sec; 0.125 sec/batch)
2017-05-08 17:07:20.671142: step 44060, loss = 0.85 (1017.8 examples/sec; 0.126 sec/batch)
2017-05-08 17:07:21.934120: step 44070, loss = 0.72 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-08 17:07:23.210619: step 44080, loss = 0.80 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:07:24.491643: step 44090, loss = 0.76 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 17:07:25.865875: step 44100, loss = 0.65 (931.4 examples/sec; 0.137 sec/batch)
2017-05-08 17:07:27.048987: step 44110, loss = 0.88 (1081.9 examples/sec; 0.118 sec/batch)
2017-05-08 17:07:28.351659: step 44120, loss = 0.94 (982.6 examples/sec; 0.130 sec/batch)
2017-05-08 17:07:29.659308: step 44130, loss = 0.81 (978.9 examples/sec; 0.131 sec/batch)
2017-05-08 17:07:30.961493: step 44140, loss = 0.99 (983.0 examples/sec; 0.130 sec/batch)
2017-05-08 17:07:32.277677: step 44150, loss = 0.74 (972.5 examples/sec; 0.132 sec/batch)
2017-05-08 17:07:33.574847: step 44160, loss = 0.71 (986.8 examples/sec; 0.130 sec/batch)
2017-05-08 17:07:34.867425: step 44170, loss = 0.83 (990.3 examples/sec; 0.129 sec/batch)
2017-05-08 17:07:36.177905: step 44180, loss = 0.72 (976.8 examples/sec; 0.131 sec/batch)
2017-05-08 17:07:37.466357: step 44190, loss = 0.65 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 17:07:38.844272: step 44200, loss = 0.92 (928.9 examples/sec; 0.138 sec/batch)
2017-05-08 17:07:40.061769: step 44210, loss = 0.94 (1051.3 examples/sec; 0.122 sec/batch)
2017-05-08 17:07:41.357863: step 44220, loss = 0.84 (987.6 examples/sec; 0.130 sec/batch)
2017-05-08 17:07:42.657691: step 44230, loss = 0.72 (984.8 examples/sec; 0.130 sec/batch)
2017-05-08 17:07:43.951291: step 44240, loss = 0.90 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:07:45.246753: step 44250, loss = 0.72 (988.1 examples/sec; 0.130 sec/batch)
2017-05-08 17:07:46.519926: step 44260, loss = 0.83 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 17:07:47.821464: step 44270, loss = 0.72 (983.5 examples/sec; 0.130 sec/batch)
2017-05-08 17:07:49.127075: step 44280, loss = 0.80 (980.4 examples/sec; 0.131 sec/batch)
2017-05-08 17:07:50.407102: step 44290, loss = 0.76 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 17:07:51.801415: step 44300, loss = 0.73 (918.0 examples/sec; 0.139 sec/batch)
2017-05-08 17:07:53.022779: step 44310, loss = 0.87 (1048.0 examples/sec; 0.122 sec/batch)
2017-05-08 17:07:54.311527: step 44320, loss = 0.72 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 17:07:55.594673: step 44330, loss = 0.74 (997.5 examples/sec; 0.128 sec/batch)
2017-05-08 17:07:56.891720: step 44340, loss = 0.99 (986.9 examples/sec; 0.130 sec/batch)
2017-05-08 17:07:58.208293: step 44350, loss = 0.84 (972.2 examples/sec; 0.132 sec/batch)
2017-05-08 17:07:59.467006: step 44360, loss = 0.73 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-08 17:08:00.754381: step 44370, loss = 0.80 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 17:08:02.040546: step 44380, loss = 0.72 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 17:08:03.304438: step 44390, loss = 0.87 (1012.8 examples/sec; 0.126 sec/batch)
2017-05-08 17:08:04.698561: step 44400, loss = 0.74 (918.1 examples/sec; 0.139 sec/batch)
2017-05-08 17:08:05.901859: step 44410, loss = 0.90 (1063.7 examples/sec; 0.120 sec/batch)
2017-05-08 17:08:07.200339: step 44420, loss = 0.76 (985.8 examples/sec; 0.130 sec/batch)
2017-05-08 17:08:08.514286: step 44430, loss = 0.80 (974.2 examples/sec; 0.131 sec/batch)
2017-05-08 17:08:09.824365: step 44440, loss = 0.73 (977.0 examples/sec; 0.131 sec/batch)
2017-05-08 17:08:11.125685: step 44450, loss = 0.81 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 17:08:12.414600: step 44460, loss = 0.86 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:08:13.695385: step 44470, loss = 0.92 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:08:14.999067: step 44480, loss = 0.73 (981.8 examples/sec; 0.130 sec/batch)
2017-05-08 17:08:16.285693: step 44490, loss = 0.65 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:08:17.671252: step 44500, loss = 0.89 (923.8 examples/sec; 0.139 sec/batch)
2017-05-08 17:08:18.898602: step 44510, loss = 0.69 (1042.9 examples/sec; 0.123 sec/batch)
2017-05-08 17:08:20.181555: step 44520, loss = 0.70 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:08:21.485420: step 44530, loss = 0.72 (981.7 examples/sec; 0.130 sec/batch)
2017-05-08 17:08:22.754597: step 44540, loss = 1.03 (1008.5 examples/sec; 0.127 sec/E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 910 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
batch)
2017-05-08 17:08:24.006275: step 44550, loss = 0.79 (1022.6 examples/sec; 0.125 sec/batch)
2017-05-08 17:08:25.278649: step 44560, loss = 0.88 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-08 17:08:26.581381: step 44570, loss = 0.61 (982.6 examples/sec; 0.130 sec/batch)
2017-05-08 17:08:27.862893: step 44580, loss = 0.86 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 17:08:29.141160: step 44590, loss = 0.69 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:08:30.501799: step 44600, loss = 0.76 (940.7 examples/sec; 0.136 sec/batch)
2017-05-08 17:08:31.681525: step 44610, loss = 0.97 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-08 17:08:32.955629: step 44620, loss = 0.76 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 17:08:34.248524: step 44630, loss = 0.75 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:08:35.533553: step 44640, loss = 0.78 (996.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:08:36.823240: step 44650, loss = 0.96 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:08:38.114485: step 44660, loss = 0.68 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 17:08:39.406008: step 44670, loss = 0.73 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:08:40.701129: step 44680, loss = 0.82 (988.3 examples/sec; 0.130 sec/batch)
2017-05-08 17:08:41.978421: step 44690, loss = 0.76 (1002.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:08:43.340956: step 44700, loss = 0.77 (939.4 examples/sec; 0.136 sec/batch)
2017-05-08 17:08:44.533719: step 44710, loss = 0.62 (1073.2 examples/sec; 0.119 sec/batch)
2017-05-08 17:08:45.840679: step 44720, loss = 0.63 (979.4 examples/sec; 0.131 sec/batch)
2017-05-08 17:08:47.123141: step 44730, loss = 0.75 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:08:48.392843: step 44740, loss = 0.73 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-08 17:08:49.663001: step 44750, loss = 0.62 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-08 17:08:50.944132: step 44760, loss = 0.72 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:08:52.213788: step 44770, loss = 0.89 (1008.2 examples/sec; 0.127 sec/batch)
2017-05-08 17:08:53.506712: step 44780, loss = 0.75 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:08:54.820248: step 44790, loss = 0.88 (974.4 examples/sec; 0.131 sec/batch)
2017-05-08 17:08:56.192347: step 44800, loss = 0.71 (932.9 examples/sec; 0.137 sec/batch)
2017-05-08 17:08:57.381039: step 44810, loss = 0.66 (1076.8 examples/sec; 0.119 sec/batch)
2017-05-08 17:08:58.668308: step 44820, loss = 0.85 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 17:08:59.947915: step 44830, loss = 0.94 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:09:01.234137: step 44840, loss = 0.66 (995.2 examples/sec; 0.129 sec/batch)
2017-05-08 17:09:02.515170: step 44850, loss = 0.89 (999.2 examples/sec; 0.128 sec/batch)
2017-05-08 17:09:03.787671: step 44860, loss = 0.80 (1005.9 examples/sec; 0.127 sec/batch)
2017-05-08 17:09:05.097994: step 44870, loss = 0.89 (976.9 examples/sec; 0.131 sec/batch)
2017-05-08 17:09:06.408924: step 44880, loss = 0.81 (976.4 examples/sec; 0.131 sec/batch)
2017-05-08 17:09:07.722065: step 44890, loss = 0.88 (974.8 examples/sec; 0.131 sec/batch)
2017-05-08 17:09:09.097189: step 44900, loss = 0.75 (930.8 examples/sec; 0.138 sec/batch)
2017-05-08 17:09:10.292222: step 44910, loss = 0.61 (1071.1 examples/sec; 0.120 sec/batch)
2017-05-08 17:09:11.565957: step 44920, loss = 0.76 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-08 17:09:12.844800: step 44930, loss = 0.86 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 17:09:14.134265: step 44940, loss = 1.25 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 17:09:15.449763: step 44950, loss = 0.75 (973.0 examples/sec; 0.132 sec/batch)
2017-05-08 17:09:16.739281: step 44960, loss = 0.71 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:09:18.021702: step 44970, loss = 0.77 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:09:19.301150: step 44980, loss = 0.66 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:09:20.584905: step 44990, loss = 0.81 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:09:21.940219: step 45000, loss = 0.79 (944.4 examples/sec; 0.136 sec/batch)
2017-05-08 17:09:23.142325: step 45010, loss = 0.73 (1064.8 examples/sec; 0.120 sec/batch)
2017-05-08 17:09:24.426381: step 45020, loss = 0.85 (996.8 examples/sec; 0.128 sec/batch)
2017-05-08 17:09:25.708917: step 45030, loss = 0.79 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 17:09:26.999274: step 45040, loss = 0.81 (992.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:09:28.270804: step 45050, loss = 0.81 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 17:09:29.537370: step 45060, loss = 0.72 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 17:09:30.815110: step 45070, loss = 0.79 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 17:09:32.098986: step 45080, loss = 0.87 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 17:09:33.383420: step 45090, loss = 0.78 (996.6 examples/sec; 0.128 sec/batch)
2017-05-08 17:09:34.763915: step 45100, loss = 0.83 (927.2 examples/sec; 0.138 sec/batch)
2017-05-08 17:09:35.974381: step 45110, loss = 0.85 (1057.4 examples/sec; 0.121 sec/batch)
2017-05-08 17:09:37.291041: step 45120, loss = 0.93 (972.2 examples/sec; 0.132 sec/batch)
2017-05-08 17:09:38.566904: step 45130, loss = 0.69 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-08 17:09:39.886268: step 45140, loss = 0.75 (970.2 examples/sec; 0.132 sec/batch)
2017-05-08 17:09:41.199387: step 45150, loss = 0.80 (974.8 examples/sec; 0.131 sec/batch)
2017-05-08 17:09:42.499334: step 45160, loss = 0.76 (984.7 examples/sec; 0.130 sec/batch)
2017-05-08 17:09:43.791279: step 45170, loss = 0.88 (990.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:09:45.093740: step 45180, loss = 0.65 (982.8 examples/sec; 0.130 sec/batch)
2017-05-08 17:09:46.391581: step 45190, loss = 0.70 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 17:09:47.788192: step 45200, loss = 0.63 (916.5 examples/sec; 0.140 sec/batch)
2017-05-08 17:09:48.970698: step 45210, loss = 0.74 (1082.4 examples/sec; 0.118 sec/batch)
2017-05-08 17:09:50.250273: step 45220, loss = 0.85 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:09:51.589787: step 45230, loss = 0.75 (955.6 examples/sec; 0.134 sec/batch)
2017-05-08 17:09:52.861639: step 45240, loss = 0.82 (1006.4 examples/sec; 0.127 sec/batch)
2017-05-08 17:09:54.132616: step 45250, loss = 0.72 (1007.1 examples/sec; 0.127 sec/batch)
2017-05-08 17:09:55.394266: step 45260, loss = 0.71 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-08 17:09:56.779550: step 45270, loss = 0.73 (924.0 examples/sec; 0.139 sec/batch)
2017-05-08 17:09:58.061147: step 45280, loss = 0.87 (998.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:09:59.351594: step 45290, loss = 0.82 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:10:00.709744: step 45300, loss = 0.78 (942.5 examples/sec; 0.136 sec/batch)
2017-05-08 17:10:01.945540: step 45310, loss = 0.70 (1035.8 examples/sec; 0.124 sec/batch)
2017-05-08 17:10:03.335138: step 45320, loss = 0.87 (921.1 examples/sec; 0.139 sec/batch)
2017-05-08 17:10:04.646208: step 45330, loss = 0.75 (976.3 examples/sec; 0.131 sec/batch)
2017-05-08 17:10:05.916689: step 45340, loss = 0.94 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 17:10:07.219149: step 45350, loss = 0.61 (982.8 examples/sec; 0.130 sec/batch)
2017-05-08 17:10:08.482891: step 45360, loss = 0.70 (1012.9 examples/sec; 0.126 sec/batch)
2017-05-08 17:10:09.766701: step 45370, loss = 0.80 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 17:10:11.052557: step 45380, loss = 0.75 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:10:12.312575: step 45390, loss = 0.69 (1015.9 examples/sec; 0.126 sec/batch)
2017-05-08 17:10:13.674674: step 45400, loss = 0.61 (939.7 examples/sec; 0.136 sec/batch)
2017-05-08 17:10:14.905337: step 45410, loss = 0.97 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-08 17:10:16.212025: step 45420, loss = 1.02 (979.6 examples/sec; 0.131 sec/batch)
2017-05-08 17:10:17.486147: step 45430, loss = 0.76 (1004.6 examples/sec; 0.127 sec/batch)
2017-05-08 17:10:18.788743: step 45440, loss = 0.68 (982.7 examples/sec; 0.130 sec/batch)
2017-05-08 17:10:20.065304: step 45450, loss = 0.69E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 930 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
 (1002.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:10:21.369470: step 45460, loss = 0.79 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 17:10:22.648304: step 45470, loss = 0.91 (1000.9 examples/sec; 0.128 sec/batch)
2017-05-08 17:10:23.951704: step 45480, loss = 0.77 (982.0 examples/sec; 0.130 sec/batch)
2017-05-08 17:10:25.243035: step 45490, loss = 0.64 (991.2 examples/sec; 0.129 sec/batch)
2017-05-08 17:10:26.606409: step 45500, loss = 0.75 (938.8 examples/sec; 0.136 sec/batch)
2017-05-08 17:10:27.806736: step 45510, loss = 0.90 (1066.4 examples/sec; 0.120 sec/batch)
2017-05-08 17:10:29.102078: step 45520, loss = 0.88 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 17:10:30.386295: step 45530, loss = 0.87 (996.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:10:31.670754: step 45540, loss = 0.94 (996.5 examples/sec; 0.128 sec/batch)
2017-05-08 17:10:32.976641: step 45550, loss = 0.92 (980.2 examples/sec; 0.131 sec/batch)
2017-05-08 17:10:34.258657: step 45560, loss = 0.83 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:10:35.541671: step 45570, loss = 0.71 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:10:36.841610: step 45580, loss = 0.80 (984.7 examples/sec; 0.130 sec/batch)
2017-05-08 17:10:38.145312: step 45590, loss = 0.78 (981.8 examples/sec; 0.130 sec/batch)
2017-05-08 17:10:39.538434: step 45600, loss = 0.69 (918.8 examples/sec; 0.139 sec/batch)
2017-05-08 17:10:40.717280: step 45610, loss = 0.82 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-08 17:10:42.027995: step 45620, loss = 0.80 (976.6 examples/sec; 0.131 sec/batch)
2017-05-08 17:10:43.341198: step 45630, loss = 0.86 (974.7 examples/sec; 0.131 sec/batch)
2017-05-08 17:10:44.633509: step 45640, loss = 0.67 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:10:45.952421: step 45650, loss = 0.74 (970.5 examples/sec; 0.132 sec/batch)
2017-05-08 17:10:47.244000: step 45660, loss = 1.08 (991.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:10:48.565708: step 45670, loss = 0.79 (968.4 examples/sec; 0.132 sec/batch)
2017-05-08 17:10:49.840214: step 45680, loss = 0.62 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 17:10:51.125048: step 45690, loss = 0.69 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 17:10:52.474130: step 45700, loss = 0.89 (948.8 examples/sec; 0.135 sec/batch)
2017-05-08 17:10:53.674513: step 45710, loss = 0.67 (1066.3 examples/sec; 0.120 sec/batch)
2017-05-08 17:10:54.964121: step 45720, loss = 0.86 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:10:56.236427: step 45730, loss = 0.72 (1006.0 examples/sec; 0.127 sec/batch)
2017-05-08 17:10:57.515599: step 45740, loss = 0.93 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:10:58.805203: step 45750, loss = 0.66 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:11:00.059718: step 45760, loss = 0.90 (1020.3 examples/sec; 0.125 sec/batch)
2017-05-08 17:11:01.344978: step 45770, loss = 0.81 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:11:02.632277: step 45780, loss = 0.69 (994.3 examples/sec; 0.129 sec/batch)
2017-05-08 17:11:03.911731: step 45790, loss = 0.77 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:11:05.294557: step 45800, loss = 0.73 (925.6 examples/sec; 0.138 sec/batch)
2017-05-08 17:11:06.474380: step 45810, loss = 0.81 (1084.9 examples/sec; 0.118 sec/batch)
2017-05-08 17:11:07.784993: step 45820, loss = 0.96 (976.6 examples/sec; 0.131 sec/batch)
2017-05-08 17:11:09.093722: step 45830, loss = 0.75 (978.1 examples/sec; 0.131 sec/batch)
2017-05-08 17:11:10.372413: step 45840, loss = 0.72 (1001.0 examples/sec; 0.128 sec/batch)
2017-05-08 17:11:11.661218: step 45850, loss = 0.88 (993.2 examples/sec; 0.129 sec/batch)
2017-05-08 17:11:12.961496: step 45860, loss = 0.83 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 17:11:14.255832: step 45870, loss = 0.80 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:11:15.546411: step 45880, loss = 0.76 (991.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:11:16.836893: step 45890, loss = 0.88 (991.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:11:18.195234: step 45900, loss = 0.65 (942.3 examples/sec; 0.136 sec/batch)
2017-05-08 17:11:19.388006: step 45910, loss = 0.66 (1073.1 examples/sec; 0.119 sec/batch)
2017-05-08 17:11:20.660032: step 45920, loss = 0.95 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-08 17:11:21.959399: step 45930, loss = 0.81 (985.1 examples/sec; 0.130 sec/batch)
2017-05-08 17:11:23.248032: step 45940, loss = 0.80 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 17:11:24.555213: step 45950, loss = 0.89 (979.2 examples/sec; 0.131 sec/batch)
2017-05-08 17:11:25.842820: step 45960, loss = 0.75 (994.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:11:27.153622: step 45970, loss = 0.84 (976.5 examples/sec; 0.131 sec/batch)
2017-05-08 17:11:28.447914: step 45980, loss = 0.74 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:11:29.755396: step 45990, loss = 0.83 (979.0 examples/sec; 0.131 sec/batch)
2017-05-08 17:11:31.164770: step 46000, loss = 0.84 (908.2 examples/sec; 0.141 sec/batch)
2017-05-08 17:11:32.346941: step 46010, loss = 0.71 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-08 17:11:33.642247: step 46020, loss = 0.74 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 17:11:34.933745: step 46030, loss = 0.71 (991.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:11:36.255860: step 46040, loss = 0.57 (968.1 examples/sec; 0.132 sec/batch)
2017-05-08 17:11:37.516385: step 46050, loss = 0.89 (1015.5 examples/sec; 0.126 sec/batch)
2017-05-08 17:11:38.799354: step 46060, loss = 0.90 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:11:40.068339: step 46070, loss = 0.67 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-08 17:11:41.347273: step 46080, loss = 0.72 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 17:11:42.634515: step 46090, loss = 0.72 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 17:11:44.001987: step 46100, loss = 0.86 (936.0 examples/sec; 0.137 sec/batch)
2017-05-08 17:11:45.191934: step 46110, loss = 0.76 (1075.7 examples/sec; 0.119 sec/batch)
2017-05-08 17:11:46.472652: step 46120, loss = 0.72 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:11:47.760914: step 46130, loss = 0.61 (993.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:11:49.058768: step 46140, loss = 0.92 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 17:11:50.352275: step 46150, loss = 0.77 (989.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:11:51.632976: step 46160, loss = 0.74 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 17:11:52.903844: step 46170, loss = 0.69 (1007.2 examples/sec; 0.127 sec/batch)
2017-05-08 17:11:54.185039: step 46180, loss = 0.83 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:11:55.464399: step 46190, loss = 0.81 (1000.5 examples/sec; 0.128 sec/batch)
2017-05-08 17:11:56.891940: step 46200, loss = 0.77 (896.6 examples/sec; 0.143 sec/batch)
2017-05-08 17:11:58.104430: step 46210, loss = 0.78 (1055.7 examples/sec; 0.121 sec/batch)
2017-05-08 17:11:59.460469: step 46220, loss = 0.96 (943.9 examples/sec; 0.136 sec/batch)
2017-05-08 17:12:00.719307: step 46230, loss = 0.79 (1016.8 examples/sec; 0.126 sec/batch)
2017-05-08 17:12:01.999354: step 46240, loss = 0.77 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 17:12:03.378853: step 46250, loss = 1.05 (927.9 examples/sec; 0.138 sec/batch)
2017-05-08 17:12:04.659601: step 46260, loss = 0.76 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:12:05.950622: step 46270, loss = 0.78 (991.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:12:07.213565: step 46280, loss = 0.79 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-08 17:12:08.570031: step 46290, loss = 0.85 (943.6 examples/sec; 0.136 sec/batch)
2017-05-08 17:12:09.939963: step 46300, loss = 0.85 (934.4 examples/sec; 0.137 sec/batch)
2017-05-08 17:12:11.122405: step 46310, loss = 0.69 (1082.5 examples/sec; 0.118 sec/batch)
2017-05-08 17:12:12.397002: step 46320, loss = 0.86 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-08 17:12:13.718959: step 46330, loss = 0.76 (968.3 examples/sec; 0.132 sec/batch)
2017-05-08 17:12:15.118684: step 46340, loss = 0.92 (914.5 examples/sec; 0.140 sec/batch)
2017-05-08 17:12:16.384149: step 46350, loss = 0.75 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-08 17:12:17.67703E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 951 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
9: step 46360, loss = 0.81 (990.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:12:18.983066: step 46370, loss = 0.82 (980.1 examples/sec; 0.131 sec/batch)
2017-05-08 17:12:20.259217: step 46380, loss = 0.88 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 17:12:21.538695: step 46390, loss = 0.64 (1000.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:12:22.934171: step 46400, loss = 0.78 (917.3 examples/sec; 0.140 sec/batch)
2017-05-08 17:12:24.131223: step 46410, loss = 0.90 (1069.3 examples/sec; 0.120 sec/batch)
2017-05-08 17:12:25.438427: step 46420, loss = 0.94 (979.2 examples/sec; 0.131 sec/batch)
2017-05-08 17:12:26.727758: step 46430, loss = 0.80 (992.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:12:27.988832: step 46440, loss = 0.95 (1015.0 examples/sec; 0.126 sec/batch)
2017-05-08 17:12:29.285275: step 46450, loss = 0.75 (987.3 examples/sec; 0.130 sec/batch)
2017-05-08 17:12:30.565091: step 46460, loss = 0.87 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:12:31.824150: step 46470, loss = 0.82 (1016.6 examples/sec; 0.126 sec/batch)
2017-05-08 17:12:33.099814: step 46480, loss = 0.69 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:12:34.390333: step 46490, loss = 0.86 (991.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:12:35.755535: step 46500, loss = 0.77 (937.6 examples/sec; 0.137 sec/batch)
2017-05-08 17:12:36.954347: step 46510, loss = 0.76 (1067.7 examples/sec; 0.120 sec/batch)
2017-05-08 17:12:38.245207: step 46520, loss = 0.85 (991.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:12:39.539652: step 46530, loss = 0.77 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:12:40.814178: step 46540, loss = 0.73 (1004.3 examples/sec; 0.127 sec/batch)
2017-05-08 17:12:42.160029: step 46550, loss = 0.77 (951.1 examples/sec; 0.135 sec/batch)
2017-05-08 17:12:43.449040: step 46560, loss = 0.67 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:12:44.707680: step 46570, loss = 0.67 (1017.0 examples/sec; 0.126 sec/batch)
2017-05-08 17:12:46.017217: step 46580, loss = 0.76 (977.4 examples/sec; 0.131 sec/batch)
2017-05-08 17:12:47.313241: step 46590, loss = 0.75 (987.6 examples/sec; 0.130 sec/batch)
2017-05-08 17:12:48.754973: step 46600, loss = 1.00 (887.8 examples/sec; 0.144 sec/batch)
2017-05-08 17:12:49.949693: step 46610, loss = 0.80 (1071.4 examples/sec; 0.119 sec/batch)
2017-05-08 17:12:51.235139: step 46620, loss = 0.70 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:12:52.533266: step 46630, loss = 0.79 (986.0 examples/sec; 0.130 sec/batch)
2017-05-08 17:12:53.838986: step 46640, loss = 0.65 (980.3 examples/sec; 0.131 sec/batch)
2017-05-08 17:12:55.105573: step 46650, loss = 0.77 (1010.6 examples/sec; 0.127 sec/batch)
2017-05-08 17:12:56.374414: step 46660, loss = 0.55 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-08 17:12:57.659617: step 46670, loss = 0.78 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:12:59.013654: step 46680, loss = 0.78 (945.3 examples/sec; 0.135 sec/batch)
2017-05-08 17:13:00.288276: step 46690, loss = 0.92 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-08 17:13:01.685071: step 46700, loss = 0.89 (916.4 examples/sec; 0.140 sec/batch)
2017-05-08 17:13:02.857194: step 46710, loss = 0.85 (1092.0 examples/sec; 0.117 sec/batch)
2017-05-08 17:13:04.176892: step 46720, loss = 0.77 (969.9 examples/sec; 0.132 sec/batch)
2017-05-08 17:13:05.498356: step 46730, loss = 0.88 (968.6 examples/sec; 0.132 sec/batch)
2017-05-08 17:13:06.774538: step 46740, loss = 0.74 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 17:13:08.049744: step 46750, loss = 0.70 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 17:13:09.350022: step 46760, loss = 0.71 (984.4 examples/sec; 0.130 sec/batch)
2017-05-08 17:13:10.680372: step 46770, loss = 0.94 (962.2 examples/sec; 0.133 sec/batch)
2017-05-08 17:13:11.978676: step 46780, loss = 0.84 (985.9 examples/sec; 0.130 sec/batch)
2017-05-08 17:13:13.261201: step 46790, loss = 0.89 (998.0 examples/sec; 0.128 sec/batch)
2017-05-08 17:13:14.659318: step 46800, loss = 0.88 (915.5 examples/sec; 0.140 sec/batch)
2017-05-08 17:13:15.825609: step 46810, loss = 0.90 (1097.5 examples/sec; 0.117 sec/batch)
2017-05-08 17:13:17.132893: step 46820, loss = 0.64 (979.1 examples/sec; 0.131 sec/batch)
2017-05-08 17:13:18.459563: step 46830, loss = 0.84 (964.8 examples/sec; 0.133 sec/batch)
2017-05-08 17:13:19.777964: step 46840, loss = 0.67 (970.9 examples/sec; 0.132 sec/batch)
2017-05-08 17:13:21.059805: step 46850, loss = 0.79 (998.6 examples/sec; 0.128 sec/batch)
2017-05-08 17:13:22.335001: step 46860, loss = 0.79 (1003.8 examples/sec; 0.128 sec/batch)
2017-05-08 17:13:23.617805: step 46870, loss = 0.78 (997.8 examples/sec; 0.128 sec/batch)
2017-05-08 17:13:24.899910: step 46880, loss = 0.74 (998.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:13:26.206642: step 46890, loss = 0.73 (979.5 examples/sec; 0.131 sec/batch)
2017-05-08 17:13:27.586074: step 46900, loss = 0.80 (927.9 examples/sec; 0.138 sec/batch)
2017-05-08 17:13:28.771948: step 46910, loss = 0.73 (1079.4 examples/sec; 0.119 sec/batch)
2017-05-08 17:13:30.043294: step 46920, loss = 0.71 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-08 17:13:31.327082: step 46930, loss = 0.65 (997.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:13:32.654024: step 46940, loss = 0.93 (964.6 examples/sec; 0.133 sec/batch)
2017-05-08 17:13:33.958393: step 46950, loss = 0.88 (981.3 examples/sec; 0.130 sec/batch)
2017-05-08 17:13:35.237426: step 46960, loss = 0.96 (1000.8 examples/sec; 0.128 sec/batch)
2017-05-08 17:13:36.531880: step 46970, loss = 0.64 (988.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:13:37.809824: step 46980, loss = 0.73 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 17:13:39.115981: step 46990, loss = 0.84 (980.0 examples/sec; 0.131 sec/batch)
2017-05-08 17:13:40.486318: step 47000, loss = 0.76 (934.1 examples/sec; 0.137 sec/batch)
2017-05-08 17:13:41.681560: step 47010, loss = 0.59 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-08 17:13:42.951990: step 47020, loss = 0.79 (1007.5 examples/sec; 0.127 sec/batch)
2017-05-08 17:13:44.225106: step 47030, loss = 0.71 (1005.4 examples/sec; 0.127 sec/batch)
2017-05-08 17:13:45.512198: step 47040, loss = 0.68 (994.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:13:46.809818: step 47050, loss = 0.69 (986.4 examples/sec; 0.130 sec/batch)
2017-05-08 17:13:48.092256: step 47060, loss = 0.95 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:13:49.372689: step 47070, loss = 0.63 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:13:50.687729: step 47080, loss = 0.64 (973.4 examples/sec; 0.132 sec/batch)
2017-05-08 17:13:51.993178: step 47090, loss = 1.02 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 17:13:53.364101: step 47100, loss = 0.79 (933.7 examples/sec; 0.137 sec/batch)
2017-05-08 17:13:54.540642: step 47110, loss = 0.68 (1087.9 examples/sec; 0.118 sec/batch)
2017-05-08 17:13:55.801865: step 47120, loss = 0.89 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-08 17:13:57.077862: step 47130, loss = 1.00 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:13:58.373479: step 47140, loss = 0.76 (988.0 examples/sec; 0.130 sec/batch)
2017-05-08 17:13:59.657294: step 47150, loss = 0.89 (997.0 examples/sec; 0.128 sec/batch)
2017-05-08 17:14:00.922366: step 47160, loss = 0.83 (1011.8 examples/sec; 0.127 sec/batch)
2017-05-08 17:14:02.234020: step 47170, loss = 0.69 (975.9 examples/sec; 0.131 sec/batch)
2017-05-08 17:14:03.528415: step 47180, loss = 0.66 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:14:04.822781: step 47190, loss = 0.79 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:14:06.219266: step 47200, loss = 0.79 (916.6 examples/sec; 0.140 sec/batch)
2017-05-08 17:14:07.403178: step 47210, loss = 0.71 (1081.2 examples/sec; 0.118 sec/batch)
2017-05-08 17:14:08.692551: step 47220, loss = 0.87 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 17:14:09.961023: step 47230, loss = 0.82 (1009.1 examples/sec; 0.127 sec/batch)
2017-05-08 17:14:11.224519: step 47240, loss = 0.85 (1013.1 examples/sec; 0.126 sec/batch)
2017-05-08 17:14:12.483229: step 47250, loss = 0.71 (1016.9 examples/sec; 0.126 sec/batch)
2017-05-08 17:14:13.757885: step 47260, loss = 0.82 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-08 17:14:15.077866: step 47270, loss = 0.70 (969.7 examples/sec; 0.132 sec/batch)
2017-05-08 17:14:16.399247: step 47280, loss = 0.77 (968.7 examples/sec; 0.132 sec/batch)
2017-05-08 17:14:17.712856: step 47290, loss = 0.92 (974.4 examples/sec; 0.131 sec/batch)
2017-05-08 17:14:19.102747: step 47300, loss = 0.68 (920.9 examples/sec; 0.139 sec/batch)
2017-05-08 17:14:20.289569: step 47310, loss = 0.80 (1078.5 examples/sec; 0.119 sec/batch)
2017-05-08 17:14:21.564999: step 47320, loss = 0.79 (1003.6 examples/sec; 0.128 sec/batch)
2017-05-08 17:14:22.853125: step 47330, loss = 0.81 (993.7 examples/sec; 0.129 sec/batch)
2017-05-08 17:14:24.130179: step 47340, loss = 0.91 (1002.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:14:25.424398: step 47350, loss = 0.60 (989.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:14:26.702131: step 47360, loss = 0.82 (1001.8 examples/sec; 0.128 sec/batch)
2017-05-08 17:14:27.985573: step 47370, loss = 0.78 (997.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:14:29.299520: step 47380, loss = 0.86 (974.2 examples/sec; 0.131 sec/batch)
2017-05-08 17:14:30.582545: step 47390, loss = 0.78 (997.6 examples/sec; 0.128 sec/batch)
2017-05-08 17:14:31.954540: step 47400, loss = 0.85 (932.9 examples/sec; 0.137 sec/batch)
2017-05-08 17:14:33.148873: step 47410, loss = 0.77 (1071.7 examples/sec; 0.119 sec/batch)
2017-05-08 17:14:34.420836: step 47420, loss = 0.83 (1006.3 examples/sec; 0.127 sec/batch)
2017-05-08 17:14:35.729708: step 47430, loss = 0.66 (977.9 examples/sec; 0.131 sec/batch)
2017-05-08 17:14:37.007645: step 47440, loss = 0.84 (1001.6 examples/sec; 0.128 sec/batch)
2017-05-08 17:14:38.311714: step 47450, loss = 0.73 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 17:14:39.591593: step 47460, loss = 0.76 (1000.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:14:40.874552: step 47470, loss = 0.65 (997.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:14:42.170583: step 47480, loss = 0.96 (987.6 examples/sec; 0.130 sec/batch)
2017-05-08 17:14:43.459437: step 47490, loss = 0.75 (993.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:14:44.819407: step 47500, loss = 0.79 (941.2 examples/sec; 0.136 sec/batch)
2017-05-08 17:14:46.020180: step 47510, loss = 0.82 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-08 17:14:47.279104: step 47520, loss = 0.71 (1016.7 examples/sec; 0.126 sec/batch)
2017-05-08 17:14:48.554883: step 47530, loss = 0.68 (1003.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:14:49.852356: step 47540, loss = 0.75 (986.5 examples/sec; 0.130 sec/batch)
2017-05-08 17:14:51.105279: step 47550, loss = 0.77 (1021.6 examples/sec; 0.125 sec/batch)
2017-05-08 17:14:52.379522: step 47560, loss = 0.81 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 17:14:53.643526: step 47570, loss = 0.63 (1012.7 examples/sec; 0.126 sec/batch)
2017-05-08 17:14:54.934606: step 47580, loss = 0.87 (991.4 examples/sec; 0.129 sec/batch)
2017-05-08 17:14:56.205812: step 47590, loss = 0.82 (1006.9 examples/sec; 0.127 sec/batch)
2017-05-08 17:14:57.573346: step 47600, loss = 0.77 (936.0 examples/sec; 0.137 sec/batch)
2017-05-08 17:14:58.779588: step 47610, loss = 0.71 (1061.1 examples/sec; 0.121 sec/batch)
2017-05-08 17:15:00.059783: step 47620, loss = 0.59 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 17:15:01.357097: step 47630, loss = 0.62 (986.7 examples/sec; 0.130 sec/batch)
2017-05-08 17:15:02.625651: step 47640, loss = 0.81 (1009.0 examples/sec; 0.127 sec/batch)
2017-05-08 17:15:03.905244: step 47650, loss = 0.87 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:15:05.210702: step 47660, loss = 0.60 (980.5 examples/sec; 0.131 sec/batch)
2017-05-08 17:15:06.474263: step 47670, loss = 0.61 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-08 17:15:07.788514: step 47680, loss = 0.91 (973.9 examples/sec; 0.131 sec/batch)
2017-05-08 17:15:09.070384: step 47690, loss = 0.62 (998.5 examples/sec; 0.128 sec/batch)
2017-05-08 17:15:10.452437: step 47700, loss = 0.95 (926.2 examples/sec; 0.138 sec/batch)
2017-05-08 17:15:11.682520: step 47710, loss = 0.65 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-08 17:15:1E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 971 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
2.960841: step 47720, loss = 0.63 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:15:14.235424: step 47730, loss = 0.97 (1004.2 examples/sec; 0.127 sec/batch)
2017-05-08 17:15:15.547203: step 47740, loss = 0.76 (975.8 examples/sec; 0.131 sec/batch)
2017-05-08 17:15:16.827152: step 47750, loss = 0.90 (1000.0 examples/sec; 0.128 sec/batch)
2017-05-08 17:15:18.120188: step 47760, loss = 0.62 (989.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:15:19.401482: step 47770, loss = 1.11 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 17:15:20.720042: step 47780, loss = 0.88 (970.7 examples/sec; 0.132 sec/batch)
2017-05-08 17:15:22.020942: step 47790, loss = 0.90 (983.9 examples/sec; 0.130 sec/batch)
2017-05-08 17:15:23.413494: step 47800, loss = 0.72 (919.2 examples/sec; 0.139 sec/batch)
2017-05-08 17:15:24.617661: step 47810, loss = 0.86 (1062.9 examples/sec; 0.120 sec/batch)
2017-05-08 17:15:25.897923: step 47820, loss = 0.76 (999.8 examples/sec; 0.128 sec/batch)
2017-05-08 17:15:27.163382: step 47830, loss = 0.70 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-08 17:15:28.428865: step 47840, loss = 0.67 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-08 17:15:29.715532: step 47850, loss = 0.74 (994.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:15:30.991261: step 47860, loss = 0.79 (1003.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:15:32.254874: step 47870, loss = 0.71 (1013.0 examples/sec; 0.126 sec/batch)
2017-05-08 17:15:33.558503: step 47880, loss = 0.61 (981.9 examples/sec; 0.130 sec/batch)
2017-05-08 17:15:34.830634: step 47890, loss = 0.79 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-08 17:15:36.179618: step 47900, loss = 0.84 (948.9 examples/sec; 0.135 sec/batch)
2017-05-08 17:15:37.370155: step 47910, loss = 0.90 (1075.1 examples/sec; 0.119 sec/batch)
2017-05-08 17:15:38.666352: step 47920, loss = 0.87 (987.5 examples/sec; 0.130 sec/batch)
2017-05-08 17:15:39.952218: step 47930, loss = 0.70 (995.4 examples/sec; 0.129 sec/batch)
2017-05-08 17:15:41.251251: step 47940, loss = 0.74 (985.3 examples/sec; 0.130 sec/batch)
2017-05-08 17:15:42.546271: step 47950, loss = 0.75 (988.4 examples/sec; 0.130 sec/batch)
2017-05-08 17:15:43.838565: step 47960, loss = 0.64 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:15:45.127137: step 47970, loss = 0.80 (993.3 examples/sec; 0.129 sec/batch)
2017-05-08 17:15:46.396414: step 47980, loss = 0.74 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-08 17:15:47.677212: step 47990, loss = 0.72 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:15:49.056630: step 48000, loss = 0.70 (927.9 examples/sec; 0.138 sec/batch)
2017-05-08 17:15:50.279834: step 48010, loss = 0.89 (1046.4 examples/sec; 0.122 sec/batch)
2017-05-08 17:15:51.549137: step 48020, loss = 0.63 (1008.4 examples/sec; 0.127 sec/batch)
2017-05-08 17:15:52.838324: step 48030, loss = 0.80 (992.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:15:54.157411: step 48040, loss = 0.78 (970.4 examples/sec; 0.132 sec/batch)
2017-05-08 17:15:55.462486: step 48050, loss = 0.77 (980.8 examples/sec; 0.131 sec/batch)
2017-05-08 17:15:56.747692: step 48060, loss = 0.84 (995.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:15:58.080031: step 48070, loss = 0.64 (960.7 examples/sec; 0.133 sec/batch)
2017-05-08 17:15:59.361190: step 48080, loss = 0.94 (999.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:16:00.664938: step 48090, loss = 0.92 (981.8 examples/sec; 0.130 sec/batch)
2017-05-08 17:16:02.062306: step 48100, loss = 0.81 (916.0 examples/sec; 0.140 sec/batch)
2017-05-08 17:16:03.258081: step 48110, loss = 0.64 (1070.4 examples/sec; 0.120 sec/batch)
2017-05-08 17:16:04.522487: step 48120, loss = 0.85 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-08 17:16:05.823862: step 48130, loss = 0.73 (983.6 examples/sec; 0.130 sec/batch)
2017-05-08 17:16:07.113369: step 48140, loss = 0.74 (992.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:16:08.415785: step 48150, loss = 0.77 (982.8 examples/sec; 0.130 sec/batch)
2017-05-08 17:16:09.696134: step 48160, loss = 0.71 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:16:10.973089: step 48170, loss = 0.86 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:16:12.267885: step 48180, loss = 0.85 (988.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:16:13.576363: step 48190, loss = 0.79 (978.2 examples/sec; 0.131 sec/batch)
2017-05-08 17:16:14.964822: step 48200, loss = 0.75 (921.9 examples/sec; 0.139 sec/batch)
2017-05-08 17:16:16.145129: step 48210, loss = 0.76 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-08 17:16:17.407622: step 48220, loss = 0.81 (1013.9 examples/sec; 0.126 sec/batch)
2017-05-08 17:16:18.710798: step 48230, loss = 0.86 (982.2 examples/sec; 0.130 sec/batch)
2017-05-08 17:16:20.008728: step 48240, loss = 0.69 (986.2 examples/sec; 0.130 sec/batch)
2017-05-08 17:16:21.298336: step 48250, loss = 0.57 (992.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:16:22.604990: step 48260, loss = 0.92 (979.6 examples/sec; 0.131 sec/batch)
2017-05-08 17:16:23.905154: step 48270, loss = 0.84 (984.5 examples/sec; 0.130 sec/batch)
2017-05-08 17:16:25.231733: step 48280, loss = 0.84 (964.9 examples/sec; 0.133 sec/batch)
2017-05-08 17:16:26.532220: step 48290, loss = 0.68 (984.3 examples/sec; 0.130 sec/batch)
2017-05-08 17:16:27.910541: step 48300, loss = 0.81 (928.7 examples/sec; 0.138 sec/batch)
2017-05-08 17:16:29.073952: step 48310, loss = 0.77 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-08 17:16:30.357248: step 48320, loss = 0.79 (997.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:16:31.644183: step 48330, loss = 0.78 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:16:32.923311: step 48340, loss = 0.73 (1000.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:16:34.227188: step 48350, loss = 0.72 (981.7 examples/sec; 0.130 sec/batch)
2017-05-08 17:16:35.520046: step 48360, loss = 0.99 (990.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:16:36.796908: step 48370, loss = 0.92 (1002.5 examples/sec; 0.128 sec/batch)
2017-05-08 17:16:38.094083: step 48380, loss = 0.82 (986.8 examples/sec; 0.130 sec/batch)
2017-05-08 17:16:39.398688: step 48390, loss = 0.76 (981.1 examples/sec; 0.130 sec/batch)
2017-05-08 17:16:40.788397: step 48400, loss = 0.75 (921.1 examples/sec; 0.139 sec/batch)
2017-05-08 17:16:41.986262: step 48410, loss = 0.73 (1068.6 examples/sec; 0.120 sec/batch)
2017-05-08 17:16:43.306012: step 48420, loss = 0.65 (969.9 examples/sec; 0.132 sec/batch)
2017-05-08 17:16:44.562493: step 48430, loss = 0.79 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-08 17:16:45.891241: step 48440, loss = 0.73 (963.3 examples/sec; 0.133 sec/batch)
2017-05-08 17:16:47.176247: step 48450, loss = 0.79 (996.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:16:48.488624: step 48460, loss = 0.68 (975.3 examples/sec; 0.131 sec/batch)
2017-05-08 17:16:49.782263: step 48470, loss = 0.68 (989.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:16:51.092563: step 48480, loss = 1.07 (976.9 examples/sec; 0.131 sec/batch)
2017-05-08 17:16:52.378828: step 48490, loss = 0.70 (995.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:16:53.794318: step 48500, loss = 0.99 (904.3 examples/sec; 0.142 sec/batch)
2017-05-08 17:16:54.979699: step 48510, loss = 0.78 (1079.8 examples/sec; 0.119 sec/batch)
2017-05-08 17:16:56.253977: step 48520, loss = 0.83 (1004.5 examples/sec; 0.127 sec/batch)
2017-05-08 17:16:57.537529: step 48530, loss = 0.64 (997.2 examples/sec; 0.128 sec/batch)
2017-05-08 17:16:58.794420: step 48540, loss = 0.81 (1018.4 examples/sec; 0.126 sec/batch)
2017-05-08 17:17:00.088084: step 48550, loss = 0.82 (989.4 examples/sec; 0.129 sec/batch)
2017-05-08 17:17:01.400553: step 48560, loss = 0.85 (975.3 examples/sec; 0.131 sec/batch)
2017-05-08 17:17:02.710302: step 48570, loss = 0.87 (977.3 examples/sec; 0.131 sec/batch)
2017-05-08 17:17:03.981705: step 48580, loss = 0.73 (1006.8 examples/sec; 0.127 sec/batch)
2017-05-08 17:17:05.266918: step 48590, loss = 0.67 (996.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:17:06.669108: step 48600, loss = 0.74 (912.9 examples/sec; 0.140 sec/batch)
2017-05-08 17:17:07.872063: step 48610, loss = 0.83 (1064.0 examples/sec; 0.120 sec/batch)
2017-05-08 17:17:09.165907: step 48620, loss = 0.74 (989.3 examples/sec; 0.129 secE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 991 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
/batch)
2017-05-08 17:17:10.477179: step 48630, loss = 0.78 (976.2 examples/sec; 0.131 sec/batch)
2017-05-08 17:17:11.750765: step 48640, loss = 0.83 (1005.0 examples/sec; 0.127 sec/batch)
2017-05-08 17:17:13.064297: step 48650, loss = 0.95 (974.5 examples/sec; 0.131 sec/batch)
2017-05-08 17:17:14.354144: step 48660, loss = 0.76 (992.4 examples/sec; 0.129 sec/batch)
2017-05-08 17:17:15.666885: step 48670, loss = 0.60 (975.0 examples/sec; 0.131 sec/batch)
2017-05-08 17:17:16.936978: step 48680, loss = 0.80 (1007.8 examples/sec; 0.127 sec/batch)
2017-05-08 17:17:18.260647: step 48690, loss = 0.73 (967.0 examples/sec; 0.132 sec/batch)
2017-05-08 17:17:19.643338: step 48700, loss = 0.82 (925.7 examples/sec; 0.138 sec/batch)
2017-05-08 17:17:20.836305: step 48710, loss = 0.78 (1073.0 examples/sec; 0.119 sec/batch)
2017-05-08 17:17:22.124781: step 48720, loss = 0.74 (993.4 examples/sec; 0.129 sec/batch)
2017-05-08 17:17:23.395856: step 48730, loss = 0.68 (1007.0 examples/sec; 0.127 sec/batch)
2017-05-08 17:17:24.683791: step 48740, loss = 0.77 (993.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:17:25.975138: step 48750, loss = 0.80 (991.2 examples/sec; 0.129 sec/batch)
2017-05-08 17:17:27.248836: step 48760, loss = 0.78 (1004.9 examples/sec; 0.127 sec/batch)
2017-05-08 17:17:28.513055: step 48770, loss = 0.80 (1012.5 examples/sec; 0.126 sec/batch)
2017-05-08 17:17:29.772675: step 48780, loss = 0.83 (1016.2 examples/sec; 0.126 sec/batch)
2017-05-08 17:17:31.050985: step 48790, loss = 0.74 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:17:32.435252: step 48800, loss = 0.91 (924.7 examples/sec; 0.138 sec/batch)
2017-05-08 17:17:33.627519: step 48810, loss = 0.66 (1073.6 examples/sec; 0.119 sec/batch)
2017-05-08 17:17:34.922820: step 48820, loss = 0.76 (988.2 examples/sec; 0.130 sec/batch)
2017-05-08 17:17:36.253248: step 48830, loss = 0.88 (962.1 examples/sec; 0.133 sec/batch)
2017-05-08 17:17:37.542248: step 48840, loss = 0.76 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:17:38.829408: step 48850, loss = 0.81 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 17:17:40.102934: step 48860, loss = 0.83 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 17:17:41.387796: step 48870, loss = 0.85 (996.2 examples/sec; 0.128 sec/batch)
2017-05-08 17:17:42.668545: step 48880, loss = 0.80 (999.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:17:43.937428: step 48890, loss = 0.90 (1008.8 examples/sec; 0.127 sec/batch)
2017-05-08 17:17:45.312854: step 48900, loss = 0.71 (930.6 examples/sec; 0.138 sec/batch)
2017-05-08 17:17:46.537622: step 48910, loss = 0.83 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-08 17:17:47.831398: step 48920, loss = 0.76 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 17:17:49.133761: step 48930, loss = 0.83 (982.8 examples/sec; 0.130 sec/batch)
2017-05-08 17:17:50.432941: step 48940, loss = 0.73 (985.2 examples/sec; 0.130 sec/batch)
2017-05-08 17:17:51.722404: step 48950, loss = 0.88 (992.7 examples/sec; 0.129 sec/batch)
2017-05-08 17:17:52.987806: step 48960, loss = 0.68 (1011.5 examples/sec; 0.127 sec/batch)
2017-05-08 17:17:54.258431: step 48970, loss = 0.66 (1007.4 examples/sec; 0.127 sec/batch)
2017-05-08 17:17:55.536273: step 48980, loss = 0.85 (1001.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:17:56.799487: step 48990, loss = 0.71 (1013.3 examples/sec; 0.126 sec/batch)
2017-05-08 17:17:58.171375: step 49000, loss = 0.91 (933.0 examples/sec; 0.137 sec/batch)
2017-05-08 17:17:59.356610: step 49010, loss = 0.80 (1080.0 examples/sec; 0.119 sec/batch)
2017-05-08 17:18:00.641192: step 49020, loss = 0.80 (996.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:18:01.943908: step 49030, loss = 0.72 (982.5 examples/sec; 0.130 sec/batch)
2017-05-08 17:18:03.220131: step 49040, loss = 0.98 (1003.0 examples/sec; 0.128 sec/batch)
2017-05-08 17:18:04.507370: step 49050, loss = 0.94 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 17:18:05.794342: step 49060, loss = 0.84 (994.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:18:07.099407: step 49070, loss = 1.08 (980.8 examples/sec; 0.131 sec/batch)
2017-05-08 17:18:08.393778: step 49080, loss = 0.74 (988.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:18:09.685941: step 49090, loss = 0.86 (990.6 examples/sec; 0.129 sec/batch)
2017-05-08 17:18:11.075187: step 49100, loss = 0.69 (921.4 examples/sec; 0.139 sec/batch)
2017-05-08 17:18:12.300579: step 49110, loss = 0.79 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-08 17:18:13.574152: step 49120, loss = 0.82 (1005.1 examples/sec; 0.127 sec/batch)
2017-05-08 17:18:14.854675: step 49130, loss = 0.83 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 17:18:16.138802: step 49140, loss = 0.69 (996.8 examples/sec; 0.128 sec/batch)
2017-05-08 17:18:17.431857: step 49150, loss = 0.78 (989.9 examples/sec; 0.129 sec/batch)
2017-05-08 17:18:18.720934: step 49160, loss = 0.83 (993.0 examples/sec; 0.129 sec/batch)
2017-05-08 17:18:20.022114: step 49170, loss = 0.79 (983.7 examples/sec; 0.130 sec/batch)
2017-05-08 17:18:21.320570: step 49180, loss = 0.80 (985.8 examples/sec; 0.130 sec/batch)
2017-05-08 17:18:22.598098: step 49190, loss = 0.80 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-08 17:18:23.958674: step 49200, loss = 0.66 (940.8 examples/sec; 0.136 sec/batch)
2017-05-08 17:18:25.152348: step 49210, loss = 0.75 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-08 17:18:26.438174: step 49220, loss = 0.70 (995.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:18:27.733099: step 49230, loss = 0.79 (988.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:18:29.035532: step 49240, loss = 0.64 (982.8 examples/sec; 0.130 sec/batch)
2017-05-08 17:18:30.340861: step 49250, loss = 0.92 (980.6 examples/sec; 0.131 sec/batch)
2017-05-08 17:18:31.617797: step 49260, loss = 0.69 (1002.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:18:32.895968: step 49270, loss = 0.79 (1001.4 examples/sec; 0.128 sec/batch)
2017-05-08 17:18:34.177208: step 49280, loss = 0.82 (999.0 examples/sec; 0.128 sec/batch)
2017-05-08 17:18:35.445590: step 49290, loss = 0.83 (1009.2 examples/sec; 0.127 sec/batch)
2017-05-08 17:18:36.834293: step 49300, loss = 0.68 (921.7 examples/sec; 0.139 sec/batch)
2017-05-08 17:18:38.015904: step 49310, loss = 0.71 (1083.3 examples/sec; 0.118 sec/batch)
2017-05-08 17:18:39.308210: step 49320, loss = 0.76 (990.5 examples/sec; 0.129 sec/batch)
2017-05-08 17:18:40.589776: step 49330, loss = 0.69 (998.8 examples/sec; 0.128 sec/batch)
2017-05-08 17:18:41.861192: step 49340, loss = 0.70 (1006.7 examples/sec; 0.127 sec/batch)
2017-05-08 17:18:43.152476: step 49350, loss = 0.74 (991.3 examples/sec; 0.129 sec/batch)
2017-05-08 17:18:44.446502: step 49360, loss = 0.83 (989.1 examples/sec; 0.129 sec/batch)
2017-05-08 17:18:45.762929: step 49370, loss = 0.77 (972.3 examples/sec; 0.132 sec/batch)
2017-05-08 17:18:47.067043: step 49380, loss = 0.92 (981.5 examples/sec; 0.130 sec/batch)
2017-05-08 17:18:48.373567: step 49390, loss = 0.79 (979.7 examples/sec; 0.131 sec/batch)
2017-05-08 17:18:49.729792: step 49400, loss = 0.79 (943.8 examples/sec; 0.136 sec/batch)
2017-05-08 17:18:50.927041: step 49410, loss = 0.68 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-08 17:18:52.214190: step 49420, loss = 0.86 (994.4 examples/sec; 0.129 sec/batch)
2017-05-08 17:18:53.505616: step 49430, loss = 0.76 (991.2 examples/sec; 0.129 sec/batch)
2017-05-08 17:18:54.787760: step 49440, loss = 0.75 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:18:56.092370: step 49450, loss = 0.85 (981.1 examples/sec; 0.130 sec/batch)
2017-05-08 17:18:57.368847: step 49460, loss = 0.87 (1002.8 examples/sec; 0.128 sec/batch)
2017-05-08 17:18:58.615785: step 49470, loss = 0.74 (1026.5 examples/sec; 0.125 sec/batch)
2017-05-08 17:18:59.898189: step 49480, loss = 0.67 (998.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:19:01.188884: step 49490, loss = 0.65 (991.7 examples/sec; 0.129 sec/batch)
2017-05-08 17:19:02.569271: step 49500, loss = 0.71 (927.3 examples/sec; 0.138 sec/batch)
2017-05-08 17:19:03.739392: step 49510, loss = 0.82 (1093.9 examples/sec; 0.117 sec/batch)
2017-05-08 17:19:05.014610: step 49520, loss = 0.79 (1003.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:19:06.300027: step 49530, loss = 0.84 E tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1011 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
(995.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:19:07.613580: step 49540, loss = 0.76 (974.5 examples/sec; 0.131 sec/batch)
2017-05-08 17:19:08.894117: step 49550, loss = 0.78 (999.6 examples/sec; 0.128 sec/batch)
2017-05-08 17:19:10.196798: step 49560, loss = 0.88 (982.6 examples/sec; 0.130 sec/batch)
2017-05-08 17:19:11.471680: step 49570, loss = 0.87 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-08 17:19:12.771661: step 49580, loss = 0.82 (984.6 examples/sec; 0.130 sec/batch)
2017-05-08 17:19:14.052333: step 49590, loss = 0.79 (999.5 examples/sec; 0.128 sec/batch)
2017-05-08 17:19:15.421024: step 49600, loss = 0.76 (935.2 examples/sec; 0.137 sec/batch)
2017-05-08 17:19:16.611723: step 49610, loss = 0.81 (1075.0 examples/sec; 0.119 sec/batch)
2017-05-08 17:19:17.886724: step 49620, loss = 0.69 (1003.9 examples/sec; 0.128 sec/batch)
2017-05-08 17:19:19.167117: step 49630, loss = 1.02 (999.7 examples/sec; 0.128 sec/batch)
2017-05-08 17:19:20.436048: step 49640, loss = 0.67 (1008.7 examples/sec; 0.127 sec/batch)
2017-05-08 17:19:21.700358: step 49650, loss = 0.72 (1012.4 examples/sec; 0.126 sec/batch)
2017-05-08 17:19:23.003977: step 49660, loss = 0.71 (981.9 examples/sec; 0.130 sec/batch)
2017-05-08 17:19:24.318713: step 49670, loss = 0.97 (973.6 examples/sec; 0.131 sec/batch)
2017-05-08 17:19:25.587957: step 49680, loss = 0.75 (1008.5 examples/sec; 0.127 sec/batch)
2017-05-08 17:19:26.894655: step 49690, loss = 0.70 (979.6 examples/sec; 0.131 sec/batch)
2017-05-08 17:19:28.282741: step 49700, loss = 1.07 (922.1 examples/sec; 0.139 sec/batch)
2017-05-08 17:19:29.502319: step 49710, loss = 0.83 (1049.5 examples/sec; 0.122 sec/batch)
2017-05-08 17:19:30.777835: step 49720, loss = 0.83 (1003.5 examples/sec; 0.128 sec/batch)
2017-05-08 17:19:32.096619: step 49730, loss = 0.83 (970.6 examples/sec; 0.132 sec/batch)
2017-05-08 17:19:33.372976: step 49740, loss = 0.86 (1002.9 examples/sec; 0.128 sec/batch)
2017-05-08 17:19:34.653051: step 49750, loss = 0.81 (999.9 examples/sec; 0.128 sec/batch)
2017-05-08 17:19:35.929027: step 49760, loss = 0.94 (1003.1 examples/sec; 0.128 sec/batch)
2017-05-08 17:19:37.224119: step 49770, loss = 0.71 (988.4 examples/sec; 0.130 sec/batch)
2017-05-08 17:19:38.509493: step 49780, loss = 0.83 (995.8 examples/sec; 0.129 sec/batch)
2017-05-08 17:19:39.821408: step 49790, loss = 0.82 (975.7 examples/sec; 0.131 sec/batch)
2017-05-08 17:19:41.199705: step 49800, loss = 0.84 (928.7 examples/sec; 0.138 sec/batch)
2017-05-08 17:19:42.424284: step 49810, loss = 0.73 (1045.3 examples/sec; 0.122 sec/batch)
2017-05-08 17:19:43.718179: step 49820, loss = 0.67 (989.3 examples/sec; 0.129 sec/batch)
2017-05-08 17:19:44.982595: step 49830, loss = 0.86 (1012.3 examples/sec; 0.126 sec/batch)
2017-05-08 17:19:46.285616: step 49840, loss = 0.96 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 17:19:47.596852: step 49850, loss = 0.75 (976.2 examples/sec; 0.131 sec/batch)
2017-05-08 17:19:48.881616: step 49860, loss = 0.71 (996.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:19:50.183163: step 49870, loss = 0.78 (983.4 examples/sec; 0.130 sec/batch)
2017-05-08 17:19:51.493920: step 49880, loss = 0.73 (976.5 examples/sec; 0.131 sec/batch)
2017-05-08 17:19:52.796989: step 49890, loss = 0.94 (982.3 examples/sec; 0.130 sec/batch)
2017-05-08 17:19:54.157716: step 49900, loss = 0.86 (940.7 examples/sec; 0.136 sec/batch)
2017-05-08 17:19:55.385701: step 49910, loss = 0.74 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-08 17:19:56.694357: step 49920, loss = 0.87 (978.1 examples/sec; 0.131 sec/batch)
2017-05-08 17:19:58.012168: step 49930, loss = 0.77 (971.3 examples/sec; 0.132 sec/batch)
2017-05-08 17:19:59.291831: step 49940, loss = 0.91 (1000.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:20:00.590378: step 49950, loss = 0.67 (985.7 examples/sec; 0.130 sec/batch)
2017-05-08 17:20:01.903671: step 49960, loss = 0.78 (974.7 examples/sec; 0.131 sec/batch)
2017-05-08 17:20:03.185885: step 49970, loss = 0.87 (998.3 examples/sec; 0.128 sec/batch)
2017-05-08 17:20:04.474570: step 49980, loss = 0.54 (993.3 examples/sec; 0.129 sec/bE tensorflow/core/util/events_writer.cc:162] The events file /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU has disappeared.
E tensorflow/core/util/events_writer.cc:131] Failed to flush 1016 events to /tmp/cifar10_train/events.out.tfevents.1494271937.GHC31.GHC.ANDREW.CMU.EDU
atch)
2017-05-08 17:20:05.782533: step 49990, loss = 1.09 (978.6 examples/sec; 0.131 sec/batch)
2017-05-08 17:20:07.157642: step 50000, loss = 0.71 (930.8 examples/sec; 0.138 sec/batch)
--- 6470.61742401 seconds ---
