
<section class="main-content">
      <h2>
<a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a><b>Competition</b></h2>

<h3>DistNet - Summary</h3>
 

  

<p>Distnet displayes techniques used in a distributed training algorithm for high-accuracy deep neural networks using Cuda on a cluster of nodes and a parameter-server architecture leveraging socket communication</p>

<h3>
<a id="designer-templates" class="anchor" href="#designer-templates" aria-hidden="true"><span class="octicon octicon-link"></span></a>Background</h3>
<p>Neural networks are a set of algorithms, modeled loosely after the human brain, that are designed to recognize patterns. They interpret sensory data through a kind of machine perception,
    labeling or clustering raw input. The patterns they recognize are numerical, contained in vectors, into which all real-world data, be it images, sound, text or time series, must be translated.
    Deep neural networks have more than a single hidden layer between the input and the output layer. Training the neural network is done through gradient descent and backpropagation. Deep neural networks
    with multiple convolution layers and followed by fully connected layers takes a long time to train due to the varying learning rates. To achieve high accuracy, the amount of data fed into these networks
    is extremely high, making the time for each epoch high. Training AlexNet on a single GPU(NVIDIA K20) takes about 100 epochs (6 days). Long training times for high-accuracy deep neural networks (DNNs) 
    impede research into new DNN architectures and slow the development of high-accuracy DNNs. Hence, we would like to explore the use of a cluster of GPU machines to accelerate the learning. 
    The speed and scalability of distributed algorithms is almost always limited by the overhead of communicating between servers; DNN training is not an exception to this rule.
    Therefore, the key consideration here is to reduce communication overhead wherever possible, while not degrading the accuracy of the DNN models that we train.</p>

<h3> <b>Algorithm</b> </h3>
<p>
 We are currently focussed on using data parallelism to reduce training time. We explore several flavours of data parallelism in terms of distributed computation and communication. 
</p>

<h3>Input</h3>
<p> To our program we have as input two pairs of images, two different RGB frames (converted to grayscale) and their two depth equivalents. The depth images have been captured from the Kinect. Each pixel in the depth image tells us how far that pixel is from the camera in millimeters. We normalize this to tell us the depths in meters.
</p>
<h3>
<!--<a id="designer-templates" class="anchor" href="#designer-templates" aria-hidden="true"><span class="octicon octicon-link"></span></a>Pre-process Depth</h3> -->

  <h3>Build Image Pyramids</h3>
<p>
  We build “pyramids” of the two grayscale and depth image pairs. The bottom-most level (level 0) of the pyramid consist of the whole images. Level i consists of a blurred and down-sampled version of level i-1. This is done so that we can track all magnitudes of motion, coarse to fine. 
 <div class = "box">
    <img src="img/rgb0.png" />
  </div>
  <div class = "box">
    <img src="img/rgb1.png" />
  </div>

</p>

  <h3>Pre-process Depth</h3>

<p>
  We preprocess the depth images to only consider pixels that are within a particular range of depths. For example, we ignore pixels that are too far away to affect tracking, or that are so far away that they may be obfuscated by other objects.
  <div class = "box">
    <img src="img/depth0.png" />
  </div>
  <div class = "box">
    <img src="img/depth1.png" />
  </div>
</p>


  <h3>Compute Image Derivatives</h3>
<p>
For each level of the grayscale images, we compute the gradients of the images in the x- and y- directions using the Sobel operator, which is a common filter used for edge detection. We threshold the image derivatives to decide which pixels have moved considerably, and only consider those pixels for measure correspondences between the two images. 
</p>

<div class = "box">
    <img src="img/dx1.png" />
</div>
<div class = "box">
    <img src="img/dy1.png" />
</div>


 <h3>Compute Textured Mask</h3>
<p>
The textured mask of each image is the set of pixels whose gradients satisfy a threshold. We only consider these pixels when computing correspondences between the grayscale images. 
</p>

<div class = "box">
    <img src="img/texturedMask.png" />
</div>

 <h3>Generate 3D point cloud</h3>
<p>
Using the image coordinates and the true depth values, we can construct the 3D coordinates corresponding the each pixel in the image by doing an inverse projection. We apply our current estimate of the rotation and translation to the 3D point cloud, and project the new cloud back onto the second image. We attempt to minimize the difference between our “estimated second image” and the true second image. 
</p>

<h3>The estimation algorithm</h3>
<p>
  In the main algorithm, we loop over the different pyramid levels, starting from the coarsest, and repeatedly do the following:
  <ol>
    <li>Compute correspondences between the two images based on our current estimate of the rotation and translation matrix.</li>
    <li>Use these correspondences to refine the estimate of the rotation and translation matrix.</li>
  </ol>
</p>

<h3>Computing correspondences</h3>
<p>
  Each call to count correspondences does the following:
  <ol>
    <li>Inverse project depth images to get the true 3D coordinates of the image pixels (only those pixels that satisfy a minimum gradient).</li>
    <li>Transform the 3D cloud with the current motion parameters.</li>
    <li>Project the transformed 3D points onto the second depth image.</li>
    <li>Store all the pixel mappings as correspondences.</li>
  </ol>
</p>


<h3> Refining the motion parameters (ksi: &xi;) </h3>
  <p> Each call to recompute Ksi does the following:
     <ol>
    <li>Calculate the squared difference between the two grayscale images at the computed points of correspondence.</li>
    <li>Use the derivatives, the cloud points and the camera properties to solve linear equations over the sets of correspondences.</li>
  </ol>
  </p>

<h3> Performance </h3>

<div class = "box">
    <img src="img/warpResult.png" />
</div>

<p>
  The above set of images shows the result of using the estimated motion parameters to warp the image at time <i>t</i>. We can see that from time <i>t</i> to time <i>t+1</i> the camera has been translated to the bottom and left. The warped image accurately reflects that motion.
</p>

<div>
  <img src="img/resultsInd.png" />
</div>

<p>
  The bar graph above compares the performances the parallel and serial versions of a few parts of the algorithm. The serial version has been taken from the contrib/ module of OpenCV. Our parallel algorithm is also based on it.
</p>

<p>
  'Compute correspondences' is the step of the algorithm that uses inverse projections and gradient thresholding to compute a set of correspondences. These correspondences are then used later in the 'Sigma' step of the algorithm to calculate the squared error between the two images. The results of 'Sigma' and 'Compute correspondences' is then encoded as a set of linear equations, which are solved in the 'Rigid Body' step to compute the rigid body transform between two camera views.
</p>

<div align="center">
  <img src="img/results.png" />
</div>

<p>
  This bar graph compares the total running times of the two algorithms. Estimating the rigid body transform between two frames takes nearly 800 milliseconds with the serial version, which means that we can process image streams at about 1.2 frames per second.
</p>

<p>
  We have managed to improve this frame rate to about 2 frames per second. This is far from real-time, but is adequate for several slow moving robots. The CubeRover, for example, moves at 15 centimeters per second. Moreover, we feel that without significant changes to the algorithm itself, it is very challenging to extract more parallelism out of Dense RGB-D odometry, due in no small part to the inherent serial nature of the algorithm (iterative refinement).
</p>

<p>
  We will present cogent arguments defending this stance in our final presentation.
</p>


      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/uprasad/CubeRover">CubeRover</a> is maintained by <a href="https://github.com/uprasad">uprasad</a> & <a href="https://github.com/nikithashr">nikithashr</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>
