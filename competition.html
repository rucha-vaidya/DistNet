
<section class="main-content">
      <h2>
<a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a><b>Competition</b></h2>

<h3>DistNet - Summary</h3>
 

  

<p>Distnet displayes techniques used in a distributed training algorithm for high-accuracy deep neural networks using Cuda on a cluster of nodes and a parameter-server architecture leveraging socket communication</p>

<h3>
<a id="designer-templates" class="anchor" href="#designer-templates" aria-hidden="true"><span class="octicon octicon-link"></span></a>Background</h3>
<p>Neural networks are a set of algorithms, modeled loosely after the human brain, that are designed to recognize patterns. They interpret sensory data through a kind of machine perception,
    labeling or clustering raw input. The patterns they recognize are numerical, contained in vectors, into which all real-world data, be it images, sound, text or time series, must be translated.
    Deep neural networks have more than a single hidden layer between the input and the output layer. Training the neural network is done through gradient descent and backpropagation. Deep neural networks
    with multiple convolution layers and followed by fully connected layers takes a long time to train due to the varying learning rates. To achieve high accuracy, the amount of data fed into these networks
    is extremely high, making the time for each epoch high. Training AlexNet on a single GPU(NVIDIA K20) takes about 100 epochs (6 days). Long training times for high-accuracy deep neural networks (DNNs) 
    impede research into new DNN architectures and slow the development of high-accuracy DNNs. Hence, we would like to explore the use of a cluster of GPU machines to accelerate the learning. 
    The speed and scalability of distributed algorithms is almost always limited by the overhead of communicating between servers; DNN training is not an exception to this rule.
    Therefore, the key consideration here is to reduce communication overhead wherever possible, while not degrading the accuracy of the DNN models that we train.</p>

<h3> <b>Algorithm</b> </h3>
<p>
 We are currently focussed on using data parallelism to reduce training time. We explore several flavours of data parallelism in terms of distributed computation and communication. Data parallelism can be achieved in two ways - feeding different input files and/or shuffling data before each run. 
</p>

<h3> <b> Technology</b></h3>
<p>We use Tensorflow for CNN layer construction and all of the CNN related operations. Message sending between different nodes happens through the low-level socket API
      </p>
      <!--<a id="designer-templates" class="anchor" href="#designer-templates" aria-hidden="true"><span class="octicon octicon-link"></span></a>Pre-process Depth</h3> -->

  <h3>Build Image Pyramids</h3>
<p>
  We build “pyramids” of the two grayscale and depth image pairs. The bottom-most level (level 0) of the pyramid consist of the whole images. Level i consists of a blurred and down-sampled version of level i-1. This is done so that we can track all magnitudes of motion, coarse to fine. 
 <div class = "box">
    <img src="img/rgb0.png" />
  </div>
  <div class = "box">
    <img src="img/rgb1.png" />
  </div>






      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/uprasad/CubeRover">CubeRover</a> is maintained by <a href="https://github.com/uprasad">uprasad</a>  <a href="https://github.com/nikithashr">nikithashr</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    
